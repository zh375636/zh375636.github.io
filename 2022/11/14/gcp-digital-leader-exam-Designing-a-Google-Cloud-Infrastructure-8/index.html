<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Course IntroductionWelcome to designing a Google Cloud infrastructure. I’m Guy Hummel, and I’ll be showing you how to build an enterprise IT solution in Google platform. To get the most from this co">
<meta property="og:type" content="article">
<meta property="og:title" content="gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8">
<meta property="og:url" content="https://example.com/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="Course IntroductionWelcome to designing a Google Cloud infrastructure. I’m Guy Hummel, and I’ll be showing you how to build an enterprise IT solution in Google platform. To get the most from this co">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-14T16:36:54.000Z">
<meta property="article:modified_time" content="2022-11-21T06:38:34.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:36:54" itemprop="dateCreated datePublished" datetime="2022-11-14T12:36:54-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:38:34" itemprop="dateModified" datetime="2022-11-21T02:38:34-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to designing a Google Cloud infrastructure. I’m Guy Hummel, and I’ll be showing you how to build an enterprise IT solution in Google platform.</p>
<p>To get the most from this course, unless you already have a lot of experience using Google Cloud, you should take the Google Cloud Platform Fundamentals and Systems Operations courses to get a solid understanding of the different components of Google Cloud. In this course, I’ll be showing you how to use these building blocks to construct an enterprise class application architecture.</p>
<p>We’re going to use a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/case-study-1/">case study</a> as an example of how to apply enterprise principles to a design. I’ll start by explaining how you would take an organization’s requirements, and translate them into the appropriate <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/compute-1/">compute</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/storage-2/">storage</a>, and network components in Google Cloud. I’ll also show you how to make it a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> design.</p>
<p>Then I’ll cover how to secure the environment, including how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/authentication-2/">authenticate</a> and give permissions to people as well as to applications using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/service-accounts-1/">service accounts</a>, how to encrypt your data, and how to comply with a rigorous security standard like PCI DSS.</p>
<p>Finally, we’ll wrap up with how to design a solution that can recover from <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disasters</a>.</p>
<p>All right, if you’re ready to learn how to create an enterprise class architecture for your Google Cloud infrastructure, then let’s get started.</p>
<h1 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h1><p>Suppose you’ve been hired to help a company called Great Inside, which offers interior design software as a service.</p>
<p>Great Inside makes its money by selling subscriptions to its web-based interior design application. It also has a free version that’s supported by advertising. Their customers are primarily in North America, but they hope to expand in Europe and Asia at some point in the future.</p>
<p>The company has grown slowly for five years, but recently closed a venture capital round, brought in experienced executives, and is now growing more quickly. The company’s existing infrastructure is not capable of scaling up quickly enough, so they would like to move to the cloud.</p>
<p>Great Inside started off with a Microsoft-centric infrastructure and then migrated to a LAMP stack. The only Microsoft infrastructure left is the payment processing system and an Active Directory server. They would like to retire their Microsoft servers in the future, other than Active Directory. But that isn’t a priority right now, and the company would like to move both types of servers to the cloud. They’ve also started a pilot project using a NoSQL database.</p>
<p>Since they accept credit cards, they need to be PCI DSS compliant. Since their volume is increasing, they need to ensure that their payment processing environment meets a higher level of compliance. Note that Great Inside passes the validation and processing of credit card information to a certified payment processor.</p>
<p>They would like to improve their disaster recovery solution. At the moment, they’re backing their data up to a cloud service, but it would take them a long time to recover from a disaster.</p>
<p>Their existing technical environment is all in a single data center.</p>
<p>They have three types of databases. MySQL for the interior design application, Microsoft SQL Server for payment processing, and a NoSQL database in the development environment.</p>
<p>They have two types of web and application servers. Apache and Tomcat are running on six servers, each with 2 dual-core CPUs, 24GB of RAM, and two mirrored 200GB disks. These servers are for their interior design application. IIS is running on four servers- two customer-facing and two internal, each with a dual-core CPU, 16GB of RAM and two mirrored 250GB disks. These servers are for payment processing.</p>
<p>They have a variety of infrastructure servers, including Active Directory and a file server for internal documents, etc.</p>
<p>Here are their business requirements. Scale easily to handle rapid growth, move as much of the development, test, and production infrastructure as possible to the cloud, and increase performance, reliability, and security while reducing management overhead.</p>
<p>And their technical requirements are: connect the data center’s network with the cloud environment’s network, encrypt all data, design <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> into all tiers, and create a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disaster recovery</a> solution that will reduce recovery time to a few hours, rather than a day.</p>
<p>I should mention up front that some aspects of this case study may not be completely realistic. It’s simplified so we can go through it in a reasonable amount of time, but it has just enough complexity to allow us to cover the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">key topics</a>.</p>
<h1 id="Compute"><a href="#Compute" class="headerlink" title="Compute"></a>Compute</h1><p>Although most Google Cloud designs include virtual machine instances, that doesn’t mean VMs are your only option for compute resources. Before you start designing a solution using only Compute Engine instances, you should consider App Engine and Kubernetes Engine.</p>
<p>App Engine is designed for people who don’t want to manage an application’s underlying infrastructure. App Engine provisions and scales all of the resources your application needs behind the scenes, without any human intervention required. That sounds great, doesn’t it? So why wouldn’t you use App Engine?</p>
<p>The main reason is that it is much easier to develop a new application on App Engine than it is to migrate an existing one to it. So if you’re developing an application from scratch, then App Engine may be a good choice. If you have an existing application, then you’ll need to check if App Engine supports the programming languages your app is written in and if your app has any operating system dependencies (such as only being able to run on Windows, which isn’t supported by App Engine).</p>
<p>You’ll also need to look at your application’s architecture to see if it would be able to run on App Engine without having to re-architect it. App Engine is designed for microservices-based apps, so if your existing application has a monolithic architecture, then it might require some work to migrate it.</p>
<p>For all of these reasons, it’s usually advisable to use App Engine only for new applications rather than existing ones.</p>
<p>The next option is Kubernetes Engine. It provides many of the benefits of App Engine, in that you don’t have to worry about the underlying operating system running your application. It also handles scaling, although you have to configure that yourself first. It does require more management than App Engine, but it doesn’t require as much management as Compute Engine.</p>
<p>The ideal case for using Kubernetes Engine is, of course, if your application already runs in containers, especially Docker containers, since that’s what Kubernetes Engine supports. On the other hand, if your application will only run on certain operating systems, especially Windows, then it won’t run in Kubernetes Engine.</p>
<p>If you have an existing app that does not currently run in containers, then you might want to see if it’s possible to containerize it so you can take advantage of Kubernetes Engine.</p>
<p>If your existing application runs on virtual machines, then the easiest way to migrate it to Google Cloud is to use Compute Engine instances. If it doesn’t run on virtual machines, then you’ll have to virtualize it before you can run it on Google Cloud.</p>
<p>Although Compute Engine requires more management than App Engine or Kubernetes Engine, it does give you ultimate flexibility. For example, you could run an application that requires Windows, a specific network driver, and high-performance GPUs.</p>
<p>Since our case study involves an existing application that doesn’t currently run in containers, we’re going to choose Compute Engine for our design.</p>
<p>The case study company, GreatInside, currently has 6 machines running Apache and Tomcat, and 4 machines running IIS. Let’s have a look at Google’s predefined machine types . We need to decide how many vCPUs and how much memory to use. Memory is pretty straightforward. Our existing machines have 24GB for the Tomcat servers and 16GB for the IIS servers. VCPUs are more complicated, though.</p>
<p>The existing Tomcat servers have two dual-core CPUs and the IIS servers have one dual-core CPU. How does that translate into vCPUs? Some people say that cores and vCPUs are equivalent, but that’s not quite true. A vCPU on a Compute Engine instance is implemented as a single hyper-thread on an Intel Xeon processor. Since each Xeon processor has 2 hyperthreads, that means you need to multiply the number of cores by 2 to get the number of threads, and thus the number of vCPUs.</p>
<p>So our Tomcat servers have the equivalent of 8 vCPUs (4 cores times 2) and our IIS servers have the equivalent of 4 vCPUs (2 cores times 2). Of course, if we really wanted to be accurate, we’d need to take into account things like the clock speed of the CPUs, but we’re not going to go that far.</p>
<p>So, we need 8 vCPUs and 24GB of RAM for the Tomcat servers and 4 vCPUs and 16GB of RAM for the IIS servers. Do any of the predefined machine types match these requirements? Well, the n1-standard-4 is almost identical to the IIS server requirements. It has 4 vCPUs and 15GB of RAM. Having one less gig of RAM is probably fine, but you can monitor it in production to make sure it’s sufficient.</p>
<p>The Tomcat servers are another story, though. The closest match is the n1-standard-8, which has 8 vCPUs and 30GB of memory. That’s 6GB more than we need, so we should consider a custom machine type. We can select the exact size we need. With this custom configuration, it says it will cost $190.54 per month. Let’s see how that compares to the n1-standard-8. That costs $194.58 per month, which is more expensive, but only 2% more.</p>
<p>I should mention that there are a couple of ways to reduce those costs: sustained-use discounts and committed-use discounts. If you know that you’re going to be running an instance continuously for a long period of time, then you can pay much less by purchasing either a one-year or three-year contract, which is called a committed-use contract. This will typically reduce the cost by up to 57%. However, that’s a pretty big commitment, so Google provides a way to reduce costs without signing a long-term contract. You start getting an automatic discount after an instance runs for more than 25% of a month, and the discount increases the longer the instance runs during that month. For most machine types, you’ll receive a sustained-use discount of 30% if you run the instance for the entire month.</p>
<p>Okay, let’s get back to our case study. Since IIS and SQL Server run on Windows, we’ll need to figure out how to license them. Let’s start with IIS. For Windows Server itself, you can either use Google’s pay-as-you-go Windows licensing or you can bring your own license. </p>
<p>There are two ways to use Google’s pay-as-you-go Windows licensing. The first way is to create a new instance with one of the pre-configured Windows Server boot disks . The second way is to import a Windows VM. There are two options for importing a VM. The first option is to import a virtual disk and turn it into an image that you can use to create a Compute Engine instance. That’s quite simple to do, but it’s not meant for migrating mission-critical applications or migrating a large number of VMs in an automated fashion.</p>
<p>A more sophisticated option is to use Cloud Migrate for Compute Engine. This service makes replicas of existing VMs you have running on-premises or on another cloud platform. It will take care of the many steps that are needed to migrate important applications. </p>
<p>If you want to bring your own Windows licenses, then you can run your Windows VMs on sole-tenant nodes, which are dedicated physical servers that are not shared with other customers.</p>
<p>If you need to run any Microsoft applications, then you’ll need licenses for those too, of course, but Microsoft is more flexible with its application licensing than with Windows licensing. If your organization has active Software Assurance contracts for its Microsoft applications, then you can move those licenses to either Compute Engine instances or sole-tenant nodes.</p>
<p>Now let’s move on to SQL Server. You can use any of the options I just mentioned, but fortunately, there are also easier options for SQL Server. One option is to create instances with pre-configured SQL Server boot disks . These include pay-as-you-go licenses for both Windows Server and SQL Server. The second option is to use Cloud SQL, which is a managed service. I’ll tell you more about it in the next video.</p>
<p>For premium Linux OSs (such as Red Hat or SUSE), licensing is much simpler. You can either create an instance with a pre-configured boot disk or you can import your Linux VM. In both cases, you can either use a Google pay-as-you-go license or bring your own license.</p>
<p>I should mention one other Compute Engine option – preemptible VMs. They’re up to 80% cheaper than regular instances, but since Google can remove them with only 30 seconds’ notice, you would usually only use them as disposable instances for things like big data batch jobs. That doesn’t fit our use case, so we’ll stick with regular instances.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><p>Each of the instances for the Tomcat and IIS servers will come with a standard persistent boot disk by default, but we might need something different. There are many options for instance storage, including Standard Persistent Disk, SSD Persistent Disk, Local SSD, RAM Disk, and Cloud Storage.</p>
<p>Standard Persistent Disks are magnetic drives. Their main advantage is low cost. SSD Persistent Disks (or solid state disks) have up to 4 times the throughput and up to 40 times the I&#x2F;O operations per second of a Standard Persistent Disk, so if you need high performance, SSDs are a must.</p>
<p>But SSD Persistent Disks aren’t even your fastest option. Local SSDs are up to 600 times as fast as Standard Persistent Disks in IOPS and up to 15 times as fast in throughput.</p>
<p>Why are Local SSDs so much faster than SSD Persistent Disks, which are obviously both using SSD technology? Well, it’s because Local SSDs are not redundant and are directly attached to an instance. That gives them major speed advantages, but with high risk because if they suffer a hardware failure, then your data will be gone. Furthermore, Local SSDs disappear when you stop or delete an instance, so you should only use them for temporary data that you can afford to lose, such as a cache.</p>
<p>There are a couple more disadvantages of Local SSDs too. First, they are only available in one size – 375GB, which is kind of an awkward number. Second, they can’t be used as boot disks.</p>
<p>If you need even faster storage, then you can use a RAM disk, which essentially makes a chunk of memory look like a filesystem. Although RAM disks are the fastest option, they’re even less durable than Local SSDs, so they’re only suitable for temporary data. It’s also an expensive option because RAM is much more expensive than SSDs.</p>
<p>One more option is Cloud Storage. This is kind of a weird way to add storage to an instance because a bucket is object storage rather than block storage. That means it can’t be used as a root disk and it may be unreliable as a mounted filesystem. So why would you ever use it? The first advantage of using Cloud Storage is that multiple instances can write to a bucket at the same time. You can’t do that with persistent disks, which can only be shared between instances in read-only mode. The danger is that one instance could overwrite changes made by another instance, so your application would have to take that into account.</p>
<p>The second advantage is that an instance can access a bucket in a different zone or region, which is great for sharing data globally, especially if it’s read-only data, which would avoid the overwriting problem.</p>
<p>However, Cloud Storage usually isn’t a good option for instance storage. It is good for general-purpose file serving, though, so it would be a potential choice for replacing GreatInside’s internal file server if they want to move it to the cloud. To do this, you’d need to use Cloud Storage FUSE, which is open source software that translates object storage names into a file and directory system. Essentially, it makes Cloud Storage buckets look like network file systems. A better choice, though, would be Cloud Filestore, which is a fully-supported file sharing service that’s designed specifically for this purpose. It’s compatible with NFS version 3.</p>
<p>So, which instance storage option should we use for our instances? Since performance is important, we should use something faster than Standard Persistent Disks. SSD Persistent Disks are many times faster than standard ones, so they’d be a good choice. Should we consider Local SSDs or RAM disks? Well, neither of those can be boot disks, so we would have to use them in addition to a persistent boot disk. The higher performance wouldn’t outweigh the extra cost and complexity of using one of these options, though, so we should just stick with SSD Persistent Disks. Furthermore, since persistent disks are redundant, we don’t need to have two mirrored disks on each instance like GreatInside does in its existing data center. We can just have a single persistent boot disk on each instance.</p>
<p>As for the size, we can specify the exact amount we need, so for the Tomcat servers, we should use one 200GB disk on each instance, and for the IIS servers, we should use one 250GB disk on each.</p>
<p>Next, we need to look at our database options. Google Cloud has 5 different database services: Cloud SQL, Cloud Datastore, Bigtable, BigQuery, and Cloud Spanner.</p>
<p>Cloud SQL is a relational database. It’s a managed service for MySQL, PostgreSQL, or Microsoft SQL Server. It’s suitable for everything from blogs to ERP and CRM to ecommerce.</p>
<p>Cloud Datastore is a NoSQL database service. Unlike a relational database, such as Cloud SQL, it is horizontally scalable. A relational database can scale vertically, meaning that you can run it on a more powerful VM to handle more transactions, but there are obviously limits to the size of a VM. You can also scale a relational database horizontally for reads by using read replicas, but most relational databases can’t scale horizontally for writes. That is a major problem that is solved by NoSQL databases.</p>
<p>Because of this and because it’s an eventually consistent database, Cloud Datastore is faster than Cloud SQL. It’s best suited to relatively simple data and queries, especially key-value pairs. Typical examples include user profiles, product catalogs, and game state. For complex queries, Cloud SQL is a better choice.</p>
<p>Cloud Bigtable is also a NoSQL database. It’s designed to scale into the petabyte range with high throughput and low latency. It does not support ACID transactions, so it shouldn’t be used for transaction processing. It’s best suited for storing huge amounts of single-keyed data. If you have less than one terabyte of data, then Bigtable is not the best solution. It can handle big data in real-time or in batch processing. Typical examples are Internet of Things applications and product recommendations.</p>
<p>BigQuery also handles huge amounts of data, but it’s more of a data warehouse. It’s something you use after data is collected, rather than being a transactional system. It’s best suited to aggregating data from many sources and letting you search it using SQL queries. In other words, it’s good for OLAP (that is, Online Analytical Processing) and business intelligence reporting.</p>
<p>Google’s newest database service is Cloud Spanner, which seems to combine the best of all worlds. It’s a relational database that also scales horizontally. That is, it combines the best features of traditional databases like Cloud SQL and the best features of NoSQL databases like Cloud Datastore. So why wouldn’t you use it for all of your database needs? Well, mostly because it’s more expensive than the other options. Also, if your application is written specifically for a particular database, such as MySQL, then Cloud SQL would be a better choice, unless you can rewrite it to work with Cloud Spanner. </p>
<p>So use Cloud Spanner when you need a relational database that is massively scalable. Typical uses are financial services and global supply chain applications.</p>
<p>Now, which database services should GreatInside use? It currently has two production databases – MySQL for the interior design application and SQL Server for payment processing. There are two ways you could migrate the MySQL database to Google Cloud. You could use Cloud SQL or run MySQL on a regular instance. Considering that GreatInside wants to reduce system management tasks, Cloud SQL would be the best choice since it’s a fully managed MySQL service, with automatic replication and backups.</p>
<p>For SQL Server, you have the same two options. You could use Cloud SQL, or you could run it on a regular instance. Again, Cloud SQL is the best choice.</p>
<p>GreatInside does have one more database – their experimental NoSQL datastore. Since the development team is still evaluating this technology, you should talk to them about trying Cloud Datastore. They should also try App Engine because Cloud Datastore works best when used with App Engine.</p>
<p>And that’s it for storage and databases.</p>
<h1 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h1><p>Before we talk about networks, we need to talk about how we can make our applications highly available.</p>
<p>If you have an application that’s running on only one VM instance, then, of course, it’s a single point of failure, and if it goes down, your application goes down. So, at a minimum, you should always have at least two VMs for every component of your solution. But where should those instances be located?</p>
<p>When you create a VM instance, it gets created in a particular zone, such as us-central1-a. A zone is an isolated location. You can think of a zone as a data center or an isolated portion of a data center.</p>
<p>If you put both instances in the same zone, then both of them could potentially go down if there’s a problem in that zone. So, you should put the instances in different zones. For performance reasons, you may need to put them in zones that are in the same region, such as us-central1. Notice that the zone name is just the region name with a dash and a letter at the end. All of the zones in a region have high-bandwidth, low-latency network connections between them, so if instances that are spread across a region need to mirror data with each other, then they can do this quickly.</p>
<p>Although “region” sounds like a geographic area, it’s just a data center campus in one location. For example, all of the zones in the us-central1 region are in Council Bluffs, Iowa. So, for maximum availability, you may also want to distribute your instances across different regions.</p>
<p>For a higher level of availability, you can use autoscaling instance groups. This was covered extensively in the “Google Cloud Platform: Systems Operations” course, so I’ll just go over the highlights.</p>
<p>An instance group consists of identical instances that perform processing for your application. If one of the instances fails, then a health check will notice this and replace the instance with a new one. If the load on the instance group gets too high, then the autoscaler will add more instances to maintain good application performance.</p>
<p>To ensure availability even if an entire zone fails, you should distribute the instances across multiple zones. Luckily, this is very easy to do. You just have to select “Multizone” when you’re creating the instance group.</p>
<p>If you want to make sure you’ll still have enough instances to handle the load if an entire zone goes down, then you should overprovision by 50%. For example, if your instances are spread across 3 zones and you need 6 instances to handle your normal traffic load, then you should provision 9 instances. That way if one of the zones goes down (which would take out 3 of the instances), you’ll still have 6 instances left in the two remaining zones.</p>
<p>You can either overprovision by 50% at all times or you could save money by just setting the upper limit on your autoscaler to at least 50% more than the normal number of instances. If you decide to depend on the autoscaler during a zone failure, then the instances in your remaining two zones will be very heavily loaded until the autoscaler provisions additional instances, so only choose this option if you can tolerate this temporary performance degradation.</p>
<p>Since GreatInside has 6 web tier instances for its main application, this is how it should be set up. For the 2 customer-facing IIS instances in the payment processing system, you’d set an upper limit of 3 instances, which is 50% more than the 2 instances that it normally needs.</p>
<p>To make the instance group work as a high availability solution, you’ll need a couple of other components. First, the instance group has to be behind a load balancer that will distribute incoming requests to different instances. Second, the instances cannot have any stateful data. Otherwise, the same instance would have to handle all requests from a given user. Although you can enable the “session affinity” option in this situation, it will ruin your high availability since a failed instance will impact all of the users on it.</p>
<p>Since most applications do have stateful data, you have to put it on other components, such as a database or Cloud Storage. Unfortunately, that just moves the availability issue to a different layer, but fortunately, Google Cloud has good ways to handle storage availability.</p>
<p>If Cloud Storage is sufficient for your stateful data needs, then you’re covered because Cloud Storage is automatically replicated either across zones in a region (for the Regional type) or across regions (for the Multi-Region type).</p>
<p>If you need a database for your stateful data, then there are different availability solutions depending on the data service.</p>
<p>With Cloud SQL, you can simply check the “High availability” box when you create a Cloud SQL instance. This will create a failover replica in another zone. In the event of a failure, Cloud SQL will automatically fail over to the replica. This option is available for MySQL, PostgreSQL, and SQL Server.</p>
<p>Since Cloud Datastore is a NoSQL database, it scales horizontally, which makes high availability easier than with Cloud SQL. Cloud Datastore automatically replicates data across zones in a region. When you create a Datastore instance, you specify which region and it does the rest.</p>
<p>Bigtable is also a NoSQL database that scales horizontally, but if you want it to replicate across multiple zones, then you’ll have to configure it to do that. You can even configure it to support replication across regions if you need that. But in its simplest configuration, it only stores data in a single zone, which gives it higher performance. It’s still stored redundantly in that configuration but within the same zone.</p>
<p>BigQuery automatically replicates data within a region, but it’s a data warehouse, so it’s not suitable for real-time stateful data storage.</p>
<p>Cloud Spanner also automatically replicates data within a region, so it’s highly available out of the box, and unlike Cloud SQL, it doesn’t need a failover replica, which is a less available solution.</p>
<p>In summary, if a NoSQL database is sufficient for your application, then Cloud Datastore is your best choice for storing stateful data. If you need to use a relational database, then either use Cloud SQL and enable high availability or use Cloud Spanner for even higher availability if you’re willing to pay a higher price.</p>
<p>Since GreatInside is going to use Cloud SQL for both MySQL and SQL Server, then we just need to enable the high availability option when we create those databases.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Networks"><a href="#Networks" class="headerlink" title="Networks"></a>Networks</h1><p>Now we know all of the components we want to use and we just need to connect them together with networks. Google provides what are called Virtual Private Clouds, or VPCs, but I’m just going to call them networks.</p>
<p>There are 5 layers in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">Google Cloud</a> you can use to isolate and manage resources: organizations, folders, projects, networks, and subnetworks.</p>
<p>You aren’t required to have organizations or folders, but they can be useful, especially for large companies.</p>
<p>Projects are required, though. You use them to provide a level of separation between resources. Not only are resources in different projects unable to communicate with each other, but they’re even in different billing accounts. Projects also have separate security controls, so for example, you could give Bob in QA the highest level of access in the Test Environment project, but a lower level of access in the Production Environment project.</p>
<p>Each project has one default network that comes with preset configurations and firewall rules to make it easier to get started, but you can customize it, or you can create up to 4 additional networks (for a total of 5). If 5 networks per project isn’t enough for you, then you can request a quota increase to support up to 15 networks in each project.</p>
<p>A network belongs to only one project, a subnet belongs to only one network, and an instance belongs to only one subnet.</p>
<p>Instances in the same subnet or even different subnets within the same network can communicate with each other. Subnetworks are used to group and manage resources.</p>
<p>A network spans all regions, but each subnet can only be in one region. A subnet allows you to define an IP address range and a default gateway for the instances you put in it. The IP address ranges of the different subnets must be non-public (such as 10.0.0.0) and must not overlap, but other than that, there are no restrictions on them. For example, they can be different sizes. They must be IPv4 addresses, though, because Compute Engine doesn’t support IPv6 yet.</p>
<p>A network can have either automatic or custom subnets. With automatic, the subnets are created for you, one in each region. With custom, you create them yourself. If you discover that you need to customize a network with automatic subnets, then you can convert it to custom mode, but once you do, you cannot convert it back to automatic mode.</p>
<p>On the default network, instances within the same subnet can communicate with each other over any TCP or UDP port, as well as with ICMP. Instances in the same network can communicate with each other, regardless of which subnets they’re in, because Google Cloud creates routes between all of the subnets in a network. However, the default network’s firewall rules only allow ssh, rdp, and icmp traffic between subnets.</p>
<p>If you don’t want instances in different subnets to be able to reach each other, then you can change the firewall rules to deny traffic between them.</p>
<p>Note that only the default network comes with predefined firewall rules. When you create a new network, it doesn’t have any firewall rules. However, the instances in that network will still be able to communicate with the Internet, assuming they have external IP addresses, because all outgoing traffic from instances is allowed. Only incoming traffic is blocked. And when an instance sends a request over the Internet, the incoming response is allowed, so two-way traffic is enabled at that point.</p>
<p>Each network includes a local DNS server so VM instances can refer to each other by name. The fully qualified domain name for an instance is [HOSTNAME].c.[PROJECT_ID].internal. This name is tied to the internal IP address of the instance. An Instance does not know its external IP address and name. That translation is handled elsewhere in the network.</p>
<p>To reach Internet resources, each VM needs an external IP address. An ephemeral external IP address is created for each VM by default, but an ephemeral address gets replaced with another one if you stop and restart the instance, so if you want an instance to always have the same IP address, then you need to assign a static IP address to it.</p>
<p>Since IPv4 addresses are a scarce resource, Google doesn’t want customers to waste them. So you’re not charged for having static IP addresses as long as you’re using them. But if a static IP address is not associated with a VM instance or if it’s associated with an instance that’s not running, then you’ll be charged for it.</p>
<p>Normally, if a VM needs to send requests to other Google services, such as Cloud Storage, then by default, it has to do so using a public IP address rather than an internal one. This is problematic if you don’t want any of your internal network communications to go over the Internet. However, if you enable the Private Google Access option in a subnet, then VMs in that subnet can connect to Google services using internal IP addresses, so their requests will go over Google’s network rather than the Internet.</p>
<p>If you want instances in different projects to communicate with each other, then you have three options: the Internet, VPC Network Peering, or a Shared VPC. Connecting over the Internet is slower, less secure, and more expensive than the other two options, so it’s not usually the best choice.</p>
<p>The simplest alternative is VPC Network Peering. This allows two VPCs to connect over a private RFC 1918 space, that is, using non-routable internal IP addresses, such as 10.x.y.z. In other words, they don’t need public IP addresses, and they communicate over Google’s network. Not only can you do this for VPCs in different projects, but you can even use it to connect VPCs in different organizations. To make this work, both sides have to set up a peering association. If only one side sets up a peering association with the other VPC, then the networks won’t be able to communicate with each other. Also bear in mind that there can’t be any overlapping IP ranges in the two networks. You’ll notice in this example that the two ranges are not overlapping.</p>
<p>A more complicated option is to use a Shared VPC. The idea is that instances in different projects can share the same network. This is kind of a weird idea. If you’ve put resources in different projects, you probably want them to be managed separately, so why would you get them to use the same network? In most cases, it’s to enforce security standards. For example, if you want to use the same firewall rules across all of your projects, then this is a good way to do that.</p>
<p>To set up a Shared VPC, you need to designate one of the projects as the host project and the others as service projects. The host project is the one that contains the Shared VPC. Instances in the service projects can use subnets in the Shared VPC. This is made possible by giving Service Project Admins the authority to create and manage instances in the Shared VPC but nothing more. Meanwhile, the Shared VPC Admins have full control over the network. Note that all of the projects in this arrangement have to be part of the same organization.</p>
<p>OK, we’ve gone over a lot of networking topics. Now how should we apply these concepts to GreatInside?</p>
<p>At a minimum, we should create separate projects for the Development, Test, and Production environments. Inside each project, we should stick with the default network. There’s no need to add any additional ones. We should also stick with automatic subnetworks. The only subnetwork we need right now is one in the US, such as us-central1, since we don’t currently have any plans to expand into other parts of the world. When GreatInside decides to add instances overseas, then they can be added to the other regional subnets.</p>
<p>The default firewall rules should also be fine, since they only allow internal traffic plus ssh, icmp, http, and https. We should remove the rule that allows rdp traffic in the Production network, though, since we don’t have any Windows instances in it.</p>
<p>We don’t want the Production, Development, and Test environments to be part of the same network, so we don’t need a Shared VPC. In fact, we don’t want them to communicate with each other at all, so we don’t need to use VPC Network Peering either.</p>
<p>By the way, you probably noticed that everything I’ve shown so far is only for the interior design application. I’m going to get into the details of how to set up the payment processing environment in the Legislation and Compliance lesson.</p>
<p>One last item is that we have to decide which components need external IP addresses. That’s easy in this case because the load balancer is the only one that needs an external IP address (and ideally it should be a static address). Users will connect to the web instances through the load balancer, so the web instances only need internal IP addresses, and for security reasons, that’s all they should have.</p>
<p>That does raise the question of how a system administrator could connect to them for troubleshooting, though. One way is to give your administrators access to the internal network by interconnecting it with the company’s on-premises network. That’s something that GreatInside has already requested, so let’s see how to do that. There are three ways: Cloud VPN, Cloud Interconnect, and Direct Peering.</p>
<p>Cloud VPN lets you set up a virtual private network connection between your own network and Google Cloud. To do this, you need to have a peer VPN gateway in your own network and it needs to use IPsec to connect to the Cloud VPN Gateway and encrypt traffic. You can have multiple tunnels to a single VPN gateway.</p>
<p>By itself, Cloud VPN requires you to make changes to static routes on your tunnels manually. But if you use Google Cloud Router, then the routes will be updated dynamically using BGP (that is, Border Gateway Protocol). Network topology changes are propagated automatically.</p>
<p>The second way to connect is called Cloud Interconnect. Instead of connecting over the Internet, you can use an enterprise-grade connection to Google’s network edge. There are two ways to do this: Dedicated Interconnect and Partner Interconnect. If your internal network extends into a colocation facility where Google has a point of presence, then you can connect your network to Google’s. This is called Dedicated Interconnect. It’s a great solution that provides higher bandwidth and lower latency than a connection over the public internet. It’s a bit expensive, though, because the minimum bandwidth is 10 Gbps.</p>
<p>If you don’t have a presence in a supported colocation facility or you want to pay for a connection that’s smaller than 10 Gbps, you can use Partner Interconnect. With this option, you connect to a service provider that has a presence in a supported colocation facility. You can purchase a monthly contract for connections as small as 50 Mbps and as large as 10 Gbps. </p>
<p>The third way is to use Peering. This is similar to Cloud Interconnect because you connect your network to Google’s network at a point of presence either directly (which is called Direct Peering) or through a service provider (which is called Carrier Peering). One big difference with peering is that it doesn’t cost anything. So why would anyone pay for Cloud Interconnect when they could peer with Google for free? Well, because with Cloud Interconnect you get a direct connection between your on-premises network and one of your VPCs in Google Cloud. You have full control over the routing between your networks. If you want to change a route, you can change it on your on-premises router, and it will be picked up by BGP. Although the peering option uses BGP, too, it’s done at the most basic level. It doesn’t create any custom routes in your VPC network.</p>
<p>Since we don’t have requirements for low latency and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> between the company network and Google Cloud, we should go with Cloud VPN to connect. We should also use Cloud Router so network routes will be updated dynamically.</p>
<p>And that’s it for networks.</p>
<h1 id="Authentication"><a href="#Authentication" class="headerlink" title="Authentication"></a>Authentication</h1><p>The first step in giving secure access to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">Google Cloud infrastructure</a> is to decide how to authenticate your users. By default, Google Cloud Platform requires users to have a Google account to access it. But if you have more than a handful of users, then you’ll want to find a centralized way to manage your user accounts. The solution is to use the G Suite Global Directory. You don’t have to use G Suite products like Google Docs, you can just use G Suite for user management.</p>
<p>Most organizations already have a user directory, so the best policy is usually to manage users in your existing directory, and then synchronize the account information in G Suite. There are three ways to do this: Google Cloud Directory Sync or GCDS, the Google Apps Admin SDK, or a third party connector.</p>
<p>Google Cloud Directory Sync is the easiest solution if you have either Active Directory or an LDAP server. It synchronizes users, groups, and other data from your existing directory to your Google Cloud Domain Directory. GCDS runs inside your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/networks-1/">network</a> on a machine that you control.</p>
<p>It’s a one-way synchronization, so GCDS doesn’t modify your existing directory. Of course the synchronization can’t be a one-time event. It has to happen on a regular basis to keep your Google Directory up-to-date.</p>
<p>To make authentication even easier for your users, you can implement single sign-on or SSO. Google Cloud Platform supports SAML 2.0-based SSO. If your system doesn’t support SAML 2.0, then you can use a third party plugin.</p>
<p>Once you’ve implemented SSO, then when a user would normally have to login, Google will redirect your authentication system. If the user is already authenticated in your system, then they don’t have to login to Google Cloud separately. If they aren’t already logged in, then they’re prompted to login.</p>
<p>In order for this to work, your users must have a matching account in Google’s Directory. So you still need to use GCDS or one of the other synchronization options.</p>
<p>In our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/case-study-1/">case study</a>, since we have an active directory server, we’ll use GCDS for synchronization and also implement single sign-on.</p>
<p>And that’s it for authentication.</p>
<h1 id="Roles"><a href="#Roles" class="headerlink" title="Roles"></a>Roles</h1><p>To give a user permission to access particular Google Cloud resources, you assign a role to them. Basic roles act at the project level. There are 3 basic roles available: Owner, Editor, and Viewer. There are also fine-grained roles for individual resources. These are called predefined roles. (They were previously known as curated roles.) For example, the Cloud SQL Viewer role gives read-only access to Cloud SQL resources.</p>
<p>You can assign roles at different levels of the hierarchy, that is, at the organization, folder, project, and resource levels. If you assign roles to the same user at different levels, then their effective permissions are the union of the permissions at the different levels.</p>
<p>For example, if you granted Marie the Viewer role at the organization level and the Editor role at the project level, then she would have Editor permissions for all of the resources in that project. The Viewer role at the organization level would not override the Editor role at the project level. Similarly, if you assigned them in the opposite way, with the Editor role at the organization level and the Viewer role at the project level, Marie would still have the Editor role for all of the resources in that project because the project-level permissions would not override the organization-level permissions.</p>
<p>There are a few principles you should apply when setting roles and permissions. </p>
<p>First, use the principle of least privilege when granting roles. That is, assign roles with the least permissions required for people to do what they need.</p>
<p>Second, whenever possible, assign roles to groups instead of to individuals. Then, when you need to grant a role to a user, you can just add them to the group. Not only is this easier to manage, but it also ensures consistent privileges among members of a particular group. You can also use a descriptive group name that makes it clear why group members need those permissions.</p>
<p>Third, keep tight control of who can add members to groups and change policies. If you don’t, then people could give themselves or others more privileges than they should have.</p>
<p>Fourth, to make sure that inappropriate policy changes aren’t made, audit all policy changes by checking the Cloud Audit Logs, which record project-level permission changes.</p>
<p>Now let’s apply these principles to GreatInside. First, you would grant the project owner role to a few key system administrators. Owners are the only ones who can change policies (unless you grant users the Organization Administrator role). You should always have more than one owner. Otherwise, if that person is unavailable or leaves the organization, it would be difficult to for someone else to take their place as owner. So avoid that situation by giving the owner role to several people, but choose wisely because owners can do just about anything. </p>
<p>Similarly, you would have a small number of G Suite administrators who could add users to groups.</p>
<p>Obviously, there would be a large number of users who would need permissions, so I’m not going to talk about every type of user, but I’ll give a couple of examples. One example would be a network administration group that you would grant the Compute Network Admin role to.</p>
<p>Another example would be a QA team. You could grant their group the editor role on the Test Environment project and the viewer role on the Production Environment project. Alternatively, if the QA people don’t need full access to the Test Environment, then you could grant them several predefined roles, such as Compute Instance Admin, Cloud SQL Admin, and Compute Storage Admin.</p>
<p>Regarding audit logs, someone would need to take on the responsibility of checking for policy changes. The Admin Activity audit logs are viewable by all project members, so you wouldn’t need to grant access to the person who does the checking.</p>
<p>And that’s it for roles.</p>
<h1 id="Service-Accounts"><a href="#Service-Accounts" class="headerlink" title="Service Accounts"></a>Service Accounts</h1><p>Now that you have user authentication and permissions figured out, it’s time to plan how your applications will access the Cloud Platform services it needs to use. To avoid embedding credentials in an application, you need to use service accounts. For example, if an application uses Cloud Datastore as a database, then it needs to have authorization to use the Datastore API.</p>
<p>You would accomplish this by enabling Datastore API access on any VM instances that will be involved in the part of the application that uses the database. By default, all VM instances run as the Compute Engine default service account. If you want something different, then you can create your own.</p>
<p>A service account has an email address and a public&#x2F;private key pair that it uses to prove its identity. Your instances use that identity when communicating with other Cloud Platform services. However, by default, an instance running as the Compute Engine default service account has limited scope in how it can interact with other services. For example, by default an instance can only read from Cloud Storage and can’t write to it.</p>
<p>To give an instance more permissions, you need to set the scope when you’re creating the VM. So, in the case of interacting with Datastore, you have to enable access to the Datastore API. You also have to enable the Datastore API at the project level, but you only have to do that once.</p>
<p>Then your application code has to obtain credentials from the service account whenever it uses the Datastore API. Google Cloud Platform uses OAuth 2.0 for API authentication and authorization. There are two ways to do it: Application Default Credentials and access tokens.</p>
<p>The easiest way is to use Google Cloud Client Libraries. They use Application Default Credentials (or ADC) to authenticate with Google APIs and send requests to those APIs. One great feature of ADC is that you can test your application locally and then deploy it to Google Cloud without changing the application code. </p>
<p>Here’s how it works. To run your code outside Google Cloud Platform, such as in your on-premise data center or on another cloud platform, create a service account and download its credentials file to the servers where the code will be running. Then set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the credentials file.</p>
<p>So while you’re developing locally, the application can authenticate using the credentials file and when you run it on a production instance, it will authenticate using the instance’s service account. This works because ADC allows applications to get credentials from multiple sources.</p>
<p>The second way is to use OAuth2 access tokens to directly connect to the API without going through a client library. One reason you’d have to use this method is if your application needs to request access to user data.</p>
<p>The way it works is the application requests an access token from the metadata server and then uses the token to make an API request. Tokens are short-lived, so your application needs to request new ones regularly.</p>
<p>If you need to write shell scripts that access other Cloud Platform services, then you can use gcloud and gsutil commands to make API calls. These two tools are included by default in most Compute Engine images and they automatically use the instance’s service account to authenticate with APIs.</p>
<p>So what service accounts would you need to create for GreatInside? The load balancer and the web instances communicate over HTTPS, so you don’t a service account for that. Since the Tomcat instances communicate with the MySQL database in Cloud SQL, you would need a service account for that. Similarly, the IIS instances communicate with SQL Server in Cloud SQL, so you’d need a service account for that, too. There may be a need for other service accounts when we add more features to our architecture, such as disaster recovery, but we’ll cover that later.</p>
<p>And that’s it for service accounts.</p>
<h1 id="Data-Protection-and-Encryption"><a href="#Data-Protection-and-Encryption" class="headerlink" title="Data Protection and Encryption"></a>Data Protection and Encryption</h1><p>Protecting data is critical in any organization. Google Cloud Platform is very strong in this area because of its default encryption policies. Before we get into encryption, though, let’s look at Access Control Lists (or ACLs).</p>
<p>ACLs specify who has access to Cloud Storage buckets and objects in buckets. I’m not going to cover this topic in depth, but there are a few things to keep in mind when you’re deciding what ACLs to apply to your Cloud Storage.</p>
<p>First, there are actually five different mechanisms for controlling access to Cloud Storage: IAM permissions, ACLs, Signed URLs, Signed Policy Documents, and Firebase Security Rules. With so many different ways to control access, you have to be careful not to create conflicting permissions. Start with the first two: IAM permissions and ACLs.</p>
<p>IAM permissions work at the project level. For example, you can specify that a user has full control of all the objects in all of the buckets in your project, but cannot create, modify, or delete the buckets themselves. So they’re a nice way to grant broad access to buckets and objects, but if you want to set fine-grained access, such as which buckets or objects a particular group can read, then you need to use ACLs.</p>
<p>The confusing thing about using these two mechanisms is that you have to look at both of them to get a complete picture of access permissions. For example, you could list the ACLs for a bucket and see that only Bob has been granted write access, but it wouldn’t show that Jill has also been granted write access to all buckets by IAM. For this reason, whenever possible, you should try to use either IAM or ACLs, but not both.</p>
<p>Another potential source of confusion is that bucket and object ACLs are independent of each other. The ACLs on a bucket do not affect the ACLs on objects inside that bucket. For example, you might think that Jane doesn’t have access to the objects in a bucket because she hasn’t been granted access to the bucket itself, but she could have been granted access to any of the objects in the bucket.</p>
<p>So you should keep a couple of principles in mind. First, apply the principle of least privilege. Grant users and groups only as much access as they need. Second, keep your access control as simple as possible. Try to use as few control mechanisms as you can.</p>
<p>If GreatInside decides to replace its internal file server using Cloud Storage, then the best way to secure the files would be to use ACLs. You would create groups to match the teams in the company and create ACLs that give those groups access to the appropriate resources. For example, you could create a bucket for each group. Then for each bucket, you would make the associated group a writer of the bucket. Finally, you would set the object default permissions so that any new objects uploaded to the bucket would get the same permissions and everyone in the group would have full access. If the company’s needs aren’t that simple, then you would set more complex ACLs.</p>
<p>Now let’s move on to encryption. To ensure that your data is encrypted at all times, it needs to be encrypted when it’s in storage (also known as “at rest”) and when it is being sent over a network (also known as “in flight”). Google Cloud Platform takes care of both of these situations.</p>
<p>Encryption in flight is handled very simply. All of the Cloud Platform services are accessible only by API (even when you’re using other methods, such as the Cloud Console or the gcloud command, they’re making API calls under the hood). And all API communication is encrypted using SSL&#x2F;TLS channels. Furthermore, every request has to include a time-limited authentication token, so the token can’t be used by an attacker after it expires. Of course, for any communications between your Google Cloud infrastructure and outside parties, such as website visitors, you have to use SSL&#x2F;TLS yourself to encrypt the traffic.</p>
<p>Encryption at rest is just as simple if you’re willing to leave it to Google because Cloud Platform encrypts all customer data at rest by default.</p>
<p>So without you having to do anything, all of your data will be encrypted both at rest and in flight. Then why isn’t this the end of this lesson? Well, because your organization might want to take on some of the encryption responsibilities itself.</p>
<p>There are actually two layers of encryption for data at rest. First, the data is broken into subfile chunks, and each chunk is encrypted with an individual data encryption key (or DEK). These keys are stored near the data to ensure low latency and high availability. The DEKs are then encrypted with a key encryption key (or KEK). The keys are AES-256 symmetric encryption keys.</p>
<p>Google always manages the data encryption keys, but your organization can manage the key encryption keys if that’s your preference. There are two options for doing this: Customer-managed encryption keys or Customer-supplied encryption keys.</p>
<p>With the customer-managed option, you use the Cloud KMS service to create, rotate (or automatically rotate), and destroy your encryption keys. The keys are hosted on Google Cloud. You can have as many keys as you want, even millions of them if you actually need that many. You can set user-level permissions on individual keys using IAM and monitor their use with Cloud Audit Logging.</p>
<p>Cloud KMS is a nice service, but why wouldn’t you just let Google manage your key encryption keys and not have to deal with it yourself? The biggest reason is compliance with standards or regulatory requirements, such as HIPAA (for health information) or PCI (for credit card information).</p>
<p>If your organization requires that you generate your own keys and&#x2F;or that they’re managed on-premises, then you have to use Customer-supplied encryption keys. Be aware that this option is only available for Cloud Storage and Compute Engine.</p>
<p>With CSEK, Google doesn’t store your key. You have to provide your key for each operation, and your key is purged from Google Cloud after the operation is complete. Here’s how to do it from the command line with each of the two supported services. To encrypt the disk on a Compute Engine instance, you add the csek-key-file flag and point it to a file that contains the key. To encrypt data you’re uploading to Cloud Storage, you have to do it a bit differently. Rather than adding an encryption flag to the gsutil command, you need to add the encryption key to your .boto file, which is the configuration file for the gsutil command. Then all of your gsutil commands will use that key.</p>
<p>It only stores an SHA256 hash of the key as a way to uniquely identify the key that was used to encrypt the data. When you make a request to read or write the data in the future, your key can be validated against the hash. The hash cannot be used to decrypt your data.</p>
<p>There’s a big risk in using this method, though. If you lose your keys, you won’t ever be able to read your data again, and you’ll end up deleting it so you won’t be paying storage charges for unreadable data.</p>
<p>So far all of the encryption methods we’ve covered, including default encryption, Cloud KMS, and CSEK have been examples of server-side encryption. This is where your data is encrypted after Google Cloud receives your data. The only major difference between the 3 methods is where the key comes from. But there is another way. It’s client-side encryption. This means that you encrypt the data before you send it to Google Cloud. Google won’t even know that it’s already encrypted and it will encrypt it again. When you read your data back, Google Cloud will decrypt it on the server side first and then you’ll decrypt your own layer of encryption on the client side. The same warning applies - if you lose your keys, your data will effectively be gone.</p>
<p>Since our case study includes credit card information, we’ll need to be PCI DSS compliant, so we should use Cloud KMS to manage our keys. I’ll talk more about PCI compliance in the next lesson.</p>
<h1 id="Legislation-and-Compliance"><a href="#Legislation-and-Compliance" class="headerlink" title="Legislation and Compliance"></a>Legislation and Compliance</h1><p>Google Cloud Platform has passed annual audits for some of the most important security standards, including SOC 1, 2, and 3, ISO 27001, and PCI DSS. It also complies with HIPAA, CSA STAR, the EU-US Privacy Shield Framework, and MPAA controls, none of which require annual audits.</p>
<p>So if your organization is required to comply with any of these standards, then you know that Google has done its part. But this is a shared responsibility because <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">your security processes and applications</a> running on top of Google’s infrastructure also need to comply.</p>
<p>You’ll notice that Network is listed for both Google and the customer. That’s because Google takes care of some parts of networking and the customer takes care of the rest. One of the most interesting areas of shared responsibility for network security is protecting against distributed denial of service (or DDoS) attacks.</p>
<p>Google provides many features to help deal with DDoS attacks, but it’s up to the customer to use them properly. Here are some of the techniques.</p>
<p>Reduce the attack surface by</p>
<ul>
<li>Isolating and securing your deployment with firewall rules</li>
<li>Google also provides anti-spoofing protection by default</li>
</ul>
<p>Isolate your internal traffic from the external world by</p>
<ul>
<li>Deploying instances without public IPs unless necessary</li>
</ul>
<p>Use Load Balancing</p>
<ul>
<li>Because a load balancer acts as a proxy that hides your internal instances</li>
</ul>
<p>Use Cloud Armor</p>
<ul>
<li>This service is specifically designed to provide DDoS defense</li>
<li>And it works with Load Balancing</li>
<li>It protects against layer 3 and layer 4 DDoS attacks</li>
</ul>
<p>Google Cloud also enforces API rate limits and resource quotas to prevent a spike in one customer’s activity from affecting other Cloud Platform customers.</p>
<p>Now it’s time to get back to the PCI DSS standard and how to comply with it. If your organization accepts credit card payments, then you need to comply with this standard or you could be fined. More importantly, if you have security flaws that allow hackers to steal credit card information from your systems, then it would be very damaging to both your customers and your reputation.</p>
<p>In the case study, GreatInside provides an interface for collecting credit card information, but it passes the validation and processing of the information to a Certified Payment Processor. This makes the company an SAQ A-EP merchant in PCI lingo. I’ll go over Google’s recommendations for how this type of merchant could comply with PCI DSS.</p>
<p>First, you have to check that the other parties involved (that is, Google Cloud and the payment processor) are certified for your volume of transactions (since there are different PCI DSS merchant levels based on the number of transactions). Google Cloud Platform has the highest level of PCI DSS certification, so that’s not a concern, but you’ll have to check your payment processor’s certification level because your volume might exceed their certification level.</p>
<p>Here’s a suggested architecture to handle the company’s credit card processing. Here’s how it works. A customer enters their credit card information in a form on your website. Then your payment-processing application sends the information to the external payment processor. Now the payment processor tells your application whether the card was accepted or declined. After that your payment processing application sends some or all of the response data to your core application, so it knows how to proceed with this customer.</p>
<p>You also need to log and monitor all of these interactions. Every instance involved in payment processing sends its logs to Stackdriver Logging and its alerts to Stackdriver Monitoring.</p>
<p>Now let’s move on to how you would set this up. To reduce the number of systems that need to be PCI-compliant, you have to fully isolate your payment-processing environment from the rest of your production environment. The best way to do this is to use a separate Google Cloud account, rather than just a separate project within your main account.</p>
<p>Then use IAM to grant access only to people who absolutely need to work on the payment-processing environment, such as people who will be deploying new versions of the application or managing the systems. These people must also pass a background check first.</p>
<p>To create the instances, you should first create your own Linux image that’s based on one of the preconfigured boot disk images and that contains the bare minimum of additional software needed to run your application. Then use this custom image when creating all of your VM instances.</p>
<p>To secure the network, create firewall rules that only allow three types of inbound traffic:</p>
<ul>
<li>HTTPS traffic from the load balancer to the payment form servers, so that customers can reach your payments page</li>
<li>Credit card authorization responses from the external payment processor to your internal payment authorization servers, and</li>
<li>VPN traffic from your internal office network to the VPN Gateway, so your authorized people can manage and audit the application and systems.</li>
</ul>
<p>Then create firewall rules for outbound traffic. There’s only one type of outbound traffic you need to allow – HTTPS traffic from the payment form servers to the external payment processor, so they can send credit card authorization requests.</p>
<p>Now all of the traffic in and out of the network is locked down, but you’ll also have to open up internal traffic, such as:</p>
<ul>
<li>From all of the instances to Google’s NTP servers for time synchronization, and</li>
<li>SSH traffic from the VPN Gateway to all of the instances, so authorized people can access the systems for maintenance</li>
</ul>
<p>OK, let’s move on to deploying your application. To be compliant, you have to make sure you’re deploying the correct application every time, that it’s deployed securely, and that no other software packages are installed during the deployment. If you don’t already have an automated deployment tool, then you might want to use Cloud Deployment Manager, which could automate the creation of everything in your payment-processing environment, even the firewall rules. It could also help you create an audit trail of deployments.</p>
<p>Since you’ve used the same custom Linux image for all of your instances, you’ll need to install additional software on each instance. For example, some instances may need a web server, while others don’t, and each instance should only have the software it needs, which will reduce your security risks. To make this process consistent and reliable, it should be automated as well. The easiest way to automate software installation and configuration is to use a configuration management tool such as Chef, Puppet, or Ansible. Cloud Academy has courses on all three of these tools, so check one out if you’re not familiar with how to use any of them. </p>
<p>There are a few packages that you’ll want to install on all instances. First there’s iptables. You can set it up to log all network activity to and from each instance. This data is required for PCI DSS compliance audits.</p>
<p>Second, each instance needs the Stackdriver Monitoring and Logging agents so it can send logs and alerts.</p>
<p>Third, each instance should run an Intrusion Detection System (or IDS) to alert you to suspicious activity.</p>
<p>Finally, your configuration management tool needs to securely retrieve and launch the latest version of your application. </p>
<p>Even with an automated deployment, you’d still need to verify the integrity of the software being deployed. You could do this by running an automated checksum comparison against each package as it’s installed. You could also run an automated code analysis tool to check for security flaws.</p>
<p>Now let’s move on to logging. To be compliant, every step in the payment-processing environment has to be monitored and recorded. All instance activity and all user activity must be logged. Stackdriver Logging is a great service for collecting logs. You can record network traffic to and from your instances by enabling VPC Flow Logs on each subnet in your VPC.</p>
<p>By the way, you might think that we need to assign a service account to the instances so they can write logs to Stackdriver, but the default service account for VM instances already grants write access to Stackdriver, so you don’t need to configure that yourself.</p>
<p>I mentioned that user activity needs to be logged, but you also need to log the activity of people who have administrative access to the environment. The easiest way is to log all shell commands.</p>
<p>The amount of log information generated by all of this is likely to be very large, so you might want to export your Stackdriver logs to BigQuery if you need to do some complex analysis.</p>
<p>In addition to logging, you also need to set up real-time monitoring alerts, such as when your IDS detects any intrusion attempts.</p>
<p>After your environment is implemented, but before any production traffic flows through it, you have to validate the environment, either by contracting a Qualified Security Assessor if you’re a Level 1 merchant or by filling out the Self Assessment Questionnaire if you’re not a Level 1 Merchant.</p>
<p>Wow, that was a lot of work, wasn’t it? Well, if you’re going to be handling credit card information, you’ll be happy when your rigorous security design prevents damaging incidents.</p>
<h1 id="Disaster-Recovery"><a href="#Disaster-Recovery" class="headerlink" title="Disaster Recovery"></a>Disaster Recovery</h1><p>In an earlier lesson, we covered how to design a highly available architecture that will keep running even if an instance fails, by using load balancers, instance groups, and redundant databases. However, there are more catastrophic events that might occur. I’m not talking about an entire city getting destroyed or anything like that (although it would be good to have an architecture that could handle that). But much smaller incidents can be disastrous too. For example, one of your databases could become corrupt. This is actually worse than the database server going down because it may take a while before you realize there’s a problem, and in the meantime, the corruption problem could get worse.</p>
<p>To recover from this sort of disaster, you need backups along with transactional log files from the corrupted database. That way you can roll back to a known-good state. Each type of database has its own method for doing this.</p>
<p>If you’re using Cloud SQL to run a MySQL database (which we are for the interior design application), then you should enable automated backups and point-in-time recovery. Then if your database becomes corrupt, you can restore it from a backup or use point-in-time recovery to bring the database back to a specific point in time. </p>
<p>If you’re using Cloud SQL for SQL Server, then you should enable automated backups. At this time, Cloud SQL does not support point-in-time recovery for SQL Server, so you can only restore a database to the point when a specific backup was taken. For both types of databases, Cloud SQL retains up to 7 automated backups for each instance.</p>
<p>If you’re hosting a database on Compute Engine instances directly, then you’ll have to configure backups and transaction logging yourself. For example, suppose that instead of using Cloud SQL for SQL Server, we ran SQL Server on a Compute Engine instance. Then we’d need to set up our own disaster recovery solution for it. Luckily, Google has a very detailed white paper on this topic. I’ll give you the highlights.</p>
<p>First, set up an automated task that copies the SQL Server database backups to Google Cloud Storage. This is where we would finally need a service account because instances can’t write to Cloud Storage by default. The SQL Server instances need to have a service account with the Storage Object Creator role. Another way to do it would be to set a Cloud Storage access scope for the instance, but service accounts are more flexible.</p>
<p>Once the database is being backed up, then if disaster strikes, you would spin up a new SQL Server instance. Either use one of Google’s preconfigured SQL Server images or your own custom disk image. It doesn’t mention this in the whitepaper, but it’s the sensible thing to do and I’ll talk about it more in a minute. Next, you can use an open-source script to restore the database and re-execute the events in the log files up to the point in time desired.</p>
<p>When you’re designing a disaster recovery solution, you need to consider RPO and RTO. RPO stands for Recovery Point Objective. This is the maximum length of time when data can be lost. It affects your backup and recovery strategy because, for example, if it’s acceptable to lose an entire day’s worth of work, then you can just recover using the previous night’s backups. If you have a short RPO, which is usually the case, then you need to make sure you are constantly backing up your data, and when recovering from database corruption, you have to carefully consider which point in time to recover to.</p>
<p>RTO stands for Recovery Time Objective. This is the maximum length of time that your application can be offline and still meet the service levels your customers expect (usually in a service level agreement).</p>
<p>In the SQL Server example, I suggested using either one of Google’s preconfigured SQL Server images or your own custom disk image that has SQL Server installed and configured. The advantage of having a custom disk image is that it helps you meet your recovery time objective because it reduces the amount of time it takes to get a new SQL Server instance running. If you have to configure SQL Server manually, that could significantly impact how long it takes to recover from a disaster.</p>
<p>As with everything, though, there are tradeoffs. If your SQL Server implementation is customized, then you’ll have to weigh the benefits of fast recovery time against the maintenance effort required to keep your custom image up-to-date . If you have a very short RTO, then you may have no choice but to maintain a custom disk image. You might be able to ease the maintenance required, though, by using a startup script to perform some of the customization. Since the startup script resides on either the metadata server or Cloud Storage, you can change it without having to create a new disk image.</p>
<p>In some cases, you may want to run an application from your own data center or from another cloud platform and use Google Cloud as a disaster recovery solution. There are many ways you could do this, but I’ll go over a couple of common designs.</p>
<p>The first way is to continuously replicate your database to an instance on Google Cloud. Then you would set up a monitoring service that would watch for failures. In the event of a disaster, the monitoring service would trigger a spin-up of an instance group and load balancer for the web tier of the application. The only part you would need to do manually is to change the DNS record to point to the load balancer’s IP address. You could use Cloud DNS or another DNS service for this.</p>
<p>This is already a low-cost solution because the only Google Cloud resource that needs to run all the time is the database instance. But you can reduce the costs even further by running the database on the smallest machine type capable of running the database service. Then if there’s a disaster, you would delete the instance, but with the option to keep the persistent disk, and spin up a bigger instance with the saved disk attached. Of course, this solution would require more manual intervention and would lengthen your downtime, so you wouldn’t want to do this if you have a short RTO.</p>
<p>If you want to reduce your downtime as much as possible, or even keep running in the event of hardware failures, you could serve your application from both your on-premises environment and your Google Cloud environment at all times. That way if you have an on-premise failure, the Google Cloud environment would already be running and serving customers. It would just need to scale up to handle the extra load, which would be automatic if you use an autoscaling instance group.</p>
<p>To make this hybrid solution work, you would need to use a DNS service that supports weighted routing, so it could split incoming traffic between the two environments. In the event of a failure, you would need to disable DNS routing to the failed environment.</p>
<p>And that’s it for disaster recovery.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">design a Google Cloud infrastructure</a>. Now you know how to map <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/compute-1/">compute</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/storage-2/">storage</a>, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/networks-1/">network</a> requirements into Google Cloud, and secure your infrastructure with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/authentication-2/">authentication</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/roles-1/">roles</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/service-accounts-1/">service accounts</a>, ACLs, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/data-protection-and-encryption-1/">encryption</a>. You also know some of the ways to design <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disaster recovery</a>, and PCIDSS compliance into your solution.</p>
<p>To learn more about Google Cloud platform, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know on the Cloud Academy community forums. Thanks and keep on learning.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/" rel="prev" title="gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7">
      <i class="fa fa-chevron-left"></i> gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/" rel="next" title="gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9">
      gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Course-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Course Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Case-Study"><span class="nav-number">2.</span> <span class="nav-text">Case Study</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Compute"><span class="nav-number">3.</span> <span class="nav-text">Compute</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Storage"><span class="nav-number">4.</span> <span class="nav-text">Storage</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#High-Availability"><span class="nav-number">5.</span> <span class="nav-text">High Availability</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Networks"><span class="nav-number">6.</span> <span class="nav-text">Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Authentication"><span class="nav-number">7.</span> <span class="nav-text">Authentication</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Roles"><span class="nav-number">8.</span> <span class="nav-text">Roles</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Service-Accounts"><span class="nav-number">9.</span> <span class="nav-text">Service Accounts</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Protection-and-Encryption"><span class="nav-number">10.</span> <span class="nav-text">Data Protection and Encryption</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Legislation-and-Compliance"><span class="nav-number">11.</span> <span class="nav-text">Legislation and Compliance</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Disaster-Recovery"><span class="nav-number">12.</span> <span class="nav-text">Disaster Recovery</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-number">13.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
