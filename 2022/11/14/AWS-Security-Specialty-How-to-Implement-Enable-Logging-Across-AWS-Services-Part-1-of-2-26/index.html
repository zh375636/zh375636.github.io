<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionHello and welcome to part one of this two part series of courses which has been designed to help you understand how AWS performs logging for a number of key services and how to use this">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26">
<meta property="og:url" content="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionHello and welcome to part one of this two part series of courses which has been designed to help you understand how AWS performs logging for a number of key services and how to use this">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-14T17:52:25.000Z">
<meta property="article:modified_time" content="2022-11-20T02:54:14.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:25" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:25-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:54:14" itemprop="dateModified" datetime="2022-11-19T22:54:14-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to part one of this two part series of courses which has been designed to help you understand how <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> performs logging for a number of key services and how to use this data captured by the logs to resolve incidents and identify security threats. </p>
<p>Before we start I would like to introduce myself. My name is Stuart Scott. I am one of the trainers here at Cloud Academy specializing in AWS Amazon Web Services. Feel free to connect with me with any questions using the details shown on screen. Alternatively you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> where one of our cloud experts will reply to your question.</p>
<p>The focus of this two part series is to understand the login process and how to monitor this data to your organization’s benefit from both an operational and security perspective. As a result those who have the following or similar roles would benefit from this content: Cloud Security Engineer, Cloud Security Architect, Cloud Administrators, Cloud Support and Operations and Compliance Managers. </p>
<p>Part one of this series is constructed of the following lectures. The Benefits of Logging. This lecture focuses on the core principle of why logging is important. CloudWatch Logs. Here we’ll look at how to implement logging using CloudWatch Logs and the associated agent. Next I’ll look at CloudTrail Logging and CloudTrail records all API calls. So here I explain how you can use these logs and how they are constructed. Next we’ll be monitoring CloudTrail Logs, and here I look at how you can use CloudWatch to monitor CloudTrail events, and then finally in this course we look at S3 Access Logs, and this lecture focuses on the logging capabilities within S3 buckets. </p>
<p>Part two of this series will continue the theme of logging across AWS services by explaining the following. CloudFront Logs, and here we’ll look at how to log the request from each user requesting access to your website and distribution. Next I look at VPC Flow Logs, and this lecture focuses on how to look at the network data traversing your network interface cards within your VPC. Next is AWS Config Logging, and here I’ll look at how AWS Config provides a timeline of changes against your AWS resources. And finally in Part two, I look at Filtering and Searching of Log Data, and this lecture looks at how to use Amazon Athena to query logs being stored on S3. </p>
<p>The objectives of this series is to enable you to understand why and when you should enable logging of key services, how to configure logging to enhance incident resolution and security analysis, and you’ll understand how to extract specific data from logging data sets. </p>
<p>This is an advanced level course series, and so you should be familiar with the following services and understand the individual use cases and feature sets. Throughout this series I will reference a number of URL links which will help and direct you to related information on specific topics. To makes these links easily available to you I’ve included them at the top of the transcript within the lecture they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#x73;&#117;&#112;&#x70;&#x6f;&#x72;&#116;&#64;&#99;&#x6c;&#111;&#x75;&#x64;&#97;&#99;&#97;&#100;&#101;&#x6d;&#x79;&#46;&#99;&#111;&#x6d;">&#x73;&#117;&#112;&#x70;&#x6f;&#x72;&#116;&#64;&#99;&#x6c;&#111;&#x75;&#x64;&#97;&#99;&#97;&#100;&#101;&#x6d;&#x79;&#46;&#99;&#111;&#x6d;</a>. </p>
<p>That brings me to the end of this lecture. Coming up next I want to start off by looking at the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/benefits-logging/">benefits that logging brings</a> to your operational environment.</p>
<h1 id="The-Benefits-of-Logging"><a href="#The-Benefits-of-Logging" class="headerlink" title="The Benefits of Logging"></a>The Benefits of Logging</h1><p>Hello, and welcome to this short lecture, where I want to discuss a few of the different benefits that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> can bring you and your infrastructure. For some, many people consider logging an afterthought, something that is implemented after it’s too late. This is often the case where an incident or breach of security has occurred that resulted in a delay of resolution and safeguarding of your environment. In hindsight of these situations, logging would have been a great idea to have had running and implemented in the first place to rectify the event quickly and efficiently, or even prevent it from happening in the first place.</p>
<p>So how can logging help?</p>
<p>Generally, logs are created by services and applications which contain a huge amount of information, which is recorded and retained on persistent storage, to be reviewed and analyzed at any time that it might be needed. Some logs can be monitored in real time, allowing automatic responses to be carried out, depending on the data contents of the log. From an auditing perspective, these logs are invaluable. They often contain vast amounts of metadata, including date stamps, source information such as IP address or usernames, and this is especially true when you’re looking at CloudTail logs. These logs can be used to help you achieve specific compliance certifications that require evidence of traceable and auditable actions that have been carried out. </p>
<p>Being able to resolve an incident as quickly as possible is paramount within your organization. Whether it’s a priority one, two, or three, being able to gain as much insight into what happened just before and just after the incident can significantly reduce your time to resolution. Using logs to ascertain the state of your environment before and after and even during the incident provides clarity and enables you to detect where the incident occurred, allowing you to pinpoint your efforts in a specific area. Quicker resolution results in a better customer experience for your organization. </p>
<p>By monitoring the data within your logs, you’re able to quickly identify potential issues that you want to be made aware of as soon as they occur. By combining this monitoring of logs with thresholds and alerts, you are able to receive automatic notifications of potential issues, threats, and incidents, prior to them becoming a production issue. By logging what’s happening within your applications, network, and other cloud infrastructure, you are able to build a baseline of performance and establish what’s routine and what isn’t. By having this baseline, you are able to identify threats and anomalies easier through the use of third party tools and management services. </p>
<p>To have a thorough understanding of what’s happening within your infrastructure provides a huge benefit to your operational teams. Having an inside look of how your infrastructure is performing and communicating helps achieve the previous benefits that I’ve already discussed, and having more data about how your environment is running far outweighs the disadvantage of not having enough information, especially when it really matters to your business in the case of incidents and security breaches. </p>
<p>That now brings me to the end of this lecture. There are many more reasons as to why you should be capturing data that can be logged. But I just wanted to provide a few key points to you. </p>
<h1 id="CloudWatch-Logging-Agent"><a href="#CloudWatch-Logging-Agent" class="headerlink" title="CloudWatch Logging Agent"></a>CloudWatch Logging Agent</h1><p>Hello and welcome to this lecture where I shall explain what CloudWatch Logs are, how they work and how they are configured. As we know CloudWatch is a monitoring service that is used to collate and collect metrics on resources running on your AWS account allowing you to monitor their performance and respond to alerts that meet to find thresholds. In addition to this, Amazon CloudWatch is a powerful tool that allows you to collect logs of your applications and a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. </p>
<p>When data is fed into Cloudwatch Logs you are able to monitor the logstream in real time and set up metric filters to search for specific events that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. Let me explain the different components of this feature to give you a better understanding of how it all fits together. Starting with the Unified CloudWatch Agent. </p>
<p>With the installation of the Unified CloudWatch Agent you are able to collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. Interestingly this metric data is in addition to the D4EZ2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. The agent can be installed on a number of different operating systems and at the time of writing this course the operating systems versions supported are as follows. </p>
<p>To install the agent and configure it requires a number of different steps. To install the agent on your EC2 instances you need to perform the following, firstly you need to create a role and attach it to the instance with permissions allowing CloudWatch to collect data from the instances in addition to interacting with AWS systems manager SSM. You then need to download and install the agent onto the EC2 instance. And lastly configure and start the CloudWatch Agent. The most efficient way of completing this installation and configuration is with the use of the EC2 systems manager service known as SSM. You will need to create two roles. One role will be used to install the agent and also to send the additional metrics gathered to CloudWatch. The other role is used to communicate with the parameter store within SSM, to store a configuration file of the agent which then can be shared with other EC2 instances. </p>
<p>From a security perspective it’s best that only one of your EC2 instances has this permission to write to the parameter store. Once the agent configuration file is stored on SSM there is no need for other EC2 instances to do the same. In fact once the write has been completed, this role should be detached from the EC2 instance and the other role applied which simply allows the agent to send data to CloudWatch. </p>
<p>The role with the additional permissions for SSM needs to be configured as follows when creating a role. The options, ‘select the type of trusted identity’ needs to be ‘AWS service’. The option to ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. The ‘Attach Permissions Policies’ needs to be ‘CloudWatch Agent Admin Policy’ and the ‘Amazon EC2 role for SSM’. </p>
<p>The role that is simply used to install the agent and send data back to CloudWatch needs the following configuration, the ‘select type of trusted identity’ needs to be ‘AWS service’. The option ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. And finally under the ‘Attach Permissions Policies’ it needs to be ‘CloudWatch Agent Server Polic’y and ‘Amazon EC2 Role for SSM’. </p>
<p>Once your roles are created you can then associate the role shown in orange here, which I shall call ‘CloudWatch Agent Admin Role’ to your EC2 instance that will store the configuration file in the parameter store. </p>
<p>From the EC2 instance with additional permissions that will be saved in the configuration file with in the parameter store of SSM, you must then install the agent which can be done using systems manager or it can be downloaded from an S3 public link either for Linux or Windows. However as mentioned earlier I will explain how to do this via SSM in particular the run command function. As a prerequisite to the CloudWatch Agent installation you’ll need to verify that your EC2 instance has access to the internet to communicate with SSM and CloudWatch endpoints. In addition to this you must also have the SSM agent installed. For some AMI’s as stated on the screen the agent may already be installed. For more information on how to install or update your SSM agent on your EC2 instance please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">link</a>. Let me now perform a very quick demonstration to show you how to install the actual CloudWatch agent on an Amazon Linux EC2 instance using the SSM role command. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration 1"></a>Start of demonstration 1</h3><p>Okay so I’m within my AWS management console in the EC2 dashboard and as you can see I have an EC2 instance here named Logging Server and I have it in a VPC that has internet access. I have the SSM agent installed because it’s based on one of the latest Linux AMI’s comes by default with this, and I’ve also attached the CloudWatch Agent Admin Role which I discussed in the previous section so I’ve met all the prerequisites to install the CloudWatch Agent. </p>
<p>Now what I want to do is use the EC2 systems manager to install the CloudWatch Agent itself. So if I load up SSM which now has it’s own console and on the left hand side if I go down to the run command and click on run command to create a new command. What I want to select for the command document is the AWS configure AWS package. So if you just select the entry and then scroll down. And then next I need to select which EC2 instance that I want as the target. So which instance I’m going to install this package on and here we can see the Logging Server so I would just highlight that. And as you can see it’s added our EC2 instance already up there. </p>
<p>Now we come down to the command parameters and the action I want to perform is an install. The name of the package is Amazon CloudWatch Agent and I want to install the latest version. You can add some other comments here and some timeouts, for this demonstration I’m just going to leave those default. If you wanted to perform this same installation via the AWS CLI then you can click on the AWS command line interface command and then based on the above parameters you can simply cut and paste this command and run that command. But for this demonstration I’m going to use the run command within SSM. </p>
<p>So if I click on run we can see here that the command ID was successfully sent. It’s currently in progress and that will just take a moment to process and install the agent, then we should get a return of successful. So if I go back to the main dashboard of the run command we can see here that it was a success. This is the package that we just run using using this command ID, so now the agent is successfully installed. </p>
<p>As you can see here I have a previous command that failed so I just want to show you that worst on here. So if we select that command and go to view details we can drill down to understand why this failed. And if we scroll down to the bottom you can see here that it failed. Select the instance ID and view the output. Now step one, we can dive deeper to see why this failed. Now if we scroll down to the error itself and here it said it failed to retrieve the manifest and it could not find the latest version of this particular package. Now what happened here was I didn’t enter the correct package name. What I entered was CloudWatch Agent instead as we performed in the demonstration just now. The correct name is Amazon CloudWatch Agent. So it couldn’t find the package that was available and that was the reason why it failed. </p>
<p>However, going back to one that was successful, we can see here, we can drill down into this just to make sure everything was okay. Again here, success and we can view the output of this as well like we done with the failed one. And we can see here that it was all installed. It found all the files and it went through and processed everything okay. And that’s it, that’s how you install the CloudWatch Agent using the SSM run command. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration 1"></a>End of demonstration 1</h3><p>On your first instance you’ll need to create the CloudWatch Agent configuration file. Without doing so, you will not be able to start the agent. This file stores configuration parameters that specify which metrics and logs to capture on the instance which are then sent to CloudWatch. It can be created manually or by using a wizard. If you create it manually then you have a much wider scope for capturing elements that are not included within the wizard. Let me show you via another demonstration how to configure the agent using the wizard. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration 2"></a>Start of demonstration 2</h3><p>So I’m now on my Logging Server instance where we installed the agent and now need to configure it. So to configure it we’ll run this command here and this will launch the Amazon CloudWatch Agent configuration wizard. And it just goes through a number of questions before it completes so let’s take a look. So the first question it asks which operating system your using Linux or Windows, take the default choice of Linux. And if we’re going to try and collect logs on EC2 instances or On-Premises So it’s EC2 for us. </p>
<p>We then have a question if we want to monitor any host metrics such as CPU, memory, etc, I’m going to set a default of yes. And if we want to monitor CPU metrics per core and here additional CloudWatch charges may apply. For this demonstration I’m going to say no. We then have a question if we want to add any EC2 dimensions such as the instance ID or the instance type into our metrics if the information is available. Say yes. Then we have a question about how often CloudWatch will collect these metrics. Whether that’s one second, 10 seconds 30 seconds or 60. I’ll accept the default of every minute, 60 seconds and then asks which default metric config we want whether that’s basic, standard, advanced or none. I’ll accept the default of basic for this demonstration. It then just shows you an example of that configuration that you just selected. If you’re happy with that you can say one for yes or two for no. And then select a different default metrics config so I will just select yes for this demonstration, I’m happy with that. </p>
<p>Now it asks the question if we have an existing CloudWatch log agent configuration file that we want to import. At the moment we don’t but what we’re trying to do is get in the position of creating one to then import into the parameter store of SSM. So at this moment our default choice is no which is correct. It then asks if we want to monitor any log files on this EC2 instance. I’m going to say no just for this demonstration because all we are trying to do is configure the CloudWatch agent at the moment. Our next question asks if we want to store the config in the SSM parameter store, and here yes we do cause we want to upload this file to the parameter store to allow all other EC2 instances to connect to the SSM parameter store to download the configuration file to prevent us from doing this on each and every instance. So I’m going to say yes which is number one that we do want to store the config in the SSM parameter store. </p>
<p>It then asks what default name you want to give this configuration file. And it just gives you a little message there saying you should use the prefix of Amazon CloudWatch hyphen if you’re using any of the AWS managed policies. So I would go with their suggested name of Amazon CloudWatch Linux and you shouldn’t run into any issues there. It then asks you which region you want to store the config in the parameter store in and it gives a default choice. I’m just going accept that default. It then asks a question about which credentials should be used to send the config to the parameter store. I’m going accept the default credentials there which should have access and we now have a message that it successfully put the config to the parameter store Amazon CloudWatch-Linux. And that’s it, that’s the end of the wizard so it’s a very simple and quick and easy wizard. And now your CloudWatch agent configuration file is configured and it’s been uploaded to the parameter store so now any other EC2 instances can click to that parameter store and simply download it and have the agent running very quickly and easily. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration 2"></a>End of demonstration 2</h3><p>Now the configuration file is configured and successfully copied to the SSM parameter store I simply now need to start the agent and again I will use the systems manager service to complete this file with the run command. Let’s take a look. </p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration 3"></a>Start of demonstration 3</h3><p>Okay so the final stage is to start the agent and again I’m going do this from the AWS systems manager so I’m at the systems manager dashboard. Again I’m going use the run command so so it’s over on the left hand side here. Click on run command. Then click on the orange run command button and this will allow us to start a new command for the command document. What we need to look for is the following, which is Amazon CloudWatch Manage Agent. And here we have the command document here. So I’m going to select that. </p>
<p>Scroll down, we then need to select our target instance and we have our Logging Server here. Now the command parameters, for the action we want to select configure rather than stop because we’re going to select the configuration file that we uploaded to the parameter store first using the EC2 mode rather than On-Premises. The optional configuration source which is SSM which is where we stored the configuration file so we’ll leave that as SSM. Now in the optional configuration location we need to enter the name of the file that we stored it as. And if you can remember that was Amazon CloudWatch-Linux. So we’ll paste that in. </p>
<p>Now under optional restart we want that as yes because it will then start the agent once it’s pulled the information from SSM. If we scroll down to the bottom simply click on run. And now we have it, the command ID was successfully sent. If we go back to our dashboard we can see that it was a success. And here the document name was the CloudWatch Manage Agent. So the CloudWatch Agent will now be running on that EC2 instance, the Logging Server. And it’s as simple as that. Now we have our logs configured and log data of our EC2 instance is being sent to CloudWatch along with the additional metric information. It’s now possible to search for specific entries within the logs for points of interest. </p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration 3"></a>End of demonstration 3</h3><p>That now brings me to the end of this lecture on CloudWatch logs where I explained how CloudWatch can be used to centralize login for more EC2 instances or applications running on your instances by determining logs past configured by the CloudWatch Agent and log groups within CloudWatch. Coming up next I should be talking about CloudTrail logs.</p>
<h1 id="CloudTrail-Logging"><a href="#CloudTrail-Logging" class="headerlink" title="CloudTrail Logging"></a>CloudTrail Logging</h1><p>Hello, and welcome to this lecture focusing on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> capabilities and configuration of AWS CloudTrail. You should already be familiar with what CloudTrail is and what it does. However, to quickly summarize: It’s a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface: CLI, from within the AWS Management Console, Or even from a request made by another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service. For example, when auto-scaling automatically sends an API request to launch or terminate an instance. These API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the new log file. </p>
<p>CloudTrail also records and associates other identifying metadata with all events. For example, the identity of the caller, which can be the user or the account that made the API call, the timestamp of when the request was initiated, and the source IP address. The logs generated are the output of the CloudTrail service and they hold all of the information relating to the API calls that have been captured. So as a result, it’s important to know what you can do with these logs in order to maximize the benefit of the data they contain. </p>
<p>So, what is a log file and what does it look like? Log files are written in a JSON format. Much like access policies within IAM and S3. Every time an API is captured, it’s associated with an event and written to a log. And new logs are created approximately every five minutes or so, but they are not delivered to a nominated S3 bucket for persistent storage for approximately 15 minutes after the API was called. So if you expect to see the log file for an API called seven minutes ago, then you may not see the log as expected for potentially another eight minutes. The log files are held by the CloudTrail service until final processing has been completed. Only then will it be delivered to S3, and optionally, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a>, depending on your configuration of the trail. </p>
<p>When an event reflecting an API call is written to a log, a number of attributes are also written to the same event capturing key data about that call. As you can see from this example. Without going through every attribute here, I just want to point out some of the more interesting ones. These being eventName. This refers to the name of the actual API that was called. EventSource. This refers to the service as to which the API called was made against. EventTime. This was the time that the call was made. SourceIPAddress. This displays the source IP address of the requester who made the API call. This is a great piece of information when trying isolate an attacker from a security perspective. UserAgent. This is the agent method that the request was made through. Example values of these are: Signin.amazonaws.com and this is what we have in our example and it simply means that user made this request from within the AWS management console. Console.amazonaws.com, this is the same as the previous, however, if this was displayed, it would mean that the request was made by the root user of the account, and lambda.amazonaws.com, this is fairly obvious, this would reflect that the request was made with AWS lambda. UserIdentity. This contains a larger set of attributes that provides information on the identity that made the API request. Once events have been written to the logs and then delivered and saved to S3, they are given a standard name and format, as shown. </p>
<p>The first three elements of this naming structure are self-explanatory. The AccountID, Name of the Service delivering the log, CloudTrail, and the region that it came from. The next part relates to the date and time. The year, months, and days. The T indicates the next part is the time reflecting hour and minutes. The Z simply means that the time is in UTC. The UniqueString value is a random 16 alphanumeric character string that is simply used by CloudTrail as a unique file identifier to ensure that it doesn’t get overwritten with the same name of another file. Currently, the FileNameFormat is defaulted to json.gz which is a compressed GZ version of a JSON text file. While we are looking at structures, let me also talk about the bucket structure way your logs are stored. </p>
<p>You may feel that the logs are all stored in one folder within your S3 bucket. However, there is a lengthy but very useful folder structure as follows: Firstly, you have your dedicated S3 BucketName that you selected during the creation of your Trail. Next, is the prefix that is also configured during Trail creation and is used to help you organize a folder structure for your logs corresponding to different Trails. Following this, is a fixed folder name of AWSLogs. Followed by the originating AWS account ID. Then another fixed folder name of CloudTrail indicating which service has delivered the logs. And after that, the RegionName of where the log file originated from. This is useful for when you have Trails that apply to multiple regions. The last three folders show the year, month and day that the log file was delivered. As you can see, although there are multiple folders underneath your nominated S3 bucket, it does provide an easy navigation method when looking for a specific log file.</p>
<p>This folder structure comes into even greater use if you have multiple AWS accounts delivering logs to the same S3 bucket. Some organizations may be using more than one AWS account, and having CloudTrail logs stored in different S3 buckets across multiple accounts can be inconvenient in certain circumstances and require additional administration to manage. Thankfully, AWS offers the ability to aggregate CloudTrail logs for multiple accounts into a single S3 bucket belonging to one of these accounts. This is why there is an accountID folder within your S3 bucket. Please note that you are unable to aggregate CloudTrail logs for multiple AWS accounts into CloudWatch logs that belongs to a single AWS account. </p>
<p>So to have all your logs from your accounts delivered to just one S3 bucket is a fairly simple process with the end result allowing you to essentially manage all your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/monitoring-cloudtrail-cloudwatch/">CloudTrail logs</a>. Let’s take a look at how this solution is configured. Firstly, you need to enable CloudTrail by creating a Trial in the AWS account that you want all log files to be delivered to. Permissions need to be applied to the destination S3 bucket allowing cross account access for CloudTrail. And once permissions have been applied to your policy, you need to edit the bucket policy and add an additional line for each AWS account requiring access. Then you need to create a new Trial in your other AWS accounts and select to use an existing S3 bucket for the log files. When prompted, add the bucket name used in step one and when alerted, accept the warning that you want to use a bucket from a different AWS account. An important point to make here when configuring the bucket selection is to ensure that you use the same prefix as the one you used when you configured the bucket in the first step. That is unless you intend to edit the bucket policy to allow CloudTrail to write to the location of a new prefix you wish to use. When you have configured your Trail, click create and your new Trail will now deliver it’s log files to the S3 bucket in your AWS account used in the first step. Again, this is a great solution that allows you to essentially manage all of your CloudTrail logs in one single account and S3 bucket. However, there may be uses such as system administrators who manage the other AWS accounts where the logs have come from that might need access to data within these logs. So how would they gain access to the S3 bucket to allow them only to access their CloudTrail logs that originated from their AWS account? </p>
<p>It could be done quite easily by configuring a few elements within IAM. Firstly, in the master account, IAM Roles would need to be created for each of the other AWS accounts requiring read access. Secondly, a policy would need to be assigned to those Roles allowing access to the relevant AWS account logs only. Lastly, users within the requesting AWS accounts would need to be able to assume this Role to gain read access for their CloudTrail logs. The easiest way to show you how to configure the permissions required is by a demonstration while I shall perform the following steps: I shall create a new Role. Apply a policy to this Role to only allow access for AWS account B’s folder in S3. Show the Trust Relationship between AWS account A and B. I will then create a new IAM user in account B. And create a Policy and apply the sts:AssumeRole permissions to this user allowing them to assume the new Role we created in account A. So let’s take a look at how and where we apply these permissions. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so, as I just said the first thing we need to do is create a new Role in our primary account. So, if we go across to IAM, which is under Security, Identity and Compliance and then once that’s loaded, we need to go across to Roles and then Create New Role. So let’s give this Role a name. I’ll call it ‘Cross Account CloudTrail’ Click on Next Step. We then need to select a Role type, and what we want to do is select Role for Coss-Account access because we will allow users in another AWS account to access the log files in this primary AWS account. And this will set up the trust relationship between this account and then my secondary account. So, for that we will select this top option of providing access between AWS accounts you own. Then next, I’ll need to enter the secondary account ID that I want to create the trust relationship with. So I’ll just enter that number. </p>
<p>Okay, then after you have entered your account ID click on next step. And now we need to attach a Policy to this Role. Prior to this demo, I set up my own Policy and this allows cross-account access to read only from my secondary account to the bucket on this primary account. But I’ll explain this Policy in a few moments and I’ll show exactly what it contains. And from here click on next step and this is just a review of the Role. So we have the Role name, the ARN, the Amazon Resource name, the trusted entities. So this is the secondary account ID that I entered, and then the actual Policy and then the link that we can give to users in the secondary account to allow them to switch Roles. So, create Role. And there we go, the cross-account CloudTrail Role that we just created. So, let’s take a look at this.</p>
<p>Firstly, I’ll show the trust relationships. So, because we added a cross-account Role access and then we entered the secondary AWS account ID, we can see that this account is trusted by our primary account and that allows entities in this account to assume this Role. Now, I mentioned earlier that I previously set up a Policy with permissions in. So let’s take a look at that Policy. I named it Cross-Account read only for CloudTrail. So if I show the Policy, as you can see it. I made a very small Policy. Very simple. Now we have an effect of allow which will allow any S3:Get and any S3:List command, so essentially, read only access on this resource here specified by this line. Now this resource links to the bucket and folder where CloudTrail logs are delivered for our secondary account as you can see here. So, essentially, what this Policy does is allow read only access to any folders within the secondary account’s CloudTrail log folders. So this account won’t be able to access any other accounts CloudTrail logs, which is important. So, if we come out of this. So let’s just have a quick recap of what we’ve achieved so far.</p>
<p>So, so far, what we’ve done: We’ve created a Role in our primary account for our secondary account access. And we’ve also assigned an access Policy to this Role in order for the secondary AWS account to access the relevant folder in S3. So now what we need to do is assign a user in the secondary account and then apply the permissions to that user to enable them to assume the new Role in the primary account. So let’s go ahead and do that. </p>
<p>Okay, so I’ve now logged into the secondary account where I’ll need to create a new user and assign the correct permissions. So, to start with, I’m going to set up a permission Policy to assign to the user. So if I go down to Security, Identity and Compliance and select IAM. And then go across to Policies, and from here I want to create a new Policy. And I am going to create my own Policy, so I’m going to select the bottom option. I’m going to call this AssumeRoleforCloudTrail And description will be Assume role in primary AWS account. And for the Policy document, I’m going to paste in a Policy that I’ve already created. As you can see, it’s only a very small Policy again. And we have an allow effect that allows the Assume Role action from the security token service against the following resource, and this resource links back to a Role on our primary account where we created the Role cross-Account CloudTrail. </p>
<p>So this Policy will allow the user to assume this Role in the primary account. So let’s go ahead and create that Policy. Let’s validate it first. And then create. Now what we need to do is to assign a user to use that Policy. Now, I created a new user earlier prior to this demo. So, let’s just find our new Policy that we just created. And here it is at the bottom, AssumeRoleforCloudTrail. And I’m going to attach a user. And I’ve called our user CloudTrailUser1. And then attach Policy. And there we go. </p>
<p>So we now have one user attached to this Policy. So that’s all the actions and steps necessary to allow a user in a secondary account to access CloudTrail log files that have been delivered to an S3 bucket in a primary account. And it would do this by using the permission Policy that we just applied to that user to access the Role in the primary account. And that Role has a Policy attached that allows S3 read access to it’s own CloudTrail logs. </p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>CloudTrail allows you to enable a feature called Log File Integrity Validation. Which simply allows you to verify that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. This is typically used for security and forensic investigations where by the integrity of the log files are critical to confirm that they have not been tampered with in any way. </p>
<p>When a log file is delivered to an S3 bucket a hash is created for it by CloudTrail. A hash file is a set of characters that are unique that are created from a data source. In this case, the log file. The hashing algorithms used by CloudTrail are SHA-256. In addition for a hash for every log file created, </p>
<p>CloudTrail creates a new file every hour, called a digest file, which is used to help verify your log files have not changed. The digest file contains details of all the logs delivered within the last hour along with a hash for each of them. These files are stored in the same bucket as the key pair. When it comes to verifying the integrity of your log files, the public key of the same key pair is used to programmatically check that the logs have not been tampered with in any way. Verification of the log files can be achieved via a programmatic access and not via the console. Using the AWS CLI, this can be checked by issuing the following command. The folder structure for the digest is very similar to the CloudTrail logs, as you can see. But the digest files are clearly distinguishable by the CloudTrail digest folder. </p>
<p>That has now taken me to the end of this lecture. Coming up next, I’ll explain how you can use CloudTrail and CloudWatch together as a monitoring solution.</p>
<h1 id="Monitoring-CloudTrail-with-CloudWatch"><a href="#Monitoring-CloudTrail-with-CloudWatch" class="headerlink" title="Monitoring CloudTrail with CloudWatch"></a>Monitoring CloudTrail with CloudWatch</h1><p>Hello and welcome to this lecture where we will look at how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudtrail-logging/">AWS CloudTrail</a> interacts with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a> and SNS to create a monitoring solution. In addition to S3, the logs from CloudTrail can be sent to CloudWatch Logs, which allows metrics and thresholds to be configured, which in turn, can utilize SNS notifications for specific events relating to API activity. CloudWatch allows for any event created by CloudTrail to be monitored. This enables a whole host of security monitoring checks to be utilized. A great example of this is to be notified when certain API calls requesting significant changes to your security groups or network access control lists within your VPC. Other examples of these checks that are common within organizations are API calls relating to it starting, stopping, rebooting, and terminating EC2 instances. If instances are being created that shouldn’t be, then your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> cost could rise dramatically and quickly. Also, if instances are being rebooted or stopped, this could have a severe impact to your services if they are not configured in a high availability and resilient solution. Changes to security policies within IAM and S3. If changes are being made to your policies that shouldn’t be, access can be inadvertently removed for authorized users and access granted to unauthorized users, having a massive impact on operational services. Even a minor change to a policy can pave the way for an untrusted user to exploit the error. Failed login attempts to the Management Console. Monitoring failed attempts here can help to prevent unauthorized access at your environment’s front door. API calls that result in failed authorization. Not only does CloudTrail track successful API calls whereby the correct authorization was met by the authenticated identify, but it also tracks unsuccessful API requests, too, which would likely be due to the permissions applied. Special attention should be given to these unsuccessful attempts, as this could be a malicious user trying to gain access. However, it could also be a legitimate user trying to access a resource they should have access to for their role, but the incorrect permissions had been applied with their associated IAM policy. </p>
<p>To configure CloudTrail to use CloudWatch, you must first create a trail. Once your trail has been created, you can then configure it to use an existing CloudWatch Log group or have CloudTrail create a new one. Having CloudTrail create a new one for you is recommended if it is your first time doing this, as CloudTrail will take care of all of the necessary roles, permissions, and polices required. You may be wondering why roles and policy are required, so let me give you a high-level overview of the simple process that takes place when sending CloudTrail logs to CloudWatch. When a log file is created by CloudTrail, it is sent to your selected S3 bucket and your chosen CloudWatch Log group, assuming your trail has been configured for this feature. To allow CloudTrail to deliver these logs to CloudWatch, CloudTrail must have the correct permissions and these are gained by assuming a role with the relevant permissions needed to run two CloudWatch APIs. The first being CreateLogStream, and this enables CloudTrail to create a CloudWatch Logs log stream in the log group, and PutLogEvents, and this allows CloudTrail to deliver CloudTrail events to the CloudWatch Logs log stream. CloudWatch then delivers logs to the CloudWatch Logs. </p>
<p>When using the AWS Management Console, you can have the CloudTrail create this role for you, along with the correct policy. By default, the role is called CloudTrail_CloudWatchLogs_Role. For those that are curious, the policy for this role looks as shown. It’s important to point out that CloudWatch Log events have a size limitation of 256 kilobytes on the events that they can process. Therefore, any events that are larger than 256 kilobytes will not be sent to CloudWatch by CloudTrail. </p>
<p>Now that you have your logs with the associated events being sent to CloudWatch, you must then configure CloudWatch to perform analysis of your CloudTrail events within the log files. This is done by configuring and adding metric filters to the log within CloudWatch. These metric filters allow you to search and count a specific value or term within your events in your log file, which then allows for customizable thresholds to be applied against them. When creating these metric filters, you must create a filter pattern which determines what exactly you want CloudWatch to monitor and extract from your files. These filter patterns are usually fully customizable strings but as a result, a very specific pattern syntax is required. So, if you’re creating these for the first time, you must understand the correct syntax. </p>
<p>Just to reiterate what we have spoken about so far, I want to provide a demonstration on how to edit an existing trail to configure it to send logs to CloudWatch Logs. I will then configure a metric filter with the associated metric pattern, and finally, I will set up an SNS alert to notify me when a particular threshold is met. So, let’s take a look. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so what I need to start with is going into CloudTrail to edit an existing trail to enable CloudWatch Logs. So, if I go down to Management Tools and click on CloudTrail and then across to Trails, as you can currently see, under CloudWatch Logs log group, there’s no log group selected. So, if we go into the trail and then scroll down to CloudWatch Logs, click on Configure, and there we can get CloudTrail to automatically set up this group and it’ll create the necessary roles and permissions, etc. So, let’s call this CloudTrail&#x2F;Demo and then click on Continue. So, we’ve given it a name and here it just gives a message to say that for CloudTrail to deliver events and logs to CloudWatch Logs, it needs to assume a role with permissions to run two API calls, which are these two here. And if we go down into the details, you can see that the IAM role that it’s going to use is the CloudTrail_CloudWatchLogs_Role and we’ll ask it to create a new policy. And here’s the policy document. </p>
<p>So, go down to Allow, and then if we scroll down to our CloudWatch Logs section, you can now see that we have a log group created in CloudWatch called CloudTrail&#x2F;Demo. So, if we now go across to CloudWatch and if we click on Logs on the left-hand side here, we can see that we have our log group that was just created by CloudTrail, and it’s CloudTrail&#x2F;Demo. Now, if we go into our log group and select it, you’ll see this log stream, which is the incoming stream of events being sent from CloudTrail. Now, as we’ve only just started, there’s only a few events coming in here, so you might want to wait a few minutes before setting up your metric filters to give you more of a test pattern to search on. So, what I might do is just leave it a couple of minutes for some more events to start streaming in before we set up our metric filters here, just so we have something to search on. </p>
<p>Okay, so I’ve left it a few minutes, so let’s go back into the log group and you can see we’ve now got a couple of streams, and if we go into these, we can see there’s a lot more events. So, if we go back a couple of pages, back to our log group, now we need to create our metric filters to allow us to define what we want to search on within our logs. So, if we select the tick next to our log group and then go up to Create Metric Filter, and here within the metric filter, we need to define a filter pattern. Now, as explained earlier, filter pattern will define what we’re actually searching for within our logs. So, for this example, I’ll keep it fairly simple. I’m going to search for any API call that’s been made from my machine, so from my IP address. So, for that, I need to enter the following command, ( $.sourceIPAddress equals 2.218.11.188, which is my IP address. And now we can test to make sure that that filter pattern’s okay using this Test Pattern box here, and what that does, that’ll run this test filter on some log data we see from this log here and the output of that log is in this box here. So, all we need to do is click on Test Pattern and we can see at the bottom here that it found 47 matches out of 50 events in the sample log. So, we know that the syntax is okay for this filter pattern, so I’m going to go ahead and assign this metric. </p>
<p>And we can see up here that we’ve got our filter name and our filter pattern, and I’m going to create a new name space for this metric and I’ll call it Demo, and the metric name will be IPAddress. And then what we need to do is click on Create Filter. Now, as you can see, our filter has been created and we have the details in this screen here. Now, what we can do at this point is create an SNS alarm so it could be notified if a certain threshold was met. So, let’s go ahead and do that. </p>
<p>So, the first thing that we need to do is add a name, so I’m going to call this SourceIPAddress and description will be Too many calls from my IP. Now, I’m going to set this to be 30. So, whenever my IP address is used as a source IP address that is greater or equal to 30 times for one consecutive period over five minutes, then I want it to set to a state of an alarm. And I want to be notified, so I’m going to enter a new list, give this a new topic, SourceIPAddressAlarm, and I want that to be sent to myself. So, as we can already see, with the current data it’s got that it has already breached the alarm, but it has gone back down below, so we’ll see how this goes and we’ll create the alarm. And this is a message just to say that I need to subscribe to that AWS notification, and I can do that in just a few moments. So, if we go across to our Alarms, we can see that we have our source IP address alarm in the state of OK. </p>
<p>So, at the minute, it’s currently below the 30 threshold. As soon as it goes above that, it will alarm and I will get a notification. Now, over the past few minutes, I’ve just been having some activity within the Management Console, and as we can now see, we do have an alarm on our alert. We can see that it just crossed the threshold, and so, I’ve received an email notification to say that it is now in a state of alarm. And if we take a quick look at that email, we can see here that it was crossed with a data point of 33 and the threshold was 30. So, that is how you set up CloudTrail to use CloudWatch with the inclusion of SNS to create alarms against API activity.</p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="S3-Access-Logs"><a href="#S3-Access-Logs" class="headerlink" title="S3 Access Logs"></a>S3 Access Logs</h1><p>Hello, and welcome to this lecture, where I shall be looking at Amazon S3 access logs, what they are, and what they contain. As you may have guessed from the name, Amazon S3 access logs collate data based on who has been accessing a particular S3 bucket, and these logs record information, such as the source bucket that was accessed, a timestamp of the event, the identity requesting access to the object in the bucket, and the action that they were performing with the object. </p>
<p>By default, when you create a new bucket, access logging is not enabled. However, should you have a requirement to understand who is accessing your S3 buckets, then this logging can quickly and easily be enabled. The configuration of this process is based upon a source bucket and a target bucket. The source bucket is the bucket in which you want to log access requests for. The target bucket is the bucket in which the access logs will be delivered to. It’s best practice to use different buckets for both the source and the target for ease of management. When configuring your buckets for logging, you need to be aware that the source and target buckets need to be in the same region. </p>
<p>To allow S3 to write logs to this target bucket, it will of course require specific permissions. These permissions allow write access for the Log Delivery group, which is a pre-defined Amazon S3 group, which is used to deliver log files to your target buckets. If the configuration of your access logging is configured using the management console, then the setup process automatically adds the Log Delivery group to the ACL of the target bucket, allowing the relevant access. However, if you were to configure the access logging using the command line, then you would need to manually configure these permissions. </p>
<p>To set up the access logs using the console is a very simple process. Firstly, you select the S3 bucket that you would like to capture access logs for, select the properties tab, select server access logging, choose Enable Logging. From the dropdown, select your target bucket, and this is the bucket in which the logs will be delivered and saved to. Enter a prefix for your log files if required, and click save. If you then look at the ACL permissions for that bucket, you’ll notice that the log delivery group has automatically been given write access for that bucket. If you wanted to enable logging on the bucket programmatically, then you can do so using the S3 API or the AWS SDKs. When doing so, you need to configure the write access for the Log Delivery Group on the target bucket as an additional action. More information on how to perform these steps can be found using the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/enable-logging-programming.html">link</a>. </p>
<p>When viewing your logs within the target bucket, you’ll notice that each entry is made up of a number of different parameters. Let me show an example of a log access file to show you the data that makes up the entry. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so I’ve just opened up one of the S3 access logs that I have, just a very small snippet. And we can see at the top here, we’ve got a couple of entries, so let’s just run through this top entry, just some of the key bits of information that make up the access log. This first entry here, and that’s the canonical ID of the owner of the source bucket. Next we have the AWS bucket itself that was accessed along with the date and time as well. Next we have some information from the requester, which is their source IP address. This hyphen means that the user was unauthenticated. If it was a user within IAM then we’d see their user ID there. This set of characters is generated by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> just as a unique ID for this request. Next we have the action that was carried out, which is a get object request against the object within the bucket which is this image file here. And again, we can see that here and also we have this response code of 200. Also we have some information here with regards to the amount of bytes sent and the object size, and also some timings in milliseconds as well for that request. We also have the referrer here of cloudacademy.com where the request initially came from, and then finally just some information regarding to the requester’s application software. So that’s a very quick summary and example of what an entry looks like within your AWS S3 access logs.</p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Hello, and welcome to this final lecture within Part One of this two-part series relating to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging in AWS</a>. In this lecture, I want to summarize and highlight the key points from the previous lectures.</p>
<p>I started off by talking about the benefits of logging to your organization. Within this lecture, we learnt that logging allows you to rectify incidents quicker and more efficiently, or even prevent the incident from happening in the first place. Also logs created by services and applications contain a huge amount of information which is then recorded and retained for later use. Some logs can be monitored in real-time allowing automatic responses to be carried out depending on the data contents of the log, and logs are invaluable from an auditing perspective as they contain vast amounts of metadata. Logs can also be used to help achieve compliance. Using logs to ascertain the state of your environment before and after and even during an incident enables you to detect where the incident occurred. And by combining the monitoring of logs with thresholds and alerts, you can configure automatic notifications of potential issues, threats and incidents prior to them becoming a production issue. Using logs, you can establish a baseline of performance allowing you to determine anomalies easier through the use of various third-party tools and management services. and finally, having more data about your environment and how its running far outweighs the disadvantages of not having enough information. </p>
<p>Following this lecture, I explained how CloudWatch Logs were configured and used. During this lecture, the following points were made. Amazon CloudWatch is a powerful tool that allows you to collect logs of your applications and a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. You are able to monitor the log stream in real-time and set up metric filters to search for specific events that you need to be alerted on or respond to. And CloudWatch Logs acts as a central repository for real-time monitoring of log data. The unified CloudWatfch agent allows you to collect logs and additional metric data from your EC2 instances and well as from on-premise servers. And to install the agent on your EC2 instances you need to create two roles. One role will be used to install the agent and also to send the additional metrics gathered to CloudWatch, and the other role is used to store a configuration information file in the parameter store within SSM. You then need to download and install the agent onto the EC2 instances using SSM and the Run Command and finally, configure and start the CloudWatch agent using a Wizard or the manual configuration. </p>
<p>I then looked at a different service, which was AWS CloudTrail, which records and tracks all API requests made within your AWS account. Within this lecture, I explain the following. When an API request is initiated, AWS CloudTrail captures a request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the log file and the logs generated are the output of the CloudTrail service. Log files are written in JSON, Javascript Object Notation format, much like access policies within IAM and S3. New logs are created approximately every five minutes but they are not delivered to the nominated S3 bucket for persistent storage for approximately 15 minutes after the API was called. The log files are held by the CloudTrail service until final processing has been completed. Log files are delivered to S3 and optionally CloudWatch Logs as well. And any logs that are delivered to S3 are given a standard naming convention of the following. AWS offers the ability to aggregate CloudTrail logs from multiple accounts into a single S3 bucket belonging to one of these accounts. But do be aware you are unable to aggregate CloudTrail Logs from multiple AWS accounts into CloudWatch Logs that belongs to a single AWS Account, and CloudTrails allows you to enable a feature called “Log file integrity validation,” which allows you to verity that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. And finally, when a log file is delivered to your S3 bucket, a hash is created for it by CloudTrail. </p>
<p>At this point, we had looked at both CloudWatch and CloudTrail. So the following lecture looked at how you could use CloudWatch to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/monitoring-cloudtrail-cloudwatch/">monitor CloudTrail Logs</a>. And here we learnt that the logs from CloudTrail can be sent to CloudWatch Logs allowing metrics and thresholds to be configured which, in turn, can utilize SNS notifications for specific events relating to API utility. CloudWatch allows for any event created by CloudTrail to be monitored and you can then configure your new and existing trails to use an existing CloudWatch Log Group, or have CloudTrail create a new one. To allow CloudTrail to deliver logs to CloudWatch, CloudTrail must have the following permissions given via role, and that’s the CreateLogStream permission and PutLogEvents. CloudWatch Log Events have a size limitation of 256KB on the events that they process and adding CloudWatch metric filters allows you to perform analysis of your CloudTrail events within the log files. And finally, metric filters allow you to search and count a specific value or term within your events in your log file. </p>
<p>The final lecture in Part 1 of this course series looked at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/s3-access-logs/">S3 Access Logs</a>. Within this lecture, the key points were as follows. Amazon S3 Access Logs collate data based on who has been accessing a particular S3 Bucket. And by default, when you create a new bucket, access logging is not enabled. S3 Access Logs are based upon a source bucket and a target bucket. The source bucket is the bucket in which you want to log access request for, and the target bucket is the bucket in which the access logs will be delivered to. It’s best practice to use different buckets for both the source and the target bucket for ease of management. And remember, the source and target buckets needs to be in the same region. During the configuration of access logs using the Management Console, permissions for right access for the log delivery group which is a predefined Amazon S3 group which is used to deliver log files to your target buckets. And if you wanted to enable logging on the bucket programmatically, then you can do so using the S3 API or the AWS SDKs. When doing so, you need to configure the write access for the Log Delivery Groups on the Target bucket as an additional action. </p>
<p>That now brings me to the end of this lecture and to the end of Part 1 of this course series. If you are ready to dive deeper into further AWS services relating to logging, including CloudFront Access Logs, VPC Flow Logs, AWS Config Logging, and how to filter data using Amazon Athena, then head over to Part Two, which can be found using the link on screen. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. </p>
<p>Thank you for your time, and good luck with your continued learning of cloud computing. Thank you.</p>
<p>URLs referenced during this course can be found below:</p>
<h1 id="3CloudWatch-Logging-Agent"><a href="#3CloudWatch-Logging-Agent" class="headerlink" title="3CloudWatch Logging Agent"></a>3<strong>CloudWatch Logging Agent</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">Metrics collected by the CloudWatch Agent</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent Linux</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/windows/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent for Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">How to install&#x2F;update your SSM Agent</a></p>
<h1 id="6S3-Access-Logs"><a href="#6S3-Access-Logs" class="headerlink" title="6S3 Access Logs"></a>6<strong>S3 Access Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/enable-logging-programming.html">Enabling Logging Programmatically</a></p>
<h1 id="7Course-Summary"><a href="#7Course-Summary" class="headerlink" title="7Course Summary"></a>7<strong>Course Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/">How to implement and enable logging across AWS services - Part 2 of 2</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/" rel="prev" title="AWS-Security-Specialty-Building-CloudWatch-Dashboards-25">
      <i class="fa fa-chevron-left"></i> AWS-Security-Specialty-Building-CloudWatch-Dashboards-25
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/" rel="next" title="AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27">
      AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Benefits-of-Logging"><span class="nav-number">2.</span> <span class="nav-text">The Benefits of Logging</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CloudWatch-Logging-Agent"><span class="nav-number">3.</span> <span class="nav-text">CloudWatch Logging Agent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-1"><span class="nav-number">3.0.1.</span> <span class="nav-text">Start of demonstration 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-1"><span class="nav-number">3.0.2.</span> <span class="nav-text">End of demonstration 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-2"><span class="nav-number">3.0.3.</span> <span class="nav-text">Start of demonstration 2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-2"><span class="nav-number">3.0.4.</span> <span class="nav-text">End of demonstration 2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-3"><span class="nav-number">3.0.5.</span> <span class="nav-text">Start of demonstration 3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-3"><span class="nav-number">3.0.6.</span> <span class="nav-text">End of demonstration 3</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CloudTrail-Logging"><span class="nav-number">4.</span> <span class="nav-text">CloudTrail Logging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration"><span class="nav-number">4.0.1.</span> <span class="nav-text">Start of demonstration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration"><span class="nav-number">4.0.2.</span> <span class="nav-text">End of demonstration</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monitoring-CloudTrail-with-CloudWatch"><span class="nav-number">5.</span> <span class="nav-text">Monitoring CloudTrail with CloudWatch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-1"><span class="nav-number">5.0.1.</span> <span class="nav-text">Start of demonstration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-1"><span class="nav-number">5.0.2.</span> <span class="nav-text">End of demonstration</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#S3-Access-Logs"><span class="nav-number">6.</span> <span class="nav-text">S3 Access Logs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-2"><span class="nav-number">6.0.1.</span> <span class="nav-text">Start of demonstration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-2"><span class="nav-number">6.0.2.</span> <span class="nav-text">End of demonstration</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Course-Summary"><span class="nav-number">7.</span> <span class="nav-text">Course Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3CloudWatch-Logging-Agent"><span class="nav-number">8.</span> <span class="nav-text">3CloudWatch Logging Agent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6S3-Access-Logs"><span class="nav-number">9.</span> <span class="nav-text">6S3 Access Logs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7Course-Summary"><span class="nav-number">10.</span> <span class="nav-text">7Course Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
</html>
