<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionWelcome to the “Optimizing Google BigQuery” course. I’m Guy Hummel and I’ll be showing you how to get the most from this big data service. BigQuery is an incredibly fast, secure, and sur">
<meta property="og:type" content="article">
<meta property="og:title" content="GCP-Professional-Architect-Optimizing-Google-BigQuery-19">
<meta property="og:url" content="https://example.com/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionWelcome to the “Optimizing Google BigQuery” course. I’m Guy Hummel and I’ll be showing you how to get the most from this big data service. BigQuery is an incredibly fast, secure, and sur">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T04:14:21.000Z">
<meta property="article:modified_time" content="2022-11-20T23:37:12.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>GCP-Professional-Architect-Optimizing-Google-BigQuery-19 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GCP-Professional-Architect-Optimizing-Google-BigQuery-19
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:21" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:21-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:37:12" itemprop="dateModified" datetime="2022-11-20T19:37:12-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to the “Optimizing Google BigQuery” course. I’m Guy Hummel and I’ll be showing you how to get the most from this big data service.</p>
<p>BigQuery is an incredibly fast, secure, and surprisingly inexpensive data warehouse, but there are ways to make it even faster, cheaper, and more secure.</p>
<p>To get the most from this course, if you don’t already have experience with BigQuery, then please take my Introduction to BigQuery course first.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>We’ll start with how you can reduce the amount of data that’s processed by your queries, which will also lower your costs. Then we’ll go into more depth on partitioned tables, which is one method for reducing processing requirements.</p>
<p>Next, we’ll talk about ways to speed up your queries, including denormalizing your data structures and using nested and repeated fields.</p>
<p>Finally, we’ll wrap up with how to use roles and authorized views for access control.</p>
<p>If you’re ready to learn how to get the most out of BigQuery, then let’s get started.</p>
<h1 id="Reducing-the-Amount-of-Data-Processed"><a href="#Reducing-the-Amount-of-Data-Processed" class="headerlink" title="Reducing the Amount of Data Processed"></a>Reducing the Amount of Data Processed</h1><p>BigQuery is relatively inexpensive to use. But if you process large amounts of data, your costs can add up quickly. At first, it looks like streaming and storage would be your highest costs. That would be true if you didn’t run many queries, but most organizations use BigQuery to run lots of queries. After all, query is even part of the name, so that must be what people use it for, right?</p>
<p>Although the streaming and storage costs are higher than the query costs on a per gigabyte basis, you only get charged for streaming once and you only get charged for storage once a month. With queries, on the other hand, you get charged every time you run a query, which can be hundreds, or even thousands, of times a month, so reducing how much data is processed by your queries is usually the best way to reduce your costs. Another benefit is that it often speeds up your queries too.</p>
<p>To see how to optimize our queries, let’s use some stock exchange data that Google makes available on Cloud Storage. First, create a dataset where you can put the tables. Click the down arrow next to your project name and select “Create new dataset”. Let’s call it “examples”.</p>
<p>Now create a table in the examples dataset. Change the Location to Google Cloud Storage. Here’s the path to the first stock exchange file.</p>
<p>There will be quite a bit of text to enter in this course, especially when we start writing queries, so if you don’t want to type everything yourself, you can copy and paste from a <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/optimizing-bigquery/">readme file</a> I put on Github. It’s at this URL.</p>
<p>Now call the Destination Table “gbpusd_201401”. For the Schema, click “Edit as Text” and put this in. Make sure you get rid of the text that was in this box already. Click “Create Table”…and it takes a little while to upload, so I’ll fast forward. I’m going to fast forward quite a few times throughout the course, so if some operations seem to take longer when you run them, it’s probably because of that.</p>
<p>Then go through the same process for the second file.</p>
<p>OK, now we can run a simple query to get a baseline before we try to optimize it. Click “Compose Query”. Then, before you do anything else, click on “Show Options” and uncheck the “Use Legacy SQL” box. I don’t recommend using Legacy SQL because you have to learn some non-standard syntax to use it. Now click “Hide Options” so it doesn’t clutter up the page. OK, click on the first table and then click “Query Table”. Let’s select star so we see how much data gets processed when we read all of the data. Let’s also get rid of the “LIMIT 1000” to make sure it’s reading the whole table.</p>
<p>Before you run it, see how much data it says it’s going to process. Click “Open Validator”, which is the green checkmark. It says it will process 46.4 meg. Now click “Run Query”. It warns us that we’ll be billed for all of the data in the table, even if we use a LIMIT clause. We actually want to read all the data in the table this time, so ignore the warning. Click “Run query”…and after a few seconds, it comes back and confirms that it processed 46.4 meg.</p>
<p>There are two basic approaches to reducing how much data is processed by a query. You can reduce the number of rows that are scanned and you can reduce the number of columns that are scanned.</p>
<p>One obvious way to try to reduce the number of rows processed is by using a LIMIT clause, but that warning message we just got said that we’d be billed for all the data in the table even if we use a LIMIT clause. Let’s try it to see if that’s true.</p>
<p>Add a “LIMIT 10” clause to the end of the query.</p>
<p>It still says this query will process 46.4 meg, but let’s run it anyway. And the warning message comes up again. Click “Run query”…it takes a few seconds, and it says that it processed the whole 46.4 meg again. It did run slightly faster, but that’s probably not because of the LIMIT clause. The processing time varies, even for the exact same query.</p>
<p>The bottom line is that we can’t reduce the number of rows scanned by using a LIMIT clause, so we’ll have to look at other approaches. I’ll show you how to do that later, but first let’s try limiting the number of columns scanned.</p>
<p>The obvious way to do that is to only select specific columns instead of doing a SELECT star. For example, on this table, instead of selecting all 5 columns, let’s just select 2 of them. Replace the star with “time, bid”.</p>
<p>That only processed 19.5 meg, so it actually worked. You probably also noticed that it didn’t give us a warning message this time. That’s because it doesn’t give you a warning when you select specific columns instead of selecting star.</p>
<p>Let’s go back to trying to reduce the number of rows processed. Have a look at the data by going to the Preview tab. Do you see a pattern? The data is sorted in ascending order by the time column. Maybe we could try to use the BETWEEN operator to only select rows that are between two dates. You can copy and paste this query from the Github file.</p>
<p>Since the validator accurately predicts how much data will be processed by a query, we don’t even have to run this to know that it won’t reduce the number of rows scanned because it says it will process 19.5 meg, which is how much it processed without the BETWEEN operator. It makes sense, when you think about it, because there’s no way for BigQuery to know that all of the data is properly sorted, so it still has to read the entire table.</p>
<p>Is there anything we can do to reduce the number of rows processed? Yes, there are a couple of ways, but they’re not easy. The first way is to put your data in separate tables. That’s kind of a brute force way of limiting your queries. It still scans the whole table, but since you only put some of your data in the table, that automatically limits how much data is processed.</p>
<p>The two tables we uploaded are actually split that way because there’s one for each month. The first is for January 2014 and the second is for February 2014. So all of the queries we’ve run so far have only been on the January data. If, instead, all of the data for 2014 were in one file, then we would have processed about 12 times as much data on every query.</p>
<p>In a normal relational database, breaking your data into tables like this is not recommended. Instead, you would use an index. But BigQuery doesn’t support indexes, so you can’t do that.</p>
<p>If you break your data into multiple tables, then how can you run queries across the tables when you need to? For example, what if you needed to run a query across all of the 2014 data? Would you have to write a long query with a UNION of all 12 tables? No, because fortunately BigQuery supports using wildcards in your table references.</p>
<p>Here’s how you would run a query across the two stock market tables. Let’s select the earliest time and the latest time from the tables. To query both tables, you just need to put in a star as the last character of the table name, which will match both 1 and 2. The wildcard has to be the last character of the table name.</p>
<p>When you run the query…you’ll see that it took the mintime from the first table because it’s in January and it took the maxtime from the second table because it’s in February.</p>
<p>You’ll recall that I mentioned that there are two ways to reduce the number of rows processed. I’ll tell you how to use the other method in the next lesson.</p>
<h1 id="Partitioned-Tables"><a href="#Partitioned-Tables" class="headerlink" title="Partitioned Tables"></a>Partitioned Tables</h1><p>In the last lesson, I showed you how you could break your data into tables to limit how much data your queries process. But that seems kind of awkward, doesn’t it? There are, in fact, quite a few disadvantages to this approach. First, you have to get your data into all of these tables somehow, probably by writing a program. Second, the more tables there are in your query, the worse your query performance is. And third, you can’t reference more than 1,000 tables in a single query. You might think you’re not likely to ever hit that limit, but it’s not as unlikely as it sounds. If you collect a lot of data every day, then you may want to have a separate table for every day. Then if you had 3 years worth of data, that would be more than 1,000 tables.</p>
<p>BigQuery has a solution for this called partitioned tables. The way it works is you put all of your data in one table, but in a separate partition for every day. Then you can limit your queries to particular days and it won’t scan the rest of the table.</p>
<p>Of course, you still have the challenge of how to divide your data into the right partitions, just like you would with dividing your data into separate files with the other approach. If you’re starting fresh and want to stream new data into a table every day, then it’s very easy. You can create an ingestion-time partitioned table, and when you stream or upload data to it, BigQuery will automatically put it in the partition for that day. It also creates a pseudo-column called _PARTITIONTIME that contains the date for each data record.</p>
<p>It wouldn’t be very useful to upload our existing data to an ingestion-time partitioned table, though, because all of the data would go into today’s partition regardless of what date was in each record.</p>
<p>The solution is to create a time-unit column-partitioned table. You just need to tell BigQuery which column contains the date, and it will put each data record into the right partition.</p>
<p>We can copy the data from our existing table and put it into a new partitioned table in a single command. You can copy it from the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/optimizing-bigquery/">repository</a>.</p>
<p>We use “bq query” because we’re going to query the existing table to get the data. We set the “use_legacy_sql” flag to “false” so it’ll use standard SQL. Then we use the “replace” flag, which isn’t strictly necessary, but if you need to run the command again, then it will overwrite the destination table instead of appending to it. Then we tell it what to call the destination table. I used the same name as the original table except with a ‘p’ for ‘partition’ at the end. To make this a partitioned table, we use the time_partitioning_field flag and set it to the column that contains the date, which is called “time” in the original table. Finally, we run a “SELECT *” to get all of the data from the original table.</p>
<p>By the way, if you wanted to partition the data based on a different unit of time than a day, you’d need to add the “time_partitioning_type”, which can be set to day, hour, month, or year. If you don’t set it, then it defaults to “day”.</p>
<p>I’m going to run the command from Cloud Shell, which is right here. I need to authorize it.</p>
<p>Alright, it’s done. You’ll notice that I recorded this demo after Google changed BigQuery’s user interface, so that’s why it looks different.Now let’s have a look at the partitioned table it created. It looks exactly the same as the original table. The only indication that there’s anything different about it is this note up here saying that it’s a partitioned table.</p>
<p>Let’s say you wanted to retrieve records only from January 9th and 10th. Here’s the SQL to do that.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT time, bid</span><br><span class="line">FROM examples.gbpusd_201401p</span><br><span class="line">WHERE time</span><br><span class="line"> BETWEEN TIMESTAMP(&#x27;2014-01-09&#x27;)</span><br><span class="line"> AND TIMESTAMP(&#x27;2014-01-10&#x27;)</span><br><span class="line">ORDER BY time ASC</span><br></pre></td></tr></table></figure>

<p>It’s basically the same as a query we tried to run earlier except that it’s querying the partitioned table instead of the original one. What a difference that makes. It says it will only process 1.3 meg. When we tried to do it with the original table, it said it would process 19.5 meg. Let’s run it to make sure it works. Yes, it only processed 1.3 meg.</p>
<p>Before we move on, I should mention another really useful feature of partitioned tables. If you have more data being added to a table every day, you may want to delete the oldest data at some point. You can configure a partitioned table to do this automatically by setting a partition expiration time. For example, you could say that each partition will be deleted after it’s 12 months old. This would ensure that you only have the latest 12 months worth of data in your table. You can also set an expiration time on an entire table if you want.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Denormalized-Data-Structures"><a href="#Denormalized-Data-Structures" class="headerlink" title="Denormalized Data Structures"></a>Denormalized Data Structures</h1><p>Now you know how to reduce the amount of data BigQuery processes, which will reduce your costs, but it won’t necessarily reduce how long it takes your queries to run. There are different strategies for doing that, but we might have to break some database structure rules.</p>
<p>One of the core principles of good schema design is normalization, which is basically to remove redundancy from data. Some of you are probably saying, “That’s a dramatic oversimplification of the concept, and removing redundancy is more of a result of normalizing data.” You’re right, of course, but for our purposes here, the lack of redundancy is what matters. I should mention, though, that not only does normalization reduce the amount of space taken up by the data, but it also improves data integrity, because if the same information is stored in multiple places, you could end up with inconsistencies between the copies.</p>
<p>Having said all that, in BigQuery, you may want to do the unthinkable and denormalize your data and add redundancy into it. Why would you do that? Well, because it can make your queries much faster, and because the downsides of having redundant data are not a problem for what we’re doing. First, data integrity is not an issue because BigQuery isn’t a transaction processing system. We’re just using it for reporting, not for recording transactions, so we’re not going to create inconsistencies in the data, since we’re not going to modify the data at all.</p>
<p>Second, the extra cost associated with storing and querying extra copies of data is often outweighed by the much faster query times. Why does denormalization make queries so much faster? Because when you query a normalized database, you will often have to join multiple tables together, but joining tables is a very time-consuming operation. It’s not very noticeable when the tables are small, but as the tables get larger, the time required to join them increases exponentially.</p>
<p>This performance comparison done by Google shows just how much of a difference it can make. The slope of this line gets so steep that it would take a ridiculously long time to join tables that have billions of rows.</p>
<p>Let’s go through an example of how to denormalize a data structure. We’ll use some MusicBrainz data, which is a freely available encyclopedia of music. Here are three data files to import into tables.</p>
<p>Create a new table in the examples dataset. Change the Location to Google Cloud Storage and paste the Data File URL. Then change the File format to JSON. Call the table “artist”. Under the Schema, click “Edit as Text”, then paste in the contents of the schema file…and create the table.</p>
<p>Now do the same for the other two tables. I’ll show the process, but I’ll speed it up so it all happens in 15 seconds, so hang on.</p>
<p>OK, here’s how these three tables are organized. They’re normalized, so there isn’t any redundant data. Now we’re going to take columns from these three tables and combine them into one table. We’re going to join these tables and save the results so that when we need to run queries in the future, they don’t have to go through the join step first, which will save a lot of time. We’re doing an inner join on the three tables using the artist. The resulting table will have lots of duplicate information, such as multiple copies of an artist’s name and multiple copies of a recording’s name.</p>
<p>Click Show Options and set the Destination Table to “recording_by_artist”. While you’re here, make sure the “Use Legacy SQL” box is not checked. Alright, now run the query, which should take about 30 or 40 seconds.</p>
<p>Since this query took a long time, it says, “We can notify you when long-running queries complete.” That could be helpful in the future, so click “Enable notifications”. Then you have to click Allow.</p>
<p>Let’s see how much extra space it takes up. The recording table takes up about 1.5 gig…and the other two tables…bring the total up to about 1.7 gig. The denormalized table takes up about 2.3GB, so it’s about 35% bigger, but the speed gains would be worth it if we were dealing with larger tables.</p>
<p>Let’s get rid of the Destination Table option so we don’t accidentally overwrite it. This table only has about 18 million rows, so the speed gain won’t be very noticeable. One benefit is much simpler queries, though. Compare this…with this.</p>
<p>Both queries do exactly the same thing.</p>
<p>You can see why this table takes up more space. The name “Red Elvises” is repeated many times. In the original tables, it only had the artist’s ID for each recording, and the artist’s name was only listed once in the artist table.</p>
<p>By the way, you probably wouldn’t use the web interface to denormalize data unless it’s a one-time conversion. If you need to denormalize data on a regular basis, then you would want to automate the process using Cloud Dataflow or something similar.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Nested-Repeated-Fields"><a href="#Nested-Repeated-Fields" class="headerlink" title="Nested Repeated Fields"></a>Nested Repeated Fields</h1><p>Denormalizing data might seem like a pretty clunky way to increase query performance. Is there a way to improve performance without having to add lots of redundant data? Yes. BigQuery supports something called “nested repeated fields”. This is a way to combine data into one table without redundancy.</p>
<p>Google has provided a very small data file to demonstrate this concept. First, open it in your browser and then right-click and do a Save As.</p>
<p>Then upload it into a new table. Change the file format to JSON. Call it “persons_data”. Now open the schema file…and copy the contents here. It’s pretty hard to understand the schema by looking at this, but it will be easier to see once you’ve created the table.</p>
<p>Now have a look at the schema. Some of the fields have a dark gray background, like phoneNumber. And under phoneNumber, you’ll see that areaCode and number look like subfields.</p>
<p>It gets more interesting with the children and citiesLived fields. Not only do they have nested fields under them, but they are also repeated fields, so each person can have multiple children and citiesLived.</p>
<p>Let’s go to the Preview to see what the data looks like. This is pretty different from a normal record because it takes up two rows and there are blanks at the beginning of the second row. One thing that can be confusing is that it looks like the child Jane lived in Seattle in 1995 and the child John lived in Stockholm in 2005 because in a normal record, all of the pieces of data on one row make up the entire record. But with repeated records, that’s not the case.</p>
<p>It’s easier to see if you look at the next record. First of all, either Mike Jones has a really hard time deciding where he wants to live or there’s something wrong with this data. Anyway, here it’s obvious that the children fields are not directly related to the citiesLived fields. Another way to see this is to look at the JSON representation of this record. Click on the JSON button. It’s not as easy to read as the table view, but it makes it very obvious how the record is organized.</p>
<p>To refer to a nested field, you can use the dot notation that it shows in the schema. For example, to get a list of all of the people in the table and their phone numbers, you would run this.</p>
<p>You refer to the phone number as phoneNumber.number. You don’t need the “LIMIT 1000” because there are only 3 records in this table. It doesn’t hurt to leave it there, but I prefer to keep unnecessary clutter out of my queries.</p>
<p>If you want to refer to a nested repeated field, though, then it takes a bit more work. Let’s say you want to see who has lived in Austin. It’s giving us an error because it doesn’t recognize the “place” field. That’s because it’s a nested repeated field, so you have to use the UNNEST function. First, put in a comma after the table name. Then unnest citiesLived. You’ll notice that the error went away because now that citiesLived is unnested, we can refer to the field as just “place” instead of citiesLived.place. Let’s also select place, so we can verify that the query is working properly.</p>
<p>It came back with the correct results because Anna Karenina and Mike Jones both lived in Austin, but John Doe didn’t.</p>
<p>What happens if you don’t specify the WHERE clause? Will it print all the places for each person?</p>
<p>Yes, it does, although it repeats the person’s name beside every city, unlike what we saw in the Preview tab. The records also come up in random order, because we didn’t specify an ORDER BY clause.</p>
<p>What if you do a SELECT *? Will it look like what we saw in the Preview tab?</p>
<p>Yes, it looks like the Preview tab…or does it? There are 9 rows in the result, but there are only 3 in the table. What’s going on? It’s because we left UNNEST(citiesLived) in the query. Now it’s very clear what the UNNEST does. It creates two new columns with the contents of the nested fields. And since our query says to select from the table and from UNNEST(citiesLived), it’s actually doing a JOIN. That’s what the comma between the two means. In fact, you could replace the comma with JOIN and it would work the same. There are 9 citiesLived records in total, one for each city that each person has lived in, which is why there are now 9 records in the result.</p>
<p>OK, so if we remove UNNEST(citiesLived), will it look like the Preview tab?</p>
<p>Ignore the warning. Yes, it does, although the records are in random order, of course.</p>
<p>In this example, we uploaded a JSON file that already contained nested repeated data. If you want to do this with your own data and it’s not already in that format, then you’ll have to create it yourself. There are many ways of doing this, such as using Cloud Dataflow, but that’s outside the scope of this course.</p>
<p>If you find that you frequently need to unnest the same repeated fields in your queries, then you might want to create a view.</p>
<p>Here’s a really simple example. Open the query where you selected the person’s name and place, but didn’t use a WHERE clause.</p>
<p>Then click Save View. For the table name, use cities_by_person. You’ll notice that the icon to the left of the name is different from the other ones. That’s because a view isn’t a table, even though it asked us to put in a table name when we created it. If you click on the view, you’ll see that the schema looks like a table’s schema, but if you click on Details, you’ll see that it’s quite a bit different.</p>
<p>In fact, it doesn’t store any data. It just stores the query. So when you use a view in your queries, it actually runs the query again. So why would you want to use a view if it just runs the query again anyway? Well, it can make your queries less complicated. Here, I’ll show you.</p>
<p>First, replace the table name (persons_data) with the name of the view (cities_by_person). Now you can remove UNNEST(citiesLived). And remove the comma too. Let’s put in a WHERE clause to make it a useful query. Let’s search for all of the people who have lived in Stockholm.</p>
<p>OK, I realize that it probably wasn’t worth the effort of creating the view, and changing our queries to use the view, all to make the queries a little simpler, but if you were regularly running queries with multiple levels of UNNESTs, then it might be handy to create a view.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Access-Control"><a href="#Access-Control" class="headerlink" title="Access Control"></a>Access Control</h1><p>Data can be one of the most precious resources an organization owns. So it’s important to keep tight control of not only who’s allowed to read your data, but also who’s allowed to modify or delete it. That might seem pretty basic, but doing this with BigQuery is more complicated than it sounds.</p>
<p>BigQuery provides several layers of access control. The top layer is primitive roles, which act at the project level. They’re administered through IAM, the Identity and Access Management system.</p>
<p>There are three primitive roles: owner, editor, and viewer. When you add a member to a project, you can assign them one of these roles, which will apply to all Google Cloud Platform services, not just BigQuery. A viewer can view all datasets and run jobs (such as queries). Editors have viewer permissions, but can also modify or delete all tables. They can’t modify datasets, but they can create new datasets. An owner has editor permissions, but can also delete all datasets and see all jobs for all users in the project.</p>
<p>Primitive roles are fine as long as you have these very simple access control requirements:</p>
<ul>
<li>First, each user can have the same permissions for all GCP resources in a project, not just BigQuery. For example, if a user has the Editor role, then they’ll be an Editor not only for BigQuery, but also for Google Cloud Engine instances or any other resources in the project. If you only have BigQuery enabled in this project, then this isn’t an issue.</li>
<li>Second, each user can have the same permissions for all datasets in a project. For example, if each team has owner permissions for their team’s datasets, then they’ll also have owner permissions for other team’s datasets in this project.</li>
<li>Third, you don’t need to separate data access permissions from job-running permissions. For example, if a user has permission to view data in a project, then they can also run queries on that data.</li>
</ul>
<p>If all of these requirements are true for a project, then primitive roles are a good solution, but otherwise, you’ll need to use predefined roles.</p>
<p>There are six predefined roles. The dataViewer, dataEditor, and dataOwner roles are essentially the same as the primitive roles except for two things: First, you can assign these roles to users for individual datasets, and second, they don’t give users permission to run jobs or queries. Those permissions can be granted through the user and jobUser roles. A jobUser can only start jobs and cancel jobs. A user, on the other hand, can perform a variety of other tasks, such as creating datasets. The admin role gives all permissions.</p>
<p>Here’s how you can use predefined roles to give more fine-grained access than primitive roles in each of these situations:</p>
<ul>
<li>To give a different level of access to BigQuery than to other GCP resources in a project, use BigQuery roles, such as BigQuery Data Editor. Then also use predefined roles for any other GCP resources they need to access, such as App Engine Admin.</li>
<li>To give a user or group a different level of permissions for a dataset in a project, click the down arrow next to the dataset and select “Share dataset”. Then add a user or group by putting in their email address. Then select one of the three roles (owner, editor, or viewer).</li>
<li>To give job-running permissions without giving data access permissions, select either BigQuery User or BigQuery Job User. In most cases, you should give BigQuery User because it lets the user list datasets and tables as well as create new datasets. Job Users can only run jobs.</li>
</ul>
<p>You might be wondering why you would want to separate data access permissions from job-running permissions because it would seem like most users would need both. That’s true in many cases, so remember that you have to assign both types of roles to users in order for them to look at the data and run queries (unless you’re using primitive roles). But there are situations when you might want to give only one or the other. For example, if you have an application that monitors the size of your tables, then you might want to assign only the BigQuery Data Viewer role to its service account. That way, even if the application developers accidentally make a change to the program that would cause a query to run, it will be disallowed. Why would that matter? Because queries incur a cost and a program could potentially run up some hefty charges if there’s a bug.</p>
<p>So far, I’ve only shown you how to set permissions at the project and dataset levels. There’s a way to set permissions at the table level and even to particular data within a table, but it’s a much more complicated process. The only way to do it is to use something called an authorized view.</p>
<p>We already created a view in the last lesson. What’s different about an authorized view is that it allows users to access the results of a query without giving them access to the tables that were queried. So, for example, if you didn’t want a particular group of users to have access to certain columns in a table, you could run a query that didn’t include those columns, and then save the results as an authorized view for that group.</p>
<p>To make this work, you need to perform four steps:</p>
<ul>
<li>First, create a separate dataset to store the view. You need to do this because if you were to put the view in the same dataset as the original tables, then the group would be able to access the tables too and not just the view.</li>
<li>Second, create the view in the new dataset.</li>
<li>Third, give the group read access to the dataset containing the view.</li>
<li>Fourth, authorize the view to access the source dataset.</li>
</ul>
<p>This assumes that you’ve already given the group permission to run queries in the project.</p>
<p>Let’s say you wanted to give a group called “team1” access to only the name, age, and gender fields in the persons_data table. Before I start, I should mention that if you don’t currently have permission to assign roles to other users, then you won’t be able to do all of the steps I’m about to show you.</p>
<p>OK, first create the new dataset. Call it “shared_views”.</p>
<p>Then, run a query to select the name, age, and gender fields from the table.</p>
<p>Now click “Save View”. Change the dataset to “shared_views” and call the table “persons_view”.</p>
<p>Then click the down arrow to the right of shared_views and select “Share dataset”. In the menu on the left, select “Group by e-mail” since we’re giving permission to a group, not a user. Then I’ll fill in the email address of the team1 group. If you have a group you can assign, then use that one. Leave the permission on “Can view” and click Add. You might want to uncheck “Notify people via email” if you’re testing this with an actual group of people. And click Save Changes.</p>
<p>Now that team1 has read access to the view, the only thing left to do is to give the view read access to the persons_data table. We need to do this because the view takes on the permissions of the person using it, and since team1 doesn’t have access to the persons_data table, they’d get an error if they tried to use this view.</p>
<p>Now we’re finally at the point where we’re going to turn this into an authorized view. In the menu next to the examples dataset, select “Share dataset”. In the menu at the left, select “Authorized View”. Then click “Select View”. Change the dataset to “shared_views” and put “persons_view” for the table. Click OK, click Add, and save the changes.</p>
<p>Now users in team1 will be able to run queries on this view, even though they don’t themselves have access to the persons_data table.</p>
<p>Before we go, you’ll probably want to delete everything you’ve created in this course. Fortunately, that will be very easy. Click the menu next to examples and select “Delete dataset”. Type in “examples” to confirm that you want to delete the dataset and all of the tables in it. Now do the same thing for the shared_views dataset.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to optimize BigQuery. Now you know how to reduce the amount of data processed by queries, speed up your queries through denormalization, and use access controls on projects, datasets, and tables.</p>
<p>To learn more about BigQuery, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know in the Comments tab below this video. Thanks and keep on learning!</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/" rel="prev" title="GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18">
      <i class="fa fa-chevron-left"></i> GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/" rel="next" title="GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20">
      GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reducing-the-Amount-of-Data-Processed"><span class="nav-number">2.</span> <span class="nav-text">Reducing the Amount of Data Processed</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Partitioned-Tables"><span class="nav-number">3.</span> <span class="nav-text">Partitioned Tables</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Denormalized-Data-Structures"><span class="nav-number">4.</span> <span class="nav-text">Denormalized Data Structures</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Nested-Repeated-Fields"><span class="nav-number">5.</span> <span class="nav-text">Nested Repeated Fields</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Access-Control"><span class="nav-number">6.</span> <span class="nav-text">Access Control</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-number">7.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
