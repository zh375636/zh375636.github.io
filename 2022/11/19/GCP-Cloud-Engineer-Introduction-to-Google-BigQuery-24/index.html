<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionWelcome to “Introduction to Google BigQuery”. My name’s Guy Hummel, and I’m a Google Certified Professional Cloud Architect and Data Engineer. If you have any questions, feel free to conne">
<meta property="og:type" content="article">
<meta property="og:title" content="GCP-Cloud-Engineer-Introduction-to-Google-BigQuery-24">
<meta property="og:url" content="https://example.com/2022/11/19/GCP-Cloud-Engineer-Introduction-to-Google-BigQuery-24/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionWelcome to “Introduction to Google BigQuery”. My name’s Guy Hummel, and I’m a Google Certified Professional Cloud Architect and Data Engineer. If you have any questions, feel free to conne">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T04:03:09.000Z">
<meta property="article:modified_time" content="2022-11-23T00:51:44.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/19/GCP-Cloud-Engineer-Introduction-to-Google-BigQuery-24/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>GCP-Cloud-Engineer-Introduction-to-Google-BigQuery-24 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Cloud-Engineer-Introduction-to-Google-BigQuery-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GCP-Cloud-Engineer-Introduction-to-Google-BigQuery-24
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:03:09" itemprop="dateCreated datePublished" datetime="2022-11-19T00:03:09-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-22 20:51:44" itemprop="dateModified" datetime="2022-11-22T20:51:44-04:00">2022-11-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Cloud-Engineer/" itemprop="url" rel="index"><span itemprop="name">GCP-Cloud-Engineer</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Cloud-Engineer-Introduction-to-Google-BigQuery-24/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Cloud-Engineer-Introduction-to-Google-BigQuery-24/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to “Introduction to Google BigQuery”. My name’s Guy Hummel, and I’m a Google Certified Professional Cloud Architect and Data Engineer. If you have any questions, feel free to connect with me on LinkedIn and send me a message, or send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>This course is intended for anyone who’s interested in analyzing data on Google Cloud Platform.</p>
<p>To get the most from this course, it would be helpful to have some experience with databases. It would also be helpful to have some familiarity with writing queries using SQL, but it’s not a requirement.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>To save you the trouble of typing in the URLs and commands shown in this course, I’ve created a GitHub repository with a readme file that contains all of them. The link to the repository is at the bottom of the course overview below.</p>
<p>We’ll start by running some basic queries and saving the results.</p>
<p>After that, I’ll show you how to load data into BigQuery from files and from other Google services.</p>
<p>Then, you’ll see how to stream data into BigQuery one record at a time.</p>
<p>Finally, we’ll wrap up with how to export data from BigQuery.</p>
<p>But first, I’ll give you a quick overview of why you’d want to use BigQuery.</p>
<p>Data warehouses have been around for decades. When databases first became popular, they were primarily used for transaction processing (and that’s still the case today). But managers also needed to analyze data and create reports, which is difficult to do when the data resides in numerous databases across an organization. So data warehouses were created to collect data from a wide variety of sources, and they were designed specifically for reporting and data analysis.</p>
<p>If data warehouse technology has been around for so long, why did Google release BigQuery, and why would you use it instead of a more established data warehouse solution? Well, for two main reasons: ease of implementation and speed.</p>
<p>First, building your own data warehouse can be expensive, time consuming, and difficult to scale. With BigQuery, on the other hand, the only thing you have to do to get started is load your data into it. And you only pay for what you use, so you don’t need to spend a lot of money building capacity to handle peak periods.</p>
<p>Second, even if you do build your own high performance data warehouse, it will probably never be as fast as BigQuery because BigQuery can process billions of rows in seconds. This speed is especially valuable if you need to perform real-time analysis of streaming data, such as from online gaming systems or Internet of Things sensors.</p>
<p>Okay, now if you’re ready to learn how to crunch big data with ease, then let’s get started. We’d love to get your feedback on this course, so please give it a rating when you’re finished.</p>
<h1 id="Running-a-Query"><a href="#Running-a-Query" class="headerlink" title="Running a Query"></a>Running a Query</h1><p>To open BigQuery, go to the Google Cloud Platform console, then find BigQuery in the menu. Alternatively, you can type “bigquery” in the search bar, which is probably easier.</p>
<p>Suppose you wanted to see which US state had the most babies with the same name in one year. There’s a public dataset with baby name data available on BigQuery. If you look under bigquery-public-data, you’ll see one called “usa_names”. If you click on it, you’ll see two tables that are almost the same. We’ll use the first one.</p>
<p>When you click on the table, it brings up the schema. If you click on the Details tab, it’ll show you a description of the data in the table. Let’s make this bigger. If you click on the Preview tab, it’ll give you a sample of the data. If you click the “Query Table” button, it will even give you the skeleton of a SQL query.</p>
<p>However, the query that it put in isn’t complete. You can tell because the Validator circle at the bottom right is a red exclamation point, which means there’s a problem. To see why, click on it and open the Validator. It says the “SELECT list must not be empty”. Let’s “SELECT *” from the table, which, if you’re not familiar with SQL, means select everything. Now the exclamation point has turned into a green check mark, so it’s a proper query.</p>
<p>We should sort the results with the biggest number at the top, so use “ORDER BY number DESC” (for “descending”), and then, since we only need to see the top results, let’s put in a LIMIT of 10. This line is quite long now, so if you want to make it easier to read, select the “Format” option. That’s better.</p>
<p>Now click the “Run” button. It only takes a few seconds to run.</p>
<p>The top result is Robert in New York in 1947, with 10,025 occurrences. You might be wondering if we did something wrong with this query because all of the top 10 names are boy’s names. Let’s look only for girl’s names and see what happens. Add “WHERE gender &#x3D; ‘F’”. Remember to put quotes around the F.</p>
<p>Now it makes sense why we only saw boys’ names before. The highest number of occurrences for a girl’s name was “Mary” in Pennsylvania in 1918, with 8,184 occurrences. Although that’s a lot of Marys, there were 9,054 Roberts in New York in 1951 and that was the 10th highest number of occurrences, so no girl’s names showed up in the top 10.</p>
<p>Before we run any more queries, let’s see how much this is costing us. I’ll cover that in the next lesson.</p>
<h1 id="Pricing"><a href="#Pricing" class="headerlink" title="Pricing"></a>Pricing</h1><p>There are two components to BigQuery pricing: storage and queries.</p>
<p>BigQuery’s storage charges are incredibly cheap. It costs two cents per gigabyte per month, which is the same price as Cloud Storage Standard. What’s even better is that if you don’t edit a table for 90 days, then the price for that table drops to one cent per gig per month until you modify the data in the table again. That’s as cheap as Nearline Storage! In fact, it’s even cheaper because when you read data from Nearline Storage, there is a one-cent per gig charge. With BigQuery storage, you aren’t charged for reading data at all.</p>
<p>Since we’ve only been using public datasets so far, there won’t be any storage charges.</p>
<p>The only other charge is for queries. (There’s also a charge for streaming data to BigQuery in real-time, but that doesn’t apply to these examples and I’ll cover it in another lesson.) For queries, the first terabyte per month is free. After that, it costs $5 per terabyte, which is half a cent per gigabyte. Wait a minute, didn’t I just say that you aren’t charged for reading data from BigQuery storage? Yes, that’s true because BigQuery charges query fees regardless of where you read the data from. For instance, if you query a dataset that’s in Cloud Storage, then you get charged at the same rate that you would from querying a dataset in BigQuery storage, so the charge isn’t for reading – it’s for processing.</p>
<p>For high-volume customers, there’s also flat-rate pricing, but it’s only worthwhile if you spend at least $2,000 per month. It only applies to query costs and not storage, which is still separate.</p>
<p>To see how much data is processed by a query, look in the Validator message area. Since there isn’t an error in the syntax, now it’s showing how much data would be processed by the query above. In this case, it’s 163 MB. Considering that the first terabyte of processing in a month is free, this won’t cost us anything, but even if we were already over the 1 terabyte mark this month, it wouldn’t cost much. How much? Less than a tenth of a cent. I’d say that’s pretty reasonable.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Saving-Query-Results"><a href="#Saving-Query-Results" class="headerlink" title="Saving Query Results"></a>Saving Query Results</h1><p>Now that you know how inexpensive it is to store data on BigQuery, you may want to save some of your query results there too.</p>
<p>It’s very easy to do that. First, though, since you don’t have a dataset to put tables in, you’ll need to create one. Click your project name and then click “Create Dataset”. You need to give it a name, so call it “babynames”. That’s the only thing you have to specify in this dialog box, but if you want to, you can set the location, which you might want to do for compliance reasons. You can also set an expiration time so that a certain number of days after a table has been created, it will be automatically deleted. You would do this if it’s temporary data that you don’t want to have to remember to delete later. Now click “Create dataset”, and you should see your new dataset show up under your project name.</p>
<p>Great, now we’re ready to run a query. Let’s open one of our previous queries, so we don’t have to type a new one in. If the list of your previous queries isn’t already showing, then click “Query History”. Click the button to the right of your first query to bring it up again.</p>
<p>Now before you re-run this query, select “Query settings” from the menu…and then select “Set a destination table for query results”. It has already set the right project and dataset name. Now we just need to tell it which table to use. If the table doesn’t already exist, it’ll create it, so you can type in whatever table name you want. Let’s call it “babynames_top10”. Click Save and then click “Run”.</p>
<p>Now if you click on the babynames dataset, you’ll see that it created the “babynames_top10” table. Then click the “Preview” tab and you’ll see the results from the query.</p>
<p>What if you decide to save the results to a table after you’ve run the query? That’s easy too. Click “Query History” and click on the first query. We didn’t specify that the results should be saved to a table when we ran this query the first time, but if you scroll down, you’ll see that it saved the results to a temporary table. Click that to see the table. Now you can click “Copy Table” and specify where you want to save it. You have to select the dataset first and then type in a new table name.</p>
<p>One more thing to be aware of is that if you don’t specify a destination table and it puts the results in a temporary table, the temporary table stays in cache for about a day. So if you run the query again within 24 hours, it’ll retrieve the cached copy and you won’t be charged for the query.</p>
<p>However, if you run a query again and specify a destination table, like we just did, then it won’t read the data from cache. So let’s run it again without the destination table option.</p>
<p>If you go to the “Job information” tab, you can see that it didn’t process any bytes because the results were cached. Also, the duration of the query was zero seconds because it just retrieved the results from cache rather than running the actual query. Of course, since we could have just looked at the results in the temporary table, there may not seem to be a lot of point in re-running the query. That’s probably true if you’re using the web interface, but if you’re using the bq command or the BigQuery API, then it might be useful in some cases.</p>
<p>Before we go, let’s get rid of the table we created. Click on the dataset name, and then click “Delete Dataset”. This will delete the dataset and all of the tables in it, so you have to type the name of the dataset before it’ll delete it. </p>
<p>Okay, that’s it for this lesson.</p>
<h1 id="Loading-Data"><a href="#Loading-Data" class="headerlink" title="Loading Data"></a>Loading Data</h1><p>It’s great having public datasets in BigQuery that you can use, but what about analyzing your own datasets? How do you get them into BigQuery? There are many ways to do this. If your data is already in another Google service, then there’s usually a way to get it into BigQuery, although sometimes it requires an intermediate step. Some of the most commonly used sources are Cloud Storage, Google Drive, Firestore, Cloud Operations, Bigtable, and Google Analytics.</p>
<p>If your data isn’t in a Google service, then you can upload it to BigQuery through the web interface, the command line, or the API. Uploading through the web interface is the simplest, although it does have limitations.</p>
<p>Suppose you want to upload a dataset of the number of occurrences of all baby names across the US for a particular year. Since you don’t already have a copy of this dataset, you’ll need to download it from the Social Security website. The URL is in the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">repository</a> for this course. Then unzip the file. You can see that it contains one text file for each year from 1880 to 2019. Let’s have a look at the 2019 file.</p>
<p>It’s a very simple file, with a name, a gender, and the number of occurrences of that name for babies born in 2019. Although the filename extension is “txt”, it’s actually a comma-separated values file (or CSV). BigQuery can upload files in 5 formats: CSV, JSON, Avro, ORC, or Parquet.</p>
<p>Before you can upload a file, you need to create a dataset and table in BigQuery to hold it. Let’s call it “babynames”.</p>
<p>Now you need to create a table in that dataset, so click the “Create Table” button. Change the Source to “Upload”. If you click on the menu, you’ll see that you could also choose Cloud Storage, Google Drive, or Cloud Bigtable. If your data were in one of those three services, then you could load it into BigQuery from here or you could even leave the data where it is and make it an external data source, also known as a federated data source. However, the performance is usually slower when you query an external data source than if the data resides in BigQuery storage, so it’s often better to copy the data into BigQuery instead.</p>
<p>Okay, back to the task at hand. Set the Location to “Upload” and click the “Browse” button. Then select the “yob2019.txt” file. Change the file format to CSV. Now you need to give your table a name. Let’s call it “names_2019”.</p>
<p>You’ll notice there’s an option to automatically detect the schema and input parameters. That’s often a very handy feature because it saves you from having to enter the schema manually. Let’s see if it works with this data file.</p>
<p>It got an error. The message is a little bit cryptic, but here’s what happened. When BigQuery tries to detect the schema, it only looks at the first 100 records. In this file, the first 100 records all have ‘F’ in the second column. BigQuery assumes this means that the second column can be either an ‘F’ for “False” or a ‘T’ for “True”, so it sets this column to Boolean. When it tries to upload records with ‘M’ in that column, it gets an error because it’s expecting an ‘F’ or a ‘T’.</p>
<p>In situations like this, you have to specify the schema manually. Fortunately, it’s pretty easy in this case. First, I’ll redo everything except the schema.</p>
<p>Okay, now instead of asking it to auto-detect the schema, click “Add field”. The first field is the name, so type “name”. It’s a string, so the type is set correctly. The mode is set to “Nullable” by default, which means that this field can be empty for some records. If we wanted to make sure that no records were missing the name, then we would set the mode to “Required”. This file isn’t missing any names, but let’s leave it as Nullable anyway.</p>
<p>Now we’ll add the second field. Call it “gender”, and leave it as a string. The third field is the number of people who have this name, so call it “count”. It’s a whole number, so set the Type to “Integer”. Okay, now click “Create table”. It only takes a couple of seconds to upload all of the data into the table.</p>
<p>All right, go to the table and click on the Preview tab to see a sample of the data. That looks right. At this point, you could run queries on this table just like you did with the table in the public dataset.</p>
<p>Okay, that was all pretty easy, but remember when I said that the web interface has limitations? One big limitation is that you can only upload files that are 10 megabytes or less in size. There are lots of data files that are bigger than that. Here’s an example. It’s a 35 meg file that contains over 200,000 questions and answers from the game show, Jeopardy. You can find the URL for this file in the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">repository</a>. If you try to load it using the web interface, it’ll give you an error.</p>
<p>In cases like this, you need to use the command line. Alternatively, you could upload it to Cloud Storage first and then move it to BigQuery, but that would be a bit of a hassle, so it’s better to use the command line.</p>
<p>If you haven’t used any of Google Cloud’s command line tools yet (that is, gcloud, gsutil, kubectl, or bq), then you’ll have to install the Google Cloud SDK. The installation instructions are at this <a target="_blank" rel="noopener" href="https://cloud.google.com/sdk/docs/install">URL</a>. If you need to install the SDK, then pause this video while you do that. .</p>
<p>Okay, first you need to create a table to put data in, which we could do through the web interface again, but let’s use the command line for that too. Before we create the table, we need to have a dataset to put the table in. If we had used a more generic name for the previous dataset, instead of “babynames”, then we could have created a table for the Jeopardy data in that dataset too. Let’s not make that mistake again. What should we call it? Maybe we should put all of the data that we downloaded from the internet in it. We could call it “downloads” or something like that, but maybe we should just call it “public”.</p>
<p>The command for all BigQuery operations is “bq”. To create a dataset, type “bq”, and then “mk” for make, and then the name of the dataset, which is “public” in this case.</p>
<p>Okay, the dataset was created. Now we need to create a table. Let’s call the table “jeopardy”. There are a couple of ways to create the table. You could create an empty table and then upload the data into it or you could do it all in one step, which is usually easier.</p>
<p>To upload a file, type “bq load”, then the autodetect flag, which tells it to automatically detect the schema so you don’t have to specify the schema yourself. Then type the name of the table you want to load it into. If you haven’t set a default dataset, then you also need to specify the dataset name. In this case, you would type “public.jeopardy”. Then type the filename you want to upload, which is JEOPARDY_CSV.csv. If you’re not in the directory where the csv file resides, then you’ll have to put in the pathname to that file. It’ll take a little while to upload the file.</p>
<p>By the way, another reason to use the command line instead of the web interface is if you need to upload lots of files at the same time. With the bq command, you can put an asterisk in the filename, which will act as a wildcard and upload all matching files.</p>
<p>Let’s have a look in the web interface again to make sure the file uploaded properly. You have to refresh the page first so you can see the updates. Great, there’s the “public” dataset and there’s the “jeopardy” table. Click on the jeopardy table, and then go to the Preview tab. It looks good. Did you notice that the column names are actually descriptive? They have names like “Show Number” and “Category” instead of generic names like “string_field_0”. That’s because the first line of the csv file listed the field names.</p>
<p>On a different topic, you might be wondering why we didn’t just rename the “babynames” dataset to “public” instead of creating a new dataset. Well, that’s because you can’t rename a dataset in BigQuery. The only way to do it is to create a new dataset, copy all of the tables from the old dataset to the new dataset, and then delete the old dataset and its tables. So choose your dataset names carefully.</p>
<p>Let’s move the names_2019 table from the babynames dataset to the public dataset. Click on it, then select “Copy Table”. Now change the destination dataset to “public” and call the destination table “babynames_2019”. Click Copy. It takes a few seconds. Now click on the “babynames” dataset, and click “Delete dataset”. This will delete the dataset and all of the tables in it, so you have to type the name of the dataset before it will delete it. There, it’s done.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Streaming-Data"><a href="#Streaming-Data" class="headerlink" title="Streaming Data"></a>Streaming Data</h1><p>So far, we’ve run queries on public datasets and on pre-existing data that we loaded into BigQuery. But there’s another way to get data into BigQuery – streaming, where you add data one record at a time instead of a whole table at a time. One example is if you need a real-time dashboard that gives you an up-to-the-minute (or even up-to-the-second) view of your data.</p>
<p>One common way to stream data into BigQuery is to use Cloud Dataflow, but that’s the subject of another course. You can stream data directly into BigQuery by calling the API in your software. I’ll show you an example using the API Explorer, which lets you make an API call from a nice interface in your browser.</p>
<p>Before we do that, we need to create a table to stream the data into. We should also create a new dataset because we’re not going to put public data in the table, so the public dataset isn’t the right place to put it. Let’s call it “streaming”. Now create the table. This time, we’re going to leave it as “Empty table” because we’re not loading any data into it right now. Let’s call the table “stream”.</p>
<p>For the schema, we could put in pretty much anything, since we’re just going to stream some test data, but let’s have at least two fields. Create a string field called “greeting” and a string field called “target”. And click “Create table”.</p>
<p>Okay, now we’ll go to the BigQuery API documentation. The URL is in the GitHub repository for this course. We’re going to use the “tabledata.insertAll” method to stream data into the table.</p>
<p>On the right-hand side is a form you can fill out to call the API. First, put in your project ID (which will be different from mine, of course). You can get your project ID from BigQuery. Then put in “streaming” for the datasetId, and “stream” for the tableId.</p>
<p>Now, in the “Request body” text box, click the plus sign and select “rows”. This saves you from having to type in everything yourself. It’s especially helpful with the brackets because we’re going to have quite a few nested brackets in a minute. Now click the plus sign under “rows” and select “[Add Item]”. It put in a couple more brackets. Then click the plus sign between those and select “json”.</p>
<p>Now we finally need to type something. This is where we say what data we want to put in the row we’re adding to the table. If you’ll recall, the first field is called “greeting”, so type that between quotes after the curly bracket. Then type colon quote Hello quote comma. Hit Enter. We called the second field “target”, so type quote target quote colon quote world quote.</p>
<p>All right, that’s a complete record, so the request body is complete. It added a bunch of junk at the bottom for some reason, so just delete those lines. Good, the errors went away.</p>
<p>Now click the “Execute” button. Okay, the return code is 200 and it’s colored green, which means the API call was successful. It also didn’t return an error, so it looks like it worked. Let’s see.</p>
<p>Go back to the BigQuery page and click on the “stream” table. Click the “Preview” tab. Great, there’s our “Hello world” message that we just sent.</p>
<p>Although that was a quick way to show you how streaming works, you likely won’t be calling the API directly when you write your own streaming code. Instead, you should use the BigQuery client libraries. </p>
<p>Google provides client libraries for C#, Go, Java, Node.js, PHP, Python, and Ruby. There’s a different way of doing a streaming insert for each language, but it’s easier than writing the API call yourself. If you need to write code to stream data into BigQuery, have a look at the documentation for your language.</p>
<p>Oh, and one more thing. Remember in the pricing lesson when I said that there’s a separate charge for streaming? Well, it’s 5 cents per GB to do streaming inserts. That actually makes it the most expensive BigQuery operation. Loading data in any other way is free, querying data costs half a cent per gig, and storing data costs 2 cents per gig, at most. But if you need to do up-to-the-minute analysis of streaming data, then 5 cents a gig is still pretty cheap.</p>
<p>And that’s it for this lesson. </p>
<h1 id="Exporting-Data"><a href="#Exporting-Data" class="headerlink" title="Exporting Data"></a>Exporting Data</h1><p>Sometimes you need to export data from BigQuery, such as when you want to use third-party tools on the data. Exporting is pretty easy, but there is only one place you can export the data to and that’s Cloud Storage. So if you want to export data anywhere else, you have to export it to Cloud Storage first, and then download it from there.</p>
<p>Let’s do that with the babynames data. Click the table name, then select “Export to GCS” from the “Export” menu. It supports CSV, JSON, and Avro formats. Just for something different, let’s select JSON. Then you can choose whether to compress it or not. GZIP is the only compression option. This is a pretty small file, so let’s not bother compressing it.</p>
<p>Now you need to specify the Cloud Storage location where you want to save the file. I’ll put it in my ca-example bucket, but you’ll have to put it somewhere else, of course. Make sure you have write access to whatever bucket you specify. You also have to put in the filename. I’ll call it babynames_2019.json. Then click the Select button…and the Export button.</p>
<p>It tells you that it started an export job. It doesn’t take long to finish. Now if we go to the Cloud Storage bucket, we can see that the file was created. If you click on the filename, you can download it to your computer.</p>
<p>Now if you open it up, you’ll see the data in JSON format, which looks far different from the CSV file we originally uploaded into the table.</p>
<p>If you want to see a history of the exports you’ve done, click on Job History. If you click on one of the jobs in the list, it’ll give you more detail. Notice that it also lists other types of jobs, such as when we loaded the data originally. You can even re-run a load job from here if you want.</p>
<p>You can also use the bq command to run an export job, but the option is called “extract” rather than “export”. That is, you use the “bq extract” command.</p>
<p>As you’ve seen, exports are quite straightforward, but they get a little more complicated if you need to export more than a gig of data. Here, I’ll show you what happens. Take a look at the games_wide table in the baseball dataset. It’s 1.76 gig. I’ll try to export it, and I’ll even compress it, so hopefully the exported file will be less than one gig. </p>
<p>It gives us an error. It says that it’s “too large to be exported to a single file. Specify a uri including a * to shard export.” That second sentence is a little cryptic, isn’t it? What it means is you have to include a wildcard so it knows to export the data in multiple files.</p>
<p>Here’s how to do that. I’ll export it again, and this time, I’ll put an asterisk after “games”. You can put the wildcard anywhere in the path except for the bucket name, but putting it just before the first file extension is usually the best place to put it as you’ll see in a second.</p>
<p>It’s running this time, which is a good sign. It’ll take a lot longer, so I’ll fast forward. Now it’s done, so if I go back to Cloud Storage, you’ll see two new files with long numbers in them. BigQuery simply appends numbers starting from 0 and goes up by one for every file, but it puts in 12 digits with all zeros at the beginning, just in case it needs to split the data into a lot of files.</p>
<p>Also notice that the sum of those two files is nowhere near one gig. It’s less than a hundred meg. That’s because BigQuery looks at the size of the source data rather than estimating the size of the destination file when it decides whether you have to split it into multiple files or not.</p>
<p>Okay, if you don’t want to incur ongoing storage charges for the data you’ve saved in this course, then you should remove it. First, delete the files you exported to Cloud Storage. Select all three files, and then click “Delete”. Now go back to BigQuery, and delete the public dataset you created. You have to type “public” to confirm the deletion. Then delete the streaming dataset. There, now everything you loaded or exported in this course should be gone.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I hope you enjoyed learning how to use BigQuery. Now you know how to load data, run queries and save the results, stream data one record at a time, and export data. Let’s do a quick review of what you learned.</p>
<p>BigQuery’s advantages over on-premises databases are ease of implementation and speed.</p>
<p>If you want to import data that’s already in another Google service, then there’s usually a way to get it into BigQuery, although sometimes it requires an intermediate step. It’s also possible to query data in certain Google services without importing it into BigQuery. However, the performance is usually slower when you query an external data source than if the data resides in BigQuery storage.</p>
<p>If your data isn’t in a Google service, then you can upload it to BigQuery through the web interface, the command line, or the API. One limitation of the web interface is that it can only upload files that are 10 megabytes or less in size. The command-line tool for all BigQuery operations is “bq”.</p>
<p>When you’re uploading data, BigQuery includes an option to automatically detect its schema, but it doesn’t always work, so you may have to specify the schema manually.</p>
<p>Another way to get data into BigQuery is streaming, where you add data one record at a time instead of a whole table at a time. This is most useful for real-time applications. Although there’s no cost to upload data to BigQuery in bulk, it does cost money to stream data into BigQuery. To add streaming code to your applications, it’s easiest to use Google’s BigQuery client libraries, which are available for many different languages.</p>
<p>BigQuery stores data in tables, and each table must be part of a dataset. You can’t rename a dataset in BigQuery.</p>
<p>When you run a query, if you don’t specify a destination table, it puts the results in a temporary table. This temporary table stays in cache for about a day. So if you run the query again within 24 hours, it’ll retrieve the cached copy, and you won’t be charged for the query.</p>
<p>Cloud Storage is the only place where you can export data from BigQuery. If you need to export more than one gig of data, then you have to shard the data into multiple files by including an asterisk in the destination filename. You can also use an asterisk when you’re uploading files.</p>
<p>To learn more about BigQuery, you can read Google’s online documentation. You can also try one of the other BigQuery courses on Cloud Academy.</p>
<p>Please give this course a rating, and if you have any questions or comments, please let us know. Thanks!</p>
<h1 id="5Loading-Data"><a href="#5Loading-Data" class="headerlink" title="5Loading Data"></a>5<strong>Loading Data</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">Course GitHub repository</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.google.com/sdk/docs/install">Google Cloud SDK Installation Instructions</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/19/GCP-Cloud-Engineer-Run-SQL-Queries-and-Analyze-Data-with-Google-Cloud-SQL-23/" rel="prev" title="GCP-Cloud-Engineer-Run-SQL-Queries-and-Analyze-Data-with-Google-Cloud-SQL-23">
      <i class="fa fa-chevron-left"></i> GCP-Cloud-Engineer-Run-SQL-Queries-and-Analyze-Data-with-Google-Cloud-SQL-23
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/19/GCP-Cloud-Engineer-Structure-and-Analyze-Data-with-Google-BigQuery-25/" rel="next" title="GCP-Cloud-Engineer-Structure-and-Analyze-Data-with-Google-BigQuery-25">
      GCP-Cloud-Engineer-Structure-and-Analyze-Data-with-Google-BigQuery-25 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Running-a-Query"><span class="nav-number">2.</span> <span class="nav-text">Running a Query</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pricing"><span class="nav-number">3.</span> <span class="nav-text">Pricing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Saving-Query-Results"><span class="nav-number">4.</span> <span class="nav-text">Saving Query Results</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Loading-Data"><span class="nav-number">5.</span> <span class="nav-text">Loading Data</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Streaming-Data"><span class="nav-number">6.</span> <span class="nav-text">Streaming Data</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Exporting-Data"><span class="nav-number">7.</span> <span class="nav-text">Exporting Data</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary"><span class="nav-number">8.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5Loading-Data"><span class="nav-number">9.</span> <span class="nav-text">5Loading Data</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
