<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionCourse GitHub repository: [https:&#x2F;&#x2F;github.com&#x2F;cloudacademy&#x2F;k8s-admin](https:&#x2F;&#x2F;cloudacademy.com&#x2F;admin&#x2F;clouda&#x2F;videos&#x2F;video&#x2F;2613&#x2F;change&#x2F;Administering Kubernetes Clusters) Logan Rakai:    We">
<meta property="og:type" content="article">
<meta property="og:title" content="CKA-Administering-Kubernetes-Clusters-6">
<meta property="og:url" content="https://example.com/2022/11/19/CKA-Administering-Kubernetes-Clusters-6/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionCourse GitHub repository: [https:&#x2F;&#x2F;github.com&#x2F;cloudacademy&#x2F;k8s-admin](https:&#x2F;&#x2F;cloudacademy.com&#x2F;admin&#x2F;clouda&#x2F;videos&#x2F;video&#x2F;2613&#x2F;change&#x2F;Administering Kubernetes Clusters) Logan Rakai:    We">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T04:39:32.000Z">
<meta property="article:modified_time" content="2022-11-21T03:49:00.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/19/CKA-Administering-Kubernetes-Clusters-6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>CKA-Administering-Kubernetes-Clusters-6 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/CKA-Administering-Kubernetes-Clusters-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CKA-Administering-Kubernetes-Clusters-6
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:39:32" itemprop="dateCreated datePublished" datetime="2022-11-19T00:39:32-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 23:49:00" itemprop="dateModified" datetime="2022-11-20T23:49:00-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CKA-Certificate/" itemprop="url" rel="index"><span itemprop="name">CKA-Certificate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/CKA-Administering-Kubernetes-Clusters-6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/CKA-Administering-Kubernetes-Clusters-6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Course GitHub repository: [<a target="_blank" rel="noopener" href="https://github.com/cloudacademy/k8s-admin]">https://github.com/cloudacademy/k8s-admin]</a>(<a target="_blank" rel="noopener" href="https://cloudacademy.com/admin/clouda/videos/video/2613/change/Administering">https://cloudacademy.com/admin/clouda/videos/video/2613/change/Administering</a> Kubernetes Clusters)</p>
<p>Logan Rakai:    Welcome to administering Kubernetes clusters. This course will develop your skills and prepare you to administer Kubernetes clusters. What you will learn will be useful for your career as a Kubernetes practitioner. Also, if you plan on taking a Kubernetes certification exam, there are some tips included to help you succeed under the high-pressure, time-limited exam situations. </p>
<p>​          I’m Logan Rakai, and I’ll be your trainer for this course. I’m a content researcher and developer here at Cloud Academy. This course will help you succeed in administering Kubernetes clusters. I’ve organized the content in the course to compliment other content here on Cloud Academy. As we go through the course, at certain points, I’ll mention where you can go next in Cloud Academy to continue developing your skills. </p>
<p>​          I have over ten years of experience in software research and development, including six years in the Cloud. I’m a certified Kubernetes administrator and a certified Kubernetes applications developer. You can connect with me on LinkedIn or on Twitter.</p>
<p>​          This course is intended for anyone that is interested in Kubernetes cluster administration, but many parts of this course appeal to a broader audience of Kubernetes users. Some of the individuals that might benefit from taking this course include: system administrators, DevOps engineers, Cluster Administrators, and Kubernetes Certification examinees. </p>
<p>​          This course has lessons covering the following topics. The first lesson focuses on giving you some pro-tips on how to effectively use <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-tips/">kubectl</a>. What you learn here will be useful for administering a cluster and using Kubernetes in general. Then we’ll get into the details of controlling where pods are scheduled in Kubernetes clusters. You’ll be able to attract or repel pods from nodes or other pods. You can ensure pods run on nodes where they are intended to run and achieve other objectives such as high-availability by distributing pods across nodes. </p>
<p>​          Next, we’ll discuss resource management models and associated commands. This lesson helps you think about using Kubernetes for the long-term when you need to consider how you’ll manage and update resources. </p>
<p>​          After that we’ll cover several networking concepts in Kubernetes. You will learn how to control internal and external access to applications running in a Kubernetes cluster. Other topics important for Kubernetes admins, such as security and trouble shooting, are the subject of several labs here on Cloud Academy. Some topics really are learned best in the hands-on environments of our labs. I strongly encourage you to try the Kubernetes labs after taking this course. </p>
<p>​          To get the most from this course, you should know about core Kubernetes resources, including pods and deployments. You should also have experience using the kubectl command line tool for working with Kubernetes clusters. You will also benefit from understanding YAML and JSON file formats. You probably already have this skill if you have the prior two.</p>
<p>​          When you work with Kubernetes, it won’t take long under YAML files make an appearance. If you are not sure if you meet all these prerequisites, I’d highly recommend you go through the introduction to Kubernetes course and deploy a stateless application in a Kubernetes clusters lab here on Cloud Academy before taking this course. </p>
<p>​          You can follow along with the course examples, and I’d encourage you to. You can use whatever Kubernetes cluster you have access to, including single-node clusters. Some concepts need multi-node clusters, but you can get most of the benefits of following along with a single-node cluster. </p>
<p>​          I’ve put resources that I used for the demos on GitHub. A link is available in the transcript for this lesson. </p>
<p>​          I’m happy to hear from you. I make content for you and I want it to be as good as it can be. Please give the course a rating and send along your feedback when you’re finished. And with that, it’s time to start developing your Kubernetes administration skills. Start the next lesson when you’re ready. </p>
<h1 id="kubectl-Tips"><a href="#kubectl-Tips" class="headerlink" title="kubectl Tips"></a>kubectl Tips</h1><p><strong>Update</strong> - From kubectl version 1.18 the <code>kubectl run</code> command can no longer be used for creating deployments. <code>kubectl create deployment</code> or manifest files can be used as alternatives. Also the <code>--export</code> option of <code>kubectl get</code> is no longer supported resulting in functional, but more verbose output manifests.</p>
<p>JSONPath Support in Kubernetes: <a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/kubectl/jsonpath/">https://kubernetes.io/docs/reference/kubectl/jsonpath/</a></p>
<p>Speaker 1:     Welcome back. This is the first lesson where we’ll get hands on with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-clusters-intro/">administering a Kubernetes cluster</a>. This and the remaining lessons will focus on showing you first hand how to accomplish different tasks at the command line. If you spend any amount of time working with the Kubernetes cluster, you’ll probably be issuing a lot of kubectl commands. This lesson is intended to give you some tips to increase your efficiency when working with kubectl.</p>
<p>​          This lesson will demonstrate enabling auto completions for kubectl or kubecontrol to up your productivity at the command line, how to get the most out of kubecontrol’s get command, quick ways to generate resource manifest files with kubecontrol and how to use kubecontrol to give you information about resource specification fields in manifest files. </p>
<p>​          I’ll be using a Kubernetes cluster I’ve stood up on Linux notes running in AWS. The cluster is the same as you use in Cloud Academy Lab environments and is also very similar to clusters in Kubernetes certification exam environments. You can follow along using any type of cluster. However, single note clusters spun up using minikube or enabling Kubernetes in Docker for Mac or Docker for Windows distributions will work just fine for this lesson. Now let’s get started.</p>
<p>​          Any time you will be using more than a few commands with kubecontrol, you will probably enjoy having command completions enabled. It is not very difficult to do, but if you ever forget, kubecontrol can tell you how. All you really need to remember is that entering kubecontrol by itself lists all of the available commands, and completion is one of them. </p>
<p>​          To display the commands for enabling completions for different systems and shells, add the help option to the completion command. You can also use -h as a short form instead of spelling out help. </p>
<p>​          With kubecontrol, it is quite common to have examples in command help pages. It is tempting to hop over to your favorite search engine when you forget how to do something, but kubecontrol has a lot of answers as long as you know how to get at them.</p>
<p>​          I’m using Linux and the bash shell so this source command is what I need to enable completions for the current shell. To have completions enabled automatically every time a shell is created, add the command to your bash profile file. Now you can easily have commands auto completed by pressing tab or list the available commands and options by pressing tab twice if there isn’t a single completion for what you’ve entered. </p>
<p>​          If I enter kubectl followed by tab twice, the available commands for kubecontrol are displayed. If I type g followed by tab, the only command starting with a g, get is completed. Completions will save you a fair amount of time and prevent typos. This is always useful, but especially if you are taking one of the time limited Kubernetes certification exams. </p>
<p>​          The get command is your go-to command for displaying resources in Kubernetes. I’ll press tab twice to show the resources that can be shown with get. Let’s say we’re interested in nodes, I’ll nodes to display all the notes in the cluster. </p>
<p>​          With completions enabled, it is easy to enter the names of resources with some tab magic, but you can also make use of the short names for resources. To list the short names for resources, you can enter kubecontrol api resources. The first column lists the full name and the second lists any short name if there is one. So if you aren’t a fan of typing nodes, feel free to simply enter no. It’s more beneficial considering the length of some resources such as certificate signing requests, which can be compressed down to CSR, a whopping savings of 23 [ASCII 00:03:47] characters. </p>
<p>​          To get a look at all the pods in a cluster, I’ll use the all name spaces option of the get command. You can also use the short name PO for pods. Only pods in the Kube system name space are running because this is a fresh cluster. There are still quite a few pods running and it doesn’t take long before there can be significantly more. </p>
<p>​          Besides selecting a specific name space, you can also use labels to filter the output. But how do you know what the labels are, you ask. You can use the show labels option for that. An additional labels column is appended showing all the labels for each pod. If you’re only interested in a subset of the labels, you can use the -L option, followed by a comma separated list of label names. For example, if you’re only interested in the K8s app label, you can use the -L option, followed by K8s app and the K8s app column is added. Any resources with their value for the label have the value shown in the K8s app column. </p>
<p>​          If you’re only interested in seeing resources with the label defined, you can use the lower case l option. The lower case l is how you filter the output. If you only want the resources with a specific value of a label, you can specify the value after an equal sign. For example, to only show the kube proxy pods, you’d enter K8s app equals kube proxy.</p>
<p>​          Likewise, you can add an exclamation mark before the equal sign to show all resources not matching the label value. Here the pods that don’t have the K8s app label defined showed up again because not having the label defined is a match for not having a specific value of the label.</p>
<p>​          To hide the pods that don’t have the label defined, just add the label name after a comma. You can join as many label queries as you like by joining them with commas. </p>
<p>​          While we’re at it, the sort by option comes in handy for organizing the output of the get command. You can sort by the value of fields in the resources manifest. For example, if you want to sort by the age of the pods, you can sort by metadata.creationTimestamp. You can verify that the age column is now sorted. </p>
<p>​          The field that you give to sort by is specified using a JSON path. It can be read as sorting by the metadata objects creation timestamp field. Although that works, for more complicated JSON path expressions, it is a good idea to wrap the path in single quotes and braces to avoid shell substitutions and start the path with a period to represent a child of the root JSON object.</p>
<p>​          This is a better way to specify the JSON path. When writing JSON paths, the root object is actually represented with a dollar sign, but here the dollar sign can be omitted because the expressions are always children of the root object. </p>
<p>​          You might be wondering how did I come up with the metadata.creationTimestamp path to sort by age. You can use the output format option of get to list all the fields in a resource. The output formats for entire resources can be either JSON or YAML. YAML is more compact so I’ll be using that. </p>
<p>​          Here’s an example to output a pod in YAML output format. Notice the output&#x3D;yaml option. You can also use -O as a short form for output.</p>
<p>​          With the sort by option, you can sort by any numeric or string value field you find in the output. If you wanted to sort by the podIP address, which is treated as a string, you would give the path .status.podIP. Here’s how the get command would look sorting by podIP.</p>
<p>​          You can trust the sort is performed correctly, but how could you verify it? There is another output format that gives additional information dependent on the type of resource, the wide format. For pods, the wide format includes the podsIP. Here you can verify that the output is indeed sorted by the value in the IP column treating the values as strings. </p>
<p>​          There is one other output format I want to mention, although there are several more. You’ve actually used the type of the format before. It is the JSON path format. You can use a JSON path expression to describe what you want to output. </p>
<p>​          Let’s try to output the podIP using the same JSON path expression for the output format. All the output disappeared. There must be something wrong with that JSON path expression. To use JSON path output effectively, you need to understand when the get command is returning a specific resource or a list of resources. </p>
<p>​          If you specify the name of a resource, for example, the name of a pod, then get will return only that specific pod. In all other cases where you don’t specify a specific resource, a list will be returned. When a list is returned, the JSON area that contains all the resources is named items. In our case, no specific pod is identified so the items array needs to be included in the output format JSON path expression. Notice that you need to use square brackets to index the array. The asterisk is a wild card, meaning all of the items in the array. </p>
<p>​          The output is not as tidy as it is with the YAML or wide output format. To clean it up, you can use a more complex expression that iterates over the items in the array to also include the name of the pod and add new lines. The expression takes some time to understand and it is only included to show you that it is possible to include more than single fields in JSON path output.</p>
<p>​          For more information about JSON path expressions, see the link to JSON path support in Kubernetes in the transcript of this video. Those tips are really good for viewing what is already in the cluster.</p>
<p>​          Now it’s time to shift the focus to creating new resources in the cluster. The create command is your friend for that. The filename option or in short form -f allows you to create a resource or multiple resources from a manifest file or a directory containing manifest files. There are also several sub commands for creating different types of resources without having to use a manifest file. To see them, just view the create help page.</p>
<p>​          It’s usually better to use manifest files so that you can version control your configuration and practice configuration as code. So why did I mention these shortcuts? You can use sub commands to generate manifest files when paired with the dry run and output format options. A dry run will stop any resource from actually being created and if you set the output to YAML, the output is an equivalent manifest file for the create command you enter.</p>
<p>​          Let’s try it out with a namespace. Here I’ve used create to generate a manifest file for a namespace named tips. I’ll redirect that output to a file in a tips directory. The create command is going to create the resources in filename order. So to ensure the name space is created first, I’ve used the number 1 in the name to force the order. </p>
<p>​          The dry run option is available for other commands that create resources as well. Let’s say you wanted to create an engine X deployment. You can use the run command to generate a manifest file using the options you provide at the command line. Here, I’ve set the image to nginx, publish container port 80, set the number of replicas to 2 and expose the deployment with a service using the expose option. The service will use the default type of cluster IP and the service port will be the same as the container port. </p>
<p>​          If any of those are not what you want, it’s now very easy to edit the fleshed out manifest file to customize it as you like. We’ll discuss services more later in this course. </p>
<p>​          For now, let’s say we are happy with the defaults except we want to put the resources in the tips namespace. I’ll redirect the output to a file prefixed with 2 so that the resources are created after the namespace. Then I’ll add the name space to the metadata. </p>
<p>​          I’ll use VI for this which is an alias for Vim on my system. VI is also the default editor for kubecontrol edit, which we will see later on in this course. You can use whatever editor you’re comfortable with. If you are preparing for a Kubernetes certification exam, you should be comfortable with the command line editor to save time copying and pasting into the exam notepad area. To learn how to become an expert at Vim, I’d recommend entering vimtutor in a Mac or Linux cell to go through a series of lessons starting from scratch.</p>
<p>​          Now to set the resources namespace field. The resources will now be created in the tips namespace. To create all the resources, I’ll use kubecontrol -f and specify the tips directory. Other commands such as delete support the same pattern of specifying a directory. When you’re finished with those resources, you can simply use the same option with the delete command. Now all the resources specified in those manifests have been deleted.</p>
<p>​          One last technique for quickly creating manifest is to use the get command to modify manifests from existing resources. That might be an obvious technique, but there’s an option to help strip out any cluster specific that you don’t want to be present in a manifest file such as the pod status and creation time.</p>
<p>​          As an example, if I get the YAML output for a kube proxy pod and count the number of lines in the output, there are 133 lines of YAML whereas with the export option, there are 92. In this case, export automatically remove 41 lines of YAML for you. </p>
<p>​          Now to close out the lesson, I have one last tip. Earlier when I was explaining how to craft JSON path sort by expressions, I said you could use YAML or JSON output of the get command to show all the fields of resources. There is actually another way and it’s very useful. It can help you with customizing generated manifests as well. It not only gives you the field names of paths, but also tells you the purpose of each fields and other useful information. All of that goodness is bundled up in the kubecontrol explain command.</p>
<p>​          There are a couple of ways to use explain. They both require you to specify a simple path that is similar to a JSON path, but you give the kind of resource first without a [leading dot 00:14:36] and follow it with the field path that you are interested in. </p>
<p>​          For example, if you want to see the top level fields of a pod resource, you enter kubecontrol explain pod and the output gives you a description of a pod and the top level fields in a pod. If you wanted to dive into the details of a field that’s further down in the hierarchy, let’s say a pod specs contain a resources a field, you just join the fields with dots. In this case, pod.spec.containers.resources. You can traverse the fields up and down in this fashion to understand what fields are used for and if you want to see examples, you can navigate to the provided info links when available.</p>
<p>​          the other way to use explain is to provide the entire schema of a field or resource by using the recursive option. For example, to see all the fields in a pods container field along with their types, you can enter kubectl explain pod.spec.containers and specify the recursive option. </p>
<p>​          Explain can save you quite a few trips to your search engine of choice when writing resource manifests. During Kubernetes certification exams, you won’t be able to use search engines so it’s important to know how to get the most out of kubecontrol for exams as well.</p>
<p>​          All right, that brings us to the end of this lesson and I wanted to cover them first so that you know how to answer any questions you might have about how to quickly create manifest files or to remind yourself about the purpose of different fields. Taking it from the top, we saw how kubecontrol completions help write lengthy commands and prevent spelling mistakes. Remember that if you don’t know the exact syntax for enabling completions, the kubecontrol completion help page has you covered.</p>
<p>​          Then we saw how to use labels to filter and the sort by option to sort with the get command to effectively view resources in a Kubernetes cluster. We finished with some techniques for quickly generating manifest files by outputting YAML from dry run commands and exporting resources from the get command. The explain command can help you customize the manifest files by explaining the schema and the purpose of different fields.</p>
<p>​          In the next lesson, we will dive into the different tools you have available to you for controlling where <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-scheduling/">pods are scheduled in the cluster</a>. Continue on to the next lesson to learn more about pod scheduling.</p>
<h1 id="Scheduling-Pods"><a href="#Scheduling-Pods" class="headerlink" title="Scheduling Pods"></a>Scheduling Pods</h1><p><strong>Update</strong> - From kubectl version 1.18 the <code>kubectl run</code> command can no longer be used for creating deployments. <code>kubectl create deployment</code> or manifest files can be used as alternatives.</p>
<p>Configuring multiple schedulers in Kubernetes: <a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/">https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/</a></p>
<p>Speaker 1:     <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-clusters-intro/">Kubernetes clusters</a> can be made up of heterogeneous nodes where some nodes may have more resources than others. For example, some nodes may have blazing fast solid state drives while others may have the latest and greatest CPUs or GPUs to test. To make sure that your applications get the resources they need and meet your performance expectations, you need to control where the pods are scheduled in the cluster.</p>
<p>​          This lesson covers the different ways that you can control pod scheduling. This lesson starts by explaining DaemonSets and helps you decide when to use them. Next, how Taints and Tolerations are used to repel pods from nodes. Then the concept of NodeSelectors and Affinity are introduced for another way to control pod placement in the cluster. Lastly, we will briefly touch on a couple of special topics in the realm of pod scheduling. DaemonSets are a kind of resource in Kubernetes. They are similar to deployments in that they both create pods and are used for long running processes. DaemonSets are different in that they ensure that one part is running on each node in the cluster.</p>
<p>​          We will see later that you can control a DaemonSet to exclude a subset of nodes but you can usually think of a DaemonSet creating a part on each node. This is how the Kube Proxy Kubernetes cluster component is deployed to each node in the cluster. Kube proxy implements network rules for Kubernetes Services and each node needs to be aware of the rule so services are reachable from each node. That makes a DaemonSet the ideal choice for deploying Kube proxy. Another example is with cluster log agents. In this diagram, the Kubernetes API server is in the middle while the nodes are represented on the right. Say that you wanted to use Fluentd as a log aggregation framework to collect all the logs in the cluster, you would need one agent running on each node.</p>
<p>​          An easy way to accomplish this is by writing a DaemonSet manifest file for Fluentd pods. You use the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-tips/">kubectl</a> create command to request the API server to create the DaemonSet, and as a result each node gets one replica of the pod. When you need to schedule one copy of a pod on each node, use DaemonSets. The next scheduling topic we’ll discuss is Taints and Tolerations. Taints are similar to node labels but they influence the pod scheduling decisions. Taints repel pods from notes. Any pod that is scheduled onto a tainted node must have a toleration for the taint. That is enough to understand the concept, but we’ll see in a demo how to use Taints and Tolerations in practice. You use Taints and Tolerations together to ensure that pods are only scheduled onto appropriate nodes in a cluster.</p>
<p>​          One example that is automatically implemented in Kubernetes is detained masternodes to prevent pods you create from being scheduled on them. In this diagram, assume the top node is the master and it has a taint. If you create a pod without any tolerations, it will be repelled by the master and can only be scheduled on other nodes in the cluster. If you include a toleration for the taint, then the pod is eligible to be scheduled on the master. It does not require the pod to be scheduled on the master, only that it won’t be repelled away from the master. It could still be scheduled onto the other nodes in the cluster. Let’s take a look at these ideas in a demo. We’ll review the example we just covered in my demo Kubernetes cluster and then I’ll illustrate how to add taints and tolerations to influence scheduling of your own applications Kubernetes.</p>
<p>​          First, let’s describe the masternode, which is the node with the name ending with 100. Near the top of the upper, we can see a taint that is automatically applied to the master. Taints look similar to labels but they have an effect associated with them. Here the taint key is node-role.kubernetes.io&#x2F;master which is the same as a label on the node. Just like the label, there is no value defined for the key. You can assign values using an equal sign just like you would for labels. Using values can give you more granularity over scheduling. For example, you can tolerate specific values of a taint but not others. In the case of a masternode, it is really a binary decision, is it a master or not? So no value is defined. Then at the end after a colon, is the associated effect. In this case the effect is NoSchedule, which means do not schedule any new pods that don’t tolerate this taint.</p>
<p>​          Other effects are prefer NoSchedule which will allow scheduling to the node if there are no other nodes that can schedule the pod, and NoExecute which will not allow pods to be scheduled and will also evict any pods that had already been scheduled onto the nod. If we list the pods in the Kube System namespace with wide output, we can see several pods on the masternode. Let’s take a look at the CoreDNS pods as an example. The CoreDNS pods are created through a deployment, so I’ll look at the deployment template to see if there are any tolerations and there it is, a toleration for the master NoSchedule taint. That is how the pod is able to schedule onto the master. If we now look at a pod for kube proxy which is deployed by a DaemonSet, we can see that there are several tolerations that ensure the pod will be eligible to schedule on all nodes including when there are limited resources available.</p>
<p>​          That is what the memory and disk-pressure tolerations do. Kubernetes will automatically taint a node when it runs low on resources, and these tolerations allow DaemonSets to be scheduled despite that. DaemonSets are automatically created with these tolerations. Furthermore, DaemonSet pods are actually scheduled by the DaemonSet controller rather than the normal cluster scheduler by default. That allows the pods to circumvent some other conditions that could otherwise prevent the pods from being scheduled onto nodes. Now let’s try it out for ourselves. Currently, neither of the worker nodes have any taints, we will taint 102 worker node with a NoSchedule taint and then create a deployment without any tolerations and one with a toleration for the new taint.</p>
<p>​          To start, kubectl has a taint command for adding taints to nodes. The Help page includes some examples and a quick rundown if you ever forget how to use taints. Note that the taint command like many kubectl commands supports the -L or label selector option to apply taints to multiple nodes with matching labels. Let’s use the taint command to create a priority equals high key value pair and a NoSchedule effect. Now only pods that tolerate the high priority taint can be scheduled onto the node. This example could be used to reserve resources for high priority workloads. Now let’s create an agent next deployment in a scheduling namespace with five replicas and see where the pods land. I won’t use a manifest file this time but I will need one later to specify tolerations.</p>
<p>​          Check the output of get in the scheduling namespace to see where the pods are scheduled. Every one of them landed on the 101 node, the only node without any taints. Let’s delete the deployment and see what happens with the high priority toleration. First, let’s check the toleration explain output to see how tolerations are defined. Note that tolerations is an array, so you can have as many as you need. For our toleration, we need to set the key value and effect to match the taint. The operator field is used to control if the value is checked or not. The default is equals which will tell us that the toleration value matches the taint value. That’s what we want, since we want to match high priority, not any value of priority. There’s also a tolerationSeconds field for setting how long to tolerate a NoExecute taint, we won’t need that either.</p>
<p>​          I’ll create a manifest by using the run command. In the manifest, I’ll set the namespace and add in the toleration and template spec mapping which defines the PodSpec. Now let’s create the deployment, and let’s check where the pods got scheduled this time. This time we see some pods landing on the tainted 102 node thanks to the toleration. I’ll delete the deployment now and remove the taint from the 102 node. You remove a taint in a similar way to how you remove a label by specifying the key and the effect with the minus sign appended at the end. That’s all for this demo.</p>
<p>​          The next topic is the opposite of taints, rather than repelling pods, these concepts attract pods to nodes. The original method for attracting pods to nodes is by using a nodeSelector. A nodeSelector is a list of labels included in a PodSpec that must match in nodes labels for the pod to be scheduled. To visualize nodeSelector, imagine assigning a set of labels to one or more nodes, in this case only the middle node has the labels but you could label as many nodes as you want to be included in the target group of nodes. Now to control that a pod is scheduled onto one of the nodes with the set of labels in the PodSpecs nodeSelector list. When the pod is created, the scheduler will only schedule the pod onto nodes matching the labels in the pods nodeSelector list.</p>
<p>​          A new method of attracting pods to nodes and one that will eventually deprecate nodeSelectors is Node Affinity. Node Affinity is more expressive than nodeSelectors. Instead of only allowing exact matches of all labels in a set as nodeSelectors are limited to, node affinity can use a variety of operators. For example, you can require a node to have at least one label within a set using the In Operator or enforce that a node does not have any labels in a set using the NotIn Operator. The latter is actually a kind of anti-affinity and is similar to a taint in repelling pods from nodes. DoesNotExist also provides an anti-affinity. Node affinities can also express preferences rather than strict requirements. This allows you to prefer to schedule pods on a set of nodes but if they are not able to, the pods can be scheduled on other nodes.</p>
<p>​          Node Affinity will eventually deprecate nodeSelectors because node affinity can express everything that a nodeSelector can, and has additional flexibility. Let’s consider an example, the nodes have each been labeled with different zones and the top and bottom nodes are labeled as as having SSDs. You can write a PodSpec that requires the pod to be scheduled in the orange or red zone, and within those two zones prefer to schedule the pod on nodes with solid state drives. When the pod is created, the scheduler will first limit the eligible nodes to the orange and red zones, and then as long as the node in the red zone has capacity, the pod will be scheduled to the SSD node in the red zone. If the red zone node couldn’t accept the pod, it would be scheduled on the orange zone node. Now we will take a quick look at how you would create a deployment that specifies the affinity fields in the example we just considered.</p>
<p>​          For this demo I have written out the spec in advance. It is similar to the toleration deployment in the last demo but with a node affinity field added. Remember to use the Explain command if you can’t recall the lengthy field names involved with node affinity. Let’s go through the different fields. The node affinity field is in the affinity mapping. We’ll see later that there are other kinds of affinity besides node affinity. Under node affinity there are two fields required during scheduling, ignored during execution, and preferred during scheduling, ignored during execution. The lengthy names are fairly self-explanatory, the required conditions are put in the first one while the preferred conditions are placed in the second. There is a plan to have a required during scheduling, required during execution field but currently node affinity will not evict pods after they’re scheduled.</p>
<p>​          Under the required field there’s always a single field name named nodeSelector terms which is comprised of a list of terms. One or more of the terms must be satisfied, that is the terms are combined using a Logical OR. It’s important to note that not all the conditions need to be satisfied as long as one is satisfied, the requirements are met. The terms are defined using matchExpressions. The matchExpressions are where you express the conditions on node labels. Each expression must have a key and an operator while the values array is required and must not be empty for the In and NotIn Operators. Since we require the pods to be scheduled in the orange or red zones, the In operator is used and the accepted values for the zone key are orange or red.</p>
<p>​          If you use more than one matchExpression, each expression must be met. That is the expressions are combined using a Logical AND that is different from the nodeSelector terms which only require one of the array of matchExpressions to be satisfied. Switching our attention to the preferred node affinity conditions, we see that it is an array of preferences. Each preference has a weight that sets the relative importance of the preference. The scheduler computes a score for eligible nodes by adding up the weights and selects the node with the highest score to schedule the pod. Along with the weight, a preference field is given. The preference field consists of the same matchExpressions we saw before in the required conditions. In this case we prefer the node hardware to have an SSD.</p>
<p>​          With the spec node affinity set like this, the pod would be scheduled as we saw in the visualization. Some labels allow you to require pods to be in different groups of data centers but they can’t ensure that pods are evenly distributed across zones to help achieve high availability. That is where the other kind of affinity comes in. Pod affinity is similar to node affinity in that you can express requirements and preferences for the nodes to schedule pods on. They also support the same operators like In and DoesNotExist but there are several differences. First, the conditions are on pod labels, the conditions are evaluated using the labels of pods running on each node. This makes pod affinity more computationally expensive compared to node affinity. Because of this, it is not recommended to use pod affinity for large clusters with a few hundred nodes or more.</p>
<p>​          Also, because pods are namespaced, their labels are implicitly namespaced, so the conditions include namespaces for where they apply. To allow maximum flexibility, there are separate top level fields for pod affinity and pod anti-affinity. After the conditions are evaluated, a topology key is used to decide which node to schedule the pod to among the nodes that have pods with labels satisfying the specified conditions. The topology key usually corresponds to a physical domain such as a zone, server rack, or cloud region. This allows you to spread pods across zones or regions for high availability or ensure that pods are scheduled in the same zone or region for performance reasons. The host name label that is automatically added by Kubernetes is also useful to control scheduling pods out of granularity of individual nodes.</p>
<p>​          One thing to remember is that every node in the cluster must define the topology key label or unexpected behavior kind of care. Let’s go through an example of how Pod affinity works. The nodes in the clusters have labels for their zone either red or orange and another label for their hardware capabilities represented with white and gray. There are also two pods already scheduled on the nodes. One pod has a green label and another has a purple label. The pod labels could refer to different applications. For example, we require a pod to be scheduled in the same zone as a node running your green pod. For this condition we use pod affinity and set the topology key to zone. We also prefer that the pod is scheduled on a node with different hardware capabilities than nodes running purple pods. This could be to prefer to reserve those nodes resources for the purple pods as long as possible.</p>
<p>​          For this we can use pod anti-affinity and set the topology key hardware capability. Now when you create the pod, the scheduler evaluates the required conditions and finds a green pod in the red zone. So the new pod is eligible to be scheduled in the red zone. The scheduler then evaluates the preferences and finds a purple pod on white hardware, so the new pod will be preferred on nonwhite hardware in a red zone. The preferences and requirements can both be satisfied by scheduling the pod on the bottom node. If the bottom node wasn’t available to schedule the pod, the pod would be scheduled to the top node to satisfy the requirement, although, the preferences couldn’t be fulfilled. We won’t demo the use of pod affinity because the ammo is very similar to that of node affinity.</p>
<p>​          Just remember to use the Explain command if you need help remembering all of the fields involved. That brings us to the final grouping of special topics related to pod scheduling. We’ll explore container resource requirements, static pods, and custom scheduling. It will be easier to explain these at the command line, so let’s hop over to my Kubernetes cluster shell. Each container in a PodSpec can set resource requirements in terms of CPU and memory required. These requirements can be set as minimum and maximum values. Let’s explain the pod.spec.containers.resources field to see this. The limits field is used to set the maximum resources a container is allowed to use while the request set the minimum required resources for the container. The requests are what’s important for scheduling since the scheduler won’t schedule a pod unless a node has enough capacity to meet the resource requests.</p>
<p>​          It is definitely a good practice to set the requests for pods so the scheduler can make better decisions and avoid overloading nodes. Let’s take a look at the CoreDNS PodSpec to see how a resource map looks. For both limits and requests, you add CPU and memory key value pairs if you want to put a constraint on the resource. The CoreDNS pod requires a minimum of 70 mebibytes of memory and a maximum of 170. It only puts a minimum request on the CPU of 100 milly CPUs or one tenth of a CPU. The supported units are given in the explain output. The next special topic is static pods. Static pods are pods that are managed directly by the nodes kubelet and not through the Kubernetes’ API server. This means that static pods are not scheduled by the Kubernetes scheduler and instead are always scheduled onto the node by the kubelet.</p>
<p>​          This is how several pods are started on the masternode. The troubleshooting in Kubernetes lab here on Cloud Academy goes into more details about this. For now I’ll just show you that the kubelet has a pod manifest file option to point to static pod manifest files that will be created by the kubelet. This is usually reserved for running system pods to help bootstrap a cluster. For example, the default scheduler pod is initialized this way. Finally, I want to mention that if all these capabilities are on pod scheduling still cannot meet your requirements, then you can create your own scheduler. How to make one is outside of the scope of this course, but I’ve put a link in the documentation in case you are interested. You can deploy custom schedulers alongside the default scheduler in the kube system namespace and use two labels to inform Kubernetes that the deployed pod is a scheduler in the control planter.</p>
<p>​          Here you can see the labels are also used by the default scheduler. Once a custom scheduler is deployed, you can set the PodSpec scheduler name field to the name of the new scheduler to have the pod scheduled by the new scheduler. That brings us to the end of this lesson all about pod scheduling. We covered a lot of ground so let’s quickly recap what we discussed. DaemonSets can be used to schedule a pod onto each node in the cluster. Taints repel pods from nodes while tolerations allow a pod to be scheduled onto tainted nodes. NodeSelectors limit the nodes eligible to run a pod based on exact matches to a list of labels. Node affinity is a more expressive way to attract pods to nodes. Pod affinity allows scheduling decisions based on pods already scheduled to nodes.</p>
<p>​          We also covered a few Special Topics starting with container resource requirements. The request field is how the scheduler decides if a pod can be scheduled onto a node based on available CPU and memory. Static pods allow a pod to run as soon as the kubelet is brought up bypassing any scheduler. Lastly, know that you can use custom schedulers if the built-in scheduling capabilities don’t meet your needs. In the next lesson, we’ll look at the various ways that you can update resources running in a Kubernetes cluster. Continue on to the next lesson to learn all about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-updates/">resource updates</a>.</p>
<h1 id="Updating-Resources"><a href="#Updating-Resources" class="headerlink" title="Updating Resources"></a>Updating Resources</h1><p><strong>Update</strong> - From kubectl version 1.18 the <code>kubectl run</code> command can no longer be used for creating deployments. <code>kubectl create deployment</code> or manifest files can be used as alternatives.</p>
<p>How apply calculates differences and merges changes:</p>
<p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-apply-calculates-differences-and-merges-changes">https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-apply-calculates-differences-and-merges-changes</a></p>
<p>Speaker 1:     As soon as you make the decision to start using Kubernetes as part of your application infrastructure, you need to think about how you will manage resources. Until now, we haven’t really had to worry about managing and maintaining the resources, since we knew we’d delete the resources after illustrating a concept with an example. There is a lot more to consider when using Kubernetes in production, and it influences everything, right down to the commands you choose to update resources with KubeCTL. This lesson explains different models for managing and updating resources with KubeCTL and helps you decide which model to use.</p>
<p>​          This lesson will explain the three different resource management paradigms provided by KubeCTL and demonstrations will illustrate the concepts in practice. You will learn about the trade-offs between the different resource management paradigms and the KubeCTL commands that you can use to follow each paradigm. In the imperative resource management paradigm, you as the administrator issue commands describing what you want to happen in the cluster. The commands that you use are characterized by being specialized to specific Kubernetes resources for creating and updating.</p>
<p>​          In KubeCTL, the imperative commands are named after verbs, to allow the user to express what they want without necessarily having to know the resources involved. For example, you don’t need to know anything about deployments to understand that the run command will run an application. </p>
<p>​          Now let’s think about some advantages and disadvantages of the imperative paradigm. It is the easiest way to get started with Kubernetes. You don’t have to know anything about kinds of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-clusters-intro/">Kubernetes resources</a> or Kubernetes APIs. Some down sides of the imperative model are that there is no embedded history of the changes being made in the cluster. This creates issues with team and doesn’t provide any easy escape mechanism when a change introduces a problem. Another drawback is that there are quite a few different verb-driven commands, and commands you issue can end up being quite lengthy and error-prone. </p>
<p>​          In addition, although there are many specialized commands and command line options, they still cannot express all the capabilities of resources, so eventually you need to use more generic commands and learn about Kubernetes resources. It’s best to use the imperative model when you need to try something out quickly, and when you’re first starting. It is not something that you should use in production.</p>
<p>​          The second resource management paradigm is imperative with configuration files. The configuration files are manifest files and are usually in YAML format, but can also be written in JSON. Instead of having many specialized commands, like with the imperative paradigm, there are only a few when you use imperative with config files. They are create, replace, and delete. Create is also used with the imperative paradigm, but what makes it different here is that all commands specify file with the file name option that can be shortened to dash F. </p>
<p>​          The advantages of using the imperative model with configuration files are that you can leverage version control systems to track the history of changes and practice configuration as code. This is a big reason for preferring to use manifest files. There are also fewer commands to use, and remember, on the disadvantage side, you need to understand resource schemas. You also need to perform the extra step of making files, although the benefits of version control certainly outweigh this drawback. A more sinister drawback is that some resources are partially managed by Kubernetes. For example, load balancer services that have external IPs are managed automatically by the cluster.</p>
<p>​          If you were to update a resource that is partially managed by the cluster, you run the risk of losing the configuration added by Kubernetes. To avoid this, you have to copy the added configuration into your manifest file before running the update. This same problem can arise if someone used an imperative command to update a resource. The next time you update with the config file, if you don’t copy the configuration change to the manifest file, the previous update would be lost. The update mechanism treats the configuration file as the single source of truth and ignores the configuration changes made outside of the configuration file.</p>
<p>​          Lastly, the commands are not well suited for directories and work best with individual files. If you were to add a new file to a directory and use create to create the resource, you’d see errors for all of the resources that were already created in that directory. It works best with single files for each tightly coupled group of resources. The imperative with configuration files paradigm can be used in production and works best with a small team and a small number of files for each application.</p>
<p>​          The final resource management paradigm is declarative with configuration files. As the name suggests, manifest files are used to describe resource changes. Where it differs from the imperative with configuration files paradigm is that you don’t specify the operation to take. This is the essential feature of declarative paradigms. Instead of specifying to update a given resource, KubeCTL determines what operation is required for each resource by analyzing the differences between the live resource configuration and the contents of the configuration file. The operations to perform are determined at the resource level. This means one command can result in creating, updating, and deleting resources in a file.</p>
<p>​          Because configuration files are used, you gain the benefits of version control. Another advantage of the declarative model is that you can do everything with a single command. You don’t need separate commands for each operation. By analyzing the differences between live configurations and configuration files, the declarative model will preserve changes that aren’t captured by configuration files. The declarative model works well with directories since each operation is determined at the resource level. Live resources that match their configuration file do not require any operation at all. </p>
<p>​          The drawbacks of the declarative with file configuration file paradigm are that it requires understanding Kubernetes resources and APIs. That was also true for the imperative with configuration files paradigm, but the declarative model is the most difficult to understand of the three. Most of the time you can trust that it is doing the right thing, but when an issue does occur, it can be difficult to understand what happened. The merge operations that happen behind the scenes are not as simple as always replacing entire resources. The declarative with configuration file paradigm requires the most knowledge of Kubernetes resources and APIs, but may be the best choice in production when directories of files are required.</p>
<p>​          It also avoids the risk of configuration loss that was possible with the imperative with configuration file paradigm. It’s worth noting that you can change from one management model to another, and we will see that in the demo. Whatever model you are using, it’s important to not carelessly mix commands intended for the other models, or you can have subtle issues creep into your cluster. The following demos will illustrate each of the different models with a deployment resource and show how to migrate from one model to another.</p>
<p>​          We’ll start with the imperative paradigm. Some examples of imperative commands for creating resources are run to create deployments or jobs, and expose to create services. In the imperative paradigm, you only need to know that run will run an application, not that run creates a Kubernetes deployment resource. Some examples of imperative commands to perform updates are scale, to increase the number of replicas, and label. </p>
<p>​          Let’s run nginx in the default name space. In other words, create an nginx deployment using an imperative command. But before I hit enter, I’ll list all the available completions to show how many different options there are, potentially creating some very messy commands. However, the minimum that you need to know to use run is the image you want to create a container from. You don’t need to know anything about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-scheduling/">pods</a> or deployments. But there aren’t enough specific verb commands to express everything you might want to do without understanding a bit about Kubernetes resources. </p>
<p>​          The next level up is the create command, followed by the type of resource to create. Note that not all resources can be created this way. In a similar manner, there aren’t single verbs for all the possible changes you would want to have in a cluster, so there is the set command, which includes subcommands for common updates you might perform. Let’s use set to request a quarter of a CPU for the nginx containers. In the command, you do need to know that it is a deployment. It doesn’t take long before you have to understand some basic resources when using the imperative model. </p>
<p>​          if the set command doesn’t provide what you need, you need to resort to using the patch or edit commands. Both require knowledge of resource specifications. Patch is the more advanced of the two, allowing you to merge patches into live resources. A patch is just a partial spec that specifies changes you want. Patches can be written in YAML or JSON. There are some complexities around patches though, such as when you patch an array, do you want to append elements to the array or replace the entire contents of the array. This gives rise to the different kinds of patches that can be performed. </p>
<p>​          The edit command is more straightforward. Let’s see how edit works. Edit opens an editor which defaults to VI in Linux. You can make changes to the spec of the live resource. If you make any changes, it will apply them once you’ve saved the file. Let’s set the memory request to 100 mebibytes, and upon saving and quitting, the deployment is updated. Edit is very close to using imperative with configuration files, but without the benefits of version controlling the configuration files. The imperative commands may be most useful for generating manifest files, as we have seen in an earlier lesson. They may also allow you to accomplish some updates faster if you are in a timed Kubernetes certification exam environment.</p>
<p>​          To migrate to an imperative with configuration file paradigm, you use the get command with your preferred output format, either YAML or JSON, and the export option. Now you update the resource by modifying the file. Let’s change the image pull policy to always, because later we will change the image’s tag to latest. In using the replace command with the dash F option, we can update the deployment following the imperative with configuration files paradigm. Recall that one of the drawbacks of this model is the potential for configuration loss. I’ll quickly demonstrate this.</p>
<p>​          First I’ll update the image for the deployment to nginx latest using the set command. And let’s verify the deployment’s images, nginx latest. That confirms it. Now if we use replace with the existing configuration file and check the image, we can see that the replace overwrote the work of the set command, and would happily overwrite any other changes not reflected in the configuration file without a warning. This also illustrates why you shouldn’t mix resource management models, but remember that sometimes the changes in the configuration are implemented by Kubernetes itself and the issue can’t entirely be removed by strictly adhering to the resource management model. If you want to gracefully handle differences between the live configuration and the configuration file, you need to use the declarative model.</p>
<p>​          To migrate from the imperative with configuration file model to the declarative model, you can use the save config option of create, replace, or update. We will demonstrate what the save config option does. Let’s take a look at the annotations of the deployment before we use save config. Currently, there is only a revision annotation that corresponds to the revision number for the current deployment roll out. Remember that annotations are similar to labels, in that they store meta data about a resource. But annotations can’t be used to filter resources like labels can.</p>
<p>​          Now we’ll use the replace command with the save config option to prepare to manage the resource using the declarative model. Now if we look at the annotations, we can see a new last applied configuration annotation. For now, you have to take my word that the annotation is actually a JSON representation of the YAML configuration file. This is the key to the declarative model, being able to compare the last applied configuration file, the live resource configuration, and a new configuration. The apply command of KubeCTL is the command that is used when practicing the declarative model. Apply is able to perform a three-way diff of the last configuration, the live configuration, and a new configuration file to perform the desired updates but not overwrite updates that aren’t specified in the configuration file. The apply command has subcommands for working with the last applied configuration notation. We can use the view last applied subcommand to get a YAML version of the annotation, and that’ll redirect the output to a file named lastapplied.YAML. Now I’ll use the diff command to compare the last applied configuration with our deployment.YAML file, to confirm that the annotation is simply the configuration file. </p>
<p>​          If there were any differences, the diff command would have displayed some output. To apply a change, you only need to edit the file and pass it to apply. Let’s change the image tag to latest and pass it to apply using the dash F option. Apply will perform its three-way diff analysis to decide what needs to change. Let’s try another example. I will remove the number of replicas from the configuration file. Now the configuration file is not controlling the number of replicas. Then I’ll use the scale command to increase the number of replicas to two, and confirm the deployment now has two replicas. </p>
<p>​          At this point, let’s note that the scale command does not impact the last applied configuration. The last applied configuration still has replicas at one, which was the case the last time apply was executed. If we execute apply again and check the number of replicas on the deployment, we see that it reverted to one. That is because the applied diff comparison decides that when a primitive field, like the energy replica field, is in a last applied configuration and not in a new configuration file, it sets the field to its default value. This is because it thinks that removing the field from the file means you want to clear the value that was set in the previous configuration. This may not be what’s intended. In our case, we wanted to set the replicas to two. This illustrates the kind of issue that can happen when you mix management models. The correct way to remove replicas from the configuration file would have been to remove it from the file, then apply the changes, then use the scale command, since the last applied configuration would no longer have any value set for the replicas.</p>
<p>​          You can also use the edit last applied command to remove the field from the annotation. Any time you remove a field from your configuration file, you should carefully consider the impact. You can also try to remove fields that you want to allow to be managed outside of the configuration file when you first create a resource.</p>
<p>​          I have added a link to this video’s transcript that explains the complete logic behind the three-way diff decision making process. Now let’s clean everything up. To do that, the apply command provides the prune option for deleting resources in a declarative way. It detects resources to delete by finding resources that have a last applied configuration annotation, but that no longer have a corresponding manifest file. This is a risky maneuver. If you have a configuration directory with several subdirectories and someone accidentally calls prune on a subdirectory, it can end up wiping out everything outside of that subdirectory. Because of this, it’s usually best to use the delete command. The delete command is more explicit, and the preferred way to delete resources for all of the resource management paradigms.</p>
<p>​          Let’s quickly recap what we covered in this lesson. The imperative management paradigm is best suited for getting started with Kubernetes and in low-risk development environments when you are quickly trying things out. The imperative with configuration files paradigm gains the benefit of version control, including collaborating with a team. You can use this model in production. The declarative with configuration files paradigm can also be used in production, and is better suited for larger projects with many resources. The analyze command will decide what operations to perform based on a three-way diff analysis between the last applied configuration, the live resource configuration, and the new configuration file. It works well, and can avoid the configuration loss issue if you carefully decide which fields to include in the configuration file. Remember to stick with whatever model you choose, or be very deliberate when mixing models to avoid the pitfalls.</p>
<p>​          This lesson helps to clarify how to think about managing resources in Kubernetes. There are many KubeCTL commands for creating and updating resources. Armed with the knowledge of different resource management paradigms, you now understand when to use each. In the next lesson, we’ll look at a few topics related to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-clusters-networking/">networking</a> in Kubernetes. When you’re ready, start the next lesson to learn more about Kubernetes networking.</p>
<h1 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h1><p>NGINX ingress controller for Kuberetes: <a target="_blank" rel="noopener" href="https://github.com/kubernetes/ingress-nginx">https://github.com/kubernetes/ingress-nginx</a></p>
<p>Speaker 1:     Kubernetes has several concepts relevant to networking. As a <a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/securing-kubernetes-clusters/">cluster admin</a>, you need to know about the concepts and how you can use them to securely provide access to applications running in a cluster.</p>
<p>​          This lesson will begin by reviewing the basic networking models employed by Kubernetes. Then we will discuss more about services. The networking basics and services topics are also covered in other content here on Cloud Academy, so we will only review the key concepts of each.</p>
<p>​          Lastly we’ll discuss Kubernetes Ingress resources. There is another concept important to Kubernetes network security called Network Policies. But we won’t talk about it here because that topic is covered well in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/securing-kubernetes-clusters/">Securing Kubernetes Clusters</a> Lab here on Cloud Academy.</p>
<p>​          The basic building block in Kubernetes is a pod. The network model for a pod is IP per pod, meaning each pod is assigned one unique IP in the cluster. Containers in the pod share the same IP address and can communicate with each other’s ports using local host. A pod is of course scheduled onto a node in the cluster. And you know it can reach the pod by using its pod IP address. Other pods in the cluster can also reach the pod using the pod’s IP address. This is thanks to whatever Kubernetes networking plug in each use. </p>
<p>​          The network plug-in implements the container network interface standard and enables pod-to-pod communication. But pods should be seen as a femoral that can be killed and restarted with a different IP. You may also have multiple replicas of a pod running. You can’t rely on a single pod IP to get the benefits of replication. This is where services come in. </p>
<p>​          The service maintains a logical set of pod replicas. Usually the set of pods are identified with labels. The diagram only includes one replica, but there could be many spread over many nodes. The service maintains a list of endpoints as pods are added and removed from the set. The service can send requests to any of the pods in the set. </p>
<p>​          Clients of the service now only need to know about the service rather than specific pods. Pods can discover services using environment variables as long as the pod was created after the service, or by using the DNS add-on in a cluster. The DNS can result in the service name or the name space qualified service name to the IP associated with the service.</p>
<p>​          The IP given to a service is called the cluster IP. Cluster IP is the most basic type of service. The cluster IP is only reachable from within the cluster. The cube proxy cluster component that runs on each node is responsible for proxying request for the service to one of the services endpoints. </p>
<p>​          The other types of services allow clients outside of the cluster to connect to the service. The first of those types of services is node port. Node port causes a given port to be opened on every node in the cluster. The cluster IP is still given to the service. Any requests to the node port of any node are routed to the cluster IP. The next type of service that allows external access is load balancer. The load balancer type exposes the service externally through a cloud provider’s load balancer.</p>
<p>​          A load balancer type also creates a cluster IP and a node port for the service. Request to the load balancer are sent to the node port and routed to the cluster IP. Different features of cloud provider load balancers such as connection draining and health checks are configured using annotations on the load balancer. </p>
<p>​          The final type of service is external name and it is different in that it is enabled by DNS, not proxying. You configure an external name service with a DNS name and request for the service return a CNAME record with the external DNS name. This can be used for services running outside of Kubernetes, such as a database as a service offering.</p>
<p>​          Services operate at layer 4 in the OSI network stack. That is the transport level of TCP and UDP. Kubernetes also provides a layer 7 service abstraction called ingresses. Layer 7 is the application layer, which is where HTTP exist. You need to have an ingress controller running in your cluster to use any ingress resources.</p>
<p>​          Ingress controllers are different from most controllers that are automatically run as part of the cube controller manager binary. Instead ingress controllers are run as normal pods in the cluster. You can choose from a variety of ingress controllers. I have put a link to one based on Nginx in the transcripts of this video. Installation steps vary depending on how your cluster is deployed. </p>
<p>​          Once you have an ingress controller in place, you can use an ingress to specify rules for connecting inbound connections to Kubernetes services. Ingresses support SSL termination, load balancing and path-based routing. We will see an example of this in a demo. We’ll see how to define an ingress to route incoming traffic to two different services based on the HTTP request path. We won’t go through the process of installing an ingress controller because that depends on the specifics of the ingress controller you choose, and where your cluster runs.</p>
<p>​          I have the manifest for an ingress that will route requests to the k8s-perspectives.com host name. To separate news and blogs services based on the request path, let’s go through the relevant fields. The first is an annotation that is ingress-controller specific. I’m using the ingress controlled named ingress Nginx, which is the one that I have linked to in the transcript.</p>
<p>​          Without getting into much detail, every time an ingress resource is created, updated or deleted, a new configuration for Nginx is generated. This particular ingress will match paths that we’ll see later and rewrite them to slash when sending requests to different services. With that ingress controller specific detail out of the way, we can focus on the controller agnostic spec. </p>
<p>​          The spec consists of a list of rules. This example has only one rule. Each rule can specify a host, but it’s not required. When a host is set, the rule will only apply when the request matches the host. If no host is set, the ingress applies regardless of the host value in the request. If you include multiple rules, setting separate values of the host field enables name-based virtual hosting allowing multiple host names for the same IP.</p>
<p>​          The other field is HTTP and it’s required for each rule. Under that is the paths field, which consists of a list of paths. Each path must specify a back end which is a service name and port. If a path field is given, the incoming request must match the path for the rule to apply. Otherwise any path will match and be directed to the back end.</p>
<p>​          In our example, the k8s-perspective web app has separate news and blog services. The URL path of the request is used to direct requests to the appropriate underlying Kubernetes service. Ingresses make it easy to accomplish this scenario. </p>
<p>​          That’s all for the lesson on Kubernetes networking. We began with a review of basic networking principles. Then we paid some extra attention to the different types of services available on Kubernetes. Lastly we discussed ingresses and how they can be used to manage external access at the HTTP layer. </p>
<p>​          We’ll wrap up the course in the next <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-clusters-summary/">lesson</a>. Continue on when you’re ready.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>JSONPath Support in Kubernetes: <a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/kubectl/jsonpath/">https://kubernetes.io/docs/reference/kubectl/jsonpath/</a></p>
<p>Logan Rakai:    Congratulations. You’ve reached the end of this course on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/administering-kubernetes-clusters/administering-kubernetes-clusters-intro/">administering Kubernetes clusters</a>. Let’s review what you’ve learned.</p>
<p>​          We began this course with some tips on how to be productive with kubectl. We saw how to use the completions command to enable shell completions for kubectl, how to use get to filter output using labels and how to format output. How to use kubectl to generate manifest files for a variety of resources, and how to use kubectl to understand the fields of resources. </p>
<p>​          We then became acquainted with the different methods for controlling where pods are scheduled in a cluster, starting with DaemonSets. Remember, Daemonsets can generally be thought of as placing one pod on each node in the cluster. Cube proxy is deployed as a DaemonSet and DaemonSets are also useful for logging agents. We saw how it taints on nodes or [pell pods 00:00:49] unless pods have a toleration to counteract the taint. Then we illustrated how note selector and note affinity can attract pods to nodes.</p>
<p>​          Pod affinity and anti-affinity can be used to retract or repel pods to nodes using the labels of pods already running on nodes. This can be used to spread pods for high availability or keep pods close for performance reasons. We finished up with some special topics, including container resource requests, static pods, and custom schedulers. </p>
<p>​          Next, we described the three frameworks for managing and updating resources. They are imperative, imperative with configuration files, and declarative with configuration files. We demonstrated each, along with potential pitfalls, and how to migrate from one to another. Remember to not carelessly mix commands that are intended for resource management models other than the one you are committed to. </p>
<p>​          The previous lesson covered networking topics, including a review of basic networking principles in Kubernetes such as IP per pod, container local host communication, and services to avoid the pitfalls of working directly with pod IPs. </p>
<p>​          We then dove into more details on the different types of services that are available. They are cluster IP for internal only access, node port to open a port on each node for access to a service, load balancer to leverage a cloud provider’s load balancer to grant external access to a Kubernetes service, and external name to access services outside of the cluster using DNS [see 00:02:15] name records. </p>
<p>​          Then we shifted our attention to ingress resources. Ingresses operate in the application layer of the OSI network stack over HDTP, compared to services that operate in the transport layer over TCP or UDP. Remember that you need to have an ingress controller in the cluster for ingress resources to have any effect. We saw how an ingress can be used to perform path based routing to different services, and they can also be used for SSL termination, load balancing and name based virtual hosting. </p>
<p>​          We certainly covered a lot of interesting topics for Kubernetes administrators, but the fun doesn’t end here. I’d encourage you to try out more Kubernetes content on CloudAcademy. There are several labs that give you hands-on experience with Kubernetes clusters. The <a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/securing-kubernetes-clusters/">Securing Kubernetes Clusters</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/troubleshooting-kubernetes/">Troubleshooting in Kubernetes</a>, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-manage-kubernetes-cluster-scratch/">Create and Manage a Kubernetes Cluster from Scratch</a> labs all are relevant to Kubernetes administration. The labs are a great place to practice and learn at the same time. But, keep on going after you’ve completed them. Try things out on your own and try to solve some challenges you can think of. Think back to the pro tips lesson to be as efficient and self-sufficient as possible. </p>
<p>​          Lastly, please share your feedback so I can find out what you want to see more of, and what you’d rather see less of. I make content for you and try to make it the best that it can be for you. Thanks for taking my course. Now go on and continue down your path to becoming a Kubernetes ninja. Until next time, I’m Logan Rakai with CloudAcademy.</p>
<h1 id="1Introduction"><a href="#1Introduction" class="headerlink" title="1Introduction"></a>1<strong>Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/k8s-admin">Course GitHub repository</a></p>
<h1 id="2kubectl-Tips"><a href="#2kubectl-Tips" class="headerlink" title="2kubectl Tips"></a>2<strong>kubectl Tips</strong></h1><p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/kubectl/jsonpath/">JSONPath Support in Kubernetes</a></p>
<h1 id="3Scheduling-Pods"><a href="#3Scheduling-Pods" class="headerlink" title="3Scheduling Pods"></a>3<strong>Scheduling Pods</strong></h1><p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/">Configuring multiple schedulers in Kubernetes</a></p>
<h1 id="4Updating-Resources"><a href="#4Updating-Resources" class="headerlink" title="4Updating Resources"></a>4<strong>Updating Resources</strong></h1><p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/overview/object-management-kubectl/declarative-config/#how-apply-calculates-differences-and-merges-changes">How apply calculates differences and merges changes</a></p>
<h1 id="5Networking"><a href="#5Networking" class="headerlink" title="5Networking"></a>5<strong>Networking</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/kubernetes/ingress-nginx">NGINX ingress controller for Kuberetes</a></p>
<h1 id="6Summary"><a href="#6Summary" class="headerlink" title="6Summary"></a>6<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/kubectl/jsonpath/">JSONPath Support in Kubernetes</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/19/CKA-Knowledge-Check-Kubernetes-Concepts-5/" rel="prev" title="CKA-Knowledge-Check-Kubernetes-Concepts-5">
      <i class="fa fa-chevron-left"></i> CKA-Knowledge-Check-Kubernetes-Concepts-5
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/19/CKA-Troubleshooting-Kubernetes-Cluster-Access-Issues-7/" rel="next" title="CKA-Troubleshooting-Kubernetes-Cluster-Access-Issues-7">
      CKA-Troubleshooting-Kubernetes-Cluster-Access-Issues-7 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kubectl-Tips"><span class="nav-number">2.</span> <span class="nav-text">kubectl Tips</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Scheduling-Pods"><span class="nav-number">3.</span> <span class="nav-text">Scheduling Pods</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Updating-Resources"><span class="nav-number">4.</span> <span class="nav-text">Updating Resources</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Networking"><span class="nav-number">5.</span> <span class="nav-text">Networking</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary"><span class="nav-number">6.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1Introduction"><span class="nav-number">7.</span> <span class="nav-text">1Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2kubectl-Tips"><span class="nav-number">8.</span> <span class="nav-text">2kubectl Tips</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3Scheduling-Pods"><span class="nav-number">9.</span> <span class="nav-text">3Scheduling Pods</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4Updating-Resources"><span class="nav-number">10.</span> <span class="nav-text">4Updating Resources</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5Networking"><span class="nav-number">11.</span> <span class="nav-text">5Networking</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6Summary"><span class="nav-number">12.</span> <span class="nav-text">6Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
