<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionWelcome to the introduction to Kubernetes Course. Kubernetes is a production grade container orchestration system that helps you maximize the benefits of using containers. Kubernetes pro">
<meta property="og:type" content="article">
<meta property="og:title" content="CKAD-Introduction-to-Kubernetes-2">
<meta property="og:url" content="https://example.com/2022/11/19/CKAD-Introduction-to-Kubernetes-2/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionWelcome to the introduction to Kubernetes Course. Kubernetes is a production grade container orchestration system that helps you maximize the benefits of using containers. Kubernetes pro">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T04:31:25.000Z">
<meta property="article:modified_time" content="2022-11-21T04:23:52.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/19/CKAD-Introduction-to-Kubernetes-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>CKAD-Introduction-to-Kubernetes-2 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/CKAD-Introduction-to-Kubernetes-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CKAD-Introduction-to-Kubernetes-2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:31:25" itemprop="dateCreated datePublished" datetime="2022-11-19T00:31:25-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 00:23:52" itemprop="dateModified" datetime="2022-11-21T00:23:52-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CKAD-Certification/" itemprop="url" rel="index"><span itemprop="name">CKAD-Certification</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/CKAD-Introduction-to-Kubernetes-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/CKAD-Introduction-to-Kubernetes-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to the introduction to Kubernetes Course. Kubernetes is a production grade container orchestration system that helps you maximize the benefits of using containers. Kubernetes provides you with the toolbox to automate the deployment, scaling, and operation of containerized applications in production.</p>
<p>In this course, we’ll teach you all about Kubernetes, including what it is and how to use it. Before we get into it, allow me to introduce myself. I’m JT Lewey and I’ll be your trainer for this course. I’m a content researcher and developer here at Cloud Academy. And I hold both the certified Kubernetes application developer certification, as well as the certified Kubernetes administrator certification. So feel free to reach out to me about either of those topics or other general DevOps questions you have.</p>
<p>Let’s talk about who should attend this course. This course is suitable for those who are looking to deploy containerized applications. This is especially useful if you already have an existing system and are evaluating deployment options. Container orchestration skills are relevant for the following positions, DevOps engineers, cloud engineers, site reliability engineers, and as well as just anybody who is a container enthusiast and looking to beef up their container orchestration skills.</p>
<p>In this course, we’re gonna be covering three main topics. And our first is gonna be an overview of Kubernetes, specifically addressing, what is it? Why is it so successful? And how can you start? From there, we’re gonna be deploying containerized applications into Kubernetes. And this involves a hands on approach. Specifically with microservices. I’ve created a <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/intro-to-k8s">GitHub repository</a> that has all the available files to you should you wish to follow along. I also suggest that you spin up our intro to Kubernetes lab where it has all the available GitHub repo files as well as a full Kubernetes environment.</p>
<p>The last topic we’re gonna be discussing, Personal Topics, I recommend you be introduced to should you wish to know more about Kubernetes. Let’s identify our concrete learning objectives. You will know these after you complete this course. Specifically, you’re gonna be able to describe Kubernetes and what it is used for. You’re going to be able to deploy single and multi container applications onto Kubernetes. You will be able to use Kubernetes services to structure any number of applications and you’ll be able to manage those applications through deployments and rollouts. You’ll also be able to ensure container pre-conditions are met and that these containers are kept healthy. You’ll be able to manage configuration maps, secrets, and how to control persistent data within Kubernetes. And lastly, you’re gonna be able to discuss the popular tools and how they can benefit you in your Kubernetes journey.</p>
<p>There are some prerequisites to this course, and to get the most out of it, you should have a solid understanding of Docker. But don’t worry. We have courses available if you’re interested in learning Docker and would like to take those before this. Next, you should have a solid understanding of YAML. But don’t worry, as it’s fairly easy to pick it up as we go. And lastly, we’re gonna be establishing a Kubernetes cluster. So any Kubernetes experience will obviously help you in learning more about the basics and more about how to configure Kubernetes.</p>
<p>My name’s Jonathan Lewey from Cloud Academy. And I’m so excited for you to take this introduction course. If you have any questions or concerns, please feel free to reach out to me on LinkedIn or <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. So enough’s enough. Let’s get into it.</p>
<h1 id="Kubernetes-Overview"><a href="#Kubernetes-Overview" class="headerlink" title="Kubernetes Overview"></a>Kubernetes Overview</h1><p>This lesson will provide a high level overview of Kubernetes. We will cover what you can do with Kubernetes including some of the core features that have driven Kubernetes success. We will also discuss the competitive landscape around Kubernetes. Kubernetes, often abbreviate as K8s, is an open-source container-orchestration tool designed to automate, deploying, scaling, and the operation of containerized applications.</p>
<p>Kubernetes was born out of Google’s experience running workloads in production on their internal Borg cluster manager for well over a decade, it is designed to grow from tens, thousands, or even millions of containers. Organizations adopting Kubernetes increased their velocity by having the ability to release faster and recover faster with Kubernetes self healing mechanisms. Kubernetes is a distributed system. Multiple machines are configured to form a cluster. Machines may be a mix of physical and virtual and they may exist on-prem or in cloud infrastructure each with their own unique hardware configurations.</p>
<p>Kubernetes places containers on machines using scheduling algorithms that consider available compute resources, requested resources priority, and a variety of other customizable constraints. Kubernetes is also smart enough to move containers to different machines as this machines are added or removed. Kubernetes is also container runtime agnostic which means you can actually use Kubernetes with different container runtimes.</p>
<p>Kubernetes most commonly uses Docker containers but can also be used with Rocket containers, for example. This kind of adaptability is a result of Kubernetes modular design. It also has a lead to Kubernetes widespread adoption and made Kubernetes one of the most active open source projects around Kubernetes also provides excellent end user abstractions by using declarative configuration for everything. Engineers can quickly deploy containers, wire up networking, scale and expose the applications to the real world. We’ll cover all of these features throughout the lesson.</p>
<p>Operation staff are not left in the dark either. Kubernetes can automatically move containers from failed machines to running machines. There are also built-in features for doing maintenance on a particular machine. Multiple clusters can also join up with each other to form a Federation. This feature is primarily for redundancy, such that, if one cluster dies, containers will automatically move to another cluster.</p>
<p>The following features also contribute to making Kubernetes a top choice for orchestrating containerized applications: the automation of deployment rollout and rollback, seamless horizontal scaling, secret management, service discovery and load balancing, support for both Linux and Windows containers, simple log collection, stateful application support, persistent volume management, CPU and memory quotas batch job processing, and role-based access control. With the popularity of containers, there’s been a surge in tools to support enterprises adopting containers in production. Kubernetes is just one example.</p>
<p>So let’s compare Kubernetes with some other tools because now that we know what Kubernetes can do it’s sometimes useful when we can compare one technology to another. We’ll compare DCOS, Amazon ECS, and Docker Swarm Mode, each has their own niche and unique strength. This section will help you understand Kubernetes approach and decide if it fits your particular use cases.</p>
<p>DCOS or Distributed Cloud Operating System is similar to Kubernetes in many ways DCOS pools compute resources into a uniform task pool, but the big difference here is that DCOS targets many different types of workloads including, but not limited to, containerized applications. This makes DCOS attractive to organizations which are not using containers for all of their applications. DCOS also includes a Package Manager to easily deploy it to his systems like, Kafka or Spark. You can even run Kubernetes on DCOS given its flexibility for different types of workloads.</p>
<p>Amazon ECS, or the Elastic Container Service is AWS’ ability to orchestrate containers. ECS allows you to create pools of compute resources and uses API calls to orchestrate containers across them. Compute resources are EC2 instances that you can manage yourself or let AWS manage them with AWS Fargate. It’s only available inside of AWS and generally, less feature compared to other open source tools. So it may be useful for those of you who are deep into the AWS ecosystem.</p>
<p>Lastly, Docker Swarm Mode is the official Docker solution for orchestrating containers across a cluster of machines. Docker Swarm Mode builds a cluster from multiple Docker hosts and distributes containers across them. It shows a similar feature set with Kubernetes or DCOS. Docker Swarm Mode works natively with the docker command. This means that associated tools like Docker Compose can target Swarm Mode clusters without any changes.</p>
<p>Docker Enterprise Edition leverages Swarm Mode to manage an enterprise-grade cluster. And Docker also provides full support for Kubernetes if you want to start out with Swarm and later swap over to Kubernetes. So if you’re not already fixed on you using Kubernetes I would recommend that you conduct your own research to understand each tool and its trade-offs. Cloud Academy has content for each option to help you make the right decision.</p>
<p>In the next lesson, we’ll go through some of our options for deploying to Kubernetes. So I’ll see you there.</p>
<h1 id="Deploying-Kubernetes"><a href="#Deploying-Kubernetes" class="headerlink" title="Deploying Kubernetes"></a>Deploying Kubernetes</h1><p>Once you’ve decided on Kubernetes, you have a variety of methods for deploying Kubernetes. This course focuses on the core concepts. But because it is only natural to ask how to get started using Kubernetes, this short lesson discusses some of your options for deploying Kubernetes.</p>
<p>Deploying Kubernetes single-node cluster. For development and test scenarios, you can run Kubernetes on a single-machine. Docker for Mac and Docker for Windows, both include support for running Kubernetes on the local machine in a single-node configuration. Just make sure Kubernetes is enabled in the settings. This is the easiest way to get started if you already have Docker installed.</p>
<p>Another option is to use minikube which supports Linux in addition to Macs and Windows. Lastly, Linux systems can use kubeadm to set up a single-node cluster. Kubeadm is used as a building block for building Kubernetes clusters, but it can effectively create single-node clusters. But be aware that kubeadm will install Kubernetes on the system itself rather than a virtual machine, like the prior methods.</p>
<p>Single-node clusters are also useful within continuous integration pipelines. In this use case, you want to create ephemeral clusters that start quickly and are in a pristine state for testing applications in Kubernetes each time you check a new code. Kubernetes in Docker, abbreviated K-in-D or kind is made specifically for this use case.</p>
<p>Deploying Kubernetes multi-node cluster. For your production workloads, you want clusters with multiple nodes to take advantage of horizontal scaling and to tolerate node failures. To decide what solution works best for you, you need to ask several key questions including, “How much control do you want over the cluster versus the amount of effort you are willing to invest in maintaining it?”</p>
<p>Fully-managed solutions free you from routine maintenance but often lag the latest Kubernetes releases by a couple of version numbers for consistency. New versions of Kubernetes are released every three months. Examples of fully-managed Kubernetes as a service solutions include Amazon Elastic Kubernetes Service or EKS, Azure Kubernetes Service or AKS, and Google Kubernetes Engine or GKE.</p>
<p>To have full control over your cluster, you should check out kubespray, kops, and kubeadm. The next question is, “Do you already have investment into and expertise with a particular cloud provider?” Cloud provider’s managed Kubernetes services integrate tightly with other services in their cloud. For example, how identity and access management is performed. There will be a lot less friction to staying close to what you already know.</p>
<p>After that, we have, “Do you need enterprise support?” Several vendors offer enterprise support and additional features on top of Kubernetes. These can include OpenShift by RedHat, Pivotal Container Service, or Rancher.</p>
<p>Another question to consider is, “Are you concerned about vendor lock-in?” If you are, you should focus on open source solutions, like kubespray and Rancher that can deploy Kubernetes clusters to a wide variety of platforms.</p>
<p>Some other questions that are not important are, “Do you want the cluster on-prem, in the cloud, or both?” Because Kubernetes provides users with an abstraction of cluster of resources to the underlining nodes that can be running in different platforms. Kubernetes itself is at the core of open source hybrid clouds. Even cloud vendor Kubernetes solutions allow using on-prem compute. For example, GKE on-prem lets you run GKE on-premise, EKS allows you to add an on-premise nodes to the cluster, and Azure Stack allows you to run AKS on-prem.</p>
<p>Another question to consider is, “Do you want to run Linux containers, Windows containers, or a mix? To support Linux containers, you need to ensure you have Linux nodes in your cluster. To support Windows containers, you need to ensure that you have Windows nodes in your cluster. Both Linux and Windows nodes can exist in the same cluster to support both types of containers.</p>
<p>All that being said, in the context of this course, Cloud Academy has you covered for the following along with a course using a real multi-node cluster. The introduction to Kubernetes playground lab provides the same cluster that will be used during this course. So if you want to follow along without setting up your own cluster, go ahead and start that lab now and feel free to use any other cluster if you’d like to.</p>
<p>In the next lesson, we’re going to be covering the Basics of Kubernetes Architecture. Continue on when you are ready.</p>
<h1 id="Kubernetes-Architecture"><a href="#Kubernetes-Architecture" class="headerlink" title="Kubernetes Architecture"></a>Kubernetes Architecture</h1><p>This lesson will cover Kubernetes Architecture. What we cover here will be enough to understand and reason about topics we’ll learn later in this course. It is intended to build a strong foundation rather than to be an exhaustive review. Kubernetes itself is a distributed system. It introduces its own dialect to the orchestration space. Internalizing the vernacular is an important part of success with Kubernetes. And we will define several terms as they arise but know that there is also a Kubernetes glossary available in the introduction to Kubernetes learning path such that you have a single point of reference for the terms you need to know and more comprehensive glossary maintained by Kubernetes is also linked from there.</p>
<p>You must also understand the architecture to have a basic understanding of how features work under the hood. The Kubernetes cluster is the highest level of abstraction to start with. Kubernetes clusters are composed of nodes and the term cluster refers to all of the machines collectively and can be thought of as the entire running system.</p>
<p>The machines in the cluster are referred to as nodes. A node may be a VM or a physical machine. Nodes are categorized as worker nodes or master nodes. Each worker node include software to run containers managed by Kubernetes control plane and the control plane runs on master nodes. The control plane is a set of APIs and software that Kubernetes users interact with. These APIs and software are collectively referred to as master components.</p>
<p>The control plane schedules containers onto nodes. So the term scheduling does not actually refer to time in this context. Think of it from a Kernel perspective the Kernel schedules processes onto CPU’s according to multiple factors. Certain processes need more or less compute or may have different quality of service rules. Ultimately the scheduler does its best to ensure that every container runs. Scheduling in this case refers to the decision process of placing containers onto nodes in accordance with their declared compute requirements.</p>
<p>In Kubernetes containers are grouped into Pods. Pods may include one or more containers. All containers in a Pod run on the same node. And the Pod is actually the smallest building block in Kubernetes. More complex and useful abstractions sit on top of Pods. Services, define networking rules for exposing Pods to other Pods or exposing Pods to the internet. And Kubernetes also uses deployment to manage the deployment configuration and changes to running Pods as well as horizontal scaling. These are fundamental terms you need to understand before we can move forward.</p>
<p>We’ll elaborate on these terms and introduce more terms as we progress throughout the course but I cannot overstate the importance of them. I suggest you replay this section as many times as you need until all of this information sinks in. Lets recap about what we’ve learned so far. Kubernetes is an orchestration tool. A group of nodes form a Kubernetes cluster. Kubernetes runs containers in groups called Pods. Kubernetes services expose Pods to the cluster as well as to the public internet. And Kubernetes deployments control rollout and rollback of Pods.</p>
<p>In the next lesson, we’re going to be seeing how to interact with Kubernetes clusters.</p>
<h1 id="Interacting-with-Kubernetes"><a href="#Interacting-with-Kubernetes" class="headerlink" title="Interacting with Kubernetes"></a>Interacting with Kubernetes</h1><p>As we have seen, the master components provide the Kubernetes control plane. The way that you retrieve and modify state information in the cluster, is by sending a request to the Kubernetes API server, which is the master component that acts as a front end for the control plane. This leads us to the first method of interacting with Kubernetes, directly communicating via rest API calls. It is possible but not common to need, to work directly with the API server. You might need to if you’re using a programming language that does not have a Kubernetes client library.</p>
<p>Client libraries are our second method of interacting with Kubernetes. Client libraries can handle the tedium of authenticating and managing individual REST API requests and responses. Kubernetes maintains official client libraries for Go, Python, Java, .NET, and JavaScript. There are also many community-maintained libraries if there isn’t official support for your language of choice. The client libraries are a great choice for the OPAs writing code to interact with Kubernetes.</p>
<p>The next method of interacting with Kubernetes is the most common, and what we will focus on in this course, it is the Kubernetes command line tool called cube control, or Kubectl. With cube control, you can issue commands that are at a high level of abstraction with each command, translating into the appropriate API server request. With cube control, you can also access clusters locally, as well as remote. Your success with Kubernetes directly correlates with your Kubectl skill. You can accomplish all your day-to-day work using Kubectl.</p>
<p>So it is vital to learn this command because it manages all different types of Kubernetes resources, and provides debugging and introspection features. Luckily, Kubectl follows an easy to understand design pattern. When you learn to manage one resource, you learn to manage them all. Let’s introduce some common sub commands to see what they look like and what Kubectl can do.</p>
<p>Starting with Kubectl create, Kubectl create creates a new Kubernetes resource. You can create several resources using the built-in sub commands of create, or you can use resources specified in a file. The files are most commonly in gamble format and are referred to, as manifests.</p>
<p>Kubectl delete, Kubectl does the opposite of create, in that it deletes a particular resource. You can do the same with a file, with resources declared inside of it. Kubectl get, returns a list of all the resources for a specified type. For example, Kubectl get, pods lists all the pods and the current namespace. Kubectl describe is going to print detailed information about a particular resource or a list of resources. As an example, Kubectl describe pod, server gives detailed information about the pod named server. Kubectl logs, print container logs for a particular pod or a specific container inside of a multi container pod.</p>
<p>We’ll go deeper into these commands and more as the course progresses. This is just enough to kickstart you for our next lesson. And our final method of interacting with Kubernetes, is through the web dashboard. The dashboard provides a nice list of dashboards, as well as, easy to navigate views of cluster resources. The web dashboard is covered in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/deploy-a-stateful-application-in-a-kubernetes-cluster/">Deploy Stateful Application in a Kubernetes Cluster</a> lab here on cloud Academy. But if you want, you can check it out after you complete this course.</p>
<p>The web dashboard is optional, so not all clusters will have it. Kubectl truly is the way to go for maximum productivity and it really doesn’t take long to get the hang of it. We are now ready to start getting our hands dirty with Kubernetes. We can’t cover everything in this introductory course, but I want to cover many of the main parts of Kubernetes. In the next lesson we’re going to be deploying our first application to Kubernetes. So see you then.</p>
<h1 id="Pods"><a href="#Pods" class="headerlink" title="Pods"></a>Pods</h1><p>This lesson will introduce you to working with Kubernetes cluster and we’re going to be specifically focusing on pods. By doing so, you will see first-hand patterns used by kubectl and some examples of manifest files. But first, let’s review the theory. Pods are the basic building block in Kubernetes. Pods contain one or more containers and we’re going to be sticking with one container per pod in this lesson but we’ll be talking about multi-container pods later. </p>
<p>All pods share a container network that allows any pod to communicate with any other pod, regardless of the nodes that the pods are running on. Each pod gets a single IP address in the container network so Kubernetes will do all the heavy lifting and make that happen. You get to work with the simple abstraction. All pods can communicate with each other and that each pod has one IP address. </p>
<p>Because pods include containers, the declaration of a pod includes all the properties that you would expect for example with Docker rhyme. These include the container image, any ports you want to publish to allow access to the container, choosing a recent policy to determine if a pod should automatically restart, when its container fails, and limits on the CPU and memory resources but there are also a variety of other properties that are specific to pods in Kubernetes. We’re going to be seeing many examples of those in the coming lessons. </p>
<p>All of the desired properties are written in a manifest file. Manifest files are used to describe all kinds of resources in Kubernetes, not only pods. Based on the kind of resource that the manifest file describes, you will configure different properties of that file. The configuration specific to each kind of resource is referred to as its specification or spec. The manifests are sent to the Kubernetes API server where the necessary actions are taken to realize what is described in the manifest. You will use kubectl to send a manifest to the API server and one way of doing this is with the kubectl create command. </p>
<p>For pod manifests, the cluster will take the following actions. Selecting a node with available resources for all of the pods’ containers, scheduling the pod to that node. The node will then download the pod’s container images and then subsequently run the containers. There are more steps involved but that is more than enough to get the idea. We mentioned before that kubectl also provides sub-commands to directly create resources without manifests. </p>
<p>It’s usually a good idea to stick with manifests for several reasons. You can check in your manifests into a source control system to check their history and rollback when needed. It makes it easy to share your work such that it can be created in other clusters and it’s also easier to work with compared to stringing together sequences of commands with many options to achieve the same result. So we’re going to be sticking with manifests for this course. </p>
<p>Now that we’re ready to see all this in action using kubectl and a Kubernetes cluster, our goal will be to deploy an Nginx web server using a Kubernetes pod. If you are using the Introduction to Kubernetes Playground, follow the instructions to the EC2 instance to connect to the Bastion or feel free to connect using a local terminal like I am. If you use a different solution for a Kubernetes cluster, simply follow their provided instructions to make sure kubectl can talk to the cluster. </p>
<p>So I’m here at my terminal, connected to the Bastion host which has kubectl configured to talk to the lab cluster. To confirm that kubectl has configured to talk to the cluster, we first can enter our first few kubectl commands. Kubectl get pods. The output tells us that no pod resources were found in the default name space of the cluster. If it wasn’t able to connect to the API server, you would have seen an error message instead so everything looks good. </p>
<p>Let’s start with a minimal example of a pod manifest to get a taste for manifests. We’ll gradually build them up as we go. I’ve prepared the 1.1 basic pod .yaml file for this. All the course files are preloaded into the source directory on the lab instance and also available on the course get hub repo. This manifest declares a pod with one container that uses the Nginx latest image. All manifests have the same top level keys, API version, kind, and metadata followed by the spec. </p>
<p>Kubernetes supports multiple API versions and version one is the core API version containing many of the most common resources such as pods and nodes. Kind indicates what the resource is. Metadata then includes information relevant to the resource that can help identify resources. The minimum amount of metadata is a name which is set to my pod. </p>
<p>Names must be unique within a Kubernetes name space and spec is specification with a clear kind and must match what is expected by the defined API version. For example, the spec can change between the beta and the generally available API version of a resource. The spec is essentially where all of the meat goes. You can refer to the official API docs for complete info on all versions and supported fields. I’ll explain the ones that we need for this course but know that there are far more left to discover. </p>
<p>The pod spec defines the containers in the pod. The minimum required field is a single container which must declare its image and name. This pod only has a single container but the yaml is a list allowing you to specify more than one. Back at the command line, we can create the pod by changing into the source directory with CD source. We then issue kubectl create -f 1.1-basic_pod.yaml. The f option tells us to create, that the create command, is going to be creating a manifest from a file. For any kubectl command, you can always depend –help to display the help page to get more information. </p>
<p>Now if we run kubectl get pods, we can see my pod is running. My pod is technically an object of a pod kind of resource but it is common to simply use resource to also describe the object as well as the kind. Kubectl shows the name, the number of running containers, the pod state, restarts, and the age of the pod in the cluster. You should memorize the get commands since you’ll use it all the time and I really mean all of the time. </p>
<p>Let’s see some more detailed information about this particular pod. Using the describe command to get complete information. Kubectl describe pod and we’re going to pipe it to more. Describe takes a resource kind just like get and to narrow in on specific resources of that kind, we add the name which you can also do with get. We’re going to be piping the output to more so we can press space bar to page throughout this output. </p>
<p>As you can see, there’s a lot more information than what get provides. The name, name space, and the node running the pod are given at the top along with other metadata. Also note that a pod is assigned an IP. No matter how many containers we include, there would be only one IP. In the containers section, we can see that the image and whether or not the container is ready. You can also the port and the container port are both set to none. </p>
<p>Ports are part of the container spec but Kubernetes assigns default values for us. Just like Docker, you need to tell Kubernetes which port to publish if you want it to be accessible. We’ll have to go back and declare our port after this. Otherwise, nothing is going to reach the web server and at the bottom, we have our events section. It lists the most recent events related to the resource. You can see that the steps Kubernetes took to start the pod from scheduling on the container image to starting the container. The events section is shared by most kinds of resources when you use describe is very helpful for debugging. </p>
<p>Let’s tell Kubernetes which port to publish to allow access to the web server. I’ve prepared the 1.2 port file specifically for that. Compared to the 1.1 file, we can see the ports mapping is added and the container port field is set to 80 for HTTP. Kubernetes is also going to be using TCP as the protocol by default and we’ll assign it an available host port automatically so that we don’t need to declare anything more. </p>
<p>Kubernetes can apply certain changes to different kinds of resources on the fly. Unfortunately, Kubernetes cannot update ports on a running pod so we need to delete the pod and recreate it. We’re going to be running our kubectl delete pod my pod to delete this pod. You can also specify with the -f with referencing to the 1.1 file and Kubernetes will delete all of the resources declared in that file. </p>
<p>Now, we can issue the command kubectl create -f 1.2.yaml. And describe the pod again. You don’t need to describe the pod every single time. I just prefer to do this to see the result of my work and make sure that everything went as I expected. Now we can see that port 80 is given as the port so you may think to try to send a request to port 80 on that noted IP but it still won’t work. Why do you think that is? Well the pod’s IP is on the container network and this lab instance is not part of the container network so it won’t work. But if we sent the request from a container in a Kubernetes pod, the request would succeed since pods can communicate with all other pods by default. We’ll see how we can access the web server from the lab instance in the next lesson. </p>
<p>Before we move on, I want to cover a couple more points and the first is shared between all resources and the second is specific to pods. In the describe section, you might have seen the labels field was set to none. Labels are key value pairs that identify resource attributes. For example, the application tier, whether it’s front end or back end or maybe a region such as US East or US West. </p>
<p>In addition to providing meaningful and identifying information, labels are used to make selections in Kubernetes. For example, you could tell kubectl to get only resources in the US West region. So our 1.3 manifest has a label added to identify the type of app that the pod is a part of. We’re using an Nginx web server and the label value is web server. You could have multiple labels but one is enough in this example. </p>
<p>Our last point that I want to make pertinent is that Kubernetes can schedule pods based on their resource requests. The pods that we’ve seen so far don’t have any resource requests set which makes it easier to schedule them because the scheduler doesn’t need to find nodes that have these requests for amounts of resources. It’ll just throw them onto any node that isn’t under pressure or starved of resources. However, these pods will be the first to be evicted if a node becomes under pressure, it needs to free up resources. That’s called best effort quality of service which was displayed in the describe output. Best effort pods can also create resource contention with other pods on the same node and usually it’s a good idea to set resource requests. </p>
<p>In the 1.4 yaml, I’ve set a resource request and limit for the pod’s container. Request sets the minimum required resources to schedule the pod onto a node and the limit is the maximum amount of resources you want the node to ever give the pod. You can set resource requests and limits for each container. There’s also support for requesting amounts with local disk by using the ephemeral storage. </p>
<p>When we create this pod, kubectl delete pod by pod and then subsequently kubectl -f 1.4, the pod will be guaranteed the resources you requested or it won’t be scheduled until those resources are available. Kubectl describe my pod will now list our pod and we’ll see this guaranteed quality of service. You need to do some benchmarking to configure a reasonable request and limit but the effort is well worth it to ensure your pods have the resources they need and best utilization of the resources in the cluster. This is one of the reasons why we are using containers in the first place. </p>
<p>For the rest of this course, we will use best effort pods since we won’t have any specific resource requirements in mind. This isn’t something you should do in protection environments, however. We’ve covered a lot in this lesson so let’s review what we covered. Pods are the basic building block in Kubernetes and contain one or more containers. You declare pods and other resources in manifest files. All manifests share an API version, kind, and metadata related to that resource. Metadata must include a name but labels are usually a good idea to also help you further filter down your resources. </p>
<p>Manifests also include a spec to configure the unique parts of each resource kind. Pod specs include the list of containers, which must specify our container name and image, but is often useful to set the resource requests and limits. We’re going to see more fields of pod specs in later lessons. </p>
<p>In our next lesson, we’re going to be making the web server running in the pods accessible from our lab VN with services. I’ll see you there.</p>
<h1 id="Services"><a href="#Services" class="headerlink" title="Services"></a>Services</h1><p>So in our previous lesson, we created a webserver Pod but at the moment, it’s inaccessible apart from other Pods in the container network, which isn’t really useful. Even for pausing the container network, it isn’t very convenient to access the webserver as it is because Pods if you’d be able to find the IP address of the webserver Pod and keep track of any changes to it. So remember that Kubernetes will reschedule Pods on to other nodes, for example if the node fails.</p>
<p>But what happens if you have a Pod that fails? Once the Pod is rescheduled it will be assigned an IP address from the available pool of addresses and not necessarily the same IP address it had before. So to overcome all of these networking issues, Kubernetes employs services. thinking back to the Kubernetes’ definition for a service, a service defines networking rules for accessing Pods in the cluster and from the internet you can declare a service to access a group of Pods using labels. And in our example, we can use our app label just like the webserver Pod for the services target. Clients could then access the service at a fixed address. And the services networking rules will direct client request to a Pod in the selected group of Pods.</p>
<p>In our example, there is only one Pod but in general there can be many. The service will also distribute these requests that come into it across the Pods to balance the load. Let’s visualize how we’ll use the service to solve our problem of accessing the webserver running in the Pod.</p>
<p>First, we’re gonna create a service that selects Pods with the app&#x3D;webserver label. That will cause the service to act as a kind of internal load balancer across those Pods. The service will also be given a static IP address and Port by Kubernetes that will allow us to access the service from outside of the container network, and even outside of the cluster.</p>
<p>Let’s see how we do it. Our first three fields are set to the same as before. The kind is now Service, metadata uses the same label as the Pod since it is related to the same application. This isn’t required but it is a good practice to stay organized. Now for the spec, the selector is our important field. The selector defines the labels to match the Pods against. At this example of targets Pods with the app&#x3D;webserver, which will select the Pod that we’ve already created. Services must also define port mappings. So, this service targets Port 80. This is the value of the Pods’ container port.</p>
<p>Lastly, is the optional type. This value defines actually how to expose the Service and we’re gonna set it to NodePort. NodePort allocates a port over this service on each node in the cluster. By doing this, you can send a request to any node in the cluster on the designated port and be able to reach that Service. The designated port will be chosen from the set of available ports on the nodes, unless you specify a NodePort as part of the specs ports.</p>
<p>Usually it is better like Kubernetes shows the NodePort from the available ports to avoid the chance that your specified port is already taken. That would cause the service to fail to create In future lessons, we’re gonna be covering alternate values to service types, so don’t worry.</p>
<p>Now, let’s create the service with our familiar kubectl create -f 2.1. And we’re gonna list the services with kubectl get services. Notice that both commands are really familiar. Kubectl follows a simple design pattern which makes it easy to manage and explore different resources. Kubectl also displays the name, Cluster-IP, External-IP, Ports, and Age of each service.</p>
<p>Let’s bring it down, starting with Cluster-IP. This is our private IP for each service. Our External-IP is not available for NodePort services but if it were, then this would be the public IP for a service. Note that the Ports column, Kubernetes will automatically allocate a Port in the Port range allocated for NodePorts which is commonly port numbers between 30,000 and 32,767.</p>
<p>Let’s describe the service to see what other information is available with kubectl describe service webserver. Just like before, you’ll see a bunch of useful debugging information. The Port was shown in the get services output and also in this output. But we also can see the Endpoint, which is the address of each Pod in the selected group, along with a container port. If there were multiple Pods selected by the label, then you would see each of them listed here.</p>
<p>Kubernetes automatically adds and removes these endpoints as matching Pods are created and deleted, and so you don’t need to do anything to manage those endpoints. Now that we know that the NodePort is on we need a nodes IP, it can be any nodes IP. And one way to list them is to grep for this address in the described node output and add the -A option to include lines after the match.</p>
<p>So let’s do that, kubectl describe nodes and we’re to pipe it to grep -i address -A 1. Nodes are resources in the cluster, just like Pods and services. So, you can use the get and describe commands on them. You can check out all the information in the describe output on your own, or for right now, we just need those IPs. The IP addresses are the internal or private IPs of our nodes inside of our cluster.</p>
<p>Our lab VM is in the same virtual network, so it can reach the nodes using these addresses. And I’ve allowed incoming traffic on the NodePort range from the lab instance in the firewall rules to allow the request. Choose any of the addresses and use the curl command to send an http request to the IP with a NodePort upended. That is the raw html output being served up by Nginx. You can try any of the node IPs and it will give your exact same result, all thanks to a Kubernetes service.</p>
<p>In this lesson, we saw that services allow us to expose Pods using a static address, even though the addresses of the underlying Pods may be changing. We also specifically used a NodePort service to gain access to the service from outside of the cluster on a static Port that is reserved on each node in the cluster. This allowed us to access the service by sending a request to any of the nodes, just not the node that is running the Pod. There is more to say about Pods and services. We will use more complex application in the future to illustrate some of the remaining topics in the next couple of lessons. Think microservices will start by covering multi-container Pods, to continue on when you’re ready.</p>
<h1 id="Multi-Container-Pods"><a href="#Multi-Container-Pods" class="headerlink" title="Multi-Container Pods"></a>Multi-Container Pods</h1><p>This lesson continues to expand upon what we’ve already learned about Pods. Specifically, we’re going to be exploring the details of working with Multi-Container Pods. This is where we really start to hit the good stuff. As we learn more about Multi-Container Pods, we’re also gonna be learning about Namespaces and Pod Logs.</p>
<p>We’re using a sample application for this lesson. It’s a simple application that increments and prints a counter. It’s split into 4 containers across 3 tiers. The application tier includes the server container that is a simple Node.js application. It accepts a post request to increment a counter and a get request to retrieve the current value of the counter. The counter is stored in the Redis container which comprises the data tier. The support tier includes a poller and a counter.</p>
<p>The poller container continually makes a get request back to the server and prints the value. The counter continually makes a post request to the server with random values. All the containers use environment variables for configuration and these Docker images are public, so we can reuse them for this exercise.</p>
<p>Let’s walk through modeling the application as Multi-Container Pods. We’ll start by creating a Namespace for this lesson. Remember that a Namespace separates different Kubernetes resources. Namespaces may be used to isolate users, environments, or applications. You can also use Kubernetes’ role-based authentication to manage users as access to resources in a given Namespace. Using Namespaces is a best practice.</p>
<p>So, let’s start using them now and we’ll continue to use them throughout the remainder of this course. The created, just like any other Kubernetes resource. Here is our Namespace manifest. Namespaces don’t require a spec. The main part is the name which is set to microservices and is a good idea to label it as well. Everything in this Namespace will relate to the counter microservices app. So let’s create the Namespace in kubectl. With kubectl create -f 3.1.</p>
<p>Use your kubectl commands, either use a –namespace or -n option to specify the Namespace, otherwise the default Namespace will be used. You could also use the kubectl create namespace command but for this course, we’re gonna be sticking to manifest. Now out of the Pod, I’ve named the Pod app. Off the top, I want to mention you can specify namespace in this metadata for this Pod but that makes this manifest slightly less portable because the Namespace can’t be overwritten at the command line.</p>
<p>Moving down to the Redis container, we’ll use the latest official Redis image. The latest version is chosen to illustrate a specific point. When you use the latest tag in Kubernetes, and it will always pull the image whenever the Pod started. This can introduce bugs, if a Pod restarts and pulls the new latest version without you realizing it.</p>
<p>Prevent always pulling the image in using an existent version, if one exist. You can set the imagePullPolicy field to IfNotPresent. It’s useful to know this but in most situations you’re better off specifying a specific tag rather than the latest. When specific tags are used, the default imagePull behavior is, IfNotPresent. So, the standard Redis port of 6379 is published with this container.</p>
<p>Now, onto the server container. The server container is straightforward. The image is the public image from this sample application. The tag is used to indicate the microservice within the microservices repository And the server, runs on port 8080, such that it is exposed. The server also requires a REDIS_URL environment variable to connect to the data tier. We can set this in the environment variable sequence.</p>
<p>So how does the server know where to find Redis? Well, because containers in a Pod share the same network stack, a result of which that they all share the same IP address. So, they can reach other containers in the Pod on the local host at their declared container port. The correct host port in this example is localhost:6379 Our imagePullPolicy is admitted because Kubernetes uses IfNotPresent when the explicit tag is given. </p>
<p>We can use the same approach for the counter and poler containers. These containers require the API_URL environment variable to reach the server in the application tier. The correct host port combo for this example is localhost:8080. </p>
<p>Now, let’s create the Pod this time but by adding the -n option to set the Namespace for this Pod, such that it’s created in a microservices Namespace. Kubectl create -f 3.2 yaml -n microservice. Remember to include the same Namespace option with oq control commands that are relating to the Pod. Otherwise you will be targeting the default Namespace. </p>
<p>If we wanted to get the Pod, we would issue kubectl get -n microservices pod app. The -n namespace option can be included anywhere after kubectl. It doesn’t have to be after get, it could be before or after. When you have tab completion enabled. It makes sense to put it earlier, to get to completions for your target namespace. </p>
<p>Let’s observe the output, and we’ll see a &#x2F;4 under the status, since we have 4 containers in the Pod. The status also summarize what is going on but it is best to describe the Pod to see what is going on in more detail. Kubectl describe -n microservice pod app. You’ll see the event log has more going on now that there are multiple containers. </p>
<p>The same events are being triggered for each container from Pulling to Starting as was the case for a Single-Container Pod is something goes awry. You should check the event log to see what’s happening behind the scenes to debug any issue. In this case, everything looks good. </p>
<p>Once the containers are running, we can look at the container logs to see what they’re doing. Logs are simply anything that is written to standard out or standard error in the container. The containers need to write messages to standard out or standard error, otherwise nothing will appear in the logs. Kubernetes records the logs and they can be viewed via the logs command the kubectl log command retrieves logs for a specific container in a given Pod. It dumps all of the logs by default or you can use the tail option to limit the number of logs present. </p>
<p>Let’s see the most 10 recent logs for the counter container in the app Pod. Was kubectl logs -n microservice app counter –tail 10. Here we can see the counter is incrementing by the count by random numbers between 1 and 10. Let’s check the value of the count by inspecting the logs for the poller container. This time we’ll use the -n -f to stream the logs in real time, which is short for follow. Kubectl logs -n microservice app poller -f. We can see the count is increasing every second as the counter continues to increment it. That confirms it, our first multi-container application is up and running. Press Control + C to stop following the logs.</p>
<p>In this lesson, we created a Multi-Container Pod that implements a 3-tier application. We use the fact that containers in the same Pod can communicate with one another using local host. We also saw how to get logs from containers running in Kubernetes by using the kubectl logs command. Remember that logs worked by recording what the container writes to standard out and standard error. The logs also allowed us to confirm that the application is working as expected by continuously incrementing that count.</p>
<p>But there are some issues with the current implementation. Because Pods are our smallest union of work, Kubernetes can only scale out by increasing the number of Pods and not the containers inside of the Pod. If we want to scale out the application tier with the current design we have to also scale out all other containers proportionately. This means that there would be multiple Redis containers running, each would have their own copy of the counter. That’s certainly not what we’re gonna be going for.</p>
<p>It is a much better approach, if we were able to scale each of these services independently. Breaking the application out into multiple Pods and connecting them with services is our ideal implementation. We’ll walk through the design in next lesson but before moving on, it’s worth noting that sometimes you do want each container in a Pod to scale proportionately. It comes down to how tightly coupled the containers are, and if it makes sense to be thinking of them as a single unit.</p>
<p>With that point out of the way, I’ll see you in our next lesson where we will leverage services to break our tightly coupled Pod design into multiple independent Pods.</p>
<h1 id="Service-Discovery"><a href="#Service-Discovery" class="headerlink" title="Service Discovery"></a>Service Discovery</h1><p>We’ve seen Services in action, and in the context of allowing external access to pods running in the cluster, when we created them, with a node port. It’s time to see how services are useful within the cluster. We’ll split our example Microservices application into three pots, one for each tier. Remember that we use the fact that the containers in the same pod can communicate with each other using the local host. But that’s not going to work with our multi-pod design.</p>
<p>That’s where Services come in. Services provide a static end point to access pods in each tier. We could directly use the individual pod IP addresses on the container network, but that would cause the application to break when pods are restarted, because their IP address could change. An added benefit of Services is they also distribute load across the selected group of pods, allowing us to take advantage of the scaling application tier across multiple server pods. </p>
<p>So to realize these benefits, we need to create a data tier service in front of the Redis pod, and an application to your Service in front of the server pod. There are two Service discovery mechanisms built into Kubernetes. The first are environment variables, and the second is DNS. Kubernetes will automatically inject environment variables into containers that provide the address to access services. The environment variables follow a naming convention so that all you need to know is the name of the service to access it. Kubernetes also constructs DNS records based on the service name and containers are automatically configured to clear the clusters, DNS, to discover those services. You’ll see examples of both techniques in this lesson.</p>
<p>We’ll start with creating a new namespace to organize the resources for this lesson. It’s called Service Discovery, and we’ll do that with kube control create, dash F 4.1. Moving on to the data tier. We have a manifest that includes multiple resources. With YAML, we’re allowed to create multiple resources by separating them with three hyphens. It’s possible to cram all the pods and services into one file, but separating them by tier mimics the way we want to manage each tier independently.</p>
<p>We have a service, and now we have our Redis pod. Both are named data tier. The pod has a tier label, which is used by the service as its selector. In our example, we only have one Microservice in the data tier, but that won’t be the case in general. You can include as many labels as necessary in the selector to get just what you need. We can get by with just this one label, in this case. Services can also publish more than one port, which makes a naming the ports mandatory to identify them. We only have one, so the name is optional.</p>
<p>In YAML, everything after our pound or hashtag symbol is a comment. Comments are for readability, and don’t affect how Kubernetes interprets the manifest. Lastly, we set the type to cluster IP, which is the default so that the line could be omitted. Cluster IP creates a virtual IP inside the cluster for internal access only. So we can now use kube control, create, dash F 4.2 YAML and append the namespace with dash N service-discovery.</p>
<p>To create the resources, the command is the same, regardless of how many resources are specified in the file. The resources in the file are created in the order they are listed in the file. Let’s check that the pod is running with kube control, get pod dash N service-discovery. Then describe the service with kube control describe service dash N service-discovery data tier. To make sure that our service has a cluster IP, and that one endpoint corresponds to the data tier pod selected by the service.</p>
<p>Let’s move on to the app tier. Again, we have a service and a pod. The service selects the pods with a tier label, matching the server pod declaration. The pod spec is the same as the one before with one exception, the value of Redis URL environment variable is set using environment variable set by Kubernetes over to service discovery. The value used to be local host 6379, but now we need to access the data tier service. </p>
<p>There are separate environment variables made available to you. The service cluster IP address is available using the environment variable, following the pattern of a service name in all capital letters, with hyphens replaced by underscores followed by underscore service, underscore host in all caps. By knowing the service name you construct the environment variable name, to discover that service IP address. </p>
<p>In our example, with the environment variable and its data tier service host, the port environment variable is similar with host replaced by port. In our example, that is data tier service port, if the port includes a name, you can also append and underscore port name in all caps, hyphens replaced by underscores, which is data tier service port Redis, in our example. The data tier service only declares one port, so the appended name is optional. </p>
<p>As a best practice you can append the service name to tolerate adding ports to the service in the future. When using environment variables in the value field, you need to enclose the variable name in parentheses and precede it with a dollar sign. This allows composing container environment variables from the Kubernetes provided values. When using environment variables for service discovery, the service must be created before the pod in order to use environment variables for service discovery. That is, Kubernetes does not update the variables of running containers. They only get set at startup. </p>
<p>The service must also be in the same namespace for the environment variables to be available. So let’s create our application tier with kube control, create dash F 4.3 YAML, dash N service discovery. Now onto the support tier. We don’t need a service for this tier, just a pod will do, and it contains the counter and polar containers used before. This time we’re gonna be using DNS for service discovery of the app tier service. </p>
<p>Kubernetes will add a DNS A records for every service. The service DNS names follow the pattern of a service name, dot service namespace. In our example that is, app dash tier.service dash discovery. However, if the service is in the same namespace, then you can simply only use the service name. The polar omits the namespace in this manifest.</p>
<p>No need to convert hyphens to underscores, or use all caps when using DNS service discovery. The cluster DNS resolves the DNS name to the service IP address. You can get service port information using DNS SRV records, but that isn’t something that we can use in the manifest file. So I’ll have to either hard-code the port information or use the service port environment variable. The counter uses a hard-coded port, and the polar uses the port environment variable for illustration. </p>
<p>It is possible to use the DNS SRV port record to configure the pod on start-up using something called iNET containers, but we’re going to be covering that later on in this course. So let’s create the support tier. Starting with Kube control create dash F 4.4 YAML, and then the namespace service discovery. </p>
<p>Now let’s check all the pods, with kube control, get pods, namespace service discovery. There are three running pods creating four containers in total. So let’s check the polar logs to see what’s going on with our account. With kube control logs dash N service discovery support tier polar, and let’s follow them. Look at that. The application is just plugging away, and that is such a satisfying result.</p>
<p>Let’s recap this lesson before jumping into the next one. We’ve covered structuring a varying number of applications using Services as interfaces between tiers. We use the cluster IP type of service. We’re accessing the data and application tiers within the cluster. We also covered how Kubernetes Services works with environment variables and DNS. That allowed us to refactor our multi container pod application into instead a multi-tier application that we stood up in this lesson.</p>
<p>When using environment variables for service discovery, the service must be created for the pod, before the pod, in order to use the environment variables for that service discovery. Their service must also be in the exact same namespace. DNS records overcome the shortcomings of environment variables. DNS records are added and removed from the clusters DNS as services are created and destroyed. The DNS name for services include a namespace, allowing communication with services and other namespaces. And finally SRV DNS records are created for service port information.</p>
<p>So what do you think? Are you getting excited about the capabilities that Kubernetes can do? It just keeps getting better and better. And it’s gonna get better in the next lesson. To put it in context, consider how we would scale our current application. We could increase the number of server pods by changing the name to something like example app tier dash one, then creating example app tier dash two, and so on. And we could glue this all together with some scripting, a bit of extra work to make the scaling easy. But what then happens when we would want to reconfigure the server container? Well, let’s see.</p>
<p>We could create an example app tier version one dash one and then example app tier version two dash one, with some updated scripting. These things could probably handle that, but what happens when something goes wrong? Or what if there’s an error in the new version? We could probably handle that by pulling the API and checking the status again, with probably some more scripting, include some more code, but there should probably be a better way to do this, which is exactly what the next lesson is going to be covering. And that is deployment. So let’s learn about it in the next lesson.</p>
<h1 id="Deployments"><a href="#Deployments" class="headerlink" title="Deployments"></a>Deployments</h1><p>The previous lessons have created pods directly, but I’ve gotta be honest with you, we’ve kind of been cheating a bit so far. You’re not really supposed to create pods directly. Instead, a pod is really just a building block. They should be created via a higher level abstraction such as deployments. This way, Kubernetes can add on useful features and higher level concepts to make your life easier.</p>
<p>This lesson is gonna be covering the basics of the deployments with the following lessons covering auto-scaling and rolling updates. So let’s start by covering some theory, and then we’ll see deployments in action. A deployment represents multiple replicas of a pod. Pods in their deployment are identical, and within a deployment’s manifest, you embed a pod template that has the same fields as this pod spec that we have written before.</p>
<p>So you describe a state in the deployment, for example, five pod replicas of Redis version five, and Kubernetes takes the steps required to bring the actual state of the cluster to that desired state that you’ve specified. If for some reason one of the five replica pods is deleted, Kubernetes will automatically create a new one to replace it. You can also modify the desired state and Kubernetes will converge the actual state to that desired state. We’ll see a bit of that in this lesson with more on updates in a later lesson. The Kubernetes master components include a deployment controller that takes care of managing the deployment.</p>
<p>Now, let’s see how this works in practice. We’ll use our microservices three tier application to demonstrate deployments and we’ll replace the individual pods with a deployments that manage the pods for us. Let’s start by creating a new namespace called deployments for this lesson. So we’re gonna do that with kubectl create dash f 5.1 yaml, just like we’ve done previously. Now, a deployment is a template for creating pods. </p>
<p>A template is used to create replicas, and a replica is a copy of a pod. Applications scale by creating more replicas. This will be more clear when you just see the YAML files and as we demonstrate more features throughout this lesson. Now, I’m comparing the data tier manifest from the last lesson to our current manifest that uses deployments. I wanna highlight how there are significant similarities, but just a few changes, and the first change is the API version is now apps version one. Higher level abstractions for managing applications are in their own API group and not part of the core API. The kind is set to deployment, and our metadata from the last lesson is directly applied to said deployment.</p>
<p>Next comes a spec. The deployment spec contains deployment-specific settings and also a pod template, which has exactly the same pod spec as the last lesson in it. It in the deployment-specific section, the replica key sets how many pods to create for this particular deployment. Kubernetes will keep this number of pods running. We set the value to one because there cannot be multiple Redis containers. We’ll have one Redis pod.</p>
<p>Next, there’s the selector mapping. Just like we saw with services, deployments use label selectors to group pods that are in the deployment. The match labels mapping should overlap with the labels declared in the pod template below, and kubectrl will complain if they don’t overlap. The pod template metadata includes labels on the pods.</p>
<p>Note that the metadata doesn’t need a name in the template because Kubernetes generates unique names for each pod in the deployment. Similar changes are made to the app tier manifest and the support tier manifest, mainly adding a selector in a template for the deployment. We can complete the same process for the app and support tiers, also setting replicas to one for both cases.</p>
<p>One is actually the default, so it isn’t strictly required, but it does emphasize that a deployment manages a group of identical replicas. So let’s create the tiers now. Be sure to set the deployments namespace, and we’ll use multiple f options to create them all in one go with kubectrl create namespace deployments f 5.2, 5.3, 5.4 YAMLs.</p>
<p>Now let’s get our deployments with kubectrl get namespace deployments deployments. Kubectrl displays three deployments and the replica information. Note that they all show one replica right now. So remember that horrible scenario I described at the end of the last lesson? Well, we can see how deployments solve the problem by asking kades for the pods.</p>
<p>Note that each pod has a hash at the end of it. Deployments add this uniqueness to the names, automatically allowing us to identify pods of a particular deployment version. We can see how this works by running more than one replica in a deployment. We’ll use kubectrl scale command for modifying replica counts. We’ll scale the number of replicas in the support tier to five, which will cause the counter to increase five times more quickly. The scale command is equivalent to editing the replica value in the manifest file and then running kubectrl apply to apply the change. It’s just optimized for this one-off use case.</p>
<p>Now, if we run kubectrl get pods in the namespace of deployments, we can see the pods again to see what happened. Note that the support tier pods continue to show two of two ready containers. This is because replicas replicate pods, not individual containers inside of a pod. Deployments ensure that the specified number of replica pods are kept running. So we can test this by deleting some pods with kubectrl delete, the namespace deployments pods support tier, and then the hash. And now watch as Kubernetes brings them back to life.</p>
<p>All right, so Kubernetes can resurrect pods and make sure the application runs the intended number of pods. As a side note, I used the Linux watch command with the dash n1 option to update the output every one second. Kubectrl also supports watching by using the w option and any changes are appended to the bottom of the output compared to overriding the entire output with the Linux watch command. You might prefer one over the other depending on what you’re watching. But let’s go ahead and scale out the app tier to five replicas, as well, with kubectrl scale namespace deployments deployment app tier replicas five.</p>
<p>Now let’s get the list of pods with kubectrl namespace deployments get pods. As you can see, Kubernetes makes it really quite painless. They did all the heavy lifting for us, and now we can confirm that the app tier service is load balancing requests across the app tier pods by describing the service, kubectrl describe namespace deployments service app tier. And now observe that the service now has five endpoints matching the number of pods in said deployment. Thanks to label selectors, the deployment and the service are able to track the pods in the tier.</p>
<p>Let’s review what we’ve done in this lesson. We’ve used deployments to have Kubernetes manage the pods in each application tier. By using deployments, we get the benefits of having Kubernetes monitor the actual number of pods and converge to our specified desired state. We also saw how we can use kubectrl scale to modify the desired number of replicas in Kubernetes. This will do what it takes to realize the number of replicas we specify. We also saw how it seamlessly integrates with services that load balance across the deployments’ pods.</p>
<p>A word of caution with scaling deployments is that you should make sure that the pods you are working with support horizontal scaling. That usually means that the pods are stateless as opposed to stateful. The data for the app tier is stored in the data tier, and we could add as many app tier pods as we like because the state of the application is stored inside of the data tier.</p>
<p>With our current setup, we can’t scale the data tier out ‘cause that would create multiple copies of the application counter. However, if we never scale the data tier, we still get the benefit of having Kubernetes return the data tier to its desired state by using deployments. We also get more benefits when it comes to the performing of updates and rollbacks, which we’ll see in a couple of lessons. So it still makes sense to use a deployment for the data tier. We rarely should be directly creating pods.</p>
<p>Kubernetes has even more tricks up its sleeve when it comes to scaling. We arbitrarily scaled the deployment, but in practice, you would like to scale based on CPU load or some other metric to react to the current state of the system to make the best use of available resources. So let’s see how to do that in the next lesson.</p>
<h1 id="Autoscaling"><a href="#Autoscaling" class="headerlink" title="Autoscaling"></a>Autoscaling</h1><p>We’ve seen deployments work their magic in the last lesson. We also saw how to scale the deployment replicas but it would be nice to not have to manually scale the deployment. That’s where autoscaling comes in. Kubernetes supports CPU-based autoscaling and autoscaling based on a custom metric that you can define. We’re gonna be focusing on CPU for this course.</p>
<p>Autoscaling works by specifying a desired target CPU percentage and a minimum and a maximum number of allowed replicas. The CPU percentage is expressed as a percentage of the CPU resource request of that Pod. Recall that Pods can set resource requests for CPU to ensure that they’re scheduled on a node with at least that much CPU available. If no CPU request is set, autoscaling won’t take any action.</p>
<p>Kubernetes will increase or decrease the number of replicas according to the average CPU usage of all of the replicas The autoscaler will also increase the number of replicas when the actual CPU usage of the current Pods exceeds the target and vice versa for decreasing the number of Pods. It will never create more replicas in the maximum nor will they decrease the number of replicas below your configuring minimum. You can configure some of the parameters of the autoscaler, but the default will work fine for us.</p>
<p>With the defaults, the autoscaler will compare the actual CPU usage to the target CPU usage. And either increase the replicas if the actual CPU is sufficiently higher than the target, or it will decrease the replicas if the actual CPU is sufficiently below the target. Otherwise it will keep the status quo. Autoscaling depends on metrics being collected in the cluster.</p>
<p>Kubernetes integrates with several solutions for collecting metrics. We’re going to be using the Metrics Server which is a solution that is maintained by Kubernetes itself. There are several manifest files on the Kubernetes Metrics Server GitHub repo that declare all of the resources. We will need to get Metrics Server up and running before we can use autoscaling.</p>
<p>Once Metrics Server is running, autoscalers will retrieve those metrics and then make calls with the Kubernetes metrics API. The lab instance includes a Metrics Server manifest in the Metrics Server sub-directory. It’s outside the scope of this course to discuss all the resources that comprise of the Metrics Server. So all we need to do is create them and we can count on metrics being collected in the cluster.</p>
<p>Here we can use the kubectl apply command and then specify the Metrics Server folder to create all of the resources within the Metrics Server folder. kubectl control will then create all of the manifests it finds in that directory. You can see quite a few of these resources are created. One of them is the deployment, in the Metrics Server, runs actually as a pod in the cluster, and that pod is managed by that deployment. It takes a minute or two for the first metrics to start trickling in.</p>
<p>Let’s confirm that the Metrics Server is running by watching the pod. With kubectl top pods namespace deployments. This will list the CPU and memory uses of each pod in the namespace. You can use the top command to benchmark a pod’s resource utilization, and then subsequently debug resource utilization issues. Our pods are all using a small fraction of one CPU. The m stands for milli. 1000 milli CPUs equals one CPU.</p>
<p>Now that we have metrics, the other thing the autoscaler depends on is having a CPU request in the deployments odd spec. Let’s see how that looks in the app-tier deployment. I’ve highlighted the change from the previous lesson. Each pod will now request 20 milli CPU. Kubernetes will only scale the pods and each node with at least 0.02 CPU’s remaining. I also set the replicas to five to keep five replicas running.</p>
<p>Now, if we try to create the resources, kubectl will tell us that they actually already exist. Create will check if a resource of a given type and name already exists and it will fail if it does. We could delete the deployment and then recreate it but it would be nice to avoid the downtime that is involved. Instead, Kubernetes provides a command that can apply changes to existing resources. That’s what kubectl applies. So let’s apply that to 6.1 now.</p>
<p>Apply will update our deployment and do include the CPU request. It will warn us about mixing create and apply, but we can go ahead and ignore that. I’d encourage you to take the certified Kubernetes administrator course here on CloudAcademy if you’d like to learn more about the differences between create and apply.</p>
<p>So we’ve set the request low enough that the five replicas can remain scheduled in the cluster as we can see if we get the deployments output. Five actual pods are ready matching the five pods we desired. This completes like the prerequisites for autoscaling. The autoscaler, which has the full name of HorizontalPodAutoscaler because it scales horizontally or out, it’s just another resource in Kubernetes we can use a manifest to declare.</p>
<p>The HorizontalPodAutoscaler kind is part of the autoscaling version one API. It’s spec includes a min and max to set and lower the upper bounds on running replicas. The targetCPUUtilizationPercentage field sets the target average CPU percentage across the replicas. With the target set to 70%, Kubernetes will decrease the number of replicas if the average CPU utilization is 63% or below and increase replicas if it is 77% or higher.</p>
<p>Lastly, the spec also includes a scale target reference, that identifies what is actually scaling. In this case, we are targeting the app-tier deployment. We’ve added the equivalent kubectl autoscale command to achieve the same result, but we’ll stick with the manifests for everything. So let’s create the autoscaler with kubectl create file 6.2. Now we can watch the deployment until the autoscaler kicks in with the watch command. Well, would you look at that, the kernel is already updated, Kubernetes does not disappoint.</p>
<p>We can also describe the HorizontalPodAutoscaler to see what events took place. Now, it would be painful to type out pod autoscaler many times, but fortunately kubectl accept shorthand notations for resource types. So we’ll just run kubectl api dash resources for a full list of those shorthand notations. The output is sorted by the API group that appears in the third column. The lone autoscaling resource is the horizontalpodautoscalers and we can use hpa as the short name.</p>
<p>So let’s describe it with kubectl describe deployments hpa. We can see the successful rescale events and the current metrics are all below the target. We can also get the HorizontalPodAutoscaler for a quick summary of the current state with kubectl get namespace deployments hpa. The first number in the target expresses the current average CPU utilization as a percentage of the CPU request. </p>
<p>We can see that we are well below the target but we are at the minimum replicas so it won’t scale any further down. Let’s say we wanted to modify the minimum to two replicas. We could modify the manifest, save it, and then use the apply command or we could use the kubectl edit command which combines those three actions into one.</p>
<p>So let’s edit the odd autoscaler. The server side version of the manifest is presented in the vai console editor. If you haven’t used vai before, don’t worry, I’ll tell you everything we need to do. In general it’s a good idea to stick with modifying our local manifest so the changes can easily be checked into a VCs, but I want you to know that the edit command is available. You’ll notice that the server’s manifest contains additional fields that we didn’t configure. The server includes several fields automatically to help it manage resources. Type dash space one to jump the cursor down to the first occurrence of space one, which is our minReplicas field value.</p>
<p>Now press A to start editing the file. Then press your right arrow key to move the cursor after the one then press backspace two to change the minReplicas to two. Then press escape to stop editing followed by colon write quit or wq, to write to the file and quit to the editor. And Kubernetes will go out and automatically apply those changes to the HorizontalPodAutoscaler. Now you can watch the deployment with the Linux watch command. It’ll typically happen within 15 seconds which is the default period for the HorizontalPodAutoscaler to check if it should scale.</p>
<p>This wraps up our tour for autoscaling Kubernetes. To recap, Kubernetes depends on metrics with being collected in a cluster before you can use autoscaling. We accomplish that by adding the Metrics Server to the cluster. You must also declare CPU request in your deployments pod template so that autoscaling can compute each pod’s percentage CPU utilization. With those prerequisites taken care of, you can use the HorizontalPodAutoscaler. You configure it with a target CPU percentage and then min and max replicas.</p>
<p>Kubernetes will do all the heavy lifting for us, dynamically scaling our deployment based on the current state of the load. While we were doing this, we were able to also pick up the kubectl apply command, to update a resources rather than deleting and recreating it. And the edit command, which is shorthand for editing a live resource and then having it automatically applied. In the next lesson, we’re gonna wrap up our coverage over deployments by discussing how to deployments help you when deploying code or configuration changes. I’ll see you there.</p>
<h1 id="Rolling-Updates-and-Rollbacks"><a href="#Rolling-Updates-and-Rollbacks" class="headerlink" title="Rolling Updates and Rollbacks"></a>Rolling Updates and Rollbacks</h1><p>The last topic we will discuss on deployments is how updates work. Kubernetes uses rollouts to update deployments. And a Kubernetes rollout is a process of updating or replacing replicas with new replicas matching a new deployment template. Changes may be configurations such as environment variables or labels, or also code changes which result in the updating of an image key of the deployment template. In a nutshell, any change to the deployment’s template will trigger a rollout.</p>
<p>Deployments have different rollout strategies, and Kubernetes uses rolling updates by default. Replicas are updated in groups, instead of all at once until the rollout is complete. This allows service to continue uninterrupted while the update is being rolled out. However, you need to consider that during the rollout there will be pods using both the old and new configuration of the application. In such, it should gracefully handle that.</p>
<p>As an alternative deployments can also be configured to use the recreate strategy which kills all of the old template pods before creating the new ones. That, of a course, incurs downtime. So we’re going to be focusing on the rolling updates in this course. We actually have already rolled out an update in the last lesson when we added the CPE request to the app tier deployments pod template.</p>
<p>Scaling is an orthogonal concept to rolling updates. So all of our scaling events do not create roll-outs. Kubectl includes commands to conveniently check, pause, resume, and rollback rollouts. So let’s check out those now. We’ll use our deployments namespace again and focus on the app tier deployment.</p>
<p>First, we will delete the existing auto scaling configuration. Auto-scaling and rollouts are compatible, but for us to easily observe rollouts as they progress we’ll need many replicas in action. Deleting the autoscaler is going to help us with that.</p>
<p>Next let’s edit the app tier deployment with the following command. We’re gonna be jumping down to replicas and start editing them just change them after two and instead enter 10. It’ll be easier to see the raw in action with a large number of replicas. Also remove the resource request by pressing escape to stop editing. Then jumping down to resources and D three D to delete the three lines comprising the resource request. This will avoid any potential problems with scheduling the replicas if all 10 of the CPU requests can be satisfied. We’ll go ahead and write quit now. And we’re going to be watching this with the Linux watch command.</p>
<p>Now it’s time to trigger a rollout. Open the app to your deployment with Q+control+edit. From here, we can see the server added the default values for the deployment strategy. Specifically, the type is rolling update in the corresponding match surge specifies how many replicas over the desired total are allowed during a rollout. A higher surge allows new pods to be created without waiting for old ones to be deleted.</p>
<p>In the maxunavailable controls how many old pods can be to be deleted without waiting for new pods to be ready. We’ll keep the defaults of 25%. You may want to configure them if you want to trade off the impact on availability or resource utilization with the speed of the rollout. For example, you can have all of the new pods start immediately, but in the worst case you can have all of the new pods and all the old pods consuming resources at the same time effectively doubling the resource utilization for a short period.</p>
<p>With those fields out of the way, let’s trigger a rollout.This command will replace server with name cloudacademy for all of our previous pods. This is just a nonfunctional change for us but it will demonstrate the rollout functionality. So we’re go ahead and apply this with right click. Then we can immediately watch the rollout status with Q+control if we’re fast enough Q controlled rollout status streams progress updates in real time. You’ll see the new replicas coming in and old replicas going out.</p>
<p>To repeat this exercise until you see the entire flow and experiment with the number of replicas maxsurge and maxunavailable as you please. Rollouts may also be paused and resume. I’m gonna be splitting my window into two to better illustrate what is going on by entering tmax. This is a terminal multiplexer and I’m going to press control+B followed by the percent symbol to split the terminal vertically.</p>
<p>To switch between the two terminals, you can enter control+B followed by the left or right arrow. In the right terminal, I’ll prepare the same rollout status command we used before so that I can watch the status change as soon as we apply an update. And then we’ll jump over to the left terminal and edit the app tier deployment again. Let’s change the container name again by entering the following.</p>
<p>Next, we will quickly write the file to apply the changes then watch the status rollouts in the right terminal and pause the rollout mid-flight in the left terminal. Now the rollout is paused, but pausing won’t pause replicas that were created before the pausing. They will continue to progress to ready. However, there will be no new replicas created after the rollout is paused. We can use the rollout resume command for exactly that purpose. The rollout picks up right where it left off and goes about its business.</p>
<p>So I’m going to stop the terminal multiplexer now by doing control+B, and Y. So now consider you found a bug in the new revision and you needed to roll back. So kubectl has a handy command exactly for that. With kubectl, rollout, undo. This will roll back to the previous revision. You can also roll back to a specific version. You can also use kubectl rollout history to get a list of all versions and then grab the specific version and pass it into that.</p>
<p>That’s all for this demonstration of rolling updates and rollbacks. But before we move on let’s scale back the app tier to one replica to give us some more CPU resources. Deployments, and rollouts are very powerful constructs. Their features cover a large swath of use cases.</p>
<p>So let’s reiterate what we’ve covered in this lesson. We learned that rollouts are triggered by updates to a deployments template. Kubernetes uses a rolling update strategy by default. We also learned that we can pause, resume, and undo rollouts of deployments. There’s still so much more that we could do with deployments and rollouts depend on container status.</p>
<p>Kubernetes assumes that created containers are immediately ready and the rollout should continue. But this does not work in all cases. We may need to wait for the web server to accept connections. So here’s another scenario. Considering an application using a relational database, the containers may start but it will fail until a database and tables are created.</p>
<p>These scenarios must be considered to build reliable applications. This is where probes in init containers come into the picture. So we’ll take a look at integrating probes and init containers in our next two lessons.</p>
<h1 id="Probes"><a href="#Probes" class="headerlink" title="Probes"></a>Probes</h1><p>The previous lesson covered deployment rollouts. Kubernetes assumes that a Pod was ready as the container was started, but that’s not always true. For example, if the container needs time to warm up Kubernetes should wait before sending any traffic to the new Pod. It’s also possible that a Pod is fully operational but after some time it becomes non-responsive. For example, if it enters a deadlock state, Kubernetes shouldn’t send any more requests to that Pod and will be better off to restart a new Pod.</p>
<p>Kubernetes provides probes to remedy both of these scenarios and probes are sometimes referred to as health checks. The first type of probe are readiness checks. They are used to probe when a Pod is ready to serve traffic. As I mentioned before, often a Pod is not ready after its containers have just started. They may need time to warm caches or load configurations.</p>
<p>Readiness probes can monitor the containers until they are ready to serve traffic. But readiness probes are also useful long after startup. For example, if the Pod depends on an external service and as service goes down, it’s not worth sending traffic to that Pod since it can’t complete it until the external service is back.</p>
<p>Readiness probes control the ready condition of a Pod. If a readiness probe succeeds, the ready condition is true, else, it is false. Services use the ready condition to determine if the Pod should be sent traffic. In this way, probes integrate with services to ensure that traffic doesn’t flow to Pods that aren’t ready. This is a familiar concept if you’ve used a cloud load balancer. Back in instances that fail health checks are not served traffic, just as services won’t serve traffic to Pods that aren’t ready.</p>
<p>Services are our load balancers in Kubernetes. The second type of probe is called a liveness probe. They are used to detect when a Pod has entered a broken state and can no longer serve traffic. In this case, Kubernetes will restart the Pod for you. That is the key difference between these two types of probes. Readiness probes determine when a service can send traffic to a Pod because it is temporarily not ready and a liveness probe decides when a Pod should be restarted because it won’t come back to life. You declare both probes in the same way. You just have to decide which course of action is appropriate if a probe fails. Stop serving traffic or restart.</p>
<p>Probes can be declared on containers in a Pod. All of the Pod’s container probes must pass for the Pod to pass. You can define any of the following as the action probe to check the container. A simple command that runs inside of a container, an HTTP GET request or the opening of a TCP socket. The command probes succeeds if the exit code of the command is zero, else, it will fail. A GET request succeeds if the response code is between 200 and 399. A TCP socket probes succeeds if a connection can be established. By default, the probes check the Pods every 10 seconds.</p>
<p>Our objective in the hands-on part of this lesson is to test our containers using probes. Specifically, we will add readiness and liveness probes to our application. We will use the application of manifest from the deployments lesson as the base of our work in this lesson. But before we get started creating probes, let’s first crystallize the concepts by relating these probes to our application. The data tier contains one redis container. This container is alive if it accepts TCP connections. The redis container is ready, if it responds to redis commands such as get or ping.</p>
<p>There is a small but important difference between the two. A server may be alive but not necessarily ready to handle incoming requests. The API server is alive if it accepts HTTP request but the API server is only ready if it is online and has a connection to redis to request an increment, the counter. The sample application has a path for each of these probes. The counter and polar containers are live and ready if they can make an HTTP request back to the API server.</p>
<p>So let’s apply this knowledge to the deployment templates. We will go in the same order we just discussed but skip the support tier because the server demonstrates the same functionality. Let’s start by creating the probes in the namespace to isolate the resources in this lesson. Now take a look at this comparison that shows the addition of a name for the port and that the probes are the only changes to the data to your deployment. The liveliness probe uses the TCP socket type of the probe in this example, and by using a named port, we can simply write the name rather than the port number. This will protect us in the future if the port number ever changes and someone forgets to update the probe port number. Also, by setting the initial delay seconds, we give the redis server an adequate time to start.</p>
<p>We can also configure failure threshold, delays and timeouts for all probes. The default value will work for this example but you can reference Kubernetes documentation for more information on different values. Next, the readiness probe uses the exact type of probe to specify command. What this does, is runs a command inside the container similar to docker exec if you’ve used that before. The redis-cli ping command test if the server is up and is ready to actually process redis specific commands. Commands are specified as a list of strings.</p>
<p>Given the consequences of failing a liveness probe is going to be restarting a Pod. It’s generally advisable to have the liveness probe at a high delay than the readiness probe. I’ll also point out that by default three sequential probes need to fail before a probe is marked as failed, so that we have some buffer. Kubernetes won’t immediately restart the Pod the first time the probe fails, but we can configure it that way if we need to.</p>
<p>The particular delay depends on our application and how long it reasonably requires to start up. Five seconds should be more than enough to start checking readiness. And by default, we only need to pass a single probe before any traffic is sent to the Pod. Having the readiness initial delay too high will prevent Pods that are able to handle traffic from receiving any. So let’s create the new and improved data tier.</p>
<p>Now we can watch the GET output for the deployment to observe the impact of the probes. Note the ready column. This will show one of one replicas when the readiness check passes. With the watch option, new changes are appended to the bottom of the output. So we can see from the bottom line, the Pod transitions to the ready state after the number of seconds in the age column in the bottom line of the output. Watch the deployment for a while to make sure things are running smoothly. If no new lines appear, there are no changes and everything has stayed up and running. However, if something did go awry, I’d recommend using a combination of the described and logs commands to debug the issue.</p>
<p>Unfortunately, failed probe events don’t show in the events output but you can use the Pod restart counter as an indicator of failed liveness probes. Logs are always the best direct way to get at them. We will add some debug logging to the service so that you can see all the incoming probe requests after this. Onto the app tier. Notice that the debug environment variable has been added which will cause all the service requests to be logged.</p>
<p>Note that this environment variable is specific to the sample application and not for general purpose settings. Further down, probes are declared and this time they are HTTP GET probes. They send request to end points built at the server specifically for checking its health. The liveness probe endpoint does not actually communicate with redis. It’s a dummy that will always return 200 okay as its response for every request. Your readiness probe endpoint checks that the data tier is available. We’re also gonna be setting the initial delay seconds so the process has adequate time to start.</p>
<p>So let’s create the app tier deployment. And we’ll subsequently watch that deployment to verify containers are alive and ready. It may take some time to start the containers and wait for the initial delay seconds on the readiness probe. But after a short delay, the replica will be ready. We can now stream the logs to see what’s happening behind the scenes.</p>
<p>First, we’re to be getting the Pods to find a Pod in the deployment, then use kubectl logs with the dash f option to follow the log stream. And I’ll use cut to filter down what is going on. We can see that Kubernetes is firing both probes in 10-second intervals. With the help of these probes, communities can take Pods out of service when they aren’t ready and restart them when they enter a broken state.</p>
<p>To summarize what we saw in this lesson, containers in Pods can declare readiness probes to allow Kubernetes to monitor when they’re ready to serve traffic and when they should temporarily be taken out of service. Containers in Pods can declare a liveliness probes to allow Kubernetes to detect when they have entered a broken state and the Pod should be restarted.</p>
<p>Both types of probes have the same format and manifest files and can make use of either command, HTTP GET or TCP socket probe types. Remember that probes kick in after containers are started. If you need to test or prepare things before the container start, there is a way to do that as well. And that is the role of init containers and it is a subject of our next lesson. I’ll see you there.</p>
<h1 id="Init-Containers"><a href="#Init-Containers" class="headerlink" title="Init Containers"></a>Init Containers</h1><p>Sometimes you need to perform some tasks or check some prerequisites before a main application container starts. Some the examples include waiting for a service to be created, downloading files, or dynamically deciding which port the application is going to use. The code that performs those tasks could be crammed into the main application, but it is better to keep a clean separation between the main application and supporting functionality to keep the smallest footprint you can for the images. However, the tasks are closely linked to the main application and are required to run before the main application starts.</p>
<p>So Kubernetes provides us with an init container as a way to run these tasks that are required to complete before our main container starts. Pods may declare any number of init containers. They run in a sequence in the order they are declared. Each init container must run to completion before the following init container begins. And once all of the init containers have completed the main containers in the pods can start.</p>
<p>Init containers use different images from the containers in the pod, and this can provide some benefits. They can contain utilities that are not desirable to include in the actual application image for security reasons. They can also contain utilities or custom code for setup that is not present in the application image. For example, there is no need to include utilities like sed or awk or dig in an application image if they are only used for setup.</p>
<p>Init containers also provide an easy way to block or delay the start-up of an application until some pre-conditions are met. They are similar to readiness probes in this sense but only run at pod startup. It can perform other useful work. All of these features together make init containers a vital part of the Kubernetes toolbox. There is one more important thing to understand about it init containers. They run every time a pod is created.</p>
<p>This means they will run once for every replica in a deployment. And if a pod restarts, to say, due to failed live-ness probes the init containers would run again as part of that restart. Thus, you have to assume that init containers run at least once. This usually means that init containers should be unique. Running it more than once should have no additional effect.</p>
<p>Let’s add an init container to our app tier that will wait for Reddis before starting any application. We’ll see that the init containers have the same field as regular containers in a pod spec. The one exception is init containers do not support readiness probes because they must run to completion before the state of the pod can be considered ready. You will receive an error if you try to include a readiness probe in an init container.</p>
<p>Let’s see what the manifest looks like in our case. We’ll just be updating the app to your deployment, so we won’t make a new namespace. I’m comparing the deployment from the previous lesson with our new version with init containers. You can see that the fields are the same as what we have seen with regular containers. I’ve used the same image as the main application for simplicity, and it has everything we need in it. The command field is used to override the image’s default entry point command.</p>
<p>For this init container, we want to run a script that waits for a successful connection with Reddis. The script is already included in the image and is executed with the NPM run script await Reddis command. This command will block until the connection is established with the configured Reddis URL provided as in an environment variable.</p>
<p>Now let’s apply those changes to the existing deployment. After that, describe the deployments pod. And observe the event log, as it now shows the entire lifecycle with init containers. The await Reddis init container runs the completion before the server container is created. You can also view the logs of init containers using the usual logs command and specifying the name of the init container as the last argument after the pod name. This is specifically important when debugging init containers which prevents the main container from ever being created.</p>
<p>This concludes our tour of init containers. They give you another mechanism for controlling the lifecycle of pods. You can use them to perform some tasks before the main containers have an opportunity to start. This could be useful for checking preconditions, such as checking that depended upon services are created or preparing dependent upon files. The files use case requires knowledge of another Kubernetes concept, namely volumes which can be used to share files between containers. We’ll discuss all we should know about volumes in the next lesson. So continue on when you’re ready.</p>
<h1 id="Volumes"><a href="#Volumes" class="headerlink" title="Volumes"></a>Volumes</h1><p>Containers in a pod share the same network stack, but each has their own file system. It could be useful to share your data between containers. For example, having an init container prepare some files that the main container depends upon. The file system of containers are also limited to the lifetime of the container, so this could present some undesirable effects. For example, if the data tier container we are using in our examples crashes or fails a likeness probe, it will be restarted, and all of our data will be lost forever.</p>
<p>So this lesson is gonna cover the different ways Kubernetes hands non-ephemeral data that bring data from containers, Kubernetes volumes, and Kubernetes persistent volumes. Our goal for this lesson is to deploy the data tier from our sample application, using a persistent volume so the data can outlive the data tier pod. Again, this lesson builds on the code from the previous lessons, so let’s first discuss more about the options for storing persistent data and then apply them to our data tier.</p>
<p>Kubernetes includes two different data storage types. Both are used by mounting a directory in one container, and then that could be shared by containers in the same pod. Pods can also use more than one volume or persistent volume. Their differences are mainly in how their lifetime is managed. One type exists for the lifetime of a particular pod and the other is independent from the lifetime of the pods. Volumes are tied to a pod and their life cycle. Volumes are used to share data between containers in a pod and to tolerate container restarts.</p>
<p>Although you can configure volumes to use durable storage types that survive pod deletion, you should consider using volumes for non-durable storage that is deleted when the pod is deleted. Default type of volume is called emptyDir and it creates an initially empty directory on the node running the pod to back the storage used by the volume. Any data written to the directory remains if a container in the pod is restarted.</p>
<p>Once the pod is deleted, the data in the volume is permanently deleted. It’s worth noting that since the data is stored on a specific node, if a pod is rescheduled to a different node, that data will be lost. If the data is too valuable to lose when a pod is deleted or rescheduled, you should consider using persistent volumes. Persistent volumes are independent from the lifetime of pods and are separately managed by Kubernetes. They work a little bit differently.</p>
<p>Pods may claim a persistent volume and use it throughout their lifetime. The persistent volumes will continue to exist outside of their pods. Persistent volumes can even be mounted by multiple pods on different nodes if the underlying storage supports multiple readers or writers. Persistent volumes can be provisioned statically in advanced by a cluster admin or dynamic for a far more flexible self-serve use case.</p>
<p>Pods must make a request for storage before they can use a persistent volume. The request is made using a persistent volume claim, or PVC. A PVC declares how much storage the pod needs, the type of persistent volume, and the access mode. The access mode describes the persistent volume and whether it is mounted in read-only, read-write, or read-write many. There are three supported access modes to choose from, read-write once, read-only many, or read-write many. If there isn’t a persistent volume available to satisfy the claim and dynamic provisioning isn’t enabled, the claim will stay in a pending state until such persistent volume is ready. The persistent volume claim is connected to the pod by using a regular volume with the type set to persistent volume claim.</p>
<p>Both volumes and persistent volumes may be backed by a wide variety of volume types. It is usually preferable to use persistent volumes for more durable types and volumes for more ephemeral storage needs. Durable volume types include the persistent disks in many cloud vendors, such as Google Cloud Engine persistent Disks, Azure Disks, and Amazon Elastic Block Store. There’s also support for more generic volume types, such as network file system or NFS and iSCSI. That is quite a lot to take in, but everything should solidify with an example.</p>
<p>So our objective is to use a persistent volume for the sample application’s data tier, since we want the data to outlive the pod. In our example, the cluster has an Amazon Elastic Block Store volume statically provisioned and ready for us to use. To see dynamic provisioning in action, I’d encourage you to complete the lab on CloudAcademy entitled “Deploy a Stateful Application in a Kubernetes Cluster.”</p>
<p>Before we get into volumes, I want to cement the issue we are trying to solve. We can illustrate the issue of a pod computer losing data when they restart by forcing a restart of the data tier pod. First off, let’s look at the counter that’s been running since our deployments lesson. That is the value of our counter at the moment, and every second, it will keep increasing.</p>
<p>Now, if I force the pod to be restarted, we can observe the impact on the counter. One way to do that is to kill the Redis process, which’ll cause the data tier container to exist, and the data tier pod will automatically restart. We can use the exec command and run a command inside of the container the same way Docker exec does. So let’s open a bash shell inside of that container.</p>
<p>The change of a command prompt tells us we’re in the container now. We can now use the kill command to stop the main process of the container. But what is that ID? The ID of the main process, which is Redis in this case, will always be one, since it is the first process that runs inside of the container. We can see the command prompt change back, since the container terminated, so our shell was also terminated.</p>
<p>Now we can look at the counter value through the poller logs again and see that it is much lower than before because the data tier was completely wiped out when the pod restarted. This is what we want to avoid. So let’s create our new namespace and get on to the volumes. Now on to the data tier. There are three additions to the manifest, a persistent volume, a persistent volume claim, and a volume to connect the claim to the pod.</p>
<p>First is our persistent volume. This is the raw storage where the data is ultimately written to by the pod’s container. It has a declared storage capacity and other attributes. Here we’ve allocated one gibibyte. The access mode of read-write once means this volume may be mounted for reading and writing by a single node at a time. Note that it is a limit on the node attachment, not pod attachment.</p>
<p>Persistent volumes may list multiple access modes in the claim that specifies the mode it requires. The persistent volume can only be claimed in a single access mode at any time. Lastly, we have an AWS Elastic Block Store mapping, which is specific to the type of storage backed by the PV. You would use a different mapping if you were not using an EBS volume for storage. And the only required key for AWS’s Elastic Block Store is the volume ID, which is uniquely identified by the EBS volume. It will be different in your environment than mine, so I’ve added an insert volume ID placeholder that we will place before we recreate the PV. </p>
<p>Next, we have the persistent volume claim. The PVC spec outlines what it is looking for in a persistent volume. For a persistent volume to be bound to a PVC, it must satisfy all the constraints in the claim. We are looking for a persistent volume that provides the read-write once access mode and has at least 128 mebibytes of storage. The claim request is less than or equal to the persistent volume’s capacity and the access mode overlaps with the available access modes in the persistent volume. This means that the PVC request is satisfied by our persistent volume and will be bound to it.</p>
<p>Lastly, the appointment’s template now includes a volume which links the PVC to deployments pod. This is accomplished by using the persistent volume claim mapping and setting the claim name to the name of the persistent volume, which is data tier volume claim. You will always use persistent volume claim when working with PVs. If you wanted to use an ephemeral storage volume, you would replace it with an emptyDir mapping or other types that don’t connect to a persistent volume.</p>
<p>Volumes can be used in the pods containers and init containers, but they must be mounted to be available in the containers. The volume mounts list includes all the volume mounts for given container. The mount pass for different containers could be different even if the volume is the same. In our case, we only have one, and we are mounting the volume at slash data, which is where the Redis is configured to store its data. This will cause all of the data to be written to the persistent volume.</p>
<p>Now we are left with replacing the volume ID placeholder with the actual ID of the Amazon EBS volume. You can get it from the EC2 console in your browser, but we’ll just use the AWS CLI in this example. The volume can be obtained from the AWS EC2 describe command as follows. The full command is available to copy in the introduction to Kubernetes playground lab. The filter only selects the persistent volume which is labeled with the type equals PV tag, and the query outputs only the volume ID of the property volume.</p>
<p>The introduction to the AWS CLI on CloudAcademy explains this more so in greater detail. We only need to get the volume ID in this case. Then we can use the stream editor or set to substitute the occurrence of insert volume ID with the volume ID stored in vol_id. And with that, we are ready to create the data tier using a persistent volume. We’ll also create the app and support tiers, which don’t have anything new compared to the previous versions.</p>
<p>Let’s get the persistent volume claim, which has a short name of PVC in kubectrl, to confirm the claims request is satisfied. The status of bound confirms that the persistent volume claim is bound to the persistent volume. Now, if we describe the data tier pod, we’ll see that the pod initially failed to schedule because the claim needs to wait a while before it is bound to the persistent volume.</p>
<p>Once it is bound, the pod is scheduled and we can see a successful attachment with the volume event. Not only can our new design tolerate data tier pod container restart, but the data will persist even if we delete the entire data tier deployment. If everything goes to plan, we should be able to recover the Redis data if we then replace the deployment. That is because the deployment template is configured to use the PVC, and the PVC is still bound to the persistent volume storing our original Redis data.</p>
<p>So let’s verify all of this. Before we delete the data tier deployment, let’s get the last log line from the poller to see where our counter is at. If we delete the deployment and then replace it, we should see a number higher than this if the data is persistent. So let’s do that. And now we’re going to confirm that there are no data tier pods left running, and we’ll now recreate the data tier deployment.</p>
<p>Now, it takes a couple of minutes for all the readiness checks to start passing again and for some old connections to time out. This is mainly a side effect of the example application not being particularly good at handling this type of situation. So after a minute or two, we can get the poller’s last log, and voila, the counter has kept on ticking upward from where we left off before deleting the deployment.</p>
<p>Our persistent volume has lived up to its name. This concludes our lesson on volumes. We’ve covered volumes, persistent volumes, and persistent volume claims. In our example, we’ve shown how to use a persistent volume to avoid data loss by keeping the data independent from the life cycle of the pod or the pod’s volume. We also saw how kubectrl exec allows us to run commands in existing containers when we demonstrated how container restarts cause data loss when volumes aren’t used. You now have a solid foundation for volumes and persistent volumes.</p>
<p>In our next lesson, we’re gonna be covering two other useful Kubernetes features that you should keep in your toolbox. They also have a nice tie-in with volumes. There’s a few more lessons to go. Keep it up and I’ll catch you in the next one.</p>
<h1 id="ConfigMaps-and-Secrets"><a href="#ConfigMaps-and-Secrets" class="headerlink" title="ConfigMaps and Secrets"></a>ConfigMaps and Secrets</h1><p>Up until now, the deployment template has included all of the configuration required by Pod containers. This is a big improvement over storing the configuration inside the binary or container image. Having configuration in the Pod spec, also makes it a lot less portable. Furthermore, if the configuration involves sensitive information, such as passwords, API keys, this also presents a security issue.</p>
<p>So Kubernetes provides us with ConfigMaps and Secrets, which are Kubernetes resources that you can use to separate the configuration from the Pod specs. This operation makes it easier to manage and change configurations. It also makes for more portable manifests. ConfigMaps and Secrets are very similar and used in the same way when it comes to Pods. One key difference is that Secrets are specifically for storing sensitive information. Secrets reduce the risk of their data being exposed. However, the cluster admin also needs to ensure that all the proper encryption and access control safeguards are in place to actually consider Secrets being safe. We’ll focus on Secrets and leave out the security details from this introductory course.</p>
<p>Another difference is that Secrets have specialized types for storing credentials, such as requiring to pull images from registries. They also are good at storing TLS private keys and certificates. But I’ll refer you to the official documentation we need to make use of those capabilities. ConfigMaps and Secret store data as key value pairs. Pods must reference ConfigMaps or Secrets to use their data. Pods can use the data by mounting them as files through a volume or as environment variables. We’ll see examples of these in the demo.</p>
<p>We’re going to be using a ConfigMap to configure Redis using a volume to mount a Config file will use and a Secret to inject sensitive environment variables into the app tier. First, let’s create a Config namespace for this demo. With key control create -f 10.1.namespace.yaml.</p>
<p>Now let’s see how the ConfigMap manifest looks. First notice that there is no spec rather than we have key value payers of the ConfigMap stores and they’re under a mapping named data. Here we have a single key named Config. You can have more than one but one is enough for our purpose. The value of Config is a multiline string that represents the file contents of a Redis configuration file. The bar or pipe symbol after ConfigMap is Yaml for starting a multiline string and causes all the following lines to be the value of Config including the Redis Config comment. The configuration files set the TCP keep alive in max memory of Redis. These are arbitrarily chosen for this example. Separating the configuration makes it easy to manage configuration separately from the Pod spec. we will have to make some initial changes to the Pod to make use of the ConfigMap but after that, the two can be managed separately.</p>
<p>Let’s take a look at the updated data tier. I’m comparing what the data tier from our probes lesson which doesn’t include the persistent volume to avoid not being able to satisfy the persistent volume claim. Starting from the volumes, a new ConfigMap type of volume is added and it references the Redis config ConfigMaps we just saw. Items declare which key value pair we want to use from the ConfigMaps. We only have one in our case, and that is Config.</p>
<p>If you have multiple environments, you could easily do things like referencing a dev configuration in one environment and a production configuration in another. The path sets the path of the file that will be mounted with a Config value. This is relative to the mount point of the volume. Up above in the container spec, the volume mounts mapping declares the use of the Config volume but amounts of that etc Redis. So the full absolute path of the Config path, will be etc Redis, Redis Comf.</p>
<p>The last change that we need is to use a custom command for the container so that Redis knows to load the Config file when it starts. We do that by setting the Redis server, etc Redis comp as the command. With this setup, we can now independently configure Redis without touching the deployment template. As a quick side note, before we create the resources, if we’re dealing with the Secret rather than a ConfigMap, the volume type would be Secret rather than ConfigMap. And the name key would be replaced with secret name. Everything else would be the same.</p>
<p>Let’s create the resources. Now let’s start a shell in the container using Q control exec to inspect the effect of our ConfigMap. Start by cutting out the contents of the etc Redis, Redis Conf file. See that the contents match the ConfigMap value that we specified. Now to prove that Redis actually loaded the Config, we can output TCP, keep alive configuration value and make sure it matches the two 40 value in the file. And there we have it, separation of configuration and Pod spec is complete, so let’s exit out of the container.</p>
<p>Before we move on, I wanna highlight how changes to the ConfigMap interact with volumes and deployments. So let’s use Kube control edit to update the ConfigMap. And let’s change the TCP keep alive value from 240 to 500. Within around a minute, the volume will reflect the change we made to the ConfigMap. That is pretty slick, but Redis only loads the configuration file in start-ups so it won’t impact the running Redis process. And because we never updated the deployments template, we’d never triggered a rollout.</p>
<p>So let’s confirm the TCP keep alive Redis hasn’t been updated using the Redis CLI. There is something to keep in mind when you separate the configuration from the Pod spec to cause the deployments Pods to restart and have Redis supply the new configuration changes, we can use kube control rollout, namespace Config, restart deployment data-tier. This will cause a rollout using the current deployment template, and when the new Pod start, the Redis containers will use the new configuration. We can verify that with the Redis CLI.</p>
<p>Now we can quickly see how secrets work and see the similarities they have with ConfigMap. We will add a secret to the app here using an environment variable. It won’t have any functional impact but it will show the idea. Here is our Secret manifest, I mentioned upfront that you usually don’t wanna check in secrets to source control given their sensitive nature. It makes more sense to have secrets managed separately. You could still use manifest files as we are here, or the Secret could be created directly with kube control.</p>
<p>The command at the bottom of the file shows how to create the same Secret without a manifest file. Focusing on the manifest file, we can see that itself has a similar structure as our ConfigMap, except for the kind being Secret rather than ConfigMap. And Secrets can use a string data mapping in addition to the data one we use in our ConfigMap. As part of the effort to reduce the risk of Secrets being exposed in plain text, they are stored as base-64 encoded strings and Kubernetes automatically decodes them when using a container. I also have to point out that basics and foreign coding does not really offer any additional security. It’s not encrypting the values, and anyone can decode base-64, so to continue to treat the encoded strings as sensitive data.</p>
<p>With that cautionary statement out of the way, the stream data mapping allows you to specify Secrets with first encoding because kubernetes will coding them for you. It’s simply a convenience. If you use the data mapping, you must specify in coded values. In the API key secret is the one that we will use in the app tier, but I’ve included the encoded and decoded key value pairs to illustrate the basics for encoding.</p>
<p>In the data mapping, the encoded value is hello, base-64 encoded. So let’s create the Secret to see this. Now, if we describe the Secret, we can only see the keys. The values are hidden as part of the best effort to shield Secret values. We can see what the values are with kubecontrol edit. From here, we can see that string data mapping is not actually stored. The values are based 64 encoded and then added to the data mapping. The decoded value we entered in string data was hello, but now it is the base-64 encoded string beginning with AGV.</p>
<p>Shifting over to the app tier deployment, the API key environment variable is added. If value from mapping is used to reference it from the source for the value. Here, the source is Secret, so the secret key ref is used. If you need to get the environment variable from a ConfigMap rather than a Secret, you would use the ConfigMap key ref instead of Secret key ref. The name is the name of the Secret, and the key is the name of the key and the Secret you want to get the value from.</p>
<p>So let’s create the app tier now. And we can use the ENV command and the container dump all the environment variables. We can find the API key variable amid the wash of variables and observe the value as the decoded value that we entered in string data of our Secret manifest file and not encoded value. There is no need to decode the value inside of the container. I’ll just mention before wrapping up, that just like with using volumes to reference Secrets or Config maps, you should restart a roll out to how the deployment pods restart with a new version of the environment variables. Environment variables do not update on the flight like volumes. So actively managing the rollout is must.</p>
<p>This concludes the lesson on ConfigMaps and Secrets. So let’s recap what we’ve learned. ConfigMaps and Secrets are used for separating configuration data from Pod specs or what would otherwise be stored in container images. ConfigMaps and Secrets both store groups of key and value data. Secret should be used when storing sensitive data. Both can be accessed in the Pod containers by the referencing them using volumes or environment variables.</p>
<p>This was our last hands-on lesson for course. And the last hands-on lessons have prepped you to get started with managing and deploying applications in Kubernetes. For our next lesson, we’re gonna be highlighting some of the areas that the Kubernetes ecosystem is particularly strong with and that I think you should know about. You’re almost at the finish line, so join me in our next lesson and we’ll cross it together.</p>
<h1 id="Kubernetes-Ecosystem"><a href="#Kubernetes-Ecosystem" class="headerlink" title="Kubernetes Ecosystem"></a>Kubernetes Ecosystem</h1><p>The Kubernetes ecosystem is a very vibrant and healthy mixture of tools that can help you get more done and work more efficiently with Kubernetes. So I wanted to give you a sense of what’s happening around the core of Kubernetes. There are really way too many tools and topics to choose from, so I’ve only selected a few that are popular and worth knowing about. So, just know that I’m touching the surface of this ocean. </p>
<p>The first tool that I’ll mention is Helm. Helm is Kubernetes’ package manager. You write packages called charts. Then you use the helm CLI to install and upgrade charts as releases on your cluster. Charts contain all the resources like services and deployments required to run a particular application. Helm charts make it easy to share and complete applications built for Kubernetes. Helm charts can also be found on the Helm hub, similar to how you would find Docker images on Docker hub. </p>
<p>As an example of how you might use Helm. In our sample application, we used Redis for our data tier. So rather than build the data tier up from scratch and managing the resources ourselves we could take advantage of the register charts available for home. There is a highly available Redis chart that comes from the single point of failure in our example application. Charts can be installed with just a single Helm CLI command. Using available charts for running common applications can really free you up to focus on the applications that are core to your business. So definitely take a look at what’s available before deciding to roll your own solution.</p>
<p>Here’s another example. You can create a chart for your entire microservice application and pack up onto the services, deployments, everything and then subsequently publish it so other members of your organization or the public could use it. The next tool I want to mention is Kustomize with a K. Kustomize allows you to customize YAML manifests in Kubernetes. It can help you manage the complexity of your applications running inside of Kubernetes.</p>
<p>An obvious example is managing different environments such as tests and stage. We saw how we could use config maps and secrets to help with that. But Kustomize makes it even easier. Kustomize works by using customization dot YAML file that declares rules for customizing or transforming resource manifest files. The original manifests are untouched and remained usable as they are, which is an, a massive benefit compared to other tools that required templating in the manifest, rendering them unusable on their own.</p>
<p>Some examples of the kinds of rules you can create with Kustomize are generating config maps and secrets from files. In our data tier we had to write the contents of the Redis config file instead of the config map data. With Kustomized you can generate it directly from the config file rather than trying to keep track of the config file and config map and keep them tied together and in sync. You can also configure common fields across multiple resources. For example, you can set the namespace, labels, annotations, and name prefixes and suffixes using Kustomize itself. That makes it easy to customize around your organization’s conventions without polluting the original manifest files. The original manifest remain pristine and easy to share, and also reusable in other situations.</p>
<p>In our example, we had to keep our tier labels and prefixes manually synchronized across all of the resources. And then we had to specify the namespace at the command line every time to avoid hard coding in any space in the manifest. Kustomize can contain all of that complexity for us. The other thing that Kustomize can do is apply patches to any field in a manifest. Kustomize allows you to find a base group of resources and apply an overlay to customize to a base. This is an easy way to manage separate environments by applying a dev name prefix and a label for development environments.</p>
<p>Kustomize has been directly integrated with kubectl since Kubernetes 1.14. The kubectl customized command prints the customized resource manifests with customization defined in customization dot YAML. To accept the customization and then realize them in your cluster you can include the –kustomize or - k option to cube control create or apply.</p>
<p>The next one I’d like to discuss is Prometheus. Prometheus is an open source monitoring and alerting system. Prometheus’s built on top of many components, but at its core is a server for pulling in time series metric data and storing it. Prometheus was originally inspired by an internal monitoring tool at Google called borgmon. Similar to how Kubernetes itself was inspired by the board project at Google. Given that history it should come as no surprise that Prometheus is the de facto standard solution for monitoring Kubernetes.</p>
<p>Kubernetes components supply all their own metrics and Prometheus format makes it easy to integrate. You can collect a lot more metrics in Prometheus in the basic metric server we used in this course. That includes metrics outside of Kubernetes. There’s also adapters that allow Kubernetes to get metrics from Prometheus so you can do things like auto-scale pods based on custom metrics in Prometheus instead of only the CPU utilization that we saw in the course. </p>
<p>Prometheus has some built in options for visualization but it is commonly paired with Grafana to to create visualizations and dashboards. Prmoetheus also lets you define alert rules to send out notifications. It’s incredibly easy to install Prometheus in a cluster, and one way to do it is by using a Helm chart. </p>
<p>I’ll round out our talk about all these tools with two members of the Kubernetes ecosystem that relate to the types of applications that you deploy on top of Kubernetes. The first is Kubeflow. Kubeflow is aimed at making deployment of machine learning workloads on Kubernetes simple, scalable, and portable. </p>
<p>Kubeflow is a complete machine learning stack. You can use it for complete end to end machine learning including building models, training them, and serving them all within Kubeflow. Being built on Kubernetes, you can deploy it anywhere and get all of the nice features that Kubernetes provides like auto-scaling. Definitely check out Kubeflow if your requirements involve machine learning. </p>
<p>And the last one is Knative. Knative is a platform built on top of Kubernetes for building, deploying, and managing serverless workloads. Serverless has gained a lot more momentum because it allows developers and companies to focus more on the code and less on the servers that run it. This trend started with AWS Lambda which is synonymous with serverless. However, as the industry shifts to multi-cloud and avoiding vendor lock-in solutions built on top of Kubernetes can be deployed anywhere. This gives you the portability that you would get with containers, but for your entire serverless platform. Knative is not the only game in town when it comes to serverless but it does have the support of industry heavyweights like Google, IBM, and SAP. </p>
<p>This concludes our lesson about the Kubernetes ecosystem. We only touched on a few topics but I hope you can see the breadth of the ecosystem. I hope you can use some of these tools that we’ve seen but know that there’s a lot more to explore on your own.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This is the last lesson in this course. I wanna congratulate you for completing this course in Kubernetes. In this lesson, I’m gonna be summarizing the high-level learning outcomes you’ve achieved and gonna be providing you with the next step for your Kubernetes journey. We’ve covered a lot of ground together. We started by explaining what Kubernetes is in our overview lesson. We followed that up with a summary of the architecture of Kubernetes and some of your options for deploying Kubernetes itself. </p>
<p>Next, we discussed the options available to you for interacting with Kubernetes. No doubt you’ve gotten to know kubectl much better than since when we first introduced it. Then we moved into the technical basics and built an application from a single pod all the way up to a self-healing, auto-scaling multi container, multi-tier application. Along the way, we’ve learned the following, Kubernetes terminology, how any number of deployments with service discovery work. Triggering, pausing, resuming, restarting, and undoing deployment roll-outs. Monitoring container liveness and readiness with probes. Preparing pods within init containers. How to configure persistent data storage which involves persistent volumes, persistent volume claims in volumes. And lastly, how to separate configuration and sensitive data from pod specs with containers using ConfigMaps and Secrets.</p>
<p>These learning outcomes should give you the confidence to start deploying applications with Kubernetes. The previous lesson took a step back and allowed you to see some of the exciting tools available to you in the vibrant Kubernetes ecosystem. So now that you’ve built a solid foundation in Kubernetes, where should you go next? Well, I’ve got a few recommendations. Cloud Academy has a lot more content on Kubernetes. In addition to the Kubernetes learning path, I’d encourage you to take a look at the certified Kubernetes administrator learning path or the certified Kubernetes application developer learning path. Both get into more details and describe more about topics that we’ve already covered but just didn’t have the time to fully explore. You can decide which one is best for you based on your current role or the kind of role you want to pursue, whether that’s more on the administrative side or more on the developer side. Both learning paths include tips on how to become a kubectl ninja and do things like generating manifests and explaining field values amongst other tips.</p>
<p>I’d also recommend that you convert your own application to run on Kubernetes. We did a similar exercise in this course, but I recommend taking an application that you know well and modeling it out to be a Kubernetes application. This will reinforce everything we have learned, and it can help you find different solutions to already existing challenges. You can also try to package your solution as a Helm chart if you want something you can share with others.</p>
<p>My last recommendation is to participate in the Kubernetes community. There are many ways to get involved. And on GitHub, you can report issues, help others solve their own issues, contribute updates and improvements to documentation, or submit pull requests for new features. If there’s a specific area of Kubernetes you are interested in, you can consider joining a special interest group, which is how Kubernetes organizes the community. There are also conferences, Slack channels, and Google groups to follow what’s happening so you can connect with other members of the community.</p>
<p>KubeCon is the flagship conference and is hosted virtually every year, and when available, in person. And with that, our journey through Kubernetes comes to an end. Please share your feedback so I can continue to improve and deliver just what you’re looking for. It may be the end of this course, but it’s only the beginning of a long and productive relationship with Kubernetes. Thank you for taking this course. And until next time, I’m Jonathan Lewey with Cloud Academy.</p>
<h1 id="1Introduction"><a href="#1Introduction" class="headerlink" title="1Introduction"></a>1<strong>Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/intro-to-k8s">Course GitHub repo</a></p>
<h1 id="11Autoscaling"><a href="#11Autoscaling" class="headerlink" title="11Autoscaling"></a>11<strong>Autoscaling</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/metrics-server">Metrics Server GitHub repo</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/19/CKAD-Certified-Kubernetes-Application-Developer-CKAD-Exam-Preparation-1/" rel="prev" title="CKAD-Certified-Kubernetes-Application-Developer-CKAD-Exam-Preparation-1">
      <i class="fa fa-chevron-left"></i> CKAD-Certified-Kubernetes-Application-Developer-CKAD-Exam-Preparation-1
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/19/CKAD-Introduction-to-Kubernetes-Playground-3/" rel="next" title="CKAD-Introduction-to-Kubernetes-Playground-3">
      CKAD-Introduction-to-Kubernetes-Playground-3 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kubernetes-Overview"><span class="nav-number">2.</span> <span class="nav-text">Kubernetes Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deploying-Kubernetes"><span class="nav-number">3.</span> <span class="nav-text">Deploying Kubernetes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kubernetes-Architecture"><span class="nav-number">4.</span> <span class="nav-text">Kubernetes Architecture</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Interacting-with-Kubernetes"><span class="nav-number">5.</span> <span class="nav-text">Interacting with Kubernetes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pods"><span class="nav-number">6.</span> <span class="nav-text">Pods</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Services"><span class="nav-number">7.</span> <span class="nav-text">Services</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multi-Container-Pods"><span class="nav-number">8.</span> <span class="nav-text">Multi-Container Pods</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Service-Discovery"><span class="nav-number">9.</span> <span class="nav-text">Service Discovery</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deployments"><span class="nav-number">10.</span> <span class="nav-text">Deployments</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Autoscaling"><span class="nav-number">11.</span> <span class="nav-text">Autoscaling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Rolling-Updates-and-Rollbacks"><span class="nav-number">12.</span> <span class="nav-text">Rolling Updates and Rollbacks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Probes"><span class="nav-number">13.</span> <span class="nav-text">Probes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Init-Containers"><span class="nav-number">14.</span> <span class="nav-text">Init Containers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Volumes"><span class="nav-number">15.</span> <span class="nav-text">Volumes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ConfigMaps-and-Secrets"><span class="nav-number">16.</span> <span class="nav-text">ConfigMaps and Secrets</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kubernetes-Ecosystem"><span class="nav-number">17.</span> <span class="nav-text">Kubernetes Ecosystem</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-number">18.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1Introduction"><span class="nav-number">19.</span> <span class="nav-text">1Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#11Autoscaling"><span class="nav-number">20.</span> <span class="nav-text">11Autoscaling</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
