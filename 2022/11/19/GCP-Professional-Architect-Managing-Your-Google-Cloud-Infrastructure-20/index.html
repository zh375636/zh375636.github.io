<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Course IntroductionWelcome to “Managing Your Google Cloud Infrastructure”. I’m Guy Hummel and I’ll be showing you how to keep your cloud systems running well. This course is about how to maintain yo">
<meta property="og:type" content="article">
<meta property="og:title" content="GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20">
<meta property="og:url" content="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="Course IntroductionWelcome to “Managing Your Google Cloud Infrastructure”. I’m Guy Hummel and I’ll be showing you how to keep your cloud systems running well. This course is about how to maintain yo">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T04:14:22.000Z">
<meta property="article:modified_time" content="2022-11-20T23:36:24.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:22" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:22-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:36:24" itemprop="dateModified" datetime="2022-11-20T19:36:24-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to “Managing Your Google Cloud Infrastructure”. I’m Guy Hummel and I’ll be showing you how to keep your cloud systems running well.</p>
<p>This course is about how to maintain your cloud infrastructure after you have implemented it. Although you can set up Google Cloud to automate many operations tasks, you will still need to monitor, test, manage, and troubleshoot it over time to make sure your systems are running properly.</p>
<p>To get the most from this course, you should know the fundamentals of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, such as how to create virtual machine instances and use Cloud Storage. If you need a refresher, then you can take our Google Cloud Platform: Fundamentals course.</p>
<p>You should also have experience with performing operations tasks, especially working with Linux. It’s also helpful to have some programming experience, although just knowing the basics of a typical programming language should be enough.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>We’ll start by going through all of the components of the Operations suite to monitor, log, report errors, debug, and trace your applications.</p>
<p>Then I’ll show you how to test your infrastructure to see how it performs under difficult conditions, including heavy load, instance failures, and cyber attacks.</p>
<p>After that, you’ll see how to get data into Cloud Storage and then how to keep it under control with lifecycle management. You’ll also learn how to optimize your Cloud SQL and Cloud CDN configurations.</p>
<p>Finally, we’ll wrap up with how to troubleshoot instance startup failures, SSH errors, and network issues.</p>
<p>If you’re ready to learn how to tame your Google Cloud infrastructure, then let’s get started.</p>
<h1 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h1><p>After you’ve implemented your infrastructure in Google Cloud Platform, the first thing you’ll want to do is set up a monitoring system that will alert you when there are major problems. The easiest way to do this is to use Cloud Operations (formerly known as Stackdriver), which is Google’s powerful monitoring, logging, and debugging tool.</p>
<p>To get to it, select “Monitoring” from the console menu. The first time you bring up Monitoring in a project, it’ll take a while to set up. I’ll fast-forward.</p>
<p>Here’s where you’ll find the instructions for installing the Monitoring Agent (and the Logging Agent). You don’t need to install the agent to be able to use Monitoring, but if you do install it, you can get more information about an instance, such as about the third-party software running on it. We don’t need to install it yet, so we’ll leave that until later.</p>
<p>Suppose you want to monitor a web server and get notified if it goes down. First, you need to create an Uptime Check. </p>
<p>For the title, let’s call it “Example”. Click “Next”. Since we want to check if a web server is up, leave the Protocol as HTTP and the Resource Type as URL.</p>
<p>For the hostname, I’m going to put in the IP address of an instance I have that’s running a web server. Leave “Check Frequency” set to 1 minute. And click “Next”. We can leave the Response Validation with the defaults, so click “Next” again.</p>
<p>This is where we set up an alert for when the uptime check fails. First, we specify how long a failure has to last before it’ll trigger an alert. Let’s leave it set to 1 minute. We should also tell it how to send alert notifications. Click the dropdown menu, and then click “Manage Notification Channels”. You can be alerted by email, text message, or a variety of other options, such as Slack. We’ll get it to send an email when the web server is down. Click “Add New”, and enter your email address and your name.</p>
<p>Okay, now close this tab in your browser, and go back to the previous one. Click Refresh and select your email. Now click the “Test” button. Since the web server at that address is up, it came back right away.</p>
<p>Now I’m going to stop Apache on the instance that’s running the web server and test it again. This time, the connection failed, as expected. Click the Create button.</p>
<p>To see the results of the uptime check, go to the menu on the left and select Uptime Checks. It’ll take a while before it runs for the first time, so don’t worry if you don’t see anything in the dashboard right away. I’ll skip ahead to when the uptime check has run. OK, now you can see it’s showing that the web server’s down. After a little while, it’ll send a notification email. Here’s what it looks like.</p>
<p>Now I’ll start Apache up again and see if the alert policy sees it. I’ll just skip ahead a few minutes. Yes, it sees that the web server is up now.</p>
<p>If you want to see data graphically, then click on Dashboards. It provides default dashboards for many Google services. To create your own, click Create Dashboard. I’ll call it “Example Dashboard”.</p>
<p>Now click the “Add Chart” button. In this search field, type “URL”. There’s the resource type we need. It’s called “Uptime Check URL”. It gives us a few different metrics to choose from. The obvious one to choose is “Check passed”, but to make things more interesting, let’s choose “Request latency”. Click Save and the graph will be added to your dashboard.</p>
<p>This graph shows the network latency between each region and the web server. You can see when the web server was down, but it gives us more information than that. This network latency data from different locations around the world can be quite helpful, especially if some of your users are reporting slow performance.</p>
<p>Note that you’ll need to refresh this page to see the latest data. You can turn on auto-refresh if you want.</p>
<p>Suppose you’d like to get more information about the instance where the web server is running, such as the CPU load. Let’s create another chart.In the search field, type “cpu load”. Let’s select the 1-minute version. For the resource type, select “VM instance”. Notice that you can even monitor Amazon EC2 instances.</p>
<p>You’ll notice that the chart is blank. That’s because we need to install the Monitoring Agent to get CPU data.</p>
<p>Go back to the Overview, and then click this link to bring up the installation instructions. The instructions are different depending on which Linux distribution you’re running on your instance. I’m running Debian, so I need to follow these instructions. I’ll fast-forward to the point after I’ve run all of the install commands.</p>
<p>While we’re here, let’s install the logging agent too. You can find a link to the documentation in the GitHub repository I created for this course. The link to the GitHub repository is at the bottom of the Overview tab below this video.</p>
<p>Installing the logging agent will prepare this instance for the next lesson when we use Cloud Logging. I’ll fast-forward again.</p>
<p>Now let’s go back and see what happened to our chart. Okay, now there’s a line showing the load average, so it worked.</p>
<p>If you’ve been following along using your own account, then you should go back and delete the monitoring you set up. First, go to the Alerting page from the menu, and then delete the policy. You have to do that before it will let you delete the uptime check. Okay, now go to “Uptime Checks” and delete the one you created. Finally, go to Dashboards, click on the one you created, and delete it.</p>
<p>That’s it for this lesson.</p>
<h1 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h1><p>Looking at real-time monitoring is great, but there will be many times when you’ll want to look at what happened in the past. In other words, you need logs.</p>
<p>For example, suppose I wanted to see when a VM instance was shut down. Compute Engine, like almost every other Google Cloud Platform service, writes to the Cloud Audit Logs. These logs keep track of who did what, where, and when.</p>
<p>There are three types of audit logs. Admin Activity logs track any actions that modify a resource. This includes everything from shutting down VMs to modifying permissions.</p>
<p>System Event logs track Google’s actions on Compute Engine resources. Some examples are maintenance of the underlying host and reclaiming a preemptible instance.</p>
<p>Data Access logs are pretty self-explanatory. They track data requests. Note that this also includes read requests on configurations and metadata. Since these logs can grow very quickly, they’re disabled by default. One exception is BigQuery Data Access logs, which are not only enabled by default, but it’s not even possible to disable them. Fortunately, you won’t get charged for them, though.</p>
<p>In the console, select “Logging”.</p>
<p>There are lots of options for filtering what you see here. You can look at the logs for your VM instances, firewall rules, projects, and many other components. You can even send logs from other cloud platforms like AWS to here. You just need to install the logging agent on any system that you want to get logs from.</p>
<p>This is a great way to centralize all of your logs. Not only does centralizing your logs make it easier to search for issues, but it can also help with security and compliance, because the logs aren’t easy to edit from a compromised node.</p>
<p>In this case, we need to look at the VM instance logs. You can choose a specific instance or all instances. I only have one instance right now, called instance-1. Since we installed the logging agent on instance-1 in the last lesson, there are already some log entries for it. </p>
<p>Here you can choose which logs you want from the instance, such as the Apache access and error logs. I could set it to “syslog” since that’s where the shutdown message will be, but I’ll just leave it at “All logs” because sometimes you might not know which log to look in.</p>
<p>You can also filter by log level, and for example, only look at critical entries. I’ll leave it at “Any log level”.</p>
<p>Finally, you can change how far back it will look for log entries. I’ll change it to the last 24 hours.</p>
<p>OK, now I’ll search for any entries that contain the word “shutdown” so I can see if this instance was shut down in the last 24 hours.</p>
<p>If you need to do really serious log analysis, then you can export the logs to BigQuery, which is Google’s data warehouse and analytics service. Before you can do that, you need to have the right permissions to export the logs. If you are the project owner then, of course, you have permission. If you’re not, then the “Create Sink” button will be greyed out, and you’ll have to ask a project owner to give you the Logs Configuration Writer role.</p>
<p>First, click the “Create Sink” button. A sink is a place where you want to send your data. Give your sink a name, such as “example-sink”. Under “Sink Service”, you have quite a few options, such as BigQuery, Cloud Storage, or a Custom destination. We’ll choose BigQuery. </p>
<p>Under “Sink Destination”, you have to choose a BigQuery dataset to receive the logs. If you don’t have one already, then click “Create new BigQuery dataset”. Give it a name, such as “example_dataset”. Note that I used an underscore instead of a dash because dashes are not allowed in BigQuery dataset names. Now click the “Create Sink” button.</p>
<p>It says the sink was created, so let’s jump over to BigQuery and see what’s there. Hmmm. It created our example dataset, but it doesn’t contain any tables, which means it doesn’t have any data. That’s weird, right? Well, it’s because when you set up a sink, it only starts exporting log entries that were made after the sink was created.</p>
<p>OK, then let’s generate some more log entries and see if they get exported. I’ll restart the VM, which will generate lots of log entries. Okay, I’ve restarted it. Now if we go back to the Logging page, do we see the new messages? Yes, we do.</p>
<p>Now let’s go back to BigQuery and see if the data’s there. Yes, there are two tables there now. Click on the syslog table. Now click the “Query Table” button. To do a search in BigQuery, you need to use SQL statements, so let’s write a simple one just to verify that the log entries are there.</p>
<p>Thankfully, it already gave me the skeleton of a SQL statement. I just need to fill in what I’m selecting. I’ll put in an asterisk to select everything, but I’ll restrict it by using a WHERE clause with the column name “textPayload” (which is the column that contains the text in the log entry)…”LIKE ‘%shutdown%’”. The percent signs are wildcards, so this SQL statement says to find any log entries that have the word “shutdown” in them somewhere.</p>
<p>Now we click the “Run” button…and it returns the matching log entries. If we scroll to the right, then we can see the textPayload field and it does indeed contain the word “shutdown” in each of the entries.</p>
<p>Of course, we did exactly the same search on the Logging page and it was way easier, so why would we want to go through all of this hassle of exporting to BigQuery and writing SQL statements? Well, because sometimes you may need to search through a huge number of log entries and need to do complicated queries. BigQuery is lightning fast when searching through big data, and if you build a complex infrastructure in Google Cloud Platform, then the volume of log data it will generate will easily qualify as big data.</p>
<p>Since we don’t want our example sink to keep exporting logs to BigQuery and incurring storage charges, let’s delete what we’ve created. On the Logging page, click on “Logs Router” in the left-hand menu, then select the sink and delete it.</p>
<p>We should also delete the BigQuery dataset, so go back to the BigQuery page, select the dataset, and click “Delete Dataset”. It wants you to be sure that you actually want to delete the dataset, so you have to type the dataset name before it will delete it.</p>
<p>One concern that you or your company may have is how to ensure the integrity of your logs. Many hackers try to cover their tracks by modifying or deleting log entries. There are a number of steps you can take to make it more difficult to do that.</p>
<p>First, apply the principle of least privilege. That is, give users the lowest level of privilege they need to perform their tasks. In this case, only give the owner role for projects and log buckets to people who absolutely need it.</p>
<p>Second, track changes by implementing object versioning on the log buckets. The Cloud Storage service automatically encrypts all data before it is written to the log buckets, but you can increase security by forcing a new version to be saved whenever an object in a log bucket is changed. Unfortunately, this won’t prevent an owner from deleting an incriminating object, which is why you need to keep tight control on which users are given the owner role.</p>
<p>Third, you could add more protection by requiring two people to inspect the logs. You could copy the logs to another project with a different owner using either a cron job or the Cloud Storage Transfer Service. Of course, this still won’t prevent an owner in the first project from deleting the original bucket before the copy occurs or from disabling the original logging.</p>
<p>So the bottom line is that a person with the owner role can get around just about anything you put in place, but you can make it nearly impossible for someone without the owner role to change the logs without you knowing about it.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Error-Reporting-and-Debugging"><a href="#Error-Reporting-and-Debugging" class="headerlink" title="Error Reporting and Debugging"></a>Error Reporting and Debugging</h1><p>So far, we’ve been looking at alerts and log messages from system software. Now it’s time to look at how to get error information from your applications. That’s where the Cloud Error Reporting service comes in. To show you how this works, I’m going to install Google’s Hello World application in App Engine and then get it to generate an error.</p>
<p>Normally, you’d use your own workstation for the development environment, but to simplify this demo, I’m going to use Cloud Shell. The nice thing about Cloud Shell is that it already has all of the packages installed that you need. When you do want to write and test Java code on your own workstation, remember that you need to install the Google Cloud SDK, the Java SE 11 Development Kit, Git, and Maven 3.5 or greater on your system.</p>
<p>First, open Cloud Shell. Next, get a copy of the Hello World application with this “git clone” command. Then go into the directory where the app is.</p>
<p>Now use the local development server to make sure the app works. To see if it’s working, click the “Web Preview” icon, and select “Preview on port 8080”. You should see a “Hello world!” message. OK, it’s working, so let’s stop the development server and upload the application to App Engine. You can stop the development server with a Ctrl-C.</p>
<p>Now use the “gcloud app deploy” command to upload it to App Engine. If you’re doing this yourself, then it may look slightly different than mine because I’ve already configured App Engine. If this is your first time deploying to App Engine, then it will likely ask you to choose a region. OK, it’s done deploying.</p>
<p>There are a couple of ways to test it. If you’re not using Cloud Shell, then you could do a “gcloud app browse”, which is pretty handy. Since we are using Cloud Shell, we’ll have to go to this URL. There’s “Hello World!” again.</p>
<p>Now, in order to see an error on the Error Reporting page, we need to generate an error. Let’s edit the code and mess something up. I’m going to add a line that I know will cause a problem. This’ll throw an exception because you can’t divide a number by zero.</p>
<p>Now run “gcloud app deploy” again. When it’s done, bring the app up in your browser again. This time it gives you a big error message, which is actually what we want, for once.</p>
<p>Let’s see if Error Reporting picked it up. The Google Cloud Console shows errors on the main dashboard, so you don’t have to go to the Error Reporting page to see them. I’ll refresh the page. There it is. Click on “Go to Error Reporting” to see what it shows. If you click on the error, you’ll see more details, including the stack trace. You can even click on the line where the error occurred and it will take you into your source code in the Debugger. However, in this case, the line it’s showing at the top of the stack trace is not in our code. We can click on this line, though, which is in our code, and it should take us there.</p>
<p>The Debugger is a great tool that you can use whether an error occurred or not. Let’s put a more subtle problem in the code and see how we can use the Debugger to figure out what’s wrong.</p>
<p>Suppose we want to check the operating system running our app, and if it’s Ubuntu, then we’ll print “Ubuntu rocks!”</p>
<p>First, we have to fix the bug that we introduced previously, so we’ll remove that line. We have to go back to the editor to do that. Now I’ll add the new code. Even if you’re not familiar with Java, this is pretty straightforward. It gets the name of the operating system, then it checks to see whether it’s equal to Ubuntu or not, and if it is, it says, “Ubuntu rocks!”, and if it isn’t, it says, “Hello world!”.</p>
<p>Now we’ll upload it to App Engine again. OK, now we’ll refresh the webpage. And it says “Hello world!” again, not “Ubuntu rocks!” That might be because the underlying operating system isn’t Ubuntu, but let’s go back to the Debugger and see if that’s the reason.</p>
<p>You’ll notice that this is still the old version of the file. First, refresh the browser. It’s still showing the old version. To get to the new version, you have to click on this drop-down menu and select the right one. The latest version should say 100% at the end. Sometimes you have to tell it where the source code is. Find “App Engine” in the list, and click the “Select source” button.</p>
<p>Now find the file. You’ll see some text on the right-hand side that says to click a line number to take a snapshot of the variables and call stack. It also points out that taking a snapshot does not stop the running application, which is good to know.</p>
<p>Click in the left-hand gutter on the line just after the “osname” variable is set. Now that the snapshot point is set, we can refresh the webpage and trigger the snapshot. If we go back to the Debugger tab, you’ll see that it’s showing the variables and call stack on the right-hand side. There’s “osname”. It’s set to “Linux”, not anything more specific. I guess it doesn’t know the specific distribution of Linux that’s running, so let’s change our code to check for Linux instead.</p>
<p>And deploy the new version. Now refresh the webpage. It worked!</p>
<p>Let’s go back to the main Error Reporting page and I’ll show you a couple of other things. First, if you’re sitting on this page watching for errors in real-time, then you should click the “AUTO RELOAD” button, which will refresh the page every 5 seconds. If you don’t want to hang around here and just want to get an email when an error occurs, then click the “Turn on notifications” button.</p>
<p>Alright, that’s it for this lesson.</p>
<h1 id="Tracing-and-Profiling"><a href="#Tracing-and-Profiling" class="headerlink" title="Tracing and Profiling"></a>Tracing and Profiling</h1><p>In the last lesson, we looked at how to debug errors in your application, but what do you do if your application is working properly but performing too slowly? That’s what Cloud Trace and Cloud Profiler are used for. Cloud Trace shows you the latency of each application request. That is, it tells you how long each request takes.</p>
<p>The Trace List is probably where you will spend most of your time. It shows you all of the traces over a specific period of time in this cool graph. It is set to “1 hour” right now, but we can change that to give a longer view. Each one of these dots is a trace of an individual request to the application. If you click on one of the dots, it brings up two more panes underneath. The Waterfall View shows what happened during the request. The first bar shows the total end-to-end time, which was 215 milliseconds in this case. The bars underneath show the time it took to complete calls performed when handling the request. In this case, we have one bar for an HTTP GET request.</p>
<p>Of course, this timeline would be a lot more useful if we were running a more complex application with multiple calls so you could see which ones were taking the most time. Each of those calls would have a bar on this chart. The Hello World application is about the simplest application possible, so you’ll just have to use your imagination here.</p>
<p>Analysis reports show you the latency distribution for your application and also attempt to identify performance bottlenecks, which is a great feature. You have to have at least 100 traces before you can run a report, though.</p>
<p>If you’re running your applications in App Engine, then it’ll automatically capture and submit traces, but if you want to trace code that’s running outside of App Engine, then you’ll have to either add instrumentation code to your applications using the Trace SDK or submit traces through the API.</p>
<p>Cloud Trace shows you which requests take the longest to run. Once you’ve determined which requests might need to be optimized, you can use Cloud Profiler to see which parts of the code for those requests are using the most CPU and memory.</p>
<p>To use Cloud Profiler, you have to add instrumentation code to your application even if it’s running in App Engine. Google has provided a sample application called shakesapp that includes this instrumentation. It’s written in the Go language. Here’s what it looks like in Cloud Profiler. This is called a flame graph, and it can be a bit confusing until you know how it works.</p>
<p>Since CPU time is selected, the bars represent the CPU time taken by each function. I ran the application seven times, so these results show the average of those seven runs. The first bar is for the entire application, which took about 13 seconds of CPU time, on average.</p>
<p>The bars underneath are color-coded according to the package they’re in. Most of these functions are part of the standard libraries for the Go language. The ones that are part of the actual application, shakesapp, are dark green in this graph. The first one just calls the second one, so the second bar is the one that matters. It calls a Go language function called MatchString. This single function takes up 58% of the CPU time for this application, so we might want to see if there’s a more efficient way to perform this operation.</p>
<p>Okay, that’s it for the Cloud Operations suite. Before we go, you might want to delete your application, so it doesn’t incur any more charges. Go to App Engine and then go to Settings. Click “Disable application”. It will ask you to type the app’s ID before you can click “DISABLE”. This doesn’t delete the application, but it does stop it from serving requests. To start the application up again, you can just click “Enable application”.</p>
<p>If you want to permanently delete the application, then you’ll have to delete the project it’s associated with, which you can do in the “IAM &amp; Admin” page. Be aware that if you delete a project, you will never be able to use that project ID again. That is, you won’t be able to create a new project with the same ID.</p>
<p>That’s it for this lesson.</p>
<h1 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h1><p>So far, we’ve been talking about monitoring and debugging your applications in production, but you’ll also need to test your application and infrastructure to see how it will perform under different conditions.</p>
<p>There are at least three types of tests you should run: load tests, where you stress your application with a heavy load. Resilience tests where you see what happens when various infrastructure components fail and vulnerability tests where you see if your application can withstand hacker attacks.</p>
<p>Ideally you should run load tests before you put your application into production. Your test should be designed to simulate real world traffic as closely as possible. You should test at the maximum load you expect to encounter which can admitingly be difficult to predict for some applications but hopefully you’ll have a reasonably good idea of how much traffic you’re likely to get. You should also measure how your Google cloud costs increase as the number of users increases.</p>
<p>If you’re expecting a wide variation in how much traffic you get then you should also test how your application performs when traffic suddenly increases.</p>
<p>Resilience testing is similar to disaster recovery testing because you’re testing what happens when infrastructure fails but the difference is that in resilience testing you’re expecting your application to keep running with little or no downtime.</p>
<p>One common testing scenario is to terminate a random instance within an autoscaling instance group. Netflix created software called Chaos Monkey that automates this sort of testing. If your application in the autoscaling instance group is stateless, then it should be able to survive this sort of failure without any noticeable impact on users.</p>
<p>Since cyber attacks are extremely common these days, your organization should put processes in place to test the security of your applications. Here are a few important ones:</p>
<p>First, ideally your software development team should have a peer review process with developers checking each other’s code for security flaws. Second, you should integrate a static code analysis tool such as HP Fortify into your continuous integration continuous deployment pipeline to automate security checking.</p>
<p>Third, at least once a year you should run penetration tests on your applications and infrastructure to see if they’re vulnerable. You can either do this yourself or contract a third party to do it. Other cloud providers typically require that you request permission before you perform penetration testing on your cloud infrastructure. Surprisingly Google does not require that you contact them.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a> also provides a useful tool called the Web Security Scanner. This service connects to the base URL of your application and follows all of the links in it while scanning for vulnerabilities, such as cross-site-scripting, mixed content, and outdated libraries. It can scan applications hosted in App Engine, Compute Engine, and Google Kubernetes Engine.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Storage-Management"><a href="#Storage-Management" class="headerlink" title="Storage Management"></a>Storage Management</h1><p>Once you’ve set up your Cloud Storage buckets and applied the right security settings to them, you’ll still need to manage them on an ongoing basis.</p>
<p>One of your first tasks will likely be to get data into your Cloud Storage buckets. If you need to upload data from an on-premise location, then you have three options:</p>
<ol>
<li>The Cloud Storage console</li>
<li>gsutil, or</li>
<li>Offline media import &#x2F; export</li>
</ol>
<p>The easiest way is to click on a bucket in the Cloud Storage console and then click “Upload Files” or “Upload Folder”. You can even view the uploaded file from the console if it’s the type of file that a web browser knows how to display.</p>
<p>The second way is to use the “gsutil” command. For example, to upload a folder called “example-folder” from your desktop, you would use “gsutil cp -r Desktop&#x2F;example-folder” and then put “gs:&#x2F;&#x2F;” and the name of the bucket. Now, if we go back to the Cloud Storage console, we can see the uploaded folder.</p>
<p>If you have a slow or expensive Internet connection, then you may want to use the third option, which is to ship your data on offline media. One way is to send hard disks, storage arrays, tapes, or other media to a third-party provider, such as Iron Mountain, and let them upload your data. </p>
<p>Another way is to use a Transfer Appliance supplied by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a>. Here’s how it works. You submit a request for a Transfer Appliance, which Google then ships to you. When you receive it, you install it in your data center and transfer your data to it. Then you ship it back to Google so they can upload it for you.</p>
<p>There are still more steps to perform, though, because the data on the Transfer Appliance is encrypted. First, Google uploads your encrypted data to a staging bucket in Cloud Storage. Next, you have to launch and configure what’s called a rehydrator instance. Then you run a rehydrator job on the instance, which will decrypt the data and copy it to a Cloud Storage bucket of your choosing. Finally, you delete the instance and send Google a request to erase the data from the Transfer Appliance and the staging bucket.</p>
<p>If you need to transfer data from another cloud provider, then you can use the Google Cloud Storage Transfer Service. I’ll show you how it works.</p>
<p>Click “Transfer” in the left-hand menu. Then click the “Create transfer” button. Under “Select source”, there are three options: you can transfer from another Google Cloud Storage bucket, an Amazon S3 bucket, or from a URL. I’m going to transfer from another Cloud Storage bucket to simulate transferring from another cloud provider because it works the same way.</p>
<p>Click the Browse button and select the bucket. Then say which files to include in the transfer by specifying their prefix, such as “example”. You can also exclude files in the same way. Another option is to specify that you only want files modified recently, say in the last 24 hours. Of course, if you want to copy the entire contents of the bucket, then you don’t need to put in any filters.</p>
<p>Now choose the destination bucket. There are also some options for how it handles overwrites and deletions. By default, an object will only be overwritten when the source version is different from the destination version. Also by default, no objects will be deleted from either the source or the destination. I’ll just leave it with the default settings.</p>
<p>Then you specify when you want the transfer to happen. You can either run it now or schedule it to run at a particular time every day. I’ll just run it now. The transfer is going to take a little while, so I’ll fast forward until it’s done. It’s a bit tedious to transfer files manually like this, which is why scheduled transfers are so nice. For example, you could have it run automatically every day to check for new files in the source bucket and transfer them to the destination bucket. Alternatively, you could do this with a cron job that runs the gsutil command, but it’s much cleaner to do it this way.</p>
<p>If you have any experience with managing data, then you know that data just keeps growing and growing over time, and if you don’t implement something to keep that growth under control, then you will either run out of storage space or have runaway costs. Since Google Cloud has nearly unlimited storage resources, that means you could easily have runaway costs.</p>
<p>To prevent that, you can use object lifecycle management. There are two ways to control costs using lifecycle management. Based on age, creation time, or number of newer versions, either:</p>
<ul>
<li>Delete objects, or</li>
<li>Move objects to a cheaper storage class, such as Nearline or Coldline Storage</li>
</ul>
<p>You can manage object lifecycle policies through the Cloud Storage console, the “gsutil” command, or the Google API Client Libraries. I’ll show you how to do it with gsutil.</p>
<p>First, you need to create a lifecycle config file that contains the rules you want to set. Here’s an example in JSON format:</p>
<p>{</p>
<p>“lifecycle”: {</p>
<p> “rule”: [</p>
<p> {</p>
<p>  “action”: {“type”: “Delete”},</p>
<p>  “condition”: {</p>
<p>   “age”: 365,</p>
<p>   “isLive”: true</p>
<p>  }</p>
<p> },</p>
<p> {</p>
<p>  “action”: {“type”: “Delete”},</p>
<p>  “condition”: {</p>
<p>   “isLive”: false,</p>
<p>   “numNewerVersions”: 3</p>
<p>  }</p>
<p> }</p>
<p>]</p>
<p>}</p>
<p>}</p>
<p>The first rule says to delete any live object older than 365 days. This might seem like a short time to live, but it’s not as draconian as it looks because it won’t completely delete the object if you’ve enabled versioning on the bucket. It will archive it instead. However, if you don’t have versioning enabled, then a Delete action will completely delete objects matching the condition and there will be no way to get them back. This is why you should test your lifecycle rules on test data before applying them to production data.</p>
<p>The “isLive” parameter only matters if you’ve turned on versioning. If an object is live, it means that it’s the most current version of that object. If it’s not live, then it is one of the archived versions of that object.</p>
<p>Let’s see if versioning is enabled on this bucket. I’ll use the gsutil command to check. It says it’s suspended. What the heck does that mean? It means versioning is disabled. I don’t know why they say suspended instead of disabled.</p>
<p>To enable versioning, we just need to change “get” to “set on”. I have a file in the ca-example bucket called “examplefile” [Show that]. Now, I’ll upload a different version of “examplefile” and see if it archives the old version. OK, it’s uploaded, now I’ll run the “gsutil ls -la” command on that file. Yes, there are two versions of it now and they have different dates and sizes. Note that if you do an “ls -l” without the ‘a’ flag, then it won’t show the different versions, so make sure you include the ‘a’ flag.</p>
<p>OK, let’s get back to the lifecycle policy. The second rule says to delete any object that has at least 3 newer versions of itself, including the live version. Unlike the first rule, this one really will delete the object because when you delete an archived object, it gets deleted forever. Although this rule has an explicit condition that the object must not be live, you don’t actually need to put in that condition because if an object has three newer versions, then it can’t be live. It has to be an archived version.</p>
<p>So, looking at the big picture for these two rules, there are two possible scenarios, depending on whether versioning is enabled or not. If versioning is enabled, then the first rule will archive any object that is more than one year old, and the second rule will delete any object that has at least three newer versions of itself. If versioning is not enabled, then the first rule will delete any object that is more than one year old, and the second rule will not do anything.</p>
<p>Suppose that instead of deleting objects older than one year, you’d like to send them to Nearline Storage, which is significantly cheaper, and send objects in Nearline Storage that are older than 3 years to Coldline Storage, which is even less expensive.</p>
<p>Here’s a lifecycle config file that will implement this policy.</p>
<p>{</p>
<p>“lifecycle”: {</p>
<p> “rule”: [</p>
<p> {</p>
<p>  “action”: {</p>
<p>   “type”: “SetStorageClass”,</p>
<p>   “storageClass”: “NEARLINE”</p>
<p>  },</p>
<p>  “condition”: {</p>
<p>   “age”: 365,</p>
<p>   “matchesStorageClass”: [“MULTI_REGIONAL”]</p>
<p>  }</p>
<p> },</p>
<p> {</p>
<p>  “action”: {</p>
<p>   “type”: “SetStorageClass”,</p>
<p>   “storageClass”: “COLDLINE”</p>
<p>  },</p>
<p>  “condition”: {</p>
<p>   “age”: 1095,</p>
<p>   “matchesStorageClass”: [“NEARLINE”]</p>
<p>  }</p>
<p> }</p>
<p>]</p>
<p>}</p>
<p>}</p>
<p>The first rule moves objects that are older than one year from Multi_Regional Storage to Nearline Storage. </p>
<p>The second rule says that if an object is in Nearline Storage and it is at least 1,095 days old (which is 3 years), then it should be moved to Coldline Storage.</p>
<p>To apply this lifecycle policy, you type “gsutil lifecycle set”, then the name of the config file, which is “lc2.json” in this case, and then the URL for the bucket, which is “gs:&#x2F;&#x2F;ca-example” in this case. Again, make sure you apply a lifecycle policy to test data before you put it into production or you risk losing valuable data.</p>
<p>Once you’ve set up a lifecycle policy, you can monitor what it’s doing in two ways:</p>
<ul>
<li>Expiration time metadata</li>
<li>Access logs</li>
</ul>
<p>To see the metadata for an object, use “gsutil ls -La” on the object. I’ll just show you the first dozen or so lines of the output because it prints all of the access control list information, which we’re not interested in right now. The output from this command may or may not contain expiration time metadata (and it doesn’t for this file), but the lifecycle policy should add that metadata when it knows the date and time that an object will be deleted.</p>
<p>Bear in mind that updates to your lifecycle configuration may take up to 24 hours to go into effect. Not only will it take up to 24 hours before your new rules kick in, but your old rules may still be active for up to 24 hours, so if you discover a mistake in your rules, that bug can still be active for another 24 hours after you fix it, which is another reason why testing your rules on test data first is so important.</p>
<p>Expiration time metadata is useful to see when your lifecycle policy is planning to delete an object, but it won’t show any of the other potential operations, such as moving an object to another storage class. If you want to see all of the operations that your lifecycle policy has actually performed, then you can look at the logs.</p>
<p>If you haven’t already set up access logs for your bucket, then here are the commands you need to use. First create a bucket to hold the logs. Remember to change “ca-example-logs” to your own log bucket name.</p>
<p>gsutil mb gs:&#x2F;&#x2F;ca-example-logs</p>
<p>Then you have to give Google Cloud Storage WRITE permission so it can put logs in this bucket.</p>
<p>gsutil acl ch -g <a href="mailto:&#99;&#108;&#111;&#117;&#x64;&#x2d;&#115;&#x74;&#x6f;&#x72;&#97;&#103;&#x65;&#45;&#x61;&#110;&#97;&#108;&#121;&#x74;&#x69;&#x63;&#x73;&#64;&#103;&#x6f;&#111;&#103;&#108;&#101;&#46;&#x63;&#x6f;&#109;">&#99;&#108;&#111;&#117;&#x64;&#x2d;&#115;&#x74;&#x6f;&#x72;&#97;&#103;&#x65;&#45;&#x61;&#110;&#97;&#108;&#121;&#x74;&#x69;&#x63;&#x73;&#64;&#103;&#x6f;&#111;&#103;&#108;&#101;&#46;&#x63;&#x6f;&#109;</a>:W gs:&#x2F;&#x2F;ca-example-logs</p>
<p>Next, you can set the default object ACL to, for example, “project-private”. You don’t have to do this, but it’s a good idea for security purposes to keep your logs private.</p>
<p>gsutil defacl set project-private gs:&#x2F;&#x2F;ca-example-logs</p>
<p>And finally, you enable logging with the “gsutil logging set on” command.</p>
<p>gsutil logging set on -b gs:&#x2F;&#x2F;ca-example-logs gs:&#x2F;&#x2F;ca-example</p>
<p>Once you have logging set up, then you can go into your logging bucket in the console and the access logs will show up there after the lifecycle policy has made changes.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Cloud-SQL-Configuration"><a href="#Cloud-SQL-Configuration" class="headerlink" title="Cloud SQL Configuration"></a>Cloud SQL Configuration</h1><p>Suppose you’ve been running a Cloud SQL for MySQL instance for a while, and it’s starting to run out of storage space. There are a few ways you could deal with this. If the database contains lots of old data that you can delete to free up space, that might be a good option. If the extra usage is caused by a temporary spike, and the data will be deleted soon anyway, then you might want to leave the storage capacity at its current level. You have to be careful, though, because if the instance runs out of space, it could go offline.</p>
<p>But you also have to be careful about increasing the instance’s storage capacity because once you increase it, the change is permanent, and you can never decrease it. So you could be paying extra money every month for space that you don’t need. In most cases, though, you’ll want to increase the storage space to avoid problems.</p>
<p>The first way to increase the capacity is very simple. You can just edit the instance’s configuration and change the storage capacity setting. However, if you think you’ll need to increase the storage space again in the future, then you might want to consider enabling the “Automatic storage increase” setting. Note that this option is enabled by default now, so you’d only have to turn it on if you disabled it when you created the instance.</p>
<p>You should also consider setting the “Automatic storage increase limit.” Since you can never decrease the storage capacity of an instance, you may want to prevent a runaway increase by setting a limit. By default, it’s set to 0, which means there’s no limit other than the maximum that’s available for the machine type of the instance.</p>
<p>While we’re on the subject of defaults, there’s another setting you may want to consider changing. Every few months, Google applies updates to each Cloud SQL instance. This requires a reboot that typically takes only a few minutes, but you’ll probably want to set a maintenance window for when Google will do the update. You can specify a 1-hour window on any day of the week. If you don’t set a window, then by default, Google can perform the update at any time.</p>
<p>Amazingly, you can change almost all of the configuration settings for a Cloud SQL instance after you’ve created it, even the machine type and the zone! The instance will go offline for a few minutes when you change the machine type or the zone, though.</p>
<p>The only settings you can’t change after you’ve created an instance are the instance ID, the region, the MySQL version (which can be set to 5.6, 5.7, or 8.0), and the storage type (that is, either SSD or HDD). Also, once you’ve configured a private IP address for an instance, you can’t remove it.</p>
<p>Another issue you might run into is called replication lag. To achieve high availability for a Cloud SQL instance, you need to create a failover replica. With this configuration, every write operation on the primary database is also made on the replica. Normally, there’s only a slight delay before the update is performed on the replica, but there are times when the replica can fall behind, and there’s a significant lag before updates are made.</p>
<p>Although the replica will not lose any of the updates, a significant lag can cause a failover to take longer if the primary fails. If the replication lag becomes too high, then Google’s SLA will no longer be valid, so it’s important to address this issue.</p>
<p>To make sure you’ll know when replication lag occurs, you can set up an alert. You just need to configure Stackdriver to monitor the seconds_behind_master metric.</p>
<p>When replication lag is caused by a temporary spike in database updates, the replica will eventually catch up, and you don’t need to take any action. If the lag is continually high, then you could try recreating the replica. If that doesn’t work, then you may need to add RAM and disk to the replica. If that still doesn’t solve the problem, then you’ll likely have to shard your database into multiple instances to spread out the load.</p>
<p>And that’s it for Cloud SQL configuration.</p>
<h1 id="Cloud-CDN-Configuration"><a href="#Cloud-CDN-Configuration" class="headerlink" title="Cloud CDN Configuration"></a>Cloud CDN Configuration</h1><p>As you probably know, Cloud CDN is a service that caches your web content in Google’s delivery network. Then when a user goes to your website, they retrieve your content from the nearest CDN location rather than from your web server.</p>
<p>Cloud CDN is a great service for serving content to your users faster, but it does complicate things, so you may need to change some settings to optimize the cache. Here’s the first problem. When you update your content, how do you make sure the cache gets updated, too, so your users get the latest content? This might sound like a simple problem, but there are several different approaches to dealing with it.</p>
<p>First, you need to set an appropriate expiration time. If you have content that changes frequently, then you should set a short expiration time so users won’t get stale content from the cache. For example, stock prices change rapidly, so you’d set a very short expiration time for them.</p>
<p>You wouldn’t want to set a short expiration time for everything, though. For example, if you set a short expiration time for your company’s logo, then your users would frequently get cache misses, and the logo would have to be retrieved from the web server, which would defeat the purpose of using a CDN. And if your company changes its logo at some point, it wouldn’t be a big deal that it would take a while before your customers see the new one.</p>
<p>Another approach for dealing with content that changes infrequently is to use versioned URLs. The idea is that if you change the name of a piece of content, then it won’t be in the cache, so users will always get the latest version.</p>
<p>Here are three different ways to use versioning. You could add a query string with a version number in it. You could add a version number to the filename. Or you could add a version number in the path.</p>
<p>You might be wondering why you can’t just remove stale cache entries directly rather than relying on expiration times and versioning. Well, you can. It’s called invalidation, but you should only use it as a last resort because <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a> charges for invalidations and also enforces rate limits on how many invalidations you can do at a time.</p>
<p>Now that we’ve covered ways to prevent cache entries from becoming stale, we should also cover the opposite problem—how to reduce the number of cache misses. As I mentioned earlier, if the content that users need is often not in the cache, then response times will be slower. Aside from setting the expiration time appropriately, another way of improving the cache hit ratio is to use custom cache keys.</p>
<p>By default, Cloud CDN creates each cache key using the full URL, but this can be inefficient. For example, many sites serve up the same content regardless of whether the URL contains http or https. If you cache a separate copy of each piece of content for each protocol, then there would be a lot of cache misses. By using a custom cache key that doesn’t include the protocol, you’d only have one copy of each piece of content in the cache for both protocols, so you’d have way fewer cache misses.</p>
<p>Why? Suppose you have a web page that hasn’t been accessed for a while, so the cached version of that page has expired and is no longer in the cache. Then suppose a user browses the http version of that page. There’ll be a cache miss, and the page will be retrieved from the web server into the cache. When another user browses the https version of that page, there will be a cache miss again if the cache keys are based on the full URL. But if you use a custom key without the protocol, then the request for the https version of the page will result in a cache hit.</p>
<p>Similarly, you can create custom cache keys that leave out the host. If you have multiple copies of your website on different hosts, then it would make sense to only have one copy of your content in the cache.</p>
<p>Dealing with the query string can be a bit more complicated. If the content should always be the same for a URL regardless of what’s in the query string, then that’s easy. You can just create a custom cache key that leaves out the query string. But if certain parts of the query string will result in different content being retrieved, then you need to specify which parts of the query string to include in the cache key.</p>
<p>Note that you can leave out any combination of protocol, host, and query string when you create your custom cache keys.</p>
<p>And that’s it for Cloud CDN configuration.</p>
<h1 id="Instance-Startup-Failures"><a href="#Instance-Startup-Failures" class="headerlink" title="Instance Startup Failures"></a>Instance Startup Failures</h1><p>What can you do if your VM instance fails to boot up completely? You can’t use SSH because the SSH server isn’t running yet. If you’re running the VM on your desktop, then you could look at the console. But how do you do that for a <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud</a> instance? Luckily, there’s a solution. You’d look at the serial port.</p>
<p>By default, you can see the output of the serial port by clicking on the instance, and then at the bottom of the page, you can click the view serial port button. This might be enough information to help you troubleshoot your problem, but it many cases, you’ll need to interact with the VM to see what’s going on. You’ll notice that there’s a button called connect to serial port, but it’s grayed out. How frustrating. To enable interactive access, you need to add meta data to the instance. This isn’t a terribly user friendly way of enabling a feature, but it’s actually not too difficult.</p>
<p>First you have to decide whether you want to enable interactive access for an individual instance or for an entire project. If you enable it on individual instances, then you’ll have to enable it manually for every instance. For convenience, you might want to enable it for an entire project, but there is a higher security risk enabling serial port access for all of your instances because there is currently no way to restrict access by IP address. So hackers could try to break in to any of your VMs through the serial port. It wouldn’t be easy though, because they’d need to know the correct SSH key, username, project ID, zone, and instance name.</p>
<p>To enable interactive access to an individual instance, you can use this gcloud command gcloud compute instances add dash meta data. Now put in the instance name, which is instance dash one in my case, and then dash dash meta data equals serial dash port dash enable equals one.</p>
<p>Now when I refresh the page, the connect to serial port button lights up. If I click on it, then it brings up another window where I can interact with the serial console.</p>
<p>By the way, if you’re connecting to a Windows instance, then you’ll need to go into the drop down menu and select port two.</p>
<p>If the serial port output showed that you have a problem with the file system on your boot disk, then you can attempt to fix it by attaching the disk to another instance.</p>
<p>First, delete the instance, but be sure to include the keep disks option. Notice that it still gives me a warning about deleting disks, even though I used the keep disks option. That’s normal.</p>
<p>Then create a new instance. I’ll call it debug dash instance.</p>
<p>Now attach the disk that we saved from the original instance. Notice that by default the name of a boot disk is the same as the name of the instance, instance dash one in this case. You can also add the device name flag so it will be obvious which device corresponds to this disk, which will be helpful in a later step.</p>
<p>Then SSH into the new instance.</p>
<p>Now you need to find out the device name for the debug disk. Look in the dev disk by ID directory.</p>
<p>Remember when I mentioned that naming the disk device would be helpful? You can see that the debug disk is SDB. The file system is on the first partition, or part one. So the device name we need to use is SDB One. Now you can run an fs-check on it.</p>
<p>Of course, I’m doing this on a good disk, so fs-check doesn’t see any problems. But if this disk had come from an instance that couldn’t boot properly, then there’s a good chance that an fs-check would find lots of problems.</p>
<p>Let’s pretend that fs-check had to clean the file system and it was successful. After that, you should verify that it will mount properly.</p>
<p>You should also check that it has a colonel file. It does, but before you celebrate, you should check one more thing, that the disk has a valid master boot record.</p>
<p>It printed out information about the file system, so this disk is good to go. Now you would create a new instance and use this disk as its boot disk.</p>
<p>That took a bit of work, but it was relatively straightforward. For a tougher challenge, try the next lesson where we tackle SSH errors.</p>
<h1 id="SSH-Errors"><a href="#SSH-Errors" class="headerlink" title="SSH Errors"></a>SSH Errors</h1><p>We’ve covered what to do when your instance won’t boot at all, but what if it boots, at least partway, but you can’t connect to it using SSH?</p>
<p>When you can’t connect with SSH, then a feeling of helplessness can set in. But, don’t worry, there are quite a few things you can do to resolve the issue.</p>
<p>Before we get into how to troubleshoot the “Connection failed” error you see here, let’s go over how to deal with the “Permission denied” error. This error can occur for obvious reasons, such as not using the right flags on your ssh command when you try to connect, but there are also some potential issues that are specific to Google Cloud Platform.</p>
<p>To understand why, you need to know the different ways you can configure SSH connections on GCP instances. First up is “OS Login”, which is the preferred method. It lets you control access by using IAM roles. The advantage of using IAM roles is that they’re the standard way to control access to all other resources on GCP, too. OS Login can also manage your public SSH keys for you, and it even supports two-factor authentication.</p>
<p>If you can’t use the OS Login method for some reason, then you can add public SSH keys to metadata. One easy way to do it is to add project-wide SSH keys by putting them in a project’s metadata. Any user who has an SSH key in a project’s metadata can connect to any instance in that project with the exception of instances that specifically block project-wide SSH keys. The other way is to add SSH keys to the metadata of each instance. This gives you more fine-grained control of who can access individual instances, but it takes a lot more effort to manage.</p>
<p>Okay, so how might these different configurations lead to a “Permission denied” error? There are several different ways. For example, OS Login disables SSH keys in metadata, so if you try to connect to an OS Login-enabled VM using a key in metadata, you’ll get an error. The opposite configuration also generates an error. That is, if you try to use a key stored in an OS Login profile to connect to a VM that doesn’t have OS Login enabled, then it won’t work. Another scenario is if you try to use a project-wide SSH key to connect to a VM that has project-wide keys disabled.</p>
<p>All right, now let’s get back to the “Connection failed” error. Here’s how to troubleshoot it.First, check your firewall rules. By default, your network will contain a firewall rule that allows SSH traffic, but you should make sure that the rule wasn’t deleted or modified. The easiest way to check is to go into the Networking section of the Google Cloud console and click on Firewall Rules. There should be a rule called “default-allow-ssh” that allows traffic on tcp port 22. The source should be all zeroes (meaning it will allow SSH requests from any IP address) and the target should say “Apply to all” targets.</p>
<p>If you don’t have this rule, then you can easily create it. Click “Create Firewall Rule”. You don’t have to name it “default-allow-ssh”, but you probably should, just so it follows the convention of network name (that is “default”), allow, and protocol (that is, “ssh”). Change the priority to 65534. Leave “Direction of traffic” as “Ingress” and “Action on match” as “Allow”. Change “Targets” to “All instances in the network”. Leave the “Source filter” as “IP ranges”. In the “Source IP ranges” field, put 0.0.0.0&#x2F;0. And put “tcp:22” in the “Protocols and ports” field. Now click the “Create” button and that’s it. Let’s try connecting with SSH again and see if that fixed it. Great, it did.</p>
<p>If you didn’t have a problem with the firewall rules, but you still can’t get in with SSH, then try connecting to port 22 manually and see if the SSH server responds. Use the “nc” command on the external IP address of your instance and then specify port 22 . If you see the SSH banner, then you know that the network connection is ok and the SSH server is running. If you don’t see the banner, then it could be a problem with either the network connection or the SSH server.</p>
<p>Next, try accessing the serial console, which I showed in the previous lesson. Look at the serial port output to see if that will tell you why you can’t connect using SSH. </p>
<p>The next thing to check is if there’s a problem with your account. Try connecting with another username, like this.</p>
<p>You can put in whatever username you want and the gcloud tool will update the project’s metadata to add the new user and allow SSH access. I’m going to call it “newuser”.</p>
<p>If that didn’t work and your instance boots from a persistent disk (which is the case by default), then you can detach the persistent disk and attach it to a new instance. Of course, this will take down your existing instance, so if the instance is serving production users properly and you don’t want to cause an outage, then skip this procedure and go to the next one.</p>
<p>First, delete the instance and be sure to include the –keep-disks option.</p>
<p>Then create a new instance (I’m going to create it with the same name as the original one) and attach the disk that we saved from the original instance. Also add the “auto-delete&#x3D;no” option so the disk isn’t automatically deleted when you delete this instance.</p>
<p>Now SSH into the new instance. That worked. If your instance is serving production users properly and you don’t want to cause an outage, then you can follow this procedure instead of the one I just did.</p>
<p>First, create an isolated network that only allows SSH connections. This is because you’re going to clone your production instance and you don’t want the clone to interfere with your production services.</p>
<p>Now add a firewall rule to allow SSH connections to the network. Then create a snapshot of the boot disk. Now you can create a new disk with the snapshot you just created. Then create a new instance in the new network. Now attach the cloned disk.</p>
<p>By the way, Google recommends that you don’t give this instance an external IP address, but that makes it much more difficult to connect to it. Considering that the instance is sitting in a network that blocks everything except SSH requests, it shouldn’t cause any disruption to your production services even though it has an external IP address.</p>
<p>Now SSH into this instance. Although the cloned disk is attached to this instance, it isn’t mounted anywhere, so you’ll have to do that manually.</p>
<p>First create a mount point. Then see what the device name of the disk is. The boot disk is disk 0, so the extra disk we attached has to be disk 1. Then mount the filesystem.</p>
<p>Now you can finally try to debug why you can’t connect to the original instance using SSH. You can look through the logs, for instance.</p>
<p>That was a pretty complicated process, but that’s the sort of thing you have to do if you don’t want to disrupt your production service while you’re debugging it. Here’s a summary of what I just showed you. First, create an isolated network that doesn’t allow any connections. Then add a firewall rule to allow SSH connections. Next, create a snapshot of the boot disk. After that, you can create a new disk from the snapshot. Then create a new instance in the new network, and attach the cloned disk to it. Finally, SSH into the instance, and mount the disk so you can inspect it.</p>
<p>If you still can’t find the reason why your instance won’t accept SSH connections and you are able to restart the instance at some point, then you can use a startup script to gather information. If you’re not sure what to put in the startup script, then you can use one provided by Google.</p>
<p>The script will run the next time the instance boots, so when you’re ready, you can run <code>gcloud compute instances reset instance-1</code>. The script sends its output to the serial port, so look there for the debugging info.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Network-Traffic-Dropping"><a href="#Network-Traffic-Dropping" class="headerlink" title="Network Traffic Dropping"></a>Network Traffic Dropping</h1><p>Another problem you might run into is network traffic dropping between an instance and other systems. The first thing to check is the firewall rules for the network the instance is in. The default network has firewall rules that allow http, https and a few other protocols. If any of those rules were deleted, that could be the reason for your network issues.</p>
<p>If instead of putting the instance in the default network, you put it in another network that you created, then make sure you created the appropriate firewall rules. When you create a new network, it doesn’t come with any firewall rules. So you have to create some to allow any traffic.</p>
<p>Another potential reason for network traffic dropping, is the idle connection timeout. Idle TCP connections are disconnected after ten minutes. If your instance initiates or accepts long lived connections, then you should adjust the TCP keep alive settings. This will prevent connections from being dropped by the idle timeout.</p>
<p>Since the timeout is ten minutes, you need to set the TCP keep alive to something less than ten minutes so connections will never be idle for that long. You need to set the keep alive on either the instance or the external client, depending on which side initiates connections.</p>
<p>Be aware that you should only set a TCP keep alive if you are having problems with connection timeouts. If you are not having that sort of problem, then setting a TCP keep alive will just increase network traffic for no good reason.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to manage your Google Cloud infrastructure. Now, you know how to monitor and debug using stack driver, test your infrastructure under challenging conditions, manage your storage while keeping costs under control, and troubleshoot issues with instance and networks.</p>
<p>To learn more about <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know on the Cloud Academy Community Forums. Thanks, and keep on learning.</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/" rel="prev" title="GCP-Professional-Architect-Optimizing-Google-BigQuery-19">
      <i class="fa fa-chevron-left"></i> GCP-Professional-Architect-Optimizing-Google-BigQuery-19
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/" rel="next" title="GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21">
      GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Course-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Course Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monitoring"><span class="nav-number">2.</span> <span class="nav-text">Monitoring</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logging"><span class="nav-number">3.</span> <span class="nav-text">Logging</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Error-Reporting-and-Debugging"><span class="nav-number">4.</span> <span class="nav-text">Error Reporting and Debugging</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tracing-and-Profiling"><span class="nav-number">5.</span> <span class="nav-text">Tracing and Profiling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Testing"><span class="nav-number">6.</span> <span class="nav-text">Testing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Storage-Management"><span class="nav-number">7.</span> <span class="nav-text">Storage Management</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Cloud-SQL-Configuration"><span class="nav-number">8.</span> <span class="nav-text">Cloud SQL Configuration</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Cloud-CDN-Configuration"><span class="nav-number">9.</span> <span class="nav-text">Cloud CDN Configuration</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Instance-Startup-Failures"><span class="nav-number">10.</span> <span class="nav-text">Instance Startup Failures</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SSH-Errors"><span class="nav-number">11.</span> <span class="nav-text">SSH Errors</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Network-Traffic-Dropping"><span class="nav-number">12.</span> <span class="nav-text">Network Traffic Dropping</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-number">13.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
