<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionHello and welcome to “Google Professional Cloud Architect Case Studies”. My name is Daniel Mease and I’ll be taking you through this course. I am a trainer at Cloud Academy with over 20">
<meta property="og:type" content="article">
<meta property="og:title" content="GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22">
<meta property="og:url" content="https://example.com/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionHello and welcome to “Google Professional Cloud Architect Case Studies”. My name is Daniel Mease and I’ll be taking you through this course. I am a trainer at Cloud Academy with over 20">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T04:14:25.000Z">
<meta property="article:modified_time" content="2022-11-20T23:36:38.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:25" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:25-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:36:38" itemprop="dateModified" datetime="2022-11-20T19:36:38-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to “Google Professional Cloud Architect Case Studies”. My name is Daniel Mease and I’ll be taking you through this course. I am a trainer at Cloud Academy with over 20 years of software and web development experience.</p>
<p>This course is intended for:</p>
<ul>
<li>Anyone planning to take the Professional Cloud Architect Exam</li>
</ul>
<p>I am going to cover the 4 case studies presented in the exam guide:</p>
<ul>
<li>EHR Healthcare</li>
<li>Helicopter Racing League</li>
<li>Mountkirk Games</li>
<li>TerramEarth</li>
</ul>
<p>Feedback on our courses is valuable, both to us as trainers and our future students. If you have any criticisms or suggestions for improvement, we would greatly appreciate it if you would share those with us.</p>
<p>Please note that, when this video was recorded, all course information was accurate. Google is constantly updating its exams and services as part of its ongoing drive to innovate. As a result, over time, minor discrepancies may appear in the course content. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. </p>
<p>So, if you notice any information that is outdated, please contact <a href="mailto:&#115;&#117;&#112;&#112;&#x6f;&#x72;&#116;&#x40;&#x63;&#x6c;&#111;&#x75;&#x64;&#x61;&#x63;&#x61;&#x64;&#101;&#109;&#x79;&#x2e;&#x63;&#111;&#109;">&#115;&#117;&#112;&#112;&#x6f;&#x72;&#116;&#x40;&#x63;&#x6c;&#111;&#x75;&#x64;&#x61;&#x63;&#x61;&#x64;&#101;&#109;&#x79;&#x2e;&#x63;&#111;&#109;</a>. This will allow us to make all necessary fixes to the course during its next release cycle.</p>
<h1 id="Preparing-for-the-Exam"><a href="#Preparing-for-the-Exam" class="headerlink" title="Preparing for the Exam"></a>Preparing for the Exam</h1><p>To help you prepare for the Professional Cloud Architect Certification Exam, Google has provided an official study guide.</p>
<p>The study guide contains every topic that is to be covered by the exam. So theoretically, if you are familiar with everything on this page, you should be able to pass the exam without any issues. At the top of this exam guide, you will find some case studies. The whole point of this course is to explain what these case studies are, why they are important, and how to use them to prepare for the exam. So first, what are these case studies? </p>
<p>When you first read them, they might seem a little confusing. Normally a “case study” is a detailed description of a real world event. So an example might be something along the lines of “Company X migrated from an on-premises data center to the cloud and saved a million dollars”. The case study would then describe in detail everything that was running before the migration, including costs. And then it would break down those migration costs, as well as the updated costs after the migration. The basic idea is to make a claim and then offer proof of that claim, using a real world example. </p>
<p>The case studies linked in the study guide are different. Each case study describes a fictitious business. Also there is no explicit “event” described either. Instead, each case study presents a problem. Maybe the company wants to be able to save money, or maybe they want to be able to scale faster. Something like that. It also will describe the existing environment, along with any business and technical requirements. So, you can think of them as theoretical scenarios.</p>
<p>So, what is the point of including these case studies in the study guide?  </p>
<p>Well that is explained in the study guide itself:</p>
<p>“During the exam for the Cloud Architect Certification, some of the questions may refer you to a case study that describes a fictitious business and solution concept. These case studies are intended to provide additional context to help you choose your answer(s). Review the case studies that may be used in the exam.”</p>
<p>So these are basically mini-scenarios that will be referenced by one or more exam questions. Essentially they are a way for Google to ask complicated questions, without requiring several pages of text for each question. Now if you come into the exam already familiar with these scenarios, then the questions can be much shorter and they don’t have to spell out the entire background.</p>
<p>So then, how do I use these case studies to prepare for the exam? There are a few things you need to do. </p>
<p>First, you should try to memorize (or at least get very familiar) with all of them. Yes, I realize memorizing fictional scenarios is probably not something you want to do. However, memorizing these scenarios will save you precious time. Every time I have taken a Google exam, they always included a link to the appropriate case study. However, reading through the whole case study takes time. That means you will have less time to think and choose an answer. If you can go into the exam already knowing the scenarios, you can save yourself a significant amount of time. That means you will be less likely to run out of time and less likely to have to leave some answers blank.</p>
<p>Second, instead of just memorizing the case studies, you should also be able to identify key pieces of information within. For example, if a case study says that the company is going to be dealing with customers’ medical records. That means that any solution will need to be very secure, because it’s going to be handling sensitive health information. You need to make sure that any solution you pick can support encryption and be HIPAA compliant. Things like that. Your answers will be very dependent upon the requirements in each case study, so it is critical that you understand what the requirements are.</p>
<p>In the following lessons I am going to go through each case study, one at a time. I am going to read through each section, and then I’ll point out any key terms that you should take note of. Now, you need to be aware that simply watching these videos will not be sufficient. You should plan to read through the case studies yourself several times. Now it does not have to be word-for-word, but memorize the details. Memorize the key requirements. That way, when a question asks you to pick a “compute” solution for one of the companies, you can pick the most appropriate answer.</p>
<p>Some of you might be thinking that you might just watch this course several times in a row. I would actually discourage you from doing that. Google can modify its exams at any time. This also means that they can modify those case studies at any time. There is a chance that one or more case studies will have been modified since I recorded this video. It would actually be best for you to read the latest version of the case studies yourself. Google does not notify people when it makes any changes. So, I will try to keep this video updated, but I can’t guarantee 100% accuracy.</p>
<p>Also please note that I cannot tell you what questions will be asked, nor can I tell you which of the case studies will be used. Your exam might refer to all four, or maybe only one. Everyone’s exam experience will be different. So use these videos as a guide, but be prepared to do some studying on your own as well.</p>
<h1 id="EHR-Healthcare"><a href="#EHR-Healthcare" class="headerlink" title="EHR Healthcare"></a>EHR Healthcare</h1><p>In this lesson, we are going to dive into the case study for a fictional company called “EHR Healthcare”. I am going to read each section, and then I’ll point out key requirements and topics for study. Alright, so let’s start with the company overview:</p>
<p>“EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi-national medical offices, hospitals, and insurance providers.”</p>
<p>Now there are a few phrases that jump out at me when I read this. First of all, I see that this company will be storing “health records”. When I hear that I think about needing to store people’s private medical information. This means you need to think very carefully about security, encryption and compliance. Of course you should always be careful with customer’s data, but medical information is in a class of it’s own. There are all kinds of laws regulating this stuff. </p>
<p>That means you should understand how to properly set up IAM roles to prevent your employees from accessing your customer’s data. You need to be familiar with picking the right services to store and transmit this information securely. Know how to detect if private information is being accidentally written out to your logs. On your exam, you could get questions about any of these topics.</p>
<p>Now the next thing that I see is “multinational”. If your customers are going to be all over the world, that implies you probably need to support multiple regions. So a single instance running in a single zone is not going to work. You will need multiple instances across multiple zones in multiple regions. Also, this suggests that you might have to think about different jurisdictions and laws in various countries. For example, the GDPR in the EU (which is the General Data Protection Regulation). So start thinking about all the different services and options you would need to support customers all over the globe. Next, let’s go through the solution concept:</p>
<p>“Due to rapid changes in the healthcare and insurance industry, EHR Healthcare’s business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google Cloud has been chosen to replace their current colocation facilities.”</p>
<p>Ok, so the first thing I notice here is “rapid changes”. This implies agility and flexibility will be very important. So to me this means that you will generally want to avoid custom solutions. Your answers should probably stick to the built-in services and things that are easy to change. So if the question involved VMs, you’d want to stick with a Google base image instead of trying to roll your own. If the question was about storage, you’d be better off using Cloud SQL instead of trying to maintain your own MySQL server. Things like that.</p>
<p>Also, I see that things have been growing “exponentially” and that things need to be able to “scale”. So for any questions involving this case study, you are going to want to make sure that your solutions are scalable. Now for compute, that means using things like managed instance groups. Or for GKE, you would use a cluster autoscaler. For storage, you need to pick solutions that won’t run out of space and can automatically handle higher levels of I&#x2F;O. All of your answers need to be able to automatically scale with demand and avoid common bottlenecks.</p>
<p>I did notice that they do have a disaster recovery plan, and so you may be asked how to migrate this over to GCP. You might also get questions about dealing with lost data, or how to deal with a major outage. You should be familiar with Google best practices for disaster recovery. I see here that they are currently using continuous deployment, so be aware of how to set up CI&#x2F;CD on Google. Know the common services involved, and how to maintain and troubleshoot issues. Alright, next let’s go through the existing technical environment:</p>
<p>“EHR’s software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire. Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB). EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time. Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.”</p>
<p>So we see they currently have multiple co-locations. As I mentioned before, this implies that any solution would need to be able to support multiple regions. So if you get a question about storage solutions, you might need to think about things like data replication. It looks like they are using containers on Kubernetes, so you could get some questions about GKE. You should be familiar with migrating applications from one cluster to another. They also previously mentioned scalability so understand how to automatically scale containers using GKE. Basically, you could get any number of Kubernetes questions on the exam.</p>
<p>It also looks like you could get any number of database questions as well. They currently are using both SQL and noSQL databases. So you should be familiar with how to migrate data over into the various offerings. You want to be able to know how to pick the right type of database depending on the data. I also notice Redis is mentioned as well, so be prepared to get questions about caching. When should data be stored temporarily vs. permanently? How do you set up a Redis cache on GCP? Etc.</p>
<p>Now this says that the APIs on-prem will NOT be migrated. So they might NOT ask you anything about building APIs, but you probably DO need to know how to connect your on-prem environment to GCP. That means you need to be familiar with things like Cloud VPN and Cloud Interconnect. Basically, any way in which your GCP services can securely access your on-prem APIs. They might even ask you how to handle DNS routing between the two.</p>
<p>The company is currently using Active Directory, so you need to be familiar with syncing to AD and using LDAP. They might even ask you to replace Active Directory with Google Cloud Identity. Also, the company is currently using monitoring and alerting, and it looks like the alerts are not effective. So you will be expected to understand how to set up effective monitoring and alerting in GCP. Next, let’s go through the business requirements:</p>
<ul>
<li>“On-board new insurance providers as quickly as possible </li>
<li>Provide a minimum 99.9% availability for all customer-facing systems</li>
<li>Provide centralized visibility and proactive action on system performance and usage Increase ability to provide insights into healthcare trends</li>
<li>Reduce latency to all customers</li>
<li>Maintain regulatory compliance</li>
<li>Decrease infrastructure administration costs</li>
<li>Make predictions and generate reports on industry trends based on provider data”</li>
</ul>
<p>So here, on-boarding quickly means that your solutions should not need a lot of setup. You don’t have to manually provision resources every time you add a new provider. The entire system needs to be highly available, so understand how to achieve that. Be familiar with services that automatically provide 99.9% availability. If a VM dies, another should automatically replace it. If a region goes down, requests should automatically be re-routed. Things like that. </p>
<p>You could also be asked about system performance and usage, so know how to track and monitor that. And know how to set up alerts, so if your performance dips or if there is a usage spike, you can be notified. It looks like they want to track insights and identify trends. So I would imagine this could involve logging, creating dashboards, and maybe even Big Data. You might be asked about using Bigtable here, so I’d be familiar with that.</p>
<p>Low latency is important, so understand how to reduce latency to your customers spread out all over the world. This sounds like supporting multiple regions. You might get questions on Cloud CDN or load balancers. Anything that might impact latency. You need to be aware about how to handle regulatory compliance. We’ve already covered this. There could be questions that require you to pick a solution based on cost. Here it talks about administration costs, so this I assume means to automate as much as possible. You don’t want to pick solutions that require an employee to manually build and maintain them. Managed services are your friend. </p>
<p>This last part talks about making predictions and generating reports. So that could imply either AI or ML questions. It definitely seems to indicate Big Data at the very least. So know how to collect data, store data, and generate custom reports. Let’s go through the technical requirements:</p>
<ul>
<li>“Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers</li>
<li>Provide a consistent way to manage customer-facing applications that are container-based</li>
<li>Provide a secure and high-performance connection between on-premises systems and Google Cloud</li>
<li>Provide consistent logging, log retention, monitoring, and alerting capabilities</li>
<li>Maintain and manage multiple container-based environments</li>
<li>Dynamically scale and provision new environments</li>
<li>Create interfaces to ingest and process data from new providers”</li>
</ul>
<p>So, we see the need to establish and maintain a connection between GCP and the on-prem. That implies VPNs, Cloud Interconnect, routing, DNS, all that and more. We are reminded that containers are being used. I think we covered that. Again, we see the connection to on-prem needs to be both secure and fast. So you should understand the different options and be able to pick the best one based upon security and speed. We know that there is going to be logging, monitoring and alerting.</p>
<p>Here we see there will be multiple container-based environments, so you might need to deal with multiple GKE clusters. Either in different regions, or for different environments such as testing vs. staging vs. production. You might also be asked about dynamically provisioning and scaling environments. Or even creating APIs. So finally, let’s read the executive statement:</p>
<p>“Our on-premises strategy has worked for years. But it has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.”</p>
<p>They are mostly just repeating themselves here. I again see they mention multiple environments. I see there may be questions about dealing with outages. There could be questions about fixing a misconfigured system. Scaling should already take care of inadequate capacity and spikes in traffic. And we already mentioned monitoring. And again, things need to be scalable, resilient and we should support multiple environments. So I think we covered everything here. You can see that this case study covers a lot of ground. But that should be it for the EHR Healthcare case study.</p>
<h1 id="Helicopter-Racing-League"><a href="#Helicopter-Racing-League" class="headerlink" title="Helicopter Racing League"></a>Helicopter Racing League</h1><p>In this lesson, I am going to walk you through the case study for a fictional company called “Helicopter Racing League”.</p>
<p>Let’s start with the company overview:</p>
<p>“Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.”</p>
<p>So what are the key terms that I see here? Well first, I notice that this is a “global sports league”. Now that implies that our services need to support multiple regions. A global audience will require using global services. Things like CDNs and load balancers. Next I see that this will be a paid service. So there might be some questions about collecting money. Possibly storing credit card information. You should be familiar with storing and encrypting sensitive data. Also this means users are going to need to log in. So you could get some questions about user authentication and authorization as well.</p>
<p>And again here, we see that we need to support customers all over the world. And we see that there will be live telemetry captured. So possibly you might get some questions about internet-of-things. I don’t think this says if they are real helicopters or just like drones. But either way, if you are sticking sensors on the helicopters and capturing data from them in real time, that could definitely suggest an IoT question or two.</p>
<p>We also see here that we are going to use this live data to make predictions. So that sounds like you could get some AI &amp; machine learning questions as well. This is starting to sound like it is going to require some very compute intensive loads. Next, let’s read the solution concept:</p>
<p>“HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.”</p>
<p>Well, that confirms that AI and ML are potential topics for questions. You should make sure to be familiar with the main offerings available there. This says they want to migrate their existing services, so make sure you know how to handle migration in AI and ML as well.</p>
<p>Here I see the phrase “emerging regions”. Now that is interesting. So not only will the customers be all over the world, but they are going to be outside of your typical areas. That sounds like your services will need to have many different regions; not just the US and Eurape. This definitely sounds like you are going to have to make heavy use of CDNs because your viewers might not have access to reliable, high speed internet. So I could see getting some questions that ask you to design a streaming solution based around these constraints.</p>
<p>Here we can see that the content is going to be both real-time and recorded. So the recorded content can be cached, but the real-time videos cannot. So I would start thinking about how you can stream content to viewers in more remote areas. Let’s move on the existing technical environment:</p>
<p>“HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows: </p>
<p>Existing content is stored in an object storage service on their existing public cloud provider. Video encoding and transcoding is performed on VMs created for each job. Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.”</p>
<p>So it seems like they are doing a lot of video editing and recording. Now, it doesn’t sound like that is going to happen in the cloud. However, they probably want to store all this footage in the cloud. Video can take up a lot of space. So you might get questions about storing massive amounts of data or uploading huge amounts of data. They also might ask you how to store all this in a cost effective way. You will want to know how to search it and retrieve it when it’s needed. Those sort of things.</p>
<p>We can see that we are potentially going have to deal with both encoding and transcoding. Now encoding is very compute heavy, but it can be done offline. So this sounds like a good use case for Spot VMs. Spot VMs would give you a bunch of temporary VMs for a cheaper price. Transcoding has to happen in real-time, so Spot VMs would not work for that. Instead you might have to use standard on-demand VMs for that on race day. Or you could look into using the transcoder API. Also, both encoding and transcoding video can be accelerated by using GPUs, so you might get a question about provisioning a VM with a GPU. So I would say that you should start thinking about how to accomplish the two tasks on GCP, and understand the different requirements of each.</p>
<p>Now having “truck mounted mobile data centers” is really quite interesting. I’m not sure exactly what the implications are. It’s possible that this might be an extra challenge to connect them to your GCP environment. You won’t be able to make the connection permanent, so I’m guessing you can’t use Interconnect. You might be stuck using a VPN connection. Also if you were to set up some firewall rules, you might not be able to hard code any IP addresses for white lists. This definitely has some potentially interesting implications. It’s probably worth sitting down and taking some time to think about how a mobile data center would be different from a standard on-prem environment.</p>
<p>Here we see they are currently using an object storage service, so you definitely should be familiar with Cloud Storage. Since we are talking about a lot of storage space being needed, I could also see you getting some questions on object storage classes or object lifecycle management. Again here we see that you could get asked about using VMs to do the encoding and transcoding. And here we see they are using Tensorflow for machine learning. You might be asked how they could best migrate this over to GCP. I would say there definitely is the possibility of getting some ML and AI questions. Ok, so let’s go through the business requirements:</p>
<p>“HRL’s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:</p>
<ul>
<li>Support ability to expose the predictive models to partners</li>
<li>Increase predictive capabilities during and before races:<ul>
<li>Race results</li>
<li>Mechanical failures</li>
<li>Crowd sentiment</li>
</ul>
</li>
<li>Increase telemetry and create additional insights</li>
<li>Measure fan engagement with new predictions</li>
<li>Enhance global availability and quality of the broadcasts</li>
<li>Increase the number of concurrent viewers</li>
<li>Minimize operational complexity</li>
<li>Ensure compliance with regulations</li>
<li>Create a merchandising revenue stream”</li>
</ul>
<p>So, we already covered AI &amp; ML. Latency could be an issue for some of the customers. But we already talked about that. This is interesting, so not only will we be dealing with Tensorflow models, but they might also want to share their models with partners. So you might want to brush up on how to do that. Now here we see that the predictions need to happen in real time. Predictions are made both before a race and during a race. So ask yourself, do you know how to do streaming predictions? How can you minimize latency to achieve real-time predictions?</p>
<p>We already covered telemetry and insights. And we need to measure engagement and make predictions. We covered that. We mentioned global availability. Quality is going to be a challenge. So ask yourself how do you maintain high quality streaming to all of your customers? In addition to having a high quality stream you also have to worry about scaling those streams up. So we are talking about a lot of bandwidth here. You are going to need some mechanism to autoscale up and down as required. You don’t want to bump up against some upper limit by having too many viewers at once.</p>
<p>Here is something new. We need to minimize operations complexity. So you want to avoid custom solutions. Try to stick to using GCP services. Also, things should automatically scale up and down as needed. You are not going to want to require a lot of manual intervention for dealing with problems. Managed services will be key. Since this is a paid service and you are dealing with a global audience, compliance could be an issue. You are going to have to deal with a lot of different local laws. You also will be handling money so there are regulations about that as well. Basically you need to be able to do audits and verify that the company is in compliance.</p>
<p>And finally there is going to be some sort of merchandising stream. Now, I’m not sure if this means they will be selling t-shirts or if they are just going to be showing ads. Or maybe both. But you should take some time to think about the impact that these things could have on the system architecture. Ok, let’s go through the technical requirements:</p>
<ul>
<li>“Maintain or increase prediction throughput and accuracy</li>
<li>Reduce viewer latency</li>
<li>Increase transcoding performance</li>
<li>Create real-time analytics of viewer consumption patterns and engagement</li>
<li>Create a data mart to enable processing of large volumes of race data”</li>
</ul>
<p>We already talked about AI &amp; ML. I already mentioned latency. We covered transcoding. And real-time analytics. Oh, here is something new. You should know what a data mart is and how to set one up. There is going to be a large amount of data, both video footage and telemetry data. So that sounds like there could be some Big Data questions in there as well. Finally, let’s go through the executive statement:</p>
<p>“Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.”</p>
<p>So yes, we talked about high quality video streaming. And that we need to do predictions. And of course these predictions need to be in real-time. Ok, this is new. Some of our prediction models need to be able to process data from all the races over the season. So not every prediction model is just going to work with current race data. Make sure you understand how to handle that. And that’s it. I think we have covered the Helicopter Racing League case study pretty thoroughly.</p>
<h1 id="Mountkirk-Games"><a href="#Mountkirk-Games" class="headerlink" title="Mountkirk Games"></a>Mountkirk Games</h1><p>In this lesson, I am going to walk you through the case study for a fictional company called “Mountkirk Games”. Let’s read the company overview:</p>
<p>“Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms after successfully migrating their on-premises environments to Google Cloud. Their most recent endeavor is to create a retro-style first-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-specific digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players across every active arena.”</p>
<p>Alright, so it appears they are going to be making online games for mobile platforms. So you might want to brush up on Google services that are designed for supporting mobile devices. I can also see that there are going to be hundreds of simultaneous players, so they are going to require infrastructure for dealing with a lot of different players at once. This is a video game so it has to be real-time and it has to be as low of latency as possible.</p>
<p>This geo-specific arena concept is interesting. So I assume they mean that all the players in Germany would be playing together, and all the players in Japan would be playing together. So expect some questions about detecting user location. And this of course implies you are going to be supporting multiple regions and zones across your services.</p>
<p>So it sounds like these arenas are going to be kept separate. But you also need to be able to access data from each and use that to create the global leaderboard. So maybe each arena is going to be a separate project, but you are going to have to access data from a separate project. So I could see some IAM permission questions popping up here. If the leaderboard is in real-time, you can’t rely on exporting the data to a bucket and then importing it later on. So maybe this will involve some APIs as well. Let’s read the solution concept:</p>
<p>“Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game’s backend on Google Kubernetes Engine so they can scale rapidly and use Google’s global load balancer to route players to the closest regional game arenas. In order to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.”</p>
<p>Ok, so again we see there could be a lot of simultaneous players. Now, this sounds like we need things to be very scalable. You don’t want to launch a new video game and no one can play it because you can’t handle the influx of new customers. Also, it appears that they are going to start using Google Kubernetes Engine. So I could think of quite a few questions about that. How to migrate a cluster. How to autoscale a cluster. How to create a new cluster. You should be familiar with all of that.</p>
<p>Oh, it looks like you could get questions on load balancers as well. You might want all players to enter at the same point, and then forward them to the appropriate regional arena. So think about how you would do that. And they also specifically call out that they plan to use Cloud Spanner. That makes sense given the requirements. So make sure you read up on Cloud Spanner. You should understand the difference between it and say Cloud SQL. You are going to have to use it to support multiple regions, so make sure you understand that. And you might even want to try to think about the types of data that should be stored in Cloud Spanner. And think about the types of data that should NOT be stored in Spanner. Alright, next let’s go through the technical environment:</p>
<p>“The existing environment was recently migrated to Google Cloud, and five games came across using lift-and-shift virtual machine migrations, with a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions and network policies. Legacy games with low traffic have been consolidated into a single project. There are also separate environments for development and testing.”</p>
<p>So I see mention of lift-and-shift VMs. You might be asked how to do that, so just be aware. You should also probably think about how you auto-scale these. Both up and down. You also should know that lift-and-shift usually doesn’t take advantage of any GCP-specific features, so you might want to start thinking about what it would take to optimize these. Maybe at some point, they want to containerize these VMs and run them on Kubernetes. So think about what that would take.</p>
<p>Here they explicitly say that each game is running in its own project. And they also use folders. So you should be familiar with the resource hierarchy of organization, folders, and projects. You need to understand permissions and policy inheritance. You should be prepared in case you get a question on how Project A can access something in Project B. And I see there could be potential questions on network policies as well. It also looks like you need to support multiple environments per game. So you want to understand how to set those up and maintain them. Your developers might need full control over development. They might have limited permissions in testing. And they probably have no permissions or at least very limited permissions for production. The business requirements are:</p>
<ul>
<li>“Support multiple gaming platforms</li>
<li>Support multiple regions</li>
<li>Support rapid iteration of game features</li>
<li>Minimize latency</li>
<li>Optimize for dynamic scaling</li>
<li>Use managed services and pooled resources</li>
<li>Minimize costs”</li>
</ul>
<p>So we already have seen multiple platforms. This could imply mobile phones, tablets, and computers. But it could also include gaming consoles as well. I’m not sure. It definitely seems like you need to think about juggling a huge array of different devices. Now, we already covered multiple regions. Rapid iteration is new. Software updates are going to need to be pushed out pretty frequently. Bug fixes are going to be very common, especially in games. And they are going to want to add new game features as well. So to me, this means you are going to have to think about things like Continuous Integration&#x2F;Continuous Deployment. With all these rapid changes, versioning is going to be critical. So you might get questions on Cloud Source Repository, Container Registry, and Artifact Registry. Basically, any solution you suggest has to be able to handle a constant amount of small change.</p>
<p>We already covered scaling. And we see managed services are going to be important. Pooled resources are going to be important. Definitely, you want to understand load balancers. And you are probably going to get at least one question about optimizing for cost. Autoscaling is great for this. You will use less resources when you can. And basically, you just want to understand cost differences between similar services. So, for example, you should realize that Cloud Spanner is very powerful, but it is also very expensive. For every option, you want to know the associated cost. Now let’s read the technical requirements:</p>
<ul>
<li>“Dynamically scale based on game activity</li>
<li>Publish scoring data on a near real–time global leaderboard</li>
<li>Store game activity logs in structured files for future analysis</li>
<li>Use GPU processing to render graphics server-side for multi-platform support</li>
<li>Support eventual migration of legacy games to this new platform”</li>
</ul>
<p>We already covered scale. We covered the leaderboards. Ok, here we see the potential for there being a lot of logs. That means you need to know how to store logs, how to organize logs, and how to search logs. I also notice that logs will be stored in structured files. So to me, that sounds like they are hinting about BigQuery. I would say you should be familiar with building a data warehouse and writing queries. Understand all the best practices about BigQuery. </p>
<p>Now they are specifically calling out GPUs here. So you might get some questions on that. You are going to want to know how to launch a GPU VM. And since they are planning on using GKE, you also want to know how to create node pools that are equipped with GPUs as well. And it looks like they want to migrate some of their older games, so you could get questions about containerizing VMs. On to the executive summary:</p>
<p>“Our last game was the first time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games using cloud-native design principles. Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top priority, although cost management is the next most important challenge. As with our first cloud-based game, we have grown to expect the cloud to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug fixes and new functionality.”</p>
<p>So this makes it look like they want to analyze player behavior. Make sure you think about collecting telemetry data and then using that for deriving insights about the players. I’ve already covered potential migration scenarios. Gaming platforms beyond mobile heavily implies gaming consoles to me. Latency and cost will be key requirements to optimize for. Ok here, advanced analytics capabilities heavily suggests that Big Data is going to be very important. So make sure you brush up on Big Query and any other Google product with the word “data” in it. Datprep, Dataflow, Data Studio. You could get questions on any of these. And we covered “rapidly iterate” already, so I think that covers the Mountkirk Games case study pretty well.</p>
<h1 id="TerramEarth"><a href="#TerramEarth" class="headerlink" title="TerramEarth"></a>TerramEarth</h1><p>In this lesson, I am going to walk you through the case study for a fictional company called “TerramEarth”. Let’s start with the company overview:</p>
<p>“TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their customers more productive.”</p>
<p>Ok, so nothing here is really jumping out at me. This seems a little bit too vague to identify any keywords. Let’s continue on to the solution concept:</p>
<p>“There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors during operation. A small subset of critical data is transmitted from the vehicles in real-time to facilitate fleet management. The rest of the sensor data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes of data per day.”</p>
<p>Ok, this section seems a lot more useful. I see that they will be collecting telemetry data from the vehicles. So this sounds like there could be some Internet of Things questions involved. It also looks like there could be some Big Data questions as well. We see many different sensors per vehicle and there are 2 million vehicles. So there could be a lot of data flowing in and being processed. You need to start thinking about how you would collect all this data, how you would store it, and then how you would process and query it. This case study sounds like it’s going to be very data-heavy.</p>
<p>So it looks like some of the data is going to be uploaded in real-time and then the rest is going to be batch uploaded at the end of the day. So you need to consider how you would potentially handle both scenarios. Now, here it says here that there is 200-500 MB per day. So if you take 200 MB * 2M &#x3D; 400 million megabytes. That’s 40 petabytes a day? So I can foresee questions on storing and archiving huge amounts of data. You are probably going to be asked about creating and running data pipelines. You should be comfortable with cleaning up and transforming data. Things like that. Let’s move on to technical environment:</p>
<p>“TerramEarth’s vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory and logistics management systems. The private data centers have multiple network interconnects configured to Google Cloud. The web frontend for dealers and customers is running in Google Cloud and allows access to stock management and analytics.”</p>
<p>Data aggregation and analysis is exactly what I would expect, based upon the previous sections. I see that clients are going to be from all over the world, so that means your services need to be running in multiple regions. This “multiple network interconnects” is interesting. You are not going to be dealing with a single on-prem environment. There will be multiple. So start thinking about the implications of that. This stock management and analytics implies that we are going to be doing lots of things with the data we derive. It’s not just going to be used for generating a few charts. This might end up getting fed into a Cloud SQL database or maybe a machine learning model. I would be expecting questions about any of the Google data tools. Now the business requirements are:</p>
<ul>
<li>“Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.</li>
<li>Decrease cloud operational costs and adapt to seasonality.</li>
<li>Increase speed and reliability of development workflow.</li>
<li>Allow remote developers to be productive without compromising code or data security.</li>
<li>Create a flexible and scalable platform for developers to create custom API services for dealers and partners”</li>
</ul>
<p>Predict and detect suggests you need to be ready for AI &amp; ML questions. “Rapidly ship parts for just-in-time repairs” suggests the use of real-time streaming predictions. It looks like they want to focus on decreasing costs. So think about things like: How can I store a lot of data for cheap? How can I save money on all the data processing? How can I scale down usage when we are in the off-season? It looks like they want more speed and reliability. That implies scalability. Think about redundancy as well. Maybe even a CI&#x2F;CD pipeline.</p>
<p>They have remote developers. So that means they are going to need to use either VPNs or maybe they are going to use Zero trust instead. So should know the tradeoffs between VPN and the Identity aware proxy. Now security is very important. So you also want to think about IAM. You want to think about keys. And you want to think about secrets. Again, they want flexibility and scalability. They need custom APIs, so know how to create those. And you should also learn the best practices for setting up APIs as well. For technical requirements we have the following:</p>
<ul>
<li>“Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting operations </li>
<li>Modernize all CI&#x2F;CD pipelines to allow developers to deploy container-based workloads in highly scalable environments</li>
<li>Allow developers to run experiments without compromising security and governance requirements</li>
<li>Create a self-service portal for internal and partner developers to create new projects </li>
<li>Request resources for data analytics jobs, and centrally manage access to the API endpoints</li>
<li>Use cloud-native solutions for keys and secrets management and optimize for identity-based access</li>
<li>Improve and standardize tools necessary for application and network monitoring and troubleshooting”</li>
</ul>
<p>Again, we see that you might get questions about setting up APIs. They say they want a gradual migration, so I’m thinking this might involve creating microservices. We get confirmation of CI&#x2F;CD pipelines here, so make sure you are familiar with all the services involved with those. They mention they are going to be using containers, so you want to know which compute resources support those. Containers usually imply Kubernetes, but not always. This is interesting. They want to be able to run experiments. Now I know App Engine makes this pretty easy, but there are other ways of doing that as well. You want to think about how you can roll out different versions at the same time. Maybe have half of your users on one, half on the other. And then you want to measure the results of that.</p>
<p>Security and governance will be important. It looks like they will need employees to be able to create new projects, so you want to understand how to set up permissions correctly to allow that. And I see data analytics jobs and API endpoints mentioned once again. They specifically mention key and secret-based management. So that confirms what I thought before. Expect questions about generating and storing keys. Know how to use Google Secret Manager. And this mention about identity-based management confirms my suspicion that you might be asked a question or two about Zero trust systems.</p>
<p>I see there is also the potential for questions on monitoring and troubleshooting. So, you want to know how to set up monitoring on GCP. You want to know how to enable logging and know where and how to search through the logs. Finally, let’s read through the executive statement:</p>
<p>“Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle downtimes. After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online fleet management services to our customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to the cloud.”</p>
<p>Ok, so they say we need to improve the operations of online fleet management. That sounds like more IoT stuff. As we saw before, they want to share their data. So think about the different ways to do that. Is it through exposing an API? Are you writing out files to a public bucket? Maybe you need to create some service accounts to access BigQuery tables. This part here about increasing autonomous operations translates to AI &amp; ML services, in my mind. And this last part means you might get some questions about migration. Alright, I think that covers just about everything in the TerramEarth case study.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Congratulations! You made it through the course.</p>
<p>Since this course was recorded, Google might have made one or more changes to the case studies. You should not assume that the case studies I presented here are the latest versions. Make sure to visit the Google Professional Cloud Architect study guide to review the latest copies of the use cases. </p>
<p>I highly suggest you read through them several times in order to get very familiar with each. Make sure you identify the main business and technical requirements. You also want to start thinking about which products and services might be the best fit for each company. If you go into the exam with this knowledge, it’s going to save you a good deal of time and it will help ensure that you can choose the best answer for any of the questions.</p>
<p>Well, that’s all I have for you today. Remember to give this course a rating, and if you have any questions or comments, please let us know. Thanks for watching, and make sure to check out our many other courses on Cloud Academy!</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/" rel="prev" title="GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21">
      <i class="fa fa-chevron-left"></i> GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/19/GCP-Professional-Architect-Google-Cloud-Networking-Challenge-23/" rel="next" title="GCP-Professional-Architect-Google-Cloud-Networking-Challenge-23">
      GCP-Professional-Architect-Google-Cloud-Networking-Challenge-23 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Preparing-for-the-Exam"><span class="nav-number">2.</span> <span class="nav-text">Preparing for the Exam</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EHR-Healthcare"><span class="nav-number">3.</span> <span class="nav-text">EHR Healthcare</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Helicopter-Racing-League"><span class="nav-number">4.</span> <span class="nav-text">Helicopter Racing League</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Mountkirk-Games"><span class="nav-number">5.</span> <span class="nav-text">Mountkirk Games</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TerramEarth"><span class="nav-number">6.</span> <span class="nav-text">TerramEarth</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary"><span class="nav-number">7.</span> <span class="nav-text">Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
