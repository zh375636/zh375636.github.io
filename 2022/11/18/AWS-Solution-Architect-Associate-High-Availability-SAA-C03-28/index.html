<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="High Availability (SAA-C03) IntroductionHello, and welcome to this course on high availability in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Solution-Architect-Associate-High-Availability-SAA-C03-28">
<meta property="og:url" content="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-High-Availability-SAA-C03-28/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="High Availability (SAA-C03) IntroductionHello, and welcome to this course on high availability in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T02:13:39.000Z">
<meta property="article:modified_time" content="2022-11-27T23:59:54.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-High-Availability-SAA-C03-28/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Solution-Architect-Associate-High-Availability-SAA-C03-28 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-High-Availability-SAA-C03-28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Solution-Architect-Associate-High-Availability-SAA-C03-28
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:39" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:39-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 19:59:54" itemprop="dateModified" datetime="2022-11-27T19:59:54-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-High-Availability-SAA-C03-28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-High-Availability-SAA-C03-28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="High-Availability-SAA-C03-Introduction"><a href="#High-Availability-SAA-C03-Introduction" class="headerlink" title="High Availability (SAA-C03) Introduction"></a>High Availability (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on high availability in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the various tools and services used to design highly available solution architectures that may be covered on the exam.</p>
<p>Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#112;&#112;&#111;&#114;&#x74;&#x40;&#99;&#x6c;&#111;&#117;&#x64;&#x61;&#x63;&#97;&#x64;&#101;&#x6d;&#x79;&#46;&#x63;&#x6f;&#109;">&#x73;&#x75;&#112;&#112;&#111;&#114;&#x74;&#x40;&#99;&#x6c;&#111;&#117;&#x64;&#x61;&#x63;&#97;&#x64;&#101;&#x6d;&#x79;&#46;&#x63;&#x6f;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about designing highly available solution architectures in AWS in preparation for the exam.</p>
<p>The objective of this course is to provide an introduction to the tools and services used to design highly available solution architectures in AWS, including:</p>
<ul>
<li>How to design for failure; </li>
<li>How to design for high availability and fault tolerance while still balancing cost efficiency; and</li>
<li>How to design for high availability with a variety of database services in AWS, including RDS, Aurora, and DynamoDB.</li>
</ul>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#x73;&#117;&#x70;&#x70;&#x6f;&#x72;&#x74;&#x40;&#99;&#108;&#x6f;&#x75;&#100;&#x61;&#x63;&#x61;&#x64;&#x65;&#x6d;&#x79;&#46;&#x63;&#111;&#x6d;">&#x73;&#117;&#x70;&#x70;&#x6f;&#x72;&#x74;&#x40;&#99;&#108;&#x6f;&#x75;&#100;&#x61;&#x63;&#x61;&#x64;&#x65;&#x6d;&#x79;&#46;&#x63;&#111;&#x6d;</a>. Thank you!</p>
<h1 id="Designing-for-Failure-at-the-Global-Infrastructure-Level"><a href="#Designing-for-Failure-at-the-Global-Infrastructure-Level" class="headerlink" title="Designing for Failure at the Global Infrastructure Level"></a>Designing for Failure at the Global Infrastructure Level</h1><p>Designing for failure is one of the most common architectural principles that we should all follow when building and deploying solutions on AWS. The phrase ‘Everything fails all of the time’ is a quote made by Dr. Werner Vogels, VP and CTO of AWS, and he’s said this a number of times, and this has helped to reinforce its importance as a design principle. It’s this mindset of understanding that failure will happen at some point within your architecture when running on the cloud, and we need to be prepared for it. </p>
<p>We don’t know when, how or why something might fail, but what we can do is prepare for failure and put steps in place to ensure we can recover from it both quickly and effectively, and so in effect, we design for failure. So how do we do this? Well let’s start with a simple scenario to see how we can improve it using the AWS global infrastructure as we go through different stages of design, all to help with the prevention of failure. </p>
<p>Let’s assume you have created a website which talks to a database on the back-end. This needs to be deployed in AWS, and so you create a new VPC in a single region, with a public subnet and a private subnet within the same availability zone. You provision a single RDS database in your private subnet, and a single EC2 instance in your public subnet acting as your web server. A very simple configuration, but it’s peppered with issues from a ‘design for failure’ perspective. </p>
<p>If an incident occurred with this design which affected the availability of us-east-1a availability zone, then your entire infrastructure would be impacted and inaccessible. Design for failure has clearly not been taken into account here! </p>
<p>Firstly, when deploying a new solution I would always suggest you use as much of the geographical infrastructure as possible to help you implement levels of high availability and remove single points of failure from your architectural designs. So this starts by having a grasp of the AWS Global architecture, primarily focusing on Availability Zones (AZ) and Regions. </p>
<p>Availability zones are essentially the physical data centers of AWS. This is where the actual compute, storage, network, and database resources are hosted that we as consumers provision virtually within our Virtual Private Clouds (VPCs). A common misconception is that a single availability zone is equal to a single data center. This is not the case, multiple data centers located close together can form a single availability zone.</p>
<p>Each AZ will always have at least 2 other AZs that are geographically located within the same area, usually a city, linked by highly resilient and very low latency private fiber optic connections. However, each AZ will be isolated from the others using separate power and network connectivity that minimizes impact to other AZs should a single AZ fail. These low latency links between AZs are used by many AWS services to replicate data for high availability and resilience purposes. Often, there are three, four, five, or more AZs linked together via these low latency connections. This localized geographical grouping of multiple AZs is defined as an AWS Region.</p>
<p>So a region is a collection of availability zones that are geographically located close to one other. This is generally indicated by AZs within the same city. AWS has deployed them across the globe to allow its worldwide customer base to take advantage of low latency connections. Every Region will act independently of the others, and each will contain at least three Availability Zones.</p>
<p>So with this flexibility of being able to deploy your resources across different geographical locations, it makes logical sense to adopt as many of these as business and operational requirements dictate. If your solution or service doesn’t need to span across multiple regions, then you should certainly consider the option of deploying your resources across multiple availability zones. </p>
<p>So let’s circle back to our basic design that we had in our scenario to see how we can improve it. In our original design our VPC was using a single AZ for both subnets. This means that if a disaster occured which impacted the same AZ that we are using in our VPC then it would impact 100% of our resources making them inaccessible. So, let’s re-architect this using multiple AZs. </p>
<p>So as you can see in this newly designed VPC we are now using more than 1 AZ for both the Private and Public subnets, us-east-1a and us-east-1b. By doing so we have also deployed additional resources. An additional Web server has been provisioned in the us-east-1b AZs public subnet in addition to an Application Load balancer to manage the traffic destined for the target group of the web servers. Application load balancers allow you to provide a flexible feature set including advanced routing and visibility features aimed for application architectures and commonly used for applications to balance HTTP or HTTPS traffic. To learn more about load balancers, please see our existing course here: </p>
<p>You will also notice that we have introduced a standby RDS instance through the use of a multi-az RDS deployment. When multi-az is configured, a secondary RDS instance, known as a replica, is deployed within a different availability zone within the same region as the primary instance. Its single and only purpose is to provide a failover option for the primary RDS instance should it become unavailable. To learn more about using RDS in a multi-az configuration, please refer to our existing course here. With these small configurational changes of utilizing an additional AZ we have now added high availability into our design in case of an AZ failure. </p>
<p>Let’s suppose a natural disaster occured and the availability of us-east-1a az became unstable and unavailable for a period of time, what would happen? You would lose half of your infrastructure, you’d lose access to your EC2 web server and your primary RDS instance, but your solution would still be operational, as you planned for such an event. The application load balancer would continue to receive and deliver traffic to your remaining web server in the target group in the us-east-1b az. Similarly, the failure of the Primary RDS instance would trigger an automated response that would promote the standby instance as a new Primary RDS database and continue to serve both write and read requests. This is a very high level example of ‘designing for failure’. Having the foresight to implement a solution that recovers from a major outage has ensured you remain operational and continue to serve your customers. </p>
<p>Now of course failure doesn’t just occur from a global infrastructure perspective, we can experience outages and failures at all different levels of a solution, for example you might experience an incident with a single EC2 instance, perhaps in our scenario, your website started to gain more attention and traffic load increased, even with 2 EC2 instances, one in each AZ the performance was not enough to meet the demand. This drain of your EC2 performance resulted in poor performance resulting in an outage, this is a similar effect to what happens when a denial of service attack occurs. To rectify this, we could add additional features to help with high availability, such as Auto-scaling groups for our EC2 instances to ensure we maintain enough capacity to serve traffic requests and to recover from instance failures. </p>
<p>However, in this course I want to maintain focus on the global infrastructure elements of the design. With this in mind, we need to realize that failures don’t just occur at the AZ level, there can be consequences that occur at the Regional level too, creating a much wider problem! If there was a regional failure of some sort, for example a service disruption to both EC2 and RDS across the us-east-1 region then this current design doesn’t provide enough flexibility to recover. You might be thinking, ‘yeah but, how likely is that going to happen?’ In reality, the answer is not very likely at all, but it can and HAS happened. Take a look at this summary report written by AWS covering the Amazon EC2 and Amazon RDS Service Disruption in the US East Region that happened a number of years ago: </p>
<p>Obviously, architecting for a regional failover carries a lot more cost, management and design features as you may need to duplicate a lot of your resources, in addition to maintaining data synchronization and data transfer, and deal with potential latency issues too. So when it comes down to how far you architect for ‘design for failure’ at the global infrastructure level, you need to determine the business impact of when your solution becomes unavailable that you’re trying to design for failure for. This includes understanding the Recovery Time Objective (RTO) and Recovery Point Objective (RPO) of your solution.</p>
<p>For example, would it cost your business 10’s of thousands of dollars if you incurred downtime for just a few seconds, effectively having a very short RTO? If so, then operating in a single Region might not be a great idea, it’s likely that you’ll want to architect a solution with regional failover capabilities. However, if you had a much longer RTO, and you incurred downtime due to a regional failure, then a multi availability zone design might be a better solution for you and meet the needs of your business more effectively. It’s a case of weighing up the cost - benefit - risk of your deployment design. </p>
<p>In our simple scenario of a web layer being run by EC2 instances, and a database layer using RDS there are a couple of points we’d need to consider when deploying a multi-regional solution for DR purposes. Such as the inclusion of design considerations with using Route 53 and Amazon CloudFront which would both sit in front of our Application Load Balancers</p>
<p>From a database perspective you would also need to consider data replication, and this would depend on a couple of factors as to how you implemented this replication which would depend on your RTO and RPO. If these values were longer rather than shorter, then one of the most cost effective methods on managing this replication would be to use a snapshot and restore approach. In this instance, you would simply perform the following steps.</p>
<ul>
<li>Firstly you would implement a schedule of when you would create a snapshot of your primary RDS database. This can be automated using other services such as AWS Lambda and CloudWatch EventBridge </li>
<li>Next, you would need to copy these snapshots to your designated DR Region.</li>
</ul>
<p>In the event of a disaster, you would quickly be able to get operational again in a new region using the snapshot and copy approach. However, like I say, this is only appropriate if your business RTO and RPO allow for this approach as copying these snapshots could take a number of hours.</p>
<p>Should you have a much shorter RTO and RPO, then this approach might not be very feasible, in which case you would need to adopt a different approach. In this case, it is recommended that you use the AWS Database Migration service. To learn more about this service, please see our existing course here: </p>
<p>If you’d like to learn how to carry out a continuous and on-going replication of your RDS server, then you can visit this AWS documentation here.</p>
<h1 id="Backup-and-DR-Strategies"><a href="#Backup-and-DR-Strategies" class="headerlink" title="Backup and DR Strategies"></a>Backup and DR Strategies</h1><p>A company typically decides on an acceptable business continuity plan based on the financial impact to the business when systems are unavailable. So the company determines the financial impact by considering many factors, such as the loss of business and the damage to its reputation due to downtime or the lack of systems availability.</p>
<p>Now, the common metrics for business continuity, commonly refer to Recovery Time Objective and the Recovery Point Objective. So let’s just delve into these two concepts.</p>
<p>So Recovery Time Objective or the RTO, is the time it takes after a disruption to restore a business process to its service level as was defined by the operational level agreement. So for example, if a disaster occurs at 12 o’clock lunchtime and the RTO is eight hours, then the disaster recovery process should restore the business process to the acceptable service level by 8:00 p.m.</p>
<p>Now the Recovery Point Objective or RPO is the acceptable amount of data loss measured in time. So just to confuse everyone, it is also a time value, but it’s a slightly different one. The two are quite different concepts. So the acceptable amount of data loss measured in time. So for example, if that disaster occurred at 12 o’clock around lunchtime and the RPO is one hour, the system should recover all data that was in the system before 11:00 a.m. So the data loss will spend only one hour between 11:00 a.m and 12:00 p.m.</p>
<p>So they’re quite different, aren’t they? Like the Recovery Point Objective is what’s the last point in the data can we successfully absorb if there is an outage and for a highly transactional business, that’s going to be extremely low. I mean, even having an hour of data loss, if you’re dealing with customer transactions, is not gonna be acceptable to a transactional business. </p>
<p>So, and that’s gonna impact how we design our systems to be as highly available and as fault tolerance possible for a transactional business like that. And another scenario might be that the business can absorb some outage, but it does need to have the systems up and running again as soon as possible. So the RTO might be the priority. And part of your business continuity planning needs to be, to define what is the priority, the Recovery Time Objective, i.e how quickly we can get the system back up and running again so it can answer queries and requests and be fully functional, or is it the Recovery Point Objective that’s our priority that we must be able go back to the last possible point in time without any data loss.</p>
<p>So there’s a number of different scenarios that we can apply in AWS to help meet the RPOs and RTOs. And the first one is what we call backup and restore now with backup and restore data is stored as a virtual tape library using AWS storage gateway or another network appliance or of a similar nature.</p>
<p>We can use import and export AWS import and export to shift large archives or in setting up archives for a backup and restore scenario. Then in the disaster, archives are recovered from Amazon S3 and restored as if we were using a virtual tape.</p>
<p>Now we need to select the appropriate tools and methods to back up our data into AWS. Three things to keep in mind first, ensure that you have an appropriate retention policy for this data. So how long we go to keep, these virtual tape archives for, is it six months? Is it a year? Is it five years? What are the commercial and, compliance requirements, etc?</p>
<p>The second is to ensure that the appropriate security measures are in place for this data, including the encryption and access policies. So can we guarantee that where it’s been stored is gonna be suitably secure and third, we need to make sure that we regularly test the recovery of the data and the restoration of the system.</p>
<p>Alright, so the second potential design is what we call pilot light. And in pilot light data is merit and the environment is scripted as a template, which can be built out and scaled in the unlikely event of a disaster and a few steps that we need to go through to make pilot light work.</p>
<p>First, we set up Amazon EC2 instances to replicate or mirror our data. Second, we ensure that we have all supporting custom software packages available in AWS. So that can be quite an over operational overhead to ensure that we have all of the latest and greatest custom software packages that we need for our environment available in AWS. And third, we need to create and maintain Amazon machine images of a key service where fast recovery is required. And then fourth, we need to regularly run these servers, test them and apply any software updates and configuration changes to ensure that they’re going to match what our production environment currently is in the event of a disaster. And then fifth we need to consider automating the provisioning of AWS services as much as possible with cloud formation.</p>
<p>So what that looks like in our recovery phase. So in the unlikely event of a disaster to recover the remainder of our environment around our pilot light, we can start our systems from the Amazon machine images on the appropriate instance types. And for our dynamic data servers, we can resize them to handle production volumes as needed or add capacity accordingly. </p>
<p>So basically horizontal scaling is often the most cost effective and scalable approach to add capacity to the pilot light system. As an example, we can add more web servers at peak times during the day. However, we can also choose larger Amazon EC2 instance types and thus scale vertically for applications such as our relational databases and file storage, for example. And any required DNS updates can be done in parallel.</p>
<p>Okay, so the third scenario we can implement is what we call warm stand by. And our key steps for preparation in a warm stand by, which is, as it says, essentially, ready to go with all key services running in the most minimal possible way.</p>
<p>So, first we set up our Amazon EC2 instances to replicate or mirror data. Secondly, we create a maintain Amazon machine images as required, third, we run our application using a minimum footprint of AWS EC2 instances or AWS infrastructure. So it’s basically the bare minimum that we can get by with. And forth, we patch and update software and configuration files in line with our live environment. So we’re essentially running a smaller version of our full production environment.</p>
<p>Then during our recovery phase, in the case of failure of the production system, the standby environment will be scaled up for production load. And the DNS records will be changed to route all traffic to the AWS environment.</p>
<p>Now our fourth potential scenario is what we call multi-site with multi-site we set up our AWS environment to duplicate our production environment. So essentially we’ve got a mirror of reproduction running in AWS. Firstly, we set up DNS waiting or a similar traffic routing technology if we’re not using route 53 to distribute incoming requests to both sites, we also configure automated fail over to reroute traffic away from the affected site in the event of an outage.</p>
<p>Now in our recovery phase, traffic is cut over to the AWS infrastructure by updating the DNS record in Route 53 and all traffic and supporting data queries are supported by the AWS infrastructure. Our multi-site scenario is usually the preferred one and where time is a priority Recovery Time and Recovery Point Time are our priorities and costs are not the main constraint the nets would be the ideal scenario.</p>
<p>Okay, so one key thing to ensure when we’re running any of our scenarios is to ensure that we test the recover data. So once we’ve restored our primary site to a working state, we then need to restore to a normal service, which is often referred to as a fail back process. So depending on your DR strategy, this typically means reversing the flow of data replication so that any data updates received while the primary site was down can be replicated back without loss of data.</p>
<p>Here’s the first for backup and restore. First we freeze the data changes on the DR site. Second, we take it back up. Third, we restore the backup to the primary site. Fourth, we re point users to the primary site and five, We unfreeze the changes. With pilot light, warm stand by and multi-site first we establish reverse mirroring and replication from the DR site back to the primary site. once the primary site has caught up with the changes. Second, we freeze data changes to the DR site. And then third, we re point users to the primary site. And then finally we unfreeze the changes.</p>
<p>Now most of those scenarios involve some sort of replication of data so let’s just talk through some of the considerations on that. When you replicate data to a remote location, you really need to think through a number of factors. First, the distance between the sites now larger distances, typically a subject more latency or jitter.</p>
<p>What is the available bandwidth? The breadth in variability of the interconnections is going to be important. If that bandwidth doesn’t support high burst activity, then it’s not gonna suit some replication models. And what is the data rate required by your applications? The data rates should be lower than the available bandwidth. And what is the replication technology that you plan to use? The replication technology should be parallel so that it can use the network effectively.</p>
<p>So let’s just look through a couple of the replication options we have, and these can be a bit confusing. So let’s just take this step by step. Okay, there’s two types of replication, synchronous replication and asynchronous replication. These two can be very confusing when you’re sitting in an exam trying to remember which one is which so let’s just step through this and hope to give you some tips for how to remember it.</p>
<p>With synchronous replication data is atomically updated in multiple locations. So this puts a dependency on network performance and availability. So when deploying a multi-AZ mode, Amazon RDS uses synchronous replication to duplicate data to a second availability zone. This ensures that data is not lost. If the primary availability zone becomes unavailable.</p>
<p>Now, the other type of replication is asynchronous replication. And with asynchronous replication, data is not atomically updated in multiple locations. It is transferred as network performance and availability allows and the application continues to write data that might not be fully replicated yet. So many database systems support asynchronous data replication, the database replica can be located remotely and the replica does not have to be completely synchronized with the primary database server. And that’s acceptable in many scenarios, for example, as a backup source or reporting read only use cases.</p>
<p>In addition to both database systems, you can also extend asynchronous replication to network file systems and data volumes. Right, very good. Let’s do a quick summary of the four options we have for disaster recovery so you’re well prepped for your exam.</p>
<p>So backup and restore, like using AWS as a virtual tape library, is likely to have the highest Recovery Time Objective because we need to factor in the time it would take us to access or download backup archives. Our Recovery Point Objective is most likely to be quite high as well, because if we’re only doing daily backups, it could be up to 24 hours.</p>
<p>With pilot light we’ve got that minimal version of our environment running on AWS, which can be expanded to full-size when needed. We’ve got potentially a lower Recovery Time Objective than we would for backup and restore, but we need to factor in that we still may need to install applications or patches onto our AMIs before we have a fully operational system. </p>
<p>Our Recovery Point Objective is going to be since the last snapshot. So it’s going to be reasonably low. For warm stand by, we’ve got that scaled-down version of a fully functional environment, always running. So our Recovery Time Objective is likely to be lower than pilot light as some of our services are always running.</p>
<p>Our Recovery Point Objective is ideally going to be quite low since it will be since our last data write if it’s a master-slave multi-AZ database, even if it’s asynchronous only, it’s still going to give us quite a good Recovery Point Objective. And the benefit of having a warm standby environment is that we can actually use it for Dev test or for one-off projects or for skunkworks, etc. And a multi site is that fully operational version of our environment running off-site or in another region. And it’s likely to give us our lowest Recovery Time Objective if we’re using active&#x2F;active fail-over, it could be a matter of seconds, with our Recovery Point Objective, likewise, it depends on the choice of data replication that we choose. But it’s gonna be since our last asynchronous or synchronous DB write and using Route 53 as an active&#x2F;active fail over, it’s gonna give us a very, very aggressive, short Recovery Point Objective and Recovery Time Objective.</p>
<p>The considerations with that is that the cost is going to be reasonably higher proportionately than the other three options we have and we need to factor in it that there will be some ongoing maintenance required to keep that kind of environment running.</p>
<p>The benefit is that we have a way of regularly testing our DR strategy. We also have a way of doing Blue-green deployments, and it gives us a lot more diversity in our IT infrastructure.</p>
<h1 id="High-Availability-vs-Fault-Tolerance"><a href="#High-Availability-vs-Fault-Tolerance" class="headerlink" title="High Availability vs Fault Tolerance"></a>High Availability vs Fault Tolerance</h1><p>What’s the difference between high availability and fault tolerance?</p>
<p>This is a question that gets asked a lot, I hear it from people who have had years of experience within the IT industry, and those who are new and just starting out. Either way, there is clearly some confusion between the two, and understandably so. They both ultimately have the same goal, to keep your systems up and running should something fail within your architecture, but there is a difference.</p>
<p>High Availability can be defined by maintaining a percentage of uptime which maintains operational performance, and so this can closely be aligned to an SLA. In fact, AWS has many SLAs for its services where they implement their own level of resilience and management to maintain that level of high availability. </p>
<p>You may also have your own SLAs for services you provide for your customers, but let’s look at a scenario to help explain the difference. Let’s assume that you have an application which has to run across a minimum of two EC2 instances to meet, let’s say, an SLA of 99.9% which allows for a downtime of 43.83 minutes per month, then we could architect our infrastructure like this. </p>
<p>Within a region we could use two different availability zones, and in each AZ we could have two EC2 instances, which are all associated with an Elastic load balancer. So in this example we have different elements contributing to a highly available solution. We have the use of two AZs and additional EC2 instances. So if an instance fails, we still have plenty of compute resources, or if an entire AZ fails then we still have the minimum of two instances to maintain the required SLA.</p>
<p>Now, let’s look at Fault Tolerance, which expands on High Availability to offer a greater level of protection should components begin to fail in your infrastructure, however, there are usually additional cost implications due to the greater level of resiliency offered. But the upside is that your uptime percentage increases and there is no interruption of service should 1 or more components fail. With that in mind, we could argue that having two AZs with two EC2 instances in each is fault-tolerant at the AZ level, as operations would be maintained at the loss of an AZ as we’d still have the minimum number of instances still running, but should another failure occur, then the SLA would be impacted. </p>
<p>So let’s look at how we take our high availability scenario that we just sketched out and adopt it with an increased fault-tolerant design approach.</p>
<p>So previously we had our single region approach, and so to increase the uptime of this solution we could deploy the app across an additional AWS region. So we could literally mirror the environment from a single region to a 2nd region. So, this means should an EC2 instance fail, we still have compute, should an AZ fail, again, we still have enough compute capacity, but now we can also maintain operation should an entire Region fail. If it does, we can then still suffer further EC2 outages, and AZ outages of that secondary region and still maintain the minimum requirements of having two EC2 instances at all times. </p>
<p>This of course offers a far greater uptime availability compared to the previous highly available solution we had with a single region, but it comes at the increased cost of running two active environments, which can tolerate any component to fail. Remember, we need to have this secondary region running to take advantage of avoiding any downtime should the primary region fail. </p>
<p>So from this we can surmise that fault-tolerant systems are intrinsically highly available, but as we have seen, a highly available solution is not necessarily completely fault-tolerant. </p>
<p>It’s down to you as to the level of High Availability or Fault Tolerance you want to implement, and this really depends on the business impact it would have when components begin to fail, and do bear in mind, it’s not if a failure occurs, it’s when a failure occurs.</p>
<h1 id="Using-AWS-Storage-Gateway-for-On-premise-Data-Backup"><a href="#Using-AWS-Storage-Gateway-for-On-premise-Data-Backup" class="headerlink" title="Using AWS Storage Gateway for On-premise Data Backup"></a>Using AWS Storage Gateway for On-premise Data Backup</h1><h3 id="Resources-referenced-within-this-lecture"><a href="#Resources-referenced-within-this-lecture" class="headerlink" title="Resources referenced within this lecture"></a>Resources referenced within this lecture</h3><p><a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/storagegateway/latest/userguide/Requirements.html#requirements-backup-sw-for-vtl">Supported Third-Party Backup Applications for Tape Gateway</a></p>
<h3 id="Lecture-Transcript"><a href="#Lecture-Transcript" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h3><p>Hello, and welcome to this lecture. I want to explain when you may want to use AWS Storage Gateway along with the different options available when using this service.</p>
<p>Storage Gateway allows you to provide a gateway between your own data center’s storage systems such as your SAN, NAS or DAS and Amazon S3 and Glacier on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>.</p>
<p>The Storage Gateway itself is a software appliance that can be stored within your own data center which allows integration between your on-premise storage and that of AWS. This connectivity can allow you scale your storage requirements both securely and cost efficiently.</p>
<p>The software appliance can be downloaded from AWS as a virtual machine which can then be installed on your VMware or Microsoft hypervisors.</p>
<p>Storage Gateway offers different configurations and options allowing you to use the service to fit your needs. It offers file, volume and tape gateway configurations which you can use to help with your DR and data backup solutions.</p>
<p>Let me explain the differences between each of these configurations, starting with file gateways.</p>
<p>File gateways allow you to securely store your files as objects within S3. Using as a type of file share which allows you mount on map drives to an S3 bucket as if the share was held locally on your own corporate network. When storing files using the file gateway they sent to S3 over HTTPS and are also encrypted with S3’s own server side encryption SSE-S3.</p>
<p>In addition to this local a on-premise cache is also provisioned for accessing your most recently accessed files to optimize latency with also helps to reduce egress traffic costs. When your file gateway’s first configured you must associate it with your S3 bucket which the gateway will then present as a NFS V.3 or V4.1 file system to your internal applications.</p>
<p>This allows you to view the bucket as a normal NFS file system, making it easy to mount as a drive on Linux or map a drive to it in Microsoft. Any files that are then written to these NFS file systems are stored in S3 as individual objects as a one to one mapping of files to objects.</p>
<p>The second option we have as a gateway configuration are volume gateways, and these can be figured in one of two different ways, Stored volume gateways and cached volume gateways.</p>
<p>Let me explain stored volume gateways first.</p>
<p>Stored volume gateways are often used as a way to backup your local storage volumes to Amazon S3 whilst ensuring your entire data library also remains locally on-premise for very low latency data access. Volumes created and configured within the storage gateway are backed by Amazon S3 and are mounted as iSCSI devices that your applications can then communicate with.</p>
<p>During the volume creation, these are mapped to your on-premise storage devices which can either hold existing data or be a new disk. As data is written to these iSCSI devices the data is actually written to your local storage services such as your own NAS, SAN or DAS storage solution. However the storage gateway then asynchronously copies this data to Amazon S3 as EBS snapshots.</p>
<p>Having your entire dataset remain locally ensures you have the lowest latency possible to access your data which may be required for specific applications or security compliance and governance controls whilst at the same time providing a backup solution which is governed by the same controls and security that S3 offers.</p>
<p>Volumes created can be between 1GiB and 16TB and for each storage gateway up to 32 stored volumes can be created which can give you a maximum total of 512TB of storage per gateway. Storage volume gateways also provide a buffer which uses your existing storage disks. This buffer is used as a staging point for data that is waiting to be written to S3.</p>
<p>During the upload process the data is sent over an encrypted SSL channel and stored in an encrypted format within S3. To add to the management and backup of your data storage gateway makes it easy to take snapshots of your storage volumes at any point, which are then stored as EBS snapshots on S3. It’s worth pointing out that these snapshots are incremental ensuring that only the data that’s changed since the last backup is copied helping to minimize storage costs on S3.</p>
<p>As you can see, gateway stored volumes makes recovering from a disaster very simple. For example, let’s consider the scenario that you lost your local application and storage layers on-premise Providing you had provision for such a situation you may have AMI templates that mimic your application tier which you could provision as EC2 instances within AWS. You could then attach EBS volumes to these instances which could be created from the storage gateway volume snapshots which would be stored on S3 giving you access to your production data required. Your applications storage infrastructure could potentially be up and running again in a matter of minutes within a VPC with connectivity from your on-premise data center.</p>
<p>Let me now compare this option to the second volume gateway option of cached volume gateways. Cached volume gateways are differed to stored volume gateways in that the primary data storage is actually Amazon S3 rather than your own local storage solution.</p>
<p>However cache volume gateways do utilize your local data storage as a buffer and the cache for recently accessed data to help maintain low latency, hence the name, Cache Volumes.</p>
<p>Again, during the creation of these volumes they are presented as iSCSI volumes which can be mounted by an application service. The volumes themselves are backed by the Amazon S3 infrastructure as opposed to your local disks as seen in the stored volume gateway deployment. As a part of this volume creation you must also select some local disks on-premise to act as your local cache and a buffer for data waiting to be uploaded to S3.</p>
<p>Again, this buffer is used as a staging point for data that is waiting to be written to S3 and during the upload process, the data is encrypted using an SSL channel where the data is then encrypted within SSE S3. The limitations is slightly different with cache volume gateways in that each volume created can be up to 32TB in size with support for up to 32 volumes meaning a total storage capacity of 1024TB per cache volume gateway.</p>
<p>Although all of your primary data used by applications is stored in S3 across volumes, it is still possible to take incremental backups of these volumes as EBS snapshots. In a DR scenario, and as I mentioned in the previous section, this then enables quick deployment of the datasets which can be attached to EC2 instances as EBS volumes containing all of your data as required.</p>
<p>The final option with AWS Storage Gateway is a tape gateway known as Gateway VTL. Virtual Tape Library. This allows you again to back up your data to S3 from your own corporate data center but also leverage Amazon Glacier for data archiving. Virtual Tape Library is essentially a cloud based tape backup solution replacing physical components with virtual ones.</p>
<p>This functionality allows you to use your existing tape backup application infrastructure within AWS providing a more robust and secure backup and archiving solution. The solution itself is comprised of the following elements.</p>
<p>- Storage Gateway. The gateway itself is configured as a tape gateway which as a capacity to hold 1500 virtual tapes.</p>
<p>- Virtual Tapes. These are a virtual equivalent to a physical backup tape cartridge which can by anything from 100 gig to two and a half terabytes in size. And any data stored on the virtual tapes are backed by AWS S3 and appear in the virtual tape library.</p>
<p>- Virtual Tape Library. VTL. As you may have guessed these are virtual equivalents to a tape library that contain your virtual tapes.</p>
<p>- Tape Drives. Every VTL comes with ten virtual tape drives which are presented to your backup applications is iSCSI devices.</p>
<p>- Media Changer. This is a virtual device that manages tapes to and from the tape drive to your VTL and again it’s presented as an iSCSI device to your backup applications.</p>
<p>- Archive. This is equivalent to an off-site tape backup storage facility where you can archive tapes from your virtual tape library to Amazon Glacier which as we already know, is used as a cold storage solution. If retrieval of the tapes are required, storage gateway uses the standard retrieval option which can take between 3 - 5 hours to retrieve your data.</p>
<p>Once your storage gateway has been configured as a tape gateway, your application and backup software can mount the tape drives along with the media changer is iSCSI devices to make the connection.</p>
<p>You can then create the required virtual tapes as you need them for backup, and your backup software can use these to back up the required data which is stored on S3.</p>
<p>For a list of the support and third party backup applications, please visit the link on screen.</p>
<p>When you want to archive virtual tapes for maybe cost optimization or compliance and governance or even DR, then the data is simply moved from Amazon S3 to Amazon Glacier.</p>
<h1 id="RDS-Multi-AZ"><a href="#RDS-Multi-AZ" class="headerlink" title="RDS Multi AZ"></a>RDS Multi AZ</h1><p>Hello and welcome to this lecture where I’m going to explain what Multi-AZ actually is when we are referring to Amazon RDS. Multi-AZ simply means Multi-Availability Zone, and so right away, we can ascertain that this is a feature that is used to help with resiliency and business continuity. When Multi-AZ is configured, a secondary RDS instance, known as a replica, is deployed within a different availability zone within the same region as the primary instance. That’s its single and only purpose, to provide a failover option for a primary RDS instance. </p>
<p>It’s not to be used as a secondary replica to offload read-only traffic to. That is the role of the read replica, which is very different. It’s important to understand that key difference, and this difference will become clearer as we make our way through this course. The replication of data between the primary RDS database and the secondary replica instance happens synchronously. Amazon RDS offers different configurations for Multi-AZ instances based on the database engine type. So, let’s look at the differences between those, and I wanna start off by looking at Oracle, MySQL, MariaDB, and PostgreSQL. All of these database engines use the failover mechanism when Multi-AZ is in use and configured, but what does this mean? If you have configured Multi-AZ for one of these engine types and an incident occurs which causes an outage to the primary RDS instance, then the RDS failover process takes over automatically. </p>
<p>This process is managed by AWS and is not something that you need to manually perform or trigger. RDS will update the DNS record to point to the secondary instance. This process can typically take between 60 and 120 seconds. The length of time is very dependent on the size of the database, its transactions, and the activity of the database at the time of failover. This automatic changeover enables you to continue using the database without the need of an engineer making any changes to your environment. </p>
<p>This failover process will happen in the following scenarios: if patching maintenance is being performed on the primary instance, if the instance of the primary database has a host failure, if the availability zone of the primary database fails, if the primary instance was rebooted with failover, and if the database instance class on the primary database is modified. As you can see, activating Multi-AZ is an effective measure and precaution to implement to ensure you have resiliency built in should an outage occur, which may result from patching being performed on the instance to a complete AZ outage, which does, of course, happen and has happened. However, if this process is automatic and is performed by RDS, how can you be made aware of when this event occurs? You need to be notified of this event to enable you to understand what caused the issue with the primary instance to trigger the failover. The RDS Failover triggers an event which is recorded as RDS-EVENT-0025 when the failover process is complete. This allows you to configure RDS to notify you by SMS or SNS when this event is triggered. For more information on configuring RDS notifications based on events, please visit the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html">URL</a>. </p>
<p>These events are also recorded within the RDS Console as well to allow you to gain further information. Let me now talk about SQL Server Multi-AZ configuration which, instead of using the RDS failover mechanism, SQL Server Multi-AZ is achieved through the use of SQL Server Mirroring. To start with, the use of Multi-AZ is not available on all versions of SQL Server. Currently, at the time of writing this course, it supports the following versions. For the latest supported versions, please see the AWS documentation relating to SQL Server. The principle, however, is much the same between SQL Server Mirroring and RDS Failover, in that, both methods are used to provision a secondary instance to act as the primary instance in the event of an outage. SQL Server Mirroring provisions a secondary RDS instance in a separate AZ than that of the primary RDS instance to help with resilience and fault tolerance. </p>
<p>Previously, I mentioned that with the failover of Multi-AZ technique, AWS automatically updates the DNS record to point to the secondary instance. With SQL Server Mirroring, both the primary and secondary instances uses the same endpoint. During an incident, the mirroring process transitions the physical network address from the failed instance to the standby mirrored instance. Before enabling SQL Mirroring, you need to ensure you have your environment configured correctly first. You need to have a database subnet group configured, which has a minimum of two different AZs within it, and this DB subnet must then be associated to the SQL Server that is going to be mirrored. It’s worth noting that you can specify which availability zone the standby mirror instance will reside in, so it’s always good practice to architect your application that communicates with the RDS database across multiple AZs. To check which AZ the standby instance is in once you have enabled mirroring, you can either use the Console where it will stipulate the location of the secondary instance or you can use the AWS CLI command of describe-db-instances. </p>
<p>Amazon Aurora is different to the previous database engines that I’ve already discussed when it comes to resiliency across more than one single availability zone. By default, Amazon Aurora DB clusters are fault tolerant, which is designed to maintain the data to withstand a complete failure of an availability zone. This is achieved within the cluster by copying and replicating data across different instances in different AZs within a single region. Should a failure occur of the primary instance, then Aurora can automatically provision and launch a new primary instance, however, this process can take up to 10 minutes, which can be a significant amount of time if the database is being used within a critical production environment.</p>
<p> However, this time can be significantly reduced if you enable Multi-AZ on your Aurora cluster, which allows RDS to automatically provision a replica within a different AZ. With this replica in place, should a failure of the primary instance occur, the replica instance is promoted to the new primary instance and the load and processing is taken over by this existing replica automatically without having to wait the 10 minutes like you did in the previous example. This creates a highly available and resilient database solution. It’s possible to create up to 15 different replicas if required, and you can associate each a priority which defines which replica will take over as primary should an incident occur. That now brings me to the end of this lecture covering Multi-AZ across RDS database instances. Coming up next, I shall be covering RDS read replicas and the roles that these instances play.</p>
<h1 id="Read-Replicas"><a href="#Read-Replicas" class="headerlink" title="Read Replicas"></a>Read Replicas</h1><p>Hello and welcome to this lecture covering read replicas. So, we now know that Multi-AZ provides a feature that allows for the fast recovery of read&#x2F;write services when your primary RDS instance fails. So, let’s look at read replicas. read replicas are not used for resiliency or as a secondary instance in the event of a failover. Instead, they can be used by your application and other services or users to serve read-only access to your database data via a separate instance, a read replica. So, for example, let’s assume we have a primary RDS instance which serves both read and write traffic. Due to the size of the instance and the amount of read-intensive traffic being directed to the database for queries, the performance of the instance is taking a hit. To help resolve this, you can create a read replica. A snapshot will be taken of your database, and if you are using Multi-AZ, then this snapshot will be taken of your secondary database instance to ensure that there are no performance impacts during this process. Once the snapshot is completed, a read replica instance is created from this data. </p>
<p>The read replica then maintains a secure asynchronous link between itself and the primary database. At this point, read-only traffic can be directed to the read replica to serve queries, perhaps on business intelligence tools. By implementing read replicas, it helps to offload this traffic from the primary instance, and therefore, helping with the overall performance. Do be aware when thinking about deploying read replicas that they are only available for MySQL, MariaDB, and PostgreSQL database engines. However, for the latest supported engines for read replicas, it is always best to consult the AWS documentation, as this can change over time. Thankfully, it is possible to deploy more than one read replica for a primary database, and there are a number of different reasons as to why you might want to do this. By adding more than one read replica, it allows you to scale your read performance to a wider range of tools and applications that need to query the data without being restricted to a single read replica. It is also possible to deploy a read replica in a different region, which significantly helps to enhance your DR capabilities. </p>
<p>It’s also possible to promote an existing read replica to replace the primary database in the event of an incident. Also, during any maintenance that is being perform on your primary instance where I&#x2F;O requests may have been suspended, then read traffic can still be served by a read replica. I now want to talk about read replicas for each DB engine type, and the slight differences between them, starting with MySQL. Read replicas are only supported where the source database is running MySQL 5.6 or later. In addition to this, another prerequisite is that the retention value of the automatic backups of the primary database needs to be set to a value of one or more. Replication is also only possible when using an InnoDB storage engine, which is transactional as opposed to MyISAM which is non-transactional. It’s also possible to have nested read replica chains. For example, you could have a read replica which replicates from your source database. </p>
<p>This read replica can then act as a source database for another read replica, and so on. However, this chain can only be a maximum of four layers deep. If you do nest your read replicas underneath each other, then the same prerequisites discussed previously must also apply to the source read replica. For example, it must be running MySQL 5.6 and have a value of one or greater for automatic backup retention. Also bear in mind that you can only up to a maximum of five read replicas per source database, but a source database could be another read replica using the nested feature as just explained. You might be wondering what happens if you have a read replica created from a source database which has Multi-AZ configured. At which point, an outage occurs and shuts down the primary instance. What happens to the read replicas? Well, the answer is that RDS automatically redirects the read replica source to the secondary database to allow the asynchronous replication of data to occur. From an operational perspective, it’s important to understand how your read replicas are performing and if they are maintaining a high level of synchronization with their source database instance. Using Amazon CloudWatch, you can monitor this value through a metic called Amazon RDS ReplicaLag. This value will show you how many seconds the read replica is behind the source database. </p>
<p>You want this value to be as close to zero as possible, or ideally, read as zero. For the MariaDB engine type, much of the information remains the same as per MySQL read replica limitations. For example, you still need to have the backup retention period greater than zero, and again, you can only have five read replicas per source database. The same read replicas nesting rules apply and you also have the same monitoring metric for CloudWatch, however, you can be running any version of MariaDB for read replicas when running this DB engine. For PostgreSQL, there are a few more differences. I’ll start with the similarities, however. You still need the automatic backup retention to be greater than one and the limitation of read replicas is five per source database. However, the replication process is slightly different. For PostgreSQL version 9.3.5 and later, the native PostgreSQL streaming replication is used to handle the replication and creation of the read replica. The connection between the master and the read replica instance allows write-ahead log data to be sent, which replicates data asynchronously between the two instances. </p>
<p>A specific role is also introduced to manage this replication when using PostgreSQL. This role only has the abilities to handle and manage the replication, and it doesn’t have any permissions to modify or change the data being transmitted across the connection. Interestingly, you are able to create a Multi-AZ read replica instance. Meaning that when you create your read replica, RDS automatically configures a secondary read replica in a different AZ of the source read replica, much like Multi-AZ for your source databases as we discussed in the previous lecture. This feature can be used even if the source database of the first read replica isn’t configured for Multi-AZ itself. This means your read replica could be more resilient than your source database if your source database isn’t configured for Multi-AZ. It’s not possible to have a nested read replica when using PostgreSQL like you can with MySQL and MariaDB database engines. However, you can still use the same monitoring metric of ReplicaLag. That now brings me to the end of this lecture covering RDS read replicas.</p>
<h1 id="Amazon-Aurora-HA-Options"><a href="#Amazon-Aurora-HA-Options" class="headerlink" title="Amazon Aurora HA Options"></a>Amazon Aurora HA Options</h1><p>Welcome back! In this lecture, you’ll learn about the HA configuration options available within <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/intro/">Amazon Aurora</a>. Knowing of these options and how to apply them will ensure that your applications run with maximum uptime.</p>
<p>For starters, Amazon Aurora which is often quoted as <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>’s fastest growing service is a database service which provides superior MySQL and Postgres engine compliant performance, and is designed in a way that separates the compute layer from the storage layer. Separating the compute layer and storage layer from each other is a key architectural decision which allows you to dial up and down the availability of your data - mostly in the way that read replicas can be easily introduced and removed at will - more on this later.</p>
<p>The compute layer when launched can be provisioned in several configurations - providing varying forms of performance and availability which I’ll cover off individually in the coming slides. The compute layer is implemented using EC2 instances, but since this is a managed service these will not show up within the EC2 console.</p>
<p>The storage layer is shared amongst all compute nodes within the cluster regardless of the cluster configuration. Aurora stores data in 10 GB blocks, with each block being replicated six times across three AZs - two within each availability zone. From an availability and durability point of view, Aurora can handle up to 3 copies lost for reads, and up to 2 copies lost for writes. This makes the data highly redundant, durable, and available. The storage layer is presented to the compute layer as a single logical volume. This same single logical volume is shared across all compute instances involved in the compute layer whether it be a master or read replica - allowing the read replicas to accomplish the near-identical query performance as the master itself.</p>
<p>When compared with RDS - the management of data from a replication viewpoint is fundamentally different. With RDS data needs to be replicated from the master to each of its replicas. Aurora, on the other hand, has no need for replication since it uses and shares a single logical volume amongst all compute instances.</p>
<p>Aurora uses a quorum and gossip protocol baked within the storage layer to ensure that the data remains consistent. Together the quorum and gossip protocol provide a continuous self-healing mechanism for the data. Reads require a quorum of 3 and Writes require a quorum of 4. The peer to peer gossip protocol is used to ensure that data is copied across each of the 6 storage nodes. If a storage node goes offline intermittently - when it comes back online it will receive all data modifications from its peers via the gossip protocol. Availability zones are connected together using very high-speed backbone interconnects - which means that the gossip protocol is very fast.</p>
<p>Aurora in general, and regardless of the compute layer setup - always provides 6 way replicated storage across 3 availability zones. Because of Aurora’s storage layer design, Aurora is only supported in regions that have 3 or more availability zones.</p>
<p>Aurora provides both automatic and manual failover of the master either of which takes approximately 30 seconds to complete - its very quick one of the great benefits of using Aurora.</p>
<p>In the event that Aurora detects the master going offline, an automatic failover will be performed. In this scenario, Aurora will either launch a replacement master or promote an existing read replica to the role of master, with the latter being the preferred option as it is quicker for this promotion to complete.</p>
<p>Connecting to an Aurora database is performed by using one of the database connection endpoints.</p>
<p>Connection endpoints are created by the service to allow you to connect particular compute instances of the cluster.</p>
<p>There are 4 different connection endpoint types. Which one you end up using is dependent on your requirements. Let’s now quickly review each of them:</p>
<ul>
<li>Cluster Endpoint: The cluster endpoint points to the current master database instance. Using the Cluster endpoint allows your application to perform read and writes against the master instance.</li>
<li>Reader Endpoint: The reader endpoint load balancers connections across the read replica fleet within the cluster.</li>
<li>Custom Endpoint: A custom endpoint load balancer’s connections across a set of cluster instances that you choose and register within the custom endpoint. Custom endpoints can be used to group instances based on instance size or maybe group them on a particular db parameter group. You can then dedicate the custom endpoint for a specific role or task within your organization - for example, you may have a requirement to generate month end reports - therefore you connect to a custom endpoint that has been specifically set up for this task. </li>
<li>Instance Endpoint: An instance endpoint maps directly to a cluster instance. Each and every cluster instance has its own instance endpoint. You can use an instance endpoint when you want fine-grained control over which instance you need to service your requests.</li>
</ul>
<p>As a general rule of thumb - read-intensive workloads should connect via the reader endpoint.</p>
<p>Reader and Custom connection endpoints are designed to load balance connections across their members - with the intention of spreading load across the member instances. Connection endpoint load balancing is implemented internally using Route 53 DNS - therefore be careful in the client layer not to cache the connection endpoint lookups longer than their specified TTLs.</p>
<p>Connection endpoints are mostly applicable and used in “<a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-single-master-multiple-read-replicas/">Single Master with Multiple Read Replica</a>” setups.</p>
<h1 id="Aurora-Single-Master-Multiple-Read-Replicas"><a href="#Aurora-Single-Master-Multiple-Read-Replicas" class="headerlink" title="Aurora Single Master - Multiple Read Replicas"></a>Aurora Single Master - Multiple Read Replicas</h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/intro/">Aurora</a> can be configured with a single read&#x2F;write master instance - and multiple read replicas. A <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-single-master-multiple-read-replicas-demo/">single master instance</a> can be configured with up to 15 read replicas when using Aurora. Replication of data is performed asynchronously in milliseconds - fast enough to give the impression that replication is happening synchronously.</p>
<p>Read Replicas share the same underlying storage layer connected to the master.</p>
<p>Read replicas can be deployed in different availability zones within the same VPC or if required can be launched as cross region replicas. Read replicas allow you increase the overall read throughput while also offloading database reads from the master instance - allowing the master to focus on serving writes. In the event of the master instance going down, any one of the read replicas can be promoted to take over the role of being master.</p>
<p>Each read replica can be tagged with a label indicating priority in terms of which one gets promoted to the role of master in the event of the master going down. The master can be rebooted in 60 or less seconds.</p>
<p>This type of cluster supports being stopped and started manually in its entirety - that is when you stop and start a cluster, all underlying compute instances are either stopped or started. When stopped the cluster remains stopped for up to 7 days, after which it will automatically restart itself.</p>
<p>Daily backups are automatically performed with a default retention period of 1 day and for which can be adjusted up to a maximum retention period of 35 days. You have the capability of specifying a window of time in which backups are automatically taken. Additionally, on-demand manual snapshots can be performed on the database at any time. Manual snapshots are stored indefinitely until you explicitly choose to delete them. Restores are performed into a new database.</p>
<h1 id="Aurora-Single-Master-Multiple-Read-Replicas-DEMO"><a href="#Aurora-Single-Master-Multiple-Read-Replicas-DEMO" class="headerlink" title="Aurora Single Master - Multiple Read Replicas DEMO"></a>Aurora Single Master - Multiple Read Replicas DEMO</h1><p>Let’s take a quick look at a demo that shows how easy it is to set up and use a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-single-master-multiple-read-replicas/">single master Aurora database</a> with multiple read replicas.</p>
<p>In this example, I’ll perform the following sequence:</p>
<ol>
<li>Launch a single master Aurora MySQL database cluster with a single read replica within the AWS RDS console.</li>
<li>Connect to the master and create a new MySQL database named demo, and within it create a new table named course.</li>
<li>Confirm that when connecting using the writer connection endpoint that SQL inserts can be performed successfully.</li>
<li>Confirm that when connecting using the reader connection endpoint that SQL inserts cannot be performed.</li>
<li>Use the AWS RDS console to add an additional read replica to demonstrate how easy it is to scale out read capacity.</li>
<li>Connect to the new read replica and confirm that the data has been successfully replicated and can be returned.</li>
<li>Perform a failover - converting one of the read replicas into the primary.</li>
</ol>
<p>Ok, let’s begin. Starting off in the AWS RDS console - I’ll create a new <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/intro/">Amazon Aurora</a> MySQL single master with multiple readers database.</p>
<p>Under the Database features, I’ll select the “One writer and multiple readers” option. I’ll set the DB cluster identifier to be “cloudacademy-db-singlemaster”. I’ll configure the credentials to be admin with a password of cloudacademy. For instance size - I’ll select the burstable classes option and then simply choose the smallest size - which happens to be the db.t2.small instance.</p>
<p>Under the Availability and Durability section, I’ll choose the option to create and deploy a read replica.</p>
<p>I’ll then deploy it into an existing Multi AZ VPC. For security groups - I’ll simply allocate an existing one which allows inbound TCP connections to the default Mysql port 3306. Connections will be made from an existing bastion host which has the standard MySQL client already installed on it.</p>
<p>Ok with all that in place, I can now go ahead and click on the “Create Database” button at the bottom. Provisioning is fairly quick and takes just a matter of minutes to complete.</p>
<p>Now that the database is ready I’ll take a copy of the writer connection endpoint. Next, I’ll jump into my local terminal and connect to the bastion host using SSH.</p>
<p>Once connected I’ll use the MySQL client utility to connect to the master database instance using the writer connection endpoint just copied. Once authenticated into the master database instance - I’ll simply create a new database named demo and then create a new course table within it. With the course table created I’ll confirm that I can perform inserts to confirm that writes can be performed - which is indeed the case.</p>
<p>Next, let’s now try connecting via the reader connection endpoint - and perform the same inserts. This time as expected we are unable to perform inserts since the reader connection endpoint connects to the read replica.</p>
<p>Next - I’ll add an additional read replica into the cluster. To do so I’ll jump back into the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> RDS console and here I’ll select the “cloudacademy-db-singlemaster” database and then under the Actions drop down, select the Add Reader option. Within the Add Reader section, I’ll leave all of the defaults as is and give the new read replica the identifier “cloudacademy-db-replica” and then click the Add reader button at the bottom.</p>
<p>Refreshing the current page you can now see that the Aurora database cluster consists of a single master and 2 read replicas.</p>
<p>Checking the new read replica status we can see that it is still in Creating status. We need to wait until it reaches the Available status - once it does I’ll copy the instance endpoint and then connect to it using the MySQL client on the bastion host. Once connected I’ll simply perform a select all on the course table to confirm that the data has been successfully replicated - which it has.</p>
<p>Finally, I’ll jump back into the RDS console. With the new read replica still selected, I’ll perform a failover to it - by selecting the failover option within the Actions dropdown menu. On the confirmation page, I’ll proceed by clicking the failover button. This kicks off the failover which will take a minute or 2 to show up within the current page. Let’s now refresh it to confirm that the cloudacademy-db-replica has now been promoted to the role of writer which it has - and that the original master is now a read-only as per the reader role which has been allocated to it.</p>
<p>In summary, this demonstration highlighted the following:</p>
<ol>
<li>How to provision a new Aurora MySQL single master database with multiple read replicas.</li>
<li>Confirmed that writes can only be made against the master and that the read replicas are for reading only.</li>
<li>How to add an additional read replica into the database cluster and have data automatically replicated into it.</li>
<li>And how to perform a failover from the master to one of the read replicas.</li>
</ol>
<p>If you’ve followed along, don’t forget to terminate your database cluster to avoid ongoing charges.</p>
<h1 id="Aurora-Multi-Master"><a href="#Aurora-Multi-Master" class="headerlink" title="Aurora Multi Master"></a>Aurora Multi Master</h1><p>An <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/intro/">Aurora</a> multi-master setup allows you to configure a pair of masters in an active-active read-write configuration which can later be scaled up on demand by the customer to a maximum of four masters. In this configuration, you can read and write to any of the provisioned master instances, providing improved fault tolerance within the compute layer.</p>
<p>In the example shown here, the configuration deploys an active-active pair of compute instances with each instance being deployed in its own availability zone. If an instance outage occurs in one availability zone, all database writes can be redirected to the remaining active instance managed by the customer in the client-side logic, and all without the need to perform a failover. This also provides protection from az outages.</p>
<p>As earlier mentioned, when required, a customer can scale up the multi-master instance count up to a maximum of four instances. Do keep in mind that in this multi-master configuration, you cannot add additional read replicas into the cluster. In terms of connection management, incoming database connections are not load-balanced by the service. Rather, the load balancing connection logic must be implemented by you and performed on the client side. With this in mind, let’s now take a quick look at a demo that shows how easy it is to set up and use a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-multi-master-setup-and-use-demo/">multi-master Aurora database cluster</a>, complete with client-side connection management.</p>
<h1 id="Aurora-Multi-Master-Setup-and-Use-DEMO"><a href="#Aurora-Multi-Master-Setup-and-Use-DEMO" class="headerlink" title="Aurora Multi-Master Setup and Use DEMO"></a>Aurora Multi-Master Setup and Use DEMO</h1><p>Let’s take a quick look at a demo that shows how easy it is to set up and use a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-multi-master/">multi master Aurora database cluster</a>.</p>
<p>In this example I’ll perform the following sequence:</p>
<ol>
<li>Launch a new multi master Aurora MySQL database cluster within the AWS RDS console.</li>
<li>Create a new database named demo, and within it create a new table named course.</li>
<li>Use the AWS RDS console to find and determine the connection points for the multi master database and set them up as environment variables named AURORA_NODE1 and AURORA_NODE2 in the local terminal.</li>
<li>Launch a Python script that implements load balancing and retry connection logic to continuously insert records into the course table.</li>
<li>Confirm connections are load balanced across both active master database nodes.</li>
<li>Crash each of the master database nodes individually.</li>
<li>Confirm connection failover to the remaining active master database node happens and databases inserts still continue.</li>
</ol>
<p>Note: The commands and script as demonstrated here onwards are available in the following CloudAcademy <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/aurora-multimaster">Github repository</a>.</p>
<p>Ok, let’s begin. Starting off in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> RDS console - I’ll create a new <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/intro/">Amazon Aurora</a> MySQL Multi Master database.</p>
<p>Under the Database features, I’ll select the “Multiple Writers” option - this is what makes the cluster a multi master. I’ll set the DB cluster identifier to be “cloudacademy-db-multi”. I’ll configure the credentials to be admin with a password of cloudacademy. For instance size - I’ll simply choose the smallest size.</p>
<p>I’ll then deploy it into an existing Multi AZ VPC. For security groups - I’ll simply allocate an existing one which allows inbound TCP connections to the default MySQL port 3306. Connections will be made from an existing bastion host which has the standard MySQL client already installed on it.</p>
<p>Ok with all that in place, I can now go ahead and click on the “Create Database” button at the bottom. Provisioning is fairly quick and takes just a matter of minutes to complete.</p>
<p>While we are waiting for the database provisioning process to complete - let’s jump over into GitHub and examine the Aurora Multimaster repo. Here within the readme we can see the commands that we will execute to create the demo database and course table.</p>
<p>The “insert-test” python script implements connection load balancing and retry logic. Lines 7 and 8 query environment variables established in the terminal - these are the connection endpoints for each of the 2 master instances. Lines 10, 11, 12 specify the database name and database credentials for authentication.</p>
<p>The “reconnect” function spanning line 14 through to 19 simply calls the reconnect function on the passed in connection and logs out the fact that a reconnection has been attempted - either succeeding or failing.</p>
<p>The remainder of the script starting from line 32 establishes 2 database connections, one to each of the master nodes. The script then inserts 100 course records into the course table. Connection load balancing is performed by simply testing whether the current value of x within the for loop is even or odd - and then alternating the connections - with one of the connections being considered the primary, and the other one taking on the role of the backup connection.</p>
<p>Ok, let’s jump back into the RDS console and confirm that our Multi Master database is ready - which it is. Next, I’ll need to gather both connection endpoints for the masters and then set them up as environment variables within an SSH session on the bastion host.</p>
<p>I’ll now jump into my local terminal and connect to the bastion host using SSH.</p>
<p>Once connected I’ll git clone the aurora multimaster git repo. Once that is completed I’ll navigate into the aurora multi master directory and do a directory listing to examine its contents. From here I then establish both the AURORA_NODE1 and AURORA_NODE1 environment variables and configure them with the connection endpoints previously highlighted and copied. </p>
<p>Next, I’ll split the terminal up into 3 individual panes using tmux. I’ll use the key sequence control plus b plus double quote to split the terminal horizontally, and then again vertically using control plus b plus percent sign. This will allow me to run 3 commands side by side and see all results at the same time. In the first pane, I will set up a watch to perform a “select count star from course” every one second. This will provide us a running count of how many records have been inserted into the course table. In the second pane, I will launch the main Python script which will be performing the inserts into the database implementing connection load balancing and retry logic. Here we can see that it has started to successfully insert new data records using connection load balancing. In the previous pane, we can see that the course table count is now increasing as expected. In the 3rd pane, I will intermittently execute the command “alter system crash” against both of the database nodes individually to simulate a crash. We should expect that the connection retry logic is exercised and that the table count view remains incrementing without any loss. This appears to be the case, which is great.</p>
<p>If we now let the main python script playout - we should see that the final table count ends up at 100 - which it finally does. This is a great result.</p>
<p>In summary, this demonstration highlighted the following:</p>
<ol>
<li>How to provision a new Aurora MySQL multi master read-write database.</li>
<li>Both database nodes are in an active-active or multi master read write configuration.</li>
<li>Connection load balancing and retry logic implemented within the Python client script is working successfully without any data loss.</li>
</ol>
<p>If you’ve followed along, please don’t forget to terminate your database cluster to avoid ongoing charges.</p>
<h1 id="Aurora-Serverless"><a href="#Aurora-Serverless" class="headerlink" title="Aurora Serverless"></a>Aurora Serverless</h1><p>Aurora Serverless is an elastic solution that autoscales the compute layer based on application demand, and only bills you when it’s in use. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless-database-cluster-demo/">Aurora Serverless</a> ideally suited towards applications which exhibit variable workloads and&#x2F;or have infrequent data accessing and modification needs.</p>
<p>When provisioning an Aurora Serverless database - you no longer need to plan and allocate instance sizes. Instead, you simply configure lower and upper limits for capacity. Capacity is measured in ACUs - which stands for Aurora Capacity Units. You have the ability to set upper and lower limits for capacity.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/intro/">Aurora</a> will continually adjust and optimize capacity based on incoming demand - and will stay within the limits specified.</p>
<p>The underlying compute instances are automatically started and stopped based on current demand. Instances can be cold booted in a matter of seconds.</p>
<p>In terms of high availability - the service is underpinned by the same fault-tolerant, self-healing storage layer. There is nothing to configure beyond the capacity settings which if required can be manually tuned.</p>
<p>The service will auto adjust the compute layer and capacity automatically behind the scenes to ensure that the database is available and has the necessary capacity when required. If the traffic starts to drop off it will begin scaling down, and if enabled, actually shut down the compute entirely when there’s no demand for it. When the compute is turned off, you only end up paying for the storage capacity used.</p>
<p>Since Aurora Serverless by design takes care of automatically scaling out to meet peak demand, there is no option nor need to add in read replicas.</p>
<p>An Aurora Serverless database is configured with a single connection endpoint which makes sense - given that it is designed to be serverless - this endpoint is obviously used for both read and writes.</p>
<p>Another interesting option when considering how to connect and execute queries against an Aurora Serverless database is the Web Service Data API feature - available only on Aurora Serverless databases. This opt in feature enables an HTTP web service interface into the database meaning you can execute and run queries against the database without the need for a JDBC driver nor connection - hence why this feature is often termed connectionless. The Web Service Data API makes implementing Lambda functions which need to perform data lookups and or mutations within an Aurora serverless database a breeze. Another benefit when using this feature is that the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> CLI has been updated to allow you to execute queries through it from the command line.</p>
<p>Aurora Serverless performs a continuous automatic backup of the database with a default retention period of 1 day - which can be manually increased to a maximum retention period of 35 days. This style of backup gives you the capability of restoring to a point in time within the currently configured backup retention period. Restores are performed to a new serverless database cluster.</p>
<h1 id="Aurora-Serverless-Database-Cluster-DEMO"><a href="#Aurora-Serverless-Database-Cluster-DEMO" class="headerlink" title="Aurora Serverless Database Cluster DEMO"></a>Aurora Serverless Database Cluster DEMO</h1><p>Let’s take a quick look at a demo that shows how easy it is to set up and use an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/">Aurora Serverless database cluster</a>.</p>
<p>In this example I’ll perform the following sequence:</p>
<ol>
<li>Launch a new Aurora Serverless MySQL database within the AWS RDS console.</li>
<li>Create a new database named demo, and within it create a new table named course.</li>
<li>Enable the Web Service Data API on the new Aurora Serverless MySQL database.</li>
<li>Use the AWS CLI to invoke the aws rds-data command to read and write into the course table.</li>
</ol>
<p>Ok, let’s begin. Starting off in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> RDS console - I’ll create a new <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/intro/">Amazon Aurora</a> MySQL Serverless database.</p>
<p>Under the Database features I’ll select the “Serverless” option - this is what makes the cluster serverless. I’ll set the DB cluster identifier to be “cloudacademy-serverless”. I’ll configure the credentials to be admin with a password of cloudacademy.</p>
<p>For capacity settings - I’ll simply go with 2Gb for minimum capacity and 16Gb for maximum capacity. I’ll also enable the “Force Scaling” and “Pause Compute Capacity” settings to ensure cost savings when activity is low.</p>
<p>I’ll then deploy it into an existing Multi AZ VPC. For security groups - I’ll simply allocate an existing one which allows inbound TCP connections to the default MySQL port 3306. Connections will be made from an existing bastion host which has the standard MySQL client already installed on it.</p>
<p>I’ll also enable the “Data API” which will activate a SQL HTTP endpoint.</p>
<p>Ok with all that in place, I can now go ahead and click on the “Create Database” button at the bottom. Provisioning is fairly quick and takes just a matter of minutes to complete.</p>
<p>While the data provisioning is taking place, I’ll head over into the Secrets Manager service and set up a new secret which will be used later on by the AWS CLI to invoke SQL commands using the SQL HTTP endpoint.</p>
<p>I’ll start by clicking on the “Store a new Secret” button. For the secret type I’ll go with the selected default which is “Credentials for RDS database”. I then need to enter in the username and password Mysql database credentials - which for the serverless database I just launched were admin and cloudacademy respectively. I then need to select the cloudacademy-serverless database and then click the Next button. I then need to allocate this secret a name and optionally a description. In this case I’ll call the secret “demo&#x2F;cloudacademy-serverless”.</p>
<p>I’ll skip assigning any tags and instead just click the Next button. I’ll accept the automatic rotation section defaults - which are to disable automatic rotation - and again just click on the Next button. The review section looks good - I can now click the Store secret button to create and store the secret. Ok, that has worked. Finally, I need to click on the new secret like so and then copy down the secret ARN highlighted here - this will be used later on to set up and authenticate to the Web Service Data API via the AWS CLI in the terminal.</p>
<p>Ok, let’s jump back into the RDS console and confirm that our Serverless database is ready - which it is. Next I’ll need to discover and copy the allocated host name.</p>
<p>I’ll now jump into my local terminal and connect to the bastion host using SSH.</p>
<p>Once connected I’ll use the MySQL client utility to connect to the serverless database using the hostname just copied. Once authenticated into the serverless database - I’ll simply create a new database named demo and then create a new course table within it.</p>
<p>Ok, I’ll now jump into a new terminal session and this time use the AWS CLI to execute a SQL insert statement to populate sample data into the course table. Here I’ll execute the following command:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws rds-data execute-statement \</span><br></pre></td></tr></table></figure>

<p>I’ll execute it within the us-west-2 region and set the secret-arn to be the arn copied when we setup the RDS secret within the secrets manager service. The resource arn is copied from the serverless database configuration section within RDS. The database is set to demo, and sql will be set to a simple insert SQL statement that will populate a new course record in our course table.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--region us-west-2 \</span><br><span class="line">--secret-arn &quot;&quot; \--</span><br><span class="line">--resource-arn &quot;&quot; \</span><br><span class="line">--database demo \</span><br><span class="line">--sql &quot;show tables&quot; \</span><br><span class="line">--output json</span><br></pre></td></tr></table></figure>

<p>Ok - executing this statement has now successfully created a new database record within the demo course table. Let’s rerun this command a few more times to insert more data.</p>
<p>Finally - I’ll now run a select statement against the table to return all data in the course table.</p>
<p>In <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/summary/">summary</a>, this demonstration highlighted the following:</p>
<ol>
<li>How to provision a new Aurora Serverless MySQL database.</li>
<li>How to enable the Web Service Data API.</li>
<li>How to connect to an Aurora Serverless database using the AWS CLI, authenticating with a secret stored in the Secrets Manager service.</li>
<li>How to execute SQL statements using the AWS CLI.</li>
</ol>
<p>If you’ve followed along, please don’t forget to terminate your database cluster to avoid ongoing charges.</p>
<h1 id="AWS-DynamoDB-HA-Options"><a href="#AWS-DynamoDB-HA-Options" class="headerlink" title="AWS DynamoDB HA Options"></a>AWS DynamoDB HA Options</h1><p>Welcome back! In this lecture, you’ll learn about the HA configuration options available within DynamoDb - which when configured maximizes the uptime of your dependent applications.</p>
<p>Firstly, DynamoDB is a NoSQL schemaless managed service built and provided by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> - and is designed internally to automatically partition data and incoming traffic across multiple partitions which are themselves stored on numerous backend servers distributed across three availability zones within a single region. This is the default starting point for a DynamoDb Table in terms of its availability posture. The amount and required availability can be increased using and configuring additional options as you’ll soon hear about.</p>
<p>A DynamoDb partition is a dedicated area of SSD storage allocated to a table and for which is automatically replicated synchronously across 3 availability zones within a particular region. DynamoDB being a managed service takes care of performing both the partition management and replication for you, therefore you can remain focused on your application design and not be distracted by the needs of data replication requirements. The synchronous AZ replication provides protection against any single node outage and&#x2F;or a full availability zone outage - which although is a rare event should never be assumed to not happen. The synchronous replication takes place using low latency interconnects between each of the availability zones within a region and ensures high-speed sub second replication.</p>
<p>Dialing up and down the provisioned throughput of a DynamoDB database is possible, and ensures that your DynamoDb database can meet the needs of your application as it grows. Changing the provisioned throughput results in additional partitions and replication which again happens automatically in the background.</p>
<p>DynamoDB provides a secondary layer of availability in the form of cross-region replication which in its latest implementation is now known as Global Tables. A Global table gives you the capability to replicate a single table across 1 or many alternate regions and in doing so protects your table from regional outages. When configured, a Global Table not only elevates the availability of your data but also enables applications to take advantage of data locality. Users can be served data directly from the closest geographically located table replica which minimizes the network latency involved in accessing the data.</p>
<p>A single DynamoDB table can be globally replicated to 1 or multiple other tables located in different AWS regions. Replication must take place within an AWS account - that is you cannot configure Global tables in different AWS accounts. Global Tables implement multi master read&#x2F;write capability with eventual consistency. Both read and writes can be performed against any one of the configured global tables. All writes will then be replicated in near sub second to time to all other globally configured tables of the same table name.</p>
<p>Existing DynamoDB tables can be converted into Global tables either by using the relevant configuration options exposed within the AWS DynamoDB console - or by using the aws cli and executing the “aws dynamodb update-table” command specifying one or several regions into which the table should be replicated.</p>
<h1 id="AWS-DynamoDB-HA-Options-Demo"><a href="#AWS-DynamoDB-HA-Options-Demo" class="headerlink" title="AWS DynamoDB HA Options Demo"></a>AWS DynamoDB HA Options Demo</h1><p>Let’s take a quick look at a demo that shows how easy it is to set up and use DynamoDB Global Tables.</p>
<p>In this example I’ll use the aws cli to perform the following sequence:</p>
<ol>
<li>Create a new table named “cloudacademy-courses” in the us-west-2 region. This table will contain courses provided by CloudAcademy.</li>
<li>Populate the “cloudacademy-courses” table with sample course data.</li>
<li>Convert the “cloudacademy-courses” table into a Global Table, deploying a replica table in the alternate region ap-southeast-2.</li>
<li>Confirm both tables are in an ACTIVE status.</li>
<li>Add new course data into the us-west-2 region hosted “cloudacademy-courses” table.</li>
<li>Watch and observe the time taken to propagate data into the replicated table in the ap-southeast-2 region.</li>
</ol>
<p>Ok let’s begin. I’ll start by browsing to the CloudAcademy DynamoDB GlobalTables <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/dynamodb-globaltables">Github repo</a>. As you can see the readme for this repo contains all of the instructions for this demo. We’ll simply copy and paste each of the steps from this readme as we later proceed. Regardless, we also need to git clone this repo to give us access to the data files used to populate the global table that we are soon to set up. Therefore I’ll simply copy the git clone URL and then jump into my local terminal and then perform a git clone using the URL just copied.</p>
<p>Navigating into the new “dynamodb-globaltables” directory. I’ll use the tree command to examine and display its contents. Here we can see the 2 data files batch.course.data1.json and batch.course.data2.json which we will use to later populate our global table.</p>
<p>Ok, next I’ll copy the step 1 instruction and then paste it into the terminal. Step 1 simply creates a new DynamoDB table named “cloudacademy-courses” and locates it in the us-west-2 region. The billing mode is set to “PAY_PER_REQUEST” - this is a requirement to be able to later create a global table off this table.</p>
<p>This looks good! Our new “cloudacademy-courses” table has been successfully created. We can confirm this by jumping into the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> DynamoDB console and then selecting the “tables” view within the Oregon region. Here we can indeed see that the new “cloudacademy-courses” table has been created and has an “Active” status. Ok let’s move on and populate this table.</p>
<p>Before I run the Step 2 command - lets take a quick look at the dataset that we are going to reference and use to populate the new “cloudacademy-courses”. Using the cat command on the batch.course.data1.json file - we can see that it contains 3 course items that will be inserted into the table. Each item has a courseid which acts as the primary key, followed by the attributes company, title, URL, duration, and instructor.</p>
<p>I’ll now execute the Step 2 command which will result in our “cloudacademy-courses” table being populated with these 3 items. Ok that looks good as per the fact that the output contains an empty UnprocessedItems object.</p>
<p>Jumping back into the AWS DynamoDB console - we can navigate into the table and select the “Items” tab to view the current set of items, and as expected we have the 3 new items that we just populated it with. </p>
<p>We are now ready to convert the “cloudacademy-courses” table into a global table - to do so I’ll copy the Step 3 command and then execute it back within the terminal. This command is going to create a replica read&#x2F;write multi master table in the ap-southeast-2 Sydney region.</p>
<p>This looks good! We can see in the output that the TableStatus is set to “UPDATING” - we’ll wait a few minutes before executing the Step 4 command - just to give the Step 3 command enough time to propagate the table changes across to the new region. While we are waiting we can jump back into the AWS DynamoDB console and take a look at the “Global Tables” tab. Here we can see that there are 2 configured regions for our “cloudacademy-courses” table. The original us-west-2 region which has an ACTIVE status, and the newly configured ap-southeast-2 region which has a CREATING status.</p>
<p>Ok, let’s now copy the Step 4 command and execute it. This simply displays the details of the newly provisioned “cloudacademy-courses” global table in located in the ap-southeast-2 region. Again we can see that it is still in a CREATING status as per the TableStatus attribute.</p>
<p>We need to wait for this table to achieve ACTIVE status - therefore lets periodically poll this table every 30 seconds by executing the Step 5 command. This command re-executes the previous command every 30 seconds and then extracts out and displays the TableStatus attribute value - which we can see is still in CREATING status. We need to pause here until this changes to ACTIVE.</p>
<p>I’ll now speed up the demo to the point where the table reaches the ACTIVE status which we can now see. Ok this is a great result - and implies that our “cloudacademy-courses” global table is ready.</p>
<p>Next, let’s watch and observe the replication of data writes to the “cloudacademy-courses” global table setup. To do so I’ll now clear the current terminal and then use the tmux command to split the terminal into 2 panes. Within the terminal, I’m using the key sequence, control plus b together with a double quote. Excellent. To navigate between the two panes, again use the key sequence, control plus b and then the up and down arrow keys.</p>
<p>In the top pane I’ll run the Step 6 pane 1 command. This will setup a watch that continuously performs a read against the ap-southwest-2 hosted “cloudacademy-courses” table every second looking for a new data item that we will next insert into the us-west-2 hosted “cloudacademy-courses” table in the bottom pane. This will allow us to observe the speed at which the global table changes propagate between regions.</p>
<p>Next, I’ll move focus to the bottom pane into which I’ll execute the Step 6 pane 2 command. This command inserts new table data - and of which the top pane command is querying for.</p>
<p>As you can see, the propagation time is approximately 1-2 seconds which is quite impressive considering the data is being replicated from Oregon in the States to Sydney in Australia.</p>
<p>Let’s now reverse the setup and this time write data into the ap-southwest-2 hosted “cloudacademy-courses” table and see it get replicated back into the us-west-2 hosted “cloudacademy-courses” table, emphasising indeed that global tables are configured as multi master read&#x2F;writes.</p>
<p>To perform this, I’ll make a copy of the batch.course.data2.json file and name it batch.course.data3.json. I’ll then use vim to edit the contents of this file and simply update each of the 3 items courseid keys with new unique values. I’ll save this back to the file system. Next, I’ll update the watch command to query from the us-west-2 region. And then in the other tmux pane I’ll rerun the aws dynamodb batch-write-item command but this time have it insert into the ap-southeast-2 region.</p>
<p>Again we can observe that the propagation time is quick. But more importantly, this time we have demonstrated that our “cloudacademy-courses” global table is truly configured in a multi master read&#x2F;write configuration - this is very cool!!</p>
<p>Finally, let’s jump back into the AWS DynamoDB console and examine the “cloudacademy-courses” table. Refreshing the items view we can see all of the expected 9 items that we populated the “cloudacademy-courses” global table with. Note that this is the view of the items as currently held within the table located in the us-west-2 region.</p>
<p>We can equally view the items held within the table located in the ap-southeast-2 region by clicking on the Global Tables tab and then clicking on the Sydney region. This will open a new browser tab for the current “cloudacademy-courses” table albeit in the Sydney region - then clicking on the items tab we can see the same 9 replicated table items.</p>
<p>Ok, to summarise what we have just demonstrated.</p>
<ol>
<li>We created a new table named “cloudacademy-courses” in the us-west-2 region.</li>
<li>We then populated the “cloudacademy-courses” table with sample course data</li>
<li>We then converted the “cloudacademy-courses” table into a Global Table, deploying a second replica table in the alternate ap-southeast-2 region.</li>
<li>We then confirmed both tables were in an ACTIVE status before proceeding.</li>
<li>We then added new course data into us-west-2 table, and then confirmed that it was replicated to the ap-southeast-2 table - and confirm replication was very quick.</li>
<li>We then repeated the previous step but in the reverse direction - that is we added new course data into ap-southeast-2 table, and then confirmed that it was replicated to the us-west-2 table - this time confirming that global tables are setup as multi master read&#x2F;writes.</li>
</ol>
<h1 id="On-Demand-Backup-and-Restore"><a href="#On-Demand-Backup-and-Restore" class="headerlink" title="On-Demand Backup and Restore"></a>On-Demand Backup and Restore</h1><p>On demand backups allow you to request a full backup of a table, as it is at the very moment the backup request is made. On demand backups are manually requested, and can be performed either through the AWS DynamoDB console or by using the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> CLI.</p>
<p>On demand backups are useful in the following situations:</p>
<ol>
<li>Table corruption - rare but possible</li>
<li>Long term regulatory, compliance, and&#x2F;or auditing data requirements</li>
<li>Testing scenarios</li>
</ol>
<p>Scheduling on demand backups provides you with the ability to restore table data back to a point in time. On demand backups can be requested anytime and there is no limit to the number of them nor their retention period. On demand backups remain in the account until they are explicitly requested to be deleted by an administrator.</p>
<p>Backups when requested and performed typically finish within seconds, and have zero impact on the table performance and availability.</p>
<p>Let’s perform a quick demonstration of performing an on demand backup using the AWS CLI.</p>
<p>Jumping into the terminal, I’ll execute the “aws dynamodb create-backup” command specifying it to back up the “cloudacademy-courses” table located in the us-west-2 region, and give it the backup name “backup1.”</p>
<p>aws dynamodb create-backup \</p>
<p>  –region us-west-2 \</p>
<p>  –table-name cloudacademy-courses \</p>
<p>  –backup-name backup1</p>
<p>This has resulted in the following output which indicates that the backup is being created.</p>
<p>We can then run the “aws dynamodb describe-backup” command specifying the backup arn and copying the backup arn from the previous output.</p>
<p>This time we can see that the backup is now available and ready.</p>
<p>We can also jump in the AWS DynamoDB console and select the Backups option on the table to view all table backups including the backup we just made.</p>
<p>Lets now restore it within the console. To do so I’ll select it and then click the restore button. I’ll give it the name “cloudacademy-courses-backup”. I’ll leave all defaults as is and then simply click the restore table button. And with that our backup is now getting restored. This typically takes approximately 2 to 5 minutes to become available.</p>
<p>I’ll skip the demo along to the point where the backup has completed restoring. Clicking on the newly restored table, and then selecting the items tab - we can indeed see that it contains all of the items captured at the time the backup was created.</p>
<h1 id="Point-in-Time-Recovery"><a href="#Point-in-Time-Recovery" class="headerlink" title="Point in Time Recovery"></a>Point in Time Recovery</h1><p>Point In Time Recovery or PITR - is an enhanced version of the on-demand backup and restore feature, providing you with the ability to perform point in time recoveries. This feature is extremely handy in situations when you’re modifying data and want a safety net in place for the situation where your data modifications didn’t result in the way they should have. With PITR in place, you can simply restore back to a specific time. PITR takes the burden away of having to plan backup schedules, etc.</p>
<p>Point In Time Recovery operates at the table level, and when enabled provides you with the ability to perform a point in time recovery for any time between the current time and the last 35 days. This feature needs to be enabled as it is disabled by default. Once enabled you can request a recovery by specifying a date and time with second precision or granularity. The restoration will always be performed into a new table - of which you specify the new table name at time of restoration request. Table restoration can be performed in the same region as the original table, or into a different region altogether.</p>
<h1 id="Point-in-Time-Recovery-Demo"><a href="#Point-in-Time-Recovery-Demo" class="headerlink" title="Point in Time Recovery Demo"></a>Point in Time Recovery Demo</h1><p>Let’s perform a quick demonstration of requesting a Point In Time Recovery. I’ll perform the demonstration within the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Dynamodb console. The first thing that we need to do is enable Point In Time Recovery on the “cloudacademy-courses” table.</p>
<p>To do so I’ll select the table and then navigate into the backups tab. Here, under the Point In Time Recovery section I’ll click the Enable option to enable it. Once it has been enabled take note of the earliest and latest restore dates - these are the parameters that your point in time recovery must be requested within.</p>
<p>It’s important to note that the Latest restore date always lags the current time by a few minutes. Let’s now examine the local system clock within the terminal to determine the current time. Next, I’ll return to the “cloudacademy-courses” table and delete one of the existing items. Returning back to the backups section I’ll perform an immediate Point In Time Recovery - and name it “pitr-recovery1”. I’ll then return to backups of the same table and wait until the latest restore date moves beyond the time that I just made the delete - when this ready I’ll take a second Point In Time Recovery - and name it “pitr-recovery2”.</p>
<p>I’ll now skip the demo forward to the point where both Point In Time Recovery tables have been successfully restored.</p>
<p>Here we can see that the first point in time recovery table still contains the table item that I deleted. This is because the latest restore date at the Point In Time Recovery request hadn’t moved beyond the time at which I performed the item deletion.</p>
<p>If we now view the second point in time recovery table we can see that it indeed contains all but the deleted item. This is because the latest restore date at the Point In Time Recovery request had now moved beyond the time at which I performed the item deletion.</p>
<p>This highlights the concept of automated point in time recovery - a very cool and useful table data recovery feature!</p>
<h1 id="High-Availability-Summary"><a href="#High-Availability-Summary" class="headerlink" title="High Availability Summary"></a>High Availability Summary</h1><p>Okay, let’s review the key points on high availability to remember for the exam. Let’s talk design patterns. First migration. If you’re migrating large amounts of objects, say to S3 or EFS or Amazon FSX for Windows File Server then consider AWS Datasync. It can be a way to reduce operational costs. Great for moving cold data to Glacier or S3 storage classes. Things like machine learning and the life sciences where you have a large volume to data, good connectivity, and you just need a really simple way of moving those into AWS. AWS Datasync has encryption built in and it also gives you IAM support. You’d need to have a really good network performance to use data sync. </p>
<p>So you’d probably need to consider using it with AWS Direct Connect. So you’ve got that throughput, and if you don’t have the throughput and it’s a really large volume, say over 50 terabytes then you’re really better using the AWS Snowball device. So, if you’re migrating data from an Oracle or a Microsoft SQL Server database, then it’s a good option to consider the AWS Database Migration Service. Frankly, it doesn’t tend to crop up much as an exam scenario, but just remember that the Database Migration Service helps you migrate data from any of the common database platforms, e.g. Oracle, Microsoft SQL Server, or my SQL to Amazon RDS. </p>
<p>So it’s a really simple way to transfer schemas and to migrate the data itself as that can migrate say, an Oracle Database to Oracle on Amazon RDS in a Multi-AZ deployment. Also always remember your storage options. Often the design requirements specify the need for maximum IO-performance. So if a solution needs storage, I always say interpret that to mean persistent storage unless it specifically says the storage does not need to be kept, all right? If it doesn’t need to be kept then you can consider Ephemeral storage which is part of the EC2 Instance as that gives you a very high IO-option. Otherwise think EBS with enhanced networking, compute, or memory, that’s it’s going to be your best for maximum performance. And think for object storage, Amazon S3 is the best durable data storage, Glacier for your archives. </p>
<p>Remember that Amazon FSX, and Amazon EFS, the Elastic File System provide fully managed network file systems that are well-suited to provide shared storage to 99% of application designs. But there are a few use cases and exceptions to keep in mind. For example, migrating highly available DB-Clusters sometimes requires an architectural pattern that will allow you to access storage from multiple hosts simultaneously. If you get stuck having to lift and shift a cluster to AWS and you don’t want to have to refactor a cluster-aware file system, like the Oracle Cluster File System or the Red Hat Global File System, then you can consider using the EBS Multi-Attach feature to coordinate storage access between instances and prevent data inconsistencies.</p>
<p>The MultiAttach can support up to 16 Linux instances that are using the Nitro system and that are in the same availability zone. So if you need to migrate a persistent database from on-premise to AWS where that database solution needs very high IOPS, say 60,000-plus IOPS and it also needs to be run on a single EBS volume, then you could consider the Nitro-based Amazon EC2 Instance with the Amazon EBS-provisioned IOPS SSD io1 or io2 with the volume provision to the required IOPS. Now the io1 and io2 Nitro Instances support that EBS Multi-Attach feature. This design pattern can help you if you need to run a database cost on one EBS Instance and have more than EC2 Instancez access it without there being data inconsistencies. </p>
<p>Remember the different options available on how you get your data in and out of AWS. We’ve got AWS Direct connect, VPN, internet connection, AWS Snowball, AWS Snowmobile, and the AWS Storage Gateway. How much data do you actually need to import or export in or out of AWS affects your chosen solution? And as a general rule, if your data will take longer than a week to transport, you should consider using AWS Snowball. And if you need to shift more than 50 terabytes of data using a Snowball device is usually the best option unless there’s very fast networking described. </p>
<p>Recovery Time Objective is the speed in which you need your system back up and running. The lower that Recovery Time Objective target, the most fail-over you need to factor in. So for example, anything under four hours as a Recovery Time Objective would require warm standby or pilot light designs and a major factor in Recovery Time Objective is restoring the data. For example, backup and restore as a strategy from a tape library or Storage Gateway is unlikely to be able to recover large volumes of data in under eight hours, so keep that in mind. The Recovery Point Objective is the point in time you need to go back to in your data.</p>
<p>Now, if this is another low value, then you probably need to consider Multi-AZ Databases with automatic failover as a solution. Amazon S3 is an ideal backup solution for on-premise corporate data centers. And there’s three different classes of Amazon S3. Standard, Infrequent Access and Amazon Glacier. Standard has eleven nines of durability and it maintains data across multiple devices and multiple availability zones within a single region. It has four nines of availability and security features including encryption, access controls and data management functionality such as lifecycle policies. Infrequent Access Class is very similar. However, there are two main differences. Firstly only three nines of availability as opposed to four, which is offered by standard. And secondly the cost. </p>
<p>Infrequent Access is cheaper than Standard, making this a very good choice for backup data. Amazon Glacier is a cheapest option of the three classes and it’s used as cold storage for data archiving. It uses different security mechanisms such as volt policies, and it can be used with S3 lifecycle rules or with the SDKs to move data from one of the other classes to Amazon Glacier. So Glacier does not offer immediate access to data. And the speed of data retrieval will depend on which method you choose, those being expedited, standard or bulk. So remember those three different ways you can get data back from Glacier. Generally, if you need something back in under an hour then Glacier is not going to cut it. </p>
<p>Cross-region replication is used to help with disaster recovery by reducing latency of data retrievals and complying with governance and compliance controls. Multipart uploads improve performance by uploading object parts in parallel for multi-threaded performance. So if you’re confronted with a scenario where you need to upload large files over a stable network, use multipart upload to maximize the use of your available bandwidth. And the benefits of multi-part uploads are speed and throughput, interruption, recovery and management of data. The security options we have with storage, IAM policies, bucket policies, access control lists, lifecycle policies, multifactor authentication, delete, and versioning. </p>
<p>Remember the AWS Snowball is a service used to securely transfer large amounts of data in and out of AWS. And you do that via the data center to Amazon S3 or from S3 back to your data center. Using this physical appliance known as a Snowball. The appliance comes in either a 50 terabyte or 80 terabyte size and the Snowball appliance is built for high speed using the RJ45, SFP+ Copper, and SFP+ Optical. All data copied to a Snowball device is encrypted by default via KMS keys, and the AWS Snowball is HIPAA compliant. Now the Storage Gateway allows you to provide a gateway between your own data center storage systems and Amazon S3 and Glacier. </p>
<p>The Storage Gateway itself is a software appliance that can be installed within your own data center. And the appliance can be downloaded as a virtual machine and is stored on one of your own local hosts. The different configuration options available for Storage Gateway are File Gateways, Volume Gateways, and Tape Gateways. Looking at File Gateways, these allow you to securely store your files as objects within Amazon S3, which is presented as an NFS-share in which clients can mount or map a drive to, so NFS, great for File Gateway. Any data is sent over HTTPS connections and all objects are automatically encrypted using SSE-S3. A local cache is provisioned in the creation of a File Gateway, which uses on-premise storage to access the most recently access files to optimize latency. So we’ve got the File Gateway, which we’ve just talked about and we also have the Volume Gateway, and there’s two types of Volume Gateway, and there’s also a Tape Gateway, but let’s talk about the Volume Gateway now. </p>
<p>Volume Gateway has stored Volume Gateways and cached Volume Gateways. Stored Volume Gateways often are used as a way to back up your local storage volumes to Amazon S3. Your entire data library is also kept on premise for minimal latency and during its creation, volumes are created and backed up by Amazon S3 and are mapped directly to on-premise storage. Stored volumes are presented as SCSI devices allowing communication from your application service. And as data is written to these volumes it’s first stored using the on-premise MIPS storage, before Storage Gateway then copies the same data asynchronously to S3. So if you need an asynchronous solution, Stored Volume Gateways. </p>
<p>Snapshots of the volume can be taken which are then stored as EBS snapshots on Amazon S3. So volume sizes can be between one gigabyte and 16 terabytes and hold up to 32 volumes giving a total storage of 512 terabytes. Data is stored in a buffer using the on-premise storage before being written to the S3 using an SSL connection and in a disaster the EBS snapshots could be used to create new EBS volumes which can then be attached to EC2 Instances. Now, the cached Volume Gateway, the primary data storage is actually Amazon S3, rather than your own on-premise storage solution as the case with stored Volume Gateways. </p>
<p>So a cache is held locally using on-premise storage for buffering and to excess recently accessed data, that minimizes latency. Volumes are presented as iSCSI devices allowing connectivity from your application service and all data sent to S3 uses an SSL connection. And this is encrypted using SSE-S3. Volumes can be 32 terabytes in size with a total of 32 volumes giving a total storage of 1024 terabytes. Again, snapshots of these volumes can be also taken which are stored on S3 as EBS snapshots and again, in a disaster, the EBS snapshots can be used to create new EBS volumes which can be attached to EC2 Instances. </p>
<p>Tape Gateways are known as virtual tape libraries, and they allow you to backup data to S3 from your own corporate data center but they also leverage Amazon Glacier for data archiving. So virtual tape libraries are essentially a cloud-based tape backup solution. Applications and backup software can mount the tape drives along with a media changer as iSCSI devices to make the connection. When virtual tapes are archived, the data is simply moved from S3 to Glacier.</p>
<p>Okay, so that’s a good start into High Availability. We’re getting well-prepared for this exam. Don’t be afraid to get into the library and try the Lab Challenges. These are the best way to prepare yourself for the certification. They provide you with a scenarios that are very much in line with the exam questions and they’re all practical. So give that a go. It’s a great way to prepare. See you in the next one.</p>
<h1 id="2Designing-for-Failure-at-the-Global-Infrastructure-Level"><a href="#2Designing-for-Failure-at-the-Global-Infrastructure-Level" class="headerlink" title="2Designing for Failure at the Global Infrastructure Level"></a>2<strong>Designing for Failure at the Global Infrastructure Level</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-elastic-load-balancing-ec2-auto-scaling-support-aws-workloads/">Using Elastic Load Balancing &amp; EC2 Auto Scaling to Support AWS Workloads</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/">When to use RDS Multi-AZ &amp; Read Replicas</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/message/65648/">Summary of the Amazon EC2 and Amazon RDS Service Disruption</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-the-aws-database-migration-service/">Introduction to the AWS Database Migration Service</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/database/introducing-ongoing-replication-from-amazon-rds-for-sql-server-using-aws-database-migration-service/">Ongoing Replication Documentation</a></p>
<h1 id="6RDS-Multi-AZ"><a href="#6RDS-Multi-AZ" class="headerlink" title="6RDS Multi AZ"></a>6<strong>RDS Multi AZ</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.html">Configuring RDS Notifications</a></p>
<h1 id="16AWS-DynamoDB-HA-Options-Demo"><a href="#16AWS-DynamoDB-HA-Options-Demo" class="headerlink" title="16AWS DynamoDB HA Options Demo"></a>16<strong>AWS DynamoDB HA Options Demo</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/dynamodb-globaltables">DynamoDB GlobalTables GitHub repo</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Databases-SAA-C03-2-of-2-27/" rel="prev" title="AWS-Solution-Architect-Associate-Knowledge-Check-Databases-SAA-C03-2-of-2-27">
      <i class="fa fa-chevron-left"></i> AWS-Solution-Architect-Associate-Knowledge-Check-Databases-SAA-C03-2-of-2-27
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-Implement-Amazon-VPC-High-Availability-Best-Practices-29/" rel="next" title="AWS-Solution-Architect-Associate-Implement-Amazon-VPC-High-Availability-Best-Practices-29">
      AWS-Solution-Architect-Associate-Implement-Amazon-VPC-High-Availability-Best-Practices-29 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#High-Availability-SAA-C03-Introduction"><span class="nav-number">1.</span> <span class="nav-text">High Availability (SAA-C03) Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Designing-for-Failure-at-the-Global-Infrastructure-Level"><span class="nav-number">2.</span> <span class="nav-text">Designing for Failure at the Global Infrastructure Level</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Backup-and-DR-Strategies"><span class="nav-number">3.</span> <span class="nav-text">Backup and DR Strategies</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#High-Availability-vs-Fault-Tolerance"><span class="nav-number">4.</span> <span class="nav-text">High Availability vs Fault Tolerance</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Using-AWS-Storage-Gateway-for-On-premise-Data-Backup"><span class="nav-number">5.</span> <span class="nav-text">Using AWS Storage Gateway for On-premise Data Backup</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Resources-referenced-within-this-lecture"><span class="nav-number">5.0.1.</span> <span class="nav-text">Resources referenced within this lecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-Transcript"><span class="nav-number">5.0.2.</span> <span class="nav-text">Lecture Transcript</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDS-Multi-AZ"><span class="nav-number">6.</span> <span class="nav-text">RDS Multi AZ</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Read-Replicas"><span class="nav-number">7.</span> <span class="nav-text">Read Replicas</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Aurora-HA-Options"><span class="nav-number">8.</span> <span class="nav-text">Amazon Aurora HA Options</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Aurora-Single-Master-Multiple-Read-Replicas"><span class="nav-number">9.</span> <span class="nav-text">Aurora Single Master - Multiple Read Replicas</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Aurora-Single-Master-Multiple-Read-Replicas-DEMO"><span class="nav-number">10.</span> <span class="nav-text">Aurora Single Master - Multiple Read Replicas DEMO</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Aurora-Multi-Master"><span class="nav-number">11.</span> <span class="nav-text">Aurora Multi Master</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Aurora-Multi-Master-Setup-and-Use-DEMO"><span class="nav-number">12.</span> <span class="nav-text">Aurora Multi-Master Setup and Use DEMO</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Aurora-Serverless"><span class="nav-number">13.</span> <span class="nav-text">Aurora Serverless</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Aurora-Serverless-Database-Cluster-DEMO"><span class="nav-number">14.</span> <span class="nav-text">Aurora Serverless Database Cluster DEMO</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-DynamoDB-HA-Options"><span class="nav-number">15.</span> <span class="nav-text">AWS DynamoDB HA Options</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-DynamoDB-HA-Options-Demo"><span class="nav-number">16.</span> <span class="nav-text">AWS DynamoDB HA Options Demo</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#On-Demand-Backup-and-Restore"><span class="nav-number">17.</span> <span class="nav-text">On-Demand Backup and Restore</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Point-in-Time-Recovery"><span class="nav-number">18.</span> <span class="nav-text">Point in Time Recovery</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Point-in-Time-Recovery-Demo"><span class="nav-number">19.</span> <span class="nav-text">Point in Time Recovery Demo</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#High-Availability-Summary"><span class="nav-number">20.</span> <span class="nav-text">High Availability Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2Designing-for-Failure-at-the-Global-Infrastructure-Level"><span class="nav-number">21.</span> <span class="nav-text">2Designing for Failure at the Global Infrastructure Level</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6RDS-Multi-AZ"><span class="nav-number">22.</span> <span class="nav-text">6RDS Multi AZ</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#16AWS-DynamoDB-HA-Options-Demo"><span class="nav-number">23.</span> <span class="nav-text">16AWS DynamoDB HA Options Demo</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
