<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Architecture (SAA-C03) IntroductionHello, and welcome to this course on AWS architectures, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Solution-Architect-Associate-Architecture-SAA-C03-31">
<meta property="og:url" content="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Architecture-SAA-C03-31/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="Architecture (SAA-C03) IntroductionHello, and welcome to this course on AWS architectures, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid6-e0830763-2c6d-4d8e-801a-615f89715753.png">
<meta property="og:image" content="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid0-57c9129f-240c-4767-a47b-64a11c2436b7.png">
<meta property="article:published_time" content="2022-11-19T02:13:44.000Z">
<meta property="article:modified_time" content="2022-11-27T23:59:16.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid6-e0830763-2c6d-4d8e-801a-615f89715753.png">

<link rel="canonical" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Architecture-SAA-C03-31/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Solution-Architect-Associate-Architecture-SAA-C03-31 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Architecture-SAA-C03-31/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Solution-Architect-Associate-Architecture-SAA-C03-31
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:44" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:44-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 19:59:16" itemprop="dateModified" datetime="2022-11-27T19:59:16-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Architecture-SAA-C03-31/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Architecture-SAA-C03-31/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Architecture-SAA-C03-Introduction"><a href="#Architecture-SAA-C03-Introduction" class="headerlink" title="Architecture (SAA-C03) Introduction"></a>Architecture (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on AWS architectures, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the tools and services that can be used to design many different types of solution architectures that may be covered on the exam.</p>
<p>Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#x75;&#x70;&#x70;&#111;&#x72;&#116;&#x40;&#99;&#108;&#111;&#x75;&#100;&#x61;&#x63;&#97;&#x64;&#x65;&#x6d;&#121;&#x2e;&#x63;&#111;&#x6d;">&#115;&#x75;&#x70;&#x70;&#111;&#x72;&#116;&#x40;&#99;&#108;&#111;&#x75;&#100;&#x61;&#x63;&#97;&#x64;&#x65;&#x6d;&#121;&#x2e;&#x63;&#111;&#x6d;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the tools and services that may be used to design different solution architectures in AWS in preparation for the exam. The objective of this course is to provide an introduction to the tools, services, and methodologies you should consider when designing different types of solutions in AWS, including:</p>
<ul>
<li>The differences between decoupled and event-driven architectures;</li>
<li>Application services such as the Amazon Simple Notification Service, or SNS; the Amazon Simple Queue Service, or SQS; and AWS Step Functions;</li>
<li>How to work with streaming data in AWS using Amazon Kinesis or Amazon Managed Streaming for Apache Kafka, known as MSK;</li>
<li>Mobile application services such as Amazon Device Farm and Pinpoint;</li>
<li>A high-level introduction to AWS machine learning services; and</li>
<li>How to design multi-tier solutions.</li>
</ul>
<p>You’ll also learn about various design considerations to keep in mind when building serverless architectures or any architectures that leverage microservices. And we’ll wrap up the course with some tips for optimizing compute and storage costs within your solutions.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up. </p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#x40;&#99;&#x6c;&#x6f;&#x75;&#x64;&#97;&#99;&#x61;&#100;&#101;&#x6d;&#121;&#x2e;&#99;&#111;&#x6d;">&#115;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#x40;&#99;&#x6c;&#x6f;&#x75;&#x64;&#97;&#99;&#x61;&#100;&#101;&#x6d;&#121;&#x2e;&#99;&#111;&#x6d;</a>. Thank you!</p>
<h1 id="What-is-a-Decoupled-and-Event-Driven-Architecture"><a href="#What-is-a-Decoupled-and-Event-Driven-Architecture" class="headerlink" title="What is a Decoupled and Event-Driven Architecture?"></a>What is a Decoupled and Event-Driven Architecture?</h1><p>Hello and welcome to this lecture where I want to explain what we mean by decoupled and event-driven architectures.</p>
<p>Firstly, let me focus on decoupled architecture, and to understand decoupling, we first need to address monolithic architectures which is how applications have been done in the past. Monolithic applications were built with a close and tight-knit relationship to each other, for example, between the front end and back end of an application. If a change was made the back end, it could easily disrupt services and operation in the front end, and that’s because they were very tightly coupled together and had a lot of built-in dependencies against each other. Although this had some advantages, it wasn’t able to offer what a decoupled architecture could.</p>
<p>When you implement and design a solution using a decoupled architecture you are building a solution put together using different components and services that can operate and execute independently of one another, instead of being closely coupled to each of its connecting components to operate and function. Each component in a decoupled solution is effectively unaware of any other changes to other components due to the segregation of boundaries applied.</p>
<p>Each service within a decoupled environment communicates with others using specific interfaces which remain constant throughout its development regardless of what configurations are made. By having this layered and independent approach, you are able to design, develop and configure each component without worrying about any dependencies within your solution. This allows your development teams to work faster and more efficiently as their scope of operation is refined on a particular service or component. They can make changes to a specific area of the application without having to worry about affecting other components, this helps to drive innovation and progress at a far greater rate.</p>
<p>As you go through this course, I will introduce you to an AWS service that is commonly used in a decoupled architecture, this being Amazon SQS, the Simple Queue Service.</p>
<p>Let me now look at Event-Driven Architectures.</p>
<p>Event-driven architectures closely relate and interact with decoupled architectures, however, services in an event-driven architecture are triggered by events that occur within the infrastructure. So what is an event? Well, an event can be a number of things, for example, a change of state, so a resource such as an EC2 instance changing from ‘running’ to ‘stopped’—that is a change of state, or perhaps an order has been placed on your website and an item has been moved from for sale to sold, that could be a change of state within your application.</p>
<p>When utilizing and implementing event-driven architectures in AWS, they will typically have three components: a producer, an event router, and consumers.</p>
<p>A producer is the element within the infrastructure that will push an event to the event router. The event router then processes the event and takes the necessary action in pushing the outcome to the consumers. By having the event router sat between both the producer and consumers, each of these two components are decoupled from each other and carry the benefits of a decoupled architecture that I discussed previously.</p>
<p>As we go through this course, I will introduce you to a number of different event-driven services, which act as the event routers, and these include Amazon SNS (the Simple Notification Service), Amazon Kinesis, and AWS Lambda.</p>
<h1 id="Introduction-to-the-Simple-Queue-Service"><a href="#Introduction-to-the-Simple-Queue-Service" class="headerlink" title="Introduction to the Simple Queue Service"></a>Introduction to the Simple Queue Service</h1><p>Hello and welcome to this lecture which will cover the SQS service, Simple Queue Service. With the continuing growth of microservices and the cloud best practice of designing decoupled systems, it’s imperative that developers have the ability to utilize the service or system that handles the delivery of messages between components. And this is where SQS comes in. SQS is a fully managed service offered by AWS that works seamlessly with serverless systems, market services and any distributed architecture. Although it’s simply a curing service for messages between components, it does much more than that. It has the capability of sending, storing and receiving these messages at scale without dropping message data, as well as utilizing different queue types depending on requirements and includes additional features such as dead-letter queues. It is also possible to configure the service using the AWS Management Console, the AWS CLI or using the AWS SDKs. Let me focus on some of the components to allow you to understand how the service is put together. </p>
<p>The service itself uses three different elements, two of which are a part of your distributed system. These being the producers and the consumers. And the third part is the actual queue, which is managed by SQS and is managed across a number of SQS service for resiliency. Let me me explain how these components work together. The producer component of your architecture is responsible for sending messages to your queue. At this point, the SQS service stores the message across a number of SQS servers for resiliency within the specified queue. This ensures that the message remains in the queue should a failure occur with one of the SQS servers. Consumers are responsible for processing the messages within your queue. As a result, when the consumer element of your architecture is ready to process the message from the queue, the message is retrieved and is then marked as being processed by activating the visibility timeout on the message. This timeout ensures that the same message will not be read and processed by another consumer. When the message has been processed, the consumer then deletes the message from the queue. </p>
<p>Before moving on, I just want to point out a little more relation to the visibility timeout. As I said, when a message is retrieved by a consumer, the visibility timeout is started. The default time is 30 seconds, but it can be set up to as long as 12 hours. During this period, the consumer processes the message. If it fails to process a message, perhaps due to a communication error, the consumer will not send a delete message request back to SQS. As a result, if the visibility timeout expires and it doesn’t receive the request to delete the message, the message will become available again in the queue for other consumers to process. This message will then appear as a new message to the queue. The value of your visibility timeout should be longer than it takes for your consumers to process your messages. </p>
<p>I mentioned earlier that there were different types of queues. These being standard queues, first in, first out queues and dead-letter queues. Standard queues, which are the default queue type upon configuration, support at-least-once delivery of messages. This means that the message might actually be delivered to the queue more than once, which is largely down to the highly distributed volume of SQS servers, which would make the message appear out of its original order or delivery. As a result, the standard queue will only offer a best effort when trying to preserve the message ordering from when the message are sent by the producers. Standard queues also offer an almost unlimited number of transactions per second, TPS, making this queue highly scalable. </p>
<p>If message ordering is critical to your solution, then standard queues might not be the right choice for you, instead, you would need to use first in, first out queues. This queue is able to ensure the order of messages is maintained and that there are no duplication of messages within the queue. Unlike standard queues, FIFO queues do have a limited number of transactions per second. These are defaulted to 300 per second for all send and receive and delete operations. If you use batching with SQS, then this changes to 3,000. Batching essentially allows you to perform actions against 10 messages at once within a single action. </p>
<p>So the key takeaways between the two queues are: for standard queues, you have unlimited throughput, at-least-once delivery and best-effort ordering. And for first in, first out queues, you have high throughput, first in, first out delivery and exactly-once processing. For both queues, it is also possible to enable encryption using server-side encryption via KMS. A dead-letter queue differs to the standard and FIFO queues as this dead-letter queue is not used as a source queue to hold messages submitted by producers. Instead, the dead-letter queue is by the source queue to send messages that fail processing for one reason or another. This could be the result of code within your application, corruption within the message or simply missing information within database that the message data relates to. </p>
<p>Either way, if the message can’t be processed by a consumer after a maximum number of trials specified, the queue will send the message to a dead-letter queue. This allows engineers to assess why the message failed to identify where the issue is to help prevent further messages from falling into the dead-letter queue. By viewing and analyzing the content of these messages, it might be possible to identify the problem and ascertain if the issue exists from the producer or consumer perspective. A couple of points to make with a dead-letter queue is that it must be configured as the same queue type as the source is used against. For example, if the source queue is a standard queue, the dead-letter queue must also be a standard queue type. And similarly, for FIFO queues, the dead-letter queue must also be configured as a FIFO queue. </p>
<p>Before I end this lecture, I just want to show a quick demonstration on how to set up a queue and some of the configuration options available during this process. Okay, so in this demonstration, I’m gonna show you had to set up a queue in SQS. So I’m currently at the AWS Management Console, and if I just search for SQS, we can see that the simple queue service comes up. So if I select on that, and this is the page that you’ll get if you’ve never used SQS before, so it’s a splash screen that just gives you a bit of information about the service. From here, I just need to click on Get Started Now. And now it’s going to ask us a few questions about the queue. Firstly, we need to enter a queue name. So I’m just gonna call this Cloud Academy. And then we have our region. Currently I’m in the EU region, which is fine. And then down here we have our type of queue. So we can either have our standard queue or our FIFO queue. For this demonstration, I’m just going to stick with the standard queue type. </p>
<p>If I scroll down a bit further, we can see at the bottom here, we have Cancel Configure Queue or Quick Create Queue. I’m gonna select Configure Queue just so we can see some additional details. And here, you can set additional options such as the visibility timeout, the message retention period, which is how long SQS will keep the message if the message isn’t deleted, and other options such as the message size and delay delivery, et cetera. In the next section down here, we have the dead-letter queue settings. If we want to set up a dead-letter queue, we select this tick box here that says Use Redrive Policy. And all the redrive policy does is simply explain what conditions has to be met for a message to be moved to the dead-letter queue. And here, we’d enter the name of the dead-letter queue. And also, the maximum number of receives a message can have before it is sent to that queue. For this demonstration, I’m gonna leave the dead-letter queue as blank. And then at the bottom here, we have the server-side encryption settings. </p>
<p>Now if we want to use SSE, we simply click on this tick box and this will open up additional options to allow us to select a customer master key, a CMK. Now AWS will provide a default CMK to use with SQS. And as you can see, this AWS SQS is the default master key that protects SQS messages when no other key is defined. We can see which account it’s in and also the ARN. The data key reuse period is a time factor that’s used to specify how long SQS can continue to use this key before it has to go back to KMS and request that key again. When I’m happy with all the settings, I simply click on Create Queue. And then we have it. So this is the dashboard of SQS. We can see that we have our Cloud Academy Queue here. And at the bottom of the screen, we can see additional metadata about this queue, such as when it was created, last updated, et cetera. And that’s it. That’s how you set up an SQS queue. It’s a very simple process, very self-intuitive and very quick and easy. </p>
<p>That now brings me to the end of this lecture which covered an introduction to the Simple Queue Service.</p>
<h1 id="Introduction-to-the-Simple-Notification-Service"><a href="#Introduction-to-the-Simple-Notification-Service" class="headerlink" title="Introduction to the Simple Notification Service"></a>Introduction to the Simple Notification Service</h1><p>Hello, and welcome to this lecture covering the SNS service. It’s likely that out of the three services covered within <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-sqs-sns-ses/">this course</a>, this is the service you may have come across the most, simply due to its integration with a number of other AWS services. The Simple Notification Service is used as a publish and subscribe messaging service. But what does this mean? SNS is centered around topics. And you can think of a topic as a group for collecting messages. Users over endpoints can then subscribe to this topic and messages or events are then published to that particular topic. When a message is published, all subscribers to that topic receive a notification of that message. This helps to implement event driven architectures within a decoupled environment. Again, much like <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/">SQS</a>, SNS is a managed service and highly scalable, allowing you to distribute messages automatically to all subscribers across your environment, including mobile devices. </p>
<p>It can be configured with the AWS management console, the CLI, or the AWS SDK. As mentioned, SNS uses a concept of publishers and subscribers, which can also be classed as consumers and producers, and works in the same principle as SQS from this perspective. The producers, or publishers, send messages to a topic, which is used as a central communication control point. Consumers, or subscribers of the topic, are then notified of this message by one of the following methods: HTTP, HTTPS, Email, Email-JSON, Amazon SQS, Application, AWS Lambda, or SMS. Subscribers don’t just have to be users. For example, it could be a web server and they can be notified of the message via the HTTP protocol. Or if it was a user, you could use the email notification method and enter their email address. SNS offers methods of controlling specific access to your topics through a topic policy. For example, you might want to restrict which protocol subscribers can use, such as SMS or HTTPS, or only allow access to this topic for a specific user. The policy themselves follow the same format as IAM policies. For more information on IAM policies, please see our existing IAM course, which is available within our library of content. I will now perform a demonstration showing you how topic policies are configured and the different options within them, which allows you to apply access security to your topics. Both SNS and SQS integrate with each other, which makes sense, as both of these services are designed to run in a highly distributed and decoupled environment. </p>
<p>By working together, a solution can be designed to send messages to subscribers through a push method. Or SQS handles incoming messages, and waits for consumers to pull data. Therefore, being able to use SNS as a producer for an SQS queue makes perfect sense from a development perspective. To do this, you’ll need to have your SQS queue subscribed to the SNS topic. And this can be achieved by performing the following steps within this demonstration. Much like SQS, SNS also integrates well with AWS Lambda, a key serverless computer service. To learn more about serverless technologies, you can view our existing learning path entitled Serverless Computing on AWS for Developers, which can be found <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/serverless-computing-aws-developers-45/">here</a>. This integration allows SNS notifications to invoke existing Lambda functions. Like SQS, the Lambda function has to be subscribed to the topic. Then when a message is sent to the topic, the message is pushed out to the Lambda function to invoke it. The function itself uses the payload of the message as an input parameter, where it can then alter the message if required, or forward the message onto another AWS service, or indeed to another SNS topic. To configure AWS Lambda to work with the topic, you can perform the following steps. From within the SNS dashboard of the AWS management console, select topics. Select the topic that you want to subscribe to with the Lambda function. Select actions, and subscribe to topic. Using the protocol menu, select the AWS Lambda option. Then you must select the Lambda function to be used from the endpoint dropdown box.</p>
<p>Finally, you can select the version or alias of the function, and to select the latest of the function, choose the latest option. Select create subscription. To gain more insight into this process and to see an example of how this can be used to create a sample message history store using SNS Lambda and Amazon DynamoDB, you can view this blog post made by AWS, found <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/mobile/invoking-aws-lambda-functions-via-amazon-sns/">here</a>. We also have a lab which will teach you how to process SNS notifications with a Lambda function. As a simple example, the lab uses python to log custom metrics to CloudWatch based on the message payload. That now brings me to the end of this lecture, which covered an introduction to the Simple Notification Service.</p>
<h1 id="AWS-Step-Functions"><a href="#AWS-Step-Functions" class="headerlink" title="AWS Step Functions"></a>AWS Step Functions</h1><p>The primary option that comes to mind when thinking about <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon Web Services</a> and Serverless workloads is AWS Lambda. It is a fantastic resource that allows for serverless compute without having to deal with the burden of the underlying compute infrastructure.</p>
<p>Unfortunately, Lambda is not exactly well known for its flexibility and ability to perform long-running and complex operations. For example, Lambda was limited for quite a while to 5 minutes of execution time for your code, with it only just recently extending out to 15 minutes. </p>
<p>Now that may seem like quite a lot of time when you are thinking about running a script or some simple calculations, but for anything more complex, it might not be enough.</p>
<p>For example; If you have ever played around with building simple applications with Lambda, you might have wanted to retry a connection, or wait until something becomes available before moving onto the next action, or even simply having the ability to run something in parallel. These are common workflows that many people desire and expect. Unfortunately, these features are not natively included with Lambda.</p>
<p>Don’t let that dissuade you from using Lambda all together because this is where <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-step-functions-1117/introduction/">AWS Step Functions</a> can take a leading role.</p>
<p>AWS Step Functions can help guide and shape these interactions and allow you to create interactive and complex systems that utilize all these features we just went over, and more, with complete orchestration and ease of transparency! With that in mind let’s dive in and talk about it.</p>
<p>AWS Step Functions can best be described as a state machine service. For those who don’t know what a state machine is, think of your standard vending machine. </p>
<p>A vending machine sits there waiting for a customer to come up to it and input money (that’s its idle state). Once money has been added into the machine, it movies onto the next state, which would be item selection. The user inputs their choice, and the machine moves into the final state of vending the product. After the workflow has been completed it returns back to the idle state, waiting for another customer.</p>
<p>AWS Step Functions allow you to create workflows just like the vending machine, where you can have your system wait for inputs, make decisions, and process information based on the input variables.</p>
<p>With this kind of orchestration, we are able to run Lambda functions in ways that are not inherently supported by the service itself.</p>
<p>For example, we can use Step Functions to run our code.</p>
<ul>
<li>In parallel, for when you have multiple items or tasks you want to process at one time In sequence, for when order is important.</li>
<li>In retry, maybe you want your code to keep executing until it succeeds, or reaches a time out of some sort.</li>
<li>If then, allows branching and logical trees for decision making.</li>
</ul>
<p>With these options available for your Lambda functions, we are able to overcome probably the greatest hurdle of serverless and Lambda, which is the 15 minute limit of code execution.</p>
<p>This ability allows you to create very powerful fully serverless applications and workflows.</p>
<p>AWS Step Functions operates by reading in your workflow from an amazon state language file - a JSON based structured language used to define your state machine and its various components. </p>
<p>Amazon State Language is a proprietary language that consists of a collection of states. These states in turn can do some type of work, and from there the machine can make the decision to move onto the next state. </p>
<p>Here is an example of what Amazon State language looks like.</p>
<p>As you can see it is very much a JSON type language, and this is helpful because it’s a familiar syntax that many developers are already used to writing in, but it might be confusing to those who are new. </p>
<p>The good news is that AWS Step Functions provides a visual representation of your state machine right in the console. </p>
<p>This visual graph updates in real time as you edit your code and provides valuable feedback during creation of your machines. </p>
<p>Additionally, this visual flow graph is inspectable during runtime and after completion. This feature allows you to get a deeper understanding of what is happening behind the scenes. Each element can be inspected to show the inputs and outputs as they appear.</p>
<p>There are eight states that your state machine can be in at any time.Let me go over these individually. </p>
<p>The Pass State is basically a debugging state or one to be used when first creating your machine. It allows you to pass its input value straight through to its output, as well as add a fixed result </p>
<p>The Task State. This is where the work actually happened. With a task, you define a resource you wish Step Functions to run as well as a timeout period. For example, you could plug in your Lambda function here to run some code. This state is used often as a sub state (or action) within other states.</p>
<p>The Choice State - given an input, the state machine chooses the correct output. Basically, an if then operation where you can run further application logic.</p>
<p>Wait - the state machine will pause and can wait until a specific time or until x amount of time has passed. This might be useful if you wanted an email for fire out at 8am everyday for example.</p>
<p>Succeed - simply the termination of the state machine in a successful fashion. Can be a part of a choice state for example to end the state machine.</p>
<p>Fail - also a termination state for the state machine, in a failed way. Fail states must have an error message and a cause.</p>
<p>Parallel State - Executes a group of states as concurrently as possible and waits for each branch to terminate before moving on. The results of each parallel branch are combined together in an array-like format and will be passed onto the next state.</p>
<p>Map State - allows you to iterate through a list of items and perform tasks on them. You can also define the number of concurrent items being worked on at one time. Think of this like a for loop for processing data.</p>
<p>Using combinations of these states to create your specific state machines - allows you to build some very dynamic and impressive serverless solutions that can scale extremely well.</p>
<p>Here is a high-level example of what a few of these state can look like in action. Imagine we wanted to create a simple app that provides image tagging and creates thumbnails for png images.</p>
<p>The state machine might look something like this.</p>
<p>Upon taking in an input image the first step would be to extract any metadata about the image if possible. This would be a task state.</p>
<p>The output from that task can be sent onto the next state which would check to see if the image format is supported. This state is a choice state </p>
<p>From there we either find that the image is unsupported and the operation fails or we move onto storing this metadata. Storing would be another task.</p>
<p>We can then send the image off to Amazon Rekognition to generate our tags and create a thumbnail in parallel. This would be a parallel state.</p>
<p>Finally, we would add the rekotags to the image itself or into a database and associate them later. </p>
<p>And then the state machine ends. Neato burrito.</p>
<p>So far I have talked a lot about using Lambda as your interaction medium when performing tasks with Step Functions, but there are actually quite a few services that Step Functions can interact with directly. If you take a look at this chart, you can see that Step Functions has quite a breadth of services available for you to use.</p>
<p>For example, you do not have to use Lambda to add an item into a table within DynamoDB, you can do it directly by calling that function specifically within DynamoDB. Here is what that might look like.</p>
<p>Here are a few other examples of what you can do natively within Step Functions:</p>
<ul>
<li>Run an Amazon Elastic Container Service or AWS Fargate task</li>
<li>Submit an AWS Batch job and wait for it to complete</li>
<li>Publish a message to an Amazon SNS topic</li>
<li>Send a message to an Amazon SQS queue</li>
<li>Start an AWS Glue job run</li>
<li>Create an Amazon SageMaker job to train a machine learning model or batch transform a data set</li>
</ul>
<p>One of the most impressive features of Aws Step Functions is its capacity for asynchronous callbacks. This means that if you have a workflow that requires something to be approved by a managing authority, or maybe you utilize a third party API that provides a service that takes hours or days or weeks to complete, Step Functions provides this ability, which can add dynamacy and resilience to your workflows. </p>
<p>We also have the ability to nest child state machines within parent state machines. This provides greater benefits the longer you work with Step Functions, because you will find repeatable patterns occurring within your workflows fairly often. For example, you might have a core step function that needs to be referenced by other tangential services.  Having the capacity to nest your functions will save you a lot of time down the road, and help with encapsulation of that core business logic.</p>
<p>Now that we have a basic understanding of what AWS Step Functions is, and the pieces that make it up, I think it would be good to see a full example of what you can create with the service. </p>
<p>This is quite possibly my favorite example, and it’s a complete video on demand workflow leveraging AWS Step Functions, AWS Elemental MediaConvert, and AWS Elemental MediaPackage.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/awslabs/video-on-demand-on-aws">https://github.com/awslabs/video-on-demand-on-aws</a></p>
<p>This architectural diagram shows a three-part, multi-faceted architecture that deals with the complete lifecycle of a video on-demand service.</p>
<p>All parts of this operation function completely serverlessly and involve multi-phased step function elements to orchestrate the entire process.</p>
<p>Starting with our source files, which might already be set up in our s3 bucket or could be placed there as they come in, we would have raw video. This video is archived in Amazon S3 Glacier while at the same time being pushed into our ‘Ingest’ Workflow by a Lambda function.</p>
<p>Let’s take a look at the ingest workflow and see what it does.</p>
<p>Inside here we have a few states that are all pretty simple to understand.</p>
<p>Input validation - Checking the file types to make sure they are supported.</p>
<p>Mediainfo, generates signed URLs for the source files and extracts metadata about the video.</p>
<p>DynamoDB Update takes all this relevant information and drops it into DynamoDB.</p>
<p>SNS Choice. A simple flag to determine if we want to be notified about the status of the uploads.</p>
<p>SNS Notification. Using Amazon SNS, it sends a notification about the status of the ingestion process, such as did it pass or fail.</p>
<p>Process Execute starts the processing workflow.</p>
<p>Again, all these tasks are completed automatically without you having to spin up any servers. Step Functions works through each state until completion and then proceeds onto the next one.</p>
<p>After ingestion, the video is processed and converted into various bitrates and sizes, with icons, and all the good stuff that you would expect from a steaming type service. Next, all the pertinent information is pushed out to be published, where it can then be delivered to the customer.</p>
<p>I won’t go into each of the workflows because I think you can see the point here. Every step function workflow you see in this architecture has its own unique job and tasks that it performs. Each can be as complicated or as simple as it requires.</p>
<h1 id="Fundamentals-of-Stream-Processing"><a href="#Fundamentals-of-Stream-Processing" class="headerlink" title="Fundamentals of Stream Processing"></a>Fundamentals of Stream Processing</h1><p>Stream processing, Machine Learning, and Artificial Intelligence are popular topics inside cloud computing. More and more companies seem to be using modern stream processing tools. Cloud providers like AWS are releasing better and more powerful streaming products, and specialists are increasingly in high demand.</p>
<p>However, what–exactly–is stream processing? The answer is complex but not complicated. It is a large and varied topic. Streaming data gained importance because not all data is created equally and its value changes over time.</p>
<p>Some information has value that can be measured in years. Other data has great value but only at the moment it is produced.</p>
<p>Before stream processing existed, large volumes of data were usually stored either in a database or on an enterprise-class server and processed all at the same time. The analysis of this data was performed using what we now call batch processing because, as it sounds, it was done in a single “batch.”</p>
<p>With batch processing, data is collected, stored, and analyzed in chunks of a fixed size on a regular schedule. The schedule depends on the frequency of data collection and the related value of the insight gained. It’s this value that is at the center of stream processing.</p>
<p>As I mentioned already, some information is nearly timeless. Think of learning the alphabet, the order of operations in algebra, or the names of people and places. This data is slow to change–if at all–and its value remains relatively constant.</p>
<p>However, some information is only valuable at the moment it’s being accessed and processed. Time-critical data is used for preventative maintenance or to react to one or more events in real time.</p>
<p>Consider those moments in your life when someone made a comment that left you speechless. It’s the perfect time for a witty comeback or retort and you have nothing to say. After walking away, you think of that perfect response but it is too late. The moment is gone forever. </p>
<p>There’s no word that I know of, in English, to describe this phenomenon but, in French, the phrase is “l’esprit de l’escalier” and, in German, “Treppenwitz.” Depending on the translation, the words mean “the spirit of the staircase” or “the wit–or joke–of the staircase.” </p>
<p>That is, after you’ve walked away from the situation and started down a flight of stairs, that’s when you think of the exact right thing to say. It’s way too late to be funny, witty, or otherwise engaging.</p>
<p>This describes how some data loses value over time. As transactions happen, at the moment, that’s when the data has value. It might be a recommendation engine that suggests an additional item, doing sentiment analysis to determine how a person feels about a product, or anomaly detection for IoT hardware failures.</p>
<p>Processing this type of data minutes, hours, or even days later becomes a type of l’esprit de l’escalier or Treppenwitz; a staircase joke. Except, it’s not funny; sales are lost, people are angry or frustrated, and devices fail.</p>
<p>The l’esprit de l’escalier problem is really an issue of latency and the value of data over time.</p>
<p>After latency, there are two other issues related to batch processing that impact the value of data. </p>
<p>The first issue that I’d like to mention involves session states. In this context, think of a session as a collection of events or transactions that are related. Batch processing systems split data into time intervals that are consistent and evenly spaced. </p>
<p>This creates a steady workload that is predictable. The trouble, here, is that, while predictable, it has no intelligence. Sessions that begin in one batch might end in a different one. This makes the analysis of related transactions difficult. </p>
<p>The second issue is that batch processing systems are also designed to wait until a specific amount of data is accumulated before processing starts. </p>
<p>Batch architectures have been optimized to process large amounts of data at a single time. So, an analysis job might have to wait for extended periods of time because the queue needs to be full before processing can begin.</p>
<p>While the size of the batch job is uniform, the time period in each batch of data is inconsistent. </p>
<p>Steam processing, then, was created to address these issues of latency, session boundaries, and inconsistent load.  </p>
<p>The term streaming is used to describe information as it flows continuously without a beginning or an end.</p>
<p>It is never-ending and provides a constant feed of events that can be acted upon without the need to be downloaded first.</p>
<p>A simple analogy is how water flows through a river or creek. Water comes from various sources, in varying speed and volumes and flows into a single, continuous, combined stream.</p>
<p>Similarly, data streams are generated by all types of sources, in various formats and volumes. </p>
<p>These sources can be applications, networking devices, server log files, website activity, banking transactions, and location data. </p>
<p>All of them can all be aggregated in real-time to respond and perform analytics from a single source of truth.</p>
<p>Stream processing, then, is acting on–or reacting to–data while it is motion. Computation happens in the moment data is produced or received. </p>
<p>When receiving an event from the stream, a Stream processing application reacts to it. This reaction might be to trigger an action, update an aggregate or similar statistic, or cache the event for future reference.</p>
<p>Multiple data streams can be processed simultaneously and <strong>Consumers</strong>, applications that process the data from a stream, can create new data streams.</p>
<p>An example of steam processing involves credit card fraud alerting. </p>
<p>Speaking from experience, I’ve been on trips where, after using my credit card in a city far from home, I’ve received a text message on my phone asking if a recent transaction was legitimate. </p>
<p>This type of processing requires evaluating data in real-time. This includes the transaction, sending alerts, receiving a response, and acting on the response.</p>
<p>Instead of batches of a fixed size, a data stream is a collection of related events or transactions. </p>
<p>Typically, a stream application has three main parts; <strong>Producers</strong>, a <strong>Data</strong> <strong>Stream</strong>, and <strong>Consumers</strong>. </p>
<p><strong>Producers</strong> collect events or transactions and put them into a Data Stream.  The <strong>Data Stream</strong>, itself, stores the data. <strong>Consumers</strong> access streams, read data, then act on it. </p>
<p>There are a number of benefits for using streaming data frameworks. </p>
<p>Some data naturally comes as a never-ending stream of events and is best processed while it is <em>in-flight</em>. </p>
<p>Batch processing is built around a data-at-rest architecture. Before processing can begin, the collection has to be stopped and the data must be stored.</p>
<p>Subsequent batches of collected data bring the need to create an aggregate across multiple batches.</p>
<p>In contrast to this, streaming architectures handle never-ending data flows naturally and  with grace. Using streams, patterns can be detected, results inspected, and multiple streams can be examined simultaneously.</p>
<p>Sometimes, the volume of data is larger than the existing storage capacity. </p>
<p>Yes, in the cloud, there seems to be no limit to available storage but that storage comes with a price tag. </p>
<p>It also comes with a human failing. Sometimes, people are afraid to delete data because they <em>might</em> need it later. </p>
<p>Using streams, raw data is processed in real-time and you retain only the information and insight that is useful. </p>
<p>Stream processing naturally fits with time-series data and the detection of patterns over time. </p>
<p>For example, when trying to detect a sequence such as the length of a web session in a continuous stream of data, it would be difficult to do in batches.</p>
<p>Time series data, such as that produced by IoT sensors, is continuous and fits naturally into a streaming data architecture.</p>
<p>There’s no almost no lag time between when events happen, insights are derived, and actions are taken.</p>
<p>Actions and analytics are up-to-date and reflect the state of the data while it is still fresh, meaningful, and valuable.</p>
<p>Streaming reduces the need for large and expensive shared databases. </p>
<p>When using a streaming framework, each stream processing application maintains its own data and state, and–because of this–stream processing fits naturally inside a microservices architecture.</p>
<p>Something that I think is important to mention is that batch processing is still required. Stream processing compliments batch computing.</p>
<p>Month-end billing is still best done using some sort of batch process. The value of billing data remains substantial and predictable. Large scale reporting does <strong>not</strong> need expensive, high-speed, low-latency compute engines.  It’s just that, as a consumer, I don’t want to have to wait 30-45 days to learn about possible fraud.</p>
<p>Stream processing is used to collect, process, and query data in either real time or near real time to detect anomalies, generate awareness, or gain insight. </p>
<p>Real-time data processing is needed because, for some types of information, the data has actionable value in the moment it was collected and its value diminishes, rapidly, over time. </p>
<p>Stream processing can provide actionable insights within milliseconds to seconds of a recorded event.</p>
<p>So how important is stream processing? </p>
<p>A better question, perhaps, is how important and&#x2F;or useful is it to have immediate insight into how the business is operating, how customers feel, or what devices are online and in use? </p>
<p>Consider real-time trading in commodities; a fraction of a second advantage can translate into millions in profit or loss. </p>
<p>What about major consumer product companies doing global launches of products where millions of people log in at the same time to purchase? On days like Black Friday or Cyber Monday people expect fast and consistent responses. </p>
<p>Not every transaction requires an immediate response, but there are many that do.</p>
<p>Businesses that specialize in e-commerce, finance, healthcare, and security need immediate responses and this is the target market for stream processing. </p>
<p>The problem is that companies need the ability to recognize that something important has happened and they need to be able to act on it in a meaningful and immediate way. </p>
<p>Immediacy matters because data can be highly perishable and its shelf life can be measured in milliseconds.</p>
<p>This brings me to the end of this lecture. Thank you for watching and letting me be part of your cloud journey.</p>
<p>If you have any feedback, positive or negative, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated.</p>
<p>For Cloud Academy, I’m Stephen Cole. Thank you!</p>
<h1 id="Amazon-Kinesis-Overview"><a href="#Amazon-Kinesis-Overview" class="headerlink" title="Amazon Kinesis Overview"></a>Amazon Kinesis Overview</h1><p>Amazon Kinesis was designed to address the complexity and costs of streaming data into the AWS cloud. </p>
<p>Kinesis makes it easy to collect, process, and analyze various types of data streams such as event logs, social media feeds, clickstream data, application data, and IoT sensor data in real time or near real-time. </p>
<p>Access to Kinesis is controlled using AWS Identity and Access Management, IAM. </p>
<p>Using the AWS Key Management Service, KMS, data is automatically protected in the stream as well as when it is placed inside an AWS storage service such as Amazon S3 or Redshift.</p>
<p>Data in transit is protected using TLS, the Transport Layer Security Protocol.</p>
<p>Amazon Kinesis is composed of four services, <strong>Kinesis Video Streams</strong>, <strong>Kinesis Data Streams</strong>, <strong>Kinesis Data Firehose</strong>, and <strong>Kinesis Data Analytics</strong>.</p>
<p><strong>Kinesis Video Streams</strong> is used to do stream processing on binary-encoded data, such as audio and video.</p>
<p><strong>Kinesis Data Streams</strong>, <strong>Kinesis Data Firehose</strong>, and <strong>Kinesis Data Analytics</strong> are used to stream base64 text-encoded data. This text-based information includes sources such as logs, click-stream data, social media feeds, financial transactions, in-game player activity, geospatial services, and telemetry from IoT devices. </p>
<p>Please note that, at the time this content was written, the course information was accurate. AWS implements hundreds of updates every month as part of its ongoing drive to innovate and enhance its services.</p>
<p>As a result, minor discrepancies may appear in the course content over time. Here at Cloud Academy, we strive to keep our courses up to date in order to provide the best training available. </p>
<p>If you notice any information that is outdated, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. </p>
<p>This will allow us to update the course during the next release cycle. We would love to hear from you. <strong>I</strong> would love to hear from you. </p>
<p>Tell me what you learned, what could use some attention, or what worked really well for you. If you discover a new and better way to do something, I’d love to learn about it. Especially if it makes life easier. I’m not lazy, I’m efficient. They look the same but we both know they’re different. Right?</p>
<p>Generally speaking, streaming data frameworks are described as having five layers; the <strong>Source</strong>, <strong>Stream Ingestion</strong>, <strong>Stream Storage</strong>, <strong>Stream Processing</strong>, and the <strong>Destination</strong>.</p>
<p>Using <strong>Kinesis Data Streams</strong> the process looks like this.</p>
<p>Data is generated by one or more <strong>sources</strong> including mobile devices, meters in smart homes, click streams, IoT sensors, or logs. </p>
<p>At the <strong>Stream Ingestion Layer</strong> data is collected by one or more <strong>Producers</strong>, formatted as <strong>Data Records</strong>, and put into a stream. </p>
<p>The Kinesis Data Stream is a <strong>Stream Storage Layer</strong> and is a high-speed buffer that stores data for between a minimum of 24 hours and, as of November 2020, 365 days. 24 hours is the default.</p>
<p>Inside Kinesis Data Streams, the Data Records are immutable. Once stored, they cannot be modified. Updates to data require a new record to be put into the stream. Data is also not removed from the stream, it can only expire. </p>
<p>The <strong>Stream Processing Layer</strong> is managed by Consumers. Consumers are also known as <strong>Amazon Kinesis Data Streams Applications</strong> and process data contained inside a stream. </p>
<p>Consumers send Data Records to the <strong>Destination Layer</strong>. This can be something like a Data Lake, a Data Warehouse, durable storage, or even another stream.</p>
<p>I’d like to take a few minutes to describe each of the four Amazon Kinesis Streaming services.</p>
<p><strong>Amazon Kinesis Video Streams</strong> is designed to stream binary-encoded data into AWS from millions of sources. Traditionally this is audio and video data but it can be any type of binary-encoded time-series data. It’s got video in its name because it is the primary use case. </p>
<p>The AWS SDKs make it possible to securely stream data to AWS for playback, storage, analytics, machine learning and other processing.</p>
<p>Data can be ingested from devices such as smartphones, security cameras, edge devices, RADAR, LIDAR, drones, satellites, and dash cams.</p>
<p>Kinesis Video Streams supports the open-source project WebRTC. This allows for two-way, real-time media streaming between web browsers, mobile applications, and connected devices.</p>
<p><strong>Amazon Kinesis Data Streams</strong> is a highly customizable streaming solution available from AWS. </p>
<p><em>Highly customizable</em> means that all parts involved with stream processing–data ingestion, monitoring, scaling, elasticity, and consumption–are done programmatically when creating a stream. AWS will provision resources only when requested.</p>
<p>One important takeaway here is that Kinesis Data Streams does <strong>not</strong> have the ability to do Auto Scaling. If you need to scale your streams, it is on you to build it into your solution.</p>
<p>To facilitate the development, management, and usage of Kinesis Data Streams, AWS provides APIs, the AWS SDKs, the AWS CLI, the Kinesis Agent for Linux, and the Kinesis Agent for Windows.</p>
<p><strong>Producers</strong> put Data Records into a Data Stream. </p>
<p>Kinesis Producers can be created using the <strong>AWS SDKs</strong>, the <strong>Kinesis Agent</strong>, the <strong>Kinesis APIs</strong>, or the <strong>Kinesis Producer Library, KPL</strong>.</p>
<p>Originally, the Kinesis Agent was only for Linux. However, AWS has released the Kinesis Agent for Windows.</p>
<p>A <strong>Kinesis Data Stream</strong> is a set of <strong>Shards</strong>. A shard contains a sequence of <strong>Data Records</strong>. Data Records are composed of a <strong>Sequence Number</strong>, a <strong>Partition Key</strong>, and a <strong>Data Blob</strong>, and they are stored as an immutable sequence of bytes.</p>
<p>In Amazon Kinesis, Kinesis Data Streams is a stream storage layer. </p>
<p>Data Records in a Kinesis Data Stream are immutable–they cannot be updated or deleted–and are available in the stream for a finite amount of time ranging between 24 hours and, as of November 2020, 8,760 hours. This translates to 365 days. </p>
<p>Originally, the default expiration was 24 hours and this default could be extended up to 7 days for an additional charge. </p>
<p>Data Records stored beyond 24 hours and up to 7 days is billed at an additional rate for each shard hour. With the November 2020 update, now after 7 days, data is billed per gigabyte per month.</p>
<p>The retention period is configured when creating a stream and can be updated using the IncreaseStreamRetentionPeriod() and DecreaseStreamRetentionPeriod() API calls.</p>
<p>There is also a charge for retrieving data older than 7 days from a Kinesis Data Stream using the GetRecords() API call.</p>
<p>There is no charge for long-term data retrieval when using the Enhanced Fanout Consumer using the SubscribeToShard() API.</p>
<p><strong>Consumers</strong>–Amazon Kinesis Data Streams Applications–get records from Kinesis Data Streams and process them. Custom applications can be created using the <strong>AWS SDKs</strong>, the <strong>Kinesis API</strong> or, the <strong>KCL,</strong> the <strong>Kinesis Client Library</strong>.</p>
<p>There are two types of consumers that can get data from a Kinesis Data Stream. The Classic Consumer will Pull data from the Stream. I have seen this also described as a Polling mechanism. </p>
<p>There’s a limit to the number of times and the amount of data consumers can pull out of a shard every second. Adding a consumer application to a shard results in having to divide the available throughput between them.</p>
<p>As of version 2 of the KCL, there’s now a Push method called <strong>Enhanced Fan Out</strong>. With Enhanced Fan Out, consumers can subscribe to a shard. This results in data being pushed automatically from the shard into a consumer application. Because consumers are not polling the shard for data, the shared limits are removed and every consumer gets 2 megabytes per second of provisioned throughput per shard.</p>
<p>It’s time to talk a little about Firehose.</p>
<p><strong>Amazon Kinesis Data Firehose</strong> is a data streaming service from AWS like Kinesis Data Streams. However, while Kinesis Data Streams is highly-customizable, Data Firehose, being fully managed, is really a streaming delivery service for data. </p>
<p>Ingested data can be dynamically transformed, scaled automatically, and is automatically delivered to a data store. </p>
<p>In my courseware, I prefer to state what things are instead of saying what they’re not. To me, it’s important to think of what is possible with the tool (or tools) I’m using.  </p>
<p>However, sometimes, I need to explain what something isn’t. In this case, Kinesis Data Firehose is <strong>not</strong> a streaming storage layer in the way that Kinesis Data Streams is. </p>
<p>Kinesis Data Firehose uses Producers to load data into streams in batches and, once inside the stream, the data is delivered to a data store. There is no need to develop Consumer applications and have custom code process data in the Data Firehose stream. </p>
<p>Unlike Kinesis Data Streams, Amazon Kinesis Data Firehose buffers incoming streaming data before delivering it to its destination. The buffer size and buffer interval is chosen when creating a delivery stream. </p>
<p>The buffer size is in megabytes and has different ranges depending on the destination. The buffer interval can range from 60 seconds to 900 seconds. </p>
<p>Essentially, data buffers inside the stream and will leave the buffer when it is either full or when the buffer interval expires. For this reason, Kinesis Data Firehose is considered a near real-time streaming solution.</p>
<p>Originally, Kinesis Data Firehose could deliver data to four data stores; <strong>Amazon S3</strong>, <strong>Amazon Redshift</strong>, <strong>Amazon Elasticsearch</strong>, or <strong>Splunk</strong>. </p>
<p>In 2020, this was expanded to include <strong>generic HTTP endpoints</strong> as well as HTTP endpoints for the 3rd-party providers <strong>Datadog</strong>, <strong>MongoDB Cloud</strong>, and <strong>New Relic</strong>.</p>
<p>Another difference between Kinesis Data Streams and Kinesis Data Firehose is that Kinesis Data Firehose will automatically scale as needed. </p>
<p>Kinesis Data Firehose can convert the format of your input data from JSON to <strong>Apache Parquet</strong> or <strong>Apache ORC</strong> before storing the data in Amazon S3. </p>
<p>Parquet and ORC are columnar data formats that save space and enable faster queries compared to row-oriented formats like JSON.</p>
<p>Kinesis Data Firehose can also invoke Lambda functions to transform incoming source-data and deliver the transformed data to its destination. </p>
<p>For example, if data is a format other than JSON–such as comma-separated values–use AWS Lambda to transform it to JSON first.</p>
<p>There is no free tier for using Kinesis Data Firehose. However, costs are only incurred when data is inside a Firehose stream. There is no bill for provisioned capacity, only used capacity.</p>
<p>The fourth, and final, primary service inside Amazon Kinesis is Data Analytics.</p>
<p><strong>Kinesis Data Analytics</strong> has the ability to read from the stream in real time and do aggregation and analysis on data while it is in motion. </p>
<p>It does this by leveraging SQL queries or with Apache Flink using Java or Scala to perform time-series analytics, feed real-time dashboards, and create real-time metrics.</p>
<p>When using Kinesis Data Firehose with Kinesis Data Analytics, data records can only be queried using SQL.</p>
<p>Apache Flink with Java and Scala apps are only available for Kinesis Data Streams. </p>
<p>Kinesis Data Analytics has built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at scale.</p>
<p>Use cases include <strong>ETL</strong>, the <strong>generation of continuous metrics</strong>, and <strong>doing responsive real-time analytics</strong>.</p>
<p>If you’re new to ETL, it stands for <strong>Extract</strong>, <strong>Transform</strong>, <strong>Load</strong> One of the primary purposes of ETL is to enrich, organize, and transform data to match the schema of a Data Lake or a Data Warehouse. </p>
<p><strong>Continuous metric generation</strong> applications monitor and report how <strong>data is trending over time</strong>. </p>
<p><strong>Real-time analytics</strong> applications <strong>trigger alarms</strong> or <strong>send notifications</strong> when certain metrics reach predefined thresholds, or–in more advanced cases–when an application <strong>detects</strong> <strong>anomalies</strong> using machine learning algorithms.</p>
<p>I’m going to conclude this overview with some pricing considerations.</p>
<p>Please be aware that there is no free tier with Amazon Kinesis. </p>
<p><strong>Kinesis Video Streams</strong> pricing is based on the volume of data ingested, the volume of data consumed, and data stored across all the video streams in an account.</p>
<p><strong>Kinesis Data Streams</strong> pricing is a little more complicated. There is an hourly cost based on the number of shards in a Kinesis Data Stream. </p>
<p>This charge is incurred whether or not data is actually in the stream. There is a separate charge when producers put data into the stream.</p>
<p>When the optional <strong>extended data retention</strong> is enabled, there’s an hourly charge per shard for data stored in a stream.</p>
<p>For <strong>consumers</strong>, charges are dependent on whether or not <strong>Enhanced Fan Out</strong> is being used. If it is, charges are based on the amount of data and the number of consumers.</p>
<p><strong>Firehose</strong> charges are based on the amount of data put into a delivery stream, for the amount of data converted by Data Firehose, and, if data is sent to a VPC, the amount of data delivered as well as an hourly charge per Availability Zone. </p>
<p><strong>Amazon Kinesis Data Analytics</strong> changes an hourly rate based on the number of Amazon Kinesis Processing Units–or KPUs–used to run a streaming application. </p>
<p>A KPU is a unit of stream processing capacity. It consists of 1 virtual CPU and 4 gigabytes of memory.</p>
<p>This brings me to the end of this lecture. Thank you for watching and letting me be part of your cloud journey. If you have any feedback, positive or negative, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your input on our content is greatly appreciated. I’m Stephen Cole for Cloud Academy. Thank you for watching.</p>
<h1 id="A-Streaming-Framework"><a href="#A-Streaming-Framework" class="headerlink" title="A Streaming Framework"></a>A Streaming Framework</h1><p>I want to take a few moments to talk about Amazon Kinesis as a streaming framework. That is, Amazon Kinesis, and its features, are really a collection of parts that work together to process data in real time or near real time.</p>
<p>First, a reminder of why streaming data exists. There are a number of common use cases for streaming data. They include industrial automation, smart cities, smart homes, data lakes, log analytics, and IoT analytics.</p>
<p>Two of the most popular use cases are log analytics feeding into data lakes and IoT analytics. IoT is a broad category of devices. Think of IoT devices as simply a connected device like a phone, tablet, or smart speaker. These are connected devices that are almost always sending data.</p>
<p>Events can be things such as search results, financial transactions, user activity, telemetry data from IoT devices, log files, and application metrics.</p>
<p>While in the stream, data is processed dynamically while it is in motion. This processing can be real-time analytics with machine learning, alerts, or the triggering of one or more actions.</p>
<p>A point that I think must be made here is that, while in a stream, data can be processed but it can <strong>not</strong> be changed. Data records are immutable. If information in a stream needs to be updated, another record is added. </p>
<p>Consumers are connected to the stream and can aggregate the incoming data, send alerts, and create new data streams that can be processed by other consumers.</p>
<p>A stream-based architecture that matches the flow of data has several advantages over batch-based processing.</p>
<p>One of these advantages is that it has low latency. Streaming systems can process events and react to them in real-time. </p>
<p>Another advantage of stream processing is that streams can be architected to reflect how people use applications. This means streams match real-world processes. </p>
<p>Put differently, stream processing matches how people interact with the data that surrounds them. Applications that have a never-ending flow of events are ideal for stream processing.</p>
<p>Recall that, with batch systems, data has to accumulate before processing can start. </p>
<p>When using stream processing, computation occurs as soon as the data arrives.</p>
<p>Data streaming can enable you to ingest, process, and analyze high volumes of high-velocity data from a variety of sources in real time.</p>
<p>In general, there are five layers of real-time data streaming. They are the <strong>source layer</strong>, the <strong>stream ingestion layer</strong>, the <strong>stream storage layer</strong>, and the <strong>stream processing layer</strong>. </p>
<p>The <strong>source layer</strong> is where the data originates. This could be something like data coming from IoT sensors, click-stream data from mobile devices and websites, or application logs.  </p>
<p>The <strong>steam ingestion layer</strong> is a <strong>Producer</strong> application tier that collects the source data, formats it appropriately, and publishes <strong>Data Records</strong> to the <strong>stream storage layer</strong>.   </p>
<p>The stream storage layer acts as a high-speed buffer for data. The <strong>stream processing layer</strong> accesses the stream storage layer using one or more applications called <strong>Consumers</strong>. </p>
<p><strong>Consumers</strong> read and process the streaming data in near-real time. This processing could include ETL–Extract, Transform, Load–operations, data aggregation, anomaly detection, or analysis. </p>
<p>The Consumers deliver Data Records to the fifth layer, the <strong>destination</strong>. This could be storage such as a Data Lake or Data Warehouse, durable storage such as Amazon S3, or some type of database.</p>
<p>Clickstream analytics can act as a recommendation engine providing actionable insights used to create personalized coupons &amp; discounts, customize search results, and guide targeted advertisements — all of which help retailers enhance the online shopping experience, increase sales, and improve conversion rates.</p>
<p>As a quick aside, if you’re new working with salespeople, the term conversion was new to me when I started working in the cloud. Instead of data formats, it means converting prospective customers to paying customers. You might hear the term related to eyeballs. That is, companies want to convert eyeballs–people looking at products–to customers that return to do more business.</p>
<p>Moving back to use cases, streaming efforts related to preventive maintenance allows equipment manufacturers and service providers to monitor quality of service, detect problems early, notify support teams, and prevent outages.</p>
<p>Streaming data can be used to alert banks and service providers to make them aware of suspected fraud in time to stop bogus transactions and quickly notify them of affected accounts.</p>
<p>Streaming data with sentiment analytics can detect unhappy users and help customer service augment a response and prevent escalations before that unhappiness turns into anger.</p>
<p>Using streaming data with a dynamic pricing engine can automatically adjust the price of a product based on factors such as current customer demand, product availability, and competitive prices in the area</p>
<p>Because of its complexity, the creation of data streaming workflows has a number of challenges. Historically, streaming applications have been “high-touch” systems that have a large amount of human-interaction that make them inconsistent and difficult to automate.</p>
<p>Data streaming applications can be difficult to set up. Streaming applications have a number of “moving parts” that tend to be brittle. </p>
<p>The source layer has to be able to communicate with the ingestion layer. The ingestion layer must be able to put data into the stream storage layer. </p>
<p>Consumer applications process the data in the stream-storage layer and either put it into a new stream or send it on to its final destination.</p>
<p>It’s expensive to create, maintain, and scale streaming solutions built in on-premises data centers in terms of both human and compute costs.</p>
<p>Issues around creating streaming applications continue with scaling operations. IoT sensor data might be seasonal like monitoring airspeed during hurricane season. It’s important to be able to increase and decrease the number of resources required to store and consume the collected data. </p>
<p>To address the challenges of creating custom streaming frameworks and applications to stream data into the AWS cloud, AWS introduced Amazon Kinesis.</p>
<p>When developing Amazon Kinesis, AWS engineers recognized that high availability and durability were a necessary part of the service and it was built to minimize the chance of data loss.</p>
<p>As a managed service, AWS provisions the compute, storage, and memory resources automatically upon request. Streaming applications use APIs to publish and consume data to and from Amazon Kinesis.</p>
<p>Kinesis is fully scalable and elastic. That is, it can grow to meet a workload’s needs and it can shrink to prevent wasting resources that, in turn, waste money.</p>
<p>Amazon Kinesis integrates with a variety of AWS services. A benefit of this is that it is possible to create workflows with little or no code that do steam processing at scale.  </p>
<p>This brings me to the end of this lecture. Thank you for watching and letting me be part of your cloud journey.</p>
<p>What I hope you got out of this lecture is that streaming is not a thing by itself. It is a collection of systems that work together to process data in real time or near real time. </p>
<p>Having a fully managed framework from AWS means that most of the work required to create a streaming data system has been done in advance. </p>
<p>Instead of worrying about streaming infrastructure, you can focus on what sort of insights and analysis that needs to be done to improve your business or organization.</p>
<p>If you have any feedback, positive or negative, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated.</p>
<p>I’m Stephen Cole for Cloud Academy. Thank you for watching!</p>
<h1 id="Introduction-to-EMR"><a href="#Introduction-to-EMR" class="headerlink" title="Introduction to EMR"></a>Introduction to EMR</h1><p>Hello and welcome to this lecture covering <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/intro-amazon-elastic-map-reduce-emr-1115/course-introduction/">Elastic MapReduce</a>, known as EMR.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> Elastic MapReduce is a managed service designed to process and analyze vast amounts of data through the use of jobs that can be short running with per second costs, or for long-running workloads allowing you to build in high availability into your architecture.</p>
<p>EMR is based on the popular and solid Apache Hadoop framework, an open-source distributed processing framework intended for big data processing. Organizations and companies can gain great benefit in using Amazon EMR because it abstracts and reduces the complexity of the infrastructure layer used with traditional MapReduce frameworks.</p>
<p>The efforts involved in implementing a healthy Hadoop cluster setup are not so trivial. So what AWS did was to encapsulate all the infrastructure of the Hadoop framework into an integrated environment so you can launch a cluster in minutes and focus on the real important part, which is not managing infrastructure but getting your data processed according to your needs.</p>
<p>Amazon EMR securely and reliably handles your data analytic use cases, including log analysis, web indexing, data warehousing, machine learning, financial analysis, scientific simulation, and bioinformatics. Amazon EMR takes advantage of Amazon EC2 instances that are configured with the Hadoop framework to deliver petabyte-scale processing power. </p>
<p>Amazon EMR supports a number of other frameworks used within the field of big data and data analytics, these include Spark, Presto, and HBase. Using the AWS Management Console or AWS CLI, you can quickly and easily create clusters for each of these frameworks. </p>
<h1 id="What-is-Amazon-MSK"><a href="#What-is-Amazon-MSK" class="headerlink" title="What is Amazon MSK?"></a>What is Amazon MSK?</h1><p>Amazon Managed Streaming for Apache Kafka or Amazon MSK allows you to run applications that utilize Apache Kafka within AWS. Kafka provides a platform for stream processing and operates as a publisher&#x2F;subscriber based durable messaging system. Its key features are the ability to intake data with extreme fault tolerance, allowing for continuous streams of these records that preserve the integrity of the data, including the order in which it was received.</p>
<p>Apache Kafka then acts as a buffer between these data producing entities and the customers that are subscribed to it. Subscribers receive information from Kafka topics on a first in, first out basis or FIFO, allowing the subscriber to have a correct timeline of the data that was produced.</p>
<p>Kafka is an open-source technology that allows for a large number of community-driven tools and add-ons. This makes it very customizable and gives developers the freedom to build and create what they need. Using Amazon MSK, you can use the native Apache Kafka APIs to create datalakes, stream information to multiple sources, and power data analytic applications and pipelines, again all within AWS.</p>
<p>The main reason you’d wanna use Amazon MSK over rolling your own implementation of Kafka is that Amazon MSK is a fully managed service. This means you don’t need to take care of any servers, you don’t need to worry about any upgrades and you also don’t need to bother with handling Apache Zookeeper, a required coordinating software that deals with orchestrating the cluster task and maintaining the state of the cluster in general.</p>
<h1 id="Amazon-MSK-and-Kafka-Under-the-Hood"><a href="#Amazon-MSK-and-Kafka-Under-the-Hood" class="headerlink" title="Amazon MSK and Kafka Under the Hood"></a>Amazon MSK and Kafka Under the Hood</h1><p>Everything you need to know about Kafka boils down to three main ideas. You have producers who create data, such as a website gathering user traffic flow information, you have topics which received the data, this information is stored with extreme fault tolerance and you have consumers which can read that data in order and know that it was never changed or modified along the way.</p>
<p>Kafka is often used as a decoupling mechanism to help relieve tension among many different producers and consumers. For instance, you might have 10 websites, all creating log information that needs to be processed.</p>
<p>Let’s say that you also have 20 microservices that each try to filter out and make predictions for various specific variables of that data. If you were to hard code all this information, you would have 200 separate connections that you need to worry about.</p>
<p>By using Kafka as an intermediary, all of that log information can be pushed into a single topic. This one topic is now the single source of truth for all of your microservices. They can each read through and gather the information they require on demand. This topic will hold the producers information until the retention period has been met. This window is configurable and has a default time of seven days.</p>
<p>Kafka also has a size-based retention policy where you configure the maximum amount of data that can be stored. Once the max amount of data has been reached, Kafka will start kicking out and removing old information. Both of these options can be configured on a per topic basis, which provides a lot of flexibility in keeping data costs down or to retain high value information for longer.</p>
<p>Each topic has a number of partitions where the data will be randomly written unless a partition key is provided. Once data has been written to a topic, it can never be changed. You can provide an update to that data, but it would just be the next entry in the partition instead of overriding the original data. The more partitions you have for a topic, the more parallelism you can have.</p>
<h1 id="Provisioning-an-Amazon-MSK-Cluster"><a href="#Provisioning-an-Amazon-MSK-Cluster" class="headerlink" title="Provisioning an Amazon MSK Cluster"></a>Provisioning an Amazon MSK Cluster</h1><p>There are two elements you need to provision when creating your Amazon MSK clusters: Broker instances and Broker storage. A Broker instance is a worker node that helps to manage the Kafka cluster. Your clusters can have multiple brokers, but can also operate as a single node, if necessary. Broker instances can be run within the same availability zone or across many availability zones to create a highly available cluster, something that many architectures will require.</p>
<p>One of the large benefits of using Amazon MSK over a roll-your-own version of Kafka is that Amazon will keep an eye on these broker nodes and replace them if they become unhealthy. Otherwise, you would have to manage all of that yourself. Broker storage is where all of your data will be kept that comes streaming into Amazon MSK.</p>
<p>Within Amazon, this storage is housed within EBS volumes, and gains all the protections that EBS provides, like durability and fault tolerance. You can also encrypt these data volumes using Amazon EBS Server-side encryption and AWS KMS, the Key Management Service. An interesting note about storage however, is that once you have assigned your Broker storage, you can only increase the amount of storage, it can not be decreased. The broker’s storage can be autoscaled upwards if you do reach your maximum capacity.</p>
<p>If you already have a Kafka cluster that you are managing yourself, either on-premises or within the cloud, you can migrate over to Amazon MSK. There are many third-party and open-source tools like MirrorMaker which can help replicate the data from your Kafka cluster into Amazon MSK.</p>
<h1 id="Amazon-Device-Farm-Service"><a href="#Amazon-Device-Farm-Service" class="headerlink" title="Amazon Device Farm Service"></a>Amazon Device Farm Service</h1><p>In this next section, we’ll describe the Amazon Device Farm Service and talk about how and why we should use it.</p>
<p>What is Amazon Device Farm? By definition, AWS Device Farm is an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real time. View video, screenshots, logs, and performance data to pinpoint and fix issues before shipping your app. So what does that mean?</p>
<p>Device Farm can replace that collection of devices that application developers and companies must purchase and maintain for app testing. It can be used for all stages of testing applications whether iterative changes or cumulative and there are hundreds of both iOS and Android devices to choose from. It can also be used to reproduce any reported issues about application performance. Device Farm can also be used through a remote access session which allows developers and testers to interact with a device through a browser, emulating the same experience they would get as they were testing the physical device.</p>
<p>With so many variables like the wide variety of phones on the market, a multitude of tablet options, different operating systems and OS versions, carrier and manufacturer modifications to firmware, along with environmental factors such as signal strength, it’s no wonder that developers struggle with building quality into their mobile applications. Purchasing the hardware for testing is expensive. Plus, it must be maintained and updated and technology changes so quickly that old devices must be replaced with new ones to keep up with the latest innovations. On top of that, the mobile application market is highly competitive. Users discard a malfunctioning app for a functioning one and a customer is lost. On top of losing a customer, poor ratings can affect the ranking of an application and make discoverability more difficult.</p>
<p>Now let’s take a look at some important features of Amazon Device Farm. The Device Farm service enables you to test applications on real mobile devices, not emulators. Amazon provides a wide variety of Android and iOS phones and tablets to choose from.</p>
<p>Device Farm offers flexible testing methods. You can perform automated tests using the built-in tests or custom scripts or use compatible open source frameworks like Appium, Calabash, Espresso, and more. The automated testing feature allows you to run many tests across many devices in parallel with results returned in minutes. You can also test manually with the remote access option. During a remote access session, you can interact with a device as if it was on your desk, but you use your browser instead. You can rotate the device, install applications, swipe and gesture, as well as other interactions.</p>
<p>In Device Farm, devices are secured on both the hardware and the software side. The physical devices in the farm cannot take video or photos of other devices in the farm. They can’t connect or communicate with each other over a WiFi connection. Each has a dedicated WiFi connection. Devices do not share any kind of network infrastructure, LAN, WAN, or internet. In a standard testing environment, the mobile device is usually connected to a host machine like a laptop or a server, and then tests are run against the device from the host machine. Device Farm offers a similar configuration. You have dedicated devices for your session. Once you have completed your session, the host is wiped clean or destroyed to ensure that you have a clean environment that is not affected by another customer’s test or even yours from a previous session.</p>
<p>Amazon provides non-rooted iOS and Android devices on which you can install any appropriate applications necessary. You can choose different languages and locations and other features to simulate the real world environment that your users experience when they use the application.</p>
<p>You can also catch potential issues during the development process. Device Farm includes service plugins and APIs that can be configured to automatically initiate tests from IDEs and continuous integration systems like Jenkins, Android Studio, and more.</p>
<p>Device Farm also provides reports containing pass&#x2F;fail information, crash reports, both the test and device logs, and they can contain screenshots based on when you want to take those screenshots. You can download testing logs and performance data. The reports provide detailed data per device and also high-level information like how many occurrences there were of a specific problem.</p>
<h1 id="Amazon-Pinpoint-Service"><a href="#Amazon-Pinpoint-Service" class="headerlink" title="Amazon Pinpoint Service"></a>Amazon Pinpoint Service</h1><p>In this section, we’ll describe the Amazon Pinpoint Service and talk about how and why we should use it.</p>
<p>So what is Amazon Pinpoint? By definition, Amazon Pinpoint makes it easy to run targeted campaigns to drive user engagement in mobile apps. Amazon Pinpoint helps you understand user behavior, define which users to target, determine which messages to send, schedule the best time to deliver the messages, and then track the results of your campaign.</p>
<p>Mobile application developers want their app to be successful. Marketers want to be able to send the right data to the right customers at the right time. Applications can send data to Amazon Pinpoint about app usage and user habits, when they are using the app, how long they’re engaged, how often do they use the app or anything else they want to know. Marketing professionals can use Amazon Pinpoint Console to define campaign specifics and see data metrics, charts, and graphs to help them determine the effectiveness of campaigns. It allows them to create targeted messages that are more likely to interest the users, and less likely to cause them to unsubscribe to notifications, leading to more effective engagement and increased purchasing.</p>
<p>An application developer can build a rich and exciting app, and get it onto the market quite easily, but once it’s out there, how do you build interest? Users may open the application once, and then never use it again. They may use it infrequently, or just the free version. Users may decide that your notifications don’t interest them, and opt out of any additional communications regarding the application. Amazon Pinpoint can help developers address these concerns proactively. Amazon provides SDKs that allow you to integrate your app with Amazon Pinpoint. Once the SDK is integrated, you have full API and CLI support, and can then instrument your app to provide information to Amazon Pinpoint, allowing the developer or marketing professionals to access the data collected, and develop a marketing strategy to improve the performance of the application in the market.</p>
<p>Marketing professionals face the daunting task of sifting through massive amounts of data and then correctly determining what is valuable and what is not. They need to figure out what’s working and how to make improvements. They have to decide how to frame messages, when to send them and determine if the message is effective. Marketers need to know if the app needs more features, or if the application interface needs some tweaking. The Amazon Pinpoint console creates a space where marketers have a rich graphic interface that allows them to analyze data, build meaningful notifications, segment users, and conduct testing to verify effectiveness.</p>
<p>Now, let’s take a look at some of the important features of the Amazon Pinpoint service. Integrating the AWS Mobile SDK into your mobile applications enables Pinpoint to collect standard usage and device attributes, as well as custom attributes defined by you, so any settings specific to your app or specific to the users using your application can be created to collect more relevant data about user and app usage. These attributes can then be used to create targeted push notifications. Amazon Pinpoint also provides graphical dashboards for viewing its real-time analytics including information gathered about user acquisition, engagement, and demographics, along with custom events, and campaign funnels. The dashboards make it easier to see and understand how users are using your applications. As discussed, Pinpoint collects data that you can use to determine key target segments. It also enables you to do this using data collected from other AWS service, like S3, or Redshift. You can also import data in CSV and JSON formats into Amazon S3, then into Pinpoint from S3. Whether you’re collecting data via Amazon Pinpoint, or importing data from third-party sources, or both, Pinpoint gives you all the tools you need to build effective segments for targeted notifications. Pinpoint also includes templates for standard and silent notifications. You can personalize messages with your custom attributes, like user game level, ranking, or location. Campaign scheduling is another useful feature. It offers flexibility by allowing you to send notifications immediately, or schedule them to occur at a certain time, either once, hourly, daily, weekly, or monthly, and then choose to deliver them based on a user’s time zone. You can even set quiet hours, and limit how often a message can be sent to specific users. To determine your campaign’s performance, Amazon Pinpoint includes campaign metrics in the dashboard, such as number of users, notifications delivered, open rate, which means how often the app is opened as a result of the campaign, and the opt-out rate for the push notification. If you have a third-party analytics system, you can also export the data collected from Pinpoint and then import it for analysis using your existing analytics system. With the A&#x2F;B testing feature, you can send different messages and use different campaign schedules for subsets of your users, and then analyze the results and use the most successful message for your target segment. We’ve talked about how useful funnel analytics can be for marketing campaigns. Amazon Pinpoint enables you to define campaign funnels by defining up to five steps in each funnel. By defining discrete steps for your campaign, you can collect user participation information that will be displayed in a graph on your funnel dashboard. Once you’ve collected the data, you’ll see how users behave during the campaign, and determine the success of the campaign.</p>
<h1 id="What-is-Amazon-Comprehend"><a href="#What-is-Amazon-Comprehend" class="headerlink" title="What is Amazon Comprehend?"></a>What is Amazon Comprehend?</h1><p>In this lecture, I want to introduce you to Amazon Comprehend. And for many, this will likely be a new service that you may not have encountered yet, as it sits outside of what is considered the core of the AWS services. So, what is it exactly? Well, Amazon Comprehend falls under the machine learning category of AWS, and it uses a continuously pre-trained model to identify and extract valuable insights from within the text of documents through the use of natural language processing, known as NLP.</p>
<p>Before I continue, let me just explain what NLP is. As stated on Wikipedia and at a high level, NLP is explained as a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. The goal is a computer capable of understanding the contents of documents, including the contextual nuances of the language within them. So essentially, it’s a great technology and process to understand language and its structure by reading texts at huge scales underpinned by machine learning, which can analyze documents far quicker than us mere mortals can.</p>
<p>So one of the key points of Amazon Comprehend is that it can produce insights for you. And that’s just a different way of saying meaningful data. And this allows you to use this data and knowledge to make changes and adjustments to your business and perhaps capitalize on this valuable data gained. For example, enhancing your customer user experience by detecting customer sentiment, which would allow you to determine what actions could lead to the most positive customer experience and outcomes.</p>
<p>So we know that Comprehend can scan documents at scale and understand the content of them. It can extract data such as key phrases, entities, and sentiment and more using a range of different APIs. So let’s now take a look at this data classification and some of the APIs to see how Comprehend defines these.</p>
<p>Key phrases. A key phrase is a combination of words that contain a noun phrase that describes something. A noun is a word used to identify people, places, or things. The noun phrase will, of course, contain a noun, but it will also include some identifiers about that noun. For example, my new blue car. This is a noun phrase. The noun is car, and new and blue are adjectives, which name attributes of the noun. For every key phrase that is detected by Comprehend, it will issue a score, and this score determines how confident Comprehend is that the string of text being referenced is a noun phrase. This scoring can then be used by your own applications to determine if it is a key phrase that should be considered.</p>
<p>Sentiment. The sentiment relates to the emotional context of a block of text. Amazon Comprehend will try to determine the underlying sentiment. For example, if the document being scanned was positive, negative, neutral, or even mixed, it will generate a percentage score rating for each of these four emotions to determine the overall sentiment of the document. A great use case for sentiment could be to read feedback comments about your products to determine if buyers were generally pleased with the product or not.</p>
<p>Entities. An entity within Comprehend can be described as a reference to a person, a place, an event, a specific date and time, in addition to commercial items and quantities. So as an example, the following text. Stuart, Jorge, and Will visited Las Vegas in December, 2021, to attend the AWS re:Invent conference. So here, Stuart, Jorge, and Will might be referenced as people, Las Vegas would be recognized as a location, December, 2021, might be seen as a date, and re:Invent conference might be considered an event.</p>
<p>Again, each of these classifications will be attributed with a score to determine the confidence of Comprehend’s selection of the text as an entity and its type. Here, you can see a list of all supported entities at the time of writing this course. For the most up-to-date list of entities, please see the following URL. Personally identifiable information, PII. PII data contains anything that could identify you as a person. Sometimes this data is considered private and so being able to identify data such as this can help you identify security risks.</p>
<p>Amazon Comprehend uses a large list of PII entities to help identify this data, as you can see in this table taken from the AWS documentation found here. As I said previously, PII contains sensitive data. As a result, Comprehend can do one of two things when it returns its results. It will either identify the PII information and classify the PII identity type it has found and present that data, or it can redact the PII data that it has found from within the document. So for example, in this text that says, “Dear Stuart, The current balance of your account, an account number, can now be accessed online. A copy of your statement has also been emailed to a 100 Cloud Street, Northampton, United Kingdom.” So in this example, Stuart would have a type of name. 1234567890 would have a type of bank account number. 100 Cloud Street, Northampton, United Kingdom would have a type of address.</p>
<p>Now, if you decided to redact this information using Comprehend, it would return the text as follows, removing the sensitive PII information. Language. Amazon Comprehend has a wide range of different languages that it can understand. And based on the text being analyzed, it can determine which is the most dominant language that the text was written in. Again, a percentage rating is used to determine the confidence level of Comprehend in its understanding of the text.</p>
<p>Syntax. When Amazon Comprehend analyzes your text documents, it passes each and every word in an effort to determine the syntactic function of the word. This allows Comprehend to build up a detailed understanding of the words in the document and their relationship to each other. It does this by classifying each word as a noun, adjective, verb, pronoun, et cetera. For a full list of the different syntax types, of which there are 17, please see the AWS URL here.</p>
<p>Topic modeling. This helps you determine the different common topics or themes that exist amongst a large corpus of text. For example, you could submit a large number of science fiction stories to Comprehend to analyze, and it might return the topics such as time travel, teleportation, telekinesis, aliens, and space travel. Using a specific learning model, Amazon Comprehend is able to detect and analyze every word, its meaning, and its context.</p>
<p>As a result, if Comprehend detects that the same word is consistently used in the same context throughout the text, it will be used to determine a topic. So topic modeling is used to help you organize your documents into different categories.</p>
<p>So in short, Amazon Comprehend is a fully managed and continuously trained NLP service backed by machine learning, which is used to analyze and detect meaningful insights from any text in UTF-8 format, which is an encoding system for Unicode, or in a semi-structured document, such as a Word doc or a PDF file.</p>
<h1 id="Fundamentals-of-Amazon-Forecast"><a href="#Fundamentals-of-Amazon-Forecast" class="headerlink" title="Fundamentals of Amazon Forecast"></a>Fundamentals of Amazon Forecast</h1><p>Amazon Forecast is a fully managed service that automatically uses machine learning to deliver accurate forecast for any time series data sets. A time series data set is a set of data points that are ordered by a unit of time. For example, monthly sales of a product, daily inventory in a warehouse, hourly Internet of Things, sensor readings, or even weekly website traffic. Time series forecasting is a method to predict the future data points in the series based on its historical trend.</p>
<p>Forecasting is important for business results because under-forecasting errors result in missed opportunities. And over-forecasting errors result in wasted resources for your business. This is applicable to multiple use cases like Amazon EC2 instance capacity planning where over-forecasting results in unused infrastructure and under-forecasting results in unmet demand. For the business of demand and inventory planning, over-forecasting results in excess inventory and under-forecasting results in loss sales.</p>
<p>Finally, for workforce planning, over-forecasting results in unused labor and under-forecasting results in overtime costs. Each of these use cases represent a data set domain. A data set domain indicates a predefined data set schema for a common use case. Amazon Forecast includes predefined data set domains for retail, inventory planning, easy to capacity planning, workforce prediction, web traffic forecast, metrics like sales and revenue and a custom category for all other types of time series forecasting.</p>
<p>Amazon forecast delivers the accuracy of machine learning forecasting technology powering amazon.com yet requires no machine learning experience. It draws from 20 years of forecasting at Amazon and packages a suite that include deep learning algorithms and statistical methods. The deep learning algorithms improve accuracy by up to 50% compared to traditional models. You can receive completion time estimates for ongoing data set imports, predicted training jobs and forecast jobs.</p>
<p>Amazon Forecast data import can use three types of datasets. They are the first, the Target Time Series. Which represents the historic time series data of items to forecast. The second, the Related Time Series, which represents related time series data such as price or web hits. Finally, the Item Metadata. Which represents attributes of the items such as category, genre or brand. The time series is the only data set that is required to generate forecast. As simple time series is a collection of item IDs, timestamps and values. Where each item ID and timestamp payer has an associated value. Time series can have multiple timestamps and values for each item ID.</p>
<p>A more complex time series may have additional dimensions where you just don’t have a unique value for each timestamp X item ID combination. But rather a value for each timestamp X item and perhaps an additional dimension combination. In this example of a retail store, a dimension could be the location of a store. While the item IDs remain unique, the timestamps and the values. Amazon Forecast automatically sets up a data pipeline, training and prediction. Making it simple to use following a general three-step sequence. First, you import your data because data sets are required to train predictors, which are used to generate forecasts. Then you train the predictor, which is a custom model with underlying infrastructure that Amazon forecast trains with your datasets. Finally, you generate forecasts by deploying your train predictors and exploring forecast results.</p>
<h1 id="Overview-of-Amazon-Sagemaker"><a href="#Overview-of-Amazon-Sagemaker" class="headerlink" title="Overview of Amazon Sagemaker"></a>Overview of Amazon Sagemaker</h1><p>Before I get away from myself and start to promise that SageMaker will solve all of your life’s problems, let’s just be very clear what SageMaker is. At its core, SageMaker is a fully managed service that provides the tools to build, train and deploy machine learning models. That’s a very concise mission statement for what SageMaker helps you achieve. It has some components in it such as manage notebooks and helping label and train models, but at its core, SageMaker should be thought of as where you go to when you need to build, train and deploy models.</p>
<p>Now, to look at how it fits into the larger Amazon stack, we have this diagram. At the top of this diagram or at the top of the stack, if you wanna be a little more formal, you have some of the application services. These are tools such as Rekognition, Transcribe, Lex, Translate and Comprehend, that are really full service options for developing a very focused machine learning model. They basically provide a heavily pre-trained model that allows you to then sometimes tweak it and fit in some custom identifiers.</p>
<p>If you’ve been attending the machine learning content course in Cloud Academy, you’ll recognize that these application services are more serviced by what we call a level one machine learning user. If you haven’t attended that class series yet, I strongly advise checking it out. I also taught that one and in that we really dive into how to actually interact with some of these programs and start to build your own models, whereas this class more focuses on SageMaker as a platform.</p>
<p>So at the top level, just to reiterate, you have some of the straightforward pre-trained machine learning models that maybe allow some tweaking and adjusting. The next layer below application services is platform services. Whereas application services are like going to a bakery and buying a cookie, platform services are more like buying a cake mix with a recipe on the side. It gives you the tools to make your machine learning product, but you need to bring your expertise on how to assemble and how to fine tune it to get the best result.</p>
<p>In this layer is where SageMaker lives along with some other very popular services, such as Spark, EMR, perhaps you’ve heard of Databricks or DataRobot. Those all live in the platform services range. Although sometimes with some of the more advanced features, they can drift towards application services.</p>
<p>Once again, to tie this into the machine learning content path, platform services are really what everyone from level two upward would use. Once you start to dabble and making your own models and prepping your own data, you start to really need a platform to get started.</p>
<p>Now below both applications and platforms are frameworks and hardware. Many of the platforms allow you to use these frameworks in order to improve your machine learning product without having to start coding from the ground up. Things such as TensorFlow, MXNet and Pytorch come with the code four models in them, and you have to start to feed it hyper parameters in data. So it’s not like you’re starting from the mathematical thesis, you’re able to import libraries. Cloud Academy actually has a lot of content on how to use these specific frameworks. So feel free to check that out as well after this class. And also down here are things such as your compute options.</p>
<p>SageMaker and Amazon in general does a really good job at tying together their entire ecosystem. So if you wanna be able to pull in things such as graphical processing units, that’s a p3 instances if you wanna talk about specific servers or attach extra CPU or memory, Amazon allows you to add hardware in order to help accelerate your solution and change its performance characteristics as you become a more advanced user. </p>
<p>So this is really the stack of how you should think about the machine learning ecosystem on Amazon. It actually really applies to all of the clouds, but the products slotted in here are more AWS specific.</p>
<p>Now, to really understand where SageMaker as a platform sits, let’s discuss the machine learning workflow. In reality, this is all machine learning is, is stepping through these four steps. Now, for those of you who are familiar with the process or data scientists, you might realize that it’s highly reductionist of me to simplify the entire workflow, but as a thought exercise, it really… we should think about it in kind of four steps. </p>
<p>So as many of you probably have realized, the first step in making a machine learning model is preparing your data. This broad task includes everything a data engineer would do such as collecting the data, cleaning it, making it accessible, annotating and transforming it. This step is kind of paradoxical in that it is both the most approachable by people, relatively new to machine learning, but it’s also the hardest. Whereas it’s easy to get started and starting to manually clean your data, but learning how to do this efficiently, reliably and consistently is actually one of the greatest challenges in machine learning.</p>
<p>Many people say that they spend 60 to 80% of their workload in the preparing the data step. So don’t skip over this thinking that it’s just a hurdle on the way to machine learning. Doing a really good job of preparing data and being able to get it together and accessible will make everything else go earlier. You can’t build a good house on a shaky foundation. So once you’ve laid your foundation, aka prepared your data, you can start to build your model. And as you’ve probably seen, there are dozens of different models out there.</p>
<p>We’ve actually done a short overview of these in some of our other classwork. Cloud Academy has some deep dives on other ones and really what model you use and which framework you pulled in, is where an experienced data scientist starts to shine. So SageMaker will help you build your model by making the frameworks available and helping you pick the right one.</p>
<p>So once you’ve built the model or picked it from a framework, you need to start to manage its training, to determine things such as it’s confusion matrix, it’s false positive, it’s false negative rate and generally understand how it runs. So in order to do this, you need to set up different training runs, different evaluation conditions, maybe try running it on different hardware. This is the stage in which you actually need to understand how your model will work in the real world and if you’ve made the right choices.</p>
<p>And finally, as if all of this wasn’t enough, building a model, prepping the data, training and tuning it, many times, one of the hardest parts and obviously the ongoing part is deploying the model into a production environment and continuing to monitor it. Many times, data will drift, maybe the underlying paradigm that the machine learning model governs changes, and you need to be aware of the models performance changes over time and the entire life cycle management. That’s a phrase you’ve probably heard before, it’s worth remembering.</p>
<p>You need to manage the models life cycle. This could be everything as we mentioned, from detecting drift, but lifecycle management also refers to making sure the underlying hardware scales up and scales down in order to handle loads. For those of you interested in this section, this starts to really go towards DevOps and beyond machine learning. So this is a good point where you might need to tap some of your DevOps talent or check out how to size underlying machine learning hardware on Amazon.</p>
<h1 id="Architecture-Basics"><a href="#Architecture-Basics" class="headerlink" title="Architecture Basics"></a>Architecture Basics</h1><p>Okay, let’s get started with some architecture basics. Now you need to be familiar with a number of technology stacks that are common to multi-tier solution designed for the associate certification. LAMP, MEAN, Serverless, and microservices are the most relevant patterns to know for the exam. So LAMP is a common tool stack used for building web services. And the acronym is made up of the first letters of the four components of that solution stack. So that’s Linux operating system, the Apache web server, MySQL as a database, and the PHP programming language. MEAN is another evolving stack pattern, and that comprises Mongo DB as a database layer, ExpressJS, Angula JS, or NodeJS, which are JavaScript frameworks.</p>
<p>Then we have our serverless architecture patterns. And serverless functions can be very beneficial in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-3-when-to-use-multi-tier/">multi-tiered designs</a>. Generally using Amazon API Gateway and AWS Lambda to replace our logic tier. Now in microservice design, it’s not so tied to the notion of tiers. So with microservice architecture patterns, each of the application components is decoupled and independently deployed and operated. However, we still often have tiers to consider with microservice architecture, especially when refactoring monolithic applications to run on microservices. But firstly, what do we mean by multi-tier architecture? Now, a business application generally needs three things. It needs something to interact with users, which often is called the presentation layer. It needs something to process those interactions. And that’s often called our logic layer, or application layer, and it generally needs some way to store the data from that logic in that interaction. And that’s commonly called our data tier. So these three services, the presentation, logic, and data are quite specialized. So they tend to be services or applications that just do those specialist things. Now the word tier means layers, and specifically, one of several layers or levels. A multi-tier architecture pattern means that these services, presentation, logic, and data are separated into tiers or levels. Now the concept of layers is a little misleading, as your technology tiers do not always sit in a hierarchy. They are not necessarily logical layers.</p>
<p>They tend to look like layers in architecture diagrams, a design doesn’t have to have three tiers, they can be more or they can be less. It’s common for web applications to have two tiers, for example. And there is no rule of thumb for how many tiers you should have in an architecture design. Multi-tier architecture provides a general framework to ensure decoupled and independently scalable application components can be developed, managed, and maintained separately. So layers can be scaled independently, and often can be managed by entirely different teams. But keep in mind that the actual services in these tiers can exist in parallel, and often in different availability zones or regions. Okay, let’s make sure that we’re clear on the concept of availability zones. And availability zones are distinct data centers located in different geographical locations, on distinct floodplains, and with independent power supplies, et cetera. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> makes it possible to make our multi-tier designs highly available by distributing our application tiers across multiple availability zones. So what that means, is that each of your tiers, can be set up to run on up to four distinct availability zones. Multi-tiers, running across multiple availability zones, provides us with a highly available, scalable, resilient architecture. Now the first point to note here is that this highly available <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-introduction/">multi-tier architecture</a> is quite complex. It is designed to be highly resilient.</p>
<h1 id="What-is-Multi-Tier-Design-and-When-Should-We-Use-it"><a href="#What-is-Multi-Tier-Design-and-When-Should-We-Use-it" class="headerlink" title="What is Multi-Tier Design and When Should We Use it?"></a>What is Multi-Tier Design and When Should We Use it?</h1><p>So how do you define what’s the right tiering when you’re designing in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>? Now to make this make sense, I want you to think of two cakes, right? One cake is a single layer birthday cake. It’s an average-size cake for an average-size birthday party. The cake icing, however, is divided into thirds.</p>
<p>So one third of the cake’s icing is red. One third is white, and the last third is blue so it looks a bit like a flat beach ball. Our other cake is a pretty big wedding cake. So it’s designed for 50 plus guests, and it has three separate layers. So each layer of the wedding cake has a different colored icing on it. The top layer is iced with red icing. The second layer has white icing and our bottom layer has the blue icing. Now each of our layers are separated by little strats or holders so that each layer is pretty much separate or a cake into itself. So keep clear in your mind that our single layer birthday cake has all three icing colors on it, on that one cake, and our wedding cake on the other hand has a separate layer for each icing color. Now it turns out that the red icing is really popular. </p>
<p>The third of the birthday cake with the red icing on it at your birthday party all gets eaten in the first hour. Now at your wedding, the red icing is super popular as well, but luckily there is a whole layer of red icing. So when the red icing layer is all gone, the waiting staff just sneak out with a replacement layer and they put it back in there and there’s plenty of cake for all once again. Okay, so now let’s take a quantum leap and think about our two cakes as being computers and the icing on each cake being parts of our business application. So the red icing is our presentation service. The white icing is our logic service, and the blue icing is our data service. So the immediate difference between these two is that the wedding cake has more cake, right? So it’s got icing for each layer so it’s obviously gonna last a lot longer, but there’s another unique difference here. If the red icing is super popular, and that red layer all gets eaten up, we can just quickly add a new red layer to our cake. Our wedding cake is still there, functioning as a cake and serving hungry eaters. On the other hand, once the red icing has all been gobbled up on our birthday cake, that’s it, we need to bake a new cake if we want more red icing. So in our analogy, the birthday cake is a single tier <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-2-architecture-basics/">architecture</a> and the wedding cake is a multi-tier architecture. And each of these two architectures has their own benefits. So the core benefit of the multi-tier architecture is that each of the layers is independent of the other layers. </p>
<p>With a multi-tier architecture, each layer can be scaled up or down to meet a specific demand for that layer. The single-layered cake is easy to make, right? Very fast, very simple, but the base under the icing is shared. So if one section of that base runs out, we have to replace the whole cake. So leaping back to computing, if we have a layer or a tier for each of our services, this means each layer is more flexible. Each of the layers is independent of the other layers, and can have its own controls and be sized appropriately. Now that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-3-when-to-use-multi-tier/">multi-tier design</a> really suits cloud environments, where we can run each layer on truly scalable base infrastructure. So <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-introduction/">multi-tier architecture</a> simply means that we have more than one layer and one layer for each of our respective services. </p>
<p>The three-tier architecture is the most popular implementation of a multi-tier architecture. So going back to our business application that usually consists of a presentation tier, a logic tier, and a data tier. Now services in each tier can run on computers, they can be EC2 instances that were provisioned in the AWS cloud. They can be run as serverless functions within AWS, i.e. run on managed infrastructure, and not even need dedicated machines or an instance to run on. So in short, the tiers could be different services or technologies. The name of a tier or even the type of technology used in it isn’t really important. The reason for wanting to separate out the layers is so that each can operate and scale independently of the other. The benefit of that is that if one layer becomes overwhelmed, it doesn’t impact the other tiers. And more importantly, again, if one tier does become overwhelmed, we can increase the resources available to that tier, without having to scale the other tiers.</p>
<h1 id="When-Should-We-Consider-Single-Tier-Architecture"><a href="#When-Should-We-Consider-Single-Tier-Architecture" class="headerlink" title="When Should We Consider Single-Tier Architecture?"></a>When Should We Consider Single-Tier Architecture?</h1><p> When should we consider a single-tier design? Single-tier generally implies that all the replication services are running on the one machine or instance.</p>
<p> So the benefit of a single-tier architecture is that we have everything in one tier. So potentially one instance or group of instances can run our entire application. The three tiers will be sharing the memory, process, and storage of that one machine or group of machines. And that is fine when we just need a simple service, say a Dev or Test environment, or we have a very small application that may not need to scale up or down to meet demand. </p>
<p>However, keep in mind to scale a single-tier architecture, you’re generally going to need to scale an instance or machine vertically, i.e, you’re gonna need a bigger machine. Yes, you can run single-tier on multiple availability zones. So yes, a single-tier architecture can be made more resilient by running it in AWS. And yes, you can put your single-tier machine or instance into an order scale group so that it effectively scales based on demand. However, the design floor in doing that is that you are not decoupling your services so that they can scale independently. That’s the big benefit of running Cloud services in a decoupled environment. </p>
<p>So if you are asked to make that a single-tier application highly available and fault tolerant, you will need to refactor it as a multi-tier architecture. In a single-tier design, you are going to have all your services on the same machine using the same resources, which can create an availability risk. If the server is down, then the entire application will be down. If availability is a requirement, you need a decoupled multi-tiered architecture. If an exam scenario describes a very simple application that doesn’t need to be highly available and fault resilient, then running their application on a single-tier architecture may be a consideration. </p>
<p>The key thing to remember is that the benefit of multi-tier architecture is that the tiers are decoupled, which means they can be scaled to meet demand. And it is a major benefit of building applications in the Cloud. If the presentation layer is suddenly hit with a 100,000 user requests, that presentation layer can be scaled to meet that demand. If the application tier needs to run end of month report, and so it’s very busy for two or three days at the end of the month, their application tier can be scaled out to meet that demand and then scale back in to its usual operating level. Responding to this type of burst activity, really suits cloud environments and specifically suits AWS Auto Scaling, and more specifically Auto Scale groups.</p>
<h1 id="Designing-a-Multi-Tier-Solution"><a href="#Designing-a-Multi-Tier-Solution" class="headerlink" title="Designing a Multi-Tier Solution"></a>Designing a Multi-Tier Solution</h1><p>Let’s move into design mode. So, we’ll review the design of a multi-tier architecture for web services, architecture patterns using instances and load balancers. Then we’ll review how we could create a similar solution using serverless services, and then using a full microservice design. So for a multi-tier design, we need to implement the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. We’re going to need to use the virtual private cloud or VPC. So, we’re building our solution within the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-connectivity/">Virtual Private Cloud</a>. </p>
<p>Now there’s four basic patterns of VPC an Amazon VPC can have a single public subnet only. And that design would suit a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-when-to-use-single-tier/">single-tier architecture</a> where we need to run perhaps a very low traffic server or service and have it available to the internet. So in this design, we’d have one instance within that public subnet, which ran all of our services. Like our front end, our logic layer and our database layer on that one machine. We can have an Amazon VPC with private and public subnets and that might suit our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-introduction/">multi-tier architectures</a>. Where we want to have our presentation layer available to the internet. And our logic and data tiers hidden from the internet in a private subnet. We can also have a VPC with public and private subnets and VPN access. So, here is an example of a three tier highly available VPC design, we have public subnets for our presentation tier, we have private subnets for our logic and database tiers. We want to have separate subnets for unique routing requirements. AWS provides us with areas called availability zones. So, using more than one availability zone allows us to build redundancy into our design and so we can spread our services within our tiers across these multiple availability zones. So, if one availability zone is taken out by an earthquake or a disaster of sorts, your application can continue to run because you’re spread across two availability zones. Now you can have up to four availability zones depending on the region and that gives you a really high availability and disaster recovery potential. So, let’s talk a little bit about subnets before we move on, so without delving too deep into subnetting, here is what you need to remember about subnets. A subnet can be either public or private. </p>
<p>A subnet is public, if it has an internet gateway, and a route to that internet gateway, okay? If it doesn’t have a route to an internet gateway, then it is a private subnet. All right, public subnets have a default route table, which enables us to allow inbound and outbound traffic to that subnet. Private subnets need a route table to direct flow within the VPC. So remember, public subnets have an internet gateway and a route to the internet gateway. Without both the gateway and the route being present. You have a private subnet. Okay, so the internet gateway is like the doorway to the outside world of your VPC. So, imagine this VPC as your big fat VPC wedding and you want to make part of your VPC wedding available as your wedding reception. So, think of the internet gateway as the entrance to your VPC wedding and the reception area one of your subnets, in the VPC. If you want to invite your friends, you’re going to need to make the reception subnet of your VPC wedding public. You need to add the internet gateway, and update the route map to direct guests from the internet gateway to the reception subnet. So, once you’ve set up the internet gateway, you then need to control who can come in and out of the internet gateway, because it is going to be visible to the public. And if you have that doorway, you need a security team working on it. </p>
<p>So, our first line of defense for our VPC wedding are security groups. They are the bouncers of your VPC wedding. Security groups control access to resources, so they work at the instance level. They work to control inbound and outbound traffic to instances within the subnet. They don’t work at the subnet level. They work at the instance level. So, network access control lists or ACL’s act as another form of firewall to control inbound and outbound traffic at the subnet level. So, we want to have a network ACL protect the subnet in each availability zone. So, network ACL’s provide individual controls that you can customize as a second layer of defense. Independent routing tables need to be configured for every private subnet to control the flow of traffic within and outside of the VPC. So remember, a subnet is a public subnet if it has the internet gateway and a route to the internet gateway. Okay, not too difficult. Hopefully it’s starting to make a bit of sense.</p>
<h1 id="Connectivity-Within-The-VPC"><a href="#Connectivity-Within-The-VPC" class="headerlink" title="Connectivity Within The VPC"></a>Connectivity Within The VPC</h1><p>- [Man] So how do instances in our VPC access the internet? </p>
<p>Well, the first way is that we can assign a public IP address to that machine. So first we assign a public IP address or an Elastic IP address or EIP to the instances that we want to have internet access. </p>
<p>That gives those instances the ability to send and receive traffic from the internet, i.e. for web service, we want to have that ability. So how do instances without public IP addresses access to the internet? Instances without a public IP address can route their traffic through what we call a NAT Gateway or a NAT Instance. Now, NAT stands for Network Address Translation. And essentially, NAT instances or services, traverse IP ranges, internet protocol number ranges. And so allow instances and private or public subnets to access the internet via Network Address Translation. So if a machine is in a subnet and it doesn’t have an EIP address, then it’s not going to be visible through the internet gateway. But if we use a NAT gateway, we can have that machine topped outbound to the internet via this Network Address Translation. So the NAT Gateway or NAT Instance allows outbound communication, but it doesn’t allow machines on the internet outside of the VPC to initiate a connection to that privately addressed instance. Okay, so let’s look at another concept of connectivity, which is highly available NAT Gateways instead of NAT Instances. Remember, NAT stands for Network Address Translation and NAT Gateways offer major advantages in terms of deployment, availability and maintenance. </p>
<p>So rather than running a NAT Instance, which is basically a machine that we have provisioned and managed and we set up that routing rule, which allows machines in a public or private subnet who do not have an Elastic IP address, who do not have an internet address to connect outbound through the NAT instance through the internet gateway, outbound to the internet. So they are basically a hopping host to get out through the internet. So remember that in terms of highly available NAT Gateways are way more available because they’re a managed service. So they scale very well and designed to deal with burst activity, et cetera. Now, another form of connectivity we can have to our VPC is using a VPN. So if you have a hardware VPN connection or direct connection, instances can route their internet traffic down the virtual private gateway to your own internet connection. Now, note the difference there. There’s the internet gateway and there’s the virtual private gateway. So a VPN connection uses a virtual private gateway. Your internet in and outbound traffic uses the Internet Gateway. You can also have services within your VPC access the internet via your existing egress points using a VPN connection. Now, a couple of things to remember with VPC design is that always makes sure you leave spare capacity for additional subnets. So always make sure that your IP addressing contains additional capacity so that you can scale it.</p>
<h1 id="When-To-Go-Serverless"><a href="#When-To-Go-Serverless" class="headerlink" title="When To Go Serverless"></a>When To Go Serverless</h1><p>Hello, and welcome to this lecture, where I will discuss how to choose between using traditional EC2 instances or serverless services for your cloud-based compute workloads. And in this course I’m going to be focusing on AWS Lambda as my serverless service of choice. Now Lambda allows you to deploy and run custom code functions on demand, without needing to provision or maintain any servers of your own. But you should know that in addition to Lambda, AWS offers around a dozen different serverless services that fall into three different categories, including serverless compute, serverless application integration, and serverless databases, each with their own specific use cases within a given architecture. And to learn much more about all of the different serverless services offered by AWS, I encourage you to check out this course:</p>
<p>A Survey of Serverless: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/survey-serverless-2399/">https://cloudacademy.com/course/survey-serverless-2399/</a> </p>
<p>So I’m going to begin by discussing the main differences between EC2 and serverless services in general, again with an emphasis on AWS Lambda. After that, I’ll explain when you might want to go serverless, or when sticking with EC2 may be a better option instead. It’s important to remember that there are no hard and fast rules here, and sometimes your decision may come down to personal preference or just your general willingness to maintain and administer servers. As you’ll see, there are scenarios when an EC2 deployment will be more cost-effective than a serverless deployment, but other times when the opposite will be true. So it’s important to understand all of the tradeoffs between EC2 and serverless architectures to know what questions to ask when deciding whether or not to go serverless.</p>
<p>So let’s start by quickly discussing EC2 instances and when it makes sense to use them. EC2 instances are, of course, your tried and true virtual servers running in the AWS Cloud. They’ve been around for a very long time and have significantly reduced the burden that’s typically associated with provisioning servers in an on-premises environment. You can choose from a wide variety of instance types that can be optimized for compute, storage, or memory, going up to dozens or even hundreds of vCPU, hundreds of gigabytes of memory, and gigabit-level network performance. But at the end of the day, these EC2 instances are still servers and bring along with them all of the typical server maintenance and administration tasks you would expect with a legacy on-premises server. And this includes everything from configuring storage and networking, to scaling and load balancing multiple servers for high availability and fault tolerance, to patching and updating the underlying operating system as well as any other installed applications. And all of these things carry an associated cost on top of the amount AWS is billing you to run the instances themselves.</p>
<p>Now speaking of billing, when it comes to your EC2 instances, you’re either going to have some sort of reserved instance, where you’re paying a fixed cost for your instance to be up and running for a predefined amount of time, such as one year or three years, or you’ll be paying an on-demand or spot instance rate for however long your instance is up and running. And knowing if you need to always have an instance up and running is a key factor when deciding whether or not to go serverless. Depending on the nature of your workloads, it may or may not make sense to pay for an EC2 instance that is up and running at all times to respond to any requests. Because keep in mind, you’re always going to be paying for that running instance, even if it’s just sitting idle for extended periods of time.</p>
<p>So one of the big advantages of serverless computing is, just as the name suggests, you aren’t responsible for any servers: from your architecture’s standpoint, it is server-less. Now of course your code is still running on a server somewhere, but AWS is going to be fully responsible for the upkeep, maintenance, and security of that server. You’ll never have any access to it, nor will you ever need to worry about configuring or administering it. And as an added bonus, serverless services are inherently highly available. So unlike EC2 instances, which require you to use Elastic Load Balancing with an Auto Scaling Group to achieve high availability, services like AWS Lambda are highly available without any additional complexity or cost.</p>
<p>Now when it comes to billing for services like Lambda, you’re only ever going to pay for what you use. So if you deploy some code to a running EC2 instance that doesn’t get executed for two months, you’re still going to pay the full amount for that instance just as if it was being fully utilized the entire time. But with Lambda, you won’t pay anything until your code executes again. And even then, you’re only going to pay for exactly how much memory you’ve allocated to your function, as well as any ephemeral storage you need above 512 MB. And this billing is down to the millisecond level of execution time. Now that being said, there are some hard limits within Lambda, like a 15 minute timeout and 10 GB memory limit that can’t be exceeded. And we’ll talk more about this shortly.</p>
<h1 id="Choosing-Between-EC2-and-Serverless"><a href="#Choosing-Between-EC2-and-Serverless" class="headerlink" title="Choosing Between EC2 and Serverless"></a>Choosing Between EC2 and Serverless</h1><p>Hello, and welcome to this lecture, where I will discuss how to choose between using traditional EC2 instances or serverless services for your cloud-based compute workloads. And in this course, I’m going to be focusing on AWS Lambda as my serverless service of choice. Now Lambda allows you to deploy and run custom code functions on demand, without needing to provision or maintain any servers of your own. But you should know that in addition to Lambda, AWS offers around a dozen different serverless services that fall into three different categories, including serverless compute, serverless application integration, and serverless databases, each with their own specific use cases within a given architecture. And to learn much more about all of the different serverless services offered by AWS, I encourage you to check out this course.</p>
<p>So I’m going to begin by discussing the main differences between EC2 and serverless services in general, again with an emphasis on AWS Lambda. After that, I’ll explain when you might want to go serverless, or when sticking with EC2 may be a better option instead. It’s important to remember that there are no hard and fast rules here, and sometimes your decision may come down to personal preference or just your general willingness to maintain and administer servers. As you’ll see, there are scenarios when an EC2 deployment will be more cost-effective than a serverless deployment, but other times when the opposite will be true. So it’s important to understand all of the tradeoffs between EC2 and serverless architectures to know what questions to ask when deciding whether or not to go serverless.</p>
<p>So let’s start by quickly discussing EC2 instances and when it makes sense to use them. EC2 instances are, of course, your tried and true virtual servers running in the AWS Cloud. They’ve been around for a very long time and have significantly reduced the burden that’s typically associated with provisioning servers in an on-premises environment. You can choose from a wide variety of instance types that can be optimized for compute, storage, or memory, going up to dozens or even hundreds of vCPU, hundreds of gigabytes of memory, and gigabit-level network performance. But at the end of the day, these EC2 instances are still servers and bring along with them all of the typical server maintenance and administration tasks you would expect with a legacy on-premises server. And this includes everything from configuring storage and networking, to scaling and load balancing multiple servers for high availability and fault tolerance, to patching and updating the underlying operating system as well as any other installed applications. And all of these things carry an associated cost on top of the amount AWS is billing you to run the instances themselves.</p>
<p>Now speaking of billing, when it comes to your EC2 instances, you’re either going to have some sort of reserved instance, where you’re paying a fixed cost for your instance to be up and running for a predefined amount of time, such as one year or three years, or you’ll be paying an on-demand or spot instance rate for however long your instance is up and running. And knowing if you need to always have an instance up and running is a key factor when deciding whether or not to go serverless. Depending on the nature of your workloads, it may or may not make sense to pay for an EC2 instance that is up and running at all times to respond to any requests. Because keep in mind, you’re always going to be paying for that running instance, even if it’s just sitting idle for extended periods of time.</p>
<p>So one of the big advantages of serverless computing is, just as the name suggests, you aren’t responsible for any servers: from your architecture’s standpoint, it is server-less. Now of course your code is still running on a server somewhere, but AWS is going to be fully responsible for the upkeep, maintenance, and security of that server. You’ll never have any access to it, nor will you ever need to worry about configuring or administering it. And as an added bonus, serverless services are inherently highly available. So unlike EC2 instances, which require you to use Elastic Load Balancing with an Auto Scaling Group to achieve high availability, services like AWS Lambda are highly available without any additional complexity or cost.</p>
<p>Now when it comes to billing for services like Lambda, you’re only ever going to pay for what you use. So if you deploy some code to a running EC2 instance that doesn’t get executed for two months, you’re still going to pay the full amount for that instance just as if it was being fully utilized the entire time. But with Lambda, you won’t pay anything until your code executes again. And even then, you’re only going to pay for exactly how much memory you’ve allocated to your function, as well as any ephemeral storage you need above 512 MB. And this billing is down to the millisecond level of execution time. Now that being said, there are some hard limits within Lambda, like a 15-minute timeout and 10 GB memory limit that can’t be exceeded. And we’ll talk more about this shortly.</p>
<p>So we’ve seen some examples of when it makes sense to use EC2 instances. Now let’s talk about when a serverless implementation might make more sense. So obviously if you aren’t subject to any of the constraints I mentioned earlier, such as needing an extreme amount of processing power or memory, and you don’t otherwise need access to an underlying server, you probably have a good candidate for a serverless architecture. Using a serverless architecture allows you to focus on solving your business problems without having to spend any time provisioning or managing infrastructure. Some other situations where you may want to consider adopting a serverless approach include:</p>
<p>When you’re developing a brand-new, greenfield application and you want to adopt the most modern approach possible.</p>
<p>Cloud-native applications often have event-driven architectures that leverage microservices, and both of these lend themselves to the use of serverless technologies that include not only Lambda, but services like the Amazon Simple Notification Service and Simple Queue Service, or SNS and SQS as well. It’s even possible to create what are called Function URLs that allow you to expose HTTPS endpoints for your Lambda functions without needing to leverage additional services like API Gateway to deploy secure, highly available microservices. And you can also leverage the AWS Serverless Application Model, or SAM, to define your serverless application’s resources using CloudFormation and create more robust, repeatable deployments.</p>
<p>The next situation is if demand levels for your workloads are unpredictable, but you know they’re still within the limits supported by Lambda.</p>
<p>In these cases, a serverless architecture makes sense because you never have to pay for any idle EC2 instances. If you’re unable to accurately forecast demand, it becomes difficult to determine the best EC2 instance family to choose for your workloads. And choosing too large of an instance could prove to be a very expensive mistake if it just ends up sitting idle most of the time.</p>
<p>And finally, Lambda makes it easy to maintain multiple concurrent versions of your functions.</p>
<p>It even allows you to direct a subset of traffic to two different function versions at the same time to support testing, making it easy to quickly iterate, update, and roll back changes to your applications in a way that would require significantly more effort with an EC2 instance.</p>
<p>Now we’ve spent a lot of time talking about Lambda so far in this course, and that’s because as one of the flagship serverless services, it’s such an integral part of so many serverless solution architectures. So if you’re interested in learning more about AWS Lambda and how to leverage it within your own serverless architectures, I encourage you to check out this course:</p>
<p>Understanding AWS Lambda to Run &amp; Scale Your Code: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/">https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/</a></p>
<h1 id="Example-Cost-Comparisons"><a href="#Example-Cost-Comparisons" class="headerlink" title="Example Cost Comparisons"></a>Example Cost Comparisons</h1><p>Now before I wrap things up, I want to illustrate by way of example just how much your costs can vary between an EC2 and a serverless architecture depending on your application’s performance requirements. And the idea here is just to show that you can’t necessarily assume a serverless architecture will always be less expensive than one that leverages EC2 instances, or vice versa. Now of course, costs alone shouldn’t dictate whether or not a serverless implementation is more appropriate for your use case. Just know that one option may be more expensive than another, and it’s always worth running the numbers to see just how much these costs may differ. So for these examples I’m going to be using the AWS Pricing Calculator to estimate my costs, which is available at <a target="_blank" rel="noopener" href="https://calculator.aws/">https://calculator.aws</a>. And I would encourage you to do the same for your use cases as well. Now obviously, AWS pricing does change over time, but the prices I’m showing you here were current at the time of this recording.</p>
<p>So let’s start by comparing a workload we want to deploy as a Lambda function with unpredictable access patterns, but we know it gets executed a total of 1 million times per month. And let’s say this function requires 1 gigabyte of memory and 1 gigabyte of ephemeral storage. And let’s say it takes a total of 500 milliseconds to execute each time. So using the AWS pricing calculator, we can see that when operating outside of the free tier, this function will cost us $8.54 per month.</p>
<p>Now let’s say we want to deploy this workload on an EC2 instance instead. And let’s assume we can use a relatively modest t4g.small instance with a single 20 gigabyte EBS volume. With just the one instance, and using an EC2 Instance Savings Plan, this architecture will cost us $9.67 per month, which is pretty close to what we’d be paying to use Lambda.</p>
<p>Of course, if we want to achieve high availability, we would need to add a second EC2 instance, which doubles our total cost to just over $19 per month. And keep in mind this still doesn’t include any kind of load balancing, or any additional costs you might incur during prolonged spikes in demand since we are using a burstable instance type here.</p>
<p>So that was an example where a serverless approach saves a little money. But what if we massively scale up the demand? Let’s say instead of 1 million requests per month, we now have 100 million requests per month. Well since our Lambda function is billed based on usage, and we’ve increased our usage by a factor of 100, you’ll see the cost for this function has also increased by a factor of 100, putting us now at just over $854 per month.</p>
<p>But let’s say we can better predict the demand for these 100 million requests, and maybe we’ve determined that we can meet this demand by always running 4 t4g.xlarge EC2 instances. So by right-sizing our instances, even factoring in the cost of frequent snapshots for our EBS volumes, this implementation will only cost us about $282 per month, and that’s a huge savings.</p>
<p>So again, the idea here isn’t to show that a particular architecture will always be the most cost-effective option, but you do see how the costs of serverless and EC2 architectures can scale very differently. And if you are able to accurately forecast demand and right-size your instances, then an EC2 instance-based architecture may be your best option in terms of both cost and performance. EC2 instances also enable you to take advantage of savings plans and reserved instances that can further reduce your costs. But keep in mind that you will be responsible for managing and administering these servers, as well as configuring things like elastic load balancing and auto scaling. So it’s important to keep those costs in mind, as well as the additional time and labor required for server administration.</p>
<p>So to wrap things up here, there are no hard and fast rules when it comes to choosing a serverless architecture versus a more traditional EC2 instance-based approach. But if you’re looking to avoid the time and energy associated with provisioning and maintaining infrastructure, then a serverless architecture is probably best for you. This is especially true if you’re building a new architecture from scratch. Serverless architectures are also ideal for:</p>
<ul>
<li>rapid prototyping,</li>
<li>event-driven applications,</li>
<li>applications with low levels of demand that are not frequently accessed, as well as</li>
<li>cron jobs that only need to be run on a fixed schedule and don’t need to be available at any other time.</li>
</ul>
<p>On the other hand, you’ll want to go with an EC2 instance-based architecture if you need to have root-level access to the underlying server operating system and complete control over your infrastructure. Keep in mind, however, that you’ll always need to have at least one instance up and running, even if that instance is just sitting idle most of the time. And if you want your application to be highly available, you’ll need at least two instances running across different availability zones.</p>
<p>Serverless services are inherently highly available and don’t require you to pay for idle or unused resources. But there will always be cases where memory and performance requirements may exceed the capabilities of services like Lambda. In these cases, you’ll have no choice but to leverage EC2 and accept the associated costs of updating, patching, and securing your instances.</p>
<h1 id="Serverless-Design-Patterns"><a href="#Serverless-Design-Patterns" class="headerlink" title="Serverless Design Patterns"></a>Serverless Design Patterns</h1><p>Okay, let’s look at serverless architecture patterns. So our previous architecture was based on instances and a fleet of instances run in an auto-scale group. Another architectural design pattern we can deploy is to run a serverless design using AWS Lambda and Amazon API Gateway. Now by serverless we mean managed computing. AWS Lambda provides compute resources as a service i.e. you don’t need to provision an instance. You don’t need to create auto-scale groups or define <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-autoscaling/">auto-scaling</a> rules. You don’t even need to install code interpreters with AWS Lambda. That’s all taken care of for you. Now the logic tier of our three tier architecture usually represents the brains of the application i.e. that’s where the computing is done so to speak. So the logic layer is where using Amazon API Gateway and AWS Lambda can provide the most benefit compared to using server-based implementations. Because Lambda and API Gateway are managed services the scaling is done automatically, you don’t need to provision the hardware or software vertically or horizontally. The scaling and most of the security is taken care of for you by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. In short using these two services makes it really easy to build highly available, scalable and secure solutions. So let’s look at how we do this. If we use AWS Lambda instead of provisioning EC2 instances it means there’s no operating systems to choose to secure patch or for us to manage. We don’t have to size or monitor or scale the instances at all and we don’t need to worry about over provisioning or under provisioning those instances. </p>
<p>Now if we use the API Gateway to manage communication between code functions and services that simplifies again how we deploy, monitor and secure our APIs. Both drastically reduce the amount of infrastructure management that we have to do. So deploying code on AWS Lambda means you don’t have to define multiple availability zones. As a managed service we leave defining where the service will run up to AWS. However, you do still need to set up public and private subnets on some instances and some designs you will still need to use a VPC. </p>
<p>So using AWS Lambda for your logic tier means it is directly integrated with your AWS data tier. You need to ensure that this data tier is appropriately isolated in a privat subnet. So for your Lambda function to access resources that you don’t want to have made public like say a private database instance you can place your AWS Lambda function inside the VPC and configure an Elastic network interface or an ENI to access your internal resources. The use of Lambda in the VPC means that databases and other storage media that your business logic depends on can remain inaccessible to the internet. The VPC also ensures that the only way to interact with your data from the internet is through the APIs that you’ve defined in the Lambda code functions that you’ve written. So using Lambda as your logic tier doesn’t limit the data storage options available in your data tier. Plus we get improved API performance via caching and content delivery which immediately means that we don’t need to create, manage and pay for Elastic load balances between our tiers. Okay, big saving there. In a serverless <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/design-multi-tier-architectures/saa-d1-introduction/">multi-tier architecture</a> each of the APIs you create will need to be integrated with a Lambda function and that executes our business logic. So code functions in AWS Lambda are called handlers and you can configure API Gateway to trigger handler functions. And so those two are tightly integrated and generally it is one Lambda function per API or one Lambda function per API method. </p>
<p>When a handler is triggered by an event, say another function completes of there’s an HTDPS request that’s made to an API Gateway listener that handler is triggered. This design enables you to be more granular in how you expose your application functionality. Inside the Lambda function the handler can reach out to any of the other dependencies you have. So for example, other methods you’ve uploaded in your code, native binaries, external web services, other libraries or even other Lambda functions. So each Lambda function itself assumes an IAM role that is assigned when the Lambda function is first deployed. So the IAM role defines the other AWS services and resources your Lambda function can interact with. So that could be Amazon S3, it might be a DynamoDB table for example. So design wise you need to include services like AWS Key Management Service or AWSKMS to store environmental variables. You need to consider using services like AWS Secrets Manager to keep credentials or API Keys safe when they’re not being used. One rule of thumb is do not store sensitive information inside a Lambda function. Our presentation layer is a static website where our content is hosted in an Amazon S3 bucket. Again we have content distributed by Amazon Cloud front. However, in this design we have implemented the AWS Certificate Manager service so that we can use a custom SSL tier list certificate. </p>
<p>Now our logic layer is serverless so we have Amazon API Gateway exposing three services: slash weddings, slash tickets and slash info. The API Gateway endpoints are secured using a custom authorizer so users can sign in using a third party identity provider like Google or Facebook for example which provides the user with an ID token. The token is then included in the API Gateway call and our custom authorizer validates these tokens and generates an IA in policy containing API execution permissions. We then have AWS Lambda functions executing our logic. So each Lambda function is assigned its own IAM role to provide access to the appropriate data source. Now in our data tier one of the benefits of using serverless functions is our logic tier is tightly integrated with the AWS data services. So in our design we are using Amazon S3 to host static content used by the slash info service. We also have Amazon DynamoDB as our persistent data store for the slash tickets and slash wedding services. So we are using the Amazon ElastiCache service as a non-persistent data cache in front of our DynamoDB instance for the slash wedding service. So remember Amazon ElastiCache improves our database performance. If the ElastiCache case doesn’t hold the data needed by the HTDP request this is considered a cache miss and so the request is sent through to DynamoDB. Okay, so that’s using serverless functions in our logic tier.</p>
<h1 id="A-Survey-of-Serverless"><a href="#A-Survey-of-Serverless" class="headerlink" title="A Survey of Serverless"></a>A Survey of Serverless</h1><p>Hello, my name is Will Meadows and I would like to welcome you to this survey of the serverless services. </p>
<p>Today we’re going to go over all the different serverless categories and services that are available within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Initially serverless was a very small domain that only offered Lambda as the sole option for someone getting into the serverless category. However, At this point in time, there are over 12 different serverless services for you to choose, each providing a unique benefit and solving a specific problem. </p>
<p>That’s why I think it’s important for someone just getting in the domain to take a survey of all of these options to get a real understanding of what it is that they want to learn about. This lecture will spend just a few minutes on each of the 12 different services to help you drive forward your learning and education. And for each service where we have a course, I will link to that lecture so that way you can continue your education on anything that you deem fit.</p>
<p>So grab a coffee, sit back and relax, and let’s take a look at all the different serverless options that AWS has available.</p>
<p>To get us started there are currently three different categories of serverless services. These categories include: serverless compute services, serverless application integration services, and serverless database services.</p>
<p>I think we should start off by looking at something you’re probably the most familiar with, which is the serverless compute services.</p>
<p>Compute is probably one of the most important things that any application will have to deal with. It is the way that the actual work gets performed. </p>
<p>Traditionally this job was completed by fleets of servers all running specific applications or code to complete their jobs. These jobs might range from running websites, big data analytics, or even dealing with long-running applications like video game servers.</p>
<p>In this category of serverless compute there are currently two different services that can complete these jobs in varying ways. Each of the options have some positives and negatives that lend themselves to some use cases over others. Let’s start off with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">AWS Lambda</a> and take a look at where it might shine.</p>
<p>Lambda was released in 2014 as the first serverless service. It offered the ability to run code, without needing to manage a server, for up to 5 minutes at a time. </p>
<p>This new paradigm of compute, the idea of functions as a service Is what really pushed forward the category of serverless as a whole.</p>
<p>Lambda is an event-driven service. This means that it needs some kind of action, or event, to trigger the code you wish to run. In many situations, the event or action is just a change of state. This change of state could be something as simple as when an image is placed into an S3 bucket.</p>
<p>These days Lambda has increased its execution time to up to 15 minutes, and has greatly increased its range of functionality from its 2014 introduction.</p>
<p>If you are just starting your journey into this space, getting a deeper understanding of lambda will really help your learning and understanding of every other serverless offering.</p>
<p>We have a course that covers AWS Lambda in-depth over here if you want to learn more: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/</a></p>
<p>This service allows you to run serverless containers on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/ecs-ec2-container-service/">Amazon ECS</a> (the elastic container service). If you are currently running a container-based application, and would like to convert to a serverless format, this is the perfect solution for you! </p>
<p>One of the coolest aspects about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/course-automatically-created-2018-10-02-113135226141/c3l4-terraform1/">Fargate</a> is that it allows you to get over the 15 minutes execution hurdle that plagues AWS lambda. You are allowed to run your Fargate tasks (containers) for an unlimited amount of time. The serverless component of AWS Fargate is in regard to where those containers actually live.</p>
<p>If you would like to learn more about ECS, Fargate, and Microservices - we have a course that covers all of that right here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/">https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/ </a></p>
<p>Application integration services are a part of the interstitial glue that helps weave everything together. They allow you to connect to and send data to other applications and AWS services. There are a number of these types of offering available nowadays, and they all fill a slightly different niche. </p>
<p>Amazon has a serverless event-based service called Amazon Eventbridge that functions as a serverless event bus. Think of an event bus as a kind of event coordinator. It allows you to intake information from both external SAAS providers, AWS services (over 90 different ones are supported), and even your own custom applications. With these input events, Amazon Eventbridge can filter, manage, and direct them to other systems that listen for them, and are able to take action based on their content. It does this filtering process with routing rules, which give you full control over your event bus.</p>
<p>If you would like to learn more about this service, please check out this course over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/">https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/</a></p>
<p>AWS Step Functions can best be described as a serverless state machine service. For those who don’t know what a state machine is, think of your standard vending machine. </p>
<p>A vending machine sits there waiting for a customer to come up to it and input money (that’s its idle state). Once money has been added into the machine, it movies onto the next state, which would be item selection. The user inputs their choice, and the machine moves into the final state of vending the product. After the workflow has been completed it returns back to the idle state, waiting for another customer.</p>
<p>AWS Step Functions allow you to create serverless workflows just like the vending machine, where you can have your system wait for inputs, make decisions, and process information based on the input variables.</p>
<p>If you would like to know more about AWS Step Functions please check out this course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-step-functions-1117/introduction/">https://cloudacademy.com/course/aws-step-functions-1117/introduction/</a></p>
<p>Amazon SQS (the simple queue service) is a messaging queue system. It can help you decouple your applications by providing a system for sending, storing, and receiving messages between multiple software components. SQS is a managed service that offers two types of queues: FIFO and Standard (which is a best-effort ordering queue, with at least once delivery).</p>
<p>Amazon SQS is a fantastically useful service that works splendidly in most serverless applications but can also find a home in many standard architectures.</p>
<p>For more on SQS please check out this course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/">https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/</a></p>
<p>This service is a Pub-Sub notification service that provides both application-to-application or application-to-person communication. This communication works well for high-throughput applications as well as many-to-many messaging between distributed systems. SNS can also act as an event-driven hub similar to Amazon Eventbridge - It’s just more bare bones.</p>
<p>For more info on Amazon SNS take a look over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/">https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/</a></p>
<p>AWS has created a fully managed service called Amazon API Gateway which helps deal with: building, publishing, monitoring, securing, and maintaining API within AWS. It works quite well at any scale, and is able to support serverless, generic web applications, and even containerized workloads on the back end. </p>
<p>You can build your APIs for public use, for private use, or even for third-party developers. The best part about it is that it is entirely serverless, and does not require you to manage any infrastructure and you pay just for what you use.</p>
<p>The service is also able to handle accepting and processing hundreds of thousands of concurrent requests. If things start to get out of hand, API Gateway is able to monitor all traffic and can throttle requests as desired.</p>
<p>Please check out this introductory course on API gateway if you are interested in learning more: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/">https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/</a></p>
<p>AWS app sync allows you to manage and synchronize data across multiple mobile devices and users. The service also allows users to modify data while offline, and having those changes be automatically synced when the device reconnects to the internet. </p>
<p>This functionality allows you to build real-time, multi-user collaborative tools and applications that work between browsers, mobile applications, and even Amazon Alexa skills. </p>
<p>AWS AppSync uses GraphQL to enable clients to fetch, change, and subscribe to data from databases, microservices, and APIs all from a single GraphQL endpoint.</p>
<p>Setting up and managing databases is a huge challenge for many organizations. Having to deal with scalability and right-sizing can take a lot of knowledge, time, and money to get set up just right. Having data storage be serverless can greatly increase your productivity as well as reduce a lot of headaches. There are a number of fantastic data storage services available serverlessly these days, so let’s take a moment to look at each of them.</p>
<p>Amazon s3 is an object-based serverless storage system that is able to handle a nearly unlimited amount of data. S3 provides great scalability, availability, and speedy performance for many different use cases. Amazon S3 is able to support files as small as zero bytes, and tops out at five terabytes. </p>
<p>Objects stored in S3 have a durability of 11 nines (99.999999999%) and so, the likelihood of losing data is extremely rare.</p>
<p>S3 has native integrations with AWS lambda, allowing you to create event-based workflows with ease. Additionally, s3 provides some of the cheapest data storage available through the use of the S3 Glacier storage class. </p>
<p>Please take a look over here at this course if you wish to learn more about Amazon S3: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/">https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/</a></p>
<p>DynamoDB is a fully managed serverless, NoSQL database that has been built to run high-performance applications at any scale. The service can operate at single-digit millisecond latency which is very valuable for time-sensitive applications that require the fastest response times. DynamoDB is a key-value store database that has no strict design schema that it needs to conform to.</p>
<p>DynamoDB is designed to be highly available. Your data is automatically replicated across three different availability zones within a geographic region. In the case of an outage or an incident affecting an entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>For more information about DynamoDB please take a look at this course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/">https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/</a></p>
<p>Amazon RDS Proxy is a fully managed, serverless, highly available database proxy for Amazon RDS. The proxy allows you to build serverless applications that are more scalable than your standard direct to RDS implementations.</p>
<p>If you are opening many new connections to your RDS databases through lambda functions or other serverless methods - you might have issues when large surges of connections are required.</p>
<p>RDS Proxy allows you to pool and share already established database connections, reducing the latency of your applications when establishing a new connection. Additionally RDS Proxy helps the availability of your serverless application by denying access to unserviceable connections that may degrade your database’s performance. </p>
<p>Aurora serverless is a fully on-demand SQL database configuration for Amazon Aurora. It automatically starts up, shuts down, and scales its capacity based on the application’s needs. It operates on a pay-per-second basis while the database is active, and can be used through a simple database endpoint. If you ever need to switch over to standard workload and leave the realm of serverless, you can do so with the click of a button.</p>
<p>Aurora is built to be highly available, fault-tolerant, and self-healing as it replicates your data 6 ways across multiple availability zones.</p>
<p>If you would like to learn more about Aurora serverless please take a look at this course over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/">https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/</a></p>
<p>AWS has developed and positioned a well-thought-out series of serverless services that can help almost any application. These days there’s pretty much an in-place stand-in for every piece of normally servered component, that can be run serverlessly.</p>
<p>I would highly recommend looking at the coursework for any of the services covered today that interested you. Each course dives directly into the service and can really help explain where it fits in an architecture and what problems it can help you solve.</p>
<p>Hopefully, you’ve enjoyed this survey of all the service services and categories within AWS. My goal was to give you just a little taste of each facet of serverless so that way you could explore and find what you want to learn about.</p>
<p>My name is Will Meadows and I’d like to thank you for spending your time here learning about Serverless. If you have any feedback, positive or negative, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated, thank you!</p>
<h1 id="The-3-stage-approach-to-migration"><a href="#The-3-stage-approach-to-migration" class="headerlink" title="The 3-stage approach to migration"></a>The 3-stage approach to migration</h1><p><strong>Instructor: Stuart Scott</strong></p>
<h1 id="The-3-stage-approach-to-migration-1"><a href="#The-3-stage-approach-to-migration-1" class="headerlink" title="The 3-stage approach to migration"></a>The 3-stage approach to migration</h1><p>When planning a migration from your own on-premise data center to AWS, it can be more effectively managed and implemented if looked at from a 3-stage perspective, and AWS has categorized its migration services to help align to this process. You may or may not use each service, however it provides a great starting point to help you plan your migration. </p>
<p>The stages that have been defined by AWS consist of:</p>
<ol>
<li>Assess </li>
<li>Mobilize</li>
<li>Migrate and Modernize</li>
</ol>
<p>The first stage, *<strong>Assess*</strong>, focuses on the start of your journey and forms the basis of understanding your current environment to determine how prepared and ready your IT infrastructure is to move to the AWS. This will help you to formulate the goals and objectives of your migration and allow you to present an effective business case to your leadership team. </p>
<p>The migration services related to this stage of the process are:</p>
<ul>
<li>Migration Evaluator</li>
<li>AWS Migration Hub</li>
</ul>
<p>Stage 2, *<strong>Mobilize*</strong> has more of an emphasis on defining details of your migration plan and your strategy, it will also help you to iron out any kinks in your business plan with regards to the potential requirements of needing to perform specific migration strategies, which usually form 1 of the these 7 methods for your applications:</p>
<ul>
<li>Relocate</li>
<li>Rehost</li>
<li>Replatform</li>
<li>Refactor</li>
<li>Repurchase</li>
<li>Retire</li>
<li>Retain</li>
</ul>
<p>With this in mind, you have the ability to understand your baseline environment in addition to identifying any skill gaps you may have within your organization.</p>
<p>The migration services related to this stage of the process are:</p>
<ul>
<li>AWS Application Discovery Service</li>
<li>AWS Control Tower</li>
</ul>
<p>Next we have stage 3, <em><strong>Migrate and Modernize*</strong></em><em>.</em>* This is when you can design your application solution to run on AWS, understanding the services required, the interconnections between different services required when migrated, in addition to the validation of the design. Depending on which of the 7 migration strategies used for each application, will ultimately depict which AWS service you should use to migrate your application and its associated data. </p>
<p>The migration services related to this stage of the process are mixed between those required to migrate servers, databases and applications which are:</p>
<ul>
<li>AWS Application Migration Service</li>
<li>AWS Database Migration Service</li>
</ul>
<p>…and then those services which are more oriented towards managing the migration of data, which include:</p>
<ul>
<li><p>AWS DataSync</p>
</li>
<li><p>AWS Transfer Family</p>
</li>
<li><p>AWS Snow Family</p>
</li>
<li><p>AWS Service Catalog</p>
</li>
<li><p>AWS Storage Gateway</p>
</li>
</ul>
<h1 id="‘Assess’-Understanding-your-environment"><a href="#‘Assess’-Understanding-your-environment" class="headerlink" title="‘Assess’ - Understanding your environment"></a>‘Assess’ - Understanding your environment</h1><p><strong>Instructor: Stuart Scott</strong></p>
<h1 id="‘Assess’-Understanding-your-environment-1"><a href="#‘Assess’-Understanding-your-environment-1" class="headerlink" title="‘Assess’ - Understanding your environment"></a>‘Assess’ - Understanding your environment</h1><p>In this lecture I want to provide you with an understanding of what the *<strong>Migration Evaluator*</strong> and *<strong>AWS Migration Hub*</strong> services are in greater detail to help you ascertain when and why you might use either of these services during the assessment stage of your migration plan.</p>
<h3 id="Migration-Evaluator"><a href="#Migration-Evaluator" class="headerlink" title="Migration Evaluator"></a><strong>Migration Evaluator</strong></h3><p>Migration Evaluator was formerly known as TSO Logic, however in 2018 AWS acquired TSO Logic and the Migration Evaluator service was born.</p>
<p>Being associated with the ‘assess’ step of your cloud migration, you can expect that Migration Evaluator has an element of data gathering. It provides a mechanism to help you to understand and implement a baseline of what your environment looks like on premise and how this could be projected to run in AWS with associated cost modeling through data analysis. This analysis is helpful to speed up your journey to a successful digital transformation onto the AWS Cloud platform.</p>
<p>The Migration Evaluator focuses primarily on understanding your compute, storage and Microsoft licenses that you are currently using and automatically helps to identify which resource options would provide the best cost optimized solution in AWS for each given workload that you are operating. This right sizing exercise for EC2 helps you to keep your expenditure low when undergoing a migration and these recommendations are determined through an algorithm that analyzes your hardwares CPU utilization, the age of the processor, the available RAM, in addition to the OS being run and its Microsoft license type. When compared to the costs of running your environment on premises, Migration Evaluator can help to reduce costs up to 50% when running in AWS. </p>
<p>To be able to perform this analysis, Migration Evaluator has an agentless collector tool that gathers the relevant data to suggest its recommendations, including inventory discovery and time-series performance data. Alternatively, these metrics and data can also be ingested by the Migration Evaluator using other 3rd party tools. It is recommended that this data collection and gathering exercise is run for at least 2 weeks. Once collected, this data can then also be read by other AWS services such as the AWS Migration Hub once it has been collected&#x2F;received by the Migration Evaluator.</p>
<p>Once the data has been received, the Migration Evaluator will be able to present Quick Insights document in a few hours, which showcase the results of the assessment and evaluation highlighting recommendations and also the projected and expected costs after the migration, in addition to identifying any issues relating to the scope of resources required based upon your workloads.</p>
<p>Following the result presented in the Quick Insights documents, you can request further analytical information in the form of a directional business case, and an Analytics engine, but this can take up to a further 7 days to receive it from the AWS Migration Evaluator team. These help to dive into and analyze different cost modeling scenarios and a full inventory of compute and storage resources.</p>
<p>If you would like to request an assessment by the Migration Evaluator then you can either speak with your AWS account manager to organize it, alternatively you can visit <a target="_blank" rel="noopener" href="http://aws.amazon.com/migration-evaluator/">http://aws.amazon.com/migration-evaluator/</a> </p>
<h3 id="AWS-Migration-Hub"><a href="#AWS-Migration-Hub" class="headerlink" title="AWS Migration Hub"></a><strong>AWS Migration Hub</strong></h3><p>The AWS Migration Hub provides a dashboard view of your migration project, and so, it’s a very useful way to plan, track and manage a migration project. It is essentially the nerve center of your migration enabling you to discover and migrate services you have been running in identified locations. </p>
<p>A migration project can often include a wide variety of services and servers often located in a number of different data centers or facilities. You might have databases located in one data center and business applications located in various points in another office or co-location. There may also be different versions and configurations of service, which will need to be taken into account if you go to migrate through a different platform. Now one of the challenges with migrations is maintaining one view of the various aspects of this project. So the AWS Migration Hub is a very powerful tool for large migrations where there are multiple locations or multiple servers and services. So how does it work? </p>
<p>Well, as the name implies, the Amazon Migration Hub is the central hub of your migration project. It is accessed and run from the AWS Management Console, and once you have activated the Migration Hub, you can use it to discover and manage your migrations. It integrates with other AWS services to discover and audit your server inventory on a number of different networks and nodes, and it can also manage and track the migration of services and servers and applications to AWS Cloud Services.</p>
<p>The Migration Hub receives and collects server inventory from a variety of different sources, these include:</p>
<ul>
<li><p><strong>Migration Hub import</strong> – This allows you to import data from your own on-premises servers and applications.</p>
</li>
<li><p><strong>Migration Evaluator Collector</strong> – As already discussed, the Migration Evaluator can be used to feed into Migration Hub providing insights to help build a business case and plan strategy.</p>
</li>
<li><p><strong>AWS Agentless Discovery Connector</strong> – This a VMware appliance that is used to collect data and discover information relating to any VMware virtual machines that you might be using within your data center. </p>
</li>
<li><p><strong>AWS Application Discovery Agent</strong> – This Agent, created by AWS, is installed on your own servers and virtual machines that you would be running in your current data center with the main objective being to collate metrics and information relating to system configuration, system performance, running processes, and network connections.</p>
</li>
</ul>
<p>Using these sources, the Migration Hub can then view technical specifications and performance information about the discovered resources. With discovery, we can get an in depth analysis of what it would look like, how many servers we have, how many applications we have, we can group them together, and perform detailed analysis. </p>
<p>When it comes to the actual migration, you can choose a migration service tool that can be selected from the AWS Migration Hub. If you are looking to migrate a server, then you can use the *<strong>AWS Application Migration Service*</strong>, if you want to migrate a database, then you would need to select the *<strong>AWS Database Migration Service*</strong>.</p>
<h1 id="‘Mobilize’-Migration-and-strategy-planning"><a href="#‘Mobilize’-Migration-and-strategy-planning" class="headerlink" title="‘Mobilize’ - Migration and strategy planning"></a>‘Mobilize’ - Migration and strategy planning</h1><p><strong>Instructor: Stuart Scott</strong></p>
<h1 id="‘Mobilize’-Migration-and-strategy-planning-1"><a href="#‘Mobilize’-Migration-and-strategy-planning-1" class="headerlink" title="‘Mobilize’ - Migration and strategy planning"></a>‘Mobilize’ - Migration and strategy planning</h1><p>The Mobilize stage of planning a migration to AWS has a focus on two key AWS services, these refer to the:</p>
<ul>
<li>AWS Application Discovery Service</li>
<li>AWS Control Tower</li>
</ul>
<p>In this lecture, I want to explain what these services are used for enabling you to determine when and if you need them as a part of your migration planning.</p>
<h3 id="AWS-Application-Discovery-Service"><a href="#AWS-Application-Discovery-Service" class="headerlink" title="AWS Application Discovery Service"></a><strong>AWS Application Discovery Service</strong></h3><p>So, the AWS Application Discovery Service follows on nicely from the Assess stage, and enables you to dive deeper into your workloads by collating additional information, such as usage, configuration, and behavior data from these workloads running within your data centers. With this useful information collated, security is of course paramount and so the Application Discovery Service encrypts all information that it has retained. This encrypted data can also be read by other AWS services, such as the AWS Migration Hub. Also, if you are familiar with Amazon Athena and Amazon QuickSight you can export the data for additional analysis and help you identify the TCO of running your workloads in AWS.</p>
<p>So how does the Application Discovery Service gather key data from your resources in your own data centers? Well it can be done in one of two different ways.</p>
<p>Firstly, through an Agent-based discovery, and secondly via an Agentless discovery.</p>
<h4 id="Agent-based-discovery"><a href="#Agent-based-discovery" class="headerlink" title="Agent-based discovery"></a><strong>Agent-based discovery</strong></h4><p>Let’s look at the agent-based option first. As expected, in this scenario an agent is installed across your fleet of servers that you want to gather information for, which can be installed on both Linux and Windows operating systems. Also, this agent can be installed both on physical servers in addition to virtual machines. When installed, the agent is then used to capture configuration data, system performance, network connections, in addition to the different processes that are running on the server, helping you to map out service dependencies.</p>
<p>Once the agent has been registered, it will connect to the AWS Application Discovery Service in the specified region and integrate with the AWS Migration Hub. From this point, the agent will gather the data and send it back to the application Discovery Service over a secure connection using Transport Layer Security (TLS) approximately every 15 minutes. For an in-depth look at the data captured, please refer to the AWS documentation found here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/application-discovery/latest/userguide/agent-data-collected.html">https://docs.aws.amazon.com/application-discovery/latest/userguide/agent-data-collected.html</a></p>
<h4 id="Agentless-Discovery"><a href="#Agentless-Discovery" class="headerlink" title="Agentless Discovery"></a><strong>Agentless Discovery</strong></h4><p>Now let’s look at the Agentless discovery, which is also referred to as the Discovery Connector. The first thing to mention is that this option is only available for gathering data on your VMware virtual machines. The reason being is that agentless discovery is configured by deploying an AWS agentless Discovery Connector (OVA file) via your VMware vCenter. Again, it captures information relating to each VM and which vCenter they are associated with, in addition to configurational data, IP and MAC address data, utilization and disk resource allocations, plus average peak metrics for RAM, CPU and Disk I&#x2F;O.</p>
<p>Much like the Agent-based discovery, it operates in a very similar way. Once the connector has been distributed as the OVA file, it will connect to the AWS Application Discovery Service and integrate with the AWS Migration Hub. However, communication between the Connector and the Application Discovery Service will only occur approximately once every 60 minutes. Also, as expected any data collected is sent over an encrypted TLS connection back to the service. </p>
<h4 id="AWS-Control-Tower"><a href="#AWS-Control-Tower" class="headerlink" title="AWS Control Tower"></a><strong>AWS Control Tower</strong></h4><p>AWS Control Tower is an essential service if you are looking to deploy a multi-account strategy as a part of your migration. It provides a simple and effective way to set up your accounts, teams, in addition to meeting governance requirements based upon best practices in the form of a Landing Zone, but what is a landing zone? </p>
<p>A Landing Zone is a multi-account architecture that follows the well-architected framework and is based around the ideas of security and compliance best practices. Your landing zone will be automatically created by AWS Control Tower and it is inherent part of the service, your landing zone is created from a series of best practice blueprints that help us set up systems that deal with identity, federated access and overall account structure, these blueprints do the following on your behalf, they create a multi-cloud environment using AWS organizations. </p>
<p>There are three organizational units that are provisioned here, the Root OU, this will be the parent OU that contains every other OU, within your landing zone, the core OU this OU contains the log archive and any audit member accounts, we generally refer to these accounts as the shared accounts and a custom OU, this OU another member OUs contain the actual working accounts that your users need to perform whatever duties they do within your AWS environment, for example your developer AWS accounts would sit within this OU.</p>
<p>The service also builds out two shared accounts, a log archive in the account which will be the place where all the logs will be sent between all accounts, it will store the logs of all API calls and resource configurations for every account within the landing zone and an audit account which has a restricted account that has been created to give your security and compliance team members read and write access to any account within your AWS landing zone.</p>
<p>From this account you’ll have programmatic access to review all other accounts, by way of a role that grants use of Lambda functions only, this account does not allow you to log into other accounts manually. Control tower will also provide identity management with the use of AWS single sign-on, default directory, this directory will house all of the AWS SSO users, you can also use it to define the scope or permissions available for each of those users. It will also provide federated access to those accounts, using AWS SSO directly, control tower then hooks up centralized logging from AWS cloud trail, and into its config, which is stored securely within AWS three and the logging account.</p>
<p>Finally it enables cross account security auditing using AWS IAM and AWS SSO to allow the audit account to perform routine checks as it wishes. In the end AWS Control Tower will have all of that automatically set up for you. That is incredible, the amount of time it might take you to create all that by hand would probably be measured in weeks if done by a single person. The service can even create pre-configured groups such as your admins, users, and auditors, you can of course create more designations that provide different levels of access based on your needs.</p>
<p>Additionally, if you have your own active directory service already going, you can plug that into AWS SSO and configure it to work directly with your system which again can be a cloud-based AD or when you’re hosting on-premises, just make sure to use AD connector or the AD service. </p>
<p>When you create your new landing zones using AWS Control Tower, there are a large number of AWS resources that are created on your behalf, you need to be very careful about deleting or removing these pre-configured resources. If you were to destroy or tamper with these resources you can send the control tower into an unknown state. </p>
<h1 id="‘Migrate-and-Modernize’-Designing-your-solutions-on-AWS"><a href="#‘Migrate-and-Modernize’-Designing-your-solutions-on-AWS" class="headerlink" title="‘Migrate and Modernize’ - Designing your solutions on AWS"></a>‘Migrate and Modernize’ - Designing your solutions on AWS</h1><p><strong>Instructor: Stuart Scott</strong></p>
<h1 id="‘Migrate-and-Modernize’-Designing-your-solutions-on-AWS-1"><a href="#‘Migrate-and-Modernize’-Designing-your-solutions-on-AWS-1" class="headerlink" title="‘Migrate and Modernize’ - Designing your solutions on AWS"></a>‘Migrate and Modernize’ - Designing your solutions on AWS</h1><p>In this lecture I want to cover some of the services that can be used to help you manage the migrate and modernize stage of your migration strategy, so the point at which you are actually moving your solutions and data into AWS. This lecture will be split into two different sections, firstly, I shall look at the services that help you to migrate your servers, database and applications, and then I shall focus on how to migrate data from your data center to the AWS cloud.</p>
<h3 id="AWS-Application-Migration-Service"><a href="#AWS-Application-Migration-Service" class="headerlink" title="AWS Application Migration Service"></a><strong>AWS Application Migration Service</strong></h3><p>The AWS Application Migration Service is a great service to help you migrate your applications to AWS with minimal downtime and interruption, such as those running SAP, Oracle, and SQL Server. The Application Migration Service is the suggested and recommended solution when migrating your applications over usine CloudEndure. </p>
<p>The service simply works on the basis of a lift-and-shift approach by converting your existing physical and virtual machines to running natively across AWS infrastructure using an agent which replicates your source servers to virtual instances in AWS while the source server continues to run. This simplified approach helps to keep cost to a minimum and helps to simplify the migration process.</p>
<p>Once your servers have been migrated, you can leverage the AWS cloud to further optimize your infrastructure through either refactoring or replatforming your applications. </p>
<p>To begin the migration, you will first need to configure a <em>replication settings template</em> which will help you to configure how data replication will be managed from each source server. You will need to specify settings for a replication server which is used to replicate data between your source servers on premise and instances in AWS, which will include:</p>
<ul>
<li>The staging area subnet</li>
<li>Instance type</li>
<li>EBS Volume type</li>
<li>EBS encryption</li>
<li>Security Groups</li>
<li>Data routing and throttling</li>
<li>Resource tags</li>
</ul>
<p>Once this template is created, you can then add your source servers, either Windows or Linux,, which is completed by installing an AWS replication agent. Once your agent is installed on your physical or virtual machines, it will appear within the Application Migration Service and will follow a Migration workflow and will display its status of each stage of the migration.</p>
<p>This image here shows how the agent integrates with the staging environment using your replication settings template, before being tested and cutover to being fully migrated resources. </p>
<p>***<img src="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid6-e0830763-2c6d-4d8e-801a-615f89715753.png" alt="alt"><br>Source:*** <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/mgn/latest/ug/adding-servers-gs.html">*<strong>https://docs.aws.amazon.com/mgn/latest/ug/adding-servers-gs.html*</strong></a></p>
<p>Launch settings within the service can be used to help you configure how each source server will be Tested before Cutover to a migrated server.  Here you will be able to configure settings such as:</p>
<ul>
<li>Instance type right-sizing</li>
<li>Private IP addressing</li>
<li>The transfer of server tags</li>
<li>OS Licensing </li>
<li>And if you want the instance to launch once it’s cutover or if you want to start it manually</li>
</ul>
<p>Of course testing is an integral part of this service, you need to be sure that your application will be running as expected before being cutover into your production environment. Testing can be managed on an individual server basis, or as a group of servers. Once your servers have been successfully tested, they can then be launched as a Cutover instance in your production environment based on the launch settings previously specified. </p>
<h3 id="AWS-Database-Migration-Service"><a href="#AWS-Database-Migration-Service" class="headerlink" title="AWS Database Migration Service"></a><strong>AWS Database Migration Service</strong></h3><p>As you probably expect from the name, this service has been designed to help you migrate your relational, NoSQL databases, and data warehouses, which could be from your own environment to AWS, with minimal downtime and security in mind. This is a very effective way to migrate your on premise databases from an array of commercial and open-source databases. </p>
<p>The Database migration service is very flexible in terms of its capabilities from a migration standpoint. You can migrate from and to the same database, such as Oracle to Oracle, but you can also migrate from different source to destination databases, making use of some of AWS’s most cost effective DB solutions. An example of this would be to migrate from an on premise database such as SQL Server, and migrating your data to the AWS service of Amazon Aurora, which is designed for very high performance and availability. </p>
<p>You may also have scenarios where by you are looking to consolidate your database environments, using the Database Migration service you can move multiple database workloads into a single Amazon Redshift environment allowing you to scale to a petabyte-sized data warehouse, providing you with an opportunity to gain insights and analyze your data. </p>
<p>If you are looking to migrate your database to a target database engine that is compatible with one another then the migration process is simplified. If components of the database share the same schema structure, data types, and code then the operation becomes more of a single step process making the DMS service very efficient.</p>
<p>However, if the source and target database engines that you have selected are different, and you might want to do this to make use of the capabilities of the different AWS database services that are available, then additional steps will be required. For example, let’s say your schemas between the source and target operate differently, then you will need to perform a schema transformation before the migration can take place using the AWS Schema Conversion Tool (AWS SCT). </p>
<p>When running a migration, you must specify your source and destination database targets. The DMS service will then create a Replication instance, which is effectively an EC2 instance with one or more replication tasks. Endpoints are also created to connect to your source datastore and your destination target datastore. Replication tasks are then run from your replication instance to move data sets from your source to your destination targets. </p>
<p>Ok, so now we have covered the first section, I can now move on to the next section where we look at the migration of data, starting with AWS DataSync.</p>
<h3 id="AWS-DataSync"><a href="#AWS-DataSync" class="headerlink" title="AWS DataSync"></a><strong>AWS DataSync</strong></h3><p>If you’ve been working with different AWS storage services for any length of time then you may have already come across this service. AWS DataSync is a service that allows you to easily and securely transfer data from your on-premise data center to <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> storage services. It can also be used to manage data transfer between 2 different AWS storage services too, so it’s a great service to help you migrate, manage, replace and move data between different storage locations. </p>
<p>At the time of writing this course, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/introduction/">AWS DataSync</a> supports the ability to work with data stored on Network File Systems shares, Server Message Block shares, and any self-managed object storage, in addition to the following AWS services:</p>
<ul>
<li>Amazon S3</li>
<li>Amazon Elastic File System</li>
<li>Amazon FSx for Windows File Server</li>
<li>AWS Snowcone</li>
</ul>
<p>When performing data transfer operations, whether this be from on-premises or between AWS storage services, DataSync support AWS VPC Endpoints and so its able to utilize the high bandwidth, low latency AWS network to it’s advantage, this helps to both simplify the management of the request and automate your data transfer across secure infrastructure. For more information on AWS VPC Endpoints, please see our AWS Networking lecture found here:</p>
<p><em><strong>VPC Endpoints</strong></em>* *<em><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-networking-features-essential-for-a-solutions-architect/vpc-endpoints/">https://cloudacademy.com/course/aws-networking-features-essential-for-a-solutions-architect/vpc-endpoints/</a></em> </p>
<p>With data transfer speed a being a key factor for a data transfer services, AWS Data Sync comes with its own purpose-built data transfer network protocol in addition to a parallel and multithreaded architecture to rapidly perform data transfer, this means that each DataSync task has the potential of utilizing 10 Gbps over a network link between your own on-prem data center and your AWS environment.</p>
<p>Obviously when working with data, especially when moving it between 2 points, security is a key concern. As a result AWS DataSync provides 2 levels that provide end-to-end security, these being encryption, in addition to data validation. </p>
<p>From an encryption perspective, encryption in transit is implemented by encrypting the data using the Transport Layer Security (TLS) protocol. When data reaches an AWS service, it also supports encryption at rest mechanisms that EFS and FSx for Windows service offers, in addition to the default encryption at rest option for Amazon S3.  </p>
<p>The 2nd point, Data Validation, ensures that your data arrives at its destination in one piece, exactly as it was when it left the source ensuring that it wasn’t compromised or damaged in any way during its transit. This additional check helps to validate the consistency of your data that was written to the AWS storage service, and that its a perfect match from when it left its source location.</p>
<p>From a cost perspective, AWS DataSync usesa flat pricing strategy based on a per-gigabyte of data transferred, this makes it easy to predict avoiding any unexpected costs.</p>
<h3 id="AWS-Transfer-Family"><a href="#AWS-Transfer-Family" class="headerlink" title="AWS Transfer Family"></a><strong>AWS Transfer Family</strong></h3><p>The AWS Transfer Family is specifically designed to help you securely transfer data both into and out of two of the most commonly used storage services that AWS has to offer, these being Amazon S3 and the Elastic File System (EFS). To learn more about Amazon S3 and Amazon EFS, please refer to the following content:</p>
<p>***Amazon S3: Deep Dive *<em><a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/deep-dive-amazon-s3-955/">https://cloudacademy.com/learning-paths/deep-dive-amazon-s3-955/</a></em> </p>
<p>***Using EFS to Create Elastic File Systems for Linux-based Workloads *<em><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/">https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/</a></em> </p>
<p>This file transfer can be completed using one of four supported protocols:</p>
<ol>
<li>Secure Shell File Transfer Protocol, more commonly known as SFTP, providing encryption over SSH</li>
<li>File Transfer Protocol Secure, referred to as FTPS and uses TLS encryption</li>
<li>File Transfer Protocol, FTP, which is an unencrypted connection</li>
<li>Applicability Statement 2, known as AS2</li>
</ol>
<p>Being a fully managed service, this transfer of data is enabled without you having to provision any of your own server infrastructure, instead the AWS Transfer Family utilizes its own file transfer protocol-enabled instance instead simplifying the process and reduces the need to alter the configuration of your applications. It is also a highly available service, operating in up to 3 different availability zones with the additional support of being backed by auto scaling to ensure your transfer requests are met without an issue.</p>
<p>The Transfer Family utilizes a Managed File Transfer Workflow (MFTW) which enables you to configure, run and implement a level of automation to help you manage your file transfers allowing you to track the process of the transfer from beginning to end. By utilizing MFTW you can configure specific processing actions that you might want to take prior to transferring your data, such as tagging, copying, enabling encryption, and filtering, plus other common file-processing actions.</p>
<p>To begin using the AWS Transfer family there are a number of steps to take. </p>
<ul>
<li>Of course, you must first have your destination storage configured and available, which will either be an S3 bucket, or an EFS File System. </li>
<li>Next, and through the use of IAM roles, you must grant the required permissions to the storage destination for the AWS Transfer Family. </li>
<li>Once your storage destination is set up with the appropriate permissions applied, you can then configure a Transfer Family Server using one of the support protocols to manage your file transfers to that storage destination. As a part of this configuration you can also implement optional CloudWatch logging metrics to help you monitor your transfer requests. </li>
<li>You must then add a User to the Transfer Server and associate it with the role you created previously allowing access to your S3 bucket of Elastic File System</li>
<li>You are then ready to complete the transfer using a Client, such as OpenSSH, WinSCP, Cyberduck or FileZilla. For additional information on how to do this for each client, please visit the following URL: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/transfer/latest/userguide/transfer-file.html">https://docs.aws.amazon.com/transfer/latest/userguide/transfer-file.html</a></li>
</ul>
<h3 id="AWS-Snow-Family"><a href="#AWS-Snow-Family" class="headerlink" title="AWS Snow Family"></a><strong>AWS Snow Family</strong></h3><p>When talking about the AWS Snow Family, I want to answer 2 simple questions:</p>
<ol>
<li>What is the snow family </li>
<li>and what does it consist of?</li>
</ol>
<p>So firstly, what is it? The snow family consists of a range of physical hardware devices that are all designed to enable you to transfer data into <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> from the edge or beyond the Cloud, such as your Data Center, but they can also be used to transfer data out of AWS too, for example, from Amazon S3 back to your Data Center. </p>
<p>It’s unusual when working with the cloud to be talking about physical devices or components, normally your interactions and operations with AWS generally happen programmatically via a browser or command line interface. The snow family is different, instead, you will be sent a piece of hardware packed with storage and compute capabilities to perform the required data transfer outside of AWS, and when complete, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">device</a> is then sent back to AWS for processing and the data uploaded to Amazon S3.</p>
<p>You can perform data transfers from as little as a few terabytes using an AWS snowcone all the way up to a staggering 100 petabytes using a single AWS snowmobile, and I’ll be talking more about these different snow family members shortly. Now of course when we are talking about migrating and transferring data at this magnitude, using traditional network connectivity is sometimes simply not feasible from a time perspective. For example, let’s assume you needed to transfer just 1petabye of data over a 1gbps using Direct Connect it would take 104 Days, 5 Hours 59 Minutes, 59.25 Seconds, not forgetting the cost of the data transfer fees too! </p>
<p>In addition to these devices packing some serious storage capacity for data transfer, some of them also come fitted with compute power, allowing you to run usable EC2 instances that have been designed for the snow family enabling your applications to run operations in often remote and difficult to reach environments, even without having a data center in sight, and when working with a lack of persistent networking connectivity or power. For example, the snowcone comes with the ability to add battery packs increasing their versatility. The enablement of running EC2 instances makes it possible to use these devices at the edge to process and analyze data much closer to the source.   </p>
<p>So let’s now take a look at what the Snow family consists of to get a better understanding of what these devices are.</p>
<p><img src="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid0-57c9129f-240c-4767-a47b-64a11c2436b7.png" alt="alt"></p>
<p>As you can see from this table, both from a physical and capacity perspective, the snowcone is the smallest followed by the snowball and finally the snowmobile. You may also notice that the snowball comes in 3 choices, compute optimized, compute optimized with GPU, and storage optimized, each offering a different use case, however, each of these 3 offerings all come in the same size device.</p>
<h3 id="AWS-Service-Catalog"><a href="#AWS-Service-Catalog" class="headerlink" title="AWS Service Catalog"></a><strong>AWS Service Catalog</strong></h3><p>AWS service catalog is an organizational tool developed with the purpose of making provisioning and creation of IT stacks easier for both the end user as well as your IT admins. </p>
<p>These stacks can include almost everything under the AWS sun – such as EC2 instances, Databases, software, and all the underlying infrastructures to create multi-tiered applications and architectures. </p>
<p>Service Catalog allows your end users to select the content that they need from a list of preapproved services that your IT or Admin teams set up ahead of time. This helps to bring down those barriers of entry for content creation, as well as helping to keep best practices and system security a key component of any deployment and migration. </p>
<p>With the ability to browse through a list of ‘Products’ which are just a set of pre-approved services, we can create and build with the full confidence that we as developers are creating solutions using only acceptable components that our security, administration, and leadership teams approve of. </p>
<p>Products are an IT service that you want to make available for deployment on AWS. A product can consist of just a single AWS Resource or can consist of multiple items such as EC2 instances, their associated EBS volumes, Database that you want them connected to, and all the monitoring capabilities you would come to expect from these services within the cloud.</p>
<p>A product can even be a package listed on the AWS Marketplace. For example, this could be helpful if you were using a database that AWS does not natively support but was available on the marketplace. In the end, a product is a service. It can range from something as small as a single instance doing basic web hosting to an enormous multi-tiered web application.</p>
<p>AWS Service Catalog requires you to upload AWS Cloud Formation Templates, and from these templates, the service will add that entire stack into the catalog as a single product. Again that product can be something as small as a single EC2 instance, or a very large multi-tiered web application.</p>
<p>Once your products are created, you can add these to AWS Service Catalog Portfolios. A portfolio is a collection of products with configuration information that helps in determining who can use the products within. Each portfolio requires a name, description, and a product owner. That last one is very important because if something goes wrong with an available product it’s important to know who to send your complaints to.</p>
<p>You can also share portfolios between other AWS accounts, and give the administrators of those accounts the ability to add their own products to your portfolio. This could be useful when you have teams that operate independently, that each deal with creating their own products. </p>
<p>One of the best features and really the whole goal of service catalog, is that you have full control over what your end users have access to. From an administrator perspective that is incredibly powerful and can help you maintain high levels of both security and credibility.</p>
<p>AWS Service Catalog allows you to apply constraints on the products within your portfolios. These constraints allow you to limit the scope and ability of your products based on predefined settings. They also allow you to have additional functionality, at least on the administrative side. </p>
<h3 id="AWS-Storage-Gateway"><a href="#AWS-Storage-Gateway" class="headerlink" title="AWS Storage Gateway"></a><strong>AWS Storage Gateway</strong></h3><p>Storage Gateway allows you to provide a gateway between your own data center’s storage systems such as your SAN, NAS or DAS and Amazon S3, Glacier, and Amazon FSx on AWS.</p>
<p>The Storage Gateway itself is a software appliance that is stored within your own data center which allows integration between your on-premise storage and that of AWS. This connectivity can allow you to massively scale your storage requirements both securely and cost efficiently.</p>
<p>The software appliance can be downloaded from AWS as a virtual machine which can then be installed on your VMware or Microsoft hypervisors.</p>
<p>Storage Gateway offers different configurations and options allowing you to use the service to fit your needs. It offers file, volume and tape gateway configurations which you can use to help with your DR and data backup solutions.</p>
<h4 id="File-Gateways"><a href="#File-Gateways" class="headerlink" title="File Gateways"></a><strong>File Gateways</strong></h4><p>File Gateways allow you to securely store your files as objects using File Gateway for Amazon S3 or you can use File Gateway configuration on Amazon FSx File Gateway, which provides access to in-cloud Amazon FSx for Windows File Server shares. </p>
<p>Using the S3 File Gateway allows you to map drives to an S3 bucket as if the share was held locally on your own corporate network. When storing files using the file gateway they are sent to S3 over HTTPS and are also encrypted with S3’s own server side encryption SSE-S3.</p>
<p>In addition to this, a local on-premise cache is also provisioned for accessing your most recently accessed files to optimize latency which also helps to reduce egress traffic costs. When your file gateway’s first configured you must associate it with your S3 bucket which the gateway will then present as a NFS V.3 or V4.1 file system to your internal applications.</p>
<p>This allows you to view the bucket as a normal NFS file system, making it easy to mount as a drive on Linux or map a drive to it in Microsoft. Any files that are then written to these NFS file systems are stored in S3 as individual objects as a one to one mapping of files to objects.</p>
<h4 id="Volume-Gateways"><a href="#Volume-Gateways" class="headerlink" title="Volume Gateways"></a><strong>Volume Gateways</strong></h4><p>Volume Gateways can be configured in one of two different ways, Stored volume gateways and cached volume gateways. Let me explain stored volume gateways first.</p>
<p><strong>Stored volume gateways</strong> are often used as a way to backup your local storage volumes to Amazon S3 whilst ensuring your entire data library also remains locally on-premise for very low latency data access. Volumes created and configured within the storage gateway are backed by Amazon S3 and are mounted as iSCSI devices that your applications can then communicate with.</p>
<p>During the volume creation, these are mapped to your on-premise storage devices which can either hold existing data or be a new disk. As data is written to these iSCSI devices the data is actually written to your local storage services such as your own NAS, SAN or DAS storage solution. However the storage gateway then asynchronously copies this data to Amazon S3 as EBS snapshots.</p>
<p>Having your entire dataset remain locally ensures you have the lowest latency possible to access your data which may be required for specific applications or security compliance and governance controls whilst at the same time providing a backup solution which is governed by the same controls and security that S3 offers. Storage volume gateways also provide a buffer which uses your existing storage disks. This buffer is used as a staging point for data that is waiting to be written to S3.</p>
<p><strong>Cached volume gateways</strong> are different from stored volume gateways in that the primary data storage is actually Amazon S3 rather than your own local storage solution. However cache volume gateways do utilize your local data storage as a buffer and the cache for recently accessed data to help maintain low latency, hence the name, Cache Volumes.</p>
<p>Again, during the creation of these volumes they are presented as iSCSI volumes which can be mounted by an application service. The volumes themselves are backed by the Amazon S3 infrastructure as opposed to your local disks as seen in the stored volume gateway deployment. As a part of this volume creation you must also select some local disks on-premise to act as your local cache and a buffer for data waiting to be uploaded to S3.</p>
<p>Although all of your primary data used by applications is stored in S3 across volumes, it is still possible to take incremental backups of these volumes as EBS snapshots. </p>
<p>The final option with AWS Storage Gateway is a tape gateway known as Gateway VTL. Virtual Tape Library. This allows you again to back up your data to S3 from your own corporate data center but also leverage Amazon Glacier for data archiving. Virtual Tape Library is essentially a cloud based tape backup solution replacing physical components with virtual ones.</p>
<p>This functionality allows you to use your existing tape backup application infrastructure within AWS providing a more robust and secure backup and archiving solution.</p>
<h1 id="Architecture-Summary"><a href="#Architecture-Summary" class="headerlink" title="Architecture Summary"></a>Architecture Summary</h1><p>Hello, Andy here to help you get ready for acing questions on design and architecture. Most of the questions you are going to face around architecture are around choosing the right service or combination of services to meet the set of requirements that you’re given. And you’re gonna find your new knowledge on compute, storage, networking and security, all plays a big part of making the right decision.</p>
<p>Let’s talk design patterns. The benefit of multi-layer architecture is the ability to decouple your layers so they can be independently scaled to meet demand. Thereby making the system more resilient and more highly available. So if you’re asked to design a high-performance service that’s gonna be recording a lot of events or transactions as fast as possible. Then you want to consider implementing a multi-tier design, right? So if it’s burst unpredictable traffic where you will also need to be able to say look up transactions or events using an ID. Then most likely you should consider using DynamoDB with global secondary indexes. That’s gonna give you the best performance, especially if you consider adding auto-scaling on the tables and global secondary indexes. That’s likely to give you the best response for that type of data need.</p>
<p>Now, if you’re designing a high-performance system say for machine learning or for data crunching and you need to access files internally then think about Amazon FSx for Lustre. It’s likely to provide the best performance for an internal high-performance file share. You can’t share files from EBS. So Lustre is a really good fast solution for making large volumes available to more than one instance. And remember FSx for Lustre only runs on Linux. So if you’re designing a solution for a Windows environment you need to use Amazon FSx for Windows file server.</p>
<p>Now Amazon FSx for Windows file server can be a really easy way to keep the same user permissions, say if you’re using Active Directory or accessing files on a Microsoft Windows platform. If you need your system to communicate on a specific port or in a specific way, say UDP, for example so then think Network Load Balancer. Network Load Balancer works on level 4. Remember the Application Load Balancer has more features works on level 7. So if your solution needs to be highly available and cost efficient then consider the Network Load Balancer for that need.</p>
<p>Put the Network Load Balancer in front of EC2 instances in multi AZs. And remember to set up your auto scaling group to add or remove instances automatically, ‘cause that’s going to keep your costs down. Now, a few common re-architecting scenarios. Credentials stored in code a common problem. So if you’re doing a code review and you find database or instance credentials in the code, you need to get them out and to put them into something more secure. So the best solution for that is the AWS Secrets Manager. You’ll need to use perhaps a Lambda function or a similar process to retrieve the credentials from AWS Secrets Manager. So if you do it that way remember you’ll need, that function will need to run using an IAM role. So it’s running securely.</p>
<p>Now, Secrets Manager best fit AWS KMS. Wouldn’t be such a good option for credentials. It’s a service designed for storing encryption keys and you wouldn’t consider cloud HSM ‘cause that’s an a hardware appliance. So it actually attracts quite a high cost. It’s well suited to your hybrid environments where you need to store keys across on prem and in the cloud. So that would be an overkill for managing secrets.</p>
<p>If you need to improve performance in delivering an application or content globally, say your services growing and you reaching new markets then you really need to consider Amazon CloudFront. That’s likely to be a best option for delivering to a wider audience and improving performance. Certainly if you need to geoblock or restrict access to some content, then think CloudFront. If you need to share content to a small group of people say another team or an office then you can use CloudFront for that. So using CloudFront signed URL so the one-time token is a really good way to provide access to resources in a managed way. Certainly better than trying to set up IAM policies on S3 buckets, for example. But you can also block bad actors or IP addresses using CloudFront.</p>
<p>You’re better off using AWS WAF to manage that type of granular access control. But if you don’t have WAF or CloudFront as options to restrict access then you can block access to resources from a specific CIDR range using Network Access Control Lists. That can block a range of IP addresses in the same CIDR range. So it gives you some control, not as much as you would get from WAF or from CloudFront. If you use a NACLs too as a denied method then you need to add a deny rule to the inbound table of your Network Access Control List. And ideally give it a lower number than any other rules that you might have in there.</p>
<p>As architects, we often get asked to meet compliance or auditing requirements. So remember that AWS CloudTrail is usually the best way to record API calls and AWS Config is often the best way to track configuration changes inside your VPC into your account. As architects, we often get asked to increase or design for a performance, decoupling architectures is the best way to start with that. SQS and SNS are perfect for decoupling layers in multi-tiered architectures.</p>
<p>If you need to process tasks like accepting a customer order and perhaps sending a confirmation email, then using SQS and SNS is a very good way to manage that. If you need to support parallel asynchronous processing, so for example, you need to have that process and that email sent at the very same time, then using the SNS Fanout method is a good way of doing that. That’s when you have steps that you want to have process simultaneously, I.e at the same time in parallel. Fanout means having the message published to an SNS topic which is then replicated and pushed to multiple Amazon SQS queues. But it could be any end point. It doesn’t have to be SQS. It could be a Lambda function or a Kinesis Data Firehouse, any HTTPS end point basically.</p>
<p>So if you need to design a customer ordering application to publish a message to an SNS topic, whenever an order is placed for a product, then any SQS queues that are subscribed to that SNS this topic will receive identical notifications for their order. That’s perfect for that scenario. So you could have an EC2 server instance attached to one of the SQS queues, which will handle actual processing of the order. And you can also have an SQS queue to handle notifying the customer that the order has commenced. You could also attach another EC2 server instance to analyzing the order for patterns and behaviors and have activities based on that. So that’s a really scalable way of handling parallel asynchronous processing, even with decoupled multi-tiered web applications, if you’re using all the best practices like DynamoDB, SQS and EC2 to decouple layers, then there’s always small improvements that can be made.</p>
<p>So it can be common when you’re decoupling to see slight delays in processing, which may not be optimal for processing something like our customer web orders. One option to improve performance immediately is to use EC2 auto scaling to scale out the middle tier instances based on the SQS queue depth. That can improve performance immediately.</p>
<p>Now, if you are experiencing a very high volume of incoming messages, let’s say 100,000 a second or more and there are multiple microservices consuming the messages then it could be an option to implement Kinesis Data Streams using a single shard. That may be more performant to have the microservices read and process messages from the Kinesis Stream. If your system is suffering performance degradation due to heavy read traffic or say people running reports, then migrating the database to RDS is always an option. It’s always gonna give you an a performance increase and especially if you’re experiencing a lot of read only SQL queries, then adding a read replica can improve performance with minimal changes to your existing applications.</p>
<p>If you’ve implemented a NoSQL database like DynamoDB then read performance can be increased by adding auto scaling to the table and adding global indexes. You can also consider as another layer adding Elasticache as a cache. That can also improve read performance. Okay, not write just read performance. And if you need features in that cache, then think Redis. If you just need speed, think MemcacheD. All right, so those are a few design patterns to remember and keep in mind.</p>
<p>We’re getting better and better prepared, we’re getting closer to passing this exam, let’s move on.</p>
<h1 id="4Introduction-to-the-Simple-Notification-Service"><a href="#4Introduction-to-the-Simple-Notification-Service" class="headerlink" title="4Introduction to the Simple Notification Service"></a>4<strong>Introduction to the Simple Notification Service</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/serverless-computing-aws-developers-45/">LP: Serverless Computing on AWS for Developers</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/mobile/invoking-aws-lambda-functions-via-amazon-sns/">Invoking AWS Lambda functions via Amazon SNS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/aws-lambda-sns-notifications/">Lab: Process Amazon SNS notifications with AWS Lambda</a></p>
<h1 id="5AWS-Step-Functions"><a href="#5AWS-Step-Functions" class="headerlink" title="5AWS Step Functions"></a>5<strong>AWS Step Functions</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/awslabs/video-on-demand-on-aws">Video on Demand on AWS</a></p>
<h1 id="15What-is-Amazon-Comprehend"><a href="#15What-is-Amazon-Comprehend" class="headerlink" title="15What is Amazon Comprehend?"></a>15<strong>What is Amazon Comprehend?</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/comprehend/latest/dg/how-entities.html">List of supported entities</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/comprehend/latest/dg/how-pii.html">Detect Personally Identifiable Information (PII)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/comprehend/latest/dg/how-syntax.html">List of Syntax Types</a></p>
<h1 id="25Choosing-Between-EC2-and-Serverless"><a href="#25Choosing-Between-EC2-and-Serverless" class="headerlink" title="25Choosing Between EC2 and Serverless"></a>25<strong>Choosing Between EC2 and Serverless</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/">Course: Understanding AWS Lambda to Run &amp; Scale Your Code</a></p>
<h1 id="26Example-Cost-Comparisons"><a href="#26Example-Cost-Comparisons" class="headerlink" title="26Example Cost Comparisons"></a>26<strong>Example Cost Comparisons</strong></h1><p><a target="_blank" rel="noopener" href="https://calculator.aws/">AWS Pricing Calculator</a></p>
<h1 id="28A-Survey-of-Serverless"><a href="#28A-Survey-of-Serverless" class="headerlink" title="28A Survey of Serverless"></a>28<strong>A Survey of Serverless</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">Understanding AWS Lambda to Run &amp; Scale Your Code</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/">Introduction to Microservices, Containers, and ECS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/">Connecting Application Data using Amazon EventBridge</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-step-functions-1117/introduction/">AWS Step Functions</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/">Introduction to the Simple Queue Service</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/">Introduction to the Simple Notification Service</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/">Introduction to API Gateway</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/">Storage Fundamentals for AWS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/">Working with DynamoDB</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/">Amazon Aurora High Availability</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-High-Availability-SAA-C03-30/" rel="prev" title="AWS-Solution-Architect-Associate-Knowledge-Check-High-Availability-SAA-C03-30">
      <i class="fa fa-chevron-left"></i> AWS-Solution-Architect-Associate-Knowledge-Check-High-Availability-SAA-C03-30
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-Preparing-to-Migrate-Servers-with-the-Application-Discovery-Service-32/" rel="next" title="AWS-Solution-Architect-Associate-Preparing-to-Migrate-Servers-with-the-Application-Discovery-Service-32">
      AWS-Solution-Architect-Associate-Preparing-to-Migrate-Servers-with-the-Application-Discovery-Service-32 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Architecture-SAA-C03-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Architecture (SAA-C03) Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-a-Decoupled-and-Event-Driven-Architecture"><span class="nav-number">2.</span> <span class="nav-text">What is a Decoupled and Event-Driven Architecture?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-the-Simple-Queue-Service"><span class="nav-number">3.</span> <span class="nav-text">Introduction to the Simple Queue Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-the-Simple-Notification-Service"><span class="nav-number">4.</span> <span class="nav-text">Introduction to the Simple Notification Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Step-Functions"><span class="nav-number">5.</span> <span class="nav-text">AWS Step Functions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Fundamentals-of-Stream-Processing"><span class="nav-number">6.</span> <span class="nav-text">Fundamentals of Stream Processing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Kinesis-Overview"><span class="nav-number">7.</span> <span class="nav-text">Amazon Kinesis Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#A-Streaming-Framework"><span class="nav-number">8.</span> <span class="nav-text">A Streaming Framework</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-EMR"><span class="nav-number">9.</span> <span class="nav-text">Introduction to EMR</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-Amazon-MSK"><span class="nav-number">10.</span> <span class="nav-text">What is Amazon MSK?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-MSK-and-Kafka-Under-the-Hood"><span class="nav-number">11.</span> <span class="nav-text">Amazon MSK and Kafka Under the Hood</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Provisioning-an-Amazon-MSK-Cluster"><span class="nav-number">12.</span> <span class="nav-text">Provisioning an Amazon MSK Cluster</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Device-Farm-Service"><span class="nav-number">13.</span> <span class="nav-text">Amazon Device Farm Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Pinpoint-Service"><span class="nav-number">14.</span> <span class="nav-text">Amazon Pinpoint Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-Amazon-Comprehend"><span class="nav-number">15.</span> <span class="nav-text">What is Amazon Comprehend?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Fundamentals-of-Amazon-Forecast"><span class="nav-number">16.</span> <span class="nav-text">Fundamentals of Amazon Forecast</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview-of-Amazon-Sagemaker"><span class="nav-number">17.</span> <span class="nav-text">Overview of Amazon Sagemaker</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Architecture-Basics"><span class="nav-number">18.</span> <span class="nav-text">Architecture Basics</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-Multi-Tier-Design-and-When-Should-We-Use-it"><span class="nav-number">19.</span> <span class="nav-text">What is Multi-Tier Design and When Should We Use it?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#When-Should-We-Consider-Single-Tier-Architecture"><span class="nav-number">20.</span> <span class="nav-text">When Should We Consider Single-Tier Architecture?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Designing-a-Multi-Tier-Solution"><span class="nav-number">21.</span> <span class="nav-text">Designing a Multi-Tier Solution</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Connectivity-Within-The-VPC"><span class="nav-number">22.</span> <span class="nav-text">Connectivity Within The VPC</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#When-To-Go-Serverless"><span class="nav-number">23.</span> <span class="nav-text">When To Go Serverless</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Choosing-Between-EC2-and-Serverless"><span class="nav-number">24.</span> <span class="nav-text">Choosing Between EC2 and Serverless</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Example-Cost-Comparisons"><span class="nav-number">25.</span> <span class="nav-text">Example Cost Comparisons</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Serverless-Design-Patterns"><span class="nav-number">26.</span> <span class="nav-text">Serverless Design Patterns</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#A-Survey-of-Serverless"><span class="nav-number">27.</span> <span class="nav-text">A Survey of Serverless</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-3-stage-approach-to-migration"><span class="nav-number">28.</span> <span class="nav-text">The 3-stage approach to migration</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-3-stage-approach-to-migration-1"><span class="nav-number">29.</span> <span class="nav-text">The 3-stage approach to migration</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E2%80%98Assess%E2%80%99-Understanding-your-environment"><span class="nav-number">30.</span> <span class="nav-text">‘Assess’ - Understanding your environment</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E2%80%98Assess%E2%80%99-Understanding-your-environment-1"><span class="nav-number">31.</span> <span class="nav-text">‘Assess’ - Understanding your environment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Migration-Evaluator"><span class="nav-number">31.0.1.</span> <span class="nav-text">Migration Evaluator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Migration-Hub"><span class="nav-number">31.0.2.</span> <span class="nav-text">AWS Migration Hub</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E2%80%98Mobilize%E2%80%99-Migration-and-strategy-planning"><span class="nav-number">32.</span> <span class="nav-text">‘Mobilize’ - Migration and strategy planning</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E2%80%98Mobilize%E2%80%99-Migration-and-strategy-planning-1"><span class="nav-number">33.</span> <span class="nav-text">‘Mobilize’ - Migration and strategy planning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Application-Discovery-Service"><span class="nav-number">33.0.1.</span> <span class="nav-text">AWS Application Discovery Service</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Agent-based-discovery"><span class="nav-number">33.0.1.1.</span> <span class="nav-text">Agent-based discovery</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Agentless-Discovery"><span class="nav-number">33.0.1.2.</span> <span class="nav-text">Agentless Discovery</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AWS-Control-Tower"><span class="nav-number">33.0.1.3.</span> <span class="nav-text">AWS Control Tower</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E2%80%98Migrate-and-Modernize%E2%80%99-Designing-your-solutions-on-AWS"><span class="nav-number">34.</span> <span class="nav-text">‘Migrate and Modernize’ - Designing your solutions on AWS</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E2%80%98Migrate-and-Modernize%E2%80%99-Designing-your-solutions-on-AWS-1"><span class="nav-number">35.</span> <span class="nav-text">‘Migrate and Modernize’ - Designing your solutions on AWS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Application-Migration-Service"><span class="nav-number">35.0.1.</span> <span class="nav-text">AWS Application Migration Service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Database-Migration-Service"><span class="nav-number">35.0.2.</span> <span class="nav-text">AWS Database Migration Service</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-DataSync"><span class="nav-number">35.0.3.</span> <span class="nav-text">AWS DataSync</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Transfer-Family"><span class="nav-number">35.0.4.</span> <span class="nav-text">AWS Transfer Family</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Snow-Family"><span class="nav-number">35.0.5.</span> <span class="nav-text">AWS Snow Family</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Service-Catalog"><span class="nav-number">35.0.6.</span> <span class="nav-text">AWS Service Catalog</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Storage-Gateway"><span class="nav-number">35.0.7.</span> <span class="nav-text">AWS Storage Gateway</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#File-Gateways"><span class="nav-number">35.0.7.1.</span> <span class="nav-text">File Gateways</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Volume-Gateways"><span class="nav-number">35.0.7.2.</span> <span class="nav-text">Volume Gateways</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Architecture-Summary"><span class="nav-number">36.</span> <span class="nav-text">Architecture Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4Introduction-to-the-Simple-Notification-Service"><span class="nav-number">37.</span> <span class="nav-text">4Introduction to the Simple Notification Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5AWS-Step-Functions"><span class="nav-number">38.</span> <span class="nav-text">5AWS Step Functions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#15What-is-Amazon-Comprehend"><span class="nav-number">39.</span> <span class="nav-text">15What is Amazon Comprehend?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#25Choosing-Between-EC2-and-Serverless"><span class="nav-number">40.</span> <span class="nav-text">25Choosing Between EC2 and Serverless</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#26Example-Cost-Comparisons"><span class="nav-number">41.</span> <span class="nav-text">26Example Cost Comparisons</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#28A-Survey-of-Serverless"><span class="nav-number">42.</span> <span class="nav-text">28A Survey of Serverless</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
