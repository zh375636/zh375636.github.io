<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionHello, and welcome to this course on Compute in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification. Before we get started">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Cloud-Practitioner-Compute-CLF-C01-5">
<meta property="og:url" content="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionHello, and welcome to this course on Compute in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification. Before we get started">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T02:03:25.000Z">
<meta property="article:modified_time" content="2022-11-20T22:58:30.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Cloud-Practitioner-Compute-CLF-C01-5 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Cloud-Practitioner-Compute-CLF-C01-5
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:25" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:25-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:30" itemprop="dateModified" datetime="2022-11-20T18:58:30-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Compute in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various compute services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#112;&#x70;&#111;&#114;&#116;&#64;&#99;&#108;&#111;&#x75;&#x64;&#97;&#x63;&#97;&#100;&#101;&#x6d;&#121;&#x2e;&#99;&#x6f;&#109;">&#x73;&#x75;&#112;&#x70;&#111;&#114;&#116;&#64;&#99;&#108;&#111;&#x75;&#x64;&#97;&#x63;&#97;&#100;&#101;&#x6d;&#121;&#x2e;&#99;&#x6f;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Compute services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Compute services in AWS, including:</p>
<ul>
<li>Elastic Compute Cloud, or EC2;</li>
<li>The Elastic Container Service, also known as ECS;</li>
<li>The Elastic Container Registry, or ECR;</li>
<li>The Elastic Container Service for Kubernetes, known as EKS;</li>
<li>AWS Elastic Beanstalk;</li>
<li>AWS Lambda;</li>
<li>AWS Batch; and</li>
<li>Amazon Lightsail.</li>
</ul>
<p>We’ll also introduce the concept of Elastic Load Balancing, along with the different types of load balancers you can provision within the AWS Cloud. And finally we’ll discuss EC2 Auto Scaling and see how it works together with Elastic Load Balancing to help you build robust, highly available web applications.</p>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="What-is-Compute-in-AWS"><a href="#What-is-Compute-in-AWS" class="headerlink" title="What is Compute in AWS?"></a>What is Compute in AWS?</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/products/compute/">Cloud Compute Index</a></p>
<p><strong>Transcript</strong></p>
<p>Hello, and welcome to this very short lecture where we are going to answer the question, what is <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">Compute in AWS</a>? Before we begin to explore Compute services, resources and features, we must first understand what is meant by the term Compute. So what is it? </p>
<p>Put simply, Compute resources can be considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions. So essentially Compute is closely related to common server components, which many of you will already be familiar with, such as CPUs and RAM. With that in mind, a physical server within a data center would be considered a Compute resource, as it may have multiple CPUs and many gigs of RAM to process instructions given by the operating system and applications. </p>
<p>Within AWS, there are a number of different services and features that offer Compute power to provide different functions. Some of these services provide Compute, which can comprise of utilizing hundreds of EC2 instances, or virtual servers, which may be used continuously for months or even years, processing millions upon millions of instructions. On the other end of this scale, you may only utilize a hew hundred milliseconds of Compute resource to execute just a few lines of code within AWS Lambda before relinquishing that Compute power. AWS Lambda is a serverless Compute resource in AWS, and I’ll cover more on this service later in this course. Compute resources can be consumed in different quantities, for different lengths of time across a range of categories, offering a wide scope of performance and benefit options. So it will really depend on your requirements as to which Compute resource you use within AWS. </p>
<p>In this course, we’ll discuss them all, allowing you to decide which is best for your implementation. As a quick high level reference, AWS offers a Cloud Compute Index, which can be found using the <a target="_blank" rel="noopener" href="https://aws.amazon.com/products/compute/">link</a> onscreen. And this shows different examples and scenarios of where you might use different Compute deployment units. That brings me to the end of this very short lecture. Now we are aware of what Compute is, let’s start by looking at some of the services offered by AWS that provide this Compute resource, starting with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/ec2-elastic-compute-cloud/">Elastic Cloud Compute, EC2</a>.</p>
<h1 id="EC2-Elastic-Compute-Cloud"><a href="#EC2-Elastic-Compute-Cloud" class="headerlink" title="EC2 - Elastic Compute Cloud"></a>EC2 - Elastic Compute Cloud</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Blog Post about Shared Responsibility Model and Security Groups</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/create-your-first-amazon-ec2-instance-1/">Lab: Create your first Amazon EC2 Instance (Linux)</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/">Lab: Ceate your first Amazon EC2 Instance (Windows)</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture where I will explain what the EC2 service is and does, and how to configure an EC2 instance, so let’s get started. As EC2 is one of the most common <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> services used within AWS. I will discuss this service in greater detail over the other services that we’ll cover in this course. </p>
<p>EC2 is arguably the first compute service that you will encounter when working with AWS. It allows you to deploy virtual servers within your AWS environment and most people will require an EC2 instance within their environment as a part of at least one of their solutions. There are a number of elements in creating your EC2 instance, which I want to break down and explain. This will hopefully help to define how the service works and answer a number of questions that you may have. The EC2 service can be broken down into the following components. Amazon machine images, AMIs, instant types, instance purchasing options, tenancy, user data, storage options and security. Let’s look at each of these individually. </p>
<p>The first point I want to cover are AMIs, Amazon Machine Images. These are essentially templates of pre-configured EC2 instances which allow you to quickly launch a new EC2 instance based on the configuration within the AMI. This prevents you from having to install an operating system or any other common applications that you might need to install on a number of other EC2 instances. From a high level perspective an AMI is an image baseline that will include an operating system and applications along with any custom configuration. AWS provides a large number of AMIs covering different operating systems from Linux to Red Hat to Microsoft Windows among others. when configuring your EC2 instance, selecting your AMI is the first configuration choice you’ll need to make. You can also create your own AMI images to help you speed up your own deployment. For example you would start with selecting an AWS AMI, let’s say a Linux server. And then once it is up and running you may then need to install a number of your own custom applications and make specific configuration changes. Now if you needed another server to perform the same functionality, you could go through the same process of selecting a Linux AWS AMI, and again manually installing the applications and making your configurations. Or once you have made those changes on the first instance you can then simply create a brand new AMI or template of that instance with all the applications installed and configurations already made. Then if you need another instance of the same configuration all you would need to do is to select your custom AMI as the base image for your instance and it will launch with Linux server, your custom applications already installed and any configurations already made. As you can see this has many benefits and certainly comes in useful when implementing auto scaling. </p>
<p>In addition to both AWS managed and your custom managed AMIs, you could also select an AMI from the AWS marketplace. The AWS marketplace is essentially an online store that allows you to purchase AMIs from trusted vendors like Cisco, Citrix, Alert Logic et cetera. These vendor AMIs may have specific applications and configurations already made, such as instances that are optimized with built-in security and monitoring tools or contained database migration systems. Lastly community AMIs also exists which are a repository of AMIs that have been created and shared by other AWS members. </p>
<p>Let’s now take a look at instance types, once you have selected your AMI from any of the different sources already discussed, you must then select an instance type. An instance type simply defines the size of the instance based on a number of different parameters, these being ECUs. This defines a number of EC2 compute units for instance, vCPUs this is the number of virtual CPUs on the instance. Physical processor, this is the process speed used on the instance. Clock speed, it’s clock speed in gigahertz. Memory, the amount of memory associated. Instance storage this is the capacity of the local instance store volumes available. EBS optimized available, this defines if the instance supports EBS optimized storage or not. Network performance, this shows the performance level of rate of data transfer. IPV6 support, this simply indicates if the instance type supports IPV6. Process architecture this shows the architected type of the processor. AES-NI, this stands for advanced encryption standard new instructions and it shows if the instance supports it for enhanced data protection. AVX this indicates if the instance supports AVX which is advanced vector extensions, which are primary used for applications focused on audio and video, scientific calculations and 3D modeling analysis. And finally Turbo which shows if the instance supports intel turbo boost and AMD turbo core technologies. The key parameters here to primarily be aware of for general usage of an EC2 instance, could be summarized as vCPUs and memory, instant storage and network performance. But obviously this really depends on your actual usage and application. Having this flexibility of variant instances allows you to select the most appropriate size or power of an instance that you need for optimal performance within your applications. These different instance types are categorized into different family types that offer distinct performance benefits, which again helps you to select the most appropriate instance for your needs. Within each of these instance families you will have a range of instant types with varied CPU, memory, storage and network performance et cetera. </p>
<p>These instance families can be summarized as follows. Micro instances, these instances have a low cost against them due to the minimal amount of CPU and memory power that they offer. These are ideal for very low throughput use cases such as low traffic websites. General-purpose, instance types within this family have a balanced mix of CPU memory and storage making them ideal for small to medium databases, tests and development servers and back-end servers. Compute optimized, as the name implies instance types within this family have a greater focus on compute. They have the highest performing processes installed allowing them to be used for high-performance front end servers, web servers, high-performance science and engineering applications and video encoding and batch processing. GPU, GPU stands for graphics processing unit. And so the instances within this family are optimized for graphic intensive applications. FPGA, this family of instances allows you to customize field programmable gate arrays. To create application specific hardware accelerations when used with applications that use massively parallel processing power such as genomics and financial computing. Memory optimized, this family include instance types that are primarily used for large-scale enterprise class in-memory applications, such as performing real time processing of unstructured data. They are also ideal for enterprise applications such as Microsoft SharePoint. These instances of the lowest cost per gigabyte of RAM against all other instance families. Storage optimized, as expected these are optimized for enhanced storage. Instances in this family use SSD backed instant storage for low latency and very high I&#x2F;O, input&#x2F;output performance, including very high IOPS which is input&#x2F;output operations per second. And these are great for analytic workloads and no SQL databases. Data file systems and log processing applications. </p>
<p>Instance purchasing options. You can purchase EC2 instances through a variety of different payment plans. These have been designed to help you save cost by selecting the most appropriate option for your deployment. The different EC2 payment options are as follows, on-demand instances, reserved instances, scheduled instances, spot instances and on-demand capacity reservations. It’s good to be aware of these different options as well having an understanding of these can help you save a considerable amount of money depending on your use case. Let me run through each option to help explain. Starting with on-demand instance</p>
<p>These are EC2 instances that you can launch at any time and have it provisioned and available to you within minutes. You can use this instance for a shorter time or for as long as you need before terminating the instance. These instances have a flat rate and is determined on the instance type selected and is paid by the second. On-demand instances are typically used for short term uses where workloads can be irregular and where workload can be interrupted. Many users of AWS use on-demand instances within their testing and development environments. And when you stop or terminate your on-demand instance you’ll stop paying for the compute resource. </p>
<p>Reserved instances allow you to purchase a discount for an instance type with set criteria for a set period of time in return for a reduced cost compared to on-demand instances. This reduction can be as much as 75%. These reservations against instances must be purchased in either one or three year time frames. Further reductions can be achieved with reserved instances depending on which payment methods you select. There are three options available to you, firstly all upfront. The complete payment for the one or three year reservation is paid. And this offers the largest discount and no further payment is required regardless of the number of hours the instance is used. Partial upfront, here a smaller upfront payment is made and then a discount is applied to all remaining hourse during the term. And finally no upfront, no upfront or partial payments are made and the smallest discount of the three models is applied to all remaining hours in the term. Reserved instances are used for long-term predictable workloads allowing you to make full use of the cost savings to be had when using compute resources offered by EC2. </p>
<p>Scheduled instances, these are similar to reserved instances and the fact that you pay for the reservations of an instance on a recurring schedule, either daily, weekly or monthly. For example you might have a weekly task that is scheduled that performs some kind of bulk processing for a number of hours at the same time every week. With scheduled instances you could set up a scheduled instance to run during that set timeframe once a week. And this prevents you for having to use the on-demand instances which would incur a higher price. You should note that when using scheduled instances but even if you didn’t use the instance you would still be charged. This allows you to provision instances for scheduled workloads that are not continuously running. Which is where a reserved instance would be the preferred choice. </p>
<p>Spot instances allows you to bid for unused EC2 compute resources, however your resource is not guaranteed for a fixed period of time. To you to spot instance you must bid higher than the current spot price which is set by AWS. And this spot price fluctuates depending on supply and demand of the unused resource. If your bid price for an instance type is higher than the spot price, then you’ll purchase that instance. But as soon as your bid price becomes lower than the fluctuating spot price, you will be issued a two-minute warning before the instance automatically terminates and is removed from your AWS environment. The bonus for spot instances is that you can bid for large EC2 instances at a very low cost point saving a huge amount on cost. Due to the nature of how the instances can be suddenly removed from your environment, spot instances are only useful for processing data and applications that can be suddenly interrupted. Such as batch jobs and background processing of data. </p>
<p>Capacity reservations allows you to reserve capacity for your EC2 instances based on different attributes. Such as instance type, platform and tenancy et cetera. Within a particular availability zone for any period of time. This ensures that you always have the available number of instances you require within a specific availability zone immediately. This capacity reservation could also be used in conjunction with your reserved instances discount providing you additional savings. </p>
<p>Let me now talk to you about EC2 tenancy and this relates to what underlying host your EC2 instance will reside on. So essentially the physical server within an AWS data center. Again there are different options available to you with pros and cons to each. Shared tenancy, this option will launch your EC2 instance on any available host with the specified resources required for your selected instance type. Regardless of which other customers and users also have EC2 instances running on the same host, hence the share tenancy name. AWS implement advanced security mechanisms to prevent one EC2 instance from accessing another on the same host. How the security is applied and operated is out of scope of this course and it is maintained by AWS themselves. Dedicated tenancy, this includes both dedicated instances and dedicated hosts. Dedicated instances are hosted on hardware that no other customer can access. It can only be accessed by your own AWS account. You may be required to launch your instances as a dedicated instance due to internal security policies or external compliance controls. Dedicated instances do incur additional charges due to the fact you are preventing other customers from running EC2 instances on the same hardware and so there will likely be unused capacity remaining. However the hardware might be shared by other resources you have running in your own account. Dedicated host, a dedicated host is effectively the same as dedicated instances. However they offer additional visibility and control, how you can place your instances on the physical host. They also allow you to use your existing licenses, such as PA-VM license or Windows Server licenses et cetera. Using dedicated hosts give you the ability to use the same host for a number of instances that you want to launch and align with any compliance and regulatory requirements. If you don’t need to address any compliance or security issues that require dedicated tenancy, then I recommend using shared tenancy to reduce your overall costs. </p>
<p>User data, during the configuration of your EC2 instance there is a section called user data. Which allows you to enter commands that will run during the first boot cycle of the instance. This is a great way to automatically perform functions upon boot, such as to pull down any additional software you want installing from any software repositories you may have. You could also download and get the latest OS updates during boot. For example you could enter yum update dash y, for a Linux instance which will then update its own software automatically at the time of boot. Storage options, as a part the configuration when setting up an EC2 instance, you are asked to select and configure your storage requirements. </p>
<p>Selecting storage for your EC2 instance will depend on the instance selected, what you intend to use the instance for and how critical the data is. Storage for EC2 can be classified between two distinct categories, persistent storage and ephemeral storage. Ephemeral meaning temporary. Persistent storage is available by attaching elastic block storage EBS volumes. And a ephemeral storage is created by some EC2 instances themselves using a local storage on the underlying host known as instance back storage. Let’s look at each of these storage options in greater depth. EBS volumes are separate devices from the EC2 instance itself. And so it’s not physically attached like ephemeral storage is. EBS volumes are considered network attached storage devices which are then logically attached to the EC2 instance via the AWS network. This principle is not dissimilar to attaching an external hard disk to your home laptop or PC. With the external hard disk represent your EBS volume and your PC represents your EC2 instance. The data on EBS volumes are automatically replicated to other EBS volumes within the same availability zone for resiliency which is managed by AWS. You can disconnect an EBS volume from your EC2 instance and the data will remain intact. Allowing you to reattach it to another EC2 instance if required. You can also implement encryption on these volumes if needed and take backup snapshots of all the data on the volume to S3. EBS volumes can be created in different sizes again with different performance capabilities depending on your requirements. </p>
<p>Ephemeral storage or instance backed storage is the storage that is physically attached to the underlying host on which the EC2 instance resides on. Looking back at our previous example, this would be similar to your own laptop or PC’s hard disk. There is a difference here though, with AWS EC2 instances as soon as the instance is stopped or terminated all saved data on a ephemeral storage is lost. If you reboot your instance then the data will remain but not if you stop it. Therefore if you have data that you need to retain it is not recommended that you use instance backed storage for this data. Instead use EBS volumes for persistent data storage. Unlike EBS volumes you are unable to detach ephemeral instance store volumes from the instance. </p>
<p>Security, security is fundamental with any AWS deployment. As so I just want to highlight a couple of points relating specifically to EC2 security. Firstly and during creation of your EC2 instance you will be asked to select a security group for your instance. A security group is essentially an instance level firewall allowing you to restrict both ingress and egress traffic by specifying what traffic allowed to communicate with it. You can restrict this communication by source ports and protocols for both inbound and outbound communication. Your instances are then associated with this security group. More information on security groups can be found in my blog post, found here covering instance level security. At the very end of your EC2 instance creation, you will need to select an existing key pair or create and download a new one. But what is a key pair and what is it used for? A key pair, as the name implies, is made up of two components, a public key and a private key. The function of key pairs is to encrypt the login information for Linux and Windows EC2 instances. And then decrypt the same information allowing you to authenticate onto the instance. The public key encrypts data such as the username and password. For Windows instances, the private key is used to decrypt this data allowing you to gain access to the login credentials including the password. For Linux instances the private key is used to remotely connect onto the instance via SSH. The public key is held and kept by AWS, and the private key is your responsibility to keep and ensure that it is not lost or compromised. So going back to when you create your EC2 instance and a new key pair. You’re given the opportunity to download the key pair, once you have done this you must keep that file safe until you’re ready to log on to the associated EC2 instance. It’s worth noting that you can use the same key pair on multiple instances to save you managing multiple private keys. Do bear in mind however should the private key become compromised access could be gained to all the instances where that key pair was used. Once you have authenticated to the EC2 instance the first time, you can set up additional less privileged access controls such as local windows accounts allowing other users to connect and authenticate to or even utilize Microsoft Active Directory. One final point regarding security on your EC2 instance it is your responsibility to maintain and install the latest OS and security patches released by the OS vendor as dictated within the AWS shared responsibility model. More information on this can be found in this blog post. </p>
<p>We have now covered the main elements of the EC2 service that should hopefully allow you to get started by creating your first EC2 instance and selecting the most appropriate configuration for your needs. But to reiterate what we have covered and make it all fit together, I will demonstrate how to create a new EC2 instance from within the console, quickly highlighting the elements we have discussed as I go through. </p>
<p>Okay so I’m logged into my AWS management console, and to start with we need to go to EC2 which is under the compute category. Now this take us to the EC2 console and from here we can simply select launch instance. Now this is the first stage of the configuration where we have to select our Amazon Machine Image, AMI. And here are a number of AWS AMIs that they supply, covering Windows, Red Hat, Linux et cetera. On the left hand side there’s just a few other options if you’ve created any AMIs yourself that would be stored here. I mentioned the AWS marketplace earlier and here you can see lots of different AMIs from other suppliers such as Trend Micro, Juniper Networks, Barracuda et cetera. And also the community AMI as well. So let’s get started with the Quick Start and look at some of the AWS supplied AMIs and I’m just going to launch this Amazon Linux box. So I’ll select that as my AMI, now we get to choose instance type. And we can filter up here with different types of instance types, general purpose, computer optimized et cetera. Just going to leave it as all, and then down here we can see the different families and the types, the vCPUs, memory et cetera that each of these has. So I’m going to leave it on the T2 micro general purpose instance. </p>
<p>So I’m going to select next configure instance details. Now I have a number of options here, the number of instances that we want to launch. I’m just going to keep it as one. If we want to launch this as a spot instance then we can select this box here to do so. But I’m just going to create it as an on-demand instance. We can select VPC that we want to run it in and we’ll have different VPCs there. You can then select the subnet if you’d like. And if you’d like to auto-assign a public IP address. We can assign a role to an EC2 instance if we need to and we can also control the shutdown behavior. So when we shut down all EC2 instance, do you want to terminate that instance or just stop. I’m going to leave that as a default or stop. There’s a couple of other controls you can put in here, enable termination protection. And what that will do, that will prevent you from terminating your instance until you uncheck this box. And you can also enable detail cloud watch monitoring if you need to. With regards to tenancy we discussed this earlier rather we shared or one of the dedicated instance or on a dedicated host, I’m going to leave that as shared. I’ll leave all the other options as default and then I’ll click on next to go to storage. This shows the current storage that comes with the AMI. We consider there’s 8 gig in size and it’s a general-purpose SSD drive. It’s a tick box here to delete the volume on termination and we can see that it’s not encrypted. If we wanted to add a new volume, we can add an EBS volume here. Again we can specify the size, eight gig or if you want to add it to 30 for example. And then we can also again delete on termination and we have the option to encrypt the EBS volume if we wanted to it’s by selecting the default AWS EBS encryption key. So now we have two drives, one of them is the root volume and an additional drive which is an EBS volume. </p>
<p>Click on next add tags, here we can add a key value pair tag to this instance. So for example a key of name and a value of my instance. And we can add additional tags as well as you can see here we can add up to 50 tags if we wanted to. So we can add a project that it belongs to or the cost et cetera, any tags that make it more usable to you. Once you’ve finished adding your tags click on configure security group. And this is where we control what can and can’t access your instance. You can create a new secure group or select an existing security group that you might already have. If we create a new security group and call it My Security Group, then you can add a description. And here are the rules for the security group. So at the moment we have SSH using TCP across port 22 and can you have the source as a custom IP address range or a single IP address. So you might just want a specific subnet in your VPC to talk to this instance or you can have anywhere, or you can have just your own IP address. Let’s put in a custom IP address range of 10.0.1.0&#x2F;24. And again you can add a description in there if you want to. You can add a new rule, for example HTTP traffic. And again you can add your source, cite anywhere then once you’re happy with your security groups click on review and launch. And this just provides a summary of all the configuration options that you’ve made up to this point. If you need to edit any other details you can just click on the right hand side here, edit the instance type the security groups et cetera. Once you’re happy with all of your information click on launch. And this is where you can select or create a new key pair to connect your instance. </p>
<p>So let’s go ahead and create a new key pair, call it My Instance, and now I need to download this key pair. Which will download the private part, once that key pair is downloaded I can then click on launch instance. Now if we go back to our dashboard by clicking on view instances. We can see here that it’s trying to launch at the minute, the status is pending and it shouldn’t take too long for that to become active. And here we go we can now see it’s up and running. </p>
<p>Before I finish this demonstration I just want to point out one last point relating to status checks that we can see from within our EC2 dashboard. These status checks are used to check the health and status of your EC2 instance and understanding what kind of faults could trigger these checks to fail. Kinda help you troubleshoot issues with your EC2 resources. There are two types of status checks. System status checks and instance status checks. If the system status check fails then it is likely to be an issue with the underlying host rather than a configuration issue with your EC2 instance. Common issues that trigger system status checks to fail are loss of power, loss of network connectivity and hardware and software issues on the underlying host. Basically a system status check failure is out of our control as the fault lies with components that AWS are responsible for. The best way to resolve this will be to stop the instance and restart. This is likely to cause the instance to launch on another physical host resolving the problem. Do not reboot the instance as this will cause the instance to continue running on the same physical server. Instance status checks, these differ from system status checks as if this fails then it would likely require your input to help them resolve the issue. This check looks at the EC2 instance itself, rather than focusing on the underlying hosts. Common issues that trigger these checks to fail are incorrect network configuration, corrupted file systems, exhausted memory or incompatible kernel. These faults will require you to troubleshoot and resolve the issue, for example changing the network configuration. If you’d like some hands-on experience with EC2, then we do offer two labs in which you can practice creating your own EC2 instances for both Linux and Windows. </p>
<p>That now brings me to the end of this lecture on EC2, like I mentioned previously this is going to be the longest and most in-depth lecture, simply due to how much of a key compute service it is in a wide range of use cases.</p>
<h1 id="ECS-Elastic-Container-Service"><a href="#ECS-Elastic-Container-Service" class="headerlink" title="ECS - Elastic Container Service"></a>ECS - Elastic Container Service</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/">Introduction to Docker</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/basics-of-using-containers-in-production/">Basics of using Containers in Production</a></p>
<p><strong>Transcript</strong></p>
<p>Hello, and welcome to this short lecture which will provide a high-level overview of the Amazon EC2 Container Service, commonly known as Amazon ECS. This service allows you to run Docker-enabled applications packaged as containers across a cluster of EC2 instances without requiring you to manage a complex and administratively heavy cluster management system. The burden of managing your own cluster management system is abstracted with the Amazon ECS service by passing that responsibility over to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a>, specifically though the use of AWS Fargate. </p>
<p>If you’re new to some of these terms such as Docker, containers, and AWS Fargate then let me quickly, in a single sentence, define what they are to help you understand this service a little easier. AWS Fargate is an engine used to enable ECS to run containers without having to manage and provision instances and clusters for containers. Docker is piece of software that allows you to automate the installation and distribution of applications inside Linux Containers. So what are containers? A Container holds everything that an application requires to enable it to run from within it’s isolated container package. This may include system libraries, code, system tools, run time, etcetera. But it does not include an operating system like a virtual machine does, and so reduces overhead of the actual container itself. </p>
<p>Containers are decoupled from the underlying operating system, making Container applications very portable, lightweight, flexible, and scalable across a cloud environment. This ensures that the application will always run as expected regardless of it’s deployment location. With this in mind, if you are already using Docker, or have existing containerized applications packaged locally, then these will work seamlessly on Amazon ECS. For more information on Docker and Containers, please see our existing content found here. Let’s now take a deeper look at the EC2 Container Service and some of the additional functions that it provides. </p>
<p>As I mentioned before, EC2 Container Service removes the need for you to manage your own cluster management system thanks to its interactions with AWS Fargate. You don’t even have to specify which instance type to use. This can be very time consuming and requires a lot of overhead to continue to monitor and maintain and scale. With Amazon ECS there is no need to install any management software for your cluster, neither is there a need to install any monitoring software either. All of this, and more, is taken care of by the service, allowing you to focus on building great applications and deploying them across your scalable cluster. </p>
<p>When launching your ECS cluster you have the option of two different deployment models: a Fargate launch and an EC2 launch. The Fargate launch requires far less configuration and simply requires you to specify the CPU and memory required, define the networking and IAM policies in addition to you having to package your applications into containers. However, with an EC2 launch you have a far greater scope of customization and configurable parameters. For example, you are responsible for patching and scaling your instances, and you can specify which instance types you used, and how many containers should be in a cluster. </p>
<p>There are use cases for both modes. You may need more granularity and control with some of your clusters due to security and compliance controls. Monitoring is taken care of through the use of AWS CloudWatch, which will monitor metrics against your containers and your cluster. Those of you who have used CloudWatch before will be aware you can easily create alarms based off of these metrics providing you notification of when specific events occur such as your cluster size scaling up or down. An Amazon ECS cluster is comprised of a collection of EC2 instances. As such, some of the functionality and features that we’ve already discussed in this course can be used with these instances. For example Security Groups to implement instance level securely at a port and protocol level, along with Elastic Load Balancing and Auto Scaling. Although these EC2 instances form a cluster, they still operate in much the same way as a single EC2 instance. So again, for example, should you need to connect to one of your instances itself, you could still use the same familiar methods such as initiating an SSH connection. </p>
<p>The clusters themselves act as a resource pool, aggregating resources such as CPU and memory. The cluster is dynamically scalable, meaning you can start your cluster as a single small instance, but it can dynamically scale to thousands of larger instances. Multiple instance types can be used within the cluster if required. Although the cluster is dynamically scalable, it’s important to point out that it can only scale within a single region. Amazon ECS is region-specific, so it can span multiple availability zones, but it cannot span multiple regions. With ECS you can schedule your containers to be deployed across your cluster based on different requirements, such as resources requirements or specific availability requirements, through the use of multiple availability zones. The instances within the Amazon ECS cluster also have a Docker daemon and an ECS agent installed. These agents communicate with each other allowing Amazon ECS commands to be translated into Docker commands.</p>
<h1 id="ECR-Elastic-Container-Registry"><a href="#ECR-Elastic-Container-Registry" class="headerlink" title="ECR - Elastic Container Registry"></a>ECR - Elastic Container Registry</h1><h3 id="Resources-referenced-within-this-lecture"><a href="#Resources-referenced-within-this-lecture" class="headerlink" title="Resources referenced within this lecture:"></a><strong>Resources referenced within this lecture:</strong></h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Overview of AWS Identity &amp; Access Managment (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Docker Push</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">Docker Pull</a></p>
<h3 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a><strong>Transcript</strong></h3><p>Hello and welcome to this lecture covering the Elastic Container Registry service, known as ECR. This service links closely with the previous service discussed, the EC2 Container Service, as it provides a secure location to store and manage your docker images that can be distributed and deployed across your applications. </p>
<p>This is a fully managed service, and as a result, you do not need to provision any infrastructure to allow you to create this registry of docker images. This is all provisioned and managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a>. This service is primarily used by developers, allowing them to push, pull, and manage their library of docker images in a central and secure location. </p>
<p>To understand the service better, let’s look at some components used. These being, registry, authorization token, repository, repository policy, and image. Let’s take a look at the registry first. The ECR registry is the object that allows you to host and store your docker images in, as well as create image repositories. Within your AWS account, you will be provided with a default registry. When your registry is created, then by default, the URL for the registry is as follows:</p>
<p><a target="_blank" rel="noopener" href="https://aws_account_id.dkr.ecr.region.amazonaws.com/">https://aws_account_id.dkr.ecr.region.amazonaws.com</a></p>
<p>where you’ll need to replace the red text with your own information that is applicable to your account or medium. Your account will have both read and write access by default to any images you create within the registry and any repositories. Access to your registry and images can be controlled via IAM policies in addition to repository policies as well, to enforce tighter and stricter security controls. As the docker command line interface doesn’t support the different AWS authentication methods that are used, then before your docker client can access your registry, It needs to be authenticated as an AWS user, which will then allow your client to both push and pull images. And this is done by using an authorization token. To begin the authorization process to allow your docker client to communicate with the default registry, you can run the get login command using the AWS CLI, as shown:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws ecr get-login --region region --no-include-email</span><br></pre></td></tr></table></figure>

<p>where the red text should be replaced with your own region. This will then produce an output response, which will be a docker login command.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u AWS -p password https://aws_account_id.dkr.ecr.region.amazonaws.com</span><br></pre></td></tr></table></figure>

<p>You must then copy this command and paste it into your docker terminal which will then authenticate your client and associate a docker CLI to your default registry. This process produces an authorization token that can be used within the registry for 12 hours, at which point, you will need to re-authenticate by following the same process. The repository are objects within your registry that allow you to group together and secure different docker images. You can create multiple repositories with the registry, allowing you to organize and manage your docker images into different categories. </p>
<p>Using policies from both IAM and repository policies, you can assign permissions to each repository allowing specific users to perform certain actions, such as performing a push or pull IP line. As I just mentioned, you can control access to your repository and images using both IAM policies and repository policies. There are a number of different IAM managed policies to help you control access to ECR, these being the three shown on the screen.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AmazonEC2ContainerRegistryFullAccess</span><br><span class="line">AmazonEC2ContainerRegistryPowerUser</span><br><span class="line">AmazonEC2ContainerRegistryReadOnly</span><br></pre></td></tr></table></figure>

<p>For more information on IAM and policies, please refer to our system course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">here</a>, which covers IAM and policy creation and management. Repository policies are resource-based policies, which means you need to ensure you add a principle to the policy to determine who has access and what permissions they have. It’s important to be aware of that for an AWS user to gain access to the registry, they will require access to the ecr get authorization token API call. Once they have this access, repository policies can control what actions those users can perform on each of the repositories. These resource-based policies are created within ECR itself and within each other repositories that you have. Once you have configured your registry, repositories, and security controls, and authenticated your docker client with ECR, you can then begin storing your docker images in the required repositories, ready to then pull down again as and when required. </p>
<p>To push an image into ECR, you can use the docker push command, and to retrieve and image you can use the docker pull command. For more information on how to perform both a push and a pull of images, please see the following links.</p>
<p><strong>Docker Push</strong>: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</a></p>
<p><strong>Docker Pull</strong>: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html</a></p>
<p>That now brings me to the end of this lecture covering the Elastic Container Registry service. Coming up in the next lecture, I shall be looking at the Amazon <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/eks-elastic-container-service-kubernetes/">Elastic Container Service for Kubernetes</a>, known as EKS.</p>
<h1 id="EKS-Elastic-Container-Service-for-Kubernetes"><a href="#EKS-Elastic-Container-Service-for-Kubernetes" class="headerlink" title="EKS - Elastic Container Service for Kubernetes"></a>EKS - Elastic Container Service for Kubernetes</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-kubernetes/">Introduction to Kubernetes</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Install Kubectl</a></p>
<p>IAM Authenticator:</p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/aws-iam-authenticator">Linux</a></p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator">MacOS</a></p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/windows/amd64/aws-iam-authenticator.exe">Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">Configuration map to joing the Worker Node to the EKS Cluster</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">Introduction to EKS</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture covering the Elastic Container Service for Kubernetes, more commonly known as EKS.</p>
<p>Firstly, for those unfamiliar with Kubernetes let me briefly explain what it is at a high level.  Kubernetes is an open-source container orchestration tool designed to automate, deploy, scale, and operate containerized applications. It is designed to grow from tens, thousands, or even millions of containers. Kubernetes is also container-runtime agnostic, which means you can actually use Kubernetes to run rocket and docker containers.</p>
<p>So back to EKS, with EKS, AWS provides a managed service allowing you to run Kubernetes across your AWS infrastructure without having to take care of provisioning and running the Kubernetes management infrastructure in what’s referred to as the control plane. You, the AWS account owner, only need to provision and maintain the worker nodes.</p>
<p>What is a control plane and what are worker nodes?</p>
<p>Kubernetes Control Plane:</p>
<p>There are a number of different components that make up the control plane and these include a number of different APIs, the kubelet processes and the Kubernetes Master, and these dictate how kubernetes and your clusters communicate with each other.  The control plane itself is run across master nodes.</p>
<p>The control plane schedules containers onto nodes. The term scheduling does not refer to time in this context. Scheduling, in this case, refers to the decision process of placing containers onto nodes in accordance with their declared, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> requirements.  The Control Plane also tracks the state of all kubernetes objects by continually monitoring the objects. So in EKS, AWS is responsible for provisioning, scaling and managing the control plane and they do this by utilising multiple availability zones for additional resilience.</p>
<p>Worker nodes:</p>
<p>Kubernetes clusters are composed of nodes and the term cluster refers to the aggregate of all of the nodes.  A node is a worker machine in Kubernetes and runs as an on-demand EC2 instance and includes software to run containers managed by the Kubernetes control plane.  For each node created, a specific AMI is used which also ensures docker and kubelet in addition to the AWS IAM authenticator is installed for security controls. These nodes are what us as the customer are responsible for managing within EKS.  Once the worker nodes are provisioned they can then connect to EKS using an endpoint.</p>
<p>For more information on Kubernetes, please see our existing course ‘Introduction to Kubernetes’ here</p>
<p>Let me provide a brief overview of what’s required to start using the EKS service.</p>
<ol>
<li><p>Create an EKS Service Role: Before you begin working with EKS you need to configure and create an IAM service-role that allows EKS to provision and configure specific resources.  This role only needs to be created once and can be used for all other EKS clusters created going forward. The role needs to have the following permissions policies attached to the role: AmazonEKSServicePolicy and AmazonEKSClusterPolicy</p>
</li>
<li><p>Create an EKS Cluster VPC: Using AWS CloudFormation you need to create a and run a CloudFormation stack based on the following template: <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml</a> which will configure a new VPC for you to use with EKS</p>
</li>
<li><p>Install kubectl and the AWS-IAM-Authenticator: Kubectl is a command line utility for Kubernetes and can be installed following the details supplied here The IAM-Authenticator is required to authenticate with the EKS cluster.  Depending on your client OS (Linux, MacOS or Windows) it can be downloaded from here:</p>
</li>
<li><p>Create your EKS Cluster: Using the EKS console you can now create your EKS cluster using the details and information from the VPC created in step 1 and 2</p>
</li>
<li><p>Configure kubectl for EKS: Using the update-kubeconfig command via the AWS CLI you need to create a kubeconfig file for your EKS cluster</p>
</li>
<li><p>Provision and configure Worker Nodes: Once your EKS cluster shows an ‘Active’ status you can launch your worker nodes using CloudFormation based on the following template: <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml</a></p>
</li>
<li><p>Configure the Worker Node to join the EKS Cluster: Using a configuration map downloaded here:</p>
</li>
</ol>
<p>curl -O <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml</a></p>
<p>You must edit it and Replace the &lt;ARN of instance role (not instance profile)&gt; with the NodeInstanceRole value from step 6</p>
<p>Your EKS Cluster and worker nodes are now configured ready for your to deploy your applications with Kubernetes.</p>
<p>For more information on EKS, please see our existing course ‘Introduction to EKS’ which will cover these points and more in greater detail <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">https://cloudacademy.com/course/introduction-to-aws-eks/</a></p>
<h1 id="AWS-Elastic-Beanstalk"><a href="#AWS-Elastic-Beanstalk" class="headerlink" title="AWS Elastic Beanstalk"></a>AWS Elastic Beanstalk</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/deploy-php-application-using-elastic-beanstalk-26/">Lab: Deploy a PHP Application using AWS Elastic Beanstalk</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/run-controlled-deploy-aws-elastic-beanstalk-43/">Lab: Run a controlled deploy with AWS Elastic Beanstalk</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture on AWS Elastic Beanstalk. AWS Elastic Beanstalk is an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a> managed service that allows you to upload the code of your web application, along with the environment configurations, which will then allow Elastic Beanstalk to automatically provision and deploy the appropriate and necessary resources required within AWS to make the web application operational. These resources can include other AWS services and features, such as EC2, Auto Scaling, application health-monitoring, and Elastic Load Balancing, in addition to capacity provisioning. This automation and simplification makes it an ideal service for engineers who may not have the familiarity or the necessary skills within AWS to deploy, provision, monitor, and scale the correct environment themselves to run the developed applications. Instead, this responsibility is passed on to AWS Elastic Beanstalk to deploy the correct infrastructure to run the uploaded code. This provides a simple, effective, and quick solution to deploying your web application. </p>
<p>Once the application is up and running, you can continue to support and maintain the environment as you would with a custom built environment. You can additionally perform some of the maintenance tasks from the Elastic Beanstalk dashboard itself. Elastic Beanstalk is able to operate with a variety of different platforms and programming languages, making it a very flexible service for your DevOps teams. Currently at the time of writing this course, Elastic Beanstalk is compatible with the following. One important point to note is that the service itself is free to use. There is no cost associated with Elastic Beanstalk, however, any resources that are created on your application’s behalf, such as EC2 instances, you will be charged for as per the standard pricing policies at the time of deployment. </p>
<p>So now we know at a high level what AWS Elastic Beanstalk is and does, let me run through some of its core components that creates the service. The application version. An application version is a very specific reference to a section of deployable code. The application version will point typically to S3, simple storage service to where the deployable code may reside. </p>
<p>The environment. An environment refers to an application version that has been deployed on AWS resources. These resources are configured and provisioned by AWS Elastic Beanstalk. At this stage, the application is deployed as a solution and becomes operational within your environment. The environment is comprised of all the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code. </p>
<p>Environment configurations. An environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave. The environment tier. This component reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. If the application manages and handles HTTP requests, then the app will be run in a web server environment. If the application does not process HTTP requests, and instead perhaps pulls data from an SQS queue, then it would run in a worker environment. I shall cover more on the differences between the web server and work environment shortly. </p>
<p>The configuration template. This is the template that provides the baseline for creating a new, unique environment configuration. Platform. The platform is a culmination of components in which you can build your application upon using Elastic Beanstalk. These comprise of the operating system of the instance, the programming language, the server type, web or application, and components of Elastic Beanstalk itself, and as a whole can be defined as a platform. Applications. Within Elastic Beanstalk, an application is a collection of different elements, such as environments, environment configurations, and application versions. In fact, you can have multiple application versions held within a single application. You can deploy your application across one of two different environment tiers, either the web server tier or the worker tier. </p>
<p>These tiers are configured differently depending on the use case of your application. The web server environment is typically used for standard web applications that operate and serve requests over HTTP port 80. This tier will typically use service and features such as Route 53, Elastic Load Balancing, Auto Scaling, EC2, and Security Groups. The worker environment is slightly different and are used by applications that will have a back-end processing task that will interact with AWS SQS, the Simple Queue Service. This tier typically uses the following AWS resources in this environment, an SQS Queue, an IAM Service Role, Auto Scaling, and EC2. </p>
<p>Now you are aware of some of the terminology and components, we can look at how AWS Elastic Beanstalk operates a very simple workflow process for your application deployment and ongoing management in what can be defined in four simple steps. Firstly, you create an application. Next, you must upload your application version of the application to Elastic Beanstalk, along with some additional configuration information regarding the application itself. This creates the environment configuration. The environment is then created by Elastic Beanstalk with the appropriate resources to run your code. Any management of your application can then take place, such as deploying new versions of your application. If the management of your applications have altered the environment configuration, then your environment will automatically be updated to reflect the new code should additional resources be required. For further information and to get some hands-on experience with AWS Elastic Beanstalk to deploy an application, take a look at our two labs which will guide you through the steps and processes we have discussed. </p>
<p>That now brings us to the end of this lecture. Coming up next, I will introduce you to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/aws-lambda/">AWS Lambda</a>.</p>
<h1 id="AWS-Lambda"><a href="#AWS-Lambda" class="headerlink" title="AWS Lambda"></a>AWS Lambda</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html#supported-event-source-s3">AWS Lambda Event Sources</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/">Understanding AWS Lambda to Run and Scale your Code</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/introduction-aws-lambda-22/">Lab: Introduction to AWS Lambda</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/aws-lambda-s3-events-55/">Lab: Process Amazon S3 events with AWS Lambda</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/automating-ebs-snapshots-lambda-and-cloudwatch-events-45/">Lab: Automating EBS snapshots with AWS Lambda</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture where we shall take an introductory look at AWS Lambda. AWS Lambda is a serverless <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> service which has been designed to allow you to run your application code without having to manage and provision your own EC2 instances. This saves you having to maintain and administer an additional layer of technical responsibility within your solution. Instead, that responsibility is passed over to AWS to manage for you. </p>
<p>Essentially, serverless means that you do not need to worry about provisioning and managing your own compute resource to run your own code, instead this is managed and provisioned by AWS. AWS will start, scout, maintain, and stop the compute resources as required, which can be just as short as just a few milliseconds. Although it’s named serverless, it does of course require service, or at least compute power, to carry out your code requests, but because the AWS user does not need to be concerned with what’s managing this compute power, or where it’s provisioned from, it’s considered serverless from the user perspective. </p>
<p>If you don’t have to spend time operating, managing, patching, and securing an EC2 instance, then you have more time to focus on the code of your application and its business logic, while at the same time, optimizing costs. With AWS Lambda, you only ever have to pay for the compute power when Lambda is in use via Lambda functions. And I shall explain more on these later. </p>
<p>AWS Lambda charges compute power per 100 milliseconds of use only when your code is running, in addition to the number of times your code runs. With sub-second metering, AWS Lambda offers a truly cost optimized solution for your serverless environment. So how does it work? Well there are essentially four steps to its operation. </p>
<p>Firstly, AWS Lambda needs to be aware of your code that you need run so you can either upload this code to AWS Lambda, or write it within the code editor that Lambda provides. Currently, AWS Lambda supports Notebook.js, JavaScript, Python, Java, Java 8 compatible, C#, .NET Core, Go, and also Ruby. It’s worth mentioning that the code that you write or upload can also include other libraries. Once your code is within Lambda, you need to configure Lambda functions to execute your code upon specific triggers from supported event sources, such as S3. As an example, a Lambda function can be triggered when an S3 event occurs, such as an object being uploaded to an S3 bucket. Once the specific trigger is initiated during the normal operations of AWS, AWS Lambda will run your code, as per your Lambda function, using only the required compute power as defined. Later in this course I’ll cover more on when and how this compute power is specified. AWS records the compute time in milliseconds and the quantity of Lambda functions run to ascertain the cost of the service. </p>
<p>For an AWS Lambda application to operate, it requires a number of different elements. The following form the key constructs of a Lambda application. Lambda function. The Lambda function is compiled of your own code that you want Lambda to invoke as per defined triggers. Event source. Event sources are AWS services that can be used to trigger your Lambda functions, or put another way, they produce the events that your Lambda function essentially responds to by invoking it. For a comprehensive list of these event sources, please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html#supported-event-source-s3">link</a> on the screen. Trigger. The Trigger is essentially an operation from an event source that causes the function to invoke. So essentially triggering that function. For example, an Amazon S3 put request could be used as a trigger. Downstream Resources. These are the resources that are required during the execution of your Lambda function. For example, your function might call upon accessing a specific SNS topic, or a particular SQS queue. So they are not used as the source of the trigger, but instead they are the resources to be used to execute the code within the function upon invocation.</p>
<p>Log streams. In an effort to help you identify issues and troubleshoot issues with your Lambda function, you can add logging statements to help you identify if your code is operating as expected into a log stream. These log streams will essentially be a sequence of events that all come from the same function and recorded in CloudWatch. In addition to log streams, Lambda also sends common metrics of your functions to CloudWatch for monitoring and alerting. At a high level, the configuration steps for creating a Lambda function via the AWS Management Console could consist of selecting a blueprint, and AWS Lambda provides a large number of common blueprint templates which are preconfigured Lambda functions. To save time on your own code, you can select one of these blueprints and then customize it as necessary. An example of one of these blueprints could be the S3 get object, which is an Amazon S3 trigger that retrieves metadata for the object that is being updated. You then need to configure your triggers, and as I just explained, the trigger is an operation from an event source that causes the function to invoke and in my previous statement, I suggested an S3 put request. And then you need to finish configuring your function. And this section requires you to either upload your code or edit it in-line and it also requires you to define the required resources, the maximum execution timeout, the IAM Role, and Handler Name. </p>
<p>A key benefit of using AWS Lambda is that it is a highly scalable serverless service, coupled with fantastic cost optimization compared to EC2 as you are only charged for Compute power while the code is running and for the number of functions called. For more information on AWS Lambda and how to configure it in detail, can be found in our following course. For your own hands on experience with AWS Lambda, please take a look at our labs which will guide you through how to create your first Lambda function. </p>
<p>That now brings me to the end of this lecture covering AWS Lambda. Coming up next, I shall be discussing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/aws-batch-cf/">AWS Batch</a>.</p>
<h1 id="AWS-Batch"><a href="#AWS-Batch" class="headerlink" title="AWS Batch"></a>AWS Batch</h1><p>Hello, and welcome to this lecture where I’ll provide a high level overview of AWS Batch. As the name suggests, this service is used to manage and run Batch computing workloads within AWS. Before we go any further, I just want to quickly clarify what Batch computing is. </p>
<p>Batch computing is primarily used in specialist use cases which require a vast amount of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> power across a cluster of compute resources to complete batch processing executing a series of jobs or tasks. Outside of a cloud environment, it can be very difficult to maintain and manage a batch computing system. It requires specific software and requires the ability to consume the resources required, which can be very costly. However, with AWS Batch, many of these constraints, administration activities and maintenance tasks are removed. You can seamlessly create a cluster of compute resources which is highly scalable, taking advantage of the elasticity if AWS, coping with any level of batch processing while optimizing the distribution of the workloads. All provisioning, monitoring, maintenance and management of the clusters themselves is taken care of by AWS, meaning there is no software to be installed by yourself. </p>
<p>There are effectively five components that make up AWS Batch service which will help you to start using the service, these being: Jobs. A job is classed as a unit of work that is to be run by AWS Batch. For example, this can be a Linux executable file, an application within an ECS cluster or a shell script. The jobs themselves run on EC2 instances as a containerized application. Each job can at any one time be in a number of different states, for example, submitted, pending, running, failed, among others. Job definitions. These define specific parameters for the jobs themselves. They dictate how the job will run and with what configuration. Some examples of these may be how many vCPUs to use for the container, which data volume should be used, which IAM role should be used, allowing access for AWS Batch to communicate with other AWS services, and mount points.</p>
<p>Job queues. Jobs that are scheduled are placed into a job queue until they run. It’s also possible to have multiple queues with different priorities if needed. One queue could be used for on-demand EC2 instances, and another queue could be used for the spot instances. Both on-demand and spot instances are supported by AWS Batch, allowing you to optimize cost, and AWS Batch can even bid on your behalf for those spot instances. </p>
<p>Job scheduling. The Job Scheduler takes control of when a job should be run and from which Compute Environment. Typically it will operate on a first-in-first-out basis, and it will look at the different job queues that you have configured, ensuring that higher priority queues are run first, assuming all dependencies of that job have been met. </p>
<p>Compute Environments. These are the environments containing the compute resources to carry out the job. The environment can be defined as managed or unmanaged. A managed environment means that the service itself will handle provisioning, scaling and termination of your Compute instances based on the configuration parameters that you would enter regarding the instance type, purchase method, such as on-demand or spot. This environment is then created as an Amazon ECS Cluster. Unmanaged environments are provisioned, managed and maintained by you, which gives greater customization. However, it does require greater administration and maintenance and also requires you to create the necessary Amazon ECS Cluster that the managed environment would have done on your behalf. </p>
<p>If you have a requirement to run multiple jobs in parallel using Batch computing, for example, to analyze financial risk models, perform media transcoding or engineering simulations, then AWS Batch would be a perfect solution.</p>
<h1 id="Amazon-Lightsail"><a href="#Amazon-Lightsail" class="headerlink" title="Amazon Lightsail"></a>Amazon Lightsail</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://lightsail.aws.amazon.com/ls/webapp/home/resources">Amazon Lightsail dashboard</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this short lecture covering Amazon Lightsail. This is another <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> service that in some respect closely resembles EC2 out of all the other compute resources we have covered so far. Amazon Lightsail is essentially a virtual private server, A VPS, backed by AWS infrastructure, much like an EC2 instance but without as many configurable steps throughout its creation. </p>
<p>It has been designed to simple, quick, and very easy to use at a low cost point for small-scale use cases by small business or for single users. With its simplicity and small-scale use, it’s commonly used to host simple websites, small applications, and blogs. You can run multiple Lightsail instances together, allowing them to communicate. And it’s even possible if required to connect it to other AWS resources and to your existing VPC, running within AWS via a peering connection. </p>
<p>To deploy a Lightsail instance, it’s easy to do all from a single page with just a few configuration options. Amazon Lightsail can be accessed either via the AWS console under the compute category, or you can go directly to the homepage of AWS Lightsail, which sits outside of the Management Console and can be found here. </p>
<p>If you want to launch a new instance, select create instance, where you can then create your instance all from just one page of options. Nice and simple. Firstly, you need to select your region and availability zone as required as to where you’d like to provision your Lightsail instance. Next, you can select your platform, Linux or Windows based, and then additional blueprint if required. If you didn’t need a blueprint, you can simply select to use the operating system only. Next, you have the option to add a launch script and a different key pair. The launch script can be a shell script that will run at the time of the launch, much like user data for an EC2 instance. By default, you are provided with a key pair to connect to your instance. However, you can select to choose an alternative one if required. Following this, you must then select your instance plan. This section defines the resources of your instance and how much you’re going to be paying on a monthly basis. The price per month option shows preset configurations based on memory, processing power, storage, and data transfer. However, you can tab through the corresponding tabs and customize the values of each to meet your needs. As you can see, it’s very clear, simple, and obvious as to what you will be paying and the resources you will get in return. The instances are charged as an on-demand price, so you’ll only pay for the resource when you’re using them. The dollar per month price is based on having the instance on continuously, which AWS calculates as 31.25 days multiplied by 24 hours. </p>
<p>The configuration options requires you to provide a unique name for your Lightsail instance. In addition to this, you’re also prompted to add key-value tags to help organize your resources. Now all of your configuration is complete. Simply click on create instance. As you can see, it’s very easy and simple to create your Lightsail VPS compared to number of different screens and configuration options required when deploying an EC2 instance. Once your Amazon Lightsail service is up and running, you then have a number of management and monitoring options, which are clear and easy to use. Connect. This option allows you to connect to your newly created instance using SSH either via inline SSH software provided by Lightsail or with your own SSH software using the key pair provided. The instance is given a public IP to allow you to connect. Storage. This provides an overview of your current storage, showing the capacity and the disk path. For example, &#x2F;dev&#x2F;sda1. You also have the ability to attach additional disks to your instance. Metrics. This allows you to view graphical metrics of your instance, such as CPU utilization, network in, network out, StatusCheckFailed, StatusCheckFailed_Instance, and StatusCheckFailed_System. These graphs can be viewed over a number of different time periods, from one hour through to two weeks. Networking. The networking tab allows you to view your IP address information along with a very simple virtual file, allowing you to control which ports your instance can accept connections from. You can also gain additional information on load balancing your traffic between instances. Snapshots. This provides a simple way to backup your instance. Tags. Here you can configure additional or edit existing tags to help you filter and organize your resources. Key-value tags can also be used to help manage your billing and control access. History. This provides simple order information of your instance, such as the date and time the instance was created or when configuration changes occurred. Delete. When you have finished with your instance, this tab allows you to delete your instance along with any data that was stored in it. </p>
<p>As you can see, Amazon Lightsail provides a lightweight solution for small projects and use cases which can be deployed quickly and cost effectively in just a few clicks. That brings us to the end of this lecture. Coming up next, I’ll provide a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/summary-to-aws-compute-fundamentals/">summary</a> of the key points and topics that we’ve learned throughout this course.</p>
<h1 id="What-is-an-Elastic-Load-Balancer-ELB"><a href="#What-is-an-Elastic-Load-Balancer-ELB" class="headerlink" title="What is an Elastic Load Balancer (ELB)?"></a>What is an Elastic Load Balancer (ELB)?</h1><p>Hello and welcome to this lecture, which is going to focus on what the AWS Elastic Load Balancer service is and does. </p>
<p>Now the main function of an Elastic Load Balancer, commonly referred to as an ELB, is to help manage and control the flow of inbound requests destined to a group of targets by distributing these requests evenly across the targeted resource group. These targets could be a fleet of EC2 instances, Lambda functions, a range of IP addresses, or even Containers. The targets defined within the ELB could be situated across different Availability Zones for additional resiliency or all placed within a single Availability Zone. Let’s look at this from a typical scenario. </p>
<p>Let’s suppose you have just created a new application, which is currently residing on a single EC2 instance within your environment, and this is being accessed by a number of users. At this stage, your architecture can be logically summarized as shown. If you are familiar with architectural design and best practices, then you would realize that using a single instance approach isn’t ideal although it would certainly work and provide a service to your users. However, this infrastructure layout brings some challenges. For example, the single instance where your application is located can fail, perhaps from a hardware or software fault. And if that happens, your application will be down and unavailable to your users. Also, if you experience a sudden spike in traffic, your instance may not be able to handle the additional load based on its performance limitations. As a result, to strengthen your infrastructure and help remediate these challenges, the unpredictable traffic spikes and high availability, et cetera, you should introduce an Elastic Load Balancer, an additional instance that’s running your application into the design as shown. </p>
<p>As you can see, in this design, the AWS Elastic Load Balancer will act as the point for receiving incoming traffic from users and evenly distribute the traffic across a greater number of instances. By default, the ELB is highly available as this is a managed service provided by AWS. And so, they ensure its resilience so we don’t have to. Although it might seem that ELB is a single point of failure, the ELB is in fact comprised of multiple instances managed by AWS. Also in this scenario, we now have three instances running our application. Now let me revisit the challenges we discussed previously. If any of these three instances fail, the ELB will automatically detect the failure based on defined metrics and divert any traffic to the remaining two healthy instances. Also if you experience a surge in traffic, then the additional instances running your application would help with the additional load. One of the many advantages of using ELB is the fact that it is managed by AWS, and it is, by definition, elastic. This means that it will automatically scale to meet your incoming traffic as the incoming traffic scales both up and down. </p>
<p>If you are system administrator or DevOps engineer running your own load balancer by yourself, then you would need to worry about scaling your load balancer and enforcing high availability. With an AWS ELB, you can create your load balancer and enable dynamic scaling with just a few clicks. Depending on your traffic distribution requirements, there are three ELBs available within AWS to choose from. </p>
<p>Firstly, the Application Load Balancer. This provides a flexible feature set for your web applications running the HTTP or HTTPS protocols. The Application Load Balancer operates at the request level, and it also provides advanced routing, TLS termination, and visibility features targeted at application architectures, allowing you to route traffic to different ports on the same EC2 instance. </p>
<p>Next, there is a Network Load Balancer. This is used for ultra-high performance for your application while at the same time managing very low latencies. It operates at connection level, routing traffic to targets within your VPC, and it’s also capable of handling millions of requests per second. </p>
<p>Finally, the Classic Load Balancer. This is primarily used for applications that were built in the existing EC2 Classic environment and operates at both the connection and request level. We’ll now talk a little bit about the components of an AWS ELB and some of the principles behind them. </p>
<p>Listeners. For every load balancer, regardless of the type used, you must configure at least one listener. The listener defines how your inbound connections are routed to your target groups based on ports and protocols set as conditions. The configurations of the listener itself differs slightly depending on which ELB you have selected. I will dive into the configuration of these as I discuss each ELB in further detail in upcoming lectures. </p>
<p>Target groups. A target group is simply a group of resources that you want your ELB to route requests to, for example a fleet of EC2 instances. You can configure your ELB with a number of different target groups, each associated with a different listener configuration and associated rules. This enables you to route traffic to different resources based upon the type of request. Rules. </p>
<p>Rules are associated to each listener that you have configured within your ELB, and they help to define how an incoming request gets routed to which target group. As you can see, your ELB can contain one or more listener. And each listener can contain one or more rules, and each rule can contain more than one condition, and all conditions in the rule equal a single action. An example rule could look as follows where the if statement resembles the conditions and the then statement acts as the action if all the conditions are met. So, depending on which listener request was responded to by the ELB, a rule based upon a priority listing would be associated containing these conditions and actions. If the request came from within the 10.0.1.0&#x2F;24 network range, which is the first condition, and was trying to carry a HTTP PUT request, the second condition, then the request would be sent to the target group entitled Group1, which is the action. </p>
<p>Health checks. The ELB associates a health check that is performed against the resources defined within the target group. These health checks allow the ELB to contact each target using a specific protocol to receive a response. If no response is received within a set of thresholds, then the ELB will mark the target as unhealthy and stop sending traffic to that target. </p>
<p>Internal or Internet-facing ELBs. There are two different schemes that can be used for your load balancers, either internal or Internet-facing. Internet-facing, as the name implies, the nodes of the ELBs that are defined as Internet-facing are accessible via the Internet and so have a public DNS name that can be resolved with public IP address. This would be in addition to an internal IP address as well. This allows the ELB to serve incoming requests from the Internet before distributing and routing the traffic to your target groups, which in this instance could be a fleet of web servers receiving HTTP or HTTPS requests. When your Internet-facing ELB communicates with its target group, it will only use the internal IP address, meaning that your target group does not need public IP addresses. An internal ELB only has an internal IP address. This means that it can only serve requests that originate from within your VPC itself. For example, you might have an internal load balancer sitting between your web servers in the public subnet and your application servers in the private subnet. </p>
<p>ELB nodes. During the creation process of your ELBs, you’re required to define which Availability Zone you’d like your ELB to operate within. For each Available Zone selected, an ELB node will be placed within that Availability Zone. As a result, you need to ensure that you have an ELB node associated to any Availability Zones for which you want to route traffic to. Without the Availability Zone associated, the ELB will not be able to route traffic to any targets within that Availability Zone even if they are defined within the target group. This is because the nodes are used by the ELB to distribute traffic to your target groups. </p>
<p>Cross-Zone load balancing. Depending on which ELB option you select, you may have the option of enabling and implementing Cross-Zone load balancing within your environment. Let’s presume you have two Availability Zones activated for your ELB with each associated load balancer receiving equal amount of traffic. One Availability Zone has six targets, and the other has four as shown. When Cross-Zone load balancing is disabled, each ELB and its associated AZ would distribute its traffic with the targets within that Availability Zone only. As we can see from the image, this results in an uneven distribution of traffic for each target across the Availability Zones. With Cross-Zone load balancing enabled, regardless of how many targets are in an associated Availability Zone, the ELBs would distribute all incoming traffic evenly between all targets, ensuring each target across the Availability Zones have an even distribution. </p>
<p>That now brings me to the end of this lecture. In the lecture, I shall be discussing server certificates and how they are used with load balancers to help terminate encrypted requests.</p>
<h1 id="SSL-Server-Certificates"><a href="#SSL-Server-Certificates" class="headerlink" title="SSL Server Certificates"></a>SSL Server Certificates</h1><p>Hello and welcome to this short lecture which will provide a high-level overview of server certificates and how they are used within your elastic load balancers. </p>
<p>As I mentioned in the previous lecture the Application Load Balancer provides a flexible feature set for your web applications running the HTTP or HTTPS protocols. As such, the ALB listener options available when creating your ALB are either the HTTP or HTTPS protocol on port 80 and 443 respectively. Configuration of your HTTP port 80 listeners is a fairly simple process, and I’ll cover this in the next lecture. However, there will times when you would need to use the HTTPS encrypted protocol as a listener and this requires some additional configuration. </p>
<p>So let me run through some of the points when using HTTPS as a listener. HTTPS is an encrypted version of the HTTP protocol and this allows an encrypted communication channel to be set up between clients initiating the request and your Application Load Balancer. However, to allow your ALB to receive encrypted traffic over HTTPS it will need a server certificate and an associated security policy. </p>
<p>SSL or Secure Sockets Layer, to give it its full name, is a cryptographic protocol, much like TLS, Transport Layer Security. Both SSL and TLS are used interchangeably when discussing certificates for your Application Load Balancer. The server certificates used by the ALB is an X.509 certificate, which is a digital ID that has been provisioned by a Certificate Authority and this Certificate Authority could be the AWS Certificate Manager service also known as ACM. This certificate is simply used to terminate the encrypted connection received from the remote client, and as a part of this termination process the request is then decrypted and forwarded to the resources in the ELB target group. </p>
<p>When you select HTTPS as your listener, you will be asked to select a certificate using one of four different options available. Either choose a certificate from ACM, upload a certificate to ACM, choose a certificate from IAM, or upload a certificate to IAM. The first two options relate to ACM. An ACM is the AWS Certificate Manager and this service allows you to create and provision SSL&#x2F;TLS server certificates to be used within your AWS environment across different services. This integration with ACM simplifies the configuration process of implementing a new certificate for your elastic load balancer and as a result, it’s the preferred option. </p>
<p>The last two options allow you to use a third-party certificate by using IAM as your certificate manager and you would select this option when deploying your ELBs in regions that are not supported by ACM. For a list of supported regions, please see the following link. For detailed information on how to upload, retrieve, and list server certificates via IAM, please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html">AWS documentation</a>. Using ACM as your certificate manager allows you to both create certificates from within ACM itself and also import existing certificates created from outside of AWS adding additional flexibility for your current third party certificates. The configuration of ACM is out of scope for this course. However, you can find further information on this service using the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">link</a>. </p>
<p>Now it’s brought me to the end of this lecture. In the next few lectures I shall be looking at the configuration of each of the defined load balancers, application, network, and classic, to provide you with more information on their components starting with the Application Load Balancer, the ALB.</p>
<h1 id="Application-Load-Balancers"><a href="#Application-Load-Balancers" class="headerlink" title="Application Load Balancers"></a>Application Load Balancers</h1><p>Hello and welcome to this lecture covering the Application Load Balancer, the ALB. The first of the three load balancers that I shall be discussing. If you are familiar with the open systems interconnection model, the OSI model, then you won’t be surprised that the ALB operates at layer seven, the application layer. The application layer of the OSI model serves as the interface for users and application processes to access network services. Everything at this layer is application specific. The application layer of the model helps to provide network services to the applications. And examples of the application process or services it offers are http, ftp, smtp and nfs. For more information on the OSI model, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/">here</a>.</p>
<p>As you can see AWS suggests you use the application load balancer if you need to provide a flexible feature set including advanced routing and visibility features aimed purely for application architectures such as microservices and containers when used in HTTP or HTTPS. Before configuring your ALB, it’s good practice to set up your target groups. Now I explained in a previous lecture that a target group is simply a group of resources that you want your ALB to route requests to. You might want to configure different target groups depending on the nature of your requests. For example, let’s say you had an internet-facing ALB, you might want a target group allocated to handle and process HTTP port 80 requests and a different target group configured to process requests from the secure HTTPS protocol using port 443. In this scenario, you could configure two different target groups and then route traffic, depending on the request, to different targets through the use of listeners and rules. </p>
<p>I now want to demonstrate how to configure an ALB and in this demonstration, I will also show you how to set up target groups as well. Let’s take a look. </p>
<p>As you can see, I’m in the AWS management console and the first thing we want to do is create our target groups and I can do this by going into the EC2 service which is found here under compute. And then if I scroll down on the left-hand side, I’ll get to the load balancing section here. Then in here, I have load balancers and target groups, but first I want to set up our target groups. So, if you select target group, as you can see there’s no target groups currently configured. So if I click on the blue button, Create target group, I now have a page of information that I need to complete. </p>
<p>So, firstly the target group name and I’m going to call this Web Servers. And then I have my target type and here I can specify it by instance, IP or Lambda function. I’m going to leave it as instance, then we can select what protocol we want. As this is going to be a web service, I’ll leave it as HTTP on port 80 and here we can select our VPC that we want this target group to exist in. So, select my appropriate virtual private cloud there. At the bottom we just have some health check settings and this is the path and protocol that the load balance will use when performing its health checks. So, for the path I’ll just put in index.html as an example. If we take a look at the advanced health check settings, here we have a number of other options that we can select. </p>
<p>This value specifies the healthy threshold which means the load balancer will have to receive five responses from the instance before deeming the previously unhealthy target healthy again, and the unhealthy threshold means that the load balancer only has to receive two failures before marking the instance as unhealthy. The timeout is simply the number of seconds that the load balancer will wait for a response, and the interval is how many seconds between each health check. Once we’re happy with our configuration, we’ll click on Create. Looks like I left this space in the name and you can’t have any spaces. So let’s just delete that and click on Create. And there you go, our target group was successfully created, and now we can see it in our list of target groups. </p>
<p>However, we don’t have any targets associated with this group yet. That was just simply the configuration of the group. So, down here we have the Description, the Targets, the Health checks, Monitoring and Tags, but if we click on the Targets tab, then here we can start adding our targets associated with this target group. And if we click on Edit, we can see here at the bottom that there’s two instances that I have running, web server one, and web server two. Now here I can select which instance I want to add and associate to this target group. For this demonstration, I’m going to select both instances and then add these as registered targets to this group. And as you can see, these two instances have now been added under the Registered targets section. Click on Save. And we can see here, that we have two registered targets which are the ones I just added, web server one and web server two, now associated to this target group. </p>
<p>Let’s just quickly look at these other tags here as well, the Health checks, that’s the health check information that we configured during the creation of the target group; Monitoring, this shows a number of CloudWatch metrics associated with the target group such as number of healthy hosts and unhealthy hosts et cetera. And then we also have Tags if you wanted to create a key-value pair for your target group and you can do so here. So, as you can see, it’s very easy to create different target groups as you need to for your load balancing. </p>
<p>Let us now go ahead and create an Application Load Balancer. So, back on the left-hand side here, again under Load Balancing, we have Load Balancers. So if you select that. Now I don’t have any load balancers configured here. So if I click on Create Load Balancer, and I can create an Application Load Balancer and Network Load Balancer or the Classic. In this example, I’m going to create the Application Load Balancer. So, click on Create. Now here we have a number of different steps. Firstly, we need to give it a name. So this would be WebServerALB, and we’ll have this as internet-facing using ipv4. Now down here we have our listeners. So this is the port and protocol that we want the load balancer to listen on and as this is our web server, let’s leave it as HTTP on port 80. If you want to add additional listeners, then you can do so just by selecting Add Listener and selecting the different protocols et cetera. Now if we scroll down to the bottom here, we can select our Availability Zones that we want to enable for our load balancer. So for eu-west-1a, let me select this subnet and for eu-west-1b I, shall select this subnet. So there are the two subnets that I want to associate with the load balancer, and each of them are in a set per availability zone as you can see here. </p>
<p>Now I need to go and configure my security settings. And I have a message here to say that the load balancer security is not using a secure listener. Now, if we were to go back and change that to HTTPS, then we would be using a secure listener and we’d also have to set up server certificates as well, but for this demonstration, I just want to show you how to create the Application Load Balancer, but generally in a wide-scale production environment, if you’re creating a load balancer for your internet-facing resources, then you’d probably want to use https for that additional security. Click on next. </p>
<p>Now we’ll need to select the security group that is going to be associated to our load balancer. So we could create a new security group, call this our Application Load Balancer. So we’ll have HTTP from any IP address, and we click on that Next Configure Reading. And this is where we can specify our target group for the Application Load Balancer. So we can create a new target group here and go through the same process as we did earlier or click on the dropdown list and select an existing target group and here we can see that we have our WebServers target group that we created earlier with all the settings already pre-filled. Click on Next Register Targets, and here we can see that these are the two targets associated with the target group. Click on Next Review. And this is just a review of all the configuration options that we made during the creation of this. Once you’re happy with that, simply click on Create. And then we have it. </p>
<p>We have our new load balancer, our WebServerALB was successfully created. So, let’s take a look. This might take it a couple of minutes for it to be provisioned. While that’s being provisioned, if we take a look at the bottom here, we can see that we have some basic configuration that we’ve set up with the availability zones, the fact that it’s internet-facing, and we have the ARN et cetera as well. We have our listener configuration here that we can change if we need to. As we can see, at the minute we’re listening on port 80. Again, we have some monitoring metrics here being carried out by CloudWatch. We’ll see a number of different CloudWatch metrics. Actually, we can now see that that the state is now active. So that’s our Application Load Balancer set up and configured. </p>
<p>Now before I finish this quick demonstration, I just want to show you the rules that I mentioned earlier with regards to listeners. So, if we go down to our Listeners tab here, we can see that we can view and edit our rules for our listener. So if we click on that, at the moment we can see that we have our default action here listening on port 80. We can see that this rule cannot be moved or deleted. That’s basically saying that this listener is listening on port 80 and for any requests then forward it to the WebServers. But we can add additional rules in here. So let’s take a look. So if we click on this plus button, we can see that we now have this option here of Insert Rule. So, let me select that, and that allows me to add a new rule in. So, first we need to add a condition. So, for example, let’s have the condition of a Source IP, then we just put in a random IP address here. So, this is saying if the source IP is this IP address, then add the following action, and here we could choose to forward it to another target group. I mean, I’ve only got one target group configured at the minute called WebServers, but if I had other target groups here with different instances associated to those target groups, then I could select a different target group to forward any requests that are received from this IP address. So that allows you to customize how your load balancer directs traffic, depending on what rules you create with your listeners. So, when I was talking about conditions and rules in a previous lecture, then this is the section that I was referring to. So I just wanted to show you that quickly within this demonstration, where you can edit your rules and add customization with conditions and actions. </p>
<p>Okay, and that’s the end of this demonstration.</p>
<h1 id="Network-Load-Balancers"><a href="#Network-Load-Balancers" class="headerlink" title="Network Load Balancers"></a>Network Load Balancers</h1><p>Hello and welcome to this lecture focusing on the network load balancer and its configuration. </p>
<p>Between the ALB and the NLB, the principles are the same as to how the overall process works, so to load balance incoming traffic from a source to its configured target groups. However, whereas the ALB work to the application level analyzing the HTTP header to direct the traffic, the network load balancer operates at Layer 4 of the OSI model enabling you to balance requests purely based on the TCP and UDP protocols. As such, a request to open a TCP or UDP connection is established to load balance the host in the target group. The listener supported by the NLB include TCP, TLS and UDP. The NLB is able to process millions of requests per second making the NLB a great choice if you need ultra high performance for your application. Also if your application logic requires a static IP address, then the NLB will need to be your choice of elastic load balancer. Unlike the application load balancer that has cross-zone load balancing always enabled, for the NLB this can either be enabled or disabled. When your NLBs are deployed and associated to different availability zones, an NLB node will be provisioned in these availability zones. The node then uses an algorithm which uses details based on the sequence, the protocol, source port, source IP, destination port and destination IP to select the target in that zone to process the request. When a connection is established with a target host, then that connection will remain open with that target for the duration of the request. Let me now provide a demonstration on how to configure and set up a network load balancer. </p>
<p>As you can see, I’m in the AWS management console. So to create our network load balancer, let’s go to EC2 under Compute. Then if we go down the left-hand side again under Load Balancing, click Load Balancers, we can see here our existing application load balancer we created before. So let’s click on Create Load Balancer and this time we’re going to create a network load balancer. So click on Create. And again, it’s very similar configuration to the application load balancer. So let’s firstly give it a name. Let’s call this DNS-NLB. This time we’ll have it internal facing. For our listener, let’s select the UDP protocol and the load balancer port is port 53 which is DNS. Again, we can select our availability zones where we want our load balancer to reside. So under eu-west-1a, let me select that subnet. And on the b, that one there. Next, configure security settings. Again, we receive this message because we’re not using a secure listener and for this demonstration that’s okay. Configure routing, now we need to associate our target group. Let’s create a new target group this time and we’ll call this DNS. For the target type, I shall leave as instance. We have our port and protocol there. Health checks under TCP. And if you wanted to, you can make any changes to your advanced health check settings there. Next, click on Register Targets. As we can see, we don’t have any registered targets as yet. If I scroll down, I can see I have one instance here so I’m going to add that to the registered list of targets. Once that’s been added, click on Next Review. Once you’re happy with all your configuration settings, click on Create. And there you have it. Your network load balancer is now created. We can see here provisioning which is our network load balancer. This is our previous application load balancer that we created earlier. So it’s a very similar process with different ports and protocols available between the load balancers. And that’s the end of this demonstration.</p>
<h1 id="Classic-Load-Balancers"><a href="#Classic-Load-Balancers" class="headerlink" title="Classic Load Balancers"></a>Classic Load Balancers</h1><p>Hello and welcome to this lecture covering the last of the load balancers that are available, the classic load balancer. The classic load balancer supports TCP, SSL&#x2F;TLS, HTTP, and HTTPS protocols. However, it does not offer as wide a range of features as the other load balancers. It is considered best practice to use the ALB over this classic load balancer unless you have an existing application running in the EC2-Classic network. Now, many of you will be unfamiliar with the EC2-Classic platform, and this is because it is no longer supported for newer AWS accounts. In fact, any account created after the 12th of April 2013 will not support EC2-Classic. </p>
<p>The EC2-Classic platform was originally introduced when the first release of EC2 was made generally available a number of years ago. The EC2-Classic platform enabled you to deploy your EC2 instances in a single flat network shared with other customers instead of inside a VPC. Although the classic load balancer doesn’t provide as many features as the application load balancer, it does offer the following which the ALB does not. It supports EC2-Classic, it supports TCP and SSL listeners, and it has support for sticky sessions using application-generated cookies. Again, the classic load balancer works in much the same way as the other load balancers already discussed, and again, cross-zone load balancing can either be enabled or disabled. Let’s now take a look at the creation of a classic load balancer. </p>
<p>So let’s now create the last type of load balancer, the classic load balancer. So again, let’s go to EC2. Down the left-hand side to load balancers. We have our previous application load balancer and our network load balancer. Let’s now create the classic load balancer. So we go across here to create. Give this a name. I’ll just call it classic. Select the VPC that I’d like to do. Now here we have our listener configuration, so for ease, let’s just have this listed on port 80. And then here, we need to select our availability zones that we’d like. So let’s select this one and also this one here. Once we’ve selected our subnets for our load balancer, we can then assign security groups. I’m going to use an existing security group that I’ve created previously. Once that’s selected, click on Configure Security Settings. Again, it’s telling us we’re not using a secure listener. Again, for this demonstration, that’s more than okay. Now we can configure our health checks. This will probably look familiar to you when we’re discussing the application load balancer. So the port and protocol using and the path, as well, the ping path, which is what the load balancer will check to make sure it can reach to determine if the instance is healthy or not. Once you’re happy with those details, select Add EC2 Instances. </p>
<p>Now here we can select the instances that you want to associate to the load balancer, and this is different to the application load balancer and the network load balancer, where we used target groups. With the classic, we simply select the instances that we want included, so we don’t use target groups for a classic load balancer. So for this example, we can select those two options, coming down across to add tags. Put in any tags you want associated for the load balancer. Click on Review and Create, confirm that you’re happy with your settings, and then click on Create. And there we have it. So if we go back here, you can now see that we have our three different load balancers that we’ve created. Here we have our application load balancer, this was our network load balancer, and here we have our classic load balancer. And it’s as simple as that. </p>
<p>Before I finish this lecture, it’s a good time to take a quick look at the comparison between the three load balancers that we’ve looked at. To help with this, AWS Provides a great table to show the feature differences between each ELB, which can be found using the link shown on screen. We can clearly see that the ALB is the most feature-rich. However, the NLB supports some significant differences to that of the ALB, such as support for static IPs, EIPs, and preserving source IP addresses. </p>
<p>That now brings me to the end of this lecture. Coming up next, I shall be looking at auto scaling and the benefits that this feature brings.</p>
<h1 id="EC2-Auto-Scaling"><a href="#EC2-Auto-Scaling" class="headerlink" title="EC2 Auto Scaling"></a>EC2 Auto Scaling</h1><p>Hello and welcome to the first of the lectures that will be covering EC2 Auto Scaling. So what exactly is EC2 Auto Scaling? Put simply, Auto Scaling is a mechanism that automatically allows you to increase or decrease your EC2 resources to meet the demand based off of custom defined metrics and thresholds. </p>
<p>In AWS, there is EC2 Auto Scaling which focuses on the scaling of your EC2 fleet, but there’s also an Auto Scaling service. This service allows you to scale Amazon ECS tasks, DynamoDB tables and indexes, in addition to Amazon Aurora replicas. For this course I will just be focusing on EC2 Auto Scaling. Let’s look at an example of how EC2 Auto Scaling can be used in practice. </p>
<p>Let’s say you had a single EC2 instance acting as a web server receiving requests from the public users across the Internet. As the requests and demand increases, so does the load on the instance. Additional processing power will be required to process the additional requests and therefore the CPU utilization would also increase. To avoid running out of CPU resource on your instance, which would lead to poor performance experienced by your end users, you would need to deploy another EC2 instance to load balance the demand and process the increased requests. With Auto Scaling, you could configure a metric to automatically launch a second instance when the CPU utilization got to 75% on the first instance. By load balancing traffic evenly, it would reduce the demand put upon each instance and reduce the chance of the first web server failing or slowing due to high CPU usage. Similarly, when the demand on your web server reduces, so would your CPU utilization. So you could also set a metric to scale back. In this example, you could configure Auto Scaling to automatically terminate one of your EC2 instances when the CPU utilization dropped to 20% as it would no longer be required due to the decreased demand. </p>
<p>By scaling your resources back helps to optimize the cost of your EC2 fleet as you only pay for resources when they are running. Through these customizable and defined metrics, you can increase, scale out, and decrease, scale in, the size of your EC2 fleet automatically with ease. This has many advantages and here are some of the key points. Firstly, automation. As this provides automatic provisioning based off of custom defined thresholds, your infrastructure can elastically provision the required resources, preventing your operations team from manually deploying and removing resources to meet demands put upon your infrastructure. Greater customer satisfaction. If you are always able to provision enough capacity within your environment when the demand increases, then it’s unlikely that your end users will experience performance issues, which will help with user retention. And cost reduction. With the ability to automatically reduce the amount of resources you have when the demand drops, you will stop paying for those resources. You only pay for an EC2 resource when it’s up and running, which is based on a per second basis. When you couple Auto Scaling with an Elastic Load Balancer, you get a real sense of how beneficial building a scalable and flexible architecture for your resources can be. </p>
<p>In the next lecture, I shall be explaining the different components of Auto Scaling before providing a demonstration on how to configure it.</p>
<h1 id="Components-of-EC2-Auto-Scaling"><a href="#Components-of-EC2-Auto-Scaling" class="headerlink" title="Components of EC2 Auto Scaling"></a>Components of EC2 Auto Scaling</h1><p>Hello and welcome to this lecture where I’ll focus on the different components of EC2 auto scaling, to help you understand how the process and service works. There are two distinct steps to the configuration. The first step is the creation of the launch configuration or launch template. And the second part is the creation of an auto scaling group. </p>
<p>When using EC2 auto scaling, you can either create a launch configuration or launch template. Both define how an auto scaling group builds new EC2 instances. They both answer a number of questions required when launching a new instance, such as which Amazon Machine Image to use or AMI, which instance type to select. If you’d like to use Spot Instances to help lower costs. If and when public IP addresses should be used for your instances. If any user data is required for automatic scripting on first boot. What storage volume configuration should be used, and what security group should be applied. You will probably be familiar with most of these steps if you have ever created an EC2 instance manually, it’s much the same. </p>
<p>A launch template is essentially a newer and more advanced version of the launch configuration. Being a template you can build a standard configuration allowing you to simplify how you launch instances for your auto scaling groups. Let me now demonstrate how to create both a launch configuration and a launch template. </p>
<p>As you can see, I’m logged into my AWS account and I’m at the Management Console. And to create our launch templates and launch configurations we need to go into EC2 under compute. So let’s take a look. Now I’m going to start by creating the launch template first. And then after that, I’ll create the launch configuration so you can see the differences between them. </p>
<p>Now on the left hand side here, under instances, you can see launch templates. So if you select that, then it’s just a quick splash screen here, just saying welcome to launch templates. And this gives you a brief summary of what it is. So to create a launch template we’ll click on the blue create launch template button. Now here we have a single page with a number of configurable parameters on them. So let’s go through each of them and take a look. So firstly, we can either create a new template or create a new template version. Now as I don’t have an existing template, we can’t create a new version of that template. So let’s start from scratch by creating a new template. Let’s give this a name. I’ll just call this launch template. And a description of demo. Now here we can specify the source template, which essentially allows you to create a template from an existing template that you might already have. And as I explained, I don’t have any other templates at the moment. So we can’t do that and I just want to show you how to create a template from scratch anyway. Now further down we have launch template contents. Now this is where we started getting to the actual configuration of what we’re going to launch. </p>
<p>And we can start by selecting the AMI ID. If we click on the search for AMI, we can have a look at the different catalogs. Either quick start if you have any of your own AMIs on the marketplace or the community AMIs. Let’s just go with a quick start. And then we can select an AMI, let’s just go with the top one here, the Amazon Linux. And select AMI, now we can select an instance type. Let’s just go for a t1 micro. And if we have any existing key pairs, we can select an existing key pair to allow us to connect to our instances. For this one, I’ll just select an existing key pair. And then network type that this instance will reside in, whether it’s in a VPC environment, or the classic environment, we’re going to go with the VPC. Now here we can also attach any security groups Again as a drop down list to allow you to select an existing security groups that you might have. So I’ll just select a couple of different groups here. Now further down, if you want to add any additional network interfaces, then you can do so here simply by clicking on add network interface and filling out the relevant fields. Don’t need to do that in this example. Now here we have storage volumes. So I’ll come with an eight gig EBS volume, general purpose. And we can specify encryption here if we want to, and the delete on termination, either yes or no, and the IOPS et cetera. And we can add additional volumes if we want to as well here. Further down we can instance tags. So let’s add a tag, say for example project cloud academy. And this will tag both the instance and the volume. If we go into advanced details. We can select if you want this to be a Spot Instance or not. We can select an instance profile which allows you to associate a role to your EC2 instance when it launches. We can select the shutdown behavior, whether you want it to terminate or simply stop when we shut it down. And there’s a number of other more advanced options that you can select with regards to your instance. And then at the very bottom we have user data if you want to run any commands on boot. Now once you’re happy with all of your information, all you need to do is simply click on create launch template. And that’s it. </p>
<p>If we get on to close, we can now see our launch template has been created. And the default version is one and the latest version is one. We can create another launch template based on this and give it a different version if we want to. So let’s see how the launch template compares to the launch configuration. </p>
<p>Now the launch configuration is further down on the left hand side on the auto scaling right at the bottom here. So if you click on launch configurations, and this is essentially the same as the launch template. Although the launch template has a few more options, and it’s more simplistic in its creation and is the preferred method. However, you can still create launch configurations, so let’s take a look. Click on create launch configuration. Now here we can select our AMI. And again, we have the different catalogs here like we do with the launch template. It’s just presented differently essentially. Select the AMI. Here we can select our instance type. And again we had this option in the launch template. Here we can give it a name. Let’s call this launch configuration. Again, you can select if you want to use Spot Instances, and you can select an IAM role. Whereas in the launch template you can select the instance profile. If we get on to advanced, there’s not as many advanced options here as there is with the launch template as we had a much longer list. However, you do still have a number of options here should you need it such as user data, specifying the Kernel ID et cetera. If we go to storage. Again, it comes with the default storage for the instance type you selected. And again you can add new volumes if you need to. So again very similar to the launch template. Here you can select your security groups, so you can select an existing group. So again, you can just select the security groups that you need here. Then click on review. And here’s a summary of all the options that you’ve selected. And once you’re happy with those, simply click create launch configuration. And finally, here you can also choose a key pair or create a new key pair should you need to. So again, let’s just let that cloud academy key pair. Then click create launch configuration. And there you go, there’s our launch configuration. </p>
<p>So there’s two different methods of creating that configuration to allow your auto scaling groups to know what instances to launch and how they should be configured. The main difference between the two is that the launch templates is presented all on a single page to allow you to quickly select your options rather than going through a number of different screens. And it also has a few more advanced features and options as well. Okay, that’s the end of the demonstration, thank you. </p>
<p>Without either the launch configuration or launch template, auto scaling would not know what instance it was launching and to which configuration. So before you create your auto scaling group, you need to have your launch configuration defined. But what does the auto scaling group do? Well, the auto scaling group defines the desired capacity and other limitations of the group using scaling policies and where the group should scale your resources, such as which availability zone. Let’s look at each of these details further via another demonstration on how to create an auto scaling group. And during this demonstration, I will create a new auto scaling group based on our previous launch template. And I’ll set up an auto scaling policy defining when to both increase and decrease the group size. Let’s take a look. </p>
<p>Again I’m within the AWS Management Console. And to create our auto scaling group, we need to go into EC2, which is under compute. And then if you scroll to the bottom on the left hand side, we can see under auto scaling, auto scaling groups, so let’s click on that. And this is where we can create our group. Firstly we click on the blue button create auto scaling group. And here we can either create it from a launch configuration or a launch template. So let’s select the launch template which is the new and preferred option. And down here we can select which launch template we would like to use. And this is the one that I created in the previous demonstrations. So once I’ve selected my launch template that I’d like to use, click on next step. Now we can give it a group name. So I just call this demo. If we had multiple versions of our launch template, then we can select the different versions there. But at the moment we just have the single version. With regards to the fleet, we can adhere to our launch template configuration. Or we can use a combination of different purchase options and instances. I just want to use the configuration that we used within our launch template. With regards to the group size, let’s start with two instances in our auto scaling group. And I can select the appropriate VPC that I’d like this to be launched in. And once I select the VPC, then I can select the subnets that I’d like. So let’s just select a couple of subnets in our VPC. Now if we go down to advanced details, here we have a number of other options. Now if you want to associate our auto scaling group to a load balancer, then we can do so here. And we can select our load balancer and target groups. But at the moment I’m going to leave this blank because in a later demonstration in the next lecture, I’m going to show you how to associate an existing auto scaling group with one of your new load balancers. So I’m just going to leave that blank for now. But if you did want to associate your auto scaling group to a load balancer during creation. then this is the place you do it. We have our instance protection down here. And we have an option protect from scale in. So if this is selected then during the scale in procedures, auto scaling won’t terminate any instances that are protected. I’m just going to remove the options for now. And also and finally a service linked role is selected. And this enables access to AWS services and resources that are used or managed by auto scaling. Once we have our configuration set, we can move on to configuring scaling policies. </p>
<p>Now here, we have two options, we can keep this group at its initial size. And as you know I set it to two instances, or we can you scaling policies to adjust the capacity of the group. And that’s what I’d like to do. Now I want to scale between two and five instances say. ‘Cause that’s the minimum and maximum number of instances that this group will scale to. Now I want to scale more auto scaling group using step or simple scanning policy. So I’m going to click on that. Now we have a policy here for increasing the group size and also a policy here for decreasing the group size. So the name for this policy, I’m just going to leave as increase group size. So auto scaling knows when to execute this policy, we need to set an alarm. So click on add new alarm. And this will send a notification. At that the moment I’ve got a notification set up as CADemo, which is an SNS topic. And I want this alarm to trigger whenever the average CPU utilization is greater than or equal to 75% for one consecutive period of five minutes. The name of this alarm will be deploy new instances. And then I simply click on create alarm. Now I want to take the action of adding just a single instance when the CPU utilization is greater than 75%. </p>
<p>Now we can do the same for the decrease group size, so whenever the average CPU utilization is less than or equal to 30%, for two consecutive periods of five minutes, and I’m going to call this remove instances and create that alarm. I’m going to say remove one instance when the CPU is less than 30%. Once your policies are set, you can click on configure notifications. So if you want to configure your notifications, simply click on add notification. And that’s my SNS topic that I had. So whenever instances are launched, terminated, fail to launch and fail to terminate, I want to receive those notifications. Let’s click on configure tags. And here, you can add any tags to auto scaling group. Just going to leave that blank for this demonstration, click on review. And here we have all of our configuration that we just made. Once you’re happy with that, click on create auto scaling group. </p>
<p>And we now have our auto scaling group configured and except here the minimum and maximum instances two and five and the availability zones et cetera. Now at the bottom here you can go into the auto scaling group details. Here we can see it’s launching two new instances because we said we want to start with two instances to start with. We can review our scaling policies. We can look at instances, monitoring, notifications, et cetera, et cetera. So if we go over to our instances, we should see two new instances that are launching. and here we can see that these two here are both initializing. So that’s our two new instances that are running because of our auto scaling group which was based off of our launch template. And that’s it. </p>
<p>In the next lecture, I should be looking at how both ELB and auto scaling combined can be used to manage your EC2 infrastructure.</p>
<h1 id="Using-ELB-and-Auto-Scaling-Together"><a href="#Using-ELB-and-Auto-Scaling-Together" class="headerlink" title="Using ELB and Auto Scaling Together"></a>Using ELB and Auto Scaling Together</h1><p>Hello and welcome to this short lecture where I shall discuss the relationship of ELBs and EC2 auto scaling. As you saw in the demonstration on the previous lecture, it’s easy to associate your EC2 auto scaling group to an elastic load balancer and this is because the two services go hand in hand to provide optimal efficiency for both the performance and cost perspective. Each service by itself provides a great way to solve particular operational hurdles. The ELB allows you to dynamically manage loads across your resources based upon target groups and rules whereas EC2 auto scaling allows you to elastically scale those target groups based upon the demand put upon your infrastructure. However, one without the other can cause an operational burden. </p>
<p>For example, let’s say you have an ELB configured but without any auto scaling. You will need to ensure that you manually add and remove targets based upon the demand. You will need to monitor this demand allowing you to manually add or remove instances as required. Now let’s look at the reverse. Let’s assume you have EC2 auto scaling configured but no elastic load balancer. How are you going to evenly distribute traffic to your EC2 fleet? </p>
<p>Hopefully, you can see the benefit of combining an ELB and auto scaling to help manage and automatically scale your EC2 compute resources both in and out. When you attach an ELB to an auto scaling group, the ELB will automatically detect the instances and start to distribute all traffic to the resources in the auto scaling group. When you want to associate an application load balancer or network load balancer, you associate the auto scaling group with the ELB target group. When you attach a classic load balancer, the EC2 fleet will be registered directly with the load balancer. </p>
<p>Let me now provide another demonstration on how to associate your ELBs with an auto scaling group. </p>
<p>Okay, so to attach an existing load balancer to your auto scaling group is very quick and simple. So firstly we go to our EC2 dashboard under Compute. We then go down to our Auto Scaling Groups at the very bottom on the left-hand side. We can see here our auto scaling group that we created in a previous demonstration. Now you’ll notice on the Details section at the bottom here there’s a section for classic load balancers and target groups and both of these are empty fields. So let’s look at how you would associate either a classic load balancer or your target group. So once our auto scaling group is selected, we go to up to Actions and then Edit. Now if we scroll down, we get to the section here Classic Load Balancers and Target Groups. Now if we want to add a classic load balancer, then you can select here and select any classic load balancers that you have configured so I can select that one for example or if you’re using an application load balancer or network load balancer, then you associate it with the target groups because the target groups are the pool of resources that the application load balancer or network load balancer are associated to. Now here we have two target groups that we created in a previous demonstration either DNS or WebServers. So we could select WebServers as our target group and simply click on Save. </p>
<p>And as you can see now, this entry here for target groups, the auto scaling group is now associated to the WebServers target group. And it’s as simple as that. So all you need to do to associate an existing load balancer to your auto scaling group is to edit the auto scaling group and if it’s a classic load balancer, add in the classic load balancer or if it’s an application load balancer or network load balancer, then you associate it to the relevant target group. And that’s it. </p>
<p>That now brings me to the end of this lecture. Coming up next, I will provide a summary of the key points made throughout this course.</p>
<h1 id="2What-is-Compute-in-AWS"><a href="#2What-is-Compute-in-AWS" class="headerlink" title="2What is Compute in AWS?"></a>2<strong>What is Compute in AWS?</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/products/compute/">Cloud Compute Index</a></p>
<h1 id="3EC2-Elastic-Compute-Cloud"><a href="#3EC2-Elastic-Compute-Cloud" class="headerlink" title="3EC2 - Elastic Compute Cloud"></a>3<strong>EC2 - Elastic Compute Cloud</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Shared Responsibility Model and Security Groups</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/create-your-first-amazon-ec2-instance-1/">Lab: Create your first Amazon EC2 Instance (Linux)</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/">Lab: Ceate your first Amazon EC2 Instance (Windows)</a></p>
<h1 id="4ECS-Elastic-Container-Service"><a href="#4ECS-Elastic-Container-Service" class="headerlink" title="4ECS - Elastic Container Service"></a>4<strong>ECS - Elastic Container Service</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/">Course: Introduction to Docker</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/basics-of-using-containers-in-production/container-orchestration-1/">Basics of using Containers in Production</a></p>
<h1 id="5ECR-Elastic-Container-Registry"><a href="#5ECR-Elastic-Container-Registry" class="headerlink" title="5ECR - Elastic Container Registry"></a>5<strong>ECR - Elastic Container Registry</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Course: Overview of AWS Identity &amp; Access Managment (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Docker Push</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">Docker Pull</a></p>
<h1 id="6EKS-Elastic-Container-Service-for-Kubernetes"><a href="#6EKS-Elastic-Container-Service-for-Kubernetes" class="headerlink" title="6EKS - Elastic Container Service for Kubernetes"></a>6<strong>EKS - Elastic Container Service for Kubernetes</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-kubernetes/">Course: Introduction to Kubernetes</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">Course: Introduction to EKS</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Install Kubectl</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/aws-iam-authenticator">Linux IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator">MacOS IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/windows/amd64/aws-iam-authenticator.exe">Windows IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">Configuration map to joing the Worker Node to the EKS Cluster</a></p>
<h1 id="7AWS-Elastic-Beanstalk"><a href="#7AWS-Elastic-Beanstalk" class="headerlink" title="7AWS Elastic Beanstalk"></a>7<strong>AWS Elastic Beanstalk</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/deploy-php-application-using-elastic-beanstalk-26/">Lab: Deploy a PHP Application using AWS Elastic Beanstalk</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/run-controlled-deploy-aws-elastic-beanstalk-43/">Lab: Run a controlled deploy with AWS Elastic Beanstalk</a></p>
<h1 id="8AWS-Lambda"><a href="#8AWS-Lambda" class="headerlink" title="8AWS Lambda"></a>8<strong>AWS Lambda</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html#supported-event-source-s3">AWS Lambda Event Sources</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/">Course: Understanding AWS Lambda to Run and Scale your Code</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/introduction-aws-lambda-22/">Lab: Introduction to AWS Lambda</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/aws-lambda-s3-events-55/">Lab: Process Amazon S3 events with AWS Lambda</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/automating-ebs-snapshots-lambda-and-cloudwatch-events-45/">Lab: Automating EBS snapshots with AWS Lambda</a></p>
<h1 id="10Amazon-Lightsail"><a href="#10Amazon-Lightsail" class="headerlink" title="10Amazon Lightsail"></a>10<strong>Amazon Lightsail</strong></h1><p><a target="_blank" rel="noopener" href="https://lightsail.aws.amazon.com/ls/webapp/home/resources">Amazon Lightsail dashboard</a></p>
<h1 id="12SSL-Server-Certificates"><a href="#12SSL-Server-Certificates" class="headerlink" title="12SSL Server Certificates"></a>12<strong>SSL Server Certificates</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#acm_region">Regions supported by ACM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html">How to retrieve and list server certificates via ACM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">Additional information on ACM</a></p>
<h1 id="13Application-Load-Balancers"><a href="#13Application-Load-Balancers" class="headerlink" title="13Application Load Balancers"></a>13<strong>Application Load Balancers</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/">Course: OSI and TCPIP Networking Models</a></p>
<h1 id="15Classic-Load-Balancers"><a href="#15Classic-Load-Balancers" class="headerlink" title="15Classic Load Balancers"></a>15<strong>Classic Load Balancers</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/elasticloadbalancing/features/#compare">Table showing the differences between the Load Balancers</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Cloud-Practitioner-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-4/" rel="prev" title="AWS-Cloud-Practitioner-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-4">
      <i class="fa fa-chevron-left"></i> AWS-Cloud-Practitioner-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-4
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Linux-6/" rel="next" title="AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Linux-6">
      AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Linux-6 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-Compute-in-AWS"><span class="nav-number">2.</span> <span class="nav-text">What is Compute in AWS?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EC2-Elastic-Compute-Cloud"><span class="nav-number">3.</span> <span class="nav-text">EC2 - Elastic Compute Cloud</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ECS-Elastic-Container-Service"><span class="nav-number">4.</span> <span class="nav-text">ECS - Elastic Container Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ECR-Elastic-Container-Registry"><span class="nav-number">5.</span> <span class="nav-text">ECR - Elastic Container Registry</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Resources-referenced-within-this-lecture"><span class="nav-number">5.0.1.</span> <span class="nav-text">Resources referenced within this lecture:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transcript"><span class="nav-number">5.0.2.</span> <span class="nav-text">Transcript</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EKS-Elastic-Container-Service-for-Kubernetes"><span class="nav-number">6.</span> <span class="nav-text">EKS - Elastic Container Service for Kubernetes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Elastic-Beanstalk"><span class="nav-number">7.</span> <span class="nav-text">AWS Elastic Beanstalk</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Lambda"><span class="nav-number">8.</span> <span class="nav-text">AWS Lambda</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Batch"><span class="nav-number">9.</span> <span class="nav-text">AWS Batch</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Lightsail"><span class="nav-number">10.</span> <span class="nav-text">Amazon Lightsail</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-an-Elastic-Load-Balancer-ELB"><span class="nav-number">11.</span> <span class="nav-text">What is an Elastic Load Balancer (ELB)?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SSL-Server-Certificates"><span class="nav-number">12.</span> <span class="nav-text">SSL Server Certificates</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Application-Load-Balancers"><span class="nav-number">13.</span> <span class="nav-text">Application Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Network-Load-Balancers"><span class="nav-number">14.</span> <span class="nav-text">Network Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Classic-Load-Balancers"><span class="nav-number">15.</span> <span class="nav-text">Classic Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EC2-Auto-Scaling"><span class="nav-number">16.</span> <span class="nav-text">EC2 Auto Scaling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Components-of-EC2-Auto-Scaling"><span class="nav-number">17.</span> <span class="nav-text">Components of EC2 Auto Scaling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Using-ELB-and-Auto-Scaling-Together"><span class="nav-number">18.</span> <span class="nav-text">Using ELB and Auto Scaling Together</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2What-is-Compute-in-AWS"><span class="nav-number">19.</span> <span class="nav-text">2What is Compute in AWS?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3EC2-Elastic-Compute-Cloud"><span class="nav-number">20.</span> <span class="nav-text">3EC2 - Elastic Compute Cloud</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4ECS-Elastic-Container-Service"><span class="nav-number">21.</span> <span class="nav-text">4ECS - Elastic Container Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5ECR-Elastic-Container-Registry"><span class="nav-number">22.</span> <span class="nav-text">5ECR - Elastic Container Registry</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6EKS-Elastic-Container-Service-for-Kubernetes"><span class="nav-number">23.</span> <span class="nav-text">6EKS - Elastic Container Service for Kubernetes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7AWS-Elastic-Beanstalk"><span class="nav-number">24.</span> <span class="nav-text">7AWS Elastic Beanstalk</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8AWS-Lambda"><span class="nav-number">25.</span> <span class="nav-text">8AWS Lambda</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#10Amazon-Lightsail"><span class="nav-number">26.</span> <span class="nav-text">10Amazon Lightsail</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#12SSL-Server-Certificates"><span class="nav-number">27.</span> <span class="nav-text">12SSL Server Certificates</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#13Application-Load-Balancers"><span class="nav-number">28.</span> <span class="nav-text">13Application Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#15Classic-Load-Balancers"><span class="nav-number">29.</span> <span class="nav-text">15Classic Load Balancers</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
