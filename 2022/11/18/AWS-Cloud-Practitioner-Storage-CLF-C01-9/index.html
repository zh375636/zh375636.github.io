<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionHello, and welcome to this course on Storage in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification. Before we get started">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Cloud-Practitioner-Storage-CLF-C01-9">
<meta property="og:url" content="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionHello, and welcome to this course on Storage in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification. Before we get started">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T02:03:31.000Z">
<meta property="article:modified_time" content="2022-11-20T22:59:02.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Cloud-Practitioner-Storage-CLF-C01-9 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Cloud-Practitioner-Storage-CLF-C01-9
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:31" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:31-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:59:02" itemprop="dateModified" datetime="2022-11-20T18:59:02-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Storage in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various Storage services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Storage services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Storage services in AWS, including:</p>
<ul>
<li>The Amazon Simple Storage Service, known as S3;</li>
<li>Amazon Elastic Block Store, or EBS; and the</li>
<li>Amazon Elastic File System, or EFS.</li>
</ul>
<p>We’ll also discuss AWS services that can assist with large-scale data storage, migration, and transfer both into and out of AWS using the AWS Snow family, as well as hybrid cloud storage services and on-premises data backup solutions using AWS Storage Gateway.</p>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="Overview-of-Amazon-S3"><a href="#Overview-of-Amazon-S3" class="headerlink" title="Overview of Amazon S3"></a>Overview of Amazon S3</h1><p>Hello and welcome to this lecture where I will introduce the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon Simple Storage Service</a>, commonly known as S3. Amazon S3 is probably the most heavily used storage service that is provided by AWS simply down to the fact that it can be a great fit for many different use cases, as well as integrating with many different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. Amazon S3 is a fully managed, object-based storage service that is highly available, highly durable, very cost-effective, and widely accessible.</p>
<p>The service itself is promoted as having unlimited storage capabilities making Amazon S3 extremely scalable, far more scalable than your own on-premise storage solution could ever be. There are, however, limitations on the individual size of a single file that it can support. The smallest file size that it supports is zero bytes and the largest file size is five terabytes. Although there is this size limitation on a maximum file size, it’s one that many of us will not perceive as an ongoing inhibitor in the majority of use cases.</p>
<p>The service operates an object storage service which means each object uploaded does not conform to a data structure hierarchy like a file system would, instead its architecture exists across a flat address space and is referenced by a unique URL. Now, if you compare this to file storage, where your data is stored as separate files within a series of directories forming a data structure hierarchy much like your own files are on your own laptop or computer then S3 is very different in comparison. S3 is a regional service and so when uploading data you as the customer are required to specify the regional location for that data to be placed in.</p>
<p>By specifying your region for your data Amazon S3 will then store and duplicate your uploaded data multiple times across multiple availability zones within that region to increase both its durability and availability. For more information on regional availability zones and other AWS global infrastructure components, please see the following blog <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-global-infrastructure/">here</a>. Objects stored in S3 have a durability of ninety-nine point nine nine nine nine nine nine nine nine nine percent, known as eleven nines of durability and so the likelihood of losing data is extremely rare and this is down to the fact that S3 stores multiple copies of the same data in different availability zones. The availability of S3 data objects is dependent on the storage class used and this can range from 99.5% to 99.99%.</p>
<p>The difference between availability and durability is this: when looking at availability AWS ensures that the uptime of Amazon S3 is between 99.5% to 99.99%, depending on the storage class, to enable you to access your stored data. The durability percentage refers to the probability of maintaining your data without it being lost through corruption, degradation of data, or other unknown potential damaging effects. When uploading objects to Amazon S3, a specific structure is used to locate your data in the flat address space.</p>
<p>To store objects in S3, you first need to define and create a bucket. You can think of a bucket as a container for your data. This bucket name must be completely unique, not just within the region you specify, but globally against all other S3 buckets that exist, of which there are many millions. And this is because of the flat address space, you simply can’t have a duplicate name. Once you have created your bucket you can then begin to upload your data within it. By default your account can have up to a hundred buckets, but this is a soft limit and a request to increase this can be made with AWS. Any object uploaded to your buckets are given a unique object key to identify it.</p>
<p>In addition to your bucket, you can if required create folders within the bucket to aid with categorization of your objects for easier data management. Although folders can provide additional management from a data organization point of view, I want to reiterate that Amazon S3 is not a file system and many features of Amazon S3 work at the bucket level and not a specific folder level and so the unique object key for every object contains the bucket, any folders that are present, and also the name of the file itself. Let me now provide a quick overview via a demonstration of the Amazon S3 console and I’ll show you how to create a bucket within the service and upload an object to that bucket and then show you the unique object key of that object. Okay so I’m currently logged into my AWS management console and I can find amazon S3 under the storage category which is down here. So if I select S3 and this has taken me to the S3 dashboard.</p>
<p>Now up here we have our buckets and this list that we have here are a list of buckets that I have already created in my account so this is the bucket name which is the unique bucket identifier. And over here we have the region in which that bucket exists in, so we have some in London some in Ireland some over in US as well, and also, the date created. Over here we have access whether these buckets can be accessed by the public or not. I’m not going to dive too deep into access and security the buckets at this stage, as this is just more of an introduction to give you an overview of the console, but we do have other courses that focus on security.</p>
<p>Now, if I go into one of these buckets, for example, this one here cloudacademyaudio, we can see that I have created a folder in here called Stuart. There’s no other objects, it’s just a folder we can see that by this little icon here, so there’s no actual objects in here, this is just a folder that I created just to help me manage and categorize any objects that I do upload. If I select that folder, I can see I have two more folders here. If I go into this one, for example, I can see that I have an object I have a PNG file. So this is an object that I have uploaded to S3 and we can see that it’s in the courses folder under Stuart under the cloudacademyaudio bucket.</p>
<p>Now, if I select this object, I can get some information about it. I can open it and download it etc. We can see the last time it was modified, the storage class that it belongs to, and I’ll be talking more about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/storage-classes/">storage classes</a> in the next lecture, if there’s any encryption at rest activated, the file size, and here is the unique identifier of the key for this object. So we can see that the key comprises of any folders within the bucket and then also the object name at the end. And this here is the unique identifier of this object on S3, so it gives it a URL.</p>
<p>Now if I want to open this I can simply click on open and as we can see, it’s just an image file, or I can download the object if I want to. So I just wanted to show you there how you can use folders within your bucket and also what the object key looks like as well. So now if I go back to the console, the main dashboard where all my buckets are, I want to show you how to create a bucket quickly. It’s very simple simply click on create bucket, then we need to give it a unique bucket name.</p>
<p>Now remember, this has to be a globally unique name, so if I type in stubucketdemo and then I can select a region that I want this bucket to be in, I’m just going to select the London region, and if I want to, I can copy settings from an existing bucket, but it’s going to go through the different screens to show you the options quickly that you can have when you’re creating a bucket. Click on Next. Here’s some management, we have some management options such as versioning and server access logging. Versioning keeps all versions of an object in the same bucket and server access logging logs requests for access to your bucket. You can also use key value pair tags, you can activate object level logging which will record any API activity with CloudTrail associated with your objects and you can also encrypt your objects as well. I’m just gonna leave all those options as default for this demonstration. Click on next.</p>
<p>Here we can set different permissions, I’m just gonna leave the default block all public access so this will prevent anyone from outside of my VPC accessing any data within my bucket. Click on next. We just have a review of the settings that we selected and then all you need to do is click on create bucket. So if I scroll down to my bucket that I just created, which was stubucketdemo. If I select that, we can see here that I’ve got no folders and no objects. So if you want to create a folder, you simply click on create folder, just give it a name. If you want to add any encryption you can do so here. I’m just going to use none as a default. Now, I can either add an object directly under this bucket or I can add it into that folder. Let me just add it directly under the bucket name of stubucketdemo.</p>
<p>So to upload an object you simply click on upload, add files, select your object that you’d like to upload or objects, click on next, you know we have some permissions here as to who can read or write to the object, and as we can see here, we have the block public access setting turned on for this bucket. Click on next. Now here we have our storage classes and depending on what storage class we select it will affect the durability, the availability, and also the cost of your object being stored. Now, I’m gonna go deeper into the different storage classes in the next lecture so I won’t go over this too deep now. For the sake of this demonstration I’m just going to select the standard storage class.</p>
<p>Then we have a review page and then simply upload. And then we have my object that’s been uploaded to my bucket. Again, if I select it, I can see the object key and also the unique object URL as well. So that’s just a very quick demonstration to show you what the S3 console looks like, how to create buckets, how to create folders, and also upload objects as well, just so hopefully you can piece things together a little bit easier if you’ve not used Amazon S3 before.</p>
<h1 id="Storage-Classes"><a href="#Storage-Classes" class="headerlink" title="Storage Classes"></a>Storage Classes</h1><p>Hello and welcome to this lecture covering <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon S3</a> storage classes. As we just saw in the demonstration, I had an option to select which storage class I wanted my uploaded object to reside in. Amazon S3 offers these different storage classes to allow you to select a class based on performance features and costs and it’s down to you to select the storage class that you require for the data. The storage classes available are as follows S3 Standard, S3 Intelligent Tiering, S3 Standard Infrequent Access, S3 One Zone Infrequent Access, S3 Glacier, and S3 Glacier Deep Archive.</p>
<p>S3 Standard. This storage class is considered a general-purpose storage class. It is ideal for a range of use cases where you need high throughput with low latency with the added ability of being able to access your data frequently. By copying data to multiple availability zones, S3 Standard offers eleven nines of durability across multiple availability zones, meaning the OData remains protected against a single availability zone failure. It also offers a 99.99% availability across the year, which is the highest availability that S3 offers. From a security standpoint this storage class also has the added support of SSL, Secure Sockets Layer, for encrypting data in transit in addition to encryption options for when the data is at rest. With management features such as lifecycle rules, objects in S3 Standard can automatically be moved to another storage class. For those unfamiliar with life cycle rules, they provide an automatic method of managing the life of your data while it is being stored on Amazon S3. By adding a life cycle wall to a bucket you are able to configure and set specific criteria that can automatically move your data from one class to another or delete it from Amazon S3 altogether. You may want to do this as a cost saving exercise by moving data to a cheaper storage class after a set period of time.</p>
<p>S3 Intelligent Tiering. This storage class is ideal for those circumstances where the frequency of access to the object is unknown. Effectively, we have unpredictable data access patterns and so by using this storage class, it can help to optimize your storage costs. Depending on your data access patterns of objects in the Intelligent Tiering Class, S3 will move your objects between two different tiers, these being frequent and infrequent access. Now, these classes are a part of the Intelligent Tiering Class itself and are separate from the existing storage classes I listed earlier. When the objects are moved to Intelligent Tiering, they are placed within the frequent access tier, which is the more expensive of the two tiers. If an object is not accessed for 30 days then AWS will automatically move the object to the cheaper tier known as the infrequent access tier. Once that same object is accessed again, it will automatically be moved back to the frequent tier. Much like S3 Standard, S3 Intelligent Tiering also offers 11 nines of durability across multiple availability zones offering protection against the loss of a single AZ. However, its availability isn’t quite as high as S3 Standard as it set at 99.9%. This storage class also has the added support of SSL for encrypting data in transit in addition to encryption options for when the data is at rest. S3 Intelligent Tiering also supports the lifecycle rules and matches the same performance throughput and low latency as S3 Standard.</p>
<p>S3 Standard infrequent access. This can be seen as the equivalent to the infrequent tier from the Intelligent Tiering class as it is designed for data that does not need to be accessed as frequently as data within the Standard tier, and yet still offers high throughput and low latency access, much like S3 Standard does. As with all other S3 storage classes, it carries that 11 9s durability across multiple AZs, again by copying your objects to multiple availability zones within a single region to protect against AZ outages. It shares the same availability as Intelligent Tiering of 99.9 percent. As a result, this storage class comes at a cheaper cost than S3 Standard. Common security features such as SSL for encryption in transit and data at rest encryption is supported as well as management controls such as lifecycle rules to automatically move objects to an alternate storage class based on your requirements.</p>
<p>S3 One Zone Infrequent Access. By now you can probably assume what this storage class comprises of based off of the previous classes that I’ve already discussed. However, again, being an infrequent storage class it is designed for objects that are unlikely to be accessed frequently. It also carries the same throughput and low latency. However, the durability, although remaining at eleven nines only exists across a single availability zone. As the name implies to this class it is one zone, as in one availability zone. So the objects will be copied multiple times to different storage locations within the same availability zone instead of across multiple availability zones. This results in a 20% storage cost reduction when compared to S3 Standard. One Zone IA does, however, offer the lowest level of availability which is currently 99.5 percent and this is down to the fact that your data is being stored in a single availability zone. Should the AZ storing your data become unavailable then you will lose access to your data or even worse it may become completely lost should the AZ be destroyed in a catastrophic event. Again, life cycle rules and encryption mechanisms are in place to protect your data both in transit and at rest.</p>
<p>S3 Glacier. The next two storage classes are associated with S3 Glacier which is used for archival data. Firstly let me explain more about S3 Glacier, as it can be accessed separately from the Amazon S3 service but closely interacts with it S3 Glacier storage classes directly interact with the Amazon S3 lifecycle rules discussed previously. However, the fundamental difference with the Amazon Glacier storage classes come at a fraction of the cost when it comes to storing the same amount of data than the S3 storage classes. So what’s the catch? Well, it doesn’t provide you the same features as Amazon S3 but more importantly, it doesn’t provide you instant access to your data.</p>
<p>So what do Amazon Glacier classes offer exactly? Well, they offer an extremely low-cost long term durable storage solution which is often referred to as cold storage, ideally suited for long term backup and archival requirements. It’s capable of storing the same data types as Amazon S3, effectively any object, however, like I just mentioned it doesn’t provide instant access to your data. In addition to this, there are other fundamental differences which makes this service fit for purpose for other use cases. The service itself has 11 nines of durability making this just as durable as Amazon S3. Again this is achieved by replicating the data across multiple different availability zones within a single region but it provides the storage at a considerably lower cost compared to that of Amazon S3. And this is because retrieval of data stored in Glacier is not an instant access retrieval process. When retrieving your data it can take up to several hours to gain access to it depending on certain criteria. The data structure within Glacier is centered around vaults and Archives. Buckets and folders are not used. They are purely used for S3.</p>
<p>A Glacier vault simply acts as a container for Glacier archives. These vaults are regional and as such during the creation of these vaults, you are asked to supply the region in which they will reside. Within these vaults, we then have our data which is stored as an archive and these archives can be any object similar to S3. Thankfully you can have unlimited archives within your Glacier vaults, so from a capacity perspective, it follows the same rule as S3. Effectively you have access to an unlimited quantity of storage for your archives and vaults. Now whereas Amazon S3 provided a nice graphical user interface to view, manage, and retrieve your data within buckets and folders, Amazon Glacier does not offer this service.</p>
<p>The Glacier dashboard within AWS management console allows you to create your vaults, set data retrieval policies, and event notifications. When it comes to moving data into S3 Glacier for the first time it’s effectively a two-step process. Firstly, you need to create your vaults as your container for your archives and this could be completed using the Glacier console. Secondly, you need to move your data into the Glacier vault using the available API or SDKs. As you may be thinking, there’s also another method of moving your data into Glacier and this is by using the S3 lifecycle rules that I discussed earlier. When it comes to retrieving your archives, which is your data, you will again have to use some form of code to do so, either the APIs, SDKs or the AWS CLI. Either way, you must first create an archival retrieval job, then request access to all or part of that archive.</p>
<p>Now you have more of an understanding of S3 Glacier, let me review the two S3 Glacier storage classes. Firstly, S3 Glacier. This is the default Standard storage class within S3 Glacier offering a highly secure using in transit and at rest encryption low-cost and durable storage solution. The durability matches that of other S3 storage classes, being 11 9s across multiple availability zones, and the availability of S3 Glacier is 99.9%. It’s simple to add data to this storage class using the S3 put APIs is in addition to S3 lifecycle rules. However, it does offer a variety of retrieval options depending on how urgently you need the data back, each offering a different price point. These being expedited, Standard, and bulk.</p>
<p>Expedited. This is used when you have an urgent requirement to retrieve your data but the request has to be less than 250 megabytes. The data is then made available to you in one to five minutes and this is the most expensive retrieval option of the three.</p>
<p>Standard. This can be used to retrieve any of your archives no matter their size but your data will be available in three to five hours, so much longer than the expedited option and this is the second most expensive of the three options.</p>
<p>And finally, bulk. This option is used to retrieve petabytes of data at a time, however, this typically takes between five and twelve hours to complete. This is the cheapest of the retrieval options so it really depends on how much data and how quickly you need it, as the retrieval speed and cost to be made by your retrieval option.</p>
<p>S3 Glacier Deep Archive. Out of all the storage classes offered by S3, Glacier Deep Archive is the cheapest and again being a Glacier class, it focuses on long-term storage. This is an ideal storage class for circumstances that require specific data retention regulations and compliance with minimal access, such as those within the financial or health sector where data records might need to be legally retained for seven years or even longer. The durability and availability matches that of S3 Glacier with eleven 9s durability across multiple AZss with 99.9% availability.</p>
<p>Adding data into deep archive follows the same processes as S3 Glacier, using S3 put APIs in addition to S3 lifecycle rules. Deep Archive, however, does not offer multiple retrieval options. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> states that the retrieval of the data will be within 12 hours or less. To summarize some of the common features between the storage classes, this table clearly shows how they differ. As you can see, the main difference of the classes is the durability and availability percentages, in addition to the pricing.</p>
<p>So when selecting your class for your data you really need to be asking yourself the following questions: how critical is my data? Does it require the highest level of durability? How reproducible is the data? Can it be easily created again if need be? and how often is the data likely to be accessed? For detailed information on Amazon S3 pricing covering all storage classes discussed please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/introduction/">Understanding and Optimizing Costs with AWS Storage Services</a>.</p>
<h1 id="EC2-Instance-Storage"><a href="#EC2-Instance-Storage" class="headerlink" title="EC2 Instance Storage"></a>EC2 Instance Storage</h1><p>Hello, and welcome to this lecture covering EC2 Instance Level Storage. Which is referred to as an instance store volume. The first point to make about EC2 instance store volumes, is that the volumes physically reside on the same host that provides your EC2 instance itself, acting as local disc drives, allowing you to store data locally to that instance. Up until now within this course we have discussed persistent storage options. But instance store volumes provide ephemeral storage for you EC2 instances. </p>
<p>Ephemeral storage means that the block level storage that it provides offers no means of persistency. Any data stored on these volumes is considered temporary. With this in mind, it is not recommended to store critical or valuable data on these ephemeral instance store volumes, as it could be lost, should an event occur. By an event, let me explain under what conditions that your data would be lost, should it be stored on one of these volumes. </p>
<p>If your instance is either stopped or terminated, then any data that you have stored on that instance store volume associated with this instance will be deleted without any means of data recovery. However, if your instance was simply rebooted, your data would remain intact. Although, you can control when your instances are stopped or terminated, giving you the opportunity to either back-up the data or move it to another persistent volume store, such as the elastic block store service. Sometimes this control is not always possible. Let’s consider you had critical data stored on an ephemeral instance store volume and then the underlying host that provided your EC2 instance and storage failed. You had no warning that this failure was going to occur, and as a result of this failure, the instance was stopped or terminated. Now all of your data on these volumes is lost. When a stop and start, or termination occurs, all the blocks on the storage volume are reset, essentially wiping data. So, you might be thinking, why use these volumes? What use do they have if there is a chance that you are going to lose data? They do, in fact, have a number of benefits. </p>
<p>From a cost perspective, the storage used is included in the price of the EC2 instance. So, you don’t have an additional spend on storage cost. The I&#x2F;O speed on these volumes can far exceed those provided by the alternative instance block storage, EBS for example. When using store optimized instance families, such as the I3 Family, it’s potentially possible to reach 3.3 million random read IOPS, and 1.4 million write IOPS. With speeds like this, it makes it ideal to handle the high demands of no SQL databases. However, any persistent data required would need to be replicate or copied to a persistent data store in this scenario. Instance store volumes are generally used for data that is frequently changing; that doesn’t need to be retained, as such, they are great to be used as a cache or buffer. They are also commonly used for service within a load balancing group, where data is replicated across the fleet such as a web server pool. </p>
<p>Not all instance types support instance store volumes. So, if you do have a need where these instance store volumes would work for your use case, then be sure to check the latest AWS documentation to ascertain if the instance type you’re looking to use supports the volume. The size of your volumes, however, will increase as you increase the EC2 instance size. </p>
<p>From a security stance, instance store volumes don’t offer any additional security features. As to be honest, they are not separate service like the previous storage options I have already explained. They are simply storage volumes attached to the same host on the EC2 instance, and they are provided as a part of the EC2 service. So, they effectively have the same security mechanisms provided by EC2. This can be IAM policies dictating which instances can and can’t be launched, and what action you can perform on the EC2 instance, itself. If you have data that needs to remain persistent, or that needs to be accessed and shared by others, then EC2 instance store volumes are not recommended. If you need to use block level storage and want a quick and easy method to maintain persistency, then there is another block level service that is recommended. This being the elastic block store service.</p>
<h1 id="Overview-of-EBS"><a href="#Overview-of-EBS" class="headerlink" title="Overview of EBS"></a>Overview of EBS</h1><p>In this lecture, I shall be talking about the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-block-store-ebs-1060/course-introduction/">Amazon Elastic Block Store</a> service, known as EBS, which provides storage to your EC2 instances via EBS volumes, which offer different benefits to that of instance store volumes used with some EC2 instances.</p>
<p>EBS provides persistent and durable block level storage. As a result, EBS volumes offer far more flexibility with regards to managing the data when compared to data stored on instance store volumes. EBS volumes can be attached to your EC2 instances, and are primarily used for data that is rapidly changing that might require a specific Input&#x2F;Output operations Per Second rate, also known as IOPS.</p>
<p>EBS volumes are independent of the EC2 instance, meaning that they exist as two different resources. They are logically attached to the instance instead of directly attached like instance store volumes. From a connectivity perspective, only a single EBS volume can only ever be attached to a single EC2 instance. However, multiple EBS volumes can be attached to a single instance.</p>
<p>Due to the EBS ability to enforce persistence of data, it doesn’t matter if your instances are intentionally or unintentionally stopped, restarted, or even terminated, the data will remain intact when configured to do so. EBS also offers the ability to provide point in time backups of the entire volume as and when you need to. These backups are known as snapshots and you can manually invoke a snapshot of your volume at any time, or use Amazon CloudWatch events to perform an automated schedule of backups to be taken at a specific date or time that can be recurring.</p>
<p>The snapshots themselves are then stored on Amazon S3 and so are very durable and reliable. They are also incremental, meaning that each snapshot will only copy data that has changed since the previous snapshot was taken. Once you have a snapshot of an EBS volume, you can then create a new volume from that snapshot. So, if for any reason you lost access to your EBS volume through or incident or disaster, you can recreate the data volume from an existing snapshot and then attach that volume to a new EC2 instance. To add additional flexibility and resilience, it is possible to copy a snapshot from one region to another.</p>
<p>Looking at the subject of high availability and resiliency, your EBS volumes are, by default, created with reliability in mind. Every write to a EBS volume is replicated multiple times within the same availability zone of your region to help prevent the complete loss of data. This means that your EBS volume itself is only available in a single availability zone. As a result, should your availability zone fail, you will lose access to your EBS volume. Should this occur, you can simply recreate the volume from your previous snapshot and attach it to another instance in another availability zone.</p>
<p>There are two types of EBS volumes available. Each have their own characteristics. These being SSD backed storage, solid state drive, and HDD backed storage, hard disk drive. This allows you to optimize your storage to fit your requirements from a cost to performance perspective.</p>
<p>SSD backed storage is better suited for scenarios that work with smaller blocks. Such as databases using transactional workloads. Or often as boot volumes for your EC2 instances. Whereas HDD backed volumes are designed for better workloads that require a higher rate of throughput, such as processing big data and logging information. So, essentially working with larger blocks of data.</p>
<p>These volume types can be broken down even further. Looking at the following table we can see how different volumes can be used for both SSD and HDD volumes types.</p>
<p>You can see that depending on the use case for your EBS volume you can select the most appropriate type. Each of these volumes also offer different performance factors which include: Volume size, Max IOPS per volume, Max throughput per volume, Max IOPS per instance, Max Throughput per instance, and Dominant performance.</p>
<p>The performance of volumes change frequently and so for the latest information on these volume types, please refer to the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">here</a>.</p>
<p>For those who are not familiar with provisioned IOPS (input&#x2F;output operations per second) volumes, they deliver enhanced predictable performance for applications requiring I&#x2F;O intensive workloads. When working with these volumes you also have the ability to specify at IOPS rate during the creation of a new EBS volume, and when the volume is attached to an EBS-optimized instance, EBS will deliver the IOPS defined and required within 10%, 99.9% of the time throughout the year.</p>
<p>The throughput optimized HDD volumes are designed for frequently accessed data and are ideally suited to work well with large data sets requiring throughput-intensive workloads, such as data streaming, big data, and log processing. These volumes will deliver the expected throughput 99% of the time over a given year, and an important point to make is that these volumes can’t be used as boot volumes for your instances.</p>
<p>The cold HDD volumes offer the lowest cost compared to all other EBS volumes types. They are suited for workloads that are large in size and accessed infrequently. They will deliver the expected throughput 99% of the time over a given year, and again, it is not possible to use these as boot volumes for your EC2 instances.</p>
<p>One great feature of EBS is its ability to enhance the security of your data, both at rest and when in transit, through data encryption. This is especially useful when you have sensitive data, such as personally identifiable information, stored in your EBS volume. And in this case, you may be required to have some form of encryption from a regulatory or governance perspective. EBS offers a very simple encryption mechanism. Simple in the fact that you don’t have to worry about managing the data keys to perform the encryption process yourself. It’s all managed and implemented by EBS. All you are required to do is to select if you want the volume encrypted or not during its creation via a checkbox.</p>
<p>The encryption process uses the AES-256 encryption algorithm and provides its encryption process by interacting with another AWS service, the key management service, known as KMS. KMS uses customer master keys, CMKs, enabling the encryption of data across a range of AWS services, such as EBS in this instance.</p>
<p>To learn more about the Key Management Service, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Any snapshot taken from an encrypted volume will also be encrypted, and also any volume created from this encrypted snapshot will also be encrypted. You should also be aware that this encryption option is only available on selected instance types.</p>
<p>One final point to make on EBS encryption is that you can create a default region setting that ensures that all EBS volumes created will be encrypted by default.</p>
<p>For a detailed overview of exactly how this encryption process works, please take a look at the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/how-to-encrypt-an-ebs-volume-the-new-amazon-ebs-encryption/">blog post</a>.</p>
<p>As EBS volumes are separate to EC2 instances, you can create an EBS volume in a couple of different ways from within the management console. During the creation of a new instance and attach it at the time of launch, or from within the EC2 dashboard of the AWS management console as a standalone volume ready to be attached to an instance when required. When creating an EBS volume during an EC2 instance launch, at step four of creating that instance, you are presented with the storage configuration options. Here you can either create a new blank volume or create it from an existing snapshot. You can also specify the size and the volume type, which we discussed previously. Importantly, you can decide what happens to the volume when the instance terminates. You can either have the volume to be deleted with the termination of the EC2 instance, or retain the volume, allowing you to maintain the data and attach it to another EC2 instance. Lastly, you also have the option of encrypting the data if required.</p>
<p>You can also create the EBS volume as a standalone volume. By selecting the volume option under EBS from within the EC2 dashboard of the management console, you can create a new EBS volume where you’ll be presented with the following screen.</p>
<p>Here you will have many of the same options. However, you can specify which availability zone that the volume will exist in, allowing you to attach it to any EC2 instance within that same availability zone. As you might remember, EBS volumes can only be attached to EC2 instances that exist within the same availability zone.</p>
<p>EBS volumes also offer the additional flexibility of being able to resize them elastically should the requirement arise. Perhaps you’re running out of disk space and need to scale up your volume. This can be achieved by modifying the volume within the console or via the AWS CLI. You can also perform the same resize of the volume by creating a snapshot of your existing volume, and then creating a new volume from that snapshot with an increased capacity size.</p>
<p>As we can see, EBS offers a number of benefits over EC2 instance store volumes. But EBS is not well suited for all storage requirements. For example, if you only needed temporary storage or multi-instance storage access, then EBS is not recommended, as EBS volumes can only be accessed by one instance at a time. Also, if you needed very high durability and availability of data storage, then you would be better suited to use Amazon S3 or EFS, the elastic file system.</p>
<p>That now brings me to the end of this lecture and to the end of this introductory course and you should now have a greater understanding of the Amazon Elastic Block Store service and how it can be used as a storage option for your EC2 instances.</p>
<p>If you have any feedback, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="What-is-the-Amazon-Elastic-File-System"><a href="#What-is-the-Amazon-Elastic-File-System" class="headerlink" title="What is the Amazon Elastic File System?"></a>What is the Amazon Elastic File System?</h1><p>Hello and welcome to this lecture where I will explain what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-file-system-1137/course-introduction/">Amazon EFS</a> service is and how it fits into the storage ecosystem. Let me start by taking a step back and looking at where the EFS service fits in within the world of AWS storage. Firstly, I want to look at the array of AWS storage offerings and compare a few of them. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> has more storage solutions than I’m going to discuss in this course and I will probably continue to add more in the future. But I’m just going to focus on three different services. The reason I’ve selected these three is that, at first glance, they may seem similar and many people can be unsure which of these solutions to choose from to fit their current storage requirements. </p>
<p>Amazon Simple Storage Service or S3 is an object storage solution. Object storage stores everything as a single object, not in small chunks or blocks. With this type of storage, you upload a file and if the file changes to replace it, the entire file will be replaced. This type of storage is best for situations where files are written once and then accessed many times. It’s not optimal for situations that require both heavy read and write access at the same time. So Amazon S3 is usually used for storage of large files such as video files, images, static websites, and backup archives. For example, Netflix uses S3 for their data streaming service. They upload large movie files once and then subscribers access and play the movies many, many times. </p>
<p>The next service is the Amazon Elastic Block Store or EBS, and it’s block-level storage. Files are not stored as single objects. They’re stored in small chunks of blocks so that only the portion of the file that is changed will be updated. This type of storage is optimized for low latency access and when fast, concurrent read and write operations are needed. EBS provides persistent block storage volumes for use with a single EC2 instance. As described, EBS is persistent, meaning that even if you stop or terminate an EC2 instance that’s using EBS, the data on the EBS volume remains intact. You should use this type of storage like a computer hard drive where you store operating system files, applications and other files you wish to obtain for use with your EC2 instance.</p>
<p>Amazon Elastic File System, or EFS, is considered file-level storage and is also optimized for low latency access, but unlike EBS, it supports access by multiple EC2 instances at once. It appears to users like a file manager interface and uses standard file system semantics such as locking files, renaming files, updating files and uses a hierarchy structure. This is just like what we’re used to on standard premise-based systems. This type of storage allows you to store files that are accessible to network resources. </p>
<p>Before diving deep on EFS, let me discuss how people are traditionally used to accessing network files and resources. In traditional premises-based networks, users access files by browsing network resources that connect to a server, perhaps via a mapped drive that has been configured for them, and once they connect, they will see a tree view of available folders and files. This functionality is generally provided by various local area network systems such as file servers or storage area network, a SAN, or network-attached storage, a NAS. </p>
<p>Now let’s move on from the traditional premises-based solutions and talk about cloud-based solutions, specifically within AWS and the Amazon Elastic File System service. EFS provides simple, scalable file storage for use with Amazon EC2 instances. Much like traditional file servers, or a SAN or a NAS, Amazon EFS provides the ability for users to browse cloud network resources. EC2 instances can be figured to access Amazon EFS instances using configured mount points. Now, mount points can be created in multiple availability zones that attach to multiple EC2 instances. So, much like your traditional land servers, EC2 instances are connected to a network file system, Amazon EFS. So from a user standpoint, the result is the same. The user accesses network resources just as they always have done except for now, it’s done using cloud resources. </p>
<p>EFS is a fully managed, highly available and durable service that allows you to create shared file systems that can easily scale to petabytes in size with low latency access. EFS has been designed to maintain a high level of throughput in addition to low latency access response, and these performance factors make EFS a desirable storage solution for a wide variety of workloads, and use cases and can meet the demands of tens, hundreds or even thousands of EC2 instances concurrently. Being a managed service, there is no need for you to provision any file servers to manage the storage elements or provide any maintenance of those servers. This makes it a very simple option to provide file-level storage within your environment. It uses standard operating system APIs, so any application that is designed to work with standard operating system APIs will work with EFS. It supports both NFS versions 4.1 and 4.0, and uses standard file system semantics such as strong consistency and file locking. It’s replicated across availability zones in a single region making EFS a highly reliable storage service. </p>
<p>As the file system can be accessed by multiple instances, it makes it a very good storage option for applications that scale across multiple instances allowing for parallel access of data. The EFS file system is also regional, and so any application deployments that span across multiple availability zones can all access the same file systems providing a level of high availability of your application storage layer. At the time of writing this course, EFS is not currently available within all regions. For a list of supported regions, please visit the following link: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region">https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region</a>. </p>
<p>That now brings me to the end of this lecture.</p>
<h1 id="Storage-Classes-and-Performance-Options"><a href="#Storage-Classes-and-Performance-Options" class="headerlink" title="Storage Classes and Performance Options"></a>Storage Classes and Performance Options</h1><p>Hello and welcome to this lecture, where I will be discussing the different storage class options that EFS provides in addition to how you can alter and configure certain performance factors depending on your use case of EFS. Now, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/introduction/">Amazon EFS</a> offers two different storage classes to you, which each offer different levels of performance and cost, these being Standard and Infrequent Access, known as IA. The Standard storage class is the default storage used when using EFS. However, Infrequent Access is generally used if you’re storing data on EFS that is rarely accessed. By selecting this class, it offers a cost reduction on your storage. </p>
<p>The result of the cheaper storage means that there is an increased first-byte latency impact when both reading and writing data in this class when compared to that of Standard. The costs are also managed slightly differently. When using IA, you are charged for the amount of storage space used, which is cheaper than that compared to Standard. However, with IA, you are also charged for each read and write you make to the storage class. This helps to ensure that you only use this storage class for data that is not accessed very frequently, for example, data that might be required for auditing purposes or historical analysis. </p>
<p>With the Standard storage class, you are only charged on the amount of storage space used per month. Both storage classes are available in all regions where EFS is supported. And importantly, they both provide the same level of availability and durability. If you are familiar with S3, then you may also be familiar with S3 lifecycle policies for data management. Within EFS, a similar feature exists known as EFS lifecycle management. When enabled, EFS will automatically move data between the Standard storage class and the IA storage class. This process occurs when a file has not been read or written to for a set period of days, which is configurable, and your options for this period range include 14, 30, 60, or 90 days. </p>
<p>Depending on your selection, EFS will move the data to the IA storage class to save on cost once that period has been met. However, as soon as that same file is accessed again, the timer is reset, and it is moved back to the Standard storage class. Again, if it has not been accessed for a further period, it will then be moved back to IA. Every time a file is accessed, its lifecycle management timer is reset. The only exceptions to data not being moved to the IA storage class is for any files that are below 128K in size and any metadata of your files, which will all remain in the Standard storage class. </p>
<p>If your EFS file system was created after February 13th, 2019, then the life cycle management feature can be switched on or off. Let me now take a look at the different performance modes that EFS offers. AWS are where the EFS can be used for a number of different use cases and workloads, and as such, each use case might require a change of performance from a throughput, IOPS, and latency point of view. As a result, AWS has introduced two different performance modes that can be defined during the creation of your EFS file system. These being General Purpose, and Max I&#x2F;O.</p>
<p>Now, General Purpose is a default performance mode and is typically used for most use cases. For example, home directories and general file-sharing environments. It offers an all-round performance and low latency file operation, and there is a limitation of this mode allowing only up to 7,000 file system operations per second to your EFS file system. If, however, you have a huge scale architecture, where your EFS file system is likely to be used by many thousands of EC2 instances concurrently, and will exceed 7,000 operations per second, then you’ll need to consider Max I&#x2F;O. Now, this mode offers virtually unlimited amounts of throughput and IOPS. The downside is, however, that your file operation latency will take a negative hit over that of General Purpose. </p>
<p>The best way to determine which performance option that you need is to run tests alongside your application. If your application sits comfortably within the limit of 7,000 operations per second, then General Purpose will be best suited, with the added plus point of lower latency. However, if your testing confirms 7,000 operations per second may be reached or exceeded, then select Max I&#x2F;O.</p>
<p>When using the General Purpose mode of operations, EFS provides a CloudWatch metric percent I&#x2F;O limit, which will allow you to view operations per second as a percentage of the top 7,000 limit. This allows you to make the decision to migrate and move to the Max I&#x2F;O file system, should your operations be reaching that limit. </p>
<p>In addition to the two performance modes, EFS also provides two different throughput modes, and throughput is measured by the rate of mebibytes. The two modes offered are Bursting Throughput and Provisioned Throughput. Data throughput patterns on file systems generally go through periods of relatively low activity with occasional spikes in burst usage, and EFS provisions throughput capacity to help manage this random activity of high peaks.</p>
<p>With the Bursting Throughput mode, which is the default mode, the amount of throughput scales as your file system grows. So the more you store, the more throughput is available to you. The default throughput available is capable of bursting to 100 mebibytes per second, however, with the standard storage class, this can burst to 100 mebibytes per second per tebibyte of storage used within the file system.</p>
<p>So, for example, presume you have five tebibytes of storage within your EFS file system. Your burst capacity could reach 500 mebibytes per second. The duration of throughput bursting is reflected by the size of the file system itself. Through the use of credits, which are accumulated during periods of low activity, operating below the baseline rate of throughput set at 50 mebibytes per tebibyte of storage used, which determines how long EFS can burst for. Every file system can reach its baseline throughput 100% of the time. By accumulating, getting credits, your file system can then burst above your baseline limit. The number of credits will dictate how long this throughput can be maintained for, and the number of burst credits for your file system can be viewed by monitoring the CloudWatch metric of BurstCreditBalance. </p>
<p>If you are finding that you’re running out of burst credits too often, then you might need to consider using the Provisioned Throughput mode. Provisioned Throughput allows you to burst above your allocated allowance, which is based upon your file system size. So if your file system was relatively small but the use case for your file system required a high throughput rate, then the default bursting throughput options may not be able to process your request quick enough. In this instance, you would need to use provisioned throughput. However, this option does incur additional charges, and you’ll pay additional costs for any bursting above the default option of bursting throughput. That brings me to the end of this lecture, now I want to shift my focus on creating and connecting to an EFS file system from a Linux based instance.</p>
<h1 id="What-is-the-Snow-Family"><a href="#What-is-the-Snow-Family" class="headerlink" title="What is the Snow Family?"></a>What is the Snow Family?</h1><p>In this lecture I want to answer 2 simple questions:</p>
<ol>
<li>What is the snow family </li>
<li>and what does it consist of?</li>
</ol>
<p>So firstly, what is it? The snow family consists of a range of physical hardware devices that are all designed to enable you to transfer data into <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> from the edge or beyond the Cloud, such as your Data Center, but they can also be used to transfer data out of AWS too, for example, from Amazon S3 back to your Data Centre. </p>
<p>It’s unusual when working with the cloud to be talking about physical devices or components, normally your interactions and operations with AWS generally happen programmatically via a browser or command line interface. The snow family is different, instead, you will be sent a piece of hardware packed with storage and compute capabilities to perform the required data transfer outside of AWS, and when complete, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">device</a> is then sent back to AWS for processing and the data uploaded to Amazon S3.</p>
<p>You can perform data transfers from as little as a few terabytes using an AWS snowcone all the way up to a staggering 100 petabytes using a single AWS snowmobile, and I’ll be talking more about these different snow family members shortly. Now of course when we are talking about migrating and transferring data at this magnitude, using traditional network connectivity is sometimes simply not feasible from a time perspective. For example, let’s assume you needed to transfer just 1petabye of data over a 1gbps using Direct Connect it would take 104 Days, 5 Hours 59 Minutes, 59.25 Seconds, not forgetting the cost of the data transfer fees too! </p>
<p>In addition to these devices packing some serious storage capacity for data transfer, some of them also come fitted with compute power, allowing you to run usable EC2 instances that have been designed for the snow family enabling your applications to run operations in often remote and difficult to reach environments, even without having a data center in sight, and when working with a lack of persistent networking connectivity or power. For example, the snowcone comes with the ability to add battery packs increasing their versatility. The enablement of running EC2 instances makes it possible to use these devices at the edge to process and analyze data much closer to the source.   </p>
<p>So let’s now take a look at what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/introduction/">snow family</a> consists of to get a better understanding of what these devices are.</p>
<p>As you can see from this table, both from a physical and capacity perspective, the snowcone is the smallest followed by the snowball and finally the snowmobile. You may also notice that the snowball comes in 3 choices, compute optimized, compute optimized with GPU, and storage optimized, each offering a different use case, however, each of these 3 offerings all come in the same size device. </p>
<h1 id="Which-Snow-Device-Do-I-Need"><a href="#Which-Snow-Device-Do-I-Need" class="headerlink" title="Which Snow Device Do I Need?"></a>Which Snow Device Do I Need?</h1><p>In this lecture, I will be looking at different scenarios to help you decide which <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/introduction/">snow device</a> to use and when.</p>
<p>So we have the following snow devices AWS Snowcone, AWS Snowball, and AWS Snowmobile. </p>
<p>The AWS Snowcone is the smallest of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/what-is-the-snow-family/">snow family</a>, this has been designed to be lightweight, easily portable, allowing you to easily use the device pretty much anywhere and under any conditions due to the ruggedness of the casing, and the added advantage of being able to run on battery should a persistent mains connection not be available. It can easily fit into a standard backpack, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> have even demonstrated that the snowcone can be attached to a drone emphasising its portability and versatility. Packed with 8TB of storage and an EC2 instance, this device is perfect for taking your computing needs way beyond the cloud and your Data Centre allowing you to capture, process, and analyze data, perhaps via IoT sensors, which can then be shipped back to AWS for data transfer, or you could even use AWS DataSync to transfer the data on-line over your traditional network connectivity. </p>
<p>For those unaware of AWS DataSync, it’s a service that allows you to easily and securely transfer data from your Snowcone or your on-premise data center, to AWS storage services. It can also be used to manage data transfer between 2 different AWS storage services too, so it’s a great service to help you migrate, manage, replace and move data between different storage locations.  </p>
<p>This is essentially the elder sibling of the Snowcone, it’s bigger in size and it contains a greater amount of storage and compute power. This brings a new set of use cases for this device, it’s primarily used for large scale data transfer operations, up to 80TB at a time, both in and out of AWS. The devices themselves can be rack mounted in your data centre, and if need be clustered in groups of 5-10 devices. Unlike Snowcones, they can’t be powered by battery expansion packs, and they are not as portable, for example, you can’t stick a snowball in a backpack and walk up a mountain, or strap it to a drone! </p>
<p>The Storage optimized snowball is targeted for data migrations and transfers with its storage being compatible with both S3 object storage and EBS volumes. The Compute optimized snowball is a great option if you need to handle compute intensive edge computing workloads in disconnected environments. From a storage perspective, it also comes with 42 TB of usable HDD capacity which comes compatible with EBS volumes and S3 object storage. The Compute Optimized with GPU option is used to accelerate AI, HPC, and graphics, which is great when working with video analysis and graphic intensive use cases. </p>
<p>Both the Snowcone and Snowball can be used for many of the same use cases which I will reference in just a moment, so if that’s the case, when would you use the snowcone over the snowball and vise versa?</p>
<p>You would use the snowcone if you:</p>
<ul>
<li>Needed a portable device that you could easily carry to difficult to reach locations and situations</li>
<li>Only needed a maximum of 8TB storage</li>
<li>If you needed the ability to perform on-line data transfer using AWS DataSync, preventing you the need to send the Snowcone back to AWS for an off-line data transfer</li>
<li>If you didn’t have a consistent power support and you needed the support of a battery pack</li>
</ul>
<p>Alternatively, you would use the Snowball device if you: </p>
<ul>
<li>Didn’t need to provide mobility to the snow device and it could remain in one location for a set period of time</li>
<li>Needed to transfer data of up to 80TB</li>
<li>Needed the ability to run enhanced graphics processing by using the Compute Optimized with GPU option</li>
<li>Had a requirement to transfer data using S3 API’s</li>
<li>Required the use of usable SSD Storage </li>
<li>Needed to optimized network ports that could reach speeds of up to 100Gbit, as Snowcones only have network port speeds of 10Gbit</li>
<li>Needed to cluster your snowballs. Clustering allows you to order between 5-10 snowball devices, acting as a single pool of resources. This allows you to gain a larger storage capacity, and also enhance the level of durability of the data should a snowball fail.  Clustering is only an option if you are looking to simply perform local compute and storage workloads without transferring any data back to AWS.</li>
<li>Needed to rack mount your devices providing the opportunity to implement temporary installations of both compute and storage</li>
<li>Required the snow device to be HIPAA compliant</li>
</ul>
<p>Ok, so that should provide a better understanding of how the Snowcone and Snowball differ. Let me now run through a couple of scenarios of when you might use these devices in the real world.</p>
<p>Both the snowcone and Snowballs are perfectly suited to provide a level of portable edge computing allowing you to collect data from wireless sensors or networked resources, for example in locations such as industrial warehouses or manufacturing plants, where you might need to collect environmental metric data. By collecting and gathering data it can then be transferred to AWS offline, or if using the snowcone it can be transferred on-line using AWS DataSync, which can then be analyzed at scale using other AWS services.</p>
<p>With storage capabilities of up to 80TB of usable HDD storage from a single snowball, it easily allows you to provide a means of securely storing and transferring a large amount of data into AWS, and you can run multiple snowballs in parallel allowing you to transfer petabytes of data if required. Being of rugged design and portable, the devices can be used in remote locations, such as mining and oil sectors, or even in the travel industry, fitted to trucks, trains, and boats, providing a mechanism of easily collecting data and then transferring it back to AWS.</p>
<p>Another common use case is from within the media and entertainment industry, the Snowcone and snowball can be used as a way to aggregate data from multiple sources before shipping it back to AWS for transfer into Amazon S3. You might get video and audio data from multiple feeds, especially if you are working in the film industry, this data can then be aggregated to your snowball device and shipped back to AWS for further processing and editing from your wider production team.</p>
<p>So we have covered Snowball and Snowcone, but let’s now turn the attention to the AWS Snowmobile, what is the primary use case for this? Well, it’s quite simple, the AWS Snowmobile is used to transfer MASSIVE amounts of data from a single location, up to 100PB per snowmobile which arrives on a truck as a ruggedized shipping container. When you are talking about data transfer of this scale you are normally looking at migrating entire data centers to a new location, or migrating entire storage libraries or repositories, and so AWS snowmobile is a great solution to help you with this when you need it done quickly, securely and cost-efficiently. You can run multiple snowmobiles in parallel which will allow you to transfer Exabytes of data! Generally, you would use AWS snowmobiles if you needed to transfer more than 10petabytes of data, anything less than this then you might want to consider using multiple snowball devices. </p>
<h1 id="Key-Features-of-the-Snow-Family"><a href="#Key-Features-of-the-Snow-Family" class="headerlink" title="Key Features of the Snow Family"></a>Key Features of the Snow Family</h1><p>The snowcone and the snowball are similar in their size and construction with regards to the casing compared to that of the snowmobile which is in a class of its own, I mean it comes on a Truck! </p>
<p>So let me talk about the Snowcone and Snowball for a moment. These two hardware appliances share some key features between them that I want to highlight. </p>
<p>When transferring data to these devices it is automatically encrypted to protect the data. This encryption is backed by keys generated by the Key Management Service (KMS). To enhance the security of the device, the encryption keys are not stored on the device during transit. If you would like to learn more about the Key Management Service, then you can see our existing course here. </p>
<p>Following on from encryption, another security feature that these devices have in common is that their enclosure is Anti-tamper in addition to specific verification checks on the boot environment when the device is first switched on. These measurements and checks help to validate the integrity of the data to ensure that it has not been interfered with at all during transit.</p>
<p>In addition to their enclosure being secure by design, it is also highly rugged and able to withstand the harshest of environments, for example, the snowcone can operate in conditions of -32ºC&#x2F;-25.6ºF to 63ºC&#x2F;145.4ºF. It’s all windproof, dustproof and water-resistant! The enclosures are designed to withstand operational vibrations and shockproof should you drop the device.</p>
<p>Each of the snow devices uses an E Ink shipping label which is pre-loaded with the delivery details entered on the associated job. When the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">snow device</a> leaves AWS premises it can be tracked via SNS, text messaging and the AWS Management Console. When the device is prepared to be returned to AWS premises, the E Ink automatically updates with the appropriate location</p>
<p>As these devices are packed full of data, it’s essential that the data is deleted once the transfer has been completed and the snow device is no longer required. As a result, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> carries out a secure erase which meets the National Institute of Standards and Technology, more commonly known as NIST for the sanitization of the media and storage. </p>
<p>Before I finish this lecture I just want to point out a couple of features related to the snowmobile. </p>
<p>Each snowmobile is sent with a connector rack allowing you to connect it to the backbone network of your own data center, as a result, this rack comes with up to 2 kilometers of network cabling.</p>
<p>The Snowmobile is designed to operate within ambient temperature up to 85F (29.4C). If the temperature exceeds this then an additional auxiliary chiller unit can be supplied by AWS following a site assessment survey. If there isn’t sufficient power to feed the snowmobile at the data center then AWS can also send a separate generator to power the snowmobile, however, this requires the same space required to home a snowmobile.</p>
<p>As with the Snowcone and snowball, the snowmobile also encrypts data backed by the Key Management Service. Also, the snowmobile is only ever operated by AWS personnel and can also be escorted by an additional security vehicle during the transit of the container to and from premises, in addition to having GPS tracking available. As added security, the container is also protected via 24&#x2F;7 video surveillance systems and alarms.</p>
<h1 id="AWS-OpsHub"><a href="#AWS-OpsHub" class="headerlink" title="AWS OpsHub"></a>AWS OpsHub</h1><p>This is going to be a very quick review of AWS OpsHub, which has been designed with the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/key-features-of-the-snow-family/">AWS Snow family</a> of services in mind. It provides a graphical user interface to help you manage your snow family of devices. It doesn’t come as a service as you would normally see in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Management Console, instead, it’s an application that can be downloaded onto a Mac OS or Windows client. </p>
<p>When you receive a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">snow device</a> from the snow family, you can download the AWS OpsHub management software which can then be used to unlock your device allowing you to configure and manage it and it’s operations. As a result, within a very short space of time, you are able to begin your data transfer through simple drag and drop operations. Previously, you were required to configure and manage the device by using the AWS CLI or REST APIs, this approach of using AWS OpsHub makes it a lot more intuitive and easier to do.</p>
<p>It can also be used to configure fleets of clustered snow devices if you have requested more than one. It also comes with a dashboard which provides an overview and summary screen displaying metrics relating to your snow device and these metrics relate to both storage and compute resources. Integrating with AWS Systems Manager, it can also easily help you with the automation of different tasks.</p>
<p>So in summary, it’s a very useful tool allowing you to quickly and easily manage your snow device upon arrival using a simple GUI.</p>
<h1 id="2Overview-of-Amazon-S3"><a href="#2Overview-of-Amazon-S3" class="headerlink" title="2Overview of Amazon S3"></a>2<strong>Overview of Amazon S3</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-global-infrastructure/">AWS global infrastructure components</a></p>
<h1 id="5Overview-of-EBS"><a href="#5Overview-of-EBS" class="headerlink" title="5Overview of EBS"></a>5<strong>Overview of EBS</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/how-to-encrypt-an-ebs-volume-the-new-amazon-ebs-encryption/">Blog post: How to Encrypt an EBS Volume</a></p>
<h1 id="6What-is-the-Amazon-Elastic-File-System"><a href="#6What-is-the-Amazon-Elastic-File-System" class="headerlink" title="6What is the Amazon Elastic File System?"></a>6<strong>What is the Amazon Elastic File System?</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region">List of supported regions</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/" rel="prev" title="AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8">
      <i class="fa fa-chevron-left"></i> AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/" rel="next" title="AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10">
      AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview-of-Amazon-S3"><span class="nav-number">2.</span> <span class="nav-text">Overview of Amazon S3</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Storage-Classes"><span class="nav-number">3.</span> <span class="nav-text">Storage Classes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EC2-Instance-Storage"><span class="nav-number">4.</span> <span class="nav-text">EC2 Instance Storage</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview-of-EBS"><span class="nav-number">5.</span> <span class="nav-text">Overview of EBS</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-the-Amazon-Elastic-File-System"><span class="nav-number">6.</span> <span class="nav-text">What is the Amazon Elastic File System?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Storage-Classes-and-Performance-Options"><span class="nav-number">7.</span> <span class="nav-text">Storage Classes and Performance Options</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-the-Snow-Family"><span class="nav-number">8.</span> <span class="nav-text">What is the Snow Family?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Which-Snow-Device-Do-I-Need"><span class="nav-number">9.</span> <span class="nav-text">Which Snow Device Do I Need?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Key-Features-of-the-Snow-Family"><span class="nav-number">10.</span> <span class="nav-text">Key Features of the Snow Family</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-OpsHub"><span class="nav-number">11.</span> <span class="nav-text">AWS OpsHub</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2Overview-of-Amazon-S3"><span class="nav-number">12.</span> <span class="nav-text">2Overview of Amazon S3</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5Overview-of-EBS"><span class="nav-number">13.</span> <span class="nav-text">5Overview of EBS</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6What-is-the-Amazon-Elastic-File-System"><span class="nav-number">14.</span> <span class="nav-text">6What is the Amazon Elastic File System?</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
