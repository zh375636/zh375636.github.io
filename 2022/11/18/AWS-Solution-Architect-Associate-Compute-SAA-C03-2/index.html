<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Compute (SAA-C03) IntroductionHello, and welcome to this course on compute in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certifi">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Solution-Architect-Associate-Compute-SAA-C03-2">
<meta property="og:url" content="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="Compute (SAA-C03) IntroductionHello, and welcome to this course on compute in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certifi">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T02:13:02.000Z">
<meta property="article:modified_time" content="2022-11-27T23:59:32.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Solution-Architect-Associate-Compute-SAA-C03-2 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Solution-Architect-Associate-Compute-SAA-C03-2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:02" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:02-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 19:59:32" itemprop="dateModified" datetime="2022-11-27T19:59:32-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Compute-SAA-C03-Introduction"><a href="#Compute-SAA-C03-Introduction" class="headerlink" title="Compute (SAA-C03) Introduction"></a>Compute (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on compute in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the various compute services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#112;&#112;&#x6f;&#114;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#117;&#x64;&#97;&#99;&#x61;&#x64;&#101;&#x6d;&#x79;&#x2e;&#x63;&#111;&#109;">&#x73;&#x75;&#112;&#112;&#x6f;&#114;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#117;&#x64;&#97;&#99;&#x61;&#x64;&#101;&#x6d;&#x79;&#x2e;&#x63;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various compute services in AWS in preparation for the exam.</p>
<p>The objective of this course is to provide an introduction to compute services in AWS for solution architects, including:</p>
<ul>
<li>Elastic Compute Cloud, or EC2;</li>
<li>The Elastic Container Service, also known as ECS;</li>
<li>The Elastic Container Registry, or ECR;</li>
<li>The Elastic Container Service for Kubernetes, known as EKS;</li>
<li>AWS Elastic Beanstalk;</li>
<li>AWS Lambda; and</li>
<li>AWS Batch.</li>
</ul>
<p>We’ll also introduce the concept of Elastic Load Balancing, along with the different types of load balancers you can provision within the AWS Cloud. We’ll discuss EC2 Auto Scaling and see how it works together with Elastic Load Balancing to help you build robust, highly available web applications. And finally we’ll discuss options for on-premises hybrid cloud architectures using AWS Outposts and VMware Cloud on AWS.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#x75;&#112;&#x70;&#111;&#x72;&#x74;&#x40;&#x63;&#108;&#111;&#117;&#x64;&#97;&#99;&#97;&#100;&#x65;&#109;&#121;&#46;&#x63;&#111;&#109;">&#115;&#x75;&#112;&#x70;&#111;&#x72;&#x74;&#x40;&#x63;&#108;&#111;&#117;&#x64;&#97;&#99;&#97;&#100;&#x65;&#109;&#121;&#46;&#x63;&#111;&#109;</a>. Thank you!</p>
<h1 id="What-is-Compute"><a href="#What-is-Compute" class="headerlink" title="What is Compute?"></a>What is Compute?</h1><p>Hello, and welcome to this very short lecture where we are going to answer the question, what is Compute in AWS? Before we begin to explore Compute services, resources and features, we must first understand what is meant by the term Compute. So what is it? </p>
<p>Put simply, Compute resources can be considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions. So essentially Compute is closely related to common server components, which many of you will already be familiar with, such as CPUs and RAM. With that in mind, a physical server within a data center would be considered a Compute resource, as it may have multiple CPUs and many gigs of RAM to process instructions given by the operating system and applications. </p>
<p>Within AWS, there are a number of different services and features that offer Compute power to provide different functions. Some of these services provide Compute, which can comprise of utilizing hundreds of EC2 instances, or virtual servers, which may be used continuously for months or even years, processing millions upon millions of instructions. On the other end of this scale, you may only utilize a hew hundred milliseconds of Compute resource to execute just a few lines of code within AWS Lambda before relinquishing that Compute power. Compute resources can be consumed in different quantities, for different lengths of time across a range of categories, offering a wide scope of performance and benefit options. So it will really depend on your requirements as to which Compute resource you use within AWS. </p>
<p>As a quick high-level reference, AWS offers a Cloud Compute Index, which can be found using the <a target="_blank" rel="noopener" href="https://aws.amazon.com/products/compute/">link</a> onscreen. And this shows different examples and scenarios of where you might use different Compute deployment units. That brings me to the end of this very short lecture. Now we are aware of what Compute is, let’s start by looking at some of the services offered by AWS that provide this Compute resource, starting with Elastic Cloud Compute, EC2.</p>
<h1 id="Amazon-EC2"><a href="#Amazon-EC2" class="headerlink" title="Amazon EC2"></a>Amazon EC2</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Blog Post about Shared Responsibility Model and Security Groups</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/create-your-first-amazon-ec2-instance-1/">Lab: Create your first Amazon EC2 Instance (Linux)</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/">Lab: Ceate your first Amazon EC2 Instance (Windows)</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture where I will explain what the EC2 service is and does, and how to configure an EC2 instance, so let’s get started. As EC2 is one of the most common <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> services used within AWS. I will discuss this service in greater detail over the other services that we’ll cover in this course. </p>
<p>EC2 is arguably the first compute service that you will encounter when working with AWS. It allows you to deploy virtual servers within your AWS environment and most people will require an EC2 instance within their environment as a part of at least one of their solutions. There are a number of elements in creating your EC2 instance, which I want to break down and explain. This will hopefully help to define how the service works and answer a number of questions that you may have. The EC2 service can be broken down into the following components. Amazon machine images, AMIs, instant types, instance purchasing options, tenancy, user data, storage options and security. Let’s look at each of these individually. </p>
<p>The first point I want to cover are AMIs, Amazon Machine Images. These are essentially templates of pre-configured EC2 instances which allow you to quickly launch a new EC2 instance based on the configuration within the AMI. This prevents you from having to install an operating system or any other common applications that you might need to install on a number of other EC2 instances. From a high level perspective an AMI is an image baseline that will include an operating system and applications along with any custom configuration. AWS provides a large number of AMIs covering different operating systems from Linux to Red Hat to Microsoft Windows among others. when configuring your EC2 instance, selecting your AMI is the first configuration choice you’ll need to make. You can also create your own AMI images to help you speed up your own deployment. For example you would start with selecting an AWS AMI, let’s say a Linux server. And then once it is up and running you may then need to install a number of your own custom applications and make specific configuration changes. Now if you needed another server to perform the same functionality, you could go through the same process of selecting a Linux AWS AMI, and again manually installing the applications and making your configurations. Or once you have made those changes on the first instance you can then simply create a brand new AMI or template of that instance with all the applications installed and configurations already made. Then if you need another instance of the same configuration all you would need to do is to select your custom AMI as the base image for your instance and it will launch with Linux server, your custom applications already installed and any configurations already made. As you can see this has many benefits and certainly comes in useful when implementing auto scaling. </p>
<p>In addition to both AWS managed and your custom managed AMIs, you could also select an AMI from the AWS marketplace. The AWS marketplace is essentially an online store that allows you to purchase AMIs from trusted vendors like Cisco, Citrix, Alert Logic et cetera. These vendor AMIs may have specific applications and configurations already made, such as instances that are optimized with built-in security and monitoring tools or contained database migration systems. Lastly community AMIs also exists which are a repository of AMIs that have been created and shared by other AWS members. </p>
<p>Let’s now take a look at instance types, once you have selected your AMI from any of the different sources already discussed, you must then select an instance type. An instance type simply defines the size of the instance based on a number of different parameters, these being ECUs. This defines a number of EC2 compute units for instance, vCPUs this is the number of virtual CPUs on the instance. Physical processor, this is the process speed used on the instance. Clock speed, it’s clock speed in gigahertz. Memory, the amount of memory associated. Incident storage this is the capacity of the local instance store volumes available. EBS optimized available, this defines if the instance supports EBS optimized storage or not. Network performance, this shows the performance level of rate of data transfer. IPV6 support, this simply indicates if the instance type supports IPV6. Process architecture this shows the architected type of the processor. AES-NI, this stands for advanced encryption standard new instructions and it shows if the instance supports it for enhanced data protection. AVX this indicates if the instance supports AVX which is advanced vector extensions, which are primary used for applications focused on audio and video, scientific calculations and 3D modeling analysis. And finally Turbo which shows if the instance supports intel turbo boost and AMD turbo core technologies. The key parameters here to primarily be aware of for general usage of an EC2 instance, could be summarized as vCPUs and memory, instant storage and network performance. But obviously this really depends on your actual usage and application. Having this flexibility of variant instances allows you to select the most appropriate size or power of an instance that you need for optimal performance within your applications. These different instance types are categorized into different family types that offer distinct performance benefits, which again helps you to select the most appropriate instance for your needs. Within each of these instance families you will have a range of instant types with varied CPU, memory, storage and network performance et cetera. </p>
<p>These instance families can be summarized as follows. Micro instances, these instances have a low cost against them due to the minimal amount of CPU and memory power that they offer. These are ideal for very low throughput use cases such as low traffic websites. General-purpose, instance types within this family have a balanced mix of CPU memory and storage making them ideal for small to medium databases, tests and development servers and back-end servers. Compute optimized, as the name implies instance types within this family have a greater focus on compute. They have the highest performing processes installed allowing them to be used for high-performance front end servers, web servers, high-performance science and engineering applications and video encoding and batch processing. GPU, GPU stands for graphics processing unit. And so the instances within this family are optimized for graphic intensive applications. FPGA, this family of instances allows you to customize field programmable gate arrays. To create application specific hardware accelerations when used with applications that use massively parallel processing power such as genomics and financial computing. Memory optimized, this family include instance types that are primarily used for large-scale enterprise class in-memory applications, such as performing real time processing of unstructured data. They are also ideal for enterprise applications such as Microsoft SharePoint. These instances of the lowest cost per gigabyte of RAM against all other instance families. Storage optimized, as expected these are optimized for enhanced storage. Instances in this family use SSD backed instant storage for low latency and very high I&#x2F;O, input&#x2F;output performance, including very high IOPS which is input&#x2F;output operations per second. And these are great for analytic workloads and no SQL databases. Data file systems and log processing applications. </p>
<p>Instance purchasing options. You can purchase EC2 instances through a variety of different payment plans. These have been designed to help you save cost by selecting the most appropriate option for your deployment. The different EC2 payment options are as follows, on-demand instances, reserved instances, scheduled instances, spot instances and on-demand capacity reservations. It’s good to be aware of these different options as well having an understanding of these can help you save a considerable amount of money depending on your use case. Let me run through each option to help explain. Starting with on-demand instance</p>
<p>These are EC2 instances that you can launch at any time and have it provisioned and available to you within minutes. You can use this instance for a shorter time or for as long as you need before terminating the instance. These instances have a flat rate and is determined on the instance type selected and is paid by the second. On-demand instances are typically used for short term uses where workloads can be irregular and where workload can be interrupted. Many users of AWS use on-demand instances within their testing and development environments. And when you stop or terminate your on-demand instance you’ll stop paying for the compute resource. </p>
<p>Reserved instances allow you to purchase a discount for an incidents type with set criteria for a set period of time in return for a reduced cost compared to on-demand instances. This reduction can be as much as 75%. These reservations against instances must be purchased in either one or three year time frames. Further reductions can be achieved with reserved instances depending on which payment methods you select. There are three options available to you, firstly all upfront. The complete payment for the one or three year reservation is paid. And this offers the largest discount and no further payment is required regardless of the number of hours the instance is used. Partial upfront, here a smaller upfront payment is made and then a discount is applied to any hours used by the instance during the term. And finally no upfront, no upfront or partial payments are made and the smallest discount of the three models is applied to any hours used by the instance. Reserved instances are used for long-term predictable workloads allowing you to make full use of the cost savings to be had when using compute resources offered by EC2. </p>
<p>Scheduled instances, these are similar to reserved instances and the fact that you pay for the reservations of an instance on a recurring schedule, either daily, weekly or monthly. For example you might have a weekly task that is scheduled that performs some kind of bulk processing for a number of hours at the same time every week. With scheduled instances you could set up a scheduled instance to run during that set timeframe once a week. And this prevents you for having to use the on-demand instances which would incur a higher price. You should note that when using scheduled instances but even if you didn’t use the instance you would still be charged. This allows you to provision instances for scheduled workloads that are not continuously running. Which is where a reserved instance would be the preferred choice. </p>
<p>Spot instances allows you to bid for unused EC2 compute resources, however your resource is not guaranteed for a fixed period of time. To you to spot instance you must bid higher than the current spot price which is set by AWS. And this spot price fluctuates depending on supply and demand of the unused resource. If your bid price for an instance type is higher than the spot price, then you’ll purchase that instance. But as soon as your bid price becomes lower than the fluctuating spot price, you will be issued a two-minute warning before the instance automatically terminates and is removed from your AWS environment. The bonus for spot instances is that you can bid for large EC2 instances at a very low cost point saving a huge amount on cost. Due to the nature of how the instances can be suddenly removed from your environment, spot instances are only useful for processing data and applications that can be suddenly interrupted. Such as batch jobs and background processing of data. </p>
<p>Capacity reservations allows you to reserve capacity for your EC2 instances based on different attributes. Such as instance type, platform and tenancy et cetera. Within a particular availability zone for any period of time. This ensures that you always have the available number of instances you require within a specific availability zone immediately. This capacity reservation could also be used in conjunction with your reserved instances discount providing you additional savings. </p>
<p>Let me now talk to you about EC2 tenancy and this relates to what underlying host your EC2 instance will reside on. So essentially the physical server within an AWS data center. Again there are different options available to you with pros and cons to each. Shared tenancy, this option will launch your EC2 instance on any available host with the specified resources required for your selected instance type. Regardless of which other customers and users also have EC2 instances running on the same host, hence the share tenancy name. AWS implement advanced security mechanisms to prevent one EC2 instance from accessing another on the same host. How the security is applied and operated is out of scope of this course and it is maintained by AWS themselves. Dedicated tenancy, this includes both dedicated instances and dedicated hosts. Dedicated instances are hosted on hardware that no other customer can access. It can only be accessed by your own AWS account. You may be required to launch your instances as a dedicated instance due to internal security policies or external compliance controls. Dedicated instances do incur additional charges due to the fact you are preventing other customers from running EC2 instances on the same hardware and so there will likely be unused capacity remaining. However the hardware might be shared by other resources you have running in your own account. Dedicated host, a dedicated host is effectively the same as dedicated instances. However they offer additional visibility and control, how you can place your instances on the physical host. They also allow you to use your existing licenses, such as PA-VM license or Windows Server licenses et cetera. Using dedicated hosts give you the ability to use the same host for a number of instances that you want to launch and align with any compliance and regulatory requirements. If you don’t need to address any compliance or security issues that require dedicated tenancy, then I recommend using shared tenancy to reduce your overall costs. </p>
<p>User data, during the configuration of your EC2 instance there is a section called user data. Which allows you to enter commands that will run during the first boot cycle of the instance. This is a great way to automatically perform functions upon boot, such as to pull down any additional software you want installing from any software repositories you may have. You could also download and get the latest OS updates during boot. For example you could enter yum update dash y, for a Linux instance which will then update its own software automatically at the time of boot. Storage options, as a part the configuration when setting up an EC2 instance, you are asked to select and configure your storage requirements. </p>
<p>Selecting storage for your EC2 instance will depend on the instance selected, what you intend to use the instance for and how critical the data is. Storage for EC2 can be classified between two distinct categories, persistent storage and ephemeral storage. Ephemeral meaning temporary. Persistent storage is available by attaching elastic block storage EBS volumes. And a ephemeral storage is created by some EC2 instances themselves using a local storage on the underlying host known as instance back storage. Let’s look at each of these storage options in greater depth. EBS volumes are separate devices from the EC2 instance itself. And so it’s not physically attached like ephemeral storage is. EBS volumes are considered network attached storage devices which are then logically attached to the EC2 instance via the AWS network. This principle is not dissimilar to attaching an external hard disk to your home laptop or PC. With the external hard disk represent your EBS volume and your PC represents your EC2 instance. The data on EBS volumes are automatically replicated to other EBS volumes within the same availability zone for resiliency which is managed by AWS. You can disconnect an EBS volume from your EC2 instance and the data will remain intact. Allowing you to reattach it to another EC2 instance if required. You can also implement encryption on these volumes if needed and take backup snapshots of all the data on the volume to S3. EBS volumes can be created in different sizes again with different performance capabilities depending on your requirements. </p>
<p>Ephemeral storage or instance backed storage is the storage that is physically attached to the underlying host on which the EC2 instance resides on. Looking back at our previous example, this would be similar to your own laptop or PC’s hard disk. There is a difference here though, with AWS EC2 instances as soon as the instance is stopped or terminated all saved data on a ephemeral storage is lost. If you reboot your instance then the data will remain but not if you stop it. Therefore if you have data that you need to retain it is not recommended that you use instance backed storage for this data. Instead use EBS volumes for persistent data storage. Unlike EBS volumes you are unable to detach ephemeral instance store volumes from the instance. </p>
<p>Security, security is fundamental with any AWS deployment. As so I just want to highlight a couple of points relating specifically to EC2 security. Firstly and during creation of your EC2 instance you will be asked to select a security group for your instance. A security group is essentially an instance level firewall allowing you to restrict both ingress and egress traffic by specifying what traffic allowed to communicate with it. You can restrict this communication by source ports and protocols for both inbound and outbound communication. Your instances are then associated with this security group. More information on security groups can be found in my blog post, found here covering instance level security. At the very end of your EC2 instance creation, you will need to select an existing key pair or create and download a new one. But what is a key pair and what is it used for? A key pair, as the name implies, is made up of two components, a public key and a private key. The function of key pairs is to encrypt the login information for Linux and Windows EC2 instances. And then decrypt the same information allowing you to authenticate onto the instance. The public key encrypts data such as the username and password. For Windows instances, the private key is used to decrypt this data allowing you to gain access to the login credentials including the password. For Linux instances the private key is used to remotely connect onto the instance via SSH. The public key is held and kept by AWS, and the private key is your responsibility to keep and ensure that it is not lost or compromised. So going back to when you create your EC2 instance and a new key pair. You’re given the opportunity to download the key pair, once you have done this you must keep that file safe until you’re ready to log on to the associated EC2 instance. It’s worth noting that you can use the same key pair on multiple instances to save you managing multiple private keys. Do bear in mind however should the private key become compromised access could be gained to all the instances where that key pair was used. Once you have authenticated to the EC2 instance the first time, you can set up additional less privileged access controls such as local windows accounts allowing other users to connect and authenticate to or even utilize Microsoft Active Directory. One final point regarding security on your EC2 instance it is your responsibility to maintain and install the latest OS and security patches released by the OS vendor as dictated within the AWS shared responsibility model. More information on this can be found in this blog post. </p>
<p>We have now covered the main elements of the EC2 service that should hopefully allow you to get started by creating your first EC2 instance and selecting the most appropriate configuration for your needs. But to reiterate what we have covered and make it all fit together, I will demonstrate how to create a new EC2 instance from within the console, quickly highlighting the elements we have discussed as I go through. </p>
<p>Okay so I’m logged into my AWS management console, and to start with we need to go to EC2 which is under the compute category. Now this take us to the EC2 console and from here we can simply select launch instance. Now this is the first stage of the configuration where we have to select our Amazon Machine Image, AMI. And here are a number of AWS AMIs that they supply, covering Windows, Red Hat, Linux et cetera. On the left hand side there’s just a few other options if you’ve created any AMIs yourself that would be stored here. I mentioned the AWS marketplace earlier and here you can see lots of different AMIs from other suppliers such as Trend Micro, Juniper Networks, Barracuda et cetera. And also the community AMI as well. So let’s get started with the Quick Start and look at some of the AWS supplied AMIs and I’m just going to launch this Amazon Linux box. So I’ll select that as my AMI, now we get to choose instance type. And we can filter up here with different types of instance types, general purpose, computer optimized et cetera. Just going to leave it as all, and then down here we can see the different families and the types, the vCPUs, memory et cetera that each of these has. So I’m going to leave it on the T2 micro general purpose instance. </p>
<p>So I’m going to select next configure instance details. Now I have a number of options here, the number of instances that we want to launch. I’m just going to keep it as one. If we want to launch this as a spot instance then we can select this box here to do so. But I’m just going to create it as an on-demand instance. We can select VPC that we want to run it in and we’ll have different VPCs there. You can then select the subnet if you’d like. And if you’d like to auto-assign a public IP address. We can assign a role to an EC2 instance if we need to and we can also control the shutdown behavior. So when we shut down all EC2 instance, do you want to terminate that instance or just stop. I’m going to leave that as a default or stop. There’s a couple of other controls you can put in here, enable termination protection. And what that will do, that will prevent you from terminating your instance until you uncheck this box. And you can also enable detail cloud watch monitoring if you need to. With regards to tenancy we discussed this earlier rather we shared or one of the dedicated instance or on a dedicated host, I’m going to leave that as shared. I’ll leave all the other options as default and then I’ll click on next to go to storage. This shows the current storage that comes with the AMI. We consider there’s 8 gig in size and it’s a general-purpose SSD drive. It’s a tick box here to delete the volume on termination and we can see that it’s not encrypted. If we wanted to add a new volume, we can add an EBS volume here. Again we can specify the size, eight gig or if you want to add it to 30 for example. And then we can also again delete on termination and we have the option to encrypt the EBS volume if we wanted to it’s by selecting the default AWS EBS encryption key. So now we have two drives, one of them is the root volume and an additional drive which is an EBS volume. </p>
<p>Click on next add tags, here we can add a key value pair tag to this instance. So for example a key of name and a value of my instance. And we can add additional tags as well as you can see here we can add up to 50 tags if we wanted to. So we can add a project that it belongs to or the cost et cetera, any tags that make it more usable to you. Once you’ve finished adding your tags click on configure security group. And this is where we control what can and can’t access your instance. You can create a new secure group or select an existing security group that you might already have. If we create a new security group and call it My Security Group, then you can add a description. And here are the rules for the security group. So at the moment we have SSH using TCP across port 22 and can you have the source as a custom IP address range or a single IP address. So you might just want a specific subnet in your VPC to talk to this instance or you can have anywhere, or you can have just your own IP address. Let’s put in a custom IP address range of 10.0.1.0&#x2F;24. And again you can add a description in there if you want to. You can add a new rule, for example HTTP traffic. And again you can add your source, cite anywhere then once you’re happy with your security groups click on review and launch. And this just provides a summary of all the configuration options that you’ve made up to this point. If you need to edit any other details you can just click on the right hand side here, edit the instance type the security groups et cetera. Once you’re happy with all of your information click on launch. And this is where you can select or create a new key pair to connect your instance. </p>
<p>So let’s go ahead and create a new key pair, call it My Instance, and now I need to download this key pair. Which will download the private part, once that key pair is downloaded I can then click on launch instance. Now if we go back to our dashboard by clicking on view instances. We can see here that it’s trying to launch at the minute, the status is pending and it shouldn’t take too long for that to become active. And here we go we can now see it’s up and running. </p>
<p>Before I finish this demonstration I just want to point out one last point relating to status checks that we can see from within our EC2 dashboard. These status checks are used to check the health and status of your EC2 instance and understanding what kind of faults could trigger these checks to fail. Kinda help you troubleshoot issues with your EC2 resources. There are two types of status checks. System status checks and instant status checks. If the system status check fails then it is likely to be an issue with the underlying host rather than a configuration issue with your EC2 instance. Common issues that trigger system status checks to fail are loss of power, loss of network connectivity and hardware and software issues on the underlying host. Basically a system status check failure is out of our control as the fault lies with components that AWS are responsible for. The best way to resolve this will be to stop the instance and restart. This is likely to cause the instance to launch on another physical host resolving the problem. Do not reboot the instance as this will cause the instance to continue running on the same physical server. Instance data checks, these differ from system status checks as if this fails then it would likely require your input to help them resolve the issue. This check looks at the EC2 instance itself, rather than focusing on the underlying hosts. Common issues that trigger this checks to fail are incorrect network configuration, corrupted file systems, exhausted memory or incompatible kernel. These faults will require you to troubleshoot and resolve the issue, for example changing the network configuration. If you’d like some hands-on experience with EC2, then we do offer two labs in which you can practice creating your own EC2 instances for both Linux and Windows. </p>
<p>That now brings me to the end of this lecture on EC2, like I mentioned previously this is going to be the longest and most in-depth lecture, simply due to how much of a key compute service it is in a wide range of use cases.</p>
<h1 id="ECS-Elastic-Container-Service"><a href="#ECS-Elastic-Container-Service" class="headerlink" title="ECS - Elastic Container Service"></a>ECS - Elastic Container Service</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/">Introduction to Docker</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/basics-of-using-containers-in-production/">Basics of using Containers in Production</a></p>
<p><strong>Transcript</strong></p>
<p>Hello, and welcome to this short lecture which will provide a high-level overview of the Amazon EC2 Container Service, commonly known as Amazon ECS. This service allows you to run Docker-enabled applications packaged as containers across a cluster of EC2 instances without requiring you to manage a complex and administratively heavy cluster management system. The burden of managing your own cluster management system is abstracted with the Amazon ECS service by passing that responsibility over to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a>, specifically though the use of AWS Fargate. </p>
<p>If you’re new to some of these terms such as Docker, containers, and AWS Fargate then let me quickly, in a single sentence, define what they are to help you understand this service a little easier. AWS Fargate is an engine used to enable ECS to run containers without having to manage and provision instances and clusters for containers. Docker is piece of software that allows you to automate the installation and distribution of applications inside Linux Containers. So what are containers? A Container holds everything that an application requires to enable it to run from within it’s isolated container package. This may include system libraries, code, system tools, run time, etcetera. But it does not include an operating system like a virtual machine does, and so reduces overhead of the actual container itself. </p>
<p>Containers are decoupled from the underlying operating system, making Container applications very portable, lightweight, flexible, and scalable across a cloud environment. This ensures that the application will always run as expected regardless of it’s deployment location. With this in mind, if you are already using Docker, or have existing containerized applications packaged locally, then these will work seamlessly on Amazon ECS. For more information on Docker and Containers, please see our existing content found here. Let’s now take a deeper look at the EC2 Container Service and some of the additional functions that it provides. </p>
<p>As I mentioned before, EC2 Container Service removes the need for you to manage your own cluster management system thanks to its interactions with AWS Fargate. You don’t even have to specify which instance type to use. This can be very time consuming and requires a lot of overhead to continue to monitor and maintain and scale. With Amazon ECS there is no need to install any management software for your cluster, neither is there a need to install any monitoring software either. All of this, and more, is taken care of by the service, allowing you to focus on building great applications and deploying them across your scalable cluster. </p>
<p>When launching your ECS cluster you have the option of two different deployment models: a Fargate launch and an EC2 launch. The Fargate launch requires far less configuration and simply requires you to specify the CPU and memory required, define the networking and IAM policies in addition to you having to package your applications into containers. However, with an EC2 launch you have a far greater scope of customization and configurable parameters. For example, you are responsible for patching and scaling your instances, and you can specify which instance types you used, and how many containers should be in a cluster. </p>
<p>There are use cases for both modes. You may need more granularity and control with some of your clusters due to security and compliance controls. Monitoring is taken care of through the use of AWS CloudWatch, which will monitor metrics against your containers and your cluster. Those of you who have used CloudWatch before will be aware you can easily create alarms based off of these metrics providing you notification of when specific events occur such as your cluster size scaling up or down. An Amazon ECS cluster is comprised of a collection of EC2 instances. As such, some of the functionality and features that we’ve already discussed in this course can be used with these instances. For example Security Groups to implement instance level securely at a port and protocol level, along with Elastic Load Balancing and Auto Scaling. Although these EC2 instances form a cluster, they still operate in much the same way as a single EC2 instance. So again, for example, should you need to connect to one of your instances itself, you could still use the same familiar methods such as initiating an SSH connection. </p>
<p>The clusters themselves act as a resource pool, aggregating resources such as CPU and memory. The cluster is dynamically scalable, meaning you can start your cluster as a single small instance, but it can dynamically scale to thousands of larger instances. Multiple instance types can be used within the cluster if required. Although the cluster is dynamically scalable, it’s important to point out that it can only scale within a single region. Amazon ECS is region-specific, so it can span multiple availability zones, but it cannot span multiple regions. With ECS you can schedule your containers to be deployed across your cluster based on different requirements, such as resources requirements or specific availability requirements, through the use of multiple availability zones. The instances within the Amazon ECS cluster also have a Docker daemon and an ECS agent installed. These agents communicate with each other allowing Amazon ECS commands to be translated into Docker commands.</p>
<h1 id="ECR-Elastic-Container-Registry"><a href="#ECR-Elastic-Container-Registry" class="headerlink" title="ECR - Elastic Container Registry"></a>ECR - Elastic Container Registry</h1><h3 id="Resources-referenced-within-this-lecture"><a href="#Resources-referenced-within-this-lecture" class="headerlink" title="Resources referenced within this lecture:"></a><strong>Resources referenced within this lecture:</strong></h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Overview of AWS Identity &amp; Access Managment (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Docker Push</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">Docker Pull</a></p>
<h3 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a><strong>Transcript</strong></h3><p>Hello and welcome to this lecture covering the Elastic Container Registry service, known as ECR. This service links closely with the previous service discussed, the EC2 Container Service, as it provides a secure location to store and manage your docker images that can be distributed and deployed across your applications. </p>
<p>This is a fully managed service, and as a result, you do not need to provision any infrastructure to allow you to create this registry of docker images. This is all provisioned and managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a>. This service is primarily used by developers, allowing them to push, pull, and manage their library of docker images in a central and secure location. </p>
<p>To understand the service better, let’s look at some components used. These being, registry, authorization token, repository, repository policy, and image. Let’s take a look at the registry first. The ECR registry is the object that allows you to host and store your docker images in, as well as create image repositories. Within your AWS account, you will be provided with a default registry. When your registry is created, then by default, the URL for the registry is as follows:</p>
<p><a target="_blank" rel="noopener" href="https://aws_account_id.dkr.ecr.region.amazonaws.com/">https://aws_account_id.dkr.ecr.region.amazonaws.com</a></p>
<p>where you’ll need to replace the red text with your own information that is applicable to your account or medium. Your account will have both read and write access by default to any images you create within the registry and any repositories. Access to your registry and images can be controlled via IAM policies in addition to repository policies as well, to enforce tighter and stricter security controls. As the docker command line interface doesn’t support the different AWS authentication methods that are used, then before your docker client can access your registry, It needs to be authenticated as an AWS user, which will then allow your client to both push and pull images. And this is done by using an authorization token. To begin the authorization process to allow your docker client to communicate with the default registry, you can run the get login command using the AWS CLI, as shown:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws ecr get-login --region region --no-include-email</span><br></pre></td></tr></table></figure>

<p>where the red text should be replaced with your own region. This will then produce an output response, which will be a docker login command.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u AWS -p password https://aws_account_id.dkr.ecr.region.amazonaws.com</span><br></pre></td></tr></table></figure>

<p>You must then copy this command and paste it into your docker terminal which will then authenticate your client and associate a docker CLI to your default registry. This process produces an authorization token that can be used within the registry for 12 hours, at which point, you will need to re-authenticate by following the same process. The repository are objects within your registry that allow you to group together and secure different docker images. You can create multiple repositories with the registry, allowing you to organize and manage your docker images into different categories. </p>
<p>Using policies from both IAM and repository policies, you can assign permissions to each repository allowing specific users to perform certain actions, such as performing a push or pull IP line. As I just mentioned, you can control access to your repository and images using both IAM policies and repository policies. There are a number of different IAM managed policies to help you control access to ECR, these being the three shown on the screen.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AmazonEC2ContainerRegistryFullAccess</span><br><span class="line">AmazonEC2ContainerRegistryPowerUser</span><br><span class="line">AmazonEC2ContainerRegistryReadOnly</span><br></pre></td></tr></table></figure>

<p>For more information on IAM and policies, please refer to our system course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">here</a>, which covers IAM and policy creation and management. Repository policies are resource-based policies, which means you need to ensure you add a principle to the policy to determine who has access and what permissions they have. It’s important to be aware of that for an AWS user to gain access to the registry, they will require access to the ecr get authorization token API call. Once they have this access, repository policies can control what actions those users can perform on each of the repositories. These resource-based policies are created within ECR itself and within each other repositories that you have. Once you have configured your registry, repositories, and security controls, and authenticated your docker client with ECR, you can then begin storing your docker images in the required repositories, ready to then pull down again as and when required. </p>
<p>To push an image into ECR, you can use the docker push command, and to retrieve and image you can use the docker pull command. For more information on how to perform both a push and a pull of images, please see the following links.</p>
<p><strong>Docker Push</strong>: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</a></p>
<p><strong>Docker Pull</strong>: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html</a></p>
<p>That now brings me to the end of this lecture covering the Elastic Container Registry service. Coming up in the next lecture, I shall be looking at the Amazon <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/eks-elastic-container-service-kubernetes/">Elastic Container Service for Kubernetes</a>, known as EKS.</p>
<h1 id="EKS-Elastic-Container-Service-for-Kubernetes"><a href="#EKS-Elastic-Container-Service-for-Kubernetes" class="headerlink" title="EKS - Elastic Container Service for Kubernetes"></a>EKS - Elastic Container Service for Kubernetes</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-kubernetes/">Introduction to Kubernetes</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Install Kubectl</a></p>
<p>IAM Authenticator:</p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/aws-iam-authenticator">Linux</a></p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator">MacOS</a></p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/windows/amd64/aws-iam-authenticator.exe">Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">Configuration map to joing the Worker Node to the EKS Cluster</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">Introduction to EKS</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture covering the Elastic Container Service for Kubernetes, more commonly known as EKS.</p>
<p>Firstly, for those unfamiliar with Kubernetes let me briefly explain what it is at a high level.  Kubernetes is an open-source container orchestration tool designed to automate, deploy, scale, and operate containerized applications. It is designed to grow from tens, thousands, or even millions of containers. Kubernetes is also container-runtime agnostic, which means you can actually use Kubernetes to run rocket and docker containers.</p>
<p>So back to EKS, with EKS, AWS provides a managed service allowing you to run Kubernetes across your AWS infrastructure without having to take care of provisioning and running the Kubernetes management infrastructure in what’s referred to as the control plane. You, the AWS account owner, only need to provision and maintain the worker nodes.</p>
<p>What is a control plane and what are worker nodes?</p>
<p>Kubernetes Control Plane:</p>
<p>There are a number of different components that make up the control plane and these include a number of different APIs, the kubelet processes and the Kubernetes Master, and these dictate how kubernetes and your clusters communicate with each other.  The control plane itself is run across master nodes.</p>
<p>The control plane schedules containers onto nodes. The term scheduling does not refer to time in this context. Scheduling, in this case, refers to the decision process of placing containers onto nodes in accordance with their declared, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> requirements.  The Control Plane also tracks the state of all kubernetes objects by continually monitoring the objects. So in EKS, AWS is responsible for provisioning, scaling and managing the control plane and they do this by utilising multiple availability zones for additional resilience.</p>
<p>Worker nodes:</p>
<p>Kubernetes clusters are composed of nodes and the term cluster refers to the aggregate of all of the nodes.  A node is a worker machine in Kubernetes and runs as an on-demand EC2 instance and includes software to run containers managed by the Kubernetes control plane.  For each node created, a specific AMI is used which also ensures docker and kubelet in addition to the AWS IAM authenticator is installed for security controls. These nodes are what us as the customer are responsible for managing within EKS.  Once the worker nodes are provisioned they can then connect to EKS using an endpoint.</p>
<p>For more information on Kubernetes, please see our existing course ‘Introduction to Kubernetes’ here</p>
<p>Let me provide a brief overview of what’s required to start using the EKS service.</p>
<ol>
<li><p>Create an EKS Service Role: Before you begin working with EKS you need to configure and create an IAM service-role that allows EKS to provision and configure specific resources.  This role only needs to be created once and can be used for all other EKS clusters created going forward. The role needs to have the following permissions policies attached to the role: AmazonEKSServicePolicy and AmazonEKSClusterPolicy</p>
</li>
<li><p>Create an EKS Cluster VPC: Using AWS CloudFormation you need to create a and run a CloudFormation stack based on the following template: <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml</a> which will configure a new VPC for you to use with EKS</p>
</li>
<li><p>Install kubectl and the AWS-IAM-Authenticator: Kubectl is a command line utility for Kubernetes and can be installed following the details supplied here The IAM-Authenticator is required to authenticate with the EKS cluster.  Depending on your client OS (Linux, MacOS or Windows) it can be downloaded from here:</p>
</li>
<li><p>Create your EKS Cluster: Using the EKS console you can now create your EKS cluster using the details and information from the VPC created in step 1 and 2</p>
</li>
<li><p>Configure kubectl for EKS: Using the update-kubeconfig command via the AWS CLI you need to create a kubeconfig file for your EKS cluster</p>
</li>
<li><p>Provision and configure Worker Nodes: Once your EKS cluster shows an ‘Active’ status you can launch your worker nodes using CloudFormation based on the following template: <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml</a></p>
</li>
<li><p>Configure the Worker Node to join the EKS Cluster: Using a configuration map downloaded here:</p>
</li>
</ol>
<p>curl -O <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml</a></p>
<p>You must edit it and Replace the &lt;ARN of instance role (not instance profile)&gt; with the NodeInstanceRole value from step 6</p>
<p>Your EKS Cluster and worker nodes are now configured ready for your to deploy your applications with Kubernetes.</p>
<p>For more information on EKS, please see our existing course ‘Introduction to EKS’ which will cover these points and more in greater detail <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">https://cloudacademy.com/course/introduction-to-aws-eks/</a></p>
<h1 id="AWS-Elastic-Beanstalk"><a href="#AWS-Elastic-Beanstalk" class="headerlink" title="AWS Elastic Beanstalk"></a>AWS Elastic Beanstalk</h1><p>Hello and welcome to this lecture on AWS Elastic Beanstalk. AWS Elastic Beanstalk is an AWS-managed service that allows you to upload the code of your web application, along with the environment configurations, which will then allow Elastic Beanstalk to automatically provision and deploy the appropriate and necessary resources required within AWS to make the web application operational. These resources can include other AWS services and features, such as EC2, Auto Scaling, application health-monitoring, and Elastic Load Balancing, in addition to capacity provisioning. This automation and simplification makes it an ideal service for engineers who may not have the familiarity or the necessary skills within AWS to deploy, provision, monitor, and scale the correct environment themselves to run the developed applications. Instead, this responsibility is passed on to AWS Elastic Beanstalk to deploy the correct infrastructure to run the uploaded code. This provides a simple, effective, and quick solution to deploying your web application. </p>
<p>Once the application is up and running, you can continue to support and maintain the environment as you would with a custom-built environment. You can additionally perform some of the maintenance tasks from the Elastic Beanstalk dashboard itself. Elastic Beanstalk is able to operate with a variety of different platforms and programming languages, making it a very flexible service for your DevOps teams. Currently, at the time of writing this course, Elastic Beanstalk is compatible with the following. One important point to note is that the service itself is free to use. There is no cost associated with Elastic Beanstalk, however, any resources that are created on your application’s behalf, such as EC2 instances, you will be charged for as per the standard pricing policies at the time of deployment. </p>
<p>So now we know at a high level what AWS Elastic Beanstalk is and does, let me run through some of its core components that create the service. The application version. An application version is a very specific reference to a section of deployable code. The application version will point typically to S3, simple storage service, to where the deployable code may reside. </p>
<p>The environment. An environment refers to an application version that has been deployed on AWS resources. These resources are configured and provisioned by AWS Elastic Beanstalk. At this stage, the application is deployed as a solution and becomes operational within your environment. The environment is comprised of all the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code. </p>
<p>Environment configurations. An environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave. The environment tier. This component reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. If the application manages and handles HTTP requests, then the app will be run in a web server environment. If the application does not process HTTP requests and instead perhaps pulls data from an SQS queue, then it would run in a worker environment. I shall cover more on the differences between the web server and work environment shortly. </p>
<p>The configuration template. This is the template that provides the baseline for creating a new, unique environment configuration. Platform. The platform is a culmination of components in which you can build your application upon using Elastic Beanstalk. These comprise of the operating system of the instance, the programming language, the server type, web or application, and components of Elastic Beanstalk itself, and as a whole can be defined as a platform. Applications. Within Elastic Beanstalk, an application is a collection of different elements, such as environments, environment configurations, and application versions. In fact, you can have multiple application versions held within a single application. You can deploy your application across one of two different environment tiers, either the web server tier or the worker tier. </p>
<p>These tiers are configured differently depending on the use case of your application. The web server environment is typically used for standard web applications that operate and serve requests over HTTP port 80. This tier will typically use services and features such as Route 53, Elastic Load Balancing, Auto Scaling, EC2, and Security Groups. The worker environment is slightly different and are used by applications that will have a back-end processing task that will interact with AWS SQS, the Simple Queue Service. This tier typically uses the following AWS resources in this environment, an SQS Queue, an IAM Service Role, Auto Scaling, and EC2. </p>
<p>Now you are aware of some of the terminology and components, we can look at how AWS Elastic Beanstalk operates a very simple workflow process for your application deployment and ongoing management in what can be defined in four simple steps. Firstly, you create an application. Next, you must upload your application version of the application to Elastic Beanstalk, along with some additional configuration information regarding the application itself. This creates the environment configuration. The environment is then created by Elastic Beanstalk with the appropriate resources to run your code. Any management of your application can then take place, such as deploying new versions of your application. If the management of your applications has altered the environment configuration, then your environment will automatically be updated to reflect the new code should additional resources be required. For further information and to get some hands-on experience with AWS Elastic Beanstalk to deploy an application, take a look at our two labs which will guide you through the steps and processes we have discussed.</p>
<h1 id="AWS-Batch"><a href="#AWS-Batch" class="headerlink" title="AWS Batch"></a>AWS Batch</h1><p>Hello, and welcome to this lecture where I’ll provide a high level overview of AWS Batch. As the name suggests, this service is used to manage and run Batch computing workloads within AWS. Before we go any further, I just want to quickly clarify what Batch computing is. </p>
<p>Batch computing is primarily used in specialist use cases which require a vast amount of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> power across a cluster of compute resources to complete batch processing executing a series of jobs or tasks. Outside of a cloud environment, it can be very difficult to maintain and manage a batch computing system. It requires specific software and requires the ability to consume the resources required, which can be very costly. However, with AWS Batch, many of these constraints, administration activities and maintenance tasks are removed. You can seamlessly create a cluster of compute resources which is highly scalable, taking advantage of the elasticity if AWS, coping with any level of batch processing while optimizing the distribution of the workloads. All provisioning, monitoring, maintenance and management of the clusters themselves is taken care of by AWS, meaning there is no software to be installed by yourself. </p>
<p>There are effectively five components that make up AWS Batch service which will help you to start using the service, these being: Jobs. A job is classed as a unit of work that is to be run by AWS Batch. For example, this can be a Linux executable file, an application within an ECS cluster or a shell script. The jobs themselves run on EC2 instances as a containerized application. Each job can at any one time be in a number of different states, for example, submitted, pending, running, failed, among others. Job definitions. These define specific parameters for the jobs themselves. They dictate how the job will run and with what configuration. Some examples of these may be how many vCPUs to use for the container, which data volume should be used, which IAM role should be used, allowing access for AWS Batch to communicate with other AWS services, and mount points.</p>
<p>Job queues. Jobs that are scheduled are placed into a job queue until they run. It’s also possible to have multiple queues with different priorities if needed. One queue could be used for on-demand EC2 instances, and another queue could be used for the spot instances. Both on-demand and spot instances are supported by AWS Batch, allowing you to optimize cost, and AWS Batch can even bid on your behalf for those spot instances. </p>
<p>Job scheduling. The Job Scheduler takes control of when a job should be run and from which Compute Environment. Typically it will operate on a first-in-first-out basis, and it will look at the different job queues that you have configured, ensuring that higher priority queues are run first, assuming all dependencies of that job have been met. </p>
<p>Compute Environments. These are the environments containing the compute resources to carry out the job. The environment can be defined as managed or unmanaged. A managed environment means that the service itself will handle provisioning, scaling and termination of your Compute instances based on the configuration parameters that you would enter regarding the instance type, purchase method, such as on-demand or spot. This environment is then created as an Amazon ECS Cluster. Unmanaged environments are provisioned, managed and maintained by you, which gives greater customization. However, it does require greater administration and maintenance and also requires you to create the necessary Amazon ECS Cluster that the managed environment would have done on your behalf. </p>
<p>If you have a requirement to run multiple jobs in parallel using Batch computing, for example, to analyze financial risk models, perform media transcoding or engineering simulations, then AWS Batch would be a perfect solution.</p>
<h1 id="EC2-Auto-Scaling"><a href="#EC2-Auto-Scaling" class="headerlink" title="EC2 Auto Scaling"></a>EC2 Auto Scaling</h1><p>Hello and welcome to the first of the lectures that will be covering EC2 Auto Scaling. So what exactly is EC2 Auto Scaling? Put simply, Auto Scaling is a mechanism that automatically allows you to increase or decrease your EC2 resources to meet the demand based off of custom defined metrics and thresholds. </p>
<p>In AWS, there is EC2 Auto Scaling which focuses on the scaling of your EC2 fleet, but there’s also an Auto Scaling service. This service allows you to scale Amazon ECS tasks, DynamoDB tables and indexes, in addition to Amazon Aurora replicas. For this course I will just be focusing on EC2 Auto Scaling. Let’s look at an example of how EC2 Auto Scaling can be used in practice. </p>
<p>Let’s say you had a single EC2 instance acting as a web server receiving requests from the public users across the Internet. As the requests and demand increases, so does the load on the instance. Additional processing power will be required to process the additional requests and therefore the CPU utilization would also increase. To avoid running out of CPU resource on your instance, which would lead to poor performance experienced by your end users, you would need to deploy another EC2 instance to load balance the demand and process the increased requests. With Auto Scaling, you could configure a metric to automatically launch a second instance when the CPU utilization got to 75% on the first instance. By load balancing traffic evenly, it would reduce the demand put upon each instance and reduce the chance of the first web server failing or slowing due to high CPU usage. Similarly, when the demand on your web server reduces, so would your CPU utilization. So you could also set a metric to scale back. In this example, you could configure Auto Scaling to automatically terminate one of your EC2 instances when the CPU utilization dropped to 20% as it would no longer be required due to the decreased demand. </p>
<p>By scaling your resources back helps to optimize the cost of your EC2 fleet as you only pay for resources when they are running. Through these customizable and defined metrics, you can increase, scale out, and decrease, scale in, the size of your EC2 fleet automatically with ease. This has many advantages and here are some of the key points. Firstly, automation. As this provides automatic provisioning based off of custom defined thresholds, your infrastructure can elastically provision the required resources, preventing your operations team from manually deploying and removing resources to meet demands put upon your infrastructure. Greater customer satisfaction. If you are always able to provision enough capacity within your environment when the demand increases, then it’s unlikely that your end users will experience performance issues, which will help with user retention. And cost reduction. With the ability to automatically reduce the amount of resources you have when the demand drops, you will stop paying for those resources. You only pay for an EC2 resource when it’s up and running, which is based on a per second basis. When you couple Auto Scaling with an Elastic Load Balancer, you get a real sense of how beneficial building a scalable and flexible architecture for your resources can be. </p>
<p>In the next lecture, I shall be explaining the different components of Auto Scaling before providing a demonstration on how to configure it.</p>
<h1 id="Components-of-EC2-Auto-Scaling"><a href="#Components-of-EC2-Auto-Scaling" class="headerlink" title="Components of EC2 Auto Scaling"></a>Components of EC2 Auto Scaling</h1><p>Hello and welcome to this lecture where I’ll focus on the different components of EC2 auto scaling, to help you understand how the process and service works. There are two distinct steps to the configuration. The first step is the creation of the launch configuration or launch template. And the second part is the creation of an auto scaling group. </p>
<p>When using EC2 auto scaling, you can either create a launch configuration or launch template. Both define how an auto scaling group builds new EC2 instances. They both answer a number of questions required when launching a new instance, such as which Amazon Machine Image to use or AMI, which instance type to select. If you’d like to use Spot Instances to help lower costs. If and when public IP addresses should be used for your instances. If any user data is required for automatic scripting on first boot. What storage volume configuration should be used, and what security group should be applied. You will probably be familiar with most of these steps if you have ever created an EC2 instance manually, it’s much the same. </p>
<p>A launch template is essentially a newer and more advanced version of the launch configuration. Being a template you can build a standard configuration allowing you to simplify how you launch instances for your auto scaling groups. Let me now demonstrate how to create both a launch configuration and a launch template. </p>
<p>As you can see, I’m logged into my AWS account and I’m at the Management Console. And to create our launch templates and launch configurations we need to go into EC2 under compute. So let’s take a look. Now I’m going to start by creating the launch template first. And then after that, I’ll create the launch configuration so you can see the differences between them. </p>
<p>Now on the left hand side here, under instances, you can see launch templates. So if you select that, then it’s just a quick splash screen here, just saying welcome to launch templates. And this gives you a brief summary of what it is. So to create a launch template we’ll click on the blue create launch template button. Now here we have a single page with a number of configurable parameters on them. So let’s go through each of them and take a look. So firstly, we can either create a new template or create a new template version. Now as I don’t have an existing template, we can’t create a new version of that template. So let’s start from scratch by creating a new template. Let’s give this a name. I’ll just call this launch template. And a description of demo. Now here we can specify the source template, which essentially allows you to create a template from an existing template that you might already have. And as I explained, I don’t have any other templates at the moment. So we can’t do that and I just want to show you how to create a template from scratch anyway. Now further down we have launch template contents. Now this is where we started getting to the actual configuration of what we’re going to launch. </p>
<p>And we can start by selecting the AMI ID. If we click on the search for AMI, we can have a look at the different catalogs. Either quick start if you have any of your own AMIs on the marketplace or the community AMIs. Let’s just go with a quick start. And then we can select an AMI, let’s just go with the top one here, the Amazon Linux. And select AMI, now we can select an instance type. Let’s just go for a t1 micro. And if we have any existing key pairs, we can select an existing key pair to allow us to connect to our instances. For this one, I’ll just select an existing key pair. And then network type that this instance will reside in, whether it’s in a VPC environment, or the classic environment, we’re going to go with the VPC. Now here we can also attach any security groups Again as a drop down list to allow you to select an existing security groups that you might have. So I’ll just select a couple of different groups here. Now further down, if you want to add any additional network interfaces, then you can do so here simply by clicking on add network interface and filling out the relevant fields. Don’t need to do that in this example. Now here we have storage volumes. So I’ll come with an eight gig EBS volume, general purpose. And we can specify encryption here if we want to, and the delete on termination, either yes or no, and the IOPS et cetera. And we can add additional volumes if we want to as well here. Further down we can instance tags. So let’s add a tag, say for example project cloud academy. And this will tag both the instance and the volume. If we go into advanced details. We can select if you want this to be a Spot Instance or not. We can select an instance profile which allows you to associate a role to your EC2 instance when it launches. We can select the shutdown behavior, whether you want it to terminate or simply stop when we shut it down. And there’s a number of other more advanced options that you can select with regards to your instance. And then at the very bottom we have user data if you want to run any commands on boot. Now once you’re happy with all of your information, all you need to do is simply click on create launch template. And that’s it. </p>
<p>If we get on to close, we can now see our launch template has been created. And the default version is one and the latest version is one. We can create another launch template based on this and give it a different version if we want to. So let’s see how the launch template compares to the launch configuration. </p>
<p>Now the launch configuration is further down on the left hand side on the auto scaling right at the bottom here. So if you click on launch configurations, and this is essentially the same as the launch template. Although the launch template has a few more options, and it’s more simplistic in its creation and is the preferred method. However, you can still create launch configurations, so let’s take a look. Click on create launch configuration. Now here we can select our AMI. And again, we have the different catalogs here like we do with the launch template. It’s just presented differently essentially. Select the AMI. Here we can select our instance type. And again we had this option in the launch template. Here we can give it a name. Let’s call this launch configuration. Again, you can select if you want to use Spot Instances, and you can select an IAM role. Whereas in the launch template you can select the instance profile. If we get on to advanced, there’s not as many advanced options here as there is with the launch template as we had a much longer list. However, you do still have a number of options here should you need it such as user data, specifying the Kernel ID et cetera. If we go to storage. Again, it comes with the default storage for the instance type you selected. And again you can add new volumes if you need to. So again very similar to the launch template. Here you can select your security groups, so you can select an existing group. So again, you can just select the security groups that you need here. Then click on review. And here’s a summary of all the options that you’ve selected. And once you’re happy with those, simply click create launch configuration. And finally, here you can also choose a key pair or create a new key pair should you need to. So again, let’s just let that cloud academy key pair. Then click create launch configuration. And there you go, there’s our launch configuration. </p>
<p>So there’s two different methods of creating that configuration to allow your auto scaling groups to know what instances to launch and how they should be configured. The main difference between the two is that the launch templates is presented all on a single page to allow you to quickly select your options rather than going through a number of different screens. And it also has a few more advanced features and options as well. Okay, that’s the end of the demonstration, thank you. </p>
<p>Without either the launch configuration or launch template, auto scaling would not know what instance it was launching and to which configuration. So before you create your auto scaling group, you need to have your launch configuration defined. But what does the auto scaling group do? Well, the auto scaling group defines the desired capacity and other limitations of the group using scaling policies and where the group should scale your resources, such as which availability zone. Let’s look at each of these details further via another demonstration on how to create an auto scaling group. And during this demonstration, I will create a new auto scaling group based on our previous launch template. And I’ll set up an auto scaling policy defining when to both increase and decrease the group size. Let’s take a look. </p>
<p>Again I’m within the AWS Management Console. And to create our auto scaling group, we need to go into EC2, which is under compute. And then if you scroll to the bottom on the left hand side, we can see under auto scaling, auto scaling groups, so let’s click on that. And this is where we can create our group. Firstly we click on the blue button create auto scaling group. And here we can either create it from a launch configuration or a launch template. So let’s select the launch template which is the new and preferred option. And down here we can select which launch template we would like to use. And this is the one that I created in the previous demonstrations. So once I’ve selected my launch template that I’d like to use, click on next step. Now we can give it a group name. So I just call this demo. If we had multiple versions of our launch template, then we can select the different versions there. But at the moment we just have the single version. With regards to the fleet, we can adhere to our launch template configuration. Or we can use a combination of different purchase options and instances. I just want to use the configuration that we used within our launch template. With regards to the group size, let’s start with two instances in our auto scaling group. And I can select the appropriate VPC that I’d like this to be launched in. And once I select the VPC, then I can select the subnets that I’d like. So let’s just select a couple of subnets in our VPC. Now if we go down to advanced details, here we have a number of other options. Now if you want to associate our auto scaling group to a load balancer, then we can do so here. And we can select our load balancer and target groups. But at the moment I’m going to leave this blank because in a later demonstration in the next lecture, I’m going to show you how to associate an existing auto scaling group with one of your new load balancers. So I’m just going to leave that blank for now. But if you did want to associate your auto scaling group to a load balancer during creation. then this is the place you do it. We have our instance protection down here. And we have an option protect from scale in. So if this is selected then during the scale in procedures, auto scaling won’t terminate any instances that are protected. I’m just going to remove the options for now. And also and finally a service linked role is selected. And this enables access to AWS services and resources that are used or managed by auto scaling. Once we have our configuration set, we can move on to configuring scaling policies. </p>
<p>Now here, we have two options, we can keep this group at its initial size. And as you know I set it to two instances, or we can you scaling policies to adjust the capacity of the group. And that’s what I’d like to do. Now I want to scale between two and five instances say. ‘Cause that’s the minimum and maximum number of instances that this group will scale to. Now I want to scale more auto scaling group using step or simple scanning policy. So I’m going to click on that. Now we have a policy here for increasing the group size and also a policy here for decreasing the group size. So the name for this policy, I’m just going to leave as increase group size. So auto scaling knows when to execute this policy, we need to set an alarm. So click on add new alarm. And this will send a notification. At that the moment I’ve got a notification set up as CADemo, which is an SNS topic. And I want this alarm to trigger whenever the average CPU utilization is greater than or equal to 75% for one consecutive period of five minutes. The name of this alarm will be deploy new instances. And then I simply click on create alarm. Now I want to take the action of adding just a single instance when the CPU utilization is greater than 75%. </p>
<p>Now we can do the same for the decrease group size, so whenever the average CPU utilization is less than or equal to 30%, for two consecutive periods of five minutes, and I’m going to call this remove instances and create that alarm. I’m going to say remove one instance when the CPU is less than 30%. Once your policies are set, you can click on configure notifications. So if you want to configure your notifications, simply click on add notification. And that’s my SNS topic that I had. So whenever instances are launched, terminated, fail to launch and fail to terminate, I want to receive those notifications. Let’s click on configure tags. And here, you can add any tags to auto scaling group. Just going to leave that blank for this demonstration, click on review. And here we have all of our configuration that we just made. Once you’re happy with that, click on create auto scaling group. </p>
<p>And we now have our auto scaling group configured and except here the minimum and maximum instances two and five and the availability zones et cetera. Now at the bottom here you can go into the auto scaling group details. Here we can see it’s launching two new instances because we said we want to start with two instances to start with. We can review our scaling policies. We can look at instances, monitoring, notifications, et cetera, et cetera. So if we go over to our instances, we should see two new instances that are launching. and here we can see that these two here are both initializing. So that’s our two new instances that are running because of our auto scaling group which was based off of our launch template. And that’s it. </p>
<p>In the next lecture, I should be looking at how both ELB and auto scaling combined can be used to manage your EC2 infrastructure.</p>
<h1 id="What-is-an-Elastic-Load-Balancer-ELB"><a href="#What-is-an-Elastic-Load-Balancer-ELB" class="headerlink" title="What is an Elastic Load Balancer (ELB)?"></a>What is an Elastic Load Balancer (ELB)?</h1><p>Hello and welcome to this lecture, which is going to focus on what the AWS Elastic Load Balancer service is and does. </p>
<p>Now the main function of an Elastic Load Balancer, commonly referred to as an ELB, is to help manage and control the flow of inbound requests destined to a group of targets by distributing these requests evenly across the targeted resource group. These targets could be a fleet of EC2 instances, Lambda functions, a range of IP addresses, or even Containers. The targets defined within the ELB could be situated across different Availability Zones for additional resiliency or all placed within a single Availability Zone. Let’s look at this from a typical scenario. </p>
<p>Let’s suppose you have just created a new application, which is currently residing on a single EC2 instance within your environment, and this is being accessed by a number of users. At this stage, your architecture can be logically summarized as shown. If you are familiar with architectural design and best practices, then you would realize that using a single instance approach isn’t ideal although it would certainly work and provide a service to your users. However, this infrastructure layout brings some challenges. For example, the single instance where your application is located can fail, perhaps from a hardware or software fault. And if that happens, your application will be down and unavailable to your users. Also, if you experience a sudden spike in traffic, your instance may not be able to handle the additional load based on its performance limitations. As a result, to strengthen your infrastructure and help remediate these challenges, the unpredictable traffic spikes and high availability, et cetera, you should introduce an Elastic Load Balancer, an additional instance that’s running your application into the design as shown. </p>
<p>As you can see, in this design, the AWS Elastic Load Balancer will act as the point for receiving incoming traffic from users and evenly distribute the traffic across a greater number of instances. By default, the ELB is highly available as this is a managed service provided by AWS. And so, they ensure its resilience so we don’t have to. Although it might seem that ELB is a single point of failure, the ELB is in fact comprised of multiple instances managed by AWS. Also in this scenario, we now have three instances running our application. Now let me revisit the challenges we discussed previously. If any of these three instances fail, the ELB will automatically detect the failure based on defined metrics and divert any traffic to the remaining two healthy instances. Also if you experience a surge in traffic, then the additional instances running your application would help with the additional load. One of the many advantages of using ELB is the fact that it is managed by AWS, and it is, by definition, elastic. This means that it will automatically scale to meet your incoming traffic as the incoming traffic scales both up and down. </p>
<p>If you are system administrator or DevOps engineer running your own load balancer by yourself, then you would need to worry about scaling your load balancer and enforcing high availability. With an AWS ELB, you can create your load balancer and enable dynamic scaling with just a few clicks. Depending on your traffic distribution requirements, there are three ELBs available within AWS to choose from. </p>
<p>Firstly, the Application Load Balancer. This provides a flexible feature set for your web applications running the HTTP or HTTPS protocols. The Application Load Balancer operates at the request level, and it also provides advanced routing, TLS termination, and visibility features targeted at application architectures, allowing you to route traffic to different ports on the same EC2 instance. </p>
<p>Next, there is a Network Load Balancer. This is used for ultra-high performance for your application while at the same time managing very low latencies. It operates at connection level, routing traffic to targets within your VPC, and it’s also capable of handling millions of requests per second. </p>
<p>Finally, the Classic Load Balancer. This is primarily used for applications that were built in the existing EC2 Classic environment and operates at both the connection and request level. We’ll now talk a little bit about the components of an AWS ELB and some of the principles behind them. </p>
<p>Listeners. For every load balancer, regardless of the type used, you must configure at least one listener. The listener defines how your inbound connections are routed to your target groups based on ports and protocols set as conditions. The configurations of the listener itself differs slightly depending on which ELB you have selected. I will dive into the configuration of these as I discuss each ELB in further detail in upcoming lectures. </p>
<p>Target groups. A target group is simply a group of resources that you want your ELB to route requests to, for example a fleet of EC2 instances. You can configure your ELB with a number of different target groups, each associated with a different listener configuration and associated rules. This enables you to route traffic to different resources based upon the type of request. Rules. </p>
<p>Rules are associated to each listener that you have configured within your ELB, and they help to define how an incoming request gets routed to which target group. As you can see, your ELB can contain one or more listener. And each listener can contain one or more rules, and each rule can contain more than one condition, and all conditions in the rule equal a single action. An example rule could look as follows where the if statement resembles the conditions and the then statement acts as the action if all the conditions are met. So, depending on which listener request was responded to by the ELB, a rule based upon a priority listing would be associated containing these conditions and actions. If the request came from within the 10.0.1.0&#x2F;24 network range, which is the first condition, and was trying to carry a HTTP PUT request, the second condition, then the request would be sent to the target group entitled Group1, which is the action. </p>
<p>Health checks. The ELB associates a health check that is performed against the resources defined within the target group. These health checks allow the ELB to contact each target using a specific protocol to receive a response. If no response is received within a set of thresholds, then the ELB will mark the target as unhealthy and stop sending traffic to that target. </p>
<p>Internal or Internet-facing ELBs. There are two different schemes that can be used for your load balancers, either internal or Internet-facing. Internet-facing, as the name implies, the nodes of the ELBs that are defined as Internet-facing are accessible via the Internet and so have a public DNS name that can be resolved with public IP address. This would be in addition to an internal IP address as well. This allows the ELB to serve incoming requests from the Internet before distributing and routing the traffic to your target groups, which in this instance could be a fleet of web servers receiving HTTP or HTTPS requests. When your Internet-facing ELB communicates with its target group, it will only use the internal IP address, meaning that your target group does not need public IP addresses. An internal ELB only has an internal IP address. This means that it can only serve requests that originate from within your VPC itself. For example, you might have an internal load balancer sitting between your web servers in the public subnet and your application servers in the private subnet. </p>
<p>ELB nodes. During the creation process of your ELBs, you’re required to define which Availability Zone you’d like your ELB to operate within. For each Available Zone selected, an ELB node will be placed within that Availability Zone. As a result, you need to ensure that you have an ELB node associated to any Availability Zones for which you want to route traffic to. Without the Availability Zone associated, the ELB will not be able to route traffic to any targets within that Availability Zone even if they are defined within the target group. This is because the nodes are used by the ELB to distribute traffic to your target groups. </p>
<p>Cross-Zone load balancing. Depending on which ELB option you select, you may have the option of enabling and implementing Cross-Zone load balancing within your environment. Let’s presume you have two Availability Zones activated for your ELB with each associated load balancer receiving equal amount of traffic. One Availability Zone has six targets, and the other has four as shown. When Cross-Zone load balancing is disabled, each ELB and its associated AZ would distribute its traffic with the targets within that Availability Zone only. As we can see from the image, this results in an uneven distribution of traffic for each target across the Availability Zones. With Cross-Zone load balancing enabled, regardless of how many targets are in an associated Availability Zone, the ELBs would distribute all incoming traffic evenly between all targets, ensuring each target across the Availability Zones have an even distribution. </p>
<p>That now brings me to the end of this lecture. In the lecture, I shall be discussing server certificates and how they are used with load balancers to help terminate encrypted requests.</p>
<h1 id="SSL-Server-Certificates"><a href="#SSL-Server-Certificates" class="headerlink" title="SSL Server Certificates"></a>SSL Server Certificates</h1><p>Hello and welcome to this short lecture which will provide a high-level overview of server certificates and how they are used within your elastic load balancers. </p>
<p>As I mentioned in the previous lecture the Application Load Balancer provides a flexible feature set for your web applications running the HTTP or HTTPS protocols. As such, the ALB listener options available when creating your ALB are either the HTTP or HTTPS protocol on port 80 and 443 respectively. Configuration of your HTTP port 80 listeners is a fairly simple process, and I’ll cover this in the next lecture. However, there will times when you would need to use the HTTPS encrypted protocol as a listener and this requires some additional configuration. </p>
<p>So let me run through some of the points when using HTTPS as a listener. HTTPS is an encrypted version of the HTTP protocol and this allows an encrypted communication channel to be set up between clients initiating the request and your Application Load Balancer. However, to allow your ALB to receive encrypted traffic over HTTPS it will need a server certificate and an associated security policy. </p>
<p>SSL or Secure Sockets Layer, to give it its full name, is a cryptographic protocol, much like TLS, Transport Layer Security. Both SSL and TLS are used interchangeably when discussing certificates for your Application Load Balancer. The server certificates used by the ALB is an X.509 certificate, which is a digital ID that has been provisioned by a Certificate Authority and this Certificate Authority could be the AWS Certificate Manager service also known as ACM. This certificate is simply used to terminate the encrypted connection received from the remote client, and as a part of this termination process the request is then decrypted and forwarded to the resources in the ELB target group. </p>
<p>When you select HTTPS as your listener, you will be asked to select a certificate using one of four different options available. Either choose a certificate from ACM, upload a certificate to ACM, choose a certificate from IAM, or upload a certificate to IAM. The first two options relate to ACM. An ACM is the AWS Certificate Manager and this service allows you to create and provision SSL&#x2F;TLS server certificates to be used within your AWS environment across different services. This integration with ACM simplifies the configuration process of implementing a new certificate for your elastic load balancer and as a result, it’s the preferred option. </p>
<p>The last two options allow you to use a third-party certificate by using IAM as your certificate manager and you would select this option when deploying your ELBs in regions that are not supported by ACM. For a list of supported regions, please see the following link. For detailed information on how to upload, retrieve, and list server certificates via IAM, please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html">AWS documentation</a>. Using ACM as your certificate manager allows you to both create certificates from within ACM itself and also import existing certificates created from outside of AWS adding additional flexibility for your current third party certificates. The configuration of ACM is out of scope for this course. However, you can find further information on this service using the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">link</a>. </p>
<p>Now it’s brought me to the end of this lecture. In the next few lectures I shall be looking at the configuration of each of the defined load balancers, application, network, and classic, to provide you with more information on their components starting with the Application Load Balancer, the ALB.</p>
<h1 id="Application-Load-Balancers"><a href="#Application-Load-Balancers" class="headerlink" title="Application Load Balancers"></a>Application Load Balancers</h1><p>Hello and welcome to this lecture covering the Application Load Balancer, the ALB. The first of the three load balancers that I shall be discussing. If you are familiar with the open systems interconnection model, the OSI model, then you won’t be surprised that the ALB operates at layer seven, the application layer. The application layer of the OSI model serves as the interface for users and application processes to access network services. Everything at this layer is application specific. The application layer of the model helps to provide network services to the applications. And examples of the application process or services it offers are http, ftp, smtp and nfs. For more information on the OSI model, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/">here</a>.</p>
<p>As you can see AWS suggests you use the application load balancer if you need to provide a flexible feature set including advanced routing and visibility features aimed purely for application architectures such as microservices and containers when used in HTTP or HTTPS. Before configuring your ALB, it’s good practice to set up your target groups. Now I explained in a previous lecture that a target group is simply a group of resources that you want your ALB to route requests to. You might want to configure different target groups depending on the nature of your requests. For example, let’s say you had an internet-facing ALB, you might want a target group allocated to handle and process HTTP port 80 requests and a different target group configured to process requests from the secure HTTPS protocol using port 443. In this scenario, you could configure two different target groups and then route traffic, depending on the request, to different targets through the use of listeners and rules. </p>
<p>I now want to demonstrate how to configure an ALB and in this demonstration, I will also show you how to set up target groups as well. Let’s take a look. </p>
<p>As you can see, I’m in the AWS management console and the first thing we want to do is create our target groups and I can do this by going into the EC2 service which is found here under compute. And then if I scroll down on the left-hand side, I’ll get to the load balancing section here. Then in here, I have load balancers and target groups, but first I want to set up our target groups. So, if you select target group, as you can see there’s no target groups currently configured. So if I click on the blue button, Create target group, I now have a page of information that I need to complete. </p>
<p>So, firstly the target group name and I’m going to call this Web Servers. And then I have my target type and here I can specify it by instance, IP or Lambda function. I’m going to leave it as instance, then we can select what protocol we want. As this is going to be a web service, I’ll leave it as HTTP on port 80 and here we can select our VPC that we want this target group to exist in. So, select my appropriate virtual private cloud there. At the bottom we just have some health check settings and this is the path and protocol that the load balance will use when performing its health checks. So, for the path I’ll just put in index.html as an example. If we take a look at the advanced health check settings, here we have a number of other options that we can select. </p>
<p>This value specifies the healthy threshold which means the load balancer will have to receive five responses from the instance before deeming the previously unhealthy target healthy again, and the unhealthy threshold means that the load balancer only has to receive two failures before marking the instance as unhealthy. The timeout is simply the number of seconds that the load balancer will wait for a response, and the interval is how many seconds between each health check. Once we’re happy with our configuration, we’ll click on Create. Looks like I left this space in the name and you can’t have any spaces. So let’s just delete that and click on Create. And there you go, our target group was successfully created, and now we can see it in our list of target groups. </p>
<p>However, we don’t have any targets associated with this group yet. That was just simply the configuration of the group. So, down here we have the Description, the Targets, the Health checks, Monitoring and Tags, but if we click on the Targets tab, then here we can start adding our targets associated with this target group. And if we click on Edit, we can see here at the bottom that there’s two instances that I have running, web server one, and web server two. Now here I can select which instance I want to add and associate to this target group. For this demonstration, I’m going to select both instances and then add these as registered targets to this group. And as you can see, these two instances have now been added under the Registered targets section. Click on Save. And we can see here, that we have two registered targets which are the ones I just added, web server one and web server two, now associated to this target group. </p>
<p>Let’s just quickly look at these other tags here as well, the Health checks, that’s the health check information that we configured during the creation of the target group; Monitoring, this shows a number of CloudWatch metrics associated with the target group such as number of healthy hosts and unhealthy hosts et cetera. And then we also have Tags if you wanted to create a key-value pair for your target group and you can do so here. So, as you can see, it’s very easy to create different target groups as you need to for your load balancing. </p>
<p>Let us now go ahead and create an Application Load Balancer. So, back on the left-hand side here, again under Load Balancing, we have Load Balancers. So if you select that. Now I don’t have any load balancers configured here. So if I click on Create Load Balancer, and I can create an Application Load Balancer and Network Load Balancer or the Classic. In this example, I’m going to create the Application Load Balancer. So, click on Create. Now here we have a number of different steps. Firstly, we need to give it a name. So this would be WebServerALB, and we’ll have this as internet-facing using ipv4. Now down here we have our listeners. So this is the port and protocol that we want the load balancer to listen on and as this is our web server, let’s leave it as HTTP on port 80. If you want to add additional listeners, then you can do so just by selecting Add Listener and selecting the different protocols et cetera. Now if we scroll down to the bottom here, we can select our Availability Zones that we want to enable for our load balancer. So for eu-west-1a, let me select this subnet and for eu-west-1b I, shall select this subnet. So there are the two subnets that I want to associate with the load balancer, and each of them are in a set per availability zone as you can see here. </p>
<p>Now I need to go and configure my security settings. And I have a message here to say that the load balancer security is not using a secure listener. Now, if we were to go back and change that to HTTPS, then we would be using a secure listener and we’d also have to set up server certificates as well, but for this demonstration, I just want to show you how to create the Application Load Balancer, but generally in a wide-scale production environment, if you’re creating a load balancer for your internet-facing resources, then you’d probably want to use https for that additional security. Click on next. </p>
<p>Now we’ll need to select the security group that is going to be associated to our load balancer. So we could create a new security group, call this our Application Load Balancer. So we’ll have HTTP from any IP address, and we click on that Next Configure Reading. And this is where we can specify our target group for the Application Load Balancer. So we can create a new target group here and go through the same process as we did earlier or click on the dropdown list and select an existing target group and here we can see that we have our WebServers target group that we created earlier with all the settings already pre-filled. Click on Next Register Targets, and here we can see that these are the two targets associated with the target group. Click on Next Review. And this is just a review of all the configuration options that we made during the creation of this. Once you’re happy with that, simply click on Create. And then we have it. </p>
<p>We have our new load balancer, our WebServerALB was successfully created. So, let’s take a look. This might take it a couple of minutes for it to be provisioned. While that’s being provisioned, if we take a look at the bottom here, we can see that we have some basic configuration that we’ve set up with the availability zones, the fact that it’s internet-facing, and we have the ARN et cetera as well. We have our listener configuration here that we can change if we need to. As we can see, at the minute we’re listening on port 80. Again, we have some monitoring metrics here being carried out by CloudWatch. We’ll see a number of different CloudWatch metrics. Actually, we can now see that that the state is now active. So that’s our Application Load Balancer set up and configured. </p>
<p>Now before I finish this quick demonstration, I just want to show you the rules that I mentioned earlier with regards to listeners. So, if we go down to our Listeners tab here, we can see that we can view and edit our rules for our listener. So if we click on that, at the moment we can see that we have our default action here listening on port 80. We can see that this rule cannot be moved or deleted. That’s basically saying that this listener is listening on port 80 and for any requests then forward it to the WebServers. But we can add additional rules in here. So let’s take a look. So if we click on this plus button, we can see that we now have this option here of Insert Rule. So, let me select that, and that allows me to add a new rule in. So, first we need to add a condition. So, for example, let’s have the condition of a Source IP, then we just put in a random IP address here. So, this is saying if the source IP is this IP address, then add the following action, and here we could choose to forward it to another target group. I mean, I’ve only got one target group configured at the minute called WebServers, but if I had other target groups here with different instances associated to those target groups, then I could select a different target group to forward any requests that are received from this IP address. So that allows you to customize how your load balancer directs traffic, depending on what rules you create with your listeners. So, when I was talking about conditions and rules in a previous lecture, then this is the section that I was referring to. So I just wanted to show you that quickly within this demonstration, where you can edit your rules and add customization with conditions and actions. </p>
<p>Okay, and that’s the end of this demonstration.</p>
<h1 id="Network-Load-Balancers"><a href="#Network-Load-Balancers" class="headerlink" title="Network Load Balancers"></a>Network Load Balancers</h1><p>Hello and welcome to this lecture focusing on the network load balancer and its configuration. </p>
<p>Between the ALB and the NLB, the principles are the same as to how the overall process works, so to load balance incoming traffic from a source to its configured target groups. However, whereas the ALB work to the application level analyzing the HTTP header to direct the traffic, the network load balancer operates at Layer 4 of the OSI model enabling you to balance requests purely based on the TCP and UDP protocols. As such, a request to open a TCP or UDP connection is established to load balance the host in the target group. The listener supported by the NLB include TCP, TLS and UDP. The NLB is able to process millions of requests per second making the NLB a great choice if you need ultra high performance for your application. Also if your application logic requires a static IP address, then the NLB will need to be your choice of elastic load balancer. Unlike the application load balancer that has cross-zone load balancing always enabled, for the NLB this can either be enabled or disabled. When your NLBs are deployed and associated to different availability zones, an NLB node will be provisioned in these availability zones. The node then uses an algorithm which uses details based on the sequence, the protocol, source port, source IP, destination port and destination IP to select the target in that zone to process the request. When a connection is established with a target host, then that connection will remain open with that target for the duration of the request. Let me now provide a demonstration on how to configure and set up a network load balancer. </p>
<p>As you can see, I’m in the AWS management console. So to create our network load balancer, let’s go to EC2 under Compute. Then if we go down the left-hand side again under Load Balancing, click Load Balancers, we can see here our existing application load balancer we created before. So let’s click on Create Load Balancer and this time we’re going to create a network load balancer. So click on Create. And again, it’s very similar configuration to the application load balancer. So let’s firstly give it a name. Let’s call this DNS-NLB. This time we’ll have it internal facing. For our listener, let’s select the UDP protocol and the load balancer port is port 53 which is DNS. Again, we can select our availability zones where we want our load balancer to reside. So under eu-west-1a, let me select that subnet. And on the b, that one there. Next, configure security settings. Again, we receive this message because we’re not using a secure listener and for this demonstration that’s okay. Configure routing, now we need to associate our target group. Let’s create a new target group this time and we’ll call this DNS. For the target type, I shall leave as instance. We have our port and protocol there. Health checks under TCP. And if you wanted to, you can make any changes to your advanced health check settings there. Next, click on Register Targets. As we can see, we don’t have any registered targets as yet. If I scroll down, I can see I have one instance here so I’m going to add that to the registered list of targets. Once that’s been added, click on Next Review. Once you’re happy with all your configuration settings, click on Create. And there you have it. Your network load balancer is now created. We can see here provisioning which is our network load balancer. This is our previous application load balancer that we created earlier. So it’s a very similar process with different ports and protocols available between the load balancers. And that’s the end of this demonstration.</p>
<h1 id="Classic-Load-Balancers"><a href="#Classic-Load-Balancers" class="headerlink" title="Classic Load Balancers"></a>Classic Load Balancers</h1><p>Hello and welcome to this lecture covering the last of the load balancers that are available, the classic load balancer. The classic load balancer supports TCP, SSL&#x2F;TLS, HTTP, and HTTPS protocols. However, it does not offer as wide a range of features as the other load balancers. It is considered best practice to use the ALB over this classic load balancer unless you have an existing application running in the EC2-Classic network. Now, many of you will be unfamiliar with the EC2-Classic platform, and this is because it is no longer supported for newer AWS accounts. In fact, any account created after the 12th of April 2013 will not support EC2-Classic. </p>
<p>The EC2-Classic platform was originally introduced when the first release of EC2 was made generally available a number of years ago. The EC2-Classic platform enabled you to deploy your EC2 instances in a single flat network shared with other customers instead of inside a VPC. Although the classic load balancer doesn’t provide as many features as the application load balancer, it does offer the following which the ALB does not. It supports EC2-Classic, it supports TCP and SSL listeners, and it has support for sticky sessions using application-generated cookies. Again, the classic load balancer works in much the same way as the other load balancers already discussed, and again, cross-zone load balancing can either be enabled or disabled. Let’s now take a look at the creation of a classic load balancer. </p>
<p>So let’s now create the last type of load balancer, the classic load balancer. So again, let’s go to EC2. Down the left-hand side to load balancers. We have our previous application load balancer and our network load balancer. Let’s now create the classic load balancer. So we go across here to create. Give this a name. I’ll just call it classic. Select the VPC that I’d like to do. Now here we have our listener configuration, so for ease, let’s just have this listed on port 80. And then here, we need to select our availability zones that we’d like. So let’s select this one and also this one here. Once we’ve selected our subnets for our load balancer, we can then assign security groups. I’m going to use an existing security group that I’ve created previously. Once that’s selected, click on Configure Security Settings. Again, it’s telling us we’re not using a secure listener. Again, for this demonstration, that’s more than okay. Now we can configure our health checks. This will probably look familiar to you when we’re discussing the application load balancer. So the port and protocol using and the path, as well, the ping path, which is what the load balancer will check to make sure it can reach to determine if the instance is healthy or not. Once you’re happy with those details, select Add EC2 Instances. </p>
<p>Now here we can select the instances that you want to associate to the load balancer, and this is different to the application load balancer and the network load balancer, where we used target groups. With the classic, we simply select the instances that we want included, so we don’t use target groups for a classic load balancer. So for this example, we can select those two options, coming down across to add tags. Put in any tags you want associated for the load balancer. Click on Review and Create, confirm that you’re happy with your settings, and then click on Create. And there we have it. So if we go back here, you can now see that we have our three different load balancers that we’ve created. Here we have our application load balancer, this was our network load balancer, and here we have our classic load balancer. And it’s as simple as that. </p>
<p>Before I finish this lecture, it’s a good time to take a quick look at the comparison between the three load balancers that we’ve looked at. To help with this, AWS Provides a great table to show the feature differences between each ELB, which can be found using the link shown on screen. We can clearly see that the ALB is the most feature-rich. However, the NLB supports some significant differences to that of the ALB, such as support for static IPs, EIPs, and preserving source IP addresses. </p>
<p>That now brings me to the end of this lecture. Coming up next, I shall be looking at auto scaling and the benefits that this feature brings.</p>
<h1 id="Using-ELB-and-Auto-Scaling-Together"><a href="#Using-ELB-and-Auto-Scaling-Together" class="headerlink" title="Using ELB and Auto Scaling Together"></a>Using ELB and Auto Scaling Together</h1><p>Hello and welcome to this short lecture where I shall discuss the relationship of ELBs and EC2 auto scaling. As you saw in the demonstration on the previous lecture, it’s easy to associate your EC2 auto scaling group to an elastic load balancer and this is because the two services go hand in hand to provide optimal efficiency for both the performance and cost perspective. Each service by itself provides a great way to solve particular operational hurdles. The ELB allows you to dynamically manage loads across your resources based upon target groups and rules whereas EC2 auto scaling allows you to elastically scale those target groups based upon the demand put upon your infrastructure. However, one without the other can cause an operational burden. </p>
<p>For example, let’s say you have an ELB configured but without any auto scaling. You will need to ensure that you manually add and remove targets based upon the demand. You will need to monitor this demand allowing you to manually add or remove instances as required. Now let’s look at the reverse. Let’s assume you have EC2 auto scaling configured but no elastic load balancer. How are you going to evenly distribute traffic to your EC2 fleet? </p>
<p>Hopefully, you can see the benefit of combining an ELB and auto scaling to help manage and automatically scale your EC2 compute resources both in and out. When you attach an ELB to an auto scaling group, the ELB will automatically detect the instances and start to distribute all traffic to the resources in the auto scaling group. When you want to associate an application load balancer or network load balancer, you associate the auto scaling group with the ELB target group. When you attach a classic load balancer, the EC2 fleet will be registered directly with the load balancer. </p>
<p>Let me now provide another demonstration on how to associate your ELBs with an auto scaling group. </p>
<p>Okay, so to attach an existing load balancer to your auto scaling group is very quick and simple. So firstly we go to our EC2 dashboard under Compute. We then go down to our Auto Scaling Groups at the very bottom on the left-hand side. We can see here our auto scaling group that we created in a previous demonstration. Now you’ll notice on the Details section at the bottom here there’s a section for classic load balancers and target groups and both of these are empty fields. So let’s look at how you would associate either a classic load balancer or your target group. So once our auto scaling group is selected, we go to up to Actions and then Edit. Now if we scroll down, we get to the section here Classic Load Balancers and Target Groups. Now if we want to add a classic load balancer, then you can select here and select any classic load balancers that you have configured so I can select that one for example or if you’re using an application load balancer or network load balancer, then you associate it with the target groups because the target groups are the pool of resources that the application load balancer or network load balancer are associated to. Now here we have two target groups that we created in a previous demonstration either DNS or WebServers. So we could select WebServers as our target group and simply click on Save. </p>
<p>And as you can see now, this entry here for target groups, the auto scaling group is now associated to the WebServers target group. And it’s as simple as that. So all you need to do to associate an existing load balancer to your auto scaling group is to edit the auto scaling group and if it’s a classic load balancer, add in the classic load balancer or if it’s an application load balancer or network load balancer, then you associate it to the relevant target group. And that’s it. </p>
<p>That now brings me to the end of this lecture. Coming up next, I will provide a summary of the key points made throughout this course.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I now want to quickly highlight some of the main points from each of the previous lectures. I started off by looking at what ELBs were and within this lecture, I explained that the main function of an ELB is to elastically manage and control the flow of inbound requests destined to a group of targets distributing requests evenly. These targets could be a fleet of EC2 instances, AWS Lambda functions, a range of IP addresses or even Containers. ELBs help you maintain your solution by minimizing outages from hardware of software faults. ALBs are also used to help handle unpredictable traffic spikes, and by default, the AWS ELB is highly available. ELBs will automatically detect failed hosts on defined metrics and divert any traffic to the remaining healthy instances. ELBs will automatically scale to meet your incoming traffic, and there are three different types available within AWS, the Application Load Balancer, the Network Load Balancer, and the Classic Load Balancer. ELB listeners define how your inbound connections are rooted to your target groups based on ports and protocols set as conditions. And target groups allow you to group resources that you want your ELB to route requests to. Rules are associated to each listener to help define how an incoming request gets routed to which target group. Health checks allow the ELB to contact each target using a specific protocol to receive a response. If no response is received within a set threshold, then the ELB will mark the target as unhealthy and stop sending traffic to that target. There are two different schemes of ELBs, firstly Internet-Facing, and the ELB nodes are accessible via internet and have a public DNS name that can be resolved to its public IP address, in addition to an internal IP address as well. The internal ELB only has an internal IP address, and this means that it can only serve requests that originate from within the VPC itself. ELB nodes are placed within the AZ where you want to perform load balancing, and nodes are used by ELBs to distribute traffic to your target groups. Cross-zone load balancing will distribute all incoming traffic evenly between all targets in all configured AZs ensuring each target across the AZs have an even distribution. </p>
<p>Next I looked at server certificates for encrypted requests and how ELBs manage these connections and during this lecture I covered the following points. When using HTTPS as a listener, it requires additional configuration relating to server certificates. HTTPS is an encrypted version of the HTTP protocol and this allows encrypted communication between clients initiating the request and your ALB. To receive encrypted traffic over HTTPS your ELB needs a server certificate and an associated security policy. The server certificate used is an X.509 certificate, and certificates are provisioned by a certificate authority which could be the AWS Certificate Manager or ACM. Certificates are used to terminate encrypted connections where the request is then decrypted and forwarded to a target group. Using the ACM simplifies the configuration process of implementing a new certificate for your ELB and this is the preferred option. It is also possible to use a third party certificate by using IAM and your certificate manager, and this option is required in regions not supported by ACM. </p>
<p>Next I looked at the individual load balancers, and during these three lectures, we learned that the application load balancer operates at layer seven of the OSI model, and you should use the ALB if you need to provide a flexible feature set, including advanced routine and visibility features aimed purely for application architectures. The ALB has cross-zone load balancing always enabled, and the listeners supported by the ALB includes HTTP and HTTPS. The network load balancer operates at layer four of the OSI model, enabling you to balance requests purely based upon the TCP and UDP protocols and the listeners supported by the NLB included TCP, UDP and TLS. The NLB is a great choice if you need ultra high performance for your application, and cross-zone load balancing on the NLB can be enabled or disabled. The classic load balancer supports TCP, SSL, TLS, HTTP and HTTPS protocols. The classic load balancer does not offer as wide a range of features as the other load balancers. It is best practice to use the ALB over the Classic Load Balancer unless you have an existing application running in the EC2-Classic network. The classic ELB has support for EC2-Classic, TCP and SSL listeners and sticky sessions. Cross-zone load balancing can either be enabled or disabled for the classic load balancer. I also performed a number of demonstrations. We showed you have to configure each of these load balances. </p>
<p>The course then had a slight change of focus as I moved onto Auto Scaling and temporarily away from Elastic Load Balancing. I began by explaining what EC2 Auto Scaling was and here I covered the following. Auto Scaling is a mechanism that automatically allows you to increase or decrease your EC2 resources to meet the demand based off of custom defined metrics and thresholds. Using metrics, you can automatically add and remove instances as and when load both increases and decreases. By scaling your resources back helps you to optimize the cost of your EC2 fleet. Advantages of Auto Scaling include automation, with the provisioning and removal of instances, greater customer satisfaction, through reliable and stable infrastructure, and cost reduction thanks to scaling-in policies. </p>
<p>This then led me onto the next lecture where I discussed the different components of Auto Scaling. The main components are covered with the Launch Configuration and Launch Template in addition to the Auto Scaling group. The Launch Configuration or Launch Template define how an Auto Scaling group builds new EC2 instances, and the Launch Template provides additional features over the launch configuration including versioning and simplifies your auto scaling configuration. The Auto Scaling Group defines the desired capacity and other limitations of the group using scaling policies and which Availability Zones the Group should scale resources in. I then performed a demonstration in this lecture on how to create the launch configuration and template in addition to auto scaling groups so you could see how they were constructed. </p>
<p>In the final lecture I touched on some points relating to how both ELBs and EC2 Auto Scaling can be used in conjunction with each other and here I explained that both ELB and Auto Scaling goes hand in hand to provide optimal efficiency from both a performance and cost perspective. However one without the other can cause an operation burden. When you want to associate an ALB or NLB to an auto scaling group, you actually associate the auto scaling group with the target group. When you attach a classic load balancer, the EC2 fleet will be registered directly with the load balancer. That now brings me to the end of this lecture and to the end of this course. You should now have a full understanding of AWS Elastic Load Balancing and EC2 Auto Scaling to help you build elastic, reliable, efficient and cost optimized EC2 solutions within your VPC. </p>
<p>If you’d like some hands on experience on some of the topics I’ve discussed in this course, then we do offer a number of labs, so please do take a look. We have the Working the Application Load Balancer lab, Working with Amazon EC2 Auto Scaling Groups and Network Load Balancer, and How to Create Your First Auto Scaling Group.</p>
<p>If you have any feedback on this course, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#x73;&#x75;&#112;&#112;&#111;&#x72;&#x74;&#x40;&#x63;&#108;&#x6f;&#117;&#x64;&#97;&#99;&#97;&#100;&#101;&#x6d;&#121;&#46;&#99;&#x6f;&#109;">&#x73;&#x75;&#112;&#112;&#111;&#x72;&#x74;&#x40;&#x63;&#108;&#x6f;&#117;&#x64;&#97;&#99;&#97;&#100;&#101;&#x6d;&#121;&#46;&#99;&#x6f;&#109;</a>. </p>
<p>Thank you for your time and good luck with your continued learning of cloud computing. Thank you. </p>
<h1 id="Introduction-to-Gateway-Load-Balancer"><a href="#Introduction-to-Gateway-Load-Balancer" class="headerlink" title="Introduction to Gateway Load Balancer"></a>Introduction to Gateway Load Balancer</h1><p>Many of AWS services and innovations are customer-driven. The AWS Gateway Load Balancer service is no exception. Many customers have relied on virtual appliances from AWS partners and the AWS marketplace. However, the deployment process and scaling for virtual appliances was difficult to say the least. First, we will need to be able to direct all traffic, inbound and outbound, from an Internet Gateway or Virtual Private Gateway to an Elastic Network Interface of a specific EC2 instance in a VPC. </p>
<p>This feature is essential and happens to be implemented using VPC Ingress routing for the Internet Gateway. Using a VPC Ingress routing, we can forward traffic to a Gateway Load Balancer by updating the route tables in a VPC. The next feature needed is to deal with IP tunneling, such as not to incur errors or conflicts with IP addressing. In a nutshell, we need to be able to grab all traffic, inbound and outbound for the VPC, and redirect it to a virtual network appliance for security processing, and not interrupt the normal flow and interactions of the request and the response. </p>
<p>The process needs to be transparent. The AWS Gateway Load Balancer uses a single point of access for all inbound and outbound traffic, and allows you to scale your virtual appliance with demand as done with other Elastic Load Balancers, like the Application Load Balancer. The Gateway Load Balancer routes traffic through healthy virtual appliances and stop sending traffic if an appliance becomes some healthy. </p>
<p>Using Gateway Load Balancer, you can also add your own logic into any networking path in AWS when you want to inspect and take action on packets. The AWS Gateway Load Balancer sends inbound and outbound traffic transparently over the same consistent route and using the same target. This implements sticky, transparent, and symmetric flow.</p>
<h1 id="Anatomy-of-the-AWS-Gateway-Load-Balancer"><a href="#Anatomy-of-the-AWS-Gateway-Load-Balancer" class="headerlink" title="Anatomy of the AWS Gateway Load Balancer"></a>Anatomy of the AWS Gateway Load Balancer</h1><p>The Gateway Load Balancer consists of two parts. The first part is basically a VPC interface endpoint to the Gateway Load Balancer. Let’s call it a VPC Gateway Load Balancer Endpoint. This endpoint is expected to be defined in the VPC where you want to protect the traffic. The second part is the actual Gateway Load Balancer, which sends traffic to a fleet of EC2 instances running third party network appliance software. The Gateway Load Balancer is required to forward packets without alteration. </p>
<p>In order to make this happen, the Gateway Load Balancer uses a tunneling protocol called GENEVE. More formally, by definition, GENEVE is a tunneling mechanism which provides extensibility while still using the offload capabilities of Network Interface Cards for performance improvement. GENEVE works by creating a Layer 2 logical network that is encapsulated in UDP packets. If that sounded a bit technical, let’s break it down. </p>
<p>A tunnel is created between the Gateway Load Balancer and the fleet of instances on the back-end. Traffic is encapsulated and sent through the tunnel to the security appliances implemented in EC2 instances, which will examine and act on packets as they’re sent or received. The Gateway Load Balancer encapsulates the packets to the target to provide separation and add some additional information about which Gateway Load Balancer Endpoint the packet came from. GENEVE uses port 6081 to get traffic from the Gateway Load Balancer, and it uses HTTP port 80 for health checks.</p>
<h1 id="Gateway-Load-Balancer-Architecture"><a href="#Gateway-Load-Balancer-Architecture" class="headerlink" title="Gateway Load Balancer Architecture"></a>Gateway Load Balancer Architecture</h1><p>One of the most fundamental architecture diagrams for the Gateway Load Balancer is shown as a way to review some of the details that we just discussed. As shown in the diagram, it’s not unusual to see the Gateway Load Balancer endpoint listed as GWLBe and define each in their own subnet per availability zone. Gateway Load Balancer endpoints can be added to a route table as the next hop and integrate the Gateway Load Balancer into the traffic flow of a VPC.</p>
<p> A Gateway Load Balancer endpoint is similar to AWS private link and operates across many accounts and VPCs with centralized control and administration. Also note, the ingress route table associated with the internet gateway in the customer VPC pointing to the Gateway Load Balancer endpoints accordingly. In the general process of setting up a Gateway Load Balancer, you need to provision a VPC dedicated to the Gateway Load Balancer and the third party virtual appliance software you’re running on EC2 instances. You provision the Gateway Load Balancer and target groups as you would any other Elastic Load Balancer like the Application or Network Load Balancer. </p>
<p>On the VPC where your application lives, you create Gateway Load Balancer endpoints on their own dedicated subnets and update the route tables to include the Gateway Load Balancer endpoints for traffic coming from your application subnets to them and traffic from the Internet Gateway to them as well, in order to integrate the security VPC to the traffic flow. </p>
<p>In general, the steps are: </p>
<p>Number 1, locate the partner’s virtual appliance software, perhaps in AWS Marketplace. </p>
<p>Step 2, launch the appliance instances in your security VPC. </p>
<p>Step 3, create a Gateway Load Balancer and target group with those appliance instances. </p>
<p>Step 4, create Gateway Load Balancer endpoints in the VPC where the traffic needs to be inspected. </p>
<p>And step 5, update route tables to make Internet Gateway move traffic to and from the Gateway endpoints and the Gateway Load Balancer endpoint as the next-hop. </p>
<p>Let’s discuss this traffic flow that includes the Gateway Load Balancer with corresponding Gateway interface endpoints in a general architecture diagram.</p>
<h1 id="Traffic-Flow-Steps-for-Gateway-Load-Balancers"><a href="#Traffic-Flow-Steps-for-Gateway-Load-Balancers" class="headerlink" title="Traffic Flow Steps for Gateway Load Balancers"></a>Traffic Flow Steps for Gateway Load Balancers</h1><p>In the architecture diagram shown, we have an application deployed to private subnets in an auto-scaling target group service by an Application Load Balancer. The Application Load Balancers are deployed to public subnets. We also have a separate security VPC with a Gateway Load Balancer and security appliances in a target group for auto-scaling. This will allow for the appliance fleet to adjust based on application load, and therefore, scaling horizontally. </p>
<p>We can follow the steps, a packet will travel in this architecture. </p>
<p>Step 1, a customer access your web application and a request is generated. </p>
<p>Step 2, the landing place for public traffic request is the Application Load Balancer. However, the Internet Gateway is configured with an Ingress route table to direct traffic to a Gateway Load Balancer endpoint. </p>
<p>In step 3, the Gateway Load Balancer endpoint directs traffic to the Gateway Load Balancer in the security VPC. </p>
<p>Step 4, at the Gateway Load Balancer, the packets are wrapped using the GENEVE tunneling protocol and dispatch through the security appliance selected. </p>
<p>Step 5, the packet analysis takes place in the security appliance. What actually happens depends on the appliance being used and the configuration that you define. </p>
<p>Step 6, after analysis, the packets are sent back, still encapsulated to the Gateway Load Balancer where the encapsulation is removed and traffic is sent to the Gateway Load Balancer endpoint where it originally came from. </p>
<p>Step 7, the corresponding Gateway Load Balancer endpoint will direct traffic to the Application Load Balancer and will target your application. </p>
<p>From step 8, the response flow is very similar in that the application response will pass through the Application Load Balancer and into the subnet with the Gateway Load Balancer endpoint in the same availability zone. This will send traffic to the Gateway Load Balancer yet again in the return path for the security VPC. </p>
<p>Step 9, the packets are encapsulated yet again and sent to the security appliance where they are processed. </p>
<p>Step 10, the packets are sent back to the Gateway Load Balancer where encapsulation is removed and packets moved to the Gateway Load Balancer endpoint. </p>
<p>Step 11, the Gateway Load Balancer endpoint will send traffic out through the Internet Gateway. </p>
<p>And in step 12, the response is received by the customer. So, as we get to see from this example flow, the Gateway Load Balancer allows you to leverage and horizontally scale third-party security appliances from the AWS Marketplace in your Amazon VPCs.</p>
<h1 id="AWS-Outposts"><a href="#AWS-Outposts" class="headerlink" title="AWS Outposts"></a>AWS Outposts</h1><p>This service is all about hybrid functionality, helping your align your applications and infrastructure from your on-premises environment with that of the AWS cloud. There are many reasons that organizations use a hybrid cloud mode, utilizing services from both public cloud providers and also their own local data centers. For example, they might need to run applications locally due to latency, security or even governance requirements. AWS understands this and, as a result, they’ve developed AWS Outposts. </p>
<p>With AWS Outposts, it’s now possible to bring the AWS cloud to your data center. This includes the same hardware used by AWS within their data centers. By bringing in AWS hardware to your data center, it allows you to use native AWS services, including the same tools and APIs as you would when running your infrastructure within AWS, the difference being is that the hardware and services will be running locally to help you maintain the need for local applications and workloads, et cetera. </p>
<p>There are two different options available when using Outposts. You can either use VMware on AWS, which will seamlessly run your existing VMware management and infrastructure, or you can use a native AWS variant, which means you can use the same APIs and management tools as you would in AWS but on premises. For those new to VMware Cloud on AWS, it is sold as a service by VMware that allows you to run your applications across VMware’s vSphere suite of products within a software-defined data center, hosted on top of the AWS public cloud. By utilizing VMware underlying cloud foundation, it provides the ability to give you access to many native AWS services and features. Couple this with the ability to continue managing your infrastructure with vSphere, vSAN, NSX and vCenter Server, it enables you to create a secure, flexible and scalable hybrid cloud infrastructure model for your organization. By bringing the same native AWS tools, API, management console and services to your on premises data center, it enables you to manage and implement a hybrid cloud approach with seamless migration abilities, burst capacity and management between local an AWS cloud environments. </p>
<p>To use and organize AWS Outposts for use in your data center, you can order them through the AWS Management Console where you can select the different compute and storage options available to you. Depending on how much hardware you need, you can simply order a single server or variants of rack sizes, including a quarter or half or full-size racks of equipment. From a technical perspective, there are a wide number of EC2 instances available at launch. For example, C5, M5, R5, as well as storage options for EBS volumes, local instance storage and local disk options. AWS Outpost at launch and in the coming months will allow you to run services such as EC2, EBS, RDS, ECS, EKS, Sagemaker and EMR. From a connection perspective, customers can make use of the PrivateLink gateway endpoint to securely and privately connect in resources to others, including Amazon S3 and DynamoDB. </p>
<p>One final point on AWS Outposts is that the service itself is fully managed. This means that you do not need to maintain a level of patch management across your infrastructure. AWS will ensure the infrastructure is patched as and when needed. </p>
<p>That brings me to the end of this lecture. Next I’ll be discussing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-compute-services-2019-reinvent-reminders/license-manager/">AWS License Manager</a>.</p>
<h1 id="What-is-VMware-Cloud-on-AWS"><a href="#What-is-VMware-Cloud-on-AWS" class="headerlink" title="What is VMware Cloud on AWS?"></a>What is VMware Cloud on AWS?</h1><p>Hello, and welcome to this lecture, where I shall explain what <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-vmware-cloud-on-aws/introduction-59/">VMware Cloud on AWS</a> is, along with an overview of its underlying architecture.</p>
<p>VMware Cloud on AWS is sold as a service by VMware that allows you to run your applications across VMware’s vSphere suite of products within an SDDC hosted on top of the AWS Public Cloud. While utilizing VMware’s underlying Cloud foundation, it provides the ability to give you access to many native AWS services and features. Couple this with the ability to continue managing your infrastructure with vSphere, vSAN, NSX, and vCenter Server, it enables you to create a secure, flexible, and scalable hybrid Cloud infrastructure model for your organization.</p>
<p>If you are currently using VMware on-premises in your data center, and you’re looking for a way to migrate your workloads to the Cloud to take advantage of some of the Cloud technology’s key characteristics, such as on-demand resourcing, scalability, flexibility, high availability, security, utility-based metering, and regional expansion, then using VMware Cloud on AWS could be a great solution for you.</p>
<p>One thing to bear in mind, however, is that, at the moment, the service is only available in the US West (Oregon) region. But there are plans to distribute the service to all other AWS regions throughout 2018. Now we know what the service is. Let me now talk a little about the underlying architecture that the service runs on.</p>
<p>The AWS architecture used for VMware Cloud on AWS is different to your standard compute services on AWS, such as EC2 that runs on top of a Xen hypervisor installed on the host, where VMware Cloud on AWS runs on bear-metal AWS infrastructure. This primarily means two things. Firstly, the host itself belongs to a single customer.</p>
<p>And secondly, the host is not running any virtualization software, such as a standard Xen hypervisor that AWS normally uses. Typically, within a normal AWS environment, many customers can share the same underlying host to run their EC2 instances by selecting an option to run their resources on shared-tenancy hosts.</p>
<p>Although it is possible to request a dedicated host if this is required, this is not, however, the same as bare-metal infrastructure used with VMware Cloud on AWS. The difference being is that the EC2 dedicated host will still use the Xen hypervisor to manage underlying virtualization, whereas bear-metal servers are stripped of this virtualization software that is normally included with AWS hosts.</p>
<p>This allows VMware to optimize the AWS host with its own ESXi bare-metal type-1 hypervisor, removing any nested virtualization. In fact, VMware Cloud on AWS does not support nested ESXi virtual machines at all.</p>
<p>From a compute perspective, and during initial availability, there is a minimum and maximum compute cluster size with your SDDC. VMware clusters are comprised of a number of physical hosts. Where the memory and CPU power from those hosts is aggregated into a pool of resources for all virtual machines in the cluster to consume. So the minimum cluster size that you can provision for VMware Cloud on AWS is comprised of four ESXi hosts.</p>
<p>An ESXi host is simply an AWS bare-metal host with VMware’s ESXi hypervisor installed on top of the hardware. Each of these four hosts contains the following hardware. 512 gig of memory, dual CPU sockets containing Intel Xeon processors, and each socket contains 18 cores running at 2. 3 gigahertz. As a result, the minimum cluster-size configuration contains 2 terabytes of memory and 144 CPU cores.</p>
<p>The maximum cluster size, if you need to scale your compute resources out, currently stands at 16 ESXi hosts, meaning that the total resources in a maximum cluster configuration will contain 8 Terabytes of memory and 576 CPU cores. The scale and boundaries of this configuration for these clusters are likely to change over time.</p>
<p>So it’s always good practice to check on the VMware site for the latest information. Now we know the compute capacity. Let’s take a look at the storage.</p>
<p>vSAN storage clusters also draw their resources from a host within the cluster which contains an all-flash array. Each host in the cluster contains the following storage. 8 non-volatile memory express devices, which allows for flash storage to be directly connected to the host, in this case, low-latency SSDs, and solid-state drives. This provides a total of 10 terabytes of raw storage capacity. As a result, the minimum cluster size would provide a 40-terabyte vSAN datastore enabled by 32 NVMe devices.</p>
<p>At the other end of the scale, if we maximize the cluster size to 16 ESXi hosts, the datastore would grow to 160 terabytes across 128 NVMe devices.</p>
<p>It’s worth noting that during the initial availability of VMware Cloud on AWS, it’s not possible to encrypt data at the datastore level or VM level. To ensure your data remains secure, AWS performs encryption at the firmware level for all NVMe devices. The encryption keys are then managed and controlled by AWS and are not shared with VMware.</p>
<p>There is a restriction regarding clusters when it comes to location, in that the clusters created cannot span multiple availability zones or regions. They are restricted to a single AWS AZ within a single region.</p>
<p>Finally, let me take a look at the networking element of VMware Cloud on AWS, which utilizes VMware NSX. Understand that the networking component on VMware Cloud on AWS is probably the most complicated part. It’s important to understand how you can connect to the service from on-premises and also how to integrate the service with your existing AWS account and infrastructure.</p>
<p>VMware NSX is a fundamental component of VMware Cloud on AWS, as it’s used for all network connectivity and provides a bridge between the three environments: your own on-premises datacenter, the SDDC running on AWS, and our virtual private cloud’s VPC in your AWS account. A VPC is an isolated segment of the AWS Cloud which allows you to provision AWS resources in a virtual network.</p>
<p>Each host within the SDDC cluster contains an Elastic Networking Adapter, an ENA, and these ENAs are network interfaces that provide high networking performance and allow throughput of up to 25 gbps. To allow connectivity between your vSphere environment within your own on-premises datacenter, your SDDC, and extending through to your AWS VPC, two gateways are required for two different networks; one is for management traffic, such as administration of vCenter Server, and another will be used for compute and application traffic, such as workload traffic of your virtual machines.</p>
<p>These two gateways are as follows. A Management Edge Gateway, MGW, and a Compute Gateway, CGW. The Management Edge Gateway for the management networking traffic works in conjunction with NSX Edge, which provides network edge security and allows users to connect through to your SDDC vCenter Server via the Internet.</p>
<p>From the management network, it’s then possible to carry out a number of network-administration tasks, such as creating IPsec VPNs, back to your on-premises datacenter, or configuring firewalls. The IPsec VPN can allow communications between your on-premises vCenter-Server instance and components running in the SDDC.</p>
<p>In addition to this, a second VPN connection can be created to allow the connectivity of the VM workloads to transition between on-premises to the SDDC via the compute gateway. Once your management edge gateway is configured, you can then use a feature called Hybrid Linked Mode to connect your vCenter Server in your SDDC with your on-premises vCenter Server.</p>
<p>The compute gateway is used for compute and VM workload traffic, and uses an NSX Edge instance along with a Distributed Logical Router, a DLR, which allows inbound and outbound traffic from your VMs over the second IPsec VPN and to an AWS VPC. Before we move on, I just want to give a little bit more information around the Hybrid Linked Mode.</p>
<p>As I just said, this allows connectivity between your on-premises vCenter Server and the one running in your SDDC. With this active, it allows you to perform a number of management activities, such as performing cold migrations of your workloads between your on-premises environment and your VMware SDDC.</p>
<p>You can use the same credentials that you use for your on-premises vCenter Server with your Cloud SDDC vCenter Server. And using a single vSphere client interface, you can monitor and manage the inventories of both on-premises and in-Cloud SDDC environments. NSX is a complex and essential component, as it manages all networking infrastructure and security across the network, which remains decoupled from the AWS VPC and networking components.</p>
<p>During the creation of your SDDC setup and configuration, you must associate it to an existing VPC within your AWS account. Having your own AWS account is a prerequisite of running VMware Cloud on AWS. During this process, an ENI, Elastic Network Interface, will be created within your own AWS account.</p>
<p>This ENI will then link back to the compute gateway within your VMware SDDC. And this connectivity allows your VMs running in your SDDC to take advantage of and to communicate with other AWS resources, such as S3, EC2, etc. The ENI, essentially, acts as an endpoint for your VMware SDDC to gain access to native AWS services.</p>
<p>Your VMs will use the compute gateway as a bridge between your VMware Cloud and AWS SDDC and the ENI running in your VPC. This traffic between your SDDC and ENI is completely private and will use AWS’ own internal network to provide the connection. It does not use an Internet gateway or any public channel. It’s an internal private link between your SDDC and your AWS VPC.</p>
<p>The creation of the network configuration of your SDDC can be seen as a split between two roles: Cloud network administrators and Cloud administrators. The Cloud network administrator role will use the VMware Cloud on AWS Web portal to configure the following components. The initial network setup, VPN connectivity, to configure firewall access rules for VM workloads and setting administrator access to vCenter Server.</p>
<p>The Cloud administrator, on the other hand, can then use the vSphere Web client to utilize the infrastructure and configuration made by the Cloud network administrator. The vSphere Web-based client allows you to connect to the SDDC vCenter Server to manage your vSphere environment. Once connected, the Cloud administrator can attach VMs to networks, create new logical networks, and control IP addressing for VMs.</p>
<h1 id="An-Overview-of-AWS-Lambda"><a href="#An-Overview-of-AWS-Lambda" class="headerlink" title="An Overview of AWS Lambda"></a>An Overview of AWS Lambda</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="An-Overview-of-AWS-Lambda-1"><a href="#An-Overview-of-AWS-Lambda-1" class="headerlink" title="An Overview of AWS Lambda"></a>An Overview of AWS Lambda</h1><p>To really understand serverless compute, you have to first understand servers. For example, I want you to think about all the work that goes into running an EC2 instance: you have to install software, patch the instance, manage scaling and high availability, configure storage, and then write your code for your application and deploy it to the instance. </p>
<p>Now think about all that infrastructure maintenance and administration going away - enabling you to focus solely on your code and business logic. That’s the idea behind serverless. Now of course, this maintenance and server administration still exists behind the scenes, however, it’s no longer your job to do it - it becomes the service’s responsibility.</p>
<p>The serverless compute service we’ll focus on in this course is called AWS Lambda. Understanding Lambda is the same as understanding almost any function in a piece of code. There are three major parts: </p>
<ol>
<li>The input </li>
<li>The function, and</li>
<li>The output.</li>
</ol>
<h3 id="The-Function"><a href="#The-Function" class="headerlink" title="The Function"></a>The Function</h3><p>Let’s start with the function. Just as EC2 is made up of instances, Lambda is made up of functions. Functions are the code that you write that represents your business logic. In this function you also configure other important details, such as permissions, environment variables, and the amount of power the function needs. The way you specify power is by choosing how much memory you want to allocate to your function. The service then uses this number and provides proportional amounts of CPU, network and disk I&#x2F;O. </p>
<p>To upload your code to the service, you can either write the code directly in the service itself or you can upload this code via a zip file or files stored in Amazon S3. The programming language you write your code in must match the runtime you select in the service. </p>
<p>There are several options for runtimes. You can use a runtime that lambda natively supports, such as:</p>
<ul>
<li>Java,</li>
<li>Go, </li>
<li>Powershell, </li>
<li>Node.js, </li>
<li>C#,</li>
<li>Python, and </li>
<li>Ruby.</li>
</ul>
<p>Or, if you want to use a language that isn’t in that list, you can choose to bring other languages by using the custom runtime API. So if you’re thrilled at the idea of running PHP or C in Lambda, the custom runtime feature is for you. </p>
<h3 id="The-Input"><a href="#The-Input" class="headerlink" title="The Input"></a>The Input</h3><p>Once you’ve uploaded or written your code - how does your code run? Well, it has to be invoked. This is where the first piece of the equation - the input - comes into play. There are several options for your function to be invoked: </p>
<ul>
<li>Your function could be invoked directly through the console, SDK, AWS toolkits, or through the CLI. </li>
<li>It could be invoked using a function URL, which is an HTTP endpoint you can enable for your Lambda function</li>
<li>Or it could be invoked automatically by a trigger, such as an AWS service or resource. These triggers will run your function in response to certain events or on a schedule that you specify. So, if you want to run your function every day at 8 am, you can do that.</li>
</ul>
<p>When you invoke your function, you can pass in events for the function to process. If a service invokes your function, they can also pass in events - however, the service will be responsible for structuring those events. For example, your code could run in response to a request from API Gateway or an S3 event, such as a PUT object API call. So once the PUT object API call is made, AWS Lambda will run your code, just as you’ve written it, using only the compute power you defined. </p>
<h3 id="The-Output"><a href="#The-Output" class="headerlink" title="The Output"></a>The Output</h3><p>Then you have the third and final piece, which is the output. Once the function is triggered, and your code runs, your Lambda function can then make calls to downstream resources. This means that from your code, you can make API calls to other services like Amazon DynamoDB, Amazon SQS, Amazon SNS and more. </p>
<p>When your Lambda function is triggered, the service automatically monitors your function through logs and metrics. You can additionally choose to write custom logging statements in your code that will help you identify if your code is operating as expected. These log streams act as a recording of the sequence of events from your function. Lambda also sends common metrics of your functions to CloudWatch for monitoring and alerting. </p>
<h3 id="Costs"><a href="#Costs" class="headerlink" title="Costs"></a>Costs</h3><p>So what do you pay for with this service? You only pay for what you use - which means three things. </p>
<ol>
<li>You are charged for the amount of requests that you send to your function. </li>
<li>The Lambda function begins charging you when it is triggered, and stops charging you when the code has been executed. Otherwise known as the duration it runs. This is rounded up to the nearest 1 millisecond of use. </li>
<li>You’re charged based on the amount of compute power you provision for your function. So if you provision the maximum amount of memory, you’ll be charged for that.</li>
</ol>
<h1 id="Invoking-a-Lambda-Function"><a href="#Invoking-a-Lambda-Function" class="headerlink" title="Invoking a Lambda Function"></a>Invoking a Lambda Function</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="Invoking-a-Lambda-Function-1"><a href="#Invoking-a-Lambda-Function-1" class="headerlink" title="Invoking a Lambda Function"></a>Invoking a Lambda Function</h1><p>There are several ways to invoke a Lambda function. You can invoke it directly using the console, the CLI, SDKs, or you can invoke it automatically by using a trigger such as an AWS service. </p>
<p>No matter how you invoke a Lambda function, even if you’re invoking it directly through the Lambda service itself, you’re using the service’s API. Every invocation goes through the Lambda API. </p>
<p>What this API provides to you is three different models of how you can invoke your function:</p>
<h3 id="Synchronous-Push-Based-Model"><a href="#Synchronous-Push-Based-Model" class="headerlink" title="Synchronous (Push-Based) Model"></a>Synchronous (Push-Based) Model</h3><p>The first choice is the synchronous or push-based model. This follows the request&#x2F;response model. For example, let’s say we have a service like API Gateway that gets a request from a client. API Gateway then sends that request to a backend, in this case Lambda. API Gateway does this by making an invoke call to that function. After the Lambda function executes, it then returns a response back to API Gateway, which returns the response to the client. Request goes out, response comes in. </p>
<p>For synchronous invocations, if the function fails, the trigger is responsible for retrying it. In some cases, this might mean that there are no retries. For example, with API Gateway, it can send the error message back to the client. </p>
<p>To invoke a Lambda function synchronously, you can set the invocation type using the AWS SDK or you can use the CLI invoke command to do this. For example, I can use the command</p>
<p><strong>aws lambda invoke –function-name my-function –cli-binary-format raw-in-base64-out –payload ‘{ “key”: “value” }’ response.json</strong></p>
<p>You can also use the –invocation-type parameter and set it as RequestResponse to invoke it synchronously. </p>
<p><strong>aws lambda invoke –function-name my-function  –invocation-type RequestResponse  –cli-binary-format raw-in-base64-out –payload ‘{ “key”: “value” }’ response.json</strong></p>
<h3 id="Asynchronous-Event-based-Model"><a href="#Asynchronous-Event-based-Model" class="headerlink" title="Asynchronous (Event-based) Model"></a>Asynchronous (Event-based) Model</h3><p>The second model is the asynchronous model, also called the event-based model. In this configuration, the response does not go back to the original service that invoked the lambda function. In fact there’s no path back up to the service that triggered the function to run, unless you write that logic yourself. </p>
<p>For example, this is common with Amazon S3 and Amazon Simple Notification Service. Say you want to trigger a lambda function to run once an object is placed in a bucket. You can do this, but it’s not going to send a response back to S3 once it finishes executing, without additional business logic. The nice thing about the asynchronous model, is that it handles retries if the function returns an error or is throttled. It also uses a built-in queue. So, any event sent to your function is placed on this queue first and then eventually sent to the function.</p>
<p>If any of these events can’t be processed for whatever reason, you can send that failed event to a dead letter queue or use Lambda destinations to send a record of the invocation to a service. The dead letter queue will receive only the content of the event, while using Lambda destinations records will include both the request context and payload as well as the response context and payload. While both are a great way to troubleshoot failed events, Destinations is the more feature rich option.</p>
<p>To invoke a lambda function asynchronously, I can use the invoke command, except this time, I would need to use the invocation type parameter and set it to be “event”. </p>
<p><strong>aws lambda invoke –function-name my-function –invocation-type Event –cli-binary-format raw-in-base64-out –payload ‘{ “key”: “value” }’ response.json</strong></p>
<h3 id="Stream-Poll-based-Model"><a href="#Stream-Poll-based-Model" class="headerlink" title="Stream (Poll-based) Model"></a>Stream (Poll-based) Model</h3><p>The last model is the stream model, also called the poll-based model. This is typically used when you need to poll messages out of stream or queue-based services such as DynamoDB streams, Kinesis streams and Amazon SQS. The Lambda service runs a poll-er on your behalf, and consumes the messages and data that comes out of them, filtering through them to invoke your Lambda function on only messages that match your use case. With this model, you would need to create an event source mapping to process items from your stream or queue.</p>
<p>An event source mapping links your event source to your Lambda function, so that the events generated from your event source will invoke your Lambda function. These mappings, the response you get back, the permissions you set up, the polling behavior and even the event itself, can be very different based on the event source you’re using. However, the way you create event source mappings stays the same. You can do this by using the SDK or CLI. Here is an example of how to create an event source mapping by using the CreateEventSourceMapping CLI command: </p>
<p>*<em>aws lambda create-event-source-mapping –function-name my-function –batch-size 500 –maximum-batching-window-in-seconds 5 –starting-position LATEST *</em></p>
<p><strong>–event-source-arn arn:aws:dynamodb:us-east-2:123456789012:table&#x2F;my-table&#x2F;stream&#x2F;2019-06-10T19:26:16.525</strong></p>
<p>You can see that in this command, I’m creating a mapping between a DynamoDB stream and my Lambda function. I’m also specifying that the event source mapping batches records together to send to my function. You can control how it batches records using the batch size and the batching window. When your batching size is met or the batching window reaches its maximum value, in this case 5 seconds, your lambda function will be invoked. </p>
<h3 id="Choosing-the-Right-Model"><a href="#Choosing-the-Right-Model" class="headerlink" title="Choosing the Right Model"></a>Choosing the Right Model</h3><p>So which one of these models is right for you? </p>
<p>Well, the easiest use case is if you’re processing messages from a stream or queue, the best choice for that is to create an event source mapping and use the polling type of invocation for Lambda. </p>
<p>The next case is if your application needs to wait for a response, then synchronous invocation is the best choice and can help you maintain order. </p>
<p>However, if you have a function that runs for long amounts of time, that does not need to wait for a response, then invoking asynchronously is the preferable option, as it offers automatic retries, a built-in queue, and a dead letter queue for failed events. </p>
<p>Now - keep in mind that if an AWS service invokes your function, the ability to select an invocation type is removed. The service gets this choice instead, and selects the invocation method for you. So all of these hard choices go away. </p>
<h1 id="Monitoring-your-Lambda-Function"><a href="#Monitoring-your-Lambda-Function" class="headerlink" title="Monitoring your Lambda Function"></a>Monitoring your Lambda Function</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="Monitoring-your-Lambda-Function-1"><a href="#Monitoring-your-Lambda-Function-1" class="headerlink" title="Monitoring your Lambda Function"></a>Monitoring your Lambda Function</h1><p>Fortunately, monitoring and troubleshooting with Lambda is more straightforward than with some of the other AWS compute Services. That’s because a lot of important monitoring and logging metrics are already configured with CloudWatch by default and built into the Lambda dashboard. </p>
<p>Let’s take a look at some of the default metrics that are provided to you through the service. I won’t get into every metric that Lambda supports but I will call out some of the main ones. There are three main categories of metrics:</p>
<ul>
<li>Invocation metrics</li>
<li>Performance metrics </li>
<li>And concurrency metrics</li>
</ul>
<h3 id="Invocation-Metrics"><a href="#Invocation-Metrics" class="headerlink" title="Invocation Metrics"></a>Invocation Metrics</h3><p>Invocation metrics are related to the outcome of the invocation. For example, there is a metric called invocations that tracks the number of times the function has been invoked. Similarly, in this category, you also have the errors metric which counts the number of failed invocations of the function. </p>
<h3 id="Performance-Metrics"><a href="#Performance-Metrics" class="headerlink" title="Performance Metrics"></a>Performance Metrics</h3><p>Performance metrics on the other hand, follow exactly what the name suggests, and provide performance details about an invocation. These metrics include duration, which measures how long the function runs in milliseconds from when it is invoked until it terminates. This category also supports the IteratorAge metric which is only used for stream-based invocations such as Amazon Kinesis. It measures in time how long Lambda took to receive a batch of records to the time of the last record written to the stream. This IteratorAge is measured in milliseconds.</p>
<h3 id="Concurrency-Metrics"><a href="#Concurrency-Metrics" class="headerlink" title="Concurrency Metrics"></a>Concurrency Metrics</h3><p>Then the last category is concurrency metrics. Before we get into the metrics, let’s understand what concurrency is. When you want to monitor metrics for concurrency, you can use concurrent executions metric, which is a combined metric for all of your Lambda functions that you have running within your AWS account in addition to functions with a custom concurrency limit. It calculates the total sum of concurrent executions at any point in time. </p>
<h3 id="Logs"><a href="#Logs" class="headerlink" title="Logs"></a>Logs</h3><p>In addition to these metrics, CloudWatch also gathers log data sent by Lambda to help you better troubleshoot and understand issues. For each function that you have running, CloudWatch will create a different log group. </p>
<p>The log group name will be prefixed with aws and lambda. When you look at Lambda logs, you’ll see details about your function execution. You can also add in custom logging statements to your function that will display in these logs. For example, if you add a print statement to a Python Lambda function, you will see the output of that print statement in these logs. This enables you to push data to CloudWatch logs automatically in addition to the managed messages that are sent by default. </p>
<p>If you want more information - check out the AWS documentation on monitoring with Lambda.</p>
<h1 id="Compute-Summary"><a href="#Compute-Summary" class="headerlink" title="Compute Summary"></a>Compute Summary</h1><p>So you’ve reached the end of the compute section and that was a big section to complete. So congratulations on getting through. So in that last section, we covered services and features such as Amazon EC2, auto scaling, elastic load balancing and serverless compute as well, which focused on AWS Lambda. So to help you with your studies for the exam I want to call out some key points that you should keep in the forefront of your mind. As my one core focus is to ensure that you are prepared and have the knowledge in need when you’re sitting in that exam chair. </p>
<p>So let’s run through when you might select certain services or make specific configuration changes to meet the requirements of different questions. So starting with EC2 two as this is most frequently mentioned compute service on the exam. We start off by looking at AMI’s, Amazon Machine Images. So these are used as the baseline template of your EC2 instances. And now the first element you need to select when creating your instance for the exam, you should be aware of the different options that they offer such as the operating system that you’ll be running in addition to any other additional software. So you will be expected to know what comes with the AMI and what doesn’t?</p>
<p>When it comes to understanding a service, it always helps to get some hands on experience with it and EC2 is no exception. And you can use our labs for this and it will really help you to establish familiarity with the different steps involved in creating an instance. And this will help you answer a lot of questions. You need to be aware of the different instance types that are available. And how the compute power and performance values fluctuate with instant size. As we discussed, some instances provide better performance depending on if your workloads are memory intensive or perhaps require that accelerated computing performance to help with data pattern matching. So having an insight into these, will help you answer any questions relating to EC2 workload efficiency. </p>
<p>Now more than likely, you’ll be asked at some point to determine the best instance purchase option to help you optimize the cost of your environment. I think I’ve at least one or two questions on this each and every time I’ve set the exam. So it’s imperative that you know the difference between on-demand, spot and reserved instances. Now depending on the scenario you will have to demonstrate your understanding of these different purchase options to help you determine under which circumstance you should use each of them. If the question talks about how your workload is predictable and will be required for perhaps one or three years and you need to optimize costs, then reserved instances should come to mind and would likely be the answer. If the question highlights how the workload can be interrupted. And again you’re looking to build a cost efficient solution. Then this would be a good use case of spot instances. So review the key differences of the purchase options and understand their specific use cases to help you optimize costs.</p>
<p>From security point of view, tenancy options of your instances can come into play. Now by default, our instances run on shared tenancy. Whereby we share the underlying host with other customers. However, you might receive questions explaining that you need to secure your infrastructure to maintain compliance and ensure that your EC2 instances do not share any underlying host with any other customer. So how could you do that? Well, the answer would fall under your tenancy options. Either dedicated instances or dedicated hosts would resolve this issue. Now with dedicated hosts, it provides additional control over the placement of your EC2 instances on those hosts. So ensure you have a good understanding of your options here.</p>
<p>Another common question scenario that comes up. Test your knowledge and understanding of how to automatically run commands on the first boot cycle of your instance. For example, you might need to perform operating system updates or install additional software from a repository when your instance first boots up. So how would you achieve this? Well, the answer lies in the user data section of your instance during its configuration. It allows you to enter commands to do exactly that. Also on this point, you can also use metadata of the instance to see the user data configuration for that instance. And this can be found by going to 169.254.169.254&#x2F;latest&#x2F;meta-data.</p>
<p>As you may or may not know, security will always be a part of every AWS certification. And the solutions architect is no different. So what are the types of security questions that may appear from an EC2 point of view? Well, key pairs could be one topic to come up and these are used to encrypt the credentials to your instances, allowing you to connect to them. Ensure you are familiar with how to connect to both Windows and Linux based instances. Now Windows uses RDP on port 3389 and Linux uses SSH, which is on port 22.</p>
<p>Let’s now take a look at auto scaling and how this might present itself in the exam. When questions on auto scaling come up, you’ll be expected to know the main function of the service and the benefits it brings such as the ability to automatically increase or decrease your EC2 resources to meet the demands of your applications. For example, you might be asked to implement an efficient way to enhance the performance of the application after users complained of poor response. Now this might be caused by a bottleneck in your EC2 resources not being able to handle and process the amount of traffic. Now by implementing auto scaling, you could automatically increase your ET2 fleet size. Thereby you would increase the amount of resources and remove the bottleneck. </p>
<p>Now you might also be assessed on your ability to optimize the cost of your EC2 fleet. Now, one way would be to remove unused resources. By implementing auto scaling, you can scale in your EC2 fleet by terminating unused capacity based on set thresholds. So auto scaling is all about optimizing performance and cost. So look out for this as an option whenever you receive a question covering this topic. Now you will likely see questions with auto scaling interlinking with elastic load balances as well. And they work very well together. Elastic load balances allow you to manage loads across your target groups. Whereas EC2 auto scaling allows you to elastically scale those target groups based upon the demand. So from an exam perspective, ensure you can differentiate between auto-scaling and ELBs. Also, ensure you are familiar with the different ELBs that exist as you’ll be assessed on when to use one ELB over another in a particular situation. </p>
<p>For example, you might be presented with a network scenario where you need to determine when your ELB should be placed. Should it be an internal or external ELB? And we’ll be using to serve encrypted traffic. In which case, what do you need to configure? Well, if using HTTPS you’ll need a service certificate perhaps issued by AWS Certificate Manager. Another scenario that I’ve come across, assesses your ability of understanding how your ELBs react to targets in your target group that are marked as unhealthy following a health check. Now, does the ELB restart the instance? Does it launch another instance or does it just ignore it? Well for the ELB, it just ignores it and continues to send request to healthy instances. It’s the job of auto-scaling to launch replacement instances not the ELB.</p>
<p>Okay, so the last area I want to cover is AWS Lambda. Now this service isn’t covered extensively on the exam but you certainly need to be aware of it and when it would be used. Now, the key is knowing that it is a serverless compute service designed to run in event-based environments to run application code without having to manage and provision your own EC2 instances. It’s really cost-effective as you only pay for compute power when Lambda functions are invoked. In addition to being charged based on the number of times your function runs, known as invocations. So you might be presented with a question where you have an application that allows you to share photos that are uploaded to S3. But every time a new object is created, you want to process code to create a thumbnail of that object. What service would you use to do this with the least administrative effort? Now, this is a perfect example of when Lambda would be used. As its code-triggered by an event. And in this case, when a new object is uploaded is that event. And there were no resources to provision, to administer as it’s serverless.</p>
<p>Okay, so that now brings me to the end of this summary. We’ve highlighted some of the key points that we’ve learned from the previous course and we’ve looked at how to approach a number of different questions that might come up that relate to compute. So hopefully you should feel ready and prepared to tackle any questions in this area. So let’s now move on to the next section.</p>
<h1 id="4ECS-Elastic-Container-Service"><a href="#4ECS-Elastic-Container-Service" class="headerlink" title="4ECS - Elastic Container Service"></a>4<strong>ECS - Elastic Container Service</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/">Course: Introduction to Docker</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/basics-of-using-containers-in-production/container-orchestration-1/">Basics of using Containers in Production</a></p>
<h1 id="5ECR-Elastic-Container-Registry"><a href="#5ECR-Elastic-Container-Registry" class="headerlink" title="5ECR - Elastic Container Registry"></a>5<strong>ECR - Elastic Container Registry</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Course: Overview of AWS Identity &amp; Access Managment (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Docker Push</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">Docker Pull</a></p>
<h1 id="6EKS-Elastic-Container-Service-for-Kubernetes"><a href="#6EKS-Elastic-Container-Service-for-Kubernetes" class="headerlink" title="6EKS - Elastic Container Service for Kubernetes"></a>6<strong>EKS - Elastic Container Service for Kubernetes</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-kubernetes/">Course: Introduction to Kubernetes</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">Course: Introduction to EKS</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Install Kubectl</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/aws-iam-authenticator">Linux IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator">MacOS IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/windows/amd64/aws-iam-authenticator.exe">Windows IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">Configuration map to joing the Worker Node to the EKS Cluster</a></p>
<h1 id="7AWS-Elastic-Beanstalk"><a href="#7AWS-Elastic-Beanstalk" class="headerlink" title="7AWS Elastic Beanstalk"></a>7<strong>AWS Elastic Beanstalk</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/deploy-php-application-using-elastic-beanstalk/">Lab: Deploy a PHP application using Elastic Beanstalk</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/run-controlled-deploy-aws-elastic-beanstalk/">Lab: Run a Controlled Deploy With AWS Elastic Beanstalk</a></p>
<h1 id="12SSL-Server-Certificates"><a href="#12SSL-Server-Certificates" class="headerlink" title="12SSL Server Certificates"></a>12<strong>SSL Server Certificates</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#acm_region">Regions supported by ACM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html">How to retrieve and list server certificates via ACM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">Additional information on ACM</a></p>
<h1 id="13Application-Load-Balancers"><a href="#13Application-Load-Balancers" class="headerlink" title="13Application Load Balancers"></a>13<strong>Application Load Balancers</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/">Course: OSI and TCPIP Networking Models</a></p>
<h1 id="15Classic-Load-Balancers"><a href="#15Classic-Load-Balancers" class="headerlink" title="15Classic Load Balancers"></a>15<strong>Classic Load Balancers</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/elasticloadbalancing/features/#compare">Table showing the differences between the Load Balancers</a></p>
<h1 id="17Summary"><a href="#17Summary" class="headerlink" title="17Summary"></a>17<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/working-application-load-balancer/">Lab: Working with the Application Load Balancer</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/working-amazon-ec2-auto-scaling-groups/">Lab: Working with Amazon EC2 Auto Scaling Groups and Network Load Balancer</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/creating-your-first-auto-scaling-group/">Lab: Creating your first Auto Scaling Group</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-SAA-C03-Learning-Path-Introduction-1/" rel="prev" title="AWS-Solution-Architect-Associate-SAA-C03-Learning-Path-Introduction-1">
      <i class="fa fa-chevron-left"></i> AWS-Solution-Architect-Associate-SAA-C03-Learning-Path-Introduction-1
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-3/" rel="next" title="AWS-Solution-Architect-Associate-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-3">
      AWS-Solution-Architect-Associate-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-3 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Compute-SAA-C03-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Compute (SAA-C03) Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-Compute"><span class="nav-number">2.</span> <span class="nav-text">What is Compute?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-EC2"><span class="nav-number">3.</span> <span class="nav-text">Amazon EC2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ECS-Elastic-Container-Service"><span class="nav-number">4.</span> <span class="nav-text">ECS - Elastic Container Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ECR-Elastic-Container-Registry"><span class="nav-number">5.</span> <span class="nav-text">ECR - Elastic Container Registry</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Resources-referenced-within-this-lecture"><span class="nav-number">5.0.1.</span> <span class="nav-text">Resources referenced within this lecture:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transcript"><span class="nav-number">5.0.2.</span> <span class="nav-text">Transcript</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EKS-Elastic-Container-Service-for-Kubernetes"><span class="nav-number">6.</span> <span class="nav-text">EKS - Elastic Container Service for Kubernetes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Elastic-Beanstalk"><span class="nav-number">7.</span> <span class="nav-text">AWS Elastic Beanstalk</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Batch"><span class="nav-number">8.</span> <span class="nav-text">AWS Batch</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EC2-Auto-Scaling"><span class="nav-number">9.</span> <span class="nav-text">EC2 Auto Scaling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Components-of-EC2-Auto-Scaling"><span class="nav-number">10.</span> <span class="nav-text">Components of EC2 Auto Scaling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-an-Elastic-Load-Balancer-ELB"><span class="nav-number">11.</span> <span class="nav-text">What is an Elastic Load Balancer (ELB)?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SSL-Server-Certificates"><span class="nav-number">12.</span> <span class="nav-text">SSL Server Certificates</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Application-Load-Balancers"><span class="nav-number">13.</span> <span class="nav-text">Application Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Network-Load-Balancers"><span class="nav-number">14.</span> <span class="nav-text">Network Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Classic-Load-Balancers"><span class="nav-number">15.</span> <span class="nav-text">Classic Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Using-ELB-and-Auto-Scaling-Together"><span class="nav-number">16.</span> <span class="nav-text">Using ELB and Auto Scaling Together</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary"><span class="nav-number">17.</span> <span class="nav-text">Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-Gateway-Load-Balancer"><span class="nav-number">18.</span> <span class="nav-text">Introduction to Gateway Load Balancer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Anatomy-of-the-AWS-Gateway-Load-Balancer"><span class="nav-number">19.</span> <span class="nav-text">Anatomy of the AWS Gateway Load Balancer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Gateway-Load-Balancer-Architecture"><span class="nav-number">20.</span> <span class="nav-text">Gateway Load Balancer Architecture</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Traffic-Flow-Steps-for-Gateway-Load-Balancers"><span class="nav-number">21.</span> <span class="nav-text">Traffic Flow Steps for Gateway Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Outposts"><span class="nav-number">22.</span> <span class="nav-text">AWS Outposts</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-VMware-Cloud-on-AWS"><span class="nav-number">23.</span> <span class="nav-text">What is VMware Cloud on AWS?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#An-Overview-of-AWS-Lambda"><span class="nav-number">24.</span> <span class="nav-text">An Overview of AWS Lambda</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#An-Overview-of-AWS-Lambda-1"><span class="nav-number">25.</span> <span class="nav-text">An Overview of AWS Lambda</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Function"><span class="nav-number">25.0.1.</span> <span class="nav-text">The Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Input"><span class="nav-number">25.0.2.</span> <span class="nav-text">The Input</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Output"><span class="nav-number">25.0.3.</span> <span class="nav-text">The Output</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Costs"><span class="nav-number">25.0.4.</span> <span class="nav-text">Costs</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Invoking-a-Lambda-Function"><span class="nav-number">26.</span> <span class="nav-text">Invoking a Lambda Function</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Invoking-a-Lambda-Function-1"><span class="nav-number">27.</span> <span class="nav-text">Invoking a Lambda Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Synchronous-Push-Based-Model"><span class="nav-number">27.0.1.</span> <span class="nav-text">Synchronous (Push-Based) Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Asynchronous-Event-based-Model"><span class="nav-number">27.0.2.</span> <span class="nav-text">Asynchronous (Event-based) Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stream-Poll-based-Model"><span class="nav-number">27.0.3.</span> <span class="nav-text">Stream (Poll-based) Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Choosing-the-Right-Model"><span class="nav-number">27.0.4.</span> <span class="nav-text">Choosing the Right Model</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monitoring-your-Lambda-Function"><span class="nav-number">28.</span> <span class="nav-text">Monitoring your Lambda Function</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monitoring-your-Lambda-Function-1"><span class="nav-number">29.</span> <span class="nav-text">Monitoring your Lambda Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Invocation-Metrics"><span class="nav-number">29.0.1.</span> <span class="nav-text">Invocation Metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Performance-Metrics"><span class="nav-number">29.0.2.</span> <span class="nav-text">Performance Metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Concurrency-Metrics"><span class="nav-number">29.0.3.</span> <span class="nav-text">Concurrency Metrics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logs"><span class="nav-number">29.0.4.</span> <span class="nav-text">Logs</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Compute-Summary"><span class="nav-number">30.</span> <span class="nav-text">Compute Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4ECS-Elastic-Container-Service"><span class="nav-number">31.</span> <span class="nav-text">4ECS - Elastic Container Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5ECR-Elastic-Container-Registry"><span class="nav-number">32.</span> <span class="nav-text">5ECR - Elastic Container Registry</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6EKS-Elastic-Container-Service-for-Kubernetes"><span class="nav-number">33.</span> <span class="nav-text">6EKS - Elastic Container Service for Kubernetes</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7AWS-Elastic-Beanstalk"><span class="nav-number">34.</span> <span class="nav-text">7AWS Elastic Beanstalk</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#12SSL-Server-Certificates"><span class="nav-number">35.</span> <span class="nav-text">12SSL Server Certificates</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#13Application-Load-Balancers"><span class="nav-number">36.</span> <span class="nav-text">13Application Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#15Classic-Load-Balancers"><span class="nav-number">37.</span> <span class="nav-text">15Classic Load Balancers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#17Summary"><span class="nav-number">38.</span> <span class="nav-text">17Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
