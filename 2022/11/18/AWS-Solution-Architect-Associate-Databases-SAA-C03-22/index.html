<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Databases (SAA-C03) IntroductionHello, and welcome to this course on databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate cer">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Solution-Architect-Associate-Databases-SAA-C03-22">
<meta property="og:url" content="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="Databases (SAA-C03) IntroductionHello, and welcome to this course on databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate cer">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T02:13:31.000Z">
<meta property="article:modified_time" content="2022-11-27T23:59:24.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Solution-Architect-Associate-Databases-SAA-C03-22 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Solution-Architect-Associate-Databases-SAA-C03-22
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:31" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:31-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 19:59:24" itemprop="dateModified" datetime="2022-11-27T19:59:24-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Databases-SAA-C03-Introduction"><a href="#Databases-SAA-C03-Introduction" class="headerlink" title="Databases (SAA-C03) Introduction"></a>Databases (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various database services currently available in AWS that may be covered on the exam.</p>
<p>Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#112;&#112;&#x6f;&#x72;&#x74;&#64;&#x63;&#x6c;&#111;&#x75;&#100;&#x61;&#99;&#97;&#x64;&#x65;&#109;&#x79;&#46;&#x63;&#x6f;&#x6d;">&#x73;&#x75;&#112;&#112;&#x6f;&#x72;&#x74;&#64;&#x63;&#x6c;&#111;&#x75;&#100;&#x61;&#99;&#97;&#x64;&#x65;&#109;&#x79;&#46;&#x63;&#x6f;&#x6d;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various database services in AWS in preparation for the exam.</p>
<p>The objective of this course is to provide an introduction to database services in AWS for solution architects, including:</p>
<ul>
<li>Managed relational databases using the Amazon Relational Database Service, or RDS, along with purchasing options and pricing; and</li>
<li>Managed NoSQL databases including Amazon DynamoDB.</li>
</ul>
<p>You’ll also learn about several other data structure stores and database services offered by AWS and see demonstrations of each of the following in action:</p>
<ul>
<li>Amazon ElastiCache;</li>
<li>Amazon Neptune;</li>
<li>Amazon Redshift;</li>
<li>Amazon DocumentDB;</li>
<li>Amazon Keyspaces; and</li>
<li>The Amazon Quantum Ledger Database, or QLDB.</li>
</ul>
<p>Finally, we’ll wrap up the course with a discussion of data lakes on AWS, including defining what a data lake is, as well as explaining the differences between data lakes and data warehouses.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#x73;&#117;&#112;&#x70;&#111;&#x72;&#x74;&#64;&#x63;&#108;&#111;&#117;&#x64;&#97;&#x63;&#x61;&#x64;&#101;&#x6d;&#121;&#x2e;&#x63;&#x6f;&#109;">&#x73;&#117;&#112;&#x70;&#111;&#x72;&#x74;&#64;&#x63;&#108;&#111;&#117;&#x64;&#97;&#x63;&#x61;&#x64;&#101;&#x6d;&#121;&#x2e;&#x63;&#x6f;&#109;</a>. Thank you!</p>
<h1 id="Amazon-Relational-Database-Service"><a href="#Amazon-Relational-Database-Service" class="headerlink" title="Amazon Relational Database Service"></a>Amazon Relational Database Service</h1><p>Hello and welcome to this lecture, where I shall be discussing the first of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-part-one-1064/course-introduction/">AWS database services</a> that I will be covering in this course, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-part-one-1064/demo-creating-an-amazon-rds-database/">Amazon Relational Database Service</a>, commonly known as RDS. I will look at a number of different common features of the service to give you a general idea of how it’s configured. So as the name suggests, this is a relational database service that provides a simple way to provision, create, and scale a relational database within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. It’s a managed service, which takes many of the mundane administrative operations out of your hands, and it’s instead managed by AWS, such as backups and the patching of both the underlying operating system in addition to the database engine software that you select.</p>
<p>Amazon RDS allows you to select from a range of different database engines. These currently include MySQL, and this is considered the number one open source relational database management system. MariaDB. This is the community-developed fork of MySQL. PostgreSQL. This database engine comes in a close second behind MySQL as the preferred open source database. Amazon Aurora. Amazon Aurora is AWS’s own fork of MySQL, which provides ultrafast processing and availability, as it has its own cloud-native database engine. Oracle. The Oracle database is a common platform in corporate environments. And SQL Server. This is a Microsoft database with a number of different licensing options.</p>
<p>In addition to so many different database engines, you also have a wide choice when it comes to selecting which compute instance you’d like to run your database on. The varying different options offer different performance and allowed to architect your environment based on your expected load. When you create your RDS database, you must select an instance to support your database from a processing and memory perspective, as shown here in this screenshot, using the MySQL database engine. Currently, these are the different instance types available to you for each of the database engines, which are categorized between general purpose and memory-optimized.</p>
<p>For a breakdown of the performance of each of these instance types, please refer to the following <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">AWS documentation</a>. For each of these instance types, you also have various instance sizes, each equating to a different performance level from a vCPU and memory perspective. For example, when looking at the T3 instance, we have the following sizes available.</p>
<p>You can deploy your RDS instance in a single availability zone. However, if high availability and resiliency is of importance when it comes to your database, then you might want to consider a feature known as Multi AZ, which stands for multi availability zones. When Multi AZ is configured, a secondary RDS instance is deployed within a different availability zone within the same region as the primary instance. The primary purpose of the second instance is to provide a failover option for your primary RDS instance. The replication of data between the primary RDS database and the secondary replica instance happens synchronously.</p>
<p>Let’s look at how Multi AZ would work in a production environment. If you have configured Multi AZ and an incident occurs which causes an outage to the primary RDS instance, then the RDS failover process takes over automatically. This process is managed by AWS, and it’s not something that you need to manually perform or trigger. RDS will update the DNS record to point to the secondary instance. This process can typically take between 60 and 120 seconds. The length of time is very dependent on the size of the database, its transactions, and the activity of the database at the time of failover. This automatic changeover enables you to continue using the database without the need of an engineer making any changes to your environment. The failover process will happen in the following scenarios. If patching maintenance has been performed in the primary instance, if the instance of the primary database has a host failure, if the availability zone of the primary database fails, if the primary instance was rebooted with failover, and if the primary database instance class on the primary database is modified.</p>
<p>As you can see, activating Multi AZ is an effective measure and precaution to implement to ensure you have resiliency built in should an outage occur. For detailed information on RDS Multi AZ, please refer to our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">here</a>.</p>
<p>Over time, your workloads on your database will fluctuate. And so how can you optimize your RDS database to ensure it is capable of meeting the demands of your load, both from a storage and compute perspective? When it comes to scaling your storage, you can use a feature called storage autoscaling. MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server all use Elastic Block Store, EBS volumes, for both data and log storage. However, Amazon Aurora uses a shared cluster storage architecture and does not use EBS. The database engines that use EBS support general purpose SSD storage, provisioned IOPS SSD storage, and magnetic storage.</p>
<p>The general purpose SSD storage is a good option for a broad range of use cases which provides single-digit millisecond latencies and offers a cost-effective storage solution. The minimum SSD storage volume for your primary dataset is 20 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle. However, the maximum for SQL Server is 16 tebibytes. Provisioned IOPS SSD. Now, this option is great for when you have workloads that operate at a very high I&#x2F;O. You can provision a minimum of 8,000 IOPS and a maximum of 80,000 for MySQL, PostgreSQL, MariaDB, and Oracle, but the maximum for SQL Server is 40,000.</p>
<p>In addition to being able to provision the IOPS needed for your workload, the minimum storage for your primary dataset is a 100 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle, and 16 tebibytes for SQL Server. Magnetic storage is simply supported to provide backwards compatibility. And so instead, AWS recommends that you select general purpose. The following screenshot shows the configuration screen when setting your storage requirements during the creation of a MySQL RDS database. </p>
<p>n this example, general purpose has been selected as the storage with a minimum of 100 gibibytes of primary storage. Under the storage autoscaling section, I’ve enabled the feature with the checkbox and set a maximum storage threshold of 1000 gibibytes. This means that one storage will start at 100 gibibytes when the database is created and the storage will automatically scale with demand up to a maximum of 1000 gibibytes without you having to intervene in any way. The maximum storage threshold can be set at 65,536 gibibytes. Aurora doesn’t use EBS and instead uses a shared cluster storage architecture which is managed by the service itself.</p>
<p>When configuring your Aurora database in the console, the option to configure and select storage options like we saw previously does not exist. Your storage will scale automatically as your database grows. To scale your compute size, which is effectively your instance, is easy to do in RDS both vertically and horizontally. Vertical scaling will enhance the performance of your database instance. For example, scaling up from an m4.large to an m4.2xlarge. Horizontal scaling will see an increase in the quantity of your current instance. For example, moving from a single m4.large to three m4.large instances in your environment through the means of read replicas.</p>
<p>At any point you can scale your RDS database vertically, changing the size of your instance. When doing so, you can select to perform the change immediately or wait for a scheduled maintenance window. For horizontal scaling, read replicas can be used by application and other services to save read only access to your database data via a separate instance. So, for example, let’s assume we have a primary RDS instance which serves both read and write traffic. Due to the size of the instance and the amount of read-intensive traffic being directed to the database for queries, the performance of the instance has taken a hit. So to help resolve this, you can create a read replica. </p>
<p>A snapshot will be taken of your database, and if you’re using Multi AZ, then this snapshot will be taken of your secondary database instance to ensure that there are no performance impacts during this process. Once the snapshot is complete, a read replica instance is created from this data. The read replica then maintains a secure asynchronous link between itself and the primary database. At this point, read only traffic can be directed to the read replica to serve queries. Before implementing read replicas, please check with the latest AWS documentation to identify database engine read replica compatibility.</p>
<p>As I mentioned previously, many of the administrative tasks for RDS are taken care of by AWS. For example, patching and automated backups. As Amazon RDS is a managed service, and from a shared responsibility model is considered a container service where you have no access to the underlying operating system on which your database runs on. As you can see, both platform and application management and operating systems falls under the realm of AWS responsibilities. As a result, AWS is responsible for both the patching of the operating system and any patching for the database engine themselves. More information on the AWS shared responsibility model can be found in my existing blog post found <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">here</a>.</p>
<p>From a backup perspective, by default, Amazon RDS provides an automatic feature seen here. This is enabled on all new RDS databases, which backs up your RDS instance to Amazon S3. You are able to configure the level of retention in days from zero to 35 and implement a level of encryption using the key management service, or KMS. More information on KMS can be found in our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>You can also perform manual backups anytime you need to, which are known as snapshots. However, these snapshots are not bound by the retention period set in the automatic backup configuration and are only deleted through a manual process. When using a MySQL-compatible Aurora database, you can also use a feature called backtrack, and this allows you to go back in time on the database to recover from an error or incident without having to perform a restore or create another database cluster.</p>
<p>As you can see from the configuration page during the Aurora database creation process, it is enabled via a checkbox and allows you to enter a number of hours of how far you would like to backtrack to with a maximum of 72 hours. In this example, I’ve entered 12 hours, and so Aurora will retain log data of all changes for 12 hours as specified. We’ve now covered the fundamentals of Amazon RDS and some of the key features to be aware of. If you’d like to get some hands-on experience of using Amazon RDS, feel free to check out the following labs.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="DEMO-Creating-an-Amazon-RDS-Database"><a href="#DEMO-Creating-an-Amazon-RDS-Database" class="headerlink" title="DEMO: Creating an Amazon RDS Database"></a>DEMO: Creating an Amazon RDS Database</h1><p>Hello, and welcome to this lecture, which will be a demonstration on how to create an RDS database. So let’s get straight into it. So as you can see, I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console. And the first thing I need to do is to go to RDS. Now you can find RDS under the Database category, and you can see that it’s the first database option. So if you select the RDS service, and this is the dashboard for Amazon RDS. As you can see, I don’t have any database instances running. So let’s go ahead and create our first database.</p>
<p>So under the Create database section here, we can either restore an existing database from Amazon S3, from a backup, or we can create a new database. For this demonstration, I’m going to create a new database. So let’s click that option. Now, firstly, we need to choose a database creation method. We can either do a standard create or an easy create. Now the standard create allows us to set more-configurable options. So for this demonstration, that’s the option I’m going to select.</p>
<p>Then we have our database engine types. As I explained previously, we have Amazon Aurora, mySQL, MariaDB, Postgres, Oracle and Microsoft SQL Server. I’m just going to select mySQL for this demonstration. So scrolling down again, we can then select our version of mySQL, whichever version we’d like. I’ll just leave it as the default option. And then we have something called Templates.</p>
<p>Now, depending on what template we select here will predefine a list of other configurable components. So here that’s highlighted is Production and this uses defaults for high availability and fast and consistent performance. The Dev&#x2F;Test template is intended for development use outside of a production environment. And the Free Tier is simply to allow you to get hands-on experience with RDS and doesn’t really use many of the features. But I want to show you the full feature set. So I’m going to select Production.</p>
<p>Now, if we go down to Settings. Here, we can enter a database instance identifier. So this is the name of the database instance, not the actual name of a database table. So I’m happy to call it database one, just leave that as default. Now we have our credentials. Now we have a master user name to connect to the database instance, so we can have admin. We can either auto-generate a password or we can select our own. So I’ll just go ahead and enter my own one.</p>
<p>Next, we have the option to select the database instance size. So we have our Standard classes or Memory Optimized or even Burstable classes. So I’m going to select the Standard class and using this dropdown, we can select the size of the instance that we want. And as you can see, there’s a different number of VCPUs and RAM. And so I’m just going to select the smallest instance size.</p>
<p>Now, if we go down to Storage, we can select our storage type. So we have General Purpose or Provisioned IOPS. If we look at the Provisioned IOPS, we can define the allocated storage and then also the number of IOPS as well, which is the input and output operations per second. For this demonstration, I’m just gonna leave it as General Purpose. I’ll just accept the storage defaults there of 20. Here we have Storage autoscaling. If we want to enable that or not, it’s just a simple tick box. RDS will automatically scale up to whatever value we put in here. So for example, 100 gig. That will give us the flexibility of starting at 20 and scaling all the way up to 100 automatically. </p>
<p>Now, if we go down to Availability and durability. Here, we have a Multi-AZ deployment. So it’s enabling this, will create another standby instance in a different availability zone to create high availability and data redundancy. For this demonstration, I’m just gonna leave it as a single AZ deployment. So I’m gonna select Do not create a standby instance. If we go down to Connectivity, we can select the VPC that we’d like this RDS instance to reside in. And you can see here, after a database is created, you can’t change the VPC selection. If you expand this option here for additional connectivity configuration, we can see a few additional options.</p>
<p>So we can select a database subnet group. And this simply defines which subnets and IP ranges the database instance can use in the VPC. Have an option here, if the database should be publicly accessible or not. If you select yes, then it will be issued a public IP address and devices and instances outside of your VPC will be able to try and connect to your database, if the VPC security groups allow it. For this demonstration, I’m just going to keep it a private RDS database, so it won’t assign any kind of public IP address. And only instances inside the VPC will be able to connect. Here, we can choose our security group, which will essentially define which resources can talk to our RDS instance.</p>
<p>Now, if we select an existing, then we can use this dropdown box here to select which security group that we’d like to use. I’m just going to select the default. I haven’t set any kind of security groups up for this as this is just a demonstration, but that’s where you would apply your security groups for your RDS instance. And as you can see, it’s added it in there. If you’d like to deploy your RDS instance in a particular availability zone, then you can select one. If you have no preference, simply select no preference. And also what port the database will be using.</p>
<p>If you go down to Database authentication. We have two options here for mySQL. Password authentication. Now this will allow anyone to connect to the instance just using the database passwords. If you want it more secure, then you can use the database password in addition to verifying that the user has permissions to access the RDS database, through permissions that were assigned directly to the user or group or role. So that just offers an additional level of security. If we go down further to Additional configuration, we can configure additional options.</p>
<p>So here we have our database options. You can enter an initial database name that will run on your database instance. Let’s just say my database. You can select a parameter group. Parameter groups is essentially a grouping of configurable parameters that operate at the database engine level. You’re able to create different parameter groups that contain different settings for the same database engine, depending on your use case and how you’d like these parameters to be configured. Now the parameter group itself sits outside of the database, and this means that the same parameter group can be applied to multiple databases. So if you update the values within the parameter group, then this will update all the databases that use that same parameter group. Depending on which database engine you select, you are able to select an option group. And these option groups allow you to configure additional features to help you manage and secure your databases. Again, like parameter groups, they sit outside of the database itself.</p>
<p>Here we have our Backup section, so we can enable our automatic backups. If you don’t want this, you can simply un-tick it, but it’s pretty useful, so I tend to leave that enabled. And here we have our backup retention period. And you can select the number of days, up to 35 days. Just leave that at seven. We have a backup window. We can select a window, select in the time and the duration. So if you have a particular time that you’d like to run your backups, you can simply add in the hours and minutes, and also how long it should run for. If you don’t have a specific window, you can simply select no preference. If you have any tags for your database, you can copy that to your backup snapshots as well.</p>
<p>When it comes to encryption, you can either encrypt your database. The default is to have your database encrypted, and then you can select your key here. Now, this is the default AWS managed key for RDS, which is used by KMS, the key management service. or you can select your own CMK, your own customer master key, if you have a different one yourself. I’m just gonna leave it as the default AWS RDS managed key.</p>
<p>Down here, we have performance insights. Performance insights allows you to implement a level of performance tuning and monitoring, which enables you to see and review the load on your database, and if any actions should be taken. Here we can make some additional monitoring changes. We can enable enhanced monitoring, and we can set the granularity of this monitoring from anything from 60 seconds to one second. I’ll leave that as a default of 60 seconds for enhanced monitoring. And I’m just going to leave RDS to create the default role for that enhanced monitoring.</p>
<p>We can export our logs to Amazon CloudWatch Logs. Either the error, the general or the slow query log or all of them, or any combination. So if you want to export any of your logs to CloudWatch Logs for additional monitoring and queries then you can do so.</p>
<p>Then we have maintenance. We can enable or disable auto minor version upgrade. And here we can see that by enabling auto minor version upgrade, it will automatically upgrade to new minor versions as they are released. And the automatic upgrades occur during the maintenance window that we’ve scheduled for the database.</p>
<p>Now, speaking of maintenance window, we can select one here. So we can select a window. We can say on a particular day, that’s good for us, Saturday at four o’clock in the morning for two hours, for example, that could be our maintenance window. So if there’s any auto minor version upgrades to take place, then these will be scheduled during our maintenance window. And then finally you have deletion protection. And this simply prevents a database from being deleted accidentally.</p>
<p>Now at the very bottom, it has your estimated monthly costs. So we can see the cost of the database instance and also for the storage. Once you’re happy with all your options, simply click Create database. And now we can see that here’s our database instance, and we can see the status is Creating. And we have a message up here saying that the database might take a few minutes to launch. Okay, we now have a message that says the database has successfully been created. And it’s as simple as that.</p>
<h1 id="RDS-vs-EC2"><a href="#RDS-vs-EC2" class="headerlink" title="RDS vs. EC2"></a>RDS vs. EC2</h1><h3 id="Instructor-Danny-Jessee"><a href="#Instructor-Danny-Jessee" class="headerlink" title="Instructor: Danny Jessee"></a>Instructor: Danny Jessee</h3><p>Hello, and welcome to this lecture, where I will discuss how to choose between using the Amazon Relational Database Service, or RDS, or Elastic Compute Cloud, or EC2 instances for your relational database deployments in the AWS Cloud. Both RDS and EC2 instances allow you to host databases securely within your own Virtual Private Cloud, or VPC. Both give you the ability to scale your databases to meet varying levels of storage, throughput, and performance demand, and both can be configured to operate within the AWS free tier. But that’s where the similarities end. In order to decide which option is best for your database, it helps to have a better understanding of the various benefits each approach has to offer. So let’s begin by discussing the advantages and disadvantages of both approaches, beginning with RDS.</p>
<h2 id="Amazon-RDS"><a href="#Amazon-RDS" class="headerlink" title="Amazon RDS"></a>Amazon RDS</h2><p>Amazon RDS is an AWS-managed service that allows you to automatically install and provision relational databases in the AWS Cloud using popular open-source and commercial database engines including MySQL, MariaDB, PostgreSQL, and even Oracle and Microsoft SQL Server. RDS also offers Amazon Aurora, which is a MySQL and PostgreSQL-compatible database with up to 5 times the throughput of MySQL, up to 64 TB of auto-scaling SSD storage, and 6-way replication of your data across 3 availability zones, all at a fraction of the cost of a traditional database deployment.</p>
<p>Now what makes RDS appealing for so many use cases is just how simple it is to quickly spin up a new database without having to install or configure any operating systems or database software. And since AWS is fully responsible for the management of your RDS instance, you never have to worry about any of the administrative tasks typically associated with managing a database like configuring backups, security, or high availability. You also never need to worry about patching or hardening the underlying operating system of the servers that host your databases. This makes RDS ideally suited for smaller development shops or any organization that can’t afford the expense of a dedicated database administrator, or DBA function. And although you don’t have access to the server that hosts your RDS database, you can still use most standard database administration tools to connect to your database and perform any necessary actions.</p>
<p>So let’s quickly run through some of the other advantages of using RDS for your database deployments in the AWS Cloud. When you choose RDS, AWS will automatically provision your database instance along with its associated storage, configure automatic backups, and even automatic minor version upgrades if you choose.</p>
<p>RDS also takes care of encryption for your databases, both in transit and at rest. And when we talk about encryption at rest, this extends beyond just your primary instance storage and also includes your database backups and snapshots as well.</p>
<p>You even have the option to choose between preconfigured templates for production or dev&#x2F;test deployments. So again, when you don’t have the time or resources for a dedicated DBA, or if you just want to get up and running as quickly as possible with your new database, it’s really great to let RDS handle all of these things for you.</p>
<h2 id="High-availability-and-scalability"><a href="#High-availability-and-scalability" class="headerlink" title="High availability and scalability"></a>High availability and scalability</h2><p>And beyond all of this, many of the supported database engines in RDS will also allow you to configure a highly available multi-AZ deployment with just a single click. And with a multi-AZ deployment, you get a 99.95% uptime SLA. So in a multi-AZ deployment, RDS will automatically create a standby, synchronous replica instance in a different availability zone than your primary instance. RDS will then automatically fail over to that standby instance in the event an infrastructure failure is ever detected on your primary instance. And these failures could include anything from an outage within an availability zone, to a loss of network connectivity, or even a compute or storage failure. And best of all, when a failure does happen, AWS will automatically update the URL associated with your database to point to your standby instance during a failover, then back to your primary instance once it’s up and running again. And all of this happens without requiring any manual intervention from an administrator whatsoever.</p>
<p>Now when it comes to scalability, RDS makes it easy to scale out your database deployments by adding asynchronous read replica instances, which can help offload read demand from your primary instance. And you can imagine how much time and effort would be involved for a DBA to have to spin up additional EC2 instances and then configure a solution like database mirroring or a failover cluster, so RDS definitely offers some advantages when it comes to configuring high availability and scalability. To learn more about when to use Multi-AZ deployments and read replicas with RDS, please check out this course:</p>
<p><em>When to use RDS Multi-AZ &amp; Read Replicas</em></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/rds-multi-az/"><em>https://cloudacademy.com/course/using-rds-multi-az-read-replicas/rds-multi-az/</em></a> </p>
<p>And to create your first RDS database, check out this hands-on lab:</p>
<p><em>Create Your First Amazon RDS Database</em></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/"><em>https://cloudacademy.com/lab/create-your-first-amazon-rds-database/</em></a> </p>
<h2 id="Amazon-EC2"><a href="#Amazon-EC2" class="headerlink" title="Amazon EC2"></a>Amazon EC2</h2><p>So as you’ve seen, RDS offers a lot of inherent advantages. It’s extremely easy to set up, it’s cost effective, and it offloads the responsibility for your more time and labor-intensive database administration tasks to AWS, allowing you to focus on your own business requirements. But there are still scenarios where it makes more sense to leverage EC2 instances to host your relational databases instead of RDS. So now, let’s talk about some of the reasons why you may want to choose EC2 instances over RDS.</p>
<p>Amazon EC2 allows you to provision virtual servers on demand that can run a variety of different operating system types and versions, whereas RDS abstracts away the underlying operating system of the database instance. So if your application requires a specific OS type or version for your database, or if you need to run a database that isn’t currently supported by RDS such as IBM DB2 or SAP HANA, you’ll need to provision your own EC2 instances. You’ll also need to use EC2 instances if your application requires you to configure any operating system-specific settings, or install any additional software directly on your database server.</p>
<p>It might also make sense for you to leverage EC2 instances if your organization already has full-time DBAs on staff who need to have access to advanced features and configuration settings within the database. For instance, RDS doesn’t support some of the more advanced features in Microsoft SQL Server such as persistent memory devices or its Resource Governor. If your application requires any of these features, you’ll have no choice but to provision SQL Server on an EC2 instance.</p>
<p>So as you’ve seen, most of the reasons you’d want to choose EC2 instances over RDS involve needing to have a greater level of control over your database or the underlying server operating system than what RDS provides. And this could involve anything from controlling exactly when or how you apply patches and perform maintenance on your database, to which ports and protocols you want your database to use. Perhaps you need to configure multiple versions or multiple instances of a particular database engine on a single server. You may even need to set up an advanced RAID and striping configuration for your database’s storage using EBS volumes. While all of these would be non-starters with RDS, they’re entirely possible when using EC2 instances.</p>
<p>Now just because you choose to use EC2 instances instead of RDS doesn’t mean you can’t still configure some of the native functionality that RDS provides such as high availability, automatic failover, and scheduled backups for your databases. Just know that setting all of this up with EC2 instances will require significantly more effort than if you just used RDS. But depending on your specific requirements, this may be a worthwhile tradeoff for you.</p>
<p>So that covers the reasons you might want to use EC2 instances for your database deployments instead of RDS. To create your first EC2 instance, please check out these hands-on labs:</p>
<p><em>Create Your First Amazon EC2 Instance (Linux)</em></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance/"><em>https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance/</em></a></p>
<p><em>Create Your First Amazon EC2 Instance (Windows)</em></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/"><em>https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/</em></a></p>
<h2 id="Final-Thoughts"><a href="#Final-Thoughts" class="headerlink" title="Final Thoughts"></a>Final Thoughts</h2><p>So to wrap things up here, you’re probably wondering: with all the benefits that RDS has to offer, surely all of this added functionality comes at a much higher cost when compared to just running your database on EC2 instances. Surprisingly, though, costs for database deployments using RDS and EC2 may be more comparable than you might expect. While RDS deployments are typically more expensive than comparable EC2 instance deployments, the difference in cost will vary depending on the specific instance and storage types you choose for your deployments. It’s also important to remember that the additional costs associated with those manual database administration tasks will become your responsibility if you choose to use EC2 instances instead of RDS.</p>
<p>So if you’re interested in getting a better sense of what these various database deployment options might cost for your applications in the AWS Cloud, you can input your specific numbers into the AWS Pricing Calculator at <a target="_blank" rel="noopener" href="https://calculator.aws/">calculator.aws</a>, where you can configure pricing estimates for different types of deployments by searching for things like RDS, Oracle, or EC2.</p>
<p>And just a couple of final thoughts: if you already have a dedicated DBA on staff, you’ll probably want to use EC2 instances for the cost savings you’ll get compared to RDS. You’ll also want to use EC2 instances if you need to provision a database type or version that isn’t supported by RDS. But in many other cases, RDS will probably be your best option. By reducing the management overhead associated with running your relational databases in the AWS Cloud, RDS allows you to worry less about tedious, labor-intensive administrative tasks, enabling you to focus on the things that deliver value to your business instead.</p>
<h1 id="RDS-Instance-Purchasing-Options"><a href="#RDS-Instance-Purchasing-Options" class="headerlink" title="RDS Instance Purchasing Options"></a>RDS Instance Purchasing Options</h1><p>Hello and welcome to this lecture where I want to cover the number of different purchasing options that are available and the associated costs. </p>
<p>You can purchase database instances through a variety of different payment plans. These have been designed to help you save costs by selecting the most appropriate option for your deployment. The different payment options within <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">RDS</a> include on-demand instances, on-demand instances (BYOL), reserved instances, reserved instances (BYOL), and serverless.</p>
<p>Currently, only the Oracle database engine uses the BYOL purchase options, all other DB engines only use on-demand instances and reserved instances, with the added exception of Aurora also using serverless as an additional purchasing option. </p>
<p>So to make things a little clearer you can see in this table which DB engines support each type of purchasing option.</p>
<p>It’s good to be aware of these different options as having an understanding of these can help you save a considerable amount of money depending on your use case. Let me run through each option to help explain. Starting with on-demand instances.</p>
<p>On-demand instances can be launched at any time and be provisioned and available to you within minutes. You can use this instance for a shorter time or for as long as you need before terminating the instance.</p>
<p>When you create an RDS database you are required to select the appropriate instance to support your DB from a processing and memory perspective, as shown here in this screenshot from the MySQL DB engine.</p>
<p>Depending on the options you select here will depend on what the instance price will be. The larger and more powerful the instance is, the more it will cost you. The cost of this compute capacity is calculated hourly with no minimum usage term. Any partial DB instance hours used are cost on per second increments. For any database changes that alter the running costs, such as modifying the instance or creating the DB instance, then a minimum of a 10-minute charge will be applied even if the DB is terminated or altered again before 10 minutes has passed.</p>
<p>At the time of writing this course, the following instance types are available for use.</p>
<p>For a breakdown of the performance of each of these instance types, please refer to the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation: <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">https://aws.amazon.com/ec2/instance-types/</a>. </p>
<p>As an example, the following table shows the pricing for each of the available instances with the MySQL DB engine within the London region, based on the deployment of an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/database-storage-and-i-o-pricing/">RDS database</a> within a single availability zone.</p>
<p>As you may or may not know, you can also deploy your RDS database within a single availability zone, or use a Multi-AZ deployment to add a layer of high availability. This will, of course, incur additional costs for your standby instance to run in another availability zone should your primary instance fail. </p>
<p>The following table shows the total costs for your multi-az deployment, which is typically double the cost of single deployment highlighted previously, again this is based upon the London region using the MySQL DB engine.</p>
<p>As well as both single-AZ and multi-AZ, some of the DB engines offer alternative options.</p>
<p>Amazon Aurora: When selecting Amazon Aurora as your DB engine, you can either select to deploy it with:</p>
<ul>
<li>Amazon Aurora with MySQL compatibility </li>
<li>Amazon Aurora with PostgreSQL compatibility</li>
</ul>
<p>Each of these options have a different price for the on-demand instances. </p>
<p>Also, you also have an option to Amazon Aurora Serverless which ultimately means there are no instances to manage. With this in mind, the on-demand pricing is not applicable, serverless pricing will be used instead and is measured in Aurora Capacity Units (ACUs). Each ACU consists of 2GB of memory in addition to any associated CPU and networking requirements. As an example, each ACU within the London region is charged at $0.07 per ACU Hour</p>
<p>Oracle: When selecting Oracle as your DB engine, your on-demand instance price will be dependant on which edition of Oracle you choose, the options available to you are:</p>
<ul>
<li>Standard Edition One (SE1)</li>
<li>Standard Edition Two (SE2)</li>
</ul>
<p>SQL Server: Much like with Oracle, the SQL Server DB engine is also dependent on the edition your select:</p>
<ul>
<li>Express</li>
<li>Web</li>
<li>Standard</li>
</ul>
<p>Again, each of these options will offer a different on-demand instance price.</p>
<p>Currently, the bring your own license options are only available with the Oracle DB engine. When using a bring your own license, you are able to use RDS with one of your pre-existing licenses that you already own, in this instance, one of your Oracle database software licenses. This differs from on-demand instances where your software licenses are already included in the price.</p>
<p>Again, much like on-demand instances that we discussed previously, you are charged by the hour with no long-term commitments of any sort.</p>
<p>Before you decide to use a BYOL license instance you need to ensure that your current license includes software update license and support for the particular instance that you are looking to create. </p>
<p>Also with BYOL for Oracle you have additional editions for deployment. This means that BYOL supports the following Oracle Editions:</p>
<ul>
<li>Standard Edition</li>
<li>Standard Edition One (SE1)</li>
<li>Standard Edition Two (SE2)</li>
<li>Enterprise Edition</li>
</ul>
<p>As you are only paying for the compute instances when using BYOL, there is no variation in prices between the different editions of Oracle being used.</p>
<p>The following tables shows the pricing for single-az deployment using Oracle with BYOL option with T3 and M5 instances.</p>
<p>The pricing for multi-az will be double the prices you see here.</p>
<p>Reserved instances allow you to purchase a discount for an instance type with set criteria for a set period of time in return for a reduced cost compared to on-demand instances. This reduction can be as much as 75%. These reservations against instances must be purchased in either one or three-year time frames. </p>
<p>Further reductions can be achieved with reserved instances depending on which payment methods you select. There are three options available to you, firstly:</p>
<ul>
<li>All upfront. The complete payment for the one or three-year reservation is paid. This offers the largest discount and no further payment is required regardless of the number of hours the instance is used. </li>
<li>Partial upfront, here a smaller upfront payment is made and then a discount is applied to any hours used by the instance during the term. </li>
<li>No upfront, no upfront or partial payments are made and the smallest discount of the three models is applied to any hours used by the instance.</li>
</ul>
<p>The following is an example of the pricing for MySQL reserved instance pricing in the London Region for the db.t3.medium instance type</p>
<p>When compared to the on-demand pricing there is a big cost-savings, especially when you factor in the difference in term length, plus any upfront payments. In this example, the on-demand pricing for the same instance type is $0.076 per hour, now if you compare this to the 3 year term, full upfront payment option of just $0.037 per hour, that’s a saving of just over 51%!</p>
<p>Again, we also have the option to implement Multi-AZ with reserved instances, and for consistency, here is the pricing of Multi-AZ with the same instance type as the example above.</p>
<p>Again, much like the on-demand pricing there are some additional deployment options when using reserved instances.</p>
<p>Amazon Aurora: When selecting Amazon Aurora as your DB engine, you can either select to deploy it with:</p>
<ul>
<li>Amazon Aurora with MySQL compatibility </li>
<li>Amazon Aurora with PostgreSQL compatibility</li>
</ul>
<p>Each of these options have a different price for the reserved instances. </p>
<p>Oracle: When selecting Oracle as your DB engine, your reserved instance price will be dependant on which edition of Oracle you choose, the options available to you are:</p>
<ul>
<li>Standard Edition One (SE1)</li>
<li>Standard Edition Two (SE2)</li>
</ul>
<p>SQL Server: Much like with Oracle, the SQL Servers DB engine is also dependent on the edition your select:</p>
<ul>
<li>Express</li>
<li>Web</li>
<li>Standard</li>
</ul>
<p>Again, each of these options will offer different reserved instance prices.</p>
<p>This follows the same principles I covered earlier when discussing BYOL for on-demand instance pricing. At the time of writing this course, it is only available for Oracle, and the same prerequisites apply, in addition to the same 4 editions being available:</p>
<ul>
<li>Standard Edition</li>
<li>Standard Edition One (SE1)</li>
<li>Standard Edition Two (SE2)</li>
<li>Enterprise Edition</li>
</ul>
<p>As you are only paying for the compute instances when using BYOL, there is no variation in prices between the different editions of Oracle being used.</p>
<p>The following table shows the reserved pricing for single-az deployment using Oracle with BYOL option for the t3.micro instance. </p>
<p>If you compare this to the pricing below which shows the same instance types, but using a standard reserved instance where the price of the license is included, you can see there is a substantial difference. </p>
<p>Again, the pricing for multi-az will be double the prices you see here.</p>
<h1 id="Database-Storage-and-I-x2F-O-Pricing"><a href="#Database-Storage-and-I-x2F-O-Pricing" class="headerlink" title="Database Storage and I&#x2F;O Pricing"></a>Database Storage and I&#x2F;O Pricing</h1><p>Hello and welcome to this lecture where I shall be looking at how both primary storage and I&#x2F;O pricing is configured.</p>
<p>We have looked at the pricing options for your database compute instances themselves, I now want to focus on the storage aspects of your databases and how these charges are calculated across the different DB engines.</p>
<p>MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server all use Elastic Block Store (EBS) volumes for both data and log storage. However, Aurora on the other hand uses a shared cluster storage architecture and does not use EBS.</p>
<p>So let me first look at the pricing for the majority of the DB engines using EBS.</p>
<p>When configuring your storage requirements, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">RDS</a> supports 3 different types:</p>
<ul>
<li>General Purpose SSD Storage</li>
<li>Provisioned IOPS (SSD) storage</li>
<li>Magnetic Storage</li>
</ul>
<p>Let’s take a closer look at each, starting with General purpose SSD storage:</p>
<ul>
<li><p>General Purpose SSD storage: This is a good option for a broad range of use cases which provides single-digit millisecond latencies and offers a cost-effective storage solution. The minimum SSD storage for your primary data set is 20 GiB, with a maximum of 64 TiB for MySQL, PostgreSQL, MariaDB, and Oracle, however, the maximum for SQL Server is 16 TiB. When using SSD, you are charged for the amount of storage provisioned and not for the number of I&#x2F;Os processed</p>
</li>
<li><p>Provisioned IOPS (SSD) storage: This option is great for when you have workloads that operate at a very high I&#x2F;O. You can provision a minimum of 8000 IOPS, and a maximum of 80000 for MySQL, PostgreSQL, MariaDB, and Oracle, but the maximum for SQL Server is 40000. In addition to being able to provision the IOPS needed for your workload, the minimum storage for your primary data set is 100 GiB, with a maximum of 64 TiB for MySQL, PostgreSQL, MariaDB and Oracle, and 16 TiB for SQL Server. The charges for this option are based upon the amount of storage provisioned in addition to the IOPS throughput selected, again, you are charged not for the total number of I&#x2F;Os processed.</p>
</li>
<li><p>Magnetic storage: Finally magnetic storage is simply supported to provide backward compatibility, and so <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> recommends that you select General Purpose instead</p>
</li>
</ul>
<p>The following screenshot shows the configuration screen when determining your storage requirements for MySQL. In this example Provisioned IOPS (SSD) has been selected as the storage, with a minimum of 100 GiB of primary storage, and a 1000 provisioned IOPS as throughput.</p>
<p>Let’s now take a look at the pricing structure for the storage.</p>
<p>The costs for your database storage has 2 different price points depending on whether it has been configured as a single-AZ or Multi-AZ deployment. Much like the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/rds-instance-purchasing-options/">instance pricing</a>, the Multi-AZ is typically twice the value of single-AZ deployment. </p>
<p>Each of the storage types, General Purpose SSD, Provisioned IOPS (SSD), and Magnetic all come at a different price. For each type of storage used, it is priced at per GB-Month, but what is the metric of GB-Month exactly? </p>
<p>Essentially this defines how many GBs of storage have been provisioned and for how long. Let me give an example. Assume we are working on a 30 day month and we receive a bill for 10GB-Months of storage, this could be the result of the following scenarios:</p>
<ul>
<li>You have 300-GB of storage running for just 24 hours </li>
<li>You have 10-GB of storage running for 720 hours </li>
<li>You have 40-GB of storage running for 180 hours</li>
</ul>
<p>These are calculated using the following formula: </p>
<p>Total provisioned storage &#x2F; (720&#x2F;number of hours running)</p>
<p>Where 720 &#x3D; number of hours in a 30-day month</p>
<p>So, looking at the examples above the calculations would be as follows:</p>
<ul>
<li>300 &#x2F; (720&#x2F;24)</li>
<li>10 &#x2F; (720&#x2F;720)</li>
<li>40 &#x2F; (720&#x2F;180)</li>
</ul>
<p>The following screenshots show the costs for each of the storage types for the MySQL DB engine in the London region under a single-az deployment:</p>
<p>I now want to revisit the storage for the Aurora DB engine. As I explained earlier, Aurora uses a shared cluster storage architecture which is managed by the service itself. When configuring your Aurora database in the console, the option to configure and select storage options like we saw previously does not even exist. Your storage will scale automatically as your database grows. As a result the pricing structure for your Aurora database is priced differently.</p>
<p>Again, the pricing metric used is GB-Months, in addition to the actual number of I&#x2F;Os processed, which are billed per million requests. The great thing about using Aurora is that you are only billed for the storage used and IOs processed, whereas with MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server you are billed for the storage provisioned regardless of if your use all of it or just a part of it. </p>
<p>As an example, the following shows the costs for both the used in GB-Months, plus the I&#x2F;Os processed (per million requests) within the London region.</p>
<h1 id="Backup-Storage-Pricing"><a href="#Backup-Storage-Pricing" class="headerlink" title="Backup Storage Pricing"></a>Backup Storage Pricing</h1><p>Now that we have looked at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/database-storage-and-i-o-pricing/">primary storage and I&#x2F;O costs</a> for your database, let’s now take a look at what costs you will incur for backups that you take of your database. Firstly, I want to identify which actions and considerations need to be taken into account and that will affect how your backup storage fluctuates:</p>
<ul>
<li><p>The first point I want to make, and it might surprise you, is that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">Amazon RDS</a> does not charge any backup storage costs that equates to the total sum of provisioned storage used with your databases within a specific region. So for example, Assume you had a MySQL DB with provisioned storage of 150GiB-Month DB, plus a MariaDB DB with 450GiB in a single region. Amazon RDS would not charge you for any storage utilised up to 600GiB-Month.</p>
</li>
<li><p>Any backup storage used over this ‘free’ tier is charged at $0.10 per GiB-Month, regardless of the region for:</p>
</li>
<li><ul>
<li>MySQL</li>
<li>PostgreSQL</li>
<li>MariaDB</li>
<li>Oracle</li>
<li>SQL Server</li>
</ul>
</li>
<li><p>Any backup storage used over this ‘free’ tier for Aurora is charged differently based upon the region. Currently at the time of writing this course, it is $0.022 per GiB-Month in the London region</p>
</li>
<li><p>Any automated backups taken use backup storage</p>
</li>
<li><p>Any manual snapshots that are taken of your database will also use backup storage</p>
</li>
<li><p>By extending your backup retention periods (how long you’d like to keep your backups for) will increase the amount of storage required for backups</p>
</li>
<li><p>Backup storage is calculated based upon the total amount of backup storage consumed within a specific region across all your RDS databases</p>
</li>
<li><p>If you copy your backups to another region, this will also increase the amount of backup storage used within that new region</p>
</li>
</ul>
<p>So all in all, backup storage for your RDS databases is relatively simple to calculate and inexpensive bearing in mind it is free to backup 100% of your provisioned database storage. </p>
<p>Let me now move onto <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/backtrack-storage-pricing/">Backtrack storage costs</a> with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> Aurora.</p>
<h1 id="Backtrack-Storage-Pricing"><a href="#Backtrack-Storage-Pricing" class="headerlink" title="Backtrack Storage Pricing"></a>Backtrack Storage Pricing</h1><p>Let me now move on to backtrack storage costs with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> Aurora. Backtrack is a feature that is only currently available for a MySQL-compatible Aurora database, using and is configured at the time of the database creation. Essentially, backtrack allows you to go back in time on the database to recover from an error or incident without having to perform a restore or create another DB cluster. For a deeper dive on on Backtrack storage, take a look at this AWS blog post found here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/">https://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/</a></p>
<p>As you can see from the configuration page during the Aurora database creation process, it is enabled via a checkbox and allows you to enter a number in hours of how far you would like to ‘backtrack’ to, with a maximum of 72 hours. In this example, I have entered 12 hours, and so Aurora will retain log data of all changes 12 hours as specified. The number of changes made directly relates to how much the Backtrack feature is going to cost you. </p>
<p>The pricing shown here is based upon a set cost per 1 million change records per hour for the London region.</p>
<p>So let’s look at an example. If you had built an Aurora database with a 12 hour backtrack setting like I had in my previous example that was generating 50,000 change records per hour the calculation would be as follows:</p>
<p>Obtain the total number of change records for your backtrack time period:</p>
<p>50,000 (change records&#x2F;hour) x 12 hours &#x3D; 600,000 change records </p>
<p>Calculating total costs based upon the London region:</p>
<p>(600,000 &#x2F; 1,000,000) x $0.014 &#x3D; $0.0084&#x2F;hour</p>
<p>To help you keep an accurate record of the number of change records, you can use Amazon CloudWatch to help you monitor the number of change records that are being generated each hour. </p>
<h1 id="Snapshot-Export-Pricing"><a href="#Snapshot-Export-Pricing" class="headerlink" title="Snapshot Export Pricing"></a>Snapshot Export Pricing</h1><p>Snapshots in RDS are your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/backup-storage-pricing/">backups</a> of your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/database-storage-and-i-o-pricing/">database</a> tables and instances, and these snapshots can then be exported out of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">Amazon RDS</a>, to Amazon S3. You might want to do this perform analysis of the data held within your database using more specific tools, for example, Amazon Athena, which is an interactive query service that allows you to perform analysis on data that you have stored in Amazon S3 using a standard SQL. </p>
<p>You could also use other services, such as Amazon SageMaker and Amazon Elastic Map Reduce (EMR), however, the point is, the data can be exported from your RDS database to S3 for further in-depth analysis as required.</p>
<p>Also, during an export of a snapshot, you can decide, through filtering, to simply export specific databases, tables, or even schemas.</p>
<p>The cost associated with performing snapshot exports are again based on a region by region basis. The example here shows the costs for the Ireland Region when using MySQL and is based upon a cost per GB of snapshot</p>
<p>For additional security and protection of your exported data, you can encrypt the data using the Key Management Service (KMS). As a result, additional KMS costs would be incurred depending on the KMS key selected. For more information on the Key Management Service, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/">https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/</a></p>
<p>Also, you will also be charged costs associated with Amazon S3 for the storage and your PUT requests. For more information on how Amazon S3 costs are calculated, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/">https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/</a></p>
<h1 id="Data-Transfer-Pricing"><a href="#Data-Transfer-Pricing" class="headerlink" title="Data Transfer Pricing"></a>Data Transfer Pricing</h1><p>When we talk about data transfer there are a number of different data paths where you might be charged for transferring data IN to and OUT of your RDS database depending on the source and destination, for example:</p>
<ul>
<li>Data transferred IN to your RDS database from the internet</li>
<li>Data transferred OUT from your RDS database to the internet</li>
<li>Data transferred OUT to Amazon CloudFront</li>
<li>Data transferred OUT to <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Regions</li>
<li>Data transferred OUT to EC2 instances in the same availability zone</li>
<li>Data transferred between availability zones for multi-az replication</li>
<li>Data transferred between an EC2 instance and an RDS instance in different availability zones of the same region</li>
<li>Data transferred when a snapshot copy is transferred to a different region</li>
</ul>
<p>Across the different DB engines, there is consistency when it comes to data transfer costs, so let’s take a look at each of the price points individually. In the following examples, I will be taking the pricing from the MySQL DB engine.</p>
<p>Firstly, let me show you under which circumstances you will NOT be charged for data transfer and so it completely free, this includes:</p>
<ul>
<li>Any data that is transferred IN to your RDS database from the internet</li>
<li>Any Data that is transferred OUT to Amazon CloudFront</li>
<li>Any data that is transferred OUT to EC2 instances in the same availability zone</li>
<li>Data transferred between availability zones for multi-az replication - For those who might be new to Multi-AZ, it simply means Multi-Availability Zone, which is a feature that is used to help with resiliency and business continuity. When Multi-AZ is configured, a secondary RDS instance, known as a replica, is deployed within a different availability zone within the same region as the primary instance. That’s its single and only purpose, to provide a failover option for a primary RDS instance.  As a result, there is a requirement for data replication between the primary and the secondary.</li>
</ul>
<p>Data transferred OUT from your RDS database to the internet: This element of data transfer is very dependent on how much data you transfer per month and is charged per GB. As a result, the resulting pricing is reflected as a tiered structure as you can see.</p>
<p>The first GB each month is free, and then as the amount of data transferred increases in Terabytes, the amount charged in GB decreases. The more you transfer out to the internet, the cheaper per GB your data transfer becomes.</p>
<p>If data is transferred from RDS out to another region, then you will incur a small charge per GB, and as it stands at the time of writing this course, this is a flat fee of $0.02 per GB for ANY region that the data is transferred to. </p>
<p>In this instance, charges will apply for Amazon EC2 regional data transfer both sides of the transfer and is charged at $0.01&#x2F;GB in each direction.</p>
<p>As explained earlier, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/snapshot-export-pricing/">Snapshots</a> in RDS are your backups of your database tables and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/rds-instance-purchasing-options/">instances</a>, and these snapshots can be both exported out of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">Amazon RDS</a> and copied to another region. These Cross-region snapshots are a great way to help you implement a disaster recovery strategy across your RDS database infrastructure.</p>
<p>When copying your snapshots across regions you are just charged for the amount of data transferred based on the size of the snapshot, based on $0.02 per GB.</p>
<h1 id="Amazon-DynamoDB"><a href="#Amazon-DynamoDB" class="headerlink" title="Amazon DynamoDB"></a>Amazon DynamoDB</h1><p>Hello and welcome to this lecture covering Amazon DynamoDB. Amazon DynamoDB is a NoSQL database, which means that it doesn’t use the common Structured Query Language, SQL. It falls into a category of databases known as key-value stores. A key value store is simply a collection of items or records, and you can look up data by using a primary key for each item or through the use of indexes.</p>
<p>Amazon DynamoDB is designed to be used for ultra high performance, which could be maintained at any scale with single-digit latency, making this a very powerful database choice used commonly for gaming, web, mobile and IoT applications to name but a few. Much like Amazon RDS, DynamoDB is also a fully managed service, taking many of the day-to-day administration operations out of your hands, giving you more time to focus on the business logic of your database. That’s one of the great things about Amazon DynamoDB, there’s no database administration required by us as a customer, no service to manage and nothing to back up. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> handles all of this for you. This makes the creation of a DynamoDB database very easy. All you have to do is set up your tables and configure the level of provision throughput that each table should have. Provision throughput refers to the level of read and write capacity that you want AWS to reserve for your table. You are charged for the total amount of throughput that you configure for your tables plus the total amount of storage space used by your data.</p>
<p>If we actually look at the configuration screen when creating a new DynamoDB database, as seen here, you can see that there are very few options required to create a new database. And in fact, in its simplest form, you can just provide a table name and a primary key, which is used to partition data across hosts for scalability and availability. You can then accept any remaining defaults and create your database, it’s as simple as that. DynamoDB tables are considered schemaless because there’s no strict design and schema that every record must conform to. As long as each item has an appropriate primary key, the item can contain varying sets of attributes. The records in a table do not need to have the same attributes or even the same number of attributes. This can be very convenient for rapid application development and if you want to add a new column to a table, you don’t need to alter the table, you can just start including the new field as an attribute when you insert new records. Likewise, you never need to adjust the data type for a column as DynamoDB generally isn’t interested in data types for individual attributes.</p>
<p>If when creating your DynamoDB database, you choose not to reset all the defaults, what other options exist? Let’s take a look. Unchecking the use default settings from the Table settings section provides you with the following. Firstly, you’ll be asked about secondary indexes, which allow you to perform queries on attributes that are not part of the table’s primary key. The default option provides no secondary index. However, you can add them here if required. DynamoDB lets you create additional indexes so that you can run queries to search your data by other attributes. If you’ve worked with relational databases, you’ve probably used indexes with those, but there are a couple of big differences in how indexes operate in DynamoDB.</p>
<p>First, each query can only use one index. If you want to query and match on two different columns, you need to create an index that can do that properly. Second, when you write your queries, you need to specify exactly which index should be used for each query. It’s not like a relational database that has a query analyzer, which can decide which indexes to use for our query. Here you need to be explicit and tell DynamoDB what index to use. DynamoDB has two different kinds of secondary indexes, global indexes let you query across the entire table to find any record that matches a particular value and by contrast, local secondary indexes can only help find data within a single partition key.</p>
<p>Following secondary indexes, you can modify the default settings applied to your table’s read&#x2F;write capacity mode. When you create a table in DynamoDB, you need to tell AWS how much capacity you want to reserve for the table. You don’t need to do this for disk space as DynamoDB will automatically allocate more space for your table as it grows. However, you do need to reserve capacity for input and output for reads and writes. Amazon charges you based on the number of read capacity units and write capacity units that you allocate. It’s important to allocate enough for your workload, but don’t allocate too much or DynamoDB could become prohibitively expensive.</p>
<p>By default, when you create a table in the AWS Console, Amazon will configure your table with five read capacity units and five write capacity units. There are two modes that you can choose from, provisioned and on-demand. Provisioned mode allows you to provision set read and writes allowed against your database per second by your application and is measured in capacity units, RCUs for reads and WCUs for writes. Depending on the transaction, each action will use one or more RCUs or WCUs. Provisioned mode is used generally when you have a predicted and forecasted workload of traffic. On-demand mode does not provision any RCUs or WCUs, instead they are scaled on demand. The downside is that it is not as cost effective as provisioned. This mode is generally used if you do not know how much workload you are expected to experience. Over time, you are likely to get more of an understanding of load and you can change your mode across to provisioned.</p>
<p>Once you have selected the provisioned mode, you will then have the opportunity to add configuration information relating to how your RCU and WCU are scaled as demand increases and decreases. As you can see, by entering your minimum and maximum provisioned capacity along with your target threshold utilization as a percentage, you can confidently rely on Amazon DynamoDB to manage the scaling operations of your throughput.</p>
<p>The last main point of the configuration allows you to set encryption of your tables, which is enabled by default for data at rest. Through the use of the key management service, KMS, you are able to select either a customer managed or AWS managed CMK to use for the encryption of your table instead of the default keys used by DynamoDB. For more information on CMKs and the key management service in general, please refer to our existing course found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Before I finish this lecture covering DynamoDB, I just want to cover some of its advantages and also what can be considered disadvantages. Some of the advantages of DynamoDB is that it’s fully managed by AWS, you don’t have to worry about backups or redundancy, although you’re welcome to set up these kinds of safeguards using some more advanced DynamoDB features.</p>
<p>As mentioned previously, DynamoDB tables are schemaless so you don’t have to define the exact data model in advance, the data model can change automatically to fit your application’s needs.</p>
<p>DynamoDB is designed to be highly available and your data is automatically replicated across three different availability zones within a geographic region. In the case of an outage or an incident affecting the entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>DynamoDB is designed to be fast, read and writes take just a few milliseconds to complete and DynamoDB will be fast no matter how large your table grows, unlike relational database, which can slow down as the table gets large. DynamoDB performance is constant and stays consistent even with tables that are many terabytes in size. You don’t have to do anything to handle this, except adjusting the provisioned throughput levels to make sure you’ve preserved enough read and write capacity for your transaction volume.</p>
<p>There are also some downsides to using DynamoDB too. As I just mentioned, your data is automatically replicated. Three copies are stored in three different availability zones and that replication usually happens quickly in milliseconds, but sometimes it can take longer and this is known as eventual consistency. This happens transparently and many operations will make sure that they’re always working on the latest copy of your data, but there are certain kinds of queries and table scans that may return older versions of data before the most recent copy. You need to be aware of how this works and you may need to adjust certain queries to require strong consistency.</p>
<p>DynamoDB’s queries aren’t as flexible as what you can do with SQL. If you are used to writing advanced queries with joins and groupings and summaries, you won’t be able to do that with DynamoDB. You’ll have to do more of the computation in your application code. This is done for performance reasons to ensure that every query finishes quickly and that complicated queries can’t hog the resources on a database server. </p>
<p>DynamoDB also has some strict limitations in the way you’re allowed to work with it. Two important limitations are the maximum record size of 400 kilobytes and the limit of 20 global indexes and five secondary indexes per table. There are other limitations that can be adjusted by contacting AWS customer support like the maximum number of tables in an AWS account.</p>
<p>Finally, although DynamoDB performance can scale up as your needs grow, your performance is limited to the amount of read and write throughput that you’ve provisioned for each table. If you expect a spike of the database use, you’ll need to provision more throughput in advance or database requests will fail with a ProvisionedThroughputExceededException message. Fortunately, you can adjust throughput at any time and it only takes a couple of minutes to adjust. Still, this means that you’ll need to monitor the throughput being used in each table or you’ll risk running out of throughput if your usage grows.</p>
<h1 id="DEMO-Creating-a-DynamoDB-Database"><a href="#DEMO-Creating-a-DynamoDB-Database" class="headerlink" title="DEMO: Creating a DynamoDB Database"></a>DEMO: Creating a DynamoDB Database</h1><p>Hello, and welcome to this lecture. This is going to be a demonstration on how to quickly, and easily create a DynamoDB database. Now, first I’ll need to go to the database category, and here we can see DynamoDB. Now I don’t have any DynamoDB databases sets up yet. So if you select create table, and you’ll be presented with this screen.</p>
<p>So first we’ll need to give it a table name, just call this my database and also a primary key. Now the primary key is essentially used to uniquely identify each item in the table. And the primary key is essentially comprised of a partition key. So let me just add one in. I’ll just call this product ID, and we can slate either a string, binary, or a number. I’ll leave that as a string. If we need to, we can also add in a sort key as well. And as we can see here the sort key simply allows you to search within a partition. Just remove that sort key.</p>
<p>Now essentially you can now create your table simply from providing that information, because this tick box here allows you to use lots of default settings that essentially fills in the rest of the configuration for you. So if you’re happy with your table name and primary key, with these default settings for your table, you can simply click create and it’s done. However, I want to uncheck the default settings so you can see the different configurable components used. So let’s take a look.</p>
<p>Now, firstly we have our secondary indexes, so you can add a secondary index, and these allow you to perform queries on attributes that are not part of the table’s primary key. Next, we have our read and write capacity mode, either provisioned or on demand. If we select the provisioned capacity mode, then we can select our read capacity units, and also our write capacity units.</p>
<p>Now scrolling down a bit further to auto scaling. We can set up auto scaling for our read and write capacity units. So when the read capacity gets to 70% utilization, we can scale up to a maximum of 40,000 units, and the same with the write capacity. So you can alter these figures if you need to, and change them to whatever values you need. As a part of that auto scaling process, DynamoDB needs an auto-scaling service link role to give it permission to do so.</p>
<p>Once you’re happy with your read and write capacity units, we can then scroll down to encryption at rest. Now by default encryption is enabled. The default option uses a key that’s owned by DynamoDB, and you are not charged for the use of any encryption keys in this default setting. However, you can use a KMS custom managed CMK, which is the CMK that you may have created, and you can select it from this box here, if you have any and enter the ARN, or you can use the KMS <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> managed CMK, which is this key here.</p>
<p>So it depends on the level of control that you want for the encryption key. First demonstration I’m just gonna leave it as the default. At the bottom here, you can add any text to a database if you’d like. So once you’re happy with the configuration, you simply click on create. As we can see the table is being created. And once it’s created, you can then use these tabs along the top to set up any alarms and review your capacity units set up your indexes, backups, etc, etc, etc. But for this demonstration, I simply wanted to show you how quickly and easy it is to set up and configure a DynamoDB table.</p>
<h1 id="DynamoDB-Accelerator-DAX"><a href="#DynamoDB-Accelerator-DAX" class="headerlink" title="DynamoDB Accelerator (DAX)"></a>DynamoDB Accelerator (DAX)</h1><p>Before we get into DAX itself, I just want to take a minute to cover a 10,000-foot overview of DynamoDB.</p>
<p>Amazon DynamoDB is a fully managed NoSQL database service. There’s no database administration required on your end, no servers to manage and nothing to back up. All of this is handled for you by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. All you have to do is set up your tables and configure the level of provisioned throughput that each table should have.</p>
<p>As DynamoDB is designed to be highly available your data is automatically replicated across multiple availability zones within a geographic region. In the case of an outage or an incident affecting an entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>DynamoDB is designed to be fast. Read and writes take just a few milliseconds to complete, and DynamoDB will be fast no matter how large your tables grow. Unlike a relational database, which can slow down as the table gets large, DynamoDB performance is constant and stays consistent even with tables that are many terabytes in size. You don’t have to do anything to handle this, except adjusting the provisioned throughput levels to make sure you’ve reserved enough read and write capacity for your transaction volume.</p>
<p>Despite its many benefits, there can be some downsides to using DynamoDB too. As I mentioned, your data is automatically replicated to different AZs, and that replication usually happens quickly, in milliseconds, but sometimes it can take longer. This is known as eventual consistency. This happens transparently and many operations will make sure that they’re always working on the latest copy of your data. But there are certain kinds of queries and table scans that may return older versions of data before the most recent copy.</p>
<p>This can be a problem in certain circumstances, and sometimes milliseconds might not be fast enough for your use case. You might have a requirement where you need microsecond response times in read-heavy workloads, and this is where DynamoDB Accelerator (DAX) comes in to play. By combining DynamoDB with DAX, you end up with a NoSQL database solution offering extreme performance.</p>
<p>So what is DAX? DAX is an in-memory cache delivering a significant performance enhancement, up to 10 times as fast as the default DynamoDB settings, allowing response times to decrease from milliseconds to microseconds. It is a fully managed feature offered by AWS and as a result, is also highly available.</p>
<p>Dax is also highly scalable making it capable of handling millions of requests per second without any requirement for you to modify any logic to your applications or solutions as its fully compliant with all DynamoDB API calls, and so it seamlessly fits into your existing architecture without any redesign and effort from your developer teams.</p>
<p>Your DAX deployment can start with a multi-node cluster, containing a minimum of 3 nodes, which you can quickly and easily modify and expand, reaching a maximum of 10 nodes, with 1 primary and 9 read replicas. This provides the ability to handle millions of requests per second.</p>
<p>Another benefit of using DAX is that it can also enable you to reduce your provisioned read capacity within DynamoDB. This is because of the fact that data is cached by DAX and so reduces the impact and amount of read requests on your DB tables, instead, these will be served by DAX from the in-memory cache. As we know, reducing the provisioned requirements on your DynamoDB database, will also reduce your overall costs.</p>
<p>From a security perspective, DAX also supports encryption at rest, this ensures that any cached data is encrypted using the 256-bit Advanced Encryption Standard algorithm with the integration of the Key Management Service (KMS) to manage the encryption keys.</p>
<p>If you are new to KMS, then you can find more on this service from our existing course found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>DAX is a separate entity to DynamoDB and so architecturally it sits outside of DynamoDB itself and is instead placed within your VPC, whereas DynamoDB sits outside of your VPC and is accessed via an endpoint.</p>
<p>During the creation of your DAX Cluster, you will be asked to select a subnet group, this is simply a grouping of subnets across different availability zones. This is to allow DAX to deploy a node in each of the subnets of the subnet group, with one of those nodes being the primary and the remaining nodes will act as read replicas.</p>
<p>To allow your EC2 instances to interact with DAX you will need to install a DAX Client on those EC2 instances. This client then intercepts with and directs all DynamoDB API calls made from your client to your new DAX cluster endpoint, where the incoming request is then load balanced and distributed across all of the nodes in the cluster.</p>
<p>To allow your EC2 instances to communicate with your DAX Cluster you must ensure that the security group associated with your DAX Cluster is open to TCP port 8111 on the inbound rule set.</p>
<p>If a request received by DAX from your client is a read request, such as a <code>GetItem</code>, <code>BatchGetItem</code>, <code>Query</code> or <code>Scan</code>, then the DAX cluster will try and process the request if it has the data cached. If DAX does not have the request in its cache (a cache miss) then the request will be sent to DynamoDB for the results to be returned to the client. These results will also then be stored by DAX within its cache and distributed to the remaining read replicas in the DAX cluster.</p>
<p>With regards to any write requested made by the client, the data is first written to DynamoDB before it is written to the cache of the DAX cluster.</p>
<p>One final point I want to make is that DAX does not process any requests relating to table operations and management, for example, if you wanted to create, update or delete tables. In this instance, the request is passed through directly to DynamoDB.</p>
<h1 id="Amazon-ElastiCache"><a href="#Amazon-ElastiCache" class="headerlink" title="Amazon ElastiCache"></a>Amazon ElastiCache</h1><p>Hello and welcome to this lecture dedicated to an overview of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> ElastiCache. Amazon ElastiCache is a service that makes it easy to deploy, operate, and scale open-source, in-memory data stores in the cloud. This service improves the performance through caching, where web applications allow you to retrieve information from fast managed in-memory data stores instead of relying entirely on slower disk-based solutions.</p>
<p>Let me take a closer look at caching and then expand our discussion to in-memory caching environments in the cloud.</p>
<p>If you think about your own computing devices, your laptop, for example, you may have a huge capacity from a persistent storage perspective via your hard drives with lots of applications installed, allowing you to work on a variety of documents at the same time. However, over time, you may notice a degradation in performance, and certain operations might start to become sluggish, especially as more and more files are opened and accessed to carry out different functions and tasks. When this happens, how can we make the computer perform faster and serve up our information quicker and enhance our response times? One solution is to install additional random access memory, known as RAM. With this additional memory, it enables our device to store frequently accessed information in memory instead of having to request the information from the hard drive, which is much slower than RAM. This process is known as caching.</p>
<p>Now let’s take this approach and relate it to how a web application uses in-memory caching. To be clear, ElastiCache is not just used for web applications. It can be used for any application that can benefit from increased performance using in-memory cache. But to explain the process, I shall focus on a web application as an example.</p>
<p>A common scenario is to have a web application that reads and writes data to persistent storage, for example, to a relational database such as RDS or a NoSQL database such as DynamoDB. However, persistent storage, like hard disk storage, tends to experience some fluctuations in latency as each piece of data needs to be written to or retrieved from a permanent media store, and this can affect the overall performance.</p>
<p>This is where an in-memory cache is useful. It’s generally used to improve read-only performance. Many websites get a high percentage of read hits, but less write hits. An in-memory cache can store frequently accessed read-only information and serve it up much quicker than having the application continually request it from a persistent data store.</p>
<p>Now imagine that your app becomes more popular and you need to scale up. Adding more web servers is not that difficult. However, instead of vertically scaling your data store for your database, you could instead add an in-memory caching layer with sub-millisecond response times, which will of course make a considerable difference. By adding more web servers along with an ElastiCache cluster, you can automatically grow your caching layer based on the increased demand. This can eliminate or reduce the need to scale up on your persistent data store.</p>
<p>ElastiCache supports both Memcached and the Redis engines, so existing applications can be easily moved to ElastiCache. But what is Memcached and Redis and which cache engine should we choose? Amazon ElastiCache for Memcached is a high-performance sub-millisecond latency Memcached-compatible in-memory key store service that can either be used as a cache in addition to a data store. Amazon ElastiCache for Redis is purely an in-memory data store designed for high performance and again providing sub-millisecond latency on a huge scale to real-time applications.</p>
<p>As we can see, both engines provide a great cache solution and are both easily provisioned using the ElastiCache service. In this table, we can see some of the primary use cases for each.</p>
<p>Generally Redis offers a more robust set of features to that of Memcached. In broad terms, Redis has more features, whereas Memcached is often recognized for its simplicity and speed of performance. Memcached really suits workloads where memory allocation is going to be consistent and the increased performance is more important than the additional features that Redis offers.</p>
<p>As you can see from this screenshot during the configuration of ElastiCache, Redis also offers an option to enable cluster mode. When Redis cluster mode is disabled, each cluster will have just a single shard. However, with cluster mode enabled, each cluster can have up to 90 shards. So now’s a good time to run through at a high level some of these fundamental elements of ElastiCache. These include nodes, shards, and clusters.</p>
<p>ElastiCache nodes. A cache node is a fixed sized chunk of secure network attached RAM, essentially the building block of the ElastiCache service, and supports a clustered configuration. As you can see here, the nodes themselves can be launched using a variety of different instance types.</p>
<p>ElastiCache for Redis shards. A Redis shard, also known as a node group when working with the API and CLI, is a group of up to 6 ElastiCache nodes.</p>
<p>ElastiCache for Redis clusters. A Redis cluster contains between one and 90 Redis shards, depending on if cluster mode is enabled or disabled. Data is then partitioned across all of the shards in that cluster. </p>
<p>ElastiCache for Memcached clusters. Clusters are a collection of one or more cache nodes. Once you’ve provisioned a cluster, Amazon ElastiCache automatically detects and replaces failed nodes, which helps reduce the risk of overloaded database, and therefore reduces the website and application load times.</p>
<p>Before I finish this lecture covering ElastiCache, I just want to point out some of the common use cases where you might use Amazon ElastiCache. Due to its incredibly fast performance and scaling abilities, this is commonly used in the online gaming industry where it’s vital that statistical information like a scoreboard is presented as quickly and as consistently as possible to all the players in the game.</p>
<p>Another common use is for social networking sites, where they need a way to store temporary session information in session management.</p>
<p>Real-time analytics is also a great use for ElastiCache, as it can be used in conjunction with other services such as Amazon Kinesis to ingest, process, and analyze data at scale. This could help with personalized ads and recommendations with sub-millisecond latency.</p>
<p>As you can see, these are applications that have lots of read-only content that would benefit from an in-memory cache. Often users are scanning these websites for information like who currently has the high score in a game, what are your friends up to, looking for how-tos, or guidance on the best restaurant. Obviously there is information written to these sites, as well, like when something is updated or changed, so that information will be sent to a permanent data store.</p>
<p>Now we’ve discussed when ElastiCache is a great solution, but when is it best to consider using something else? ElastiCache should never be used to store your only version of data records, since a cache is designed to be a temporary data store. So when data persistence is necessary, like when working with primary data records, or when we need write performance rather than read performance, a persistent data store should be used instead of an ElastiCache.</p>
<p>Amazon ElastiCache can be a very useful tool for developers. Application developers are always looking for ways to ensure the best performance and availability for their application. Databases and other permanent storage options are relatively slow and users are impatient today. Waiting a few more seconds for an app to respond can mean losing business to a competitor. Developers are thrilled when an app becomes popular, but then there is a challenge of scaling. Adding front end support like more web servers is fairly simple, but when it comes to scaling persistent storage, it starts getting complicated and messy. Then there are a number of management tasks to consider. Updating, patching, monitoring, and securing data takes time. Using ElastiCache allows the developer to focus on building and improving apps by simplifying and reducing administrative tasks.</p>
<h1 id="DEMO-Creating-an-ElastiCache-Cluster"><a href="#DEMO-Creating-an-ElastiCache-Cluster" class="headerlink" title="DEMO: Creating an ElastiCache Cluster"></a>DEMO: Creating an ElastiCache Cluster</h1><p>Hello and welcome to this lecture, which is going to be a demonstration on how to create an Amazon ElastiCache cluster. So let’s get straight into it. So I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console, and you can find ElastiCache under the database category down here. So if we select on ElastiCache, we’re then presented with this screen if you haven’t got any ElastiCache clusters already, which I haven’t.</p>
<p>So to get started, simply click on the get started now button. And now we’re in the configuration page of creating our Amazon ElastiCache cluster. So the first option we have is to choose our cluster engine, which is either Redis or Memcached.</p>
<p>Now, if you remember back to the lecture we just covered, Redis generally offers a more robust set of features to that of Memcached, whereas Memcached is recognized for its simplicity and speed of performance. And for this demonstration, I’m just gonna stick with the Redis cluster engine. If you want to enable cluster mode, we can do so here through the tick box. With it enabled, we can have up to 90 shards per cluster. For this demonstration, I’m just going to leave that unchecked.</p>
<p>Now, if we go down to the Redis settings, we can enter our name here. Just put the name of demo. Same for the description. Then we have our engine version compatibility, which is essentially the version of Redis that you’ll install on your nodes. Just leave that as the default. The port that Redis will use. This is the port that the node will accept connections from.</p>
<p>Here we have a parameter group, and much like RDS, this just contains configurational settings about your engine. So if you have a different parameter group that you’d like to use for your cluster, then you can select it here. I’m just gonna use the default.</p>
<p>Scroll down to the node type. If we have a look at these, now, these are all the different nodes that you can have within your cluster. So the different sizes offering different memory and network performance, and we have different instance families along the top here, depending on what your requirements are. So I’m just gonna leave that as the default.</p>
<p>You can then select the number of replicas that you’d like to use and also if you’d like to configure multi-availability zone, which is used for high availability, and also as an automatic failover should your primary node fail. For this demonstration, I’m going to uncheck multi-AZ.</p>
<p>Now if we scroll down to the advanced Redis settings, here we can create a subnet group. Now, this subnet group is essentially used to define where your nodes can reside. So let’s create a new subnet group. So I’ll just call this Subnet1. And the same for the description. And then we can select the VPC that we want to use for our ElastiCache cluster.</p>
<p>So let’s just select the top one here, and then we can select which subnets that we want to include in our subnet group. So I’m just going to select two subnets. We can then select our availability zone placement. We can either have no preference, or if we have specific zones that we’d like to place these, to place our nodes in, then we can do so here. I’m gonna select no preference.</p>
<p>Down to security. Down here we can add any security groups which will control access to our clusters, and so you can add any security groups in that you need. If you want to apply encryption at rest, simply tick the box, and you can either use the default encryption key, which is managed by ElastiCache, or if you have your own CMK, then you can select it from here, for example.</p>
<p>So I have my own KMS key, which is my CMK. So if I wanted to, then I can select that own customer managed CMK. For this demonstration, I’m just going to select the default encryption key. And similarly, if you want encryption in transit, then you can do so by enabling the tick box there.</p>
<p>Now, if you want to import any data to your cluster from the creation, then you can select an S3 folder that might contain an RDS backup that will populate your cluster with data. And then we have our backups. If you want to enable automatic backups, simply tick the check box and then select the number of days for your retention period and if you have a backup window and what time that would be if you want one. I’m gonna select no preference.</p>
<p>And then finally, maintenance. If there’s any maintenance to take place, you can specify a maintenance window, again with day and time and duration. For this demonstration, I’m just gonna select no preference, and you can either have a topic for any notifications with regards to your, with regards to any maintenance carried out. So you can select one of your existing SNS topics or disable notifications. Then once you’re happy with the configuration, simply click create. As you can see, the status is creating, and that will just take a couple of minutes to run through.</p>
<p>Okay, so that took a few minutes to come up, but we can now see the status is available. And if we click on the name of the cluster, then we can see a number of different nodes that have been created. We have the primary and two replicas. If we look at the description, then we can see all of the information about this cluster. For example, the primary endpoint, the reader endpoint, the engine that it’s running, the availability zones, how many nodes, etc, and also the security groups and parameter groups, et cetera. Basically all the configuration information that we went through in its creation. And that’s it. And that’s how you set up an ElastiCache cluster.</p>
<h1 id="Amazon-Neptune"><a href="#Amazon-Neptune" class="headerlink" title="Amazon Neptune"></a>Amazon Neptune</h1><p>Hello, and welcome to this lecture on Amazon Neptune. Amazon Neptune may not be as widely utilized as perhaps <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> RDS or Amazon DynamoDB, simply due to what it is designed for. Amazon Neptune is a fast, reliable, secure, and fully managed graph database service.</p>
<p>For those who are unfamiliar with what a graph database is useful, they are essentially used to help you both store and navigate relationships between highly connected data which could contain billions of separate relationships. As a result, graph databases are ideal if their focus is on being able to identify these relationships of interconnected data, rather than the actual data itself. Trying to perform queries against complex relationships will be very difficult in a normal relational database model. And so graph databases are recommended in this scenario instead.</p>
<p>Before I continue, let me run through some of the use cases to help you understand when and where you might use Amazon Neptune as a graph database to solidify the importance of being able to query complex relationships.</p>
<p>Social networking. Graph databases are a powerful asset when used within a social networking environment. As you can imagine, there are vast webs of tightly networked data that run across social networking platforms, and understanding these relationships and being able to query against them is vital to being able to build and maintain effective social network applications. For example, being able to present the latest feed updates to your end users with relevant news from all the groups that they belong to can be easily generated using graph databases, thanks to the highly scalable and enhanced performance of Amazon Neptune.</p>
<p>Fraud detection. Security should always be a number one priority in any cloud deployment solution, and using Amazon Neptune can help you from a security standpoint, using its high performance capabilities. If you are carrying out financial transactions within your environment, then you can build applications that allow Neptune to analyze the financial relationships of transactions to help you detect potential fortunate activity patterns in near real time response times. For example, you might be able to detect that multiple parties are trying to use the same financial details, all from various different locations.</p>
<p>Recommendation engines. Recommendation engines are widely used across many different websites, largely, eCommerce sites that recommend products based upon your search and purchase history. Using Neptune as a key component within your recommendation engine allows it to perform complex queries based upon various different activities and operations made by the user that will help determine recommendations of what your customer may like to purchase next.</p>
<p>I’ve simply highlighted some of the common scenarios where you might use Amazon Neptune within your solutions. However, there are many, many more use cases available that focus on the relationships between vast amounts of highly interconnected data sets.</p>
<p>Now we have more of an understanding of when and where you might use Amazon Neptune, let’s take a look at some of its components.</p>
<p>Amazon Neptune uses its own graph database engine and supports two graph query frameworks. These being Apache Tinkerpop Gremlin, and this allows you to query your graph running on your Neptune database, using the Gremlin traversal language. And we have the Worldwide Web Consortium Sparql. The Sparql query language has been designed to work with the internet and can be used to run queries against your Neptune database graph.</p>
<p>When creating your Amazon Neptune database, you must create a name for your Neptune database cluster, but what is a cluster?</p>
<p>An Amazon Neptune database cluster is comprised of a single, or if required, multiple database instances across different availability zones, in addition to a virtual database cluster volume which contains the data across all instances within the cluster. The single cluster volume consists of a number of Solid State Discs, SSDs. As your graph database grows, your shared volume will automatically scale an increase in size as required to a maximum of 64 terabytes.</p>
<p>To ensure high availability is factored into Neptune, each cluster maintains a separate copy of the shared volume in at least three different availability zones. This provides a high level of durability to the data. </p>
<p>From a storage perspective, Amazon Neptune has another great feature to help with the durability and reliability of data being stored across your shared cluster, this being Neptune Storage Auto-Repair.</p>
<p>Storage Auto-Repair will automatically find and detect any segment failures that are present in the SSDs that make up the shared volume, and then automatically repair that segment using the data from the other volumes in the cluster. This ensures that the data loss is minimized and the need to restore from a failure is drastically reduced.</p>
<p>Similarly to other AWS database services, Amazon Neptune also has the capability to implement and run replica instances. If replicas are used, then each Neptune cluster will contain a primary database instance, which will be responsible for any read and write operations. The Neptune replicas, however, are used to scale your read operations, and so support read-only operations to the same cluster volume that the primary database instance connects to. As the replicas connect to the same source data as the primary, any read query results served by the replicas have minimal lag, typically less than a 100 milliseconds after new data has been written to the volume.</p>
<p>A maximum limit of 15 replicas per crust exists which can span multiple availability zones. And this ensures that should have failure occur in the availability zone hosting the primary database, one of the Neptune read replicas in a different AZ will be promoted to the primary database instance, and adopt both read and write operations. This process usually takes about 30 seconds.</p>
<p>Data is synchronized between the primary database instance and each replica synchronously. And in addition to providing a failover to your primary database instance, they offer support to read only queries. These queries can be served by your replicas, instead of utilizing resources on your primary instance. When you have created your Amazon Neptune database, you need to understand how to connect to it, and this is achieved through endpoints. An endpoint is simply a URL address and a port that points to your components. There are three different types of Amazon Neptune endpoints, these being cluster endpoint, reader endpoint, and instance endpoint.</p>
<p>Let me take a quick look at each of these endpoints individually, starting with a cluster endpoint. For every Neptune database cluster that you create, you will have a cluster endpoint, and this points directly to the current primary database instance of that cluster. This endpoint should be used by applications that required both read and write access to the database. Earlier, I explained that if a primary instant fails and you have a read replica available, then Neptune will automatically failover to one of these replicas and act as the primary, providing read and write access. When this happens, the cluster endpoint will then point to the new primary instance without any changes required by your applications accessing the database.</p>
<p>Reader endpoints. As you might expect by the name, this endpoint is purely used to connect to any read replicas you might have configured. This is used to allow applications to access your database on a read only basis for queries. Only a single reader endpoint exists, even if you have multiple read replicas. Connection served by the read replicas will be performed on a round-robin basis, and it’s important to point out that the endpoint does not load balance your traffic in any way across the available replicas in your cluster.</p>
<p>Instance endpoints. For every instance within your cluster, including your primary and read replica instances, they will each have their own unique instance endpoint that will point to itself. This allows you to direct certain traffic to specific instances within the cluster. You might want to do this for load balancing reasons across your applications reading from your replicas.</p>
<h1 id="DEMO-Creating-an-Amazon-Neptune-Database"><a href="#DEMO-Creating-an-Amazon-Neptune-Database" class="headerlink" title="DEMO: Creating an Amazon Neptune Database"></a>DEMO: Creating an Amazon Neptune Database</h1><p>Hello, and welcome to this lecture, which is going to be a demonstration on how to create an Amazon Neptune database. So, I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Management Console, and we need to go down to Databases and then select Amazon Neptune. If you don’t have any Neptune databases created as yet, then you’ll be presented with this screen. Simply launch Amazon Neptune. And now we’re down to the configuration screen of our database.</p>
<p>Now, the first option we have is the engine options. So this is essentially the version of Neptune that you can use. Just going to select the latest one, which is the default. Then down to Settings, this is the database cluster identifier. So this will be the name of our database cluster. I’m just gonna leave it as a default as database-1 for this demonstration. But just be aware that the name must be unique within the region for all your database clusters that you own.</p>
<p>Next, we have templates. So we have either production or development and testing. The production one uses defaults for high availability and fast, consistent performance, whereas the dev and test won’t have as many high availability features. So, let’s go with the production template.</p>
<p>We can then select our database instance size. So we have a number of different instances here, all offering different quantities of VCPUs and memory, etc. So I’m just going to select the smallest one for this demonstration.</p>
<p>Down to Availability &amp; durability, we can either create a multi-AZ deployment of Amazon Neptune, or have a single availability zone deployment. If you want high availability, then you’d create the read replica in different zones. For this demonstration, I’m just going to select no to multi-AZ.</p>
<p>When we come down to connectivity, we can select our VPC that we want to deploy the Neptune database in, so select your VPC. If we expand this additional connectivity configuration, we can then select a subnet group which will define which subnets within your VPC that Amazon Neptune can use. In this VPC, I only have the default subnet group. Then we have our VPC security group which acts as a virtual firewall, which will define what traffic can talk to your database and over which ports as well. So you can select an existing security group, or you can create a new security group. I’m just going to select the default security group just for this demonstration.</p>
<p>As we can see, it’s added it in there. You can then decide which zone you’d like to place your database in, if you have a preference, that is, or you can select no preference. And also the port that it will use for application connections. If you’d like to, you can add a tag for your database.</p>
<p>And then finally, if we look at additional configuration, we have a number of database options, so we could provide a name for the actual instance of your database. Again, we have parameter groups that we’ve discussed in the previous lectures, and we have a parameter group for the actual cluster itself, and also for the individual database.</p>
<p>For authentication, you can enable IAM database authentication as well, which will manage access through users and roles. For security purposes, I recommend enabling that. You can define a failover priority, and the failover priority allows you to define on your Neptune replicas which one should be promoted to be prime instance should your prime instance fail. And the priorities range from zero for the highest priority to 15 to the lowest priority. And you can configure each replica with a different priority. I’ll just leave that as a default, no preference.</p>
<p>Down to Backup, these are our automatic backups. And again, we can choose our attention period, which is essentially the number of days that it should keep its automated backups. Encryption is enabled by default through KMS, and here it’s using the AWS RDS KMS key as its default key, which is fine. But if you’d like to select a different one, then you can select any CMKs that you have created. As we can see here, we have one I’ve created called MyCMK. But I’ll just leave it as the default AWS managed KMS key. If you’d like to export your logs, then you can tick the audit log and have that exported to CloudWatch Logs for further analysis.</p>
<p>When it comes to maintenance, you can enable auto minor version upgrade, and this will automatically upgrade to any new minor versions as they are released. And again, we have a maintenance window. So you can select a predefined window where you’d like any maintenance to be scheduled, or select no preference. And finally, you can enable deletion protection. And this simply stops anyone from going ahead and accidentally deleting your database. If you want to delete the database, then you have to modify the settings, uncheck that check box, and then you can delete the database. So like I say, it prevents any accidental deletion, you have to do it with intent. Just gonna leave that unchecked. And then once you’ve set all your settings, simply select Create database.</p>
<p>As we can see, it’s creating our database. If we went for the multi-AZ option, then we’d have another instance under this cluster as well. Now, that will just take a few minutes to start up and create. We can now see that the cluster is available, but it’s still creating our database instance. We can now see the availability zone that it’s placed that instance in, so it’s eu-west-1b. Okay, we can now see that the database instance is also available.</p>
<p>So let’s just take a quick look at these. So firstly, the cluster. As we can see here, we can see the connectivity of the cluster, so we have the cluster end point that I mentioned in the previous lecture, and also the reader end point as well.</p>
<p>For the monitoring, we can see some of the CloudWatch metrics that have been used. And then if we look at logs and events, we can see any logs that are being generated. Configuration is essentially the different options that we selected during the creation of the cluster. Similarly with the maintenance and backups, any maintenance or backups that are scheduled. And also our tags. And it’s a similar story for the actual instance itself. So let’s take a look at that.</p>
<p>We have the same options, connectivity and security, so we can see which VPC it resides in, the subnet group, and the subnets associated with that subnet group. We have the end point. Again, different CloudWatch metrics that are being captured. Any logs and events. Configuration, again, this is a lot of the configuration options that we defined during its creation, such as the KMS key and the instance size, etc, and any maintenance windows that have been scheduled.</p>
<p>So it’s as simple as that, it’s very easy to set up, and many of the configuration options are similar to the previous databases that we’ve also set up within this course.</p>
<h1 id="Amazon-Redshift"><a href="#Amazon-Redshift" class="headerlink" title="Amazon Redshift"></a>Amazon Redshift</h1><p>Hello, and welcome to this lecture where I will look at Amazon Redshift. Amazon Redshift is a fast, fully-managed, petabyte-scale data warehouse. And it’s designed for high performance and analysis of information capable of storing and processing petabytes of data and provide access to this data, using your existing business intelligence tools, using standard SQL. It operates as a relational database management system, and therefore is compatible with other RDBMS applications. Redshift itself is based upon PostgreSQL 8.0.2, but it contains a number of differences from PostgreSQL. These differences are out of scope for this course, but for more information, please refer to the documentation <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">here</a>.</p>
<p>A data warehouse is used to consolidate data from multiple sources to allow you to run business intelligent tools, across your data, to help you identify actionable business information, which can then be used to direct and drive your organization to make effective data-driven decisions to the benefit of your company.</p>
<p>As a result, using a data warehouse is a very effective way to manage your reporting and data analysis at scale. A data warehouse, by its very nature, needs to be able to store huge amounts of data and its data may be subjected to different data operations such as data cleansing, which as an example, may identify, correct, replace or remove incomplete records from a table or recordset.</p>
<p>This can be expanded upon for the need to perform an extract, transform and load or an ETL job. This is the common paradigm by which data from multiple systems is combined to a single database data store or warehouse for legacy storage or analytics.</p>
<p>Extraction is the process of retrieving data from one or more sources. Either online, brick &amp; mortar, legacy data, Salesforce data and many others. After retrieving the data, ETL is to compute work that loads it into a staging area and prepares it for the next phase.</p>
<p>Transformation is the process of mapping, reformatting, conforming, adding meaning and more to prepare the data in a way that is more easily consumed. One example of this is the transformation and computation where currency amounts are converted from US dollars to euros.</p>
<p>Loading involves successfully inserting the transform data into the target database data store, or in this case, a data warehouse. All of this work is processed in what the business intelligent developers call an ETL job.</p>
<p>Now we have an understanding of what Amazon Redshift is. Let’s move on to looking at the architecture of the service and the components that is built upon.</p>
<p>Let me start with clusters and nodes. A cluster can be considered the main or core component of the Amazon Redshift service. And in every cluster, it will run its own Redshift engine, which will contain at least one database. As the name implies, a cluster is effectively a grouping of another component, and these being compute nodes.</p>
<p>Each will contain at least one compute node. However, if the cluster is provisioned with more than one compute node, then Amazon Redshift will add another component called a leader node.</p>
<p>Compute nodes all contain their own quantity of CPU attached storage and memory. And there are different nodes that offer different performances. For example, the following RA3 node types. Also, as you can see here, the dense compute node types.</p>
<p>The leader node of the cluster has the role of coordinating communication between your compute nodes in your cluster and your external applications accessing your Redshift data warehouse. So the leader node is essentially gateway into your cluster from your applications. When external applications are querying the data in your warehouse, the leader node will create execution plans, containing code to return the required results from the database.</p>
<p>If the query from the external application references tables associated with the compute nodes, then this code is then distributed to the compute nodes in the cluster to obtain the required data, which is then sent back to the leader node. If the query does not reference tables stored on the compute nodes, then the query will run on the leader node only.</p>
<p>Each compute node itself is also split into slices, known as node slices. A node slice is simply a partition of a compute node where the nodes memory and disk spaces split. Each node slice then processes operations given by the leader node where parallel operations can then be performed across all slices and all nodes at once for the same query. As I mentioned previously, compute nodes can have different capacities and these capacities determine how many slices each compute node can be split into.</p>
<p>When creating a table, it is possible to distribute rows of that table across different nodes slices based upon how the distribution case is defined for the table. For a deeper understanding on how to select the best distribution style, please see the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">here</a>.</p>
<p>When your Amazon Redshift database is created, you will of course connect to it using your applications. Typically these applications will be your analytic and business intelligence tools, that you’re running with your organization. Communication between your BI applications and Redshift, will use industry standard open database connectivity, ODBC. And Java database conductivity, JDBC drivers for PostgreSQL.</p>
<p>The performance that Amazon Redshift can generate is a huge benefit to many organizations. In fact, at the time of writing this course, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> currently boasts that it’s three times faster than other cloud data warehouses.</p>
<p>From a query perspective, Amazon Redshift has a number of features to return results quickly and effectively. Let’s take a look at a few of them.</p>
<p>Firstly, massively parallel processing. As highlighted in the previous section by associating rows from tables across different nodes slices and nodes. It allows the node leader to generate execution plans, to distribute crews from external applications across multiple compute nodes at once, allowing them to work together to generate the end result, which is an aggregated by the leader node.</p>
<p>Columnar data storage. This is used as a way of reducing the number of times the database has to perform disk I&#x2F;O, which helps to enhance query performance. Reducing the data retrievals from the disk means there is more memory capacity to carry out in memory processing of the query results. Result caching. Caching in general is a great way to implement a level of optimization.</p>
<p>Result caching helps to reduce the time it takes to carry out queries by caching some results of the queries in the memory of the leader node in a cluster. As a result, when a query is submitted, the leader node will check its own cache copy of the results and if a successful match is found, the cached results are used instead of executing another query on your Redshift cluster.</p>
<p>Amazon Redshift also integrates with Amazon CloudWatch, allowing you to monitor the performance of your physical resources, such as CPU utilization and throughput. In addition to this, Redshift also generates query and load performance data that enables you to track overall database performance. Any data relating to query and load performance is only accessible from within the Redshift console itself and not Amazon CloudWatch.</p>
<p>During the creation of your Redshift cluster, you can as an optional element, select up to 10 different IAM roles to associate with your cluster. This allows you to grant the Amazon Redshift principle, redshift.amazonaws.com access to other services on your behalf, for example, Amazon S3 where you might have a data lake. Accessing data within S3 will require a set of credentials to authorize Redshift access to S3. And the best way to do that is by using an IAM role. Therefore, if you intend to perform actions such as this when using your Amazon Redshift cluster, you might need to consider which access you need and what roles you will need to create.</p>
<p>To learn more about IAM and roles, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">here</a>. In the next lecture, I want to show you how to create a new Redshift cluster.</p>
<h1 id="DEMO-Creating-an-Amazon-Redshift-Cluster"><a href="#DEMO-Creating-an-Amazon-Redshift-Cluster" class="headerlink" title="DEMO: Creating an Amazon Redshift Cluster"></a>DEMO: Creating an Amazon Redshift Cluster</h1><p>Hello and welcome to this lecture. This is going to be a quick demonstration on how to set up an Amazon Redshift cluster. So as you can see, I’m in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console at the moment, and to find Redshift, we can scroll down to the database category and we can see Amazon Redshift here. So if we select on that, we’re then taken to this splash screen here. So I don’t have any Redshift clusters at the moment.</p>
<p>So to start with, all I need to do is to click on the orange create cluster button, and this takes us to the configuration page. And the first thing we need to do is give it a name. So I’m just going to call this my cluster. Then we can choose our node type. So we have the RA3 nodes and the dense compute nodes here. Now, AWS recommends the RA3 nodes due to the high performance and the managed storage aspect, or you have the dense compute nodes here.</p>
<p>Now, for this demonstration, I’m just going to select a dense compute node. Now, if you scroll down, we can select the number of nodes that we would like. As you can see, it ranges from one to 32, so we can scroll up or down. And also if you see in this configuration summary section the cost per month, and the total compressed storage will also change with the amount of nodes that I select. So there you can see it is scrolling up and down. I’m just going to leave it as two nodes.</p>
<p>Now I can scroll down to the database configurations. We can give the database a name, and also the port that it’s going to be using, and also a master username and password. So let me just enter a password. And then we have cluster permissions. Now, this is an optional step. So if you want your AWS Redshift cluster to interact with other AWS services on your behalf, for example, maybe Amazon S3, you might want to import data, then you can associate an IAM role that has access to S3 to allow that process to happen. But as I said, this is an optional component.</p>
<p>Now, at the very bottom here, we have additional configuration. Now, these are the default settings. So we have a default network, default backup options, maintenance, default security groups, and also a parameter group, as well. But if you turn off those default settings, then you can go through and modify any of those components. For example, network and security. You can select the VPC for it to run in. You can select the security groups that are associated with your clusters to define what resources can access it. You can also define a subnet group which defines what subnets that the clusters will be launched in and also any availability zones. You can also specify if you want any cluster traffic to purely route through your VPC and if you want your cluster to be publicly accessible or not. So there’s a few network and security features that you can change there.</p>
<p>Looking at database configurations, here you can select a parameter group if you have any configured, and you can also configure any encryption using AWS KMS, and if you want to use the default Redshift key, or if you want to use one of your own CMKs, for example, I have a CMK here in my account. For this demonstration, I’m just gonna disable encryption.</p>
<p>Under maintenance, you can set a maintenance window so that the day and time of the week that any maintenance will be carried out to your cluster. And also you can specify which cluster version you’d like, and you have three options. Either use the most current approved cluster version, use the cluster version before the current version, or use the cluster version with beta releases of new versions. I’ll just leave that as current.</p>
<p>Under monitoring, you can have CloudWatch alarms. So for example, you can create a new alarm for disk usage threshold when that reaches 80%, and then you can notify people via an SNS topic that you might already have configured. I’ll say no alarms. And finally, backup. And also you can specify your snapshot retention, which is how long you’ll keep the backups for. And finally, if you want to configure cross-region snapshot, you can either enable that or disable it. And this will back up your cluster to a different region. So if you enable it, you can then select an alternate region to where your cluster currently resides. I’m just going to disable that.</p>
<p>So there are the different options that are available, but I’m just going to select the defaults that it already suggested. And then once you’re happy with your settings, simply click create cluster. As we can see here now, it’s now creating our cluster. This might take a few minutes, so I’ll come back when that’s done.</p>
<p>Okay, as you can see, the cluster is now available. If we select the dashboard, then we can see that we have one new cluster in the Ireland region with two nodes, and we can see that it’s already taken an automated snapshot, as well.</p>
<p>So cluster overview here, so we can see a number of queries, any database connections, disk space used, CPU utilization. As you can see, there’s not much going on at the moment. We’ve simply just created it. If we have any alarms, and down here, any events, and also a query overview here. So I won’t go into any more detail than that.</p>
<p>This is just a very high-level, quick introduction on how to create an Amazon Redshift cluster. And that’s it.</p>
<h1 id="Amazon-DocumentDB-With-MongoDB-Compatibility"><a href="#Amazon-DocumentDB-With-MongoDB-Compatibility" class="headerlink" title="Amazon DocumentDB (With MongoDB Compatibility)"></a>Amazon DocumentDB (With MongoDB Compatibility)</h1><p>Hello and welcome to this lecture looking at Amazon DocumentDB. This <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/working-with-aws-databases-274/">database service</a> runs in a Virtual Private Cloud and is a non-relational fully managed service, which again is highly scalable, very fast, and much like many other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services conforms to levels maintaining high availability. It will be of no surprise, as the name implies, that DocumentDB is a document database, which provides the ability to quickly and easily store any JSON-like document data which can then be queried and indexed. Indexing enhances the speed of retrieving data within a database thanks to an indexing data structure stored within the database.</p>
<p>The ability to scale within AWS is important across all services, and DocumentDB has the ability to scale both its compute and storage separately from each other. This decoupled approach creates a flexible scaling pattern allowing you to scale the resource as when you need to.</p>
<p>As your database grows in size, Amazon DocumentDB will automatically increase the size of your storage by 10G each time, up to a maximum of 64TB to ensure that you do not run out of storage space.</p>
<p>Amazon DocumentDB has full compatibility with MongoDB, which again is another document database, meaning that if required, you can easily migrate any existing MongoDB databases you might have into Amazon DocumentDB using the Database Migration Service . With this compatibility with MongoDB, it means you don’t have to update any of your code in your applications or modify any toolsets that you are using, making this a simple transition in Amazon DocumentDB if you decide to migrate your database.</p>
<p>The AWS Database Migration Service allows you to connect to a source database, read the source data, format the data for consumption by a target database, and then load the data into that target database. The AWS Database Migration Service can migrate your data to and from commercial and open-source databases.</p>
<p>For more information on the AWS Database migration service, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-the-aws-database-migration-service/dms-introduction/">here</a>.</p>
<p>Let me now explain the architecture of Amazon DocumentDB. Firstly, I want to cover the base architecture of the service, and If you are familiar with Amazon Neptune, then architecturally Amazon DocumentDB is similar in many ways.</p>
<p>The database itself is comprised of a core component, a cluster, and this cluster is composed of a single or multiple DB instances, up to 16 in total, which can span across different availability zones within a single region. Across this cluster is a shared cluster storage volume that supports every instance within the cluster, meaning that every instance sees the same storage volume.</p>
<p>The instances backing the DocumentDB cluster provide the processing power to support read and write requests against the cluster storage volume, and as I mentioned you can have up to 16 DB instances. There will only ever be a single primary DB instance performing write operations in the cluster at any one time. The remaining instances within the cluster if they have been configured will all be read replica instances, serving only read requests.</p>
<p>Read replicas, as we have seen in previous AWS database services in this course series helps to reduce the load on the primary database instances by processing read requests from clients. As a result, DocumentDB is able to process a very high-volume of these kinds of requests.</p>
<p>DocumentDB supports up to 15 read replicas across different availability zones within the same region, much like Amazon Neptune, and also shares the same underlying storage of the primary instance across a shared volume.</p>
<p>The Primary DocumentDB instance will be responsible for both read and write operations. However, the replicas will only process read requests to the cluster volume. As the replicas connect to the same source data as the primary, any read query results served by the replicas have minimal lag, typically down to single-digit milliseconds. Data is synchronized is maintained synchronously between both the primary DB instance and each replica in the region.</p>
<p>DocumentDB uses a principal of endpoints to connect to different components of your database. An endpoint is a URL address with an identified port that points to your infrastructure. There are three different types of DocumentDB endpoints, these being: Cluster endpoint, Reader endpoint, and Instance endpoint.</p>
<p>Cluster Endpoints: Each DocumentDB database will have a cluster endpoint that is associated with the current primary DB instance of the cluster. This endpoint should be used by any applications that require both read and write access to the database. Should a failure of your primary DB instance occur, then DocumentDB will promote either a read replica to the primary DB or if no read replica is configured, DocumentDB will create a new primary instance. When this happens, the cluster endpoint will then point to the new primary instance without any changes required by your applications accessing the database.</p>
<p>Reader Endpoints: A Reader endpoint allows connectivity to any read replicas that you have configured within the region. Applications can use this endpoint to access your database for read requests, typically when performing a query. Only a single reader endpoint will exist, even if you have multiple read replicas. as a result, DocumentDB will manage the forwarding of any read requests onto a specific read replica associated with the primary DB.</p>
<p>Instance endpoints: For every instance within your cluster, including your primary and read replica instances, they will each have their own unique instance endpoint that will point to its own host. This allows you to direct certain traffic to specific instances within the cluster, you might want to do this for load-balancing reasons across your applications reading from your replicas.</p>
<p>DocumentDB performs automatic backups for you based upon a schedule created during the creation of your database. A feature of these automatic backups allows you to restore back to any point in time during your retention period, known as point-in-time-recovery. As a part of the automated daily backup process, DocumentDB captures any transaction logs that have been created as and when updates to your Database were made, this is to ensure it can perform a point-in-time-recovery. These backups are automatically stored on Amazon S3 for durability and availability.</p>
<p>The automated backups themselves are performed daily, and the backup retention period determines how long DocumentDB will keep and maintain your backups for and can be set between 0 and 35 days. For automatic backups to take place, the retention period must be set to at least one day. If the retention is set to 0 then automatic backups will not take place and you will not be able to perform point-in-time restores. If you did have it configured, then your point-in-time restores can take place for any duration within the retention period.</p>
<p>The backup window allows you to define a time period in which the backup snapshot could be taken. This allows you to set it at a time when the database itself will be of low utilization to prevent the backup process impacting the performance of the database itself.</p>
<p>In the next lecture, I will be looking at how to create a new Document DBCluster, which will cover these backup settings in addition to further configurational changes, so let’s move on and take a look.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="DEMO-Creating-an-Amazon-DocumentDB-Cluster"><a href="#DEMO-Creating-an-Amazon-DocumentDB-Cluster" class="headerlink" title="DEMO: Creating an Amazon DocumentDB Cluster"></a>DEMO: Creating an Amazon DocumentDB Cluster</h1><p>Hello and welcome to this lecture. We’re all going to be showing you how to set up an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> document DB database. From the management console, we need to select Amazon DocumentDB from the Database category.</p>
<p>Now as you can see I don’t have any clusters set up at the moment and it’s very simple to get going to create one. So let me go ahead up to create and the first thing I need to do is to enter a cluster identifier. So this is just a unique cluster name so I’ll just call this MyCluster. And then we can select our instance class. So we have a number or different instances down here all different sizes with different number of VCPUs and RAM quantity. I’m just gonna select the smallest. Then we can have the number of instances. Anything from one to 16. So there’ll be one primary and the others will be replicas. Let’s just leave that as three. </p>
<p>Then we need to enter a master username and password so we just add that in now. And it gives us an estimated hourly cost for the instance size that we selected. So here we can see that it’s 92 cents per hour. If we select down here show advanced settings then we have more configuration options that we can change. So we can select which VPC that we would like it in. We can select the subnet group to define which subnets the instances will be launched in. And also associate any VPC security groups for those instances as well.</p>
<p>If we go down to cluster options we can specify the port and if we have any parameter groups configured we can select parameter group there. Over here you can see parameter groups where you can specify a template of pre-configured options for your database as explained previously. We can look at encryption-at-rest. So we can enable encryption using a default master key, or we can select one of our pre-configured CMKs that we have with KMS. And we can see the account and KMS key ID that’s being used. So we can have that enabled or disable the encryption. I’ll leave that enabled and use the default master key.</p>
<p>If we look at the backup options we can specify retention period. Anything up to 35 days. And also the backup window as well which is the daily time range in which automated backups will be created. We can also export different logs over to Amazon CloudWatch and this will use the predefined IAM role.</p>
<p>For maintenance, we can specify a specific maintenance window to start at a specific day and time and duration. If you have any preference simply select no preference. Again you can add any tags that you’d like for your cluster and enable deletion protection which prevents the cluster from being accidentally deleted. Once you’re happy with your configure options simply click create cluster. As we can see the status is currently creating. That will just take a couple of minutes so I’ll come back when that is active.</p>
<p>Okay, you can now see the status is available. We have three instances. If we take a look at the cluster we can see the engine version. We have some details here on how to connect the database and we have our security group here.</p>
<p>If we look at configuration we can see the ARN details of the database. We can see when it was created, the cluster endpoints, and the reader endpoints, and many other relevance of configuration that we defined during the creation.</p>
<p>If we look at monitoring we can see some CloudWatch statistics, but as we’ve only just created this database, there’s not enough data available, and finally any events and tags that have been detected. If we go to instances we can see the three instances that have been created for my cluster. As you can see these instances are still being created even if a cluster has been created. So that will just take a few more minutes.</p>
<p>I’ve just waited a few minutes and we can see that we now have two of the instances available and it’s just creating the last one. So as you can see it’s a fairly simple process and follows a lot of the previous databases that we’ve looked at in its general configuration set up. We created the cluster and also as part of the process, created three different instances. One as a writer which is the primary, and then two reader replicas.</p>
<h3 id="Lectures-1"><a href="#Lectures-1" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="Amazon-Keyspaces-for-Apache-Cassandra"><a href="#Amazon-Keyspaces-for-Apache-Cassandra" class="headerlink" title="Amazon Keyspaces (for Apache Cassandra)"></a>Amazon Keyspaces (for Apache Cassandra)</h1><p>Hello, and welcome to this short lecture, in which we’ll look into the final <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/working-with-aws-databases-274/">database service</a> of this course series, Amazon Keyspaces for Apache Cassandra. Firstly, let’s answer the question that some people ask when seeing this service. What is Apache Cassandra? To summarize it quickly, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Apache_Cassandra">Wikipedia</a> explains that “Apache Cassandra is a free, open-source, distributed, wide column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.” </p>
<p>So now we have a high-level awareness of Amazon Cassandra. Let’s see how Amazon Keyspaces fits into this. Keyspaces is a serverless, fully-managed service designed to be highly scalable, highly available, and importantly, compatible with Apache Cassandra, meaning you can use all the same tools and code as you do normally with your existing Apache Cassandra databases.</p>
<p>Being a serverless service. It removes the need for you to provision, patch, and manage instances yourself. Instead, all of this is taken care of by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> on your behalf. Boasting unlimited throughput, Amazon Keyspaces is designed for massive scale solutions, allowing you to service business-critical workloads requiring thousands of requests per second. The key features of Amazon Keyspaces is that it can offer extreme performance, scalability, and elasticity, and grows at the rate of demand for your applications, ensuring you only pay for what you use.</p>
<p>Traditionally, Cassandra architectures are comprised of a cluster of nodes, which have to be created, provisioned, managed, patched, and backed up by you. As your Cassandra database grows, so does the amount of nodes, leading to greater administrative resources in managing the infrastructure. Using Amazon Keyspaces removes the need for you to manage this infrastructure, and instead you can focus on the business logic of the database and your applications that interact with it to ensure you are getting the best performance possible.</p>
<p>Amazon Keyspaces is a great choice if you’re looking to build applications where low latency is essential, for example, route optimization applications or trade monitoring. And of course, if you’re looking for an easier way of managing your existing Cassandra databases prices in the cloud without the burden of maintaining your own infrastructure.</p>
<p>To help understand the service in greater detail, let’s look at some of the components of the service.</p>
<p>First let me explain the difference between keyspaces and tables. In Cassandra, a keyspace is essentially a grouping of tables that are related and are used by your applications to read and write data. Also, the keyspace in Cassandra also helps to define how your tables are replicated across multiple nodes in the cluster. However, because Amazon Keyspaces is fully managed and serverless, the entire storage layer is abstracted from being administered and configured by us as customers. Instead, it is managed by AWS. And so here, the keyspace component in Amazon Keyspaces exist in their logical meaning rather than holding the responsibility for us to manage any kind of replication.</p>
<p>Tables are where your database writes are stored, effectively, the data that is held within your database. In each table, there will be a primary key that consists of a partition key and one or more columns. When a new table is created, encryption at rest is automatically enabled, and any clients that want to connect to your tables will require a transport layer security connection for encrypted in transit connectivity.</p>
<p>In the next lecture, I will show you how to set up a keyspace and then a table that will reside within that keyspace. Much like Amazon DynamoDB, Keyspaces offers two different throughput capacity modes when working with your read and writes to and from your tables. These options allow you to customize how your throughput is managed, helping you to optimize it for your workloads.</p>
<p>The options available are on-demand and provisioned. On-demand throughput capacity is a default option when creating your tables and is capable of processing thousands of requests per second. The pricing for this option is based upon the number of read and writes made against your tables by your applications, meaning you only pay for what you’re using.</p>
<p>As your workload fluctuates, it is able to scale to any increased throughput that the database has previously reached instantaneously. However, if additional throughput is required above and beyond existing thresholds, then Amazon Keyspaces works quickly to respond to meet the needs required by your applications.</p>
<p>As a result, this can be a good selection for your throughput if you’re dealing with unknown or unpredictable workloads.</p>
<p>Provisioned throughput capacity is a better choice for you if you are dealing with more predictable workloads, which allows you to specify your predicted number of reads and writes per second, which would enable your tables to meet those throughput speeds faster than on-demand would. You can also use automatic scaling to alter the change of throughput if you experience fluctuation, or as your database naturally grows, using upper and lower the thresholds.</p>
<p>When working with Amazon Keyspaces, you’ll need to use CQL, the Cassandra Query Language, which is the language you use to communicate with your Amazon Keyspaces. In many respects, it is similar to SQL, structured query language. And as a result, this helps to reduce the learning curve when moving from a relational database using SQL, such as MySQL.</p>
<p>There are a number of ways to run queries using CQL. Firstly, from within the Amazon Keyspaces dashboard within the AWS management console, you can use the CQL editor, which can return as many as a thousand records per query. If you are querying more than a thousand records, then you will need to run multiple queries together. You can run them on a CQLSH client, and more information on this can be found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.cqlsh.html">here</a>, or you can run them programmatically using an Apache 2 licensed Cassandra client driver (more info <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.html">here</a>).</p>
<p>In the next lecture, I’ll be demonstrating how to create a keyspace and then a table within that keyspace, so let’s take a look.</p>
<h3 id="Lectures-2"><a href="#Lectures-2" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="DEMO-Creating-a-Keyspace-and-Table-in-Amazon-Keyspaces-for-Apache-Cassandra"><a href="#DEMO-Creating-a-Keyspace-and-Table-in-Amazon-Keyspaces-for-Apache-Cassandra" class="headerlink" title="DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)"></a>DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</h1><p>Hello and welcome to this lecture, which will be a demonstration on how to create an Amazon Keyspaces database. And what I shall do, I shall create a keyspace and then a table that resides in that keyspace. So let’s take a look.</p>
<p>So firstly, scroll down to the database category and then select Amazon keyspaces. As you can see, I don’t have any keyspaces set up at the moment so I just need to click get started. Now much like in the QLDB demonstration, Amazon keyspaces also has a quick getting started guide that gives you a quick tutorial on how to create a keyspace, then how to create a table in your keyspace and then how to populate it with data and query your data.</p>
<p>As this is a fundamental level course, we’re just going to create the keyspace and then create a table. Now I’m not going to use the getting started guide but if you’d like to explore this service further, then I recommend you going through those four steps.</p>
<p>Okay, so to create a keyspace we can select keyspaces on the left side here. Now remember a keyspace is essentially going to be a grouping of tables, that will be used by your applications to read all my data to. And it also defines how the replication is managed as well.</p>
<p>So from here, we simply click create keyspace. Now we need to give it a name, I’m just gonna call this; my keyspace. We can add any optional tabs if we’d like to. And this is the query that will be executed to create the keyspace that we specified above. So we can say this is going to create the name of my keyspace and it will also manage the replication across a single region strategy. So <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> will configure this replication for us across multiple availability zones. Click create keyspace and then we can see it’s active. So it’s very easy to create the keyspace.</p>
<p>Now we need to create a table within that keyspace. So if we now select that keyspace, we can see that it doesn’t have any tables. Now the table itself, is where your database rights are stored. So now we need to create a table within the keyspace. So select create table. We can select the keyspace that we want this table to belong to. As we only have one, that’s the my keyspace that we just created.</p>
<p>Now we can enter a table name. I’ll just call this my table. Here, we can enter the schema information. So we can enter any columns that we want, for example; product name and then the type field for that column. As you can see, there’s lots of different types that you can use. For this, I’ll just use the character string. And if we want more than one column, we can add another column. For example; product ID, and this can be a decimal field.</p>
<p>Now we can add our partition key and we can see that a partition key is composed of one or more columns that are used to uniquely identify rows within a table. So we can select the columns that we’d like to act as the partition key. So as you can see, you can add more than one column. We then have clustering columns, which is optional, and this helps you to sort data within a partition.</p>
<p>For this demonstration, I’m not gonna have any clustered columns. Then if you scroll down to the read and write capacity settings, here we have the on-demand and provision modes that I discussed in a previous lecture. So the on-demand option simplifies billing for the actual read and writes that the application performs. Whereas provisioned, you can manage and optimize the reads and writes in advance.</p>
<p>So as you can see for read capacity, we can have this automatically scaling. I can set the minimum capacity units and the maximum capacity units. And when the utilization gets to 70%, it will automatically scale up. Similarly with the write capacity. You can copy the settings that you have above for the read scaling or you can set it manually.</p>
<p>So you can have different scaling patterns for both read and write capacity. I’m just gonna leave this as on-demand capacity mode, just for simplification. And then if we go down even further, we can see our point in time of recovery. We can either enable this or disable. It’s always best to keep this enabled, just to allow you to recover from any incident at any point. Then you can add any optional tags. And finally, this just shows you the query that will be executed to create the table in keyspace, along with the column names and the types and also the primary key. And it also sets your capacity modes and the fact that we have point-in-time recovery enabled.</p>
<p>Once you’re happy with all your options, simply select create table. And that has now created the my table in my keyspace. As you can see, the status is now active and if you select the table, you can see the partition key and your columns as well. And if you need to add any more columns, then you can do so here.</p>
<p>So that was just a very quick overview of how to firstly create a keyspace and then create a table. And finally, remember you can add additional tables to a keyspace. So if we go back into our keyspace, you can then go ahead and create an additional table within the same keyspace. And that’s it for this demonstration.</p>
<h3 id="Lectures-3"><a href="#Lectures-3" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="Amazon-Quantum-Ledger-Database-QLDB"><a href="#Amazon-Quantum-Ledger-Database-QLDB" class="headerlink" title="Amazon Quantum Ledger Database (QLDB)"></a>Amazon Quantum Ledger Database (QLDB)</h1><p>Hello and welcome to this lecture, covering Amazon Quantum Ledger Database, which is also known as Amazon QLDB. This was released in September of 2019.</p>
<p>So to start with, what actually is Amazon QLDB? It’s yet another fully managed and serverless <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/working-with-aws-databases-274/">database service</a>, which has been designed as a ledger database. This has a whole host of use cases. One quick example would be for recording financial data over a period of time. QLDB would allow to maintain a complete history of accounting and transactional data between multiple parties in an immutable, transparent and cryptographic way through the use of the cryptographic algorithm, SHA-256, making it highly secure.</p>
<p>This means you can rest assured that nothing has changed or can be changed through the use of a database journal, which is configured as append-only. Essentially, the immutable transaction log that records all entries in a sequenced manner over time. This service therefore negates the need for an organization to develop and implement their own ledger applications.</p>
<p>This may sound similar to blockchain technology where a ledger is also used. However, in blockchain, that ledger is distributed across multiple hosts in a decentralized environment, whereas QLDB is owned and managed by a central and trusted authority. This removes the requirement of a consensus of everyone across the network, which is required with blockchain.</p>
<p>Often, ledger applications to fulfill these requirements are added to relational databases, and this quickly becomes difficult to manage since they are not immutable, which makes errors difficult to trace, especially during audits.</p>
<p>I mentioned earlier that QLDB is serverless. So again, the administration of having to maintain the underlying infrastructure is removed and all scaling is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, which includes any read and write limitations of the database.</p>
<p>Amazon QLDB is great for scenarios where you can maintain an accurate record of changes requiring the utmost integrity assurance. So to help get an understanding of how QLDB is used across different industries, let me take a look at a couple of examples and use cases for the service.</p>
<p>So QLDB would be a great fit within the insurance industry, which a claim by its nature can be a long winded and extensive process involving many different parties and operations over a long time period. You could implement different processes, systems and applications to track, audit and record the claim history via relational databases and custom auditing mechanisms verifying the validity of the records. However, this could all be replaced with Amazon QLDB. Using an immutable append-only framework, prevents the ability to manipulate previous data entries, which helps to prevent fraudulent activity.</p>
<p>Another use case where QLDB would be a good fit is within the human resources field, including payroll processes. Accuracy and auditing is essential when it comes to employee data, which is often confidential, including payroll information. HR tracks and records an employee’s history covering their performance, benefits, training, remuneration and much more. Having a clearly defined verifiable employment history that can be encrypted, trusted, and reliable containing all elements of the individual held centrally makes Amazon QLDB a great fit.</p>
<p>So we can see that Amazon QLDB is really about maintaining an immutable ledger with cryptographic abilities to enable the verifiable tracking of changes over time. There are many different use cases where this can be used, and I’ve just highlighted a couple here to provide more of an understanding of how this would be used.</p>
<p>Let’s now take a look at some of the concepts and components that make up the service.</p>
<p>Firstly, and as I’ve mentioned a few times already during this lecture, QLDB is based upon a ledger. So let’s look at the structure of a ledger database.</p>
<p>Data for your QLDB database is placed into tables of Amazon ion documents. Now these ion documents have been created internally at Amazon and on open-source, self-describing data serialization format, which is a superset of JSON, JavaScript Object Notation. This means that any JSON document is also classed as a valid Amazon ion document. The document is also allowed store by structured and unstructured data.</p>
<p>So going back to the tables, they all effectively compromise of a group of Amazon ion documents and their revisions. As with most documents, when a revision is made, it usually signifies a change, an update, a replacement. Basically, something changes to the document, making a revision. Now, as we know, QLDB by design maintains an audit history of all changes and so that revision is saved in addition to all previous versions of the same ion document. This journal of transactional changes allows you to easily query document history across all document iterations.</p>
<p>Changes to these documents are done so via database transactions. In doing this transaction, Amazon QLDB will read data from its ledger, perform the update as required, and then save the changes to the journal. To be clear, the journal acts as an append-only transactional log and maintains the source of truth for that document and the entire history of changes to that document, ensuring that it remains immutable.</p>
<p>Each time a change is committed to the journal, a sequence number is added to identify its place in the change history. In addition to this, an SHA-256 bit hash is used for verification purposes, which creates a cryptographic digest file of the journal. Now this identifies an encrypted signature of the changes made to your document and the history of your entire document at that point in time. This can then be used to verify the integrity of the changes made in relation to the digest file created. This helps to ensure that the data within your document has not been altered or changed in any way since it was first written to in QLDB.</p>
<p>For a deeper understanding of how this whole process works, please review the following: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/qldb/latest/developerguide/verification.html">https://docs.aws.amazon.com/qldb/latest/developerguide/verification.html</a></p>
<p>Now we have a base understanding of the ledger itself through the use of tables and documents, including the ledger. Let me now take a look at how storage is used for QLDB.</p>
<p>Amazon QLDB uses two different methods of storage, each for very different uses, these being journal storage and index storage.</p>
<p>Journal storage is the storage that is used to hold the history of changes made within the ledger database. So this will hold all of the immutable changes and history to the ion documents within your table.</p>
<p>Index storage on the other hand is the storage that is used to provision the tables and indexes within the ledger database and it’s optimized for querying.</p>
<p>With QLDB being a fully managed serverless service, this storage is managed for you and there are no specifications to select or make during the creation of your ledger database. In the next lecture, I will show you how simple it is to create a ledger and load some sample data into the database.</p>
<p>The final point I want to cover with Amazon QLDB is, is integration with Amazon Kinesis through the use of QLDB streams.</p>
<p>Amazon Kinesis makes it easy to collect, process, and analyze real-time streaming data so you can get timely insights and react quickly to new information. With Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, your data lakes and data warehouses, or build your own real-time applications using this data. Kinesis enables you to process and analyze data as it arrives and respond in real-time instead of having to wait until all your data is collected before the processing can begin.</p>
<p>Using QLDB streams, you are able to capture all changes that are made to the journal and feed this information into an Amazon Kinesis data stream in near real-time. This allows you to architect solutions whereby other AWS services could process this data from Kinesis to provide additional benefit. For example, this is a great way to implement event-driven architectures. Event-driven architectures are triggered by events that occur within the infrastructure.</p>
<p>So in this case, suppose your ledger contained financial accounts recording transactions and in this instance, an event could be a Lambda function that triggers an SNS notification to an account owner when a finance balance drops below a certain threshold following an update that has been made to the journal.</p>
<h3 id="Lectures-4"><a href="#Lectures-4" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="DEMO-Creating-a-Ledger-using-Amazon-QLDB"><a href="#DEMO-Creating-a-Ledger-using-Amazon-QLDB" class="headerlink" title="DEMO: Creating a Ledger using Amazon QLDB"></a>DEMO: Creating a Ledger using Amazon QLDB</h1><p>Hello and welcome to this lecture. This is going to be a quick demonstration to show you how to set up an Amazon QLDB Ledger database. It’s very quick and it’s very simple. So let’s take a look.</p>
<p>Firstly if we scroll down to the database category and select Amazon QLDB. Now before I create a new ledger I just want to show you the getting started option. So if you select on getting started on the left here, it comes with a tutorial to help you get set up for your first QLDB database. So it will show you how to create your first ledger, how to load some sample application data, how to query and modify your data, and also verify a document. So that might be something that you want to look into just to have a play with and get used to the database. However, let me show you how to create a ledger.</p>
<p>So if we go back to ledgers, as you can see we have none here. Select create ledger. Let’s give it a name. I’ll just call it MyLedger and then you can add any optional tags if you’d like to. Then simply click on create ledger. And as you can see the status is creating. That’ll only take a few seconds. Okay, now you can see this is now active.</p>
<p>So our ledger is active. If we take a look we can see the ID, the journal size, the index storage size, and the ARN and the region, et cetera. And also in the CloudWatch metrics. But as we’ve only just set this up, there’s no data available at the moment.</p>
<p>Now your ledger is created. You can then start creating tables, etc. Now if we go back to the getting started section under the sample application data you can select your ledger that you just created and load some sample data in there or you can use a manual option to create tables, indexes, etc. So let me just show you that manual option.</p>
<p>So if you select the link it will actually take you to some <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation and it will show you the automatic option where it will pre fill your ledger with some vehicle registration data or you can select the manual option as well. So this will just guide you through on how to create tables using QLDB and populate your database.</p>
<p>So as you can see it’s very easy to get started with QLDB and there’s some great tutorials here for you as well to guide you through exactly how to set it up, run queries, modify documents, et cetera. But for this demonstration, I just wanted to show you the dashboard and how to create that initial ledger which as you can see was a very simple process. Creating the tables and queries and using all the functions of QLDB is out of scope of this fundamentals course.</p>
<h3 id="Lectures-5"><a href="#Lectures-5" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="What-is-a-Data-Lake"><a href="#What-is-a-Data-Lake" class="headerlink" title="What is a Data Lake?"></a>What is a Data Lake?</h1><p>What is a data lake? A data lake is a place for your business or enterprise to store and collect data. The data you store in your data lake may be structured or unstructured, meaning it can have a defined schema or not. </p>
<p>The goal of our data lake is to have a single place where all of our business information can exist, and eventually, we can have some type of analytics performed on it. This data can be from our transactional systems and line of business applications. It could also be from various IoT devices, mobile applications, and even social media.</p>
<p>Companies that are able to aggregate, and work on their data, and derive meaning from it will be able to outperform their peers. These companies might do so through the use of generic data analytics or even by using machine learning to provide valuable insights.</p>
<p>This is why it is important to manage and create a safe place for all your data to live, A.K.A. a data lake.</p>
<h1 id="What-is-the-Difference-between-a-Data-Lake-and-a-Data-Warehouse"><a href="#What-is-the-Difference-between-a-Data-Lake-and-a-Data-Warehouse" class="headerlink" title="What is the Difference between a Data Lake and a Data Warehouse?"></a>What is the Difference between a Data Lake and a Data Warehouse?</h1><p>What is the difference between a data lake and a data warehouse?</p>
<p>When first getting into this space there might be some confusion between data lakes and data warehouses. That is fairly common.</p>
<p>The main difference between a data lake and a data warehouse is specificity and structure. </p>
<p>A data lake is a formless blob of information, it is a pool of knowledge where we try to capture any relevant data from our business so that we can perform analytics on it.</p>
<p>A data warehouse is a specialized tool that allows you to perform analysis on a portion of that data, so you can make meaningful decisions from it. Generally, it is a subset of the data from the data lake with a specialized purpose. Your data warehouse Is an optimized database that is dealing with normalized, transformed, and cleaned-up versions of the data from the data lake.</p>
<h1 id="Why-Can’t-We-Just-Store-All-This-Information-Into-a-Data-Warehouse"><a href="#Why-Can’t-We-Just-Store-All-This-Information-Into-a-Data-Warehouse" class="headerlink" title="Why Can’t We Just Store All This Information Into a Data Warehouse?"></a>Why Can’t We Just Store All This Information Into a Data Warehouse?</h1><p>Why can’t we just store all this information into a data warehouse?</p>
<p>Well, This was exactly what was happening for a long period of time. Unfortunately, as the speed of business has increased, and so has the sheer volume of data. Data warehouses were unable to keep up with the amount of curating and scaling required to support such volumes of data. It was becoming cost-prohibitive and slowing down query speed to try and maintain all of this data in an active database.</p>
<p>So the advent of the data lake became a necessity. We needed a place where we could store large volumes of information for cheap.</p>
<h1 id="What-Makes-Up-a-Good-Data-Lake"><a href="#What-Makes-Up-a-Good-Data-Lake" class="headerlink" title="What Makes Up a Good Data Lake?"></a>What Makes Up a Good Data Lake?</h1><p>What makes up a good data lake?</p>
<p>A good data lake will deal with these five challenges well: Storage (the lake itself), Data Movement (how the data gets to the lake), Data Cataloging and Discovery(finding the data and classifying it), Generic Analytics (making sense of that data), and Predictive analytics ( making educated guesses about the future based on the data).</p>
<p>Storage: Let’s take a look at storage first. The reason people moved into using data lakes was that storage costs were becoming burdensome because the sheer volume of data was starting to crush people. What service does AWS offer that can easily deal with large and crushing volumes of raw data? Well, the first thing I would think about would be something like S3. </p>
<p>S3 is particularly good in this scenario, not only because it can deal with the large volume of data, but also because it can handle unstructured data. You could fill it with log files, json transaction documents, blobs of binary output, it takes anything. A normal database would not be particularly suitable for this task.</p>
<p>The other benefit of using S3 is that we can set up lifecycle policies to help deal with the cost of the ever-increasing data burden. This allows us to put infrequently accessed data into a cheaper storage tier, and even to eventually put it into glacier ( the deep archival service) when we are fairly certain that, that data isn’t going to be used for a long while. We can of course return the cold data back into S3 standard if we ever need to.</p>
<p>Data Movement: Another important thing to figure out when building your data lakes is how the heck are you planning to actually get your data into it. We know that S3 is what we should use for storage, but what mechanisms do we want to use to get all the stuff… into s3?</p>
<p>We could of course manually move large folders of archived log data into whatever bucket we are using for our data lake, but that idea is not super scalable and honestly just feels bad. It would be great to automatically push our business data into this bucket.</p>
<p>There are a few ways of getting your data into your bucket, be it from actively streaming your data with kinesis, to using a direct connection from on-premises to bring in large quantities of data, or using the database migration service to move your database information into s3, or you might even have to have snowball devices delivered to some faraway outpost once a month to collect research data to have sent back to AWS. </p>
<p>Whatever your method, you will need a way to move your data into AWS, and you will prefer that whatever way you use is automated.</p>
<p>Data Cataloging and Discovery: Once you have all of your data within your data lake (your s3 bucket of choice) It becomes necessary to start cataloging and understanding the types of data you have. If we do not spend at least a little time working through our data and managing it, we will quickly turn our data lake into a data swamp.</p>
<p>Think about what would happen as you add terabytes to petabytes of data, folders, and folders of the stuff, into the same bucket. As you do this over long periods of time, your knowledge of what is what and where it lives will fade. This makes it near impossible for anyone else to find specific data sets they want to work with. </p>
<p>This is why we need to catalog our data. We need to create some data about the data - metadata. This will help future persons discover what it is they need from our data lake, without them having to spend hours, days, or weeks trying to figure out where or what it is.</p>
<p>Things that might be helpful to know for example is what formats are the various data stored in - is it mostly JSON, CSV, Parquet… it is compressed data, is it sensitive data? And maybe you might want to add additional tags, like this is data from Twitter, or from customer reviews.</p>
<p>There are many ways you can go about this to create your own data catalog. For example, you might have an upload event on your s3 bucket that triggers a lambda function to store some metadata information in DynamoDB about the new data that was just uploaded. </p>
<p>From there we could push that information into ElasticSearch to browse through and query that data. This is a very do-it-yourself approach and could be a little tricky to get set up correctly.</p>
<p>I would recommend instead that you take a look at AWS glue. This service is a managed transform engine that allows you to run ELT pipelines - but for our uses, it also contains a very robust data catalog that we can leverage. </p>
<p>The glue data catalog even contains built-in crawlers that can crawl through various data sources and automatically populate the catalog for you. This includes your S3 buckets, databases, and data warehouses. They can be scheduled to run at certain times or based on events like new upload into that s3 bucket.</p>
<p>Analytics: Why would we be collecting all this data if we did not want to know information about that data. Our data is a record of the past and that record can give us great insights into what was successful and what was a failure for our business.</p>
<p>There are a number of great AWS services that can help you start to make sense of your data. These services range in their analytical ability and what their goals are. </p>
<p>For example, if you wanted to get some real-time information about your data lake, or at least the information being streamed into it from kinesis or Amazon MSK for example, you can use Kinesis Data Analytics to get a real time feed of what your streaming data is up to.</p>
<p>If you were looking to interactively scrub through your data we have Amazon Athena, a purposely built service that makes it easy to analyze data in Amazon S3 using standard SQL.</p>
<p>If you have some section of your data that you want to create dashboards and graphs for, that’s where something like Amazon Quicksight can be added to your solution.</p>
<p>And, we also have data warehousing services like redshift that you can place a subset of our data lake within to perform general analytics on to try and derive some meaning from that data.</p>
<p>Predictive analytics: Being able to perform predictive analytics will allow you to gain some possible future insight into your business though your data. You can start to build out systems that help with this through the use of machine learning services.</p>
<p>One of the most important things for machine learning is having a robust data set to work with. This is why it works so well to have a data lake where you can pull subsets of data from.</p>
<p>Amazon offers AWS sage maker as a quick way to get into creating, training, and running your own models within AWS. </p>
<p>Additionally, aws has a series of deep learning AMI that come pre-configured with popular deep learning frames and interfaces. This included TensorFLow, PYtorch, Apache MXNet, Chainer, Gluon, Horovod, and Kera. There are no additional charges for using these AMIs, they are still pay-as-you-go like other instance types.</p>
<h1 id="How-do-I-Actually-Build-a-Data-Lake"><a href="#How-do-I-Actually-Build-a-Data-Lake" class="headerlink" title="How do I Actually Build a Data Lake?"></a>How do I Actually Build a Data Lake?</h1><p><strong>Ok, so how do I actually build a data lake?</strong></p>
<p>So there are two ways you can actually go about creating your data lake. You can try to assemble all of these interconnected data lake pieces by hand; which can take quite a bit of know-how and a lot of time.</p>
<p>There are also a few deployable templates floating around from AWS that can help with this process - take a look over here to see a template and an architecture build guide: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/solutions/latest/data-lake-solution/architecture.html">https://docs.aws.amazon.com/solutions/latest/data-lake-solution/architecture.html</a></p>
<p>Or we can use the AWS Lake formation service, which promises to make setting up your secure data lake take only a matter of days, instead of weeks or months.</p>
<p>It does this by identifying existing data sources within Amazon S3, relational databases, and NoSQL databases that you want to move into your data lake. It then will crawl and catalog and prepare all that data for you to perform analytic on. You can also target log files from things like CloudTrail, Kinesis Fire Hose, Elastic Load Balancers, and CloudFront. All this data can be grabbed all at once, or it can be taken incrementally. </p>
<p>All of this functionally is managed by using ‘blueprints’ where you simply:</p>
<ol>
<li>Point to the source data</li>
<li>Point where you want to load that data in the data lake</li>
<li>Specify how often you want to load that data</li>
</ol>
<p>And the blueprint:</p>
<ol>
<li>Discover the sources table schema</li>
<li>Automatically converts to a new target format</li>
<li>Partitions the data based on partitioning schema</li>
<li>Keeps track of the data that was already processed.</li>
<li>Allows you to customize all the above</li>
</ol>
<p>AWS Lake formation will take care of user security by creating self-service access to that data through your choice of analytic services.</p>
<p>It does this by setting up users’ access within lake formation, by tying data access with access control policies within the data catalog instead of with each individual service. So when a user comes to lake formation to see some data - their credentials and access roles are sent to lake formation, lake formation digests that and determines what data that person is allowed to access, and gives them a new token to carry with them that services like Athena, Redshift, and EMR will honor.</p>
<p>This allows you to define permissions once, and then open access to a range of managed services and have those permissions enforced. </p>
<p>There is no additional pricing for using the Lake Formation service, but you do have to pay for all the services it uses though. This means you have to pay for any AWS Glue usage during the crawling and cataloging phases. You will have to pay for the data residency within S3. You will have to pay for any Athena queries you might make on the data when looking up information.</p>
<p>So while the orchestration of all the services doesn’t cost anything, there are many fees that you should be aware of when architecting your solutions if cost is a concern.</p>
<h1 id="Databases-Summary"><a href="#Databases-Summary" class="headerlink" title="Databases Summary"></a>Databases Summary</h1><p>You’re deep into the learning path now, and we’ve covered off compute, storage, networking, and now databases. So congratulations on getting this far. So again, let’s take a recap of some of the most important elements that you need to be aware of for the exam. For me personally, when I was studying for the AWS Solutions Architect Associate exam, I found databases the hardest aspect to grasp and I’m sure I’m not alone from that perspective but you’ve got through the theory which is the hardest part. Okay, let’s focus on the essential must-know points for the certification, and this mainly covers RDS, DynamoDB, and ElastiCache. So RDS, let’s hit the key points. So what is RDS? It’s a managed relational database service, which means all of the patching required of the underlying instance and software that runs RDS is going to be managed by AWS. </p>
<p>In addition to automated backups of your database. So that again will be managed by AWS as well. So this takes a lot of the administrative burden out allowing you to just work on the data within your databases. So even though automatic backups are taken care of you can still perform your own manual snapshot backups. Now, because it’s managed, it’s quick and easy to set up. It has a range of different database engines available to suit your needs, and if you couple this with a scalable and resizable capacity delivery of high performance then this is a great choice for mobile and web applications in addition to e-commerce applications as well. Now, remember, if you need to build in high availability to your audience database you’ll need to use the Multi-AZ feature of RDS to offer additional resiliency in case your primary instance fails. Now, we will talk more about high availability in a later course in this learning path, but Multi-AZ and read replicas will very likely come up in the exam. </p>
<p>So the previous course gave a good introduction to these features, but we will dive into them more in more detail in a later course. I want to point out Aurora which is one of the database engines of RDS and its compatibility with MySQL and PostgreSQL. It’s highly likely that Aurora will be mentioned in the exam and it’s usually in relation to performance and cost-related questions. Now, this is because it can run fives times faster than MySQL and three times faster than PostgreSQL while still maintaining tight security, reliability, and availability, and its also extremely cost-effective operating at just one 10th of the cost of other databases. Now it’s also capable of running 15 read replicas and it can operate across three different availability zones while continuously backing up allowing to recover to any point in time, so it’s super powerful. So let’s see how RDS compares to DynamoDB. So the first big difference to RDS is that it’s not a relational database service. </p>
<p>It’s a NoSQL key-value database. It’s super fast designed for ultra-high performance and capable of handling up to 20 million requests per second. It’s massively scalable, multi-regional, and can scale with single-digit latency regardless of how big the database gets. Whereas with RDS, the larger the database the slower it can get, but with DynamoDB it maintains the ultra-low latency so make sure you bear that in mind. Much like RDS though, it’s fully managed. So again, you just need to focus on your data as all patch management and backups are taken care of by AWS but instead of having to select an instance like RDS, there is no need to with DynamoDB as it’s serverless. Now, you will find that due to these features it’s used for applications related to gaming, media and entertainment, finance and banking, and any solutions that require that single-digit millisecond latency. </p>
<p>Okay, so let’s now take a look at Amazon ElastiCache. So ElastiCache makes it easy to deploy, operate, and scale open-source, in-memory data stores in the cloud. So it basically improves performance of your applications through caching. Now, one key point to note for the exam is that it supports two different engines, Memcached and Redis. So I recommend you have a solid understanding of when to use each engine. Now, genuinely if you are looking for an in-memory key store and or cache that offers high-performance sub-millisecond latency, then Memcached is your best choice. </p>
<p>And it’s also recognized for its speed, performance, and also its simplicity as well. Whereas Redis offers a more robust set of features of that of Memcached while still offering that ultra high performance. So a common scenario might come across could be that you have a web application that reads and writes data to a persistent storage, for example, RDS, but however persistent storage tends to experience some fluctuations in latency as each piece of data needs to be written to or retrieved and this is affecting the overall performance. </p>
<p>So what could you do to enhance the performance of read requests? Well, the answer would be to implement an in-memory cache like ElastiCache, and this is especially true in industries such as online gaming and social networking sites.</p>
<p>So that’s some of the top points I suggest you must note when looking at databases for the exam. So to recap at a high level, RDS is a managed relational database service that supports multiple database engines specifically Amazon Aurora, which is highly performance and cost-effective, and it’s a PostgreSQL and MySQL compatible engine. Whereas DynamoDB is a managed serverless, NoSQL database service designed for ultra-high performance, providing single-digit latency. And ElastiCache is used as an in-memory data store to improve the read performance of your applications through caching and is supported by Redis and Memcached. Okay, so that’s the short summary wrapped up. So if you’re ready, let’s take the next step to become an AWS certified.</p>
<h1 id="2Amazon-Relational-Database-Service"><a href="#2Amazon-Relational-Database-Service" class="headerlink" title="2Amazon Relational Database Service"></a>2<strong>Amazon Relational Database Service</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">Instance Type Performance</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">Course: When to use RDS Multi-AZ &amp; Read Replicas</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Blog post: AWS Shared Responsibility Model</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Lab: Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Lab: Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="5RDS-Instance-Purchasing-Options"><a href="#5RDS-Instance-Purchasing-Options" class="headerlink" title="5RDS Instance Purchasing Options"></a>5<strong>RDS Instance Purchasing Options</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">Instance Type Performance</a></p>
<h1 id="8Backtrack-Storage-Pricing"><a href="#8Backtrack-Storage-Pricing" class="headerlink" title="8Backtrack Storage Pricing"></a>8<strong>Backtrack Storage Pricing</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/">Backtrack Storage Blog Post</a></p>
<h1 id="9Snapshot-Export-Pricing"><a href="#9Snapshot-Export-Pricing" class="headerlink" title="9Snapshot Export Pricing"></a>9<strong>Snapshot Export Pricing</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/introduction/">Course: Understanding and Optimizing Costs with AWS Storage Services</a></p>
<h1 id="11Amazon-DynamoDB"><a href="#11Amazon-DynamoDB" class="headerlink" title="11Amazon DynamoDB"></a>11<strong>Amazon DynamoDB</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<h1 id="18Amazon-Redshift"><a href="#18Amazon-Redshift" class="headerlink" title="18Amazon Redshift"></a>18<strong>Amazon Redshift</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">Differences between Redshift and PostgreSQL</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">Selecting a Distribution Style</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Course: Overview of AWS IAM</a></p>
<h1 id="20Amazon-DocumentDB-With-MongoDB-Compatibility"><a href="#20Amazon-DocumentDB-With-MongoDB-Compatibility" class="headerlink" title="20Amazon DocumentDB (With MongoDB Compatibility)"></a>20<strong>Amazon DocumentDB (With MongoDB Compatibility)</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-the-aws-database-migration-service/dms-introduction/">Course: AWS Database Migration Service</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-the-aws-database-migration-service/dms-introduction/">Course: AWS Database Migration Service</a></p>
<h1 id="22Amazon-Keyspaces-for-Apache-Cassandra"><a href="#22Amazon-Keyspaces-for-Apache-Cassandra" class="headerlink" title="22Amazon Keyspaces (for Apache Cassandra)"></a>22<strong>Amazon Keyspaces (for Apache Cassandra)</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.cqlsh.html">Running queries on a CQLSH client</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.html">Running queries programmatically using an Apache 2 licensed Cassandra client driver</a></p>
<h1 id="24Amazon-Quantum-Ledger-Database-QLDB"><a href="#24Amazon-Quantum-Ledger-Database-QLDB" class="headerlink" title="24Amazon Quantum Ledger Database (QLDB)"></a>24<strong>Amazon Quantum Ledger Database (QLDB)</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/qldb/latest/developerguide/verification.html">QLDB Verification</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-2-of-2-21/" rel="prev" title="AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-2-of-2-21">
      <i class="fa fa-chevron-left"></i> AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-2-of-2-21
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-RDS-Database-23/" rel="next" title="AWS-Solution-Architect-Associate-Create-Your-First-Amazon-RDS-Database-23">
      AWS-Solution-Architect-Associate-Create-Your-First-Amazon-RDS-Database-23 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Databases-SAA-C03-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Databases (SAA-C03) Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Relational-Database-Service"><span class="nav-number">2.</span> <span class="nav-text">Amazon Relational Database Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-an-Amazon-RDS-Database"><span class="nav-number">3.</span> <span class="nav-text">DEMO: Creating an Amazon RDS Database</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDS-vs-EC2"><span class="nav-number">4.</span> <span class="nav-text">RDS vs. EC2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Instructor-Danny-Jessee"><span class="nav-number">4.0.1.</span> <span class="nav-text">Instructor: Danny Jessee</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Amazon-RDS"><span class="nav-number">4.1.</span> <span class="nav-text">Amazon RDS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#High-availability-and-scalability"><span class="nav-number">4.2.</span> <span class="nav-text">High availability and scalability</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Amazon-EC2"><span class="nav-number">4.3.</span> <span class="nav-text">Amazon EC2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Final-Thoughts"><span class="nav-number">4.4.</span> <span class="nav-text">Final Thoughts</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RDS-Instance-Purchasing-Options"><span class="nav-number">5.</span> <span class="nav-text">RDS Instance Purchasing Options</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Database-Storage-and-I-x2F-O-Pricing"><span class="nav-number">6.</span> <span class="nav-text">Database Storage and I&#x2F;O Pricing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Backup-Storage-Pricing"><span class="nav-number">7.</span> <span class="nav-text">Backup Storage Pricing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Backtrack-Storage-Pricing"><span class="nav-number">8.</span> <span class="nav-text">Backtrack Storage Pricing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Snapshot-Export-Pricing"><span class="nav-number">9.</span> <span class="nav-text">Snapshot Export Pricing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Transfer-Pricing"><span class="nav-number">10.</span> <span class="nav-text">Data Transfer Pricing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-DynamoDB"><span class="nav-number">11.</span> <span class="nav-text">Amazon DynamoDB</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-a-DynamoDB-Database"><span class="nav-number">12.</span> <span class="nav-text">DEMO: Creating a DynamoDB Database</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DynamoDB-Accelerator-DAX"><span class="nav-number">13.</span> <span class="nav-text">DynamoDB Accelerator (DAX)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-ElastiCache"><span class="nav-number">14.</span> <span class="nav-text">Amazon ElastiCache</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-an-ElastiCache-Cluster"><span class="nav-number">15.</span> <span class="nav-text">DEMO: Creating an ElastiCache Cluster</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Neptune"><span class="nav-number">16.</span> <span class="nav-text">Amazon Neptune</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-an-Amazon-Neptune-Database"><span class="nav-number">17.</span> <span class="nav-text">DEMO: Creating an Amazon Neptune Database</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Redshift"><span class="nav-number">18.</span> <span class="nav-text">Amazon Redshift</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-an-Amazon-Redshift-Cluster"><span class="nav-number">19.</span> <span class="nav-text">DEMO: Creating an Amazon Redshift Cluster</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-DocumentDB-With-MongoDB-Compatibility"><span class="nav-number">20.</span> <span class="nav-text">Amazon DocumentDB (With MongoDB Compatibility)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lectures"><span class="nav-number">20.0.1.</span> <span class="nav-text">Lectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-an-Amazon-DocumentDB-Cluster"><span class="nav-number">21.</span> <span class="nav-text">DEMO: Creating an Amazon DocumentDB Cluster</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lectures-1"><span class="nav-number">21.0.1.</span> <span class="nav-text">Lectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Keyspaces-for-Apache-Cassandra"><span class="nav-number">22.</span> <span class="nav-text">Amazon Keyspaces (for Apache Cassandra)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lectures-2"><span class="nav-number">22.0.1.</span> <span class="nav-text">Lectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-a-Keyspace-and-Table-in-Amazon-Keyspaces-for-Apache-Cassandra"><span class="nav-number">23.</span> <span class="nav-text">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lectures-3"><span class="nav-number">23.0.1.</span> <span class="nav-text">Lectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Quantum-Ledger-Database-QLDB"><span class="nav-number">24.</span> <span class="nav-text">Amazon Quantum Ledger Database (QLDB)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lectures-4"><span class="nav-number">24.0.1.</span> <span class="nav-text">Lectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-a-Ledger-using-Amazon-QLDB"><span class="nav-number">25.</span> <span class="nav-text">DEMO: Creating a Ledger using Amazon QLDB</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lectures-5"><span class="nav-number">25.0.1.</span> <span class="nav-text">Lectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-a-Data-Lake"><span class="nav-number">26.</span> <span class="nav-text">What is a Data Lake?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-the-Difference-between-a-Data-Lake-and-a-Data-Warehouse"><span class="nav-number">27.</span> <span class="nav-text">What is the Difference between a Data Lake and a Data Warehouse?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Why-Can%E2%80%99t-We-Just-Store-All-This-Information-Into-a-Data-Warehouse"><span class="nav-number">28.</span> <span class="nav-text">Why Can’t We Just Store All This Information Into a Data Warehouse?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-Makes-Up-a-Good-Data-Lake"><span class="nav-number">29.</span> <span class="nav-text">What Makes Up a Good Data Lake?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#How-do-I-Actually-Build-a-Data-Lake"><span class="nav-number">30.</span> <span class="nav-text">How do I Actually Build a Data Lake?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Databases-Summary"><span class="nav-number">31.</span> <span class="nav-text">Databases Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2Amazon-Relational-Database-Service"><span class="nav-number">32.</span> <span class="nav-text">2Amazon Relational Database Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5RDS-Instance-Purchasing-Options"><span class="nav-number">33.</span> <span class="nav-text">5RDS Instance Purchasing Options</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#8Backtrack-Storage-Pricing"><span class="nav-number">34.</span> <span class="nav-text">8Backtrack Storage Pricing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9Snapshot-Export-Pricing"><span class="nav-number">35.</span> <span class="nav-text">9Snapshot Export Pricing</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#11Amazon-DynamoDB"><span class="nav-number">36.</span> <span class="nav-text">11Amazon DynamoDB</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#18Amazon-Redshift"><span class="nav-number">37.</span> <span class="nav-text">18Amazon Redshift</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#20Amazon-DocumentDB-With-MongoDB-Compatibility"><span class="nav-number">38.</span> <span class="nav-text">20Amazon DocumentDB (With MongoDB Compatibility)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#22Amazon-Keyspaces-for-Apache-Cassandra"><span class="nav-number">39.</span> <span class="nav-text">22Amazon Keyspaces (for Apache Cassandra)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#24Amazon-Quantum-Ledger-Database-QLDB"><span class="nav-number">40.</span> <span class="nav-text">24Amazon Quantum Ledger Database (QLDB)</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
