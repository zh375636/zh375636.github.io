<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="IntroductionHello, and welcome to this course on Databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification. Before we get start">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Cloud-Practitioner-Databases-CLF-C01-13">
<meta property="og:url" content="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="IntroductionHello, and welcome to this course on Databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification. Before we get start">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-11-19T02:03:36.000Z">
<meta property="article:modified_time" content="2022-11-20T22:58:38.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Cloud-Practitioner-Databases-CLF-C01-13 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Cloud-Practitioner-Databases-CLF-C01-13
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:36" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:36-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:38" itemprop="dateModified" datetime="2022-11-20T18:58:38-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various Database services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#117;&#x70;&#112;&#111;&#114;&#x74;&#64;&#99;&#x6c;&#x6f;&#x75;&#100;&#97;&#99;&#97;&#x64;&#x65;&#109;&#121;&#46;&#99;&#111;&#109;">&#115;&#117;&#x70;&#112;&#111;&#114;&#x74;&#64;&#99;&#x6c;&#x6f;&#x75;&#100;&#97;&#99;&#97;&#x64;&#x65;&#109;&#121;&#46;&#99;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Database services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Database services in AWS, including:</p>
<ul>
<li>Managed relational databases using the Amazon Relational Database Service, or RDS;</li>
<li>Managed NoSQL databases including Amazon DynamoDB; and</li>
<li>Data warehousing using Amazon Redshift.</li>
</ul>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="The-AWS-Database-Landscape"><a href="#The-AWS-Database-Landscape" class="headerlink" title="The AWS Database Landscape"></a>The AWS Database Landscape</h1><p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> cloud has thousands of services and features. It’s easy to be amazed and overwhelmed by the number of choices available. </p>
<p>However, inside AWS, the three primary types of services used by customers are compute, storage, and databases.</p>
<p>My focus, in this course, is on the managed database offerings available from AWS and the types of workloads they support.</p>
<p>Databases are the foundation of modern application development. A database’s implementation and how data is structured will determine how well an application will perform as it scales.</p>
<p>There are two primary types of databases, relational and non-relational. </p>
<p>Relational databases, sometimes called SQL databases because they use the Structured Query Language to manage information storage and retrieval, have been commercially available since the 1970s and are optimized around data storage.</p>
<p>Sometimes the Structured Query Language is called Ess-Queue-Ell and other times it’s pronounced Sequel. Both are correct.</p>
<p>Non-relational databases, also called NoSQL databases because data is stored and retrieved primarily using methods other than SQL, became popular in the 21st Century. Over the past 20+ years, developers have created web-based applications that needed to process large amounts of unstructured and semi-structured data quickly and reliably. </p>
<p>NoSQL databases are often distributed databases where data is stored on multiple computers or nodes.</p>
<p>A database is any type of mechanism used for storing, managing, and retrieving information. It is a repository–or collection–of data. </p>
<p>Though, an application developer probably thinks of a database as a place to put the stuff their application needs to run.</p>
<p>There are nine primary categories of databases available on AWS.</p>
<ol>
<li>Relational Databases</li>
<li>Key-Value Databases</li>
<li>Document Databases</li>
<li>In-Memory Stores</li>
<li>Graph Databases</li>
<li>Columnar Databases</li>
<li>Time Series Databases</li>
<li>Quantum Ledger Databases</li>
<li>Search</li>
</ol>
<p>Each one of these database types is optimized to support a specific type of workload. </p>
<p>Matching an application with the appropriate database type is essential for highly performant and cost-efficient operation.</p>
<p>I can remember a time when choosing a database was a platform choice. The underlying technology was almost secondary. Three or four vendors would be considered, one would be chosen as a primary platform, and every application would be built using it. It was less of a database management system and more of a vendor relationship.</p>
<p>Many people had a favorite database and felt that it could be used to manage any type of workload. On a small scale, this might be true. </p>
<p>If I have a small car, I can take 2-3 kids to school every day without issue.</p>
<p>However, what happens when I need to get 20 kids to school? The small car would still work but would be terribly inefficient. It would take several trips, take a considerable amount of time, be very hard on the car, and waste resources. A better option is the school bus.</p>
<p>Thankfully, the days of choosing a single database solution for all applications are long past. </p>
<p>While it is possible to make a general-purpose database manage just about any type of workload, it will not scale as demand increases. </p>
<p>One of the reasons this is important is because the cloud has the promise of agility. That is, being able to respond to changes in a business environment as they happen. </p>
<p>This includes the ability to grow as needed and it’s called scalability.</p>
<p>Another promise of the cloud, one that is talked about but not always understood, is elasticity. That is, it’s great to be able to scale up to meet demand but, elasticity allows for the opposite to happen. When demand decreases, so does the scale and its related expenses.</p>
<p>Who wants to pay for idle resources?</p>
<p>Today, it’s important for application developers to examine the data and consider its size, shape, and computational requirements for processing and analysis. These three things determine what type of database is needed for a particular application to allow for scalability and elasticity. </p>
<p>This also means there could be multiple databases used by an application; each one serving a unique purpose. </p>
<p>The two primary workload types are operational and analytical.</p>
<p>Operational applications are the ones often referred to as Online Transactional Processing applications, OLTP. </p>
<p>OLTP applications are among the most common built today. A transaction is a record of an exchange. OLTP is centered around a set of common business processes that is regular, repeatable, and durable. </p>
<p>It could be something like e-commerce, a content management system, or information management. </p>
<p>Data goes in and reports come out. This is regular, expected work. Often, the database on the back end is relational. The data is very structured, and the results are predictable.</p>
<p>The other type of workload is the analytical application. Online Analytics Processing applications, OLAP, are run–as needed–for things like business intelligence workloads and data analysis. </p>
<p>The goal is to gain insight. Workloads are often Retrospective, Streaming, and Predictive.</p>
<p>Retrospective analytics examine an organization’s history. What happened last quarter, last year, or–maybe–the last five years. </p>
<p>Streaming workloads are gathering data, in real-time, to discover trends or raise an alarm.</p>
<p>Predictive analytics uses data to try to look into the future. There are applications that are beginning to use Machine Learning and Artificial Intelligence to improve. </p>
<p>Applications that process analytics workloads are run on-demand. It is unknown what questions they’re going to answer. The data used has little structure and the workloads, themselves, are unpredictable.</p>
<p>Databases power 21st Century applications.</p>
<p>The efficient use of resources in the cloud requires organizations to be agile. Agility implies being responsive to change. However, because costs in the cloud are primarily based on consumption, it is important to consider the requirements of scalability and elasticity.</p>
<p>Resources should grow as demand increases but they also need to shrink as that demand subsides.</p>
<p>Online Transactional Processing Applications, OLTP, are usually powered by relational databases. The data is highly structured, controlled, and predictable.</p>
<p>Online Analytics Processing Applications, OLAP, are often powered on the backend using non-relational databases. The data for these applications is either semi-structured or unstructured, the workloads are unpredictable, and the output is used to answer questions about the unknown. </p>
<p>That’s a little about what databases do, but why is structured data important? The answer to that question involves something called a schema, and you’ll learn about it in the next lecture on relational databases.</p>
<h1 id="Relational-Databases"><a href="#Relational-Databases" class="headerlink" title="Relational Databases"></a>Relational Databases</h1><p>Relational databases have been commercially available since the 1970s. They provide an efficient, intuitive, and flexible way to store and report on highly-structured data. </p>
<p>These structures, called schemas, are defined before any data can be entered into the database.</p>
<p>Schemas are designed and built based on reporting requirements. </p>
<p>This means that a database’s expected output drives the creation of the database and how data is stored inside it.</p>
<p>Once a schema has been defined, database administrators and programmers work backward from these requirements to define how data will be stored inside the database.</p>
<p>No data can be stored in a database until this work has been completed. </p>
<p>Schema changes to existing databases are expensive in terms of time and compute power.  It also has a risk of corrupting data and breaking existing reports.</p>
<p>Data in a relational database is stored in tables. Each table–sometimes called a relation–contains one or more rows of data. </p>
<p>Each row–sometimes called a record–contains a collection of logically related data that is identified by a key. </p>
<p>The pieces of data stored in a row are called attributes or fields.</p>
<p>Visually, a table looks like a spreadsheet that has rows and columns. </p>
<p>Stored in one of the columns, each table has a primary key that uniquely identifies the information stored in each row. </p>
<p>Relationships between tables are created using these keys and there are rules that govern their behavior. The primary key in one table is a foreign key in another.</p>
<p>Data integrity is of particular concern in a relational database, there are a number of constraints that ensure the data contained in tables is reliable and accurate.</p>
<p>These reliability features–commonly referred to as ACID transactions–are atomicity, consistency, isolation, and durability. </p>
<p>Atomicity refers to the elements that make up a single database transaction. A transaction could have multiple parts. It is treated as a single unit that either succeeds completely or fails completely.</p>
<p>Consistency refers to the database’s state. Transactions must take the database from one valid state to another valid state.</p>
<p>Isolation prevents one transaction from interfering with another.</p>
<p>Durability ensures that data changes become permanent once the transaction is committed to the database.</p>
<p>Data in a relational database must be kept in a known and stable state. </p>
<p>As part of the requirements to maintain database stability, Primary and Foreign Keys are constrained–they have rules that govern them–to ensure the integrity of database tables.</p>
<p>Entity Integrity ensures that, in a table, the primary key is unique to the table and it has a value. Primary keys cannot be blank or null.</p>
<p>Referential Integrity requires that every value in a Foreign Key column exists as the Primary Key of its originating table. If four tables are related and a record is deleted in one of them, then the corresponding records in related tables must be deleted as well.  </p>
<p>The standard user and application programming interface–or API–of relational databases is the Structured Query Language, SQL. </p>
<p>Pronounced as either Ess-Queue-Ell or Sequel, it can be used either interactively or programmatically to create, update, and maintain the data inside a relational database.</p>
<p>SQL is the dominant query language for relational databases. </p>
<p>SQL is an industry standard, it is interoperable between database engines and application programming languages, well-documented, and stable.</p>
<p>Security is one of the most important responsibilities of a database administrator. </p>
<p>Relational database engines have built-in features for securing and protecting data but planning and effort are required to properly implement them. </p>
<p>These features include user authentication, authorization, and audit logging.</p>
<p>As part of the structure, data stored in relational databases is highly normalized. Normalization is a process where information is organized efficiently and consistently before storing it.</p>
<p>Duplicate data is discarded.</p>
<p>Closely related fields are grouped together.</p>
<p>Data should only be stored one time in a relational database. Fields that are logically related, like a first and last name, should be stored in the same table. </p>
<p>Removing redundancy and keeping similar data close reduces storage costs and improves the efficiency of data retrieval.</p>
<p>Relational databases are not partition tolerant. A data partition, in this case, refers to the disk. </p>
<p>Adding another disk would be like creating a second copy of the database. This copy, or partition, is called a shard. </p>
<p>When a shard is created, it uses the original database’s schema. This is a horizontal partition of a database.</p>
<p>To use it, logic outside of the database must be created to direct queries to the correct database.</p>
<p>This is because relational databases are designed to validate how data is stored. They do not check to see if information belongs inside it.</p>
<p>To illustrate, here’s a weather database split into a pair of shards, Rain and Snow. They are identical except for the information stored inside them.</p>
<p>An application determines if data should be stored in Rain or if it should be stored in Snow.</p>
<p>If a record belonging in Rain ends up in Snow and it matches the database schema, it will be stored. </p>
<p>However, since that record belongs in Rain, the reports will be wrong and applications will break when trying to query data.</p>
<p>Because of this complexity, most of the time relational databases are scaled vertically. </p>
<p>Horizontal scaling adds a copy of the database server. Vertical scaling is growing the server; usually by adding memory, CPU, or expanding a disk volume.</p>
<p>Vertical scaling has limits. There are only so many resources that will fit inside a server. Once these limits have been reached, a database will either need to be redesigned or broken into shards.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> has six fully-managed database engines available inside the Relational Database Service, RDS. </p>
<p>They are Amazon Aurora, MySQL, Postgres, MariaDB, Oracle, and Microsoft SQL Server.</p>
<p>Amazon Aurora is AWS’s cloud-native version of MySQL and Postgres. </p>
<p>As a review…</p>
<p>Relational databases are highly-structured data stores.</p>
<p>The structure is called a schema.</p>
<p>The schema defines how data is stored in tables.</p>
<p>Inside tables there are rows and columns.</p>
<p>A row is a record and each column is an attribute or field of the record.</p>
<p>Tables have keys that identify data in a table.</p>
<p>A Primary Key uniquely identifies a row in a table.</p>
<p>Foreign Keys are used to connect data in a row to rows in other tables.</p>
<p>Scaling is usually done vertically by adding compute resources to an existing database.</p>
<p>Horizontal scaling is called sharding and requires logic outside of the database.</p>
<p>Relational databases are ideal for applications that do online transactional processing. </p>
<p>These OLTP applications include online banking, e-commerce sites, inventory management, human resource management, and financial services.</p>
<p>OLTP transactions usually perform specific tasks and involve a single record or a small selection of records.</p>
<p>An online banking customer might send money from a checking account to a saving account.</p>
<p>A transaction like this involves two accounts and no other customers of the bank.</p>
<p>But, what about analytical applications where hundreds, thousands, or millions of transactions need to be processed quickly, efficiently, and at a low cost? </p>
<p>That’s where non-relational databases are helpful. Though, unlike the various relational database engines that have similar needs around structured data, the size &amp; shape of the unstructured or semi-structured data determine the type of non-relational database to choose.</p>
<p>These non-relational databases are often called NoSQL databases because, when they were first developed, they used something other than SQL to store and retrieve data. </p>
<p>However, over time, SQL has been adapted to be used with some of these non-relational databases. Because of this, NoSQL can also mean “Not Only SQL.”</p>
<p>If any type of data can be stored in a relational database, why bother with a non-relational database? </p>
<p>In the next lecture, let’s learn about NoSQL Databases, what they are, and what differentiates them from relational databases.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/the-aws-database-landscape/">The AWS Database Landscape</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/nosql-databases/">NoSQL Databases</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/types-of-managed-nosql-on-aws-part-1/">Types of Managed NoSQL on AWS - Part 1</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/types-of-managed-nosql-on-aws-part-2/">Types of Managed NoSQL on AWS - Part 2</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/summary-and-conclusion/">Summary and Conclusion</a></p>
<h1 id="NoSQL-Databases"><a href="#NoSQL-Databases" class="headerlink" title="NoSQL Databases"></a>NoSQL Databases</h1><p>Relational databases are highly structured repositories of data. They use schemas to define how information is organized and that schema must exist before the database can even be created.</p>
<p>This fixed nature of data structures makes relational databases sub-optimal for analytical processes where data is semi-structured or unstructured.</p>
<p>While relational databases are highly-structured repositories of information, non-relational databases do not use a fixed table structure. They are schema-less.</p>
<p>Since it doesn’t use a predefined schema that is enforced by a database engine, a non-relational database can use structured, semi-structured, and unstructured data without difficulty.</p>
<p>NoSQL is a general term that refers to a particular type of database model. It encompasses a wide variety of different models that don’t fit into the relational model.</p>
<p>Non-relational NoSQL-type databases have been around since the 1960s, but it wasn’t until the early 2000s that the NoSQL approach started to have broad appeal and a new generation of NoSQL systems began to hit the market.</p>
<p>Today, the term NoSQL describes a family of schema-less, non-relational, distributed data stores.</p>
<p>NoSQL databases are popular with developers because they do not require an upfront schema design; they are able to build code without waiting for a database to be designed and built.</p>
<p>It’s this flexibility–a dynamic approach to organizing data–that has been popular with companies needing to store unstructured or rapidly changing data.</p>
<p>The term NoSQL has two meanings. In the beginning, it described databases that used mechanisms other than SQL to manage data. </p>
<p>There was “No SQL” used when accessing and manipulating data.</p>
<p>The definition has been expanded to mean, “Not Only SQL.” Some systems use SQL along with other technologies and query languages.</p>
<p>There are people that argue that the one thing all NoSQL databases have in common is that they’re non-relational and that a better name would be, “NoREL.” </p>
<p>Personally, I don’t think I have enough free time to care that much about it.</p>
<p>NoSQL databases, in general, share a few basic characteristics. </p>
<p>They are non-relational, open-source, schema-less, horizontally scalable, and do not adhere to ACID constraints.</p>
<p>Most NoSQL databases access data using their own Application Programming Interface, API. However, some NoSQL databases use a subset of SQL for data management.</p>
<p>In many cases, the non-relational model is a good fit for an application’s requirements. </p>
<p>The data might be unstructured or semi-structured. The amount of data might be impractical for a relational database. Or, the data might be of one single type and doesn’t need the controls that come with a relational database.</p>
<p>Being open source is not a requirement of NoSQL databases. It’s more of a NoSQL observation. There are many relational and non-relational databases that open source projects. However, the developers of NoSQL databases lean towards providing open-source solutions.</p>
<p>Most NoSQL databases have no fixed schema. </p>
<p>Relational databases require a schema to be designed before the database is created. NoSQL databases don’t. Instead, schemas can be created dynamically as data is accessed or embedded into the data itself.</p>
<p>NoSQL databases have a reputation for being more flexible with the data they can accept and support agile and DevOps philosophies.</p>
<p>NoSQL databases are often run in clusters of computing nodes.</p>
<p>Data is partitioned across multiple computers so that each computer can perform a specific task independently of the others.</p>
<p>Each node performs its task without having to share CPU, memory, or storage with other nodes.</p>
<p>This is known as a shared-nothing architecture.</p>
<p>Most NoSQL databases relax ACID constraints found in relational databases.</p>
<p>NoSQL solutions were developed around the purpose of providing high availability and scalability in a distributed environment.</p>
<p>To do this, either consistency or durability has to be sacrificed. By relaxing consistency, distributed systems can be highly available and durable. </p>
<p>Using a NoSQL approach, inconsistent data is expected. There’s no problem as long as it’s recognized and managed appropriately.</p>
<p>Currently, there is no standard query language that is supported by all NoSQL databases. </p>
<p>Some NoSQL databases have their own query language. Others use languages such as JavaScript, Java, Python, XQuery, and SPARQL.</p>
<p>NoSQL databases are a family of non-relational databases that include Key-Value Databases, Column Family Stores, Document Stores, and Graph Stores.</p>
<p>Key-Value databases are the simplest NoSQL data stores to use from an API perspective. Using a RESTful API, a client can get the value for the key, put a value for a key, or delete a key from the data store. </p>
<p>A Document Store Database is a database that uses a document-oriented model to store information. Each document contains semi-structured data that can be queried. Essentially, the schema for the data is built into the document, itself, and can change as needed. </p>
<p>Here is an example of a simple document store. It’s written in JSON, JavaScript Object Notation. What makes this different than a key-value store is that, for some of the values, there are nested key-value pairs that can be indexed and retrieved.</p>
<p>A Graph Store is a database that uses a graphical model to represent and store information. It has two primary components, Vertices and Edges.</p>
<p>Those are some of the types of NoSQL databases that are available, and I’ll eventually cover them in more detail, but why use them? What advantages do NoSQL databases have over relational databases?Scaling a NoSQL database is easier and less expensive than scaling a relational database because the scaling is horizontal instead of vertical. In general, for relational databases to scale, they must add memory, CPU, or storage. This is vertical scaling. However, NoSQL scaling is done by adding a compute or disk node. This is horizontal scaling. NoSQL databases generally trade consistency for performance and scalability.</p>
<p>Relational databases have four properties that support reliability. These properties, commonly referred to as ACID, are atomicity, consistency, isolation, and durability.</p>
<p>Consistency refers to the database’s state. In a relational database, a transaction takes a database from one valid state to another valid state. With most NoSQL databases, it’s possible for data to be inconsistent; a query might return old or stale data.</p>
<p>You might hear this phenomenon described as being eventually consistent. Over time, data that is spread across storage nodes will replicate and become consistent. What makes this behavior acceptable is that developers can anticipate this eventual consistency and allow for it. That said, some NoSQL databases do support strong consistency. </p>
<p>To review, NoSQL is a general term that refers loosely to a particular type of database model, or database management system.</p>
<p>NoSQL databases generally share a number of characteristics. They are Non-relational, databases, Open-source, Schema-less, and Horizontally Scalable.</p>
<p>Additionally, NoSQL databases do not generally adhere to the ACID principles found in relational databases and most do not use SQL to access data.</p>
<p>This is a good time to discuss the types of fully-managed NoSQL databases available from <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Or, it would be, but this is the end of this lecture.</p>
<p>In the next lecture, I’m going to describe, in some detail, the types of managed NoSQL database available on AWS. It won’t be overly technical. It’s a discussion, really, about what’s possible and how to start thinking about your data.</p>
<h1 id="Types-of-Managed-NoSQL-on-AWS-Part-1"><a href="#Types-of-Managed-NoSQL-on-AWS-Part-1" class="headerlink" title="Types of Managed NoSQL on AWS - Part 1"></a>Types of Managed NoSQL on AWS - Part 1</h1><p>Hello and welcome to this lecture about the types of managed NoSQL databases available on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Managed services are those where the provisioning and maintenance is done by AWS according to your specifications on your behalf. </p>
<p>This lecture will serve as an overview of four types of managed NoSQL database technologies available on AWS. I will cover Key-Value stores, Document stores, Column Family stores, and In-Memory stores. In the next lecture, I will continue my overview and discuss Graph databases, Time Series databases, Ledger databases, and Search databases. </p>
<p>This lecture–as well as the one that follows it– will not cover implementation details. It is mostly a discussion of what’s possible.  </p>
<p>I’m going to provide an overview of each NoSQL technology, name the service from AWS that uses it, and provide a use case.</p>
<p>Let’s get started.</p>
<p>In a relational database, data is stored in tables composed of rows and columns. These tables and the types of data they’re going to store are defined prior to application development. This allows for storage and access patterns to be optimized.</p>
<p>It also means that relational databases are relatively inflexible. </p>
<p>Key-Value Databases, also called Key-Value Stores, are often considered to be the simplest type of NoSQL database. They are typically more flexible than relational databases and offer fast performance for reads and writes. </p>
<p>The AWS managed NoSQL database that is a Key-Value store is DynamoDB.</p>
<p>Key-Value stores are designed for storing, retrieving, and managing associative arrays and are well suited for working with large amounts of data.</p>
<p>An associative array, also known as a dictionary or a hash table, stores data with a unique identifier called a key. The data stored, which could be one or more items, is the value. </p>
<p>These are simple examples of key-value pairs. </p>
<p>It is also possible to store lists as the value.</p>
<p>Key-Value Stores have no schema that defines the structure of the data. There is only the key and its associated value.  </p>
<p>The key in a key-value pair must be unique. This is a unique identifier that allows access to the value associated with that key.</p>
<p>Before using a key-value store, it helps to have a naming convention for key names. It will help keep key-value stores consistent and minimize confusion.</p>
<p>The value in a key-value store can be text, numbers, a list of items, documents, or another key-value pair.</p>
<p>In key-value stores, data is stored and retrieved using operations such as get, put, and delete. </p>
<p>Queries to Key-Value Stores are simple. Lookups are based on the key and retrieval is often measured in milliseconds regardless of the size of the data returned.</p>
<p>Key-Value Stores are not optimized for search. It’s very expensive to scan in terms of time and cost. </p>
<p>They are not suitable for applications requiring frequent updates or complex queries involving specific data values. </p>
<p>There are several types of data and access patterns that are well suited for Key-Value Stores.</p>
<p>Web applications can store user profiles, shopping cart data, and preferences in a Key-Value Store. </p>
<p>Real-time recommendation engines and advertising systems are often powered by Key-Value Stores. </p>
<p>Key-Value Stores are commonly used for in-memory data caching. They can speed up applications by minimizing reads and writes to slower disk-based systems. </p>
<p>Binary objects, such as pictures and other multimedia items–can be stored in key-value databases. However, a better solution–in terms of time and cost–is to save binary files in object storage and use a key-value database for lookups.</p>
<p>DynamoDB is a key-value database. However, since it can store key-value pairs as a value, it is also a type of NoSQL Document database.</p>
<p>Document Databases were invented to store semi-structured data. Instead of having the structure defined as part of the database in advance–like a relational database–each document in the database has its own unique schema that defines its structure.</p>
<p>The AWS managed NoSQL document database service is Amazon DocumentDB. As a document database, Amazon DocumentDB is designed to store, query, and index JSON data.</p>
<p>Document Databases are similar to Key-Value Databases in that they also have a key and a value. The difference is that, in a Document Database, the value contains structured or semi-structured data. This structured&#x2F;semi-structured value is referred to as a Document.</p>
<p>Here is an example of a document that could be stored in a document database. It is written in JSON, JavaScript Object Notation.</p>
<p>In semi-structured data, there is no separation between the schema and the data. Each document stored has its own unique schema that defines what it contains.</p>
<p>The database engine uses this structure of the stored data to create metadata that is used for database optimization and queries.</p>
<p>Consider an application to track patient records in a doctor’s office. A patient–a person–does not fit in a relational database row.  There is no schema that can be used to describe every person on earth.</p>
<p>When visiting the doctor, data is generated and entered by multiple people. There’s insurance information, billing, height, weight, blood pressure, medications, and related information.</p>
<p>Defining a person’s medical history in rows is impractical and inefficient.</p>
<p>A more efficient way is to think of patient information as a collection of documents. At every appointment, a new document is added with updated information.</p>
<p>Document Stores scale horizontally. Data can be stored over multiple nodes that can number in the 1,000s.</p>
<p>One benefit that document store databases have over key-value databases is that, in a document store, the data inside the document can be queried.</p>
<p>This is different from a Key-Value store where a query returns the value in its entirety. </p>
<p>In a document store, queries can be run against the structure of a document as well as the elements inside it to return only the information required.</p>
<p>Document Databases have a variety of use cases. They are used in web applications, for managing user-generated content, shopping catalogs, gaming, and for storing sensor data from IoT devices.</p>
<p>Where a relational database uses rows to store similar types of data, a Column Store is a type of NoSQL database that stores data using a column-oriented model. </p>
<p>On AWS, the NoSQL column store available as a managed service is Amazon Keyspaces.  </p>
<p>Using columns allows the database to precisely access data needed to answer a query without having to scan each row in a table and discard unwanted items.</p>
<p>Column Store databases are also referred to as:</p>
<ul>
<li>Column databases</li>
<li>Column-Family databases</li>
<li>Column-Oriented databases</li>
<li>Wide-Column Stores</li>
<li>Columnar databases</li>
<li>Columnar stores</li>
</ul>
<p>A column store database uses a concept called a keyspace to define the data it contains.</p>
<p>A keyspace is similar to a relational database’s schema. The keyspace contains a collection of column families that look like tables from a relational database.</p>
<p>The column families contain rows and these rows contain columns.</p>
<p>A closer look at a column family shows:</p>
<p>A Column Family consists of multiple rows.</p>
<p>Each row can contain a different number of columns.</p>
<p>Each column is limited to its row.</p>
<p>Columns are kept in their own row. They do not span all rows like a relational database does. Each column contains a name-value pair along with a timestamp.</p>
<p>Here’s how each row is constructed. From left to right there is a row key and one or more columns. </p>
<p>The row key is a unique identifier for that row.</p>
<p>Each column contains a name-value pair and a timestamp.</p>
<p>The timestamp is the date and time the data was inserted. This is often used to determine the most recent version of the data.</p>
<p>Some Column-Family databases have composite columns that allow for objects to be nested inside a column.</p>
<p>Column stores are efficient doing data compression and partitioning.</p>
<p>Due to their structure, columnar databases excel at doing aggregation-type queries. That is, they can SUM, COUNT, and calculate AVG values easily.</p>
<p>Columnar databases scale well. They are suitable for workloads that do Massively Parallel Processing where data is spread across a large cluster of compute nodes that could number in the 1,000s.</p>
<p>Columnar stores can be loaded fast and efficiently. A one-billion row table can be loaded into a columnar store in seconds with queries and analysis starting almost immediately.</p>
<p>From an end-user perspective, the metadata in a columnar database looks and feels like a relational database. Some columnar database engines are SQL compliant and support the same controls that maintain the data’s state.</p>
<p>NoSQL databases tend to be either Key-Value type stores or Document stores. Columnar Store databases are neither.</p>
<p>Columnar databases are typically used with analytical applications, data warehousing, and Big Data processing.</p>
<p>In-Memory data stores are used by applications that require real-time access to data. Since the data is stored in memory, In-Memory stores provide microsecond latency to applications.</p>
<p>These stores are used as caches and the managed NoSQL service available from AWS is Amazon ElastiCache. </p>
<p>Amazon ElastiCache has two NoSQL In-Memory database engines; Redis and Memcached.</p>
<p>Before I go too much farther, I think it’s important to explain that a caching system is not a database. It is something that sits in front of a database to improve throughput. It also removes the need for putting a caching layer inside an application.</p>
<p>The primary purpose of an in-memory key-value store is to provide inexpensive access to data with sub-millisecond latency.</p>
<p>Most data stores have areas of data that are frequently accessed but rarely updated. Querying a database and getting the results from disk is always slower and more expensive than locating a key in a key-value pair cache. </p>
<p>Some relational database queries are expensive to perform. This might be a query that requires data from multiple tables or one that does a number of calculations before returning a result. </p>
<p>By caching query results, the cost of the query is only incurred once. The data can be returned multiple times without needing to run the query again.</p>
<p>An In-Memory data store keeps its entire dataset in RAM and is not stored on disk. The reward is speed. However, there is a downside. The risk when using an In-Memory store is that when a machine goes down the data is lost. </p>
<p>Some In-Memory stores, like Redis, are able to add persistence for recovery by saving a transaction log to disk and taking snapshots of datasets stored in memory.  </p>
<p>Cached data is stale data. It is important to know, before implementing an in-memory cache, if an application can tolerate stale data and, if it can, in what context.</p>
<p>As an example, if an application displays stock prices, customers might be willing to accept staleness with a disclaimer saying prices are delayed by 5 minutes. However, a stockbroker will want real-time data.</p>
<p>Caching should provide a speed or cost advantage. It doesn’t make sense to cache data that is dynamic or that is seldom accessed. </p>
<p>For caching to provide a benefit, data should be relatively static and frequently accessed like a personal profile on a social media site. </p>
<p>An in-memory store is well-suited to be a frond-end for relational databases and key-value stores. </p>
<p>It can provide a high-performance middle-tier for applications having high request rates or low-latency requirements.</p>
<p>In-memory stores can be used to cache session data, web pages, and leaderboards.</p>
<p>This has been a high-level overview of Key-Value stores, Document stores, Column Family stores, and In-Memory stores available on AWS. In my next lecture, I’m going to continue the discussion and cover Graph databases, Time Series databases, Ledger databases, and Search databases.</p>
<h1 id="Types-of-Managed-NoSQL-on-AWS-Part-2"><a href="#Types-of-Managed-NoSQL-on-AWS-Part-2" class="headerlink" title="Types of Managed NoSQL on AWS - Part 2"></a>Types of Managed NoSQL on AWS - Part 2</h1><p>Hello and welcome to this lecture about the types of managed NoSQL databases available on AWS. Managed services are those where the provisioning and maintenance are done by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> according to your specifications on your behalf.</p>
<p>This lecture is a continuation of the previous one and will serve as an overview and discussion of Graph databases, Time Series databases, Ledger databases, and Search databases.</p>
<p>Like the previous lecture, it will not cover implementation details. It is mostly a discussion of what’s possible.  </p>
<p>I will provide an overview of each NoSQL technology, name the service from AWS that uses it, and provide a use case.</p>
<p>Let’s get started.</p>
<p>A Graph database is a database that uses a graphical model to represent and store data about relationships. Relationship data is important for things such as building social networking applications, recommendation engines, doing fraud detection, creating knowledge graphs, and modeling the life sciences.</p>
<p>The AWS managed NoSQL graph database is Amazon Neptune. </p>
<p>Graph databases are composed of three elements, vertices, edges, and properties.</p>
<p>Vertices, also called nodes, are objects such as people or artifacts. Each node in a graph database has a unique identifier expressed in key-value pairs.</p>
<p>The singular of vertices is vertex. A vertex can represent data such as integers, string, people, locations, and buildings.</p>
<p>Edges represent the connection–or relationship–between two objects. Each edge is defined by a unique identifier that provides details about a starting or ending node along with a set of properties.</p>
<p>The vertices and edges can each have properties associated with them. This allows a graph database to depict complex relationships between otherwise unrelated data.</p>
<p>Here is a simple graph database.</p>
<p>The circles are the vertices and the arrows represent edges. Each edge has a property that defines the relationship.</p>
<p>The composer node, John Williams, has relationships with a number of movies. However, he’s got two relationships with the movie Schlinder’s List because he wrote the movie’s score and won an Academy Award for his work.</p>
<p>As more data is added to the database, the schema changes to match the relationships.</p>
<p>Many graph database management systems use their own proprietary query language. Some graph database systems support access using methods such as Gremlin from Apache TinkerPop, JavaScript, JSON, XQuery, and SPARQL. </p>
<p>Depending on the graph database, they can process either transactional or analytical workloads.</p>
<p>Graph databases can process large sets of user profiles and interactions to build social networking applications.</p>
<p>Graph databases can store relationships between customer interests, friends, and purchase history to create recommendations.</p>
<p>Use graph databases to process financial and purchase transactions in near real-time to detect fraud patterns.</p>
<p>A knowledge graph stores information in a graph model and uses graph queries to enable the navigation of highly connected datasets.</p>
<p>Use a knowledge graph to add topical information to product catalogs, build and query complex models of regulatory rules, or model general information.</p>
<p>Graph databases can be used to create applications that store and navigate the life sciences.</p>
<p>Use a graph database to map a computer network and answer questions about hosts and application usage. If a malicious file is on a host, a graph database could be used to find the connection between the hosts that spread the malicious file and trace it back to the host that downloaded it.</p>
<p>Time-series Databases efficiently collect, synthesize, and derive insights from data that changes over time.</p>
<p>The AWS managed NoSQL database for time-stream data is Amazon Timestream.</p>
<p>In a Time-Series Database data is collected at regular intervals as the value and is stored with the time as the key. </p>
<p>While it’s possible to retrieve a single item from time-series data–like the price of an item–computation is usually applied over a range of time data to return a result.</p>
<p>The primary purpose of a Time-Series Database is to provide answers. A query will process a range of data, do the appropriate computations, and return the results.  </p>
<p>For example, determining the MIN, MAX, and AVG of CPU utilization on a database server over the past seven days.</p>
<p>Time-series databases are ideal for DevOps applications that collect data millions of times per second and analyze that data in real-time to improve application performance and availability. </p>
<p>Use Time-Series databases to quickly analyze time-series data generated by IoT applications using analytic functions such as smoothing, approximation, and interpolation. </p>
<p>Time stream databases can be used to store and analyze clickstream data to understand user activity across applications over a period of time. </p>
<p>Use a Time Stream database to store and analyze time-series data for industrial equipment maintenance, trade monitoring, fleet management, and route planning.</p>
<p>Ledger Databases provide a centralized and trusted authority to maintain a scalable, immutable, and cryptographically verifiable record of transactions for an application.</p>
<p>The AWS managed NoSQL ledger database is the Amazon Quantum Ledger Database.</p>
<p>These databases maintain their trust, in part, by being fully auditable and transparent. All transactions are recorded in a log to track activity.</p>
<p>QLDBs are immutable. This means that the data in the database remains unchanged once saved. Instead, the action of updating data creates a new version of the record. Changes to the database do not overwrite existing database records.</p>
<p>Cryptographic verification is used to ensure data is immutable. When a record is committed, a hash is created by the database.</p>
<p>Hashing is an algorithm performed on data to produce a number called a checksum or hash. This hash is used to verify that data has not been modified, tampered with, or corrupted. </p>
<p>No matter how many times the hashing algorithm is run against the data, the hash will always be the same when the data is the same.</p>
<p>Quantum Ledger Databases use blockchain technology when creating hashes. This means they use two pieces of information to create a hash value; the record data and the hash of the previous record. This ensures that the entire chain of records is valid.</p>
<p>Anyone can create an audit log to show how data is used, but how can they legally prove that the data has not been altered? </p>
<p>Even with the best user interfaces and audit tracking, a skilled programmer can change electronic records without leaving a trace.</p>
<p>Blockchains can be used to build trust and ensure policy, governance, and regulation of data processes. </p>
<p>Banks often need a centralized ledger-like application to keep track of critical data such as credit and debit card transactions. </p>
<p>Instead of building a custom ledger with complicated auditing functionality, a ledger database can easily store an accurate and complete record of financial transactions.</p>
<p>Manufacturing companies have a need to reconcile data between supply chain systems to track the manufacturing history of a product. </p>
<p>A ledger database can be used to record the history of each transaction and provide the details of each individual batch of a product manufactured at a facility.</p>
<p>Insurance applications need a way to track the history of claims.</p>
<p>Instead of building complex auditing functionality using relational databases, insurance companies can use a ledger database to maintain the history of claims.</p>
<p>When conflicts arise, a ledger database can cryptographically verify the integrity of the claims data.</p>
<p>HR systems have to track and maintain a record of employee details such as payroll, bonus, benefits, performance history, and insurance. </p>
<p>By implementing a system-of-record application using a ledger database, companies can easily maintain a trusted and complete record of the digital history of employees in a single place.</p>
<p>Retailers need to access information on each stage of a product’s supply chain.</p>
<p>With a ledger database, retail companies can track the full history of inventory and supply chain transactions.</p>
<p>Search engines help people find the information they need. Search databases are optimized to store and retrieve search-related data and typically offer specialized methods such as full-text search, complex search expressions, and the ranking of search results.</p>
<p>The managed NoSQL offering from AWS is the Amazon Elasticsearch Service.</p>
<p>Search databases securely ingest unstructured data from multiple locations, store and index it, and make it searchable.</p>
<p>Data ingestion is the process of taking raw data from a variety of sources, then parsing, normalizing, and enriching it.</p>
<p>The raw data sources include logs, system metrics, and web applications.</p>
<p>Once ingested, the data is indexed inside the search database. An index is a collection of documents that are related to each other. The search database, Elasticsearch, stores indexes as JSON documents. Each document has a set of keys that have corresponding values.</p>
<p>Elasticsearch uses a data structure called an inverted index that provides fast full-text searches.</p>
<p>An inverted index lists every unique word that appears in any document and identifies all of the documents where each word occurs.</p>
<p>Search databases can be used to provide a fast, personalized search experience for applications, websites, and data lake catalogs.</p>
<p>A real estate business could use a search database to help people find homes in a desired location, at a chosen price range, from among thousands of properties. </p>
<p>Search databases can be used to store, analyze, and correlate application and infrastructure log data to find and fix issues.</p>
<p>Use search databases to analyze network and systems logs for real-time threat detection and incident management. </p>
<p>Well, that covers the types of fully-managed NoSQL databases available on AWS. It was a fair amount of information. However, it should give you an idea of what’s possible in the AWS cloud.</p>
<p>In my next lecture, I’m going to do a quick summary of relational and non-relational databases, the AWS fully-managed options, and their use cases. </p>
<p>I’ll also give you some options for your next steps; depending on your needs and interests.</p>
<p>This has been a high-level overview of Graph databases, Time Series databases, Ledger databases, and Search databases. </p>
<p>NoSQL databases let developers divide complex applications into manageable pieces and create rich experiences for customers.</p>
<p>In the final lecture in this series, I’ll summarize what I’ve covered about the managed database types on AWS and give you an idea of what next steps you should take.</p>
<p>Thanks for watching.</p>
<h1 id="Amazon-Relational-Database-Service"><a href="#Amazon-Relational-Database-Service" class="headerlink" title="Amazon Relational Database Service"></a>Amazon Relational Database Service</h1><p>Hello and welcome to this lecture, where I shall be discussing the first of the AWS database services that I will be covering in this <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/course-introduction/">course</a>, the Amazon Relational Database Service, commonly known as RDS. I will look at a number of different common features of the service to give you a general idea of how it’s configured. So as the name suggests, this is a relational database service that provides a simple way to provision, create, and scale a relational database within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. It’s a managed service, which takes many of the mundane administrative operations out of your hands, and it’s instead managed by AWS, such as backups and the patching of both the underlying operating system in addition to the database engine software that you select.</p>
<p>Amazon RDS allows you to select from a range of different database engines. These currently include MySQL, and this is considered the number one open source relational database management system. MariaDB. This is the community-developed fork of MySQL. PostgreSQL. This database engine comes in a close second behind MySQL as the preferred open source database. Amazon Aurora. Amazon Aurora is AWS’s own fork of MySQL, which provides ultrafast processing and availability, as it has its own cloud-native database engine. Oracle. The Oracle database is a common platform in corporate environments. And SQL Server. This is a Microsoft database with a number of different licensing options.</p>
<p>In addition to so many different database engines, you also have a wide choice when it comes to selecting which compute instance you’d like to run your database on. The varying different options offer different performance and allowed to architect your environment based on your expected load. When you create your RDS database, you must select an instance to support your database from a processing and memory perspective, as shown here in this screenshot, using the MySQL database engine. Currently, these are the different instance types available to you for each of the database engines, which are categorized between general purpose and memory-optimized.</p>
<p>For a breakdown of the performance of each of these instance types, please refer to the following <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">AWS documentation</a>. For each of these instance types, you also have various instance sizes, each equating to a different performance level from a vCPU and memory perspective. For example, when looking at the T3 instance, we have the following sizes available.</p>
<p>You can deploy your RDS instance in a single availability zone. However, if high availability and resiliency is of importance when it comes to your database, then you might want to consider a feature known as Multi AZ, which stands for multi availability zones. When Multi AZ is configured, a secondary RDS instance is deployed within a different availability zone within the same region as the primary instance. The primary purpose of the second instance is to provide a failover option for your primary RDS instance. The replication of data between the primary RDS database and the secondary replica instance happens synchronously.</p>
<p>Let’s look at how Multi AZ would work in a production environment. If you have configured Multi AZ and an incident occurs which causes an outage to the primary RDS instance, then the RDS failover process takes over automatically. This process is managed by AWS, and it’s not something that you need to manually perform or trigger. RDS will update the DNS record to point to the secondary instance. This process can typically take between 60 and 120 seconds. The length of time is very dependent on the size of the database, its transactions, and the activity of the database at the time of failover. This automatic changeover enables you to continue using the database without the need of an engineer making any changes to your environment. The failover process will happen in the following scenarios. If patching maintenance has been performed in the primary instance, if the instance of the primary database has a host failure, if the availability zone of the primary database fails, if the primary instance was rebooted with failover, and if the primary database instance class on the primary database is modified.</p>
<p>As you can see, activating Multi AZ is an effective measure and precaution to implement to ensure you have resiliency built in should an outage occur. For detailed information on RDS Multi AZ, please refer to our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">here</a>.</p>
<p>Over time, your workloads on your database will fluctuate. And so how can you optimize your RDS database to ensure it is capable of meeting the demands of your load, both from a storage and compute perspective? When it comes to scaling your storage, you can use a feature called storage autoscaling. MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server all use Elastic Block Store, EBS volumes, for both data and log storage. However, Amazon Aurora uses a shared cluster storage architecture and does not use EBS. The database engines that use EBS support general purpose SSD storage, provisioned IOPS SSD storage, and magnetic storage.</p>
<p>The general purpose SSD storage is a good option for a broad range of use cases which provides single-digit millisecond latencies and offers a cost-effective storage solution. The minimum SSD storage volume for your primary dataset is 20 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle. However, the maximum for SQL Server is 16 tebibytes. Provisioned IOPS SSD. Now, this option is great for when you have workloads that operate at a very high I&#x2F;O. You can provision a minimum of 8,000 IOPS and a maximum of 80,000 for MySQL, PostgreSQL, MariaDB, and Oracle, but the maximum for SQL Server is 40,000.</p>
<p>In addition to being able to provision the IOPS needed for your workload, the minimum storage for your primary dataset is a 100 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle, and 16 tebibytes for SQL Server. Magnetic storage is simply supported to provide backwards compatibility. And so instead, AWS recommends that you select general purpose. The following screenshot shows the configuration screen when setting your storage requirements during the creation of a MySQL RDS database. </p>
<p>n this example, general purpose has been selected as the storage with a minimum of 100 gibibytes of primary storage. Under the storage autoscaling section, I’ve enabled the feature with the checkbox and set a maximum storage threshold of 1000 gibibytes. This means that one storage will start at 100 gibibytes when the database is created and the storage will automatically scale with demand up to a maximum of 1000 gibibytes without you having to intervene in any way. The maximum storage threshold can be set at 65,536 gibibytes. Aurora doesn’t use EBS and instead uses a shared cluster storage architecture which is managed by the service itself.</p>
<p>When configuring your Aurora database in the console, the option to configure and select storage options like we saw previously does not exist. Your storage will scale automatically as your database grows. To scale your compute size, which is effectively your instance, is easy to do in RDS both vertically and horizontally. Vertical scaling will enhance the performance of your database instance. For example, scaling up from an m4.large to an m4.2xlarge. Horizontal scaling will see an increase in the quantity of your current instance. For example, moving from a single m4.large to three m4.large instances in your environment through the means of read replicas.</p>
<p>At any point you can scale your RDS database vertically, changing the size of your instance. When doing so, you can select to perform the change immediately or wait for a scheduled maintenance window. For horizontal scaling, read replicas can be used by application and other services to save read only access to your database data via a separate instance. So, for example, let’s assume we have a primary RDS instance which serves both read and write traffic. Due to the size of the instance and the amount of read-intensive traffic being directed to the database for queries, the performance of the instance has taken a hit. So to help resolve this, you can create a read replica. </p>
<p>A snapshot will be taken of your database, and if you’re using Multi AZ, then this snapshot will be taken of your secondary database instance to ensure that there are no performance impacts during this process. Once the snapshot is complete, a read replica instance is created from this data. The read replica then maintains a secure asynchronous link between itself and the primary database. At this point, read only traffic can be directed to the read replica to serve queries. Before implementing read replicas, please check with the latest AWS documentation to identify database engine read replica compatibility.</p>
<p>As I mentioned previously, many of the administrative tasks for RDS are taken care of by AWS. For example, patching and automated backups. As Amazon RDS is a managed service, and from a shared responsibility model is considered a container service where you have no access to the underlying operating system on which your database runs on. As you can see, both platform and application management and operating systems falls under the realm of AWS responsibilities. As a result, AWS is responsible for both the patching of the operating system and any patching for the database engine themselves. More information on the AWS shared responsibility model can be found in my existing blog post found <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">here</a>.</p>
<p>From a backup perspective, by default, Amazon RDS provides an automatic feature seen here. This is enabled on all new RDS databases, which backs up your RDS instance to Amazon S3. You are able to configure the level of retention in days from zero to 35 and implement a level of encryption using the key management service, or KMS. More information on KMS can be found in our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>You can also perform manual backups anytime you need to, which are known as snapshots. However, these snapshots are not bound by the retention period set in the automatic backup configuration and are only deleted through a manual process. When using a MySQL-compatible Aurora database, you can also use a feature called backtrack, and this allows you to go back in time on the database to recover from an error or incident without having to perform a restore or create another database cluster.</p>
<p>As you can see from the configuration page during the Aurora database creation process, it is enabled via a checkbox and allows you to enter a number of hours of how far you would like to backtrack to with a maximum of 72 hours. In this example, I’ve entered 12 hours, and so Aurora will retain log data of all changes for 12 hours as specified. We’ve now covered the fundamentals of Amazon RDS and some of the key features to be aware of. If you’d like to get some hands-on experience of using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/demo-creating-amazon-rds-database-2/">Amazon RDS</a>, feel free to check out the following labs.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="DEMO-Creating-an-Amazon-RDS-Database"><a href="#DEMO-Creating-an-Amazon-RDS-Database" class="headerlink" title="DEMO: Creating an Amazon RDS Database"></a>DEMO: Creating an Amazon RDS Database</h1><p>Hello, and welcome to this lecture, which will be a demonstration on how to create an RDS database. So let’s get straight into it. So as you can see, I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console. And the first thing I need to do is to go to RDS. Now you can find RDS under the Database category, and you can see that it’s the first database option. So if you select the RDS service, and this is the dashboard for <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/amazon-relational-database-service-2/">Amazon RDS</a>. As you can see, I don’t have any database instances running. So let’s go ahead and create our first database.</p>
<p>So under the Create database section here, we can either restore an existing database from Amazon S3, from a backup, or we can create a new database. For this demonstration, I’m going to create a new database. So let’s click that option. Now, firstly, we need to choose a database creation method. We can either do a standard create or an easy create. Now the standard create allows us to set more-configurable options. So for this demonstration, that’s the option I’m going to select.</p>
<p>Then we have our database engine types. As I explained previously, we have Amazon Aurora, MySQL, MariaDB, Postgres, Oracle, and Microsoft SQL Server. I’m just going to select MySQL for this demonstration. So scrolling down again, we can then select our version of MySQL, whichever version we’d like. I’ll just leave it as the default option. And then we have something called Templates.</p>
<p>Now, depending on what template we select here will predefine a list of other configurable components. So here that’s highlighted is Production and this uses defaults for high availability and fast and consistent performance. The Dev&#x2F;Test template is intended for development use outside of a production environment. And the Free Tier is simply to allow you to get hands-on experience with RDS and doesn’t really use many of the features. But I want to show you the full feature set. So I’m going to select Production.</p>
<p>Now, if we go down to Settings. Here, we can enter a database instance identifier. So this is the name of the database instance, not the actual name of a database table. So I’m happy to call it database one, just leave that as default. Now we have our credentials. Now we have a master user name to connect to the database instance, so we can have admin. We can either auto-generate a password or we can select our own. So I’ll just go ahead and enter my own one.</p>
<p>Next, we have the option to select the database instance size. So we have our Standard classes or Memory Optimized or even Burstable classes. So I’m going to select the Standard class and using this dropdown, we can select the size of the instance that we want. And as you can see, there’s a different number of VCPUs and RAM. And so I’m just going to select the smallest instance size.</p>
<p>Now, if we go down to Storage, we can select our storage type. So we have General Purpose or Provisioned IOPS. If we look at the Provisioned IOPS, we can define the allocated storage and then also the number of IOPS as well, which is the input and output operations per second. For this demonstration, I’m just gonna leave it as General Purpose. I’ll just accept the storage defaults there of 20. Here we have Storage autoscaling. If we want to enable that or not, it’s just a simple tick box. RDS will automatically scale up to whatever value we put in here. So for example, 100 gig. That will give us the flexibility of starting at 20 and scaling all the way up to 100 automatically. </p>
<p>Now, if we go down to Availability and durability. Here, we have a Multi-AZ deployment. So it’s enabling this, will create another standby instance in a different availability zone to create high availability and data redundancy. For this demonstration, I’m just gonna leave it as a single AZ deployment. So I’m gonna select Do not create a standby instance. If we go down to Connectivity, we can select the VPC that we’d like this RDS instance to reside in. And you can see here, after a database is created, you can’t change the VPC selection. If you expand this option here for additional connectivity configuration, we can see a few additional options.</p>
<p>So we can select a database subnet group. And this simply defines which subnets and IP ranges the database instance can use in the VPC. Have an option here, if the database should be publicly accessible or not. If you select yes, then it will be issued a public IP address and devices and instances outside of your VPC will be able to try and connect to your database, if the VPC security groups allow it. For this demonstration, I’m just going to keep it a private RDS database, so it won’t assign any kind of public IP address. And only instances inside the VPC will be able to connect. Here, we can choose our security group, which will essentially define which resources can talk to our RDS instance.</p>
<p>Now, if we select an existing, then we can use this dropdown box here to select which security group that we’d like to use. I’m just going to select the default. I haven’t set any kind of security groups up for this as this is just a demonstration, but that’s where you would apply your security groups for your RDS instance. And as you can see, it’s added it in there. If you’d like to deploy your RDS instance in a particular availability zone, then you can select one. If you have no preference, simply select no preference. And also what port the database will be using.</p>
<p>If you go down to Database authentication. We have two options here for MySQL. Password authentication. Now this will allow anyone to connect to the instance just using the database passwords. If you want it more secure, then you can use the database password in addition to verifying that the user has permissions to access the RDS database, through permissions that were assigned directly to the user or group or role. So that just offers an additional level of security. If we go down further to Additional configuration, we can configure additional options.</p>
<p>So here we have our database options. You can enter an initial database name that will run on your database instance. Let’s just say my database. You can select a parameter group. Parameter groups is essentially a grouping of configurable parameters that operate at the database engine level. You’re able to create different parameter groups that contain different settings for the same database engine, depending on your use case and how you’d like these parameters to be configured. Now the parameter group itself sits outside of the database, and this means that the same parameter group can be applied to multiple databases. So if you update the values within the parameter group, then this will update all the databases that use that same parameter group. Depending on which database engine you select, you are able to select an option group. And these option groups allow you to configure additional features to help you manage and secure your databases. Again, like parameter groups, they sit outside of the database itself.</p>
<p>Here we have our Backup section, so we can enable our automatic backups. If you don’t want this, you can simply un-tick it, but it’s pretty useful, so I tend to leave that enabled. And here we have our backup retention period. And you can select the number of days, up to 35 days. Just leave that at seven. We have a backup window. We can select a window, select in the time and the duration. So if you have a particular time that you’d like to run your backups, you can simply add in the hours and minutes, and also how long it should run for. If you don’t have a specific window, you can simply select no preference. If you have any tags for your database, you can copy that to your backup snapshots as well.</p>
<p>When it comes to encryption, you can either encrypt your database. The default is to have your database encrypted, and then you can select your key here. Now, this is the default AWS managed key for RDS, which is used by KMS, the key management service. or you can select your own CMK, your own customer master key, if you have a different one yourself. I’m just gonna leave it as the default AWS RDS managed key.</p>
<p>Down here, we have performance insights. Performance insights allows you to implement a level of performance tuning and monitoring, which enables you to see and review the load on your database, and if any actions should be taken. Here we can make some additional monitoring changes. We can enable enhanced monitoring, and we can set the granularity of this monitoring from anything from 60 seconds to one second. I’ll leave that as a default of 60 seconds for enhanced monitoring. And I’m just going to leave RDS to create the default role for that enhanced monitoring.</p>
<p>We can export our logs to Amazon CloudWatch Logs. Either the error, the general or the slow query log or all of them, or any combination. So if you want to export any of your logs to CloudWatch Logs for additional monitoring and queries then you can do so.</p>
<p>Then we have maintenance. We can enable or disable auto minor version upgrade. And here we can see that by enabling auto minor version upgrade, it will automatically upgrade to new minor versions as they are released. And the automatic upgrades occur during the maintenance window that we’ve scheduled for the database.</p>
<p>Now, speaking of maintenance window, we can select one here. So we can select a window. We can say on a particular day, that’s good for us, Saturday at four o’clock in the morning for two hours, for example, that could be our maintenance window. So if there’s any auto minor version upgrades to take place, then these will be scheduled during our maintenance window. And then finally you have deletion protection. And this simply prevents a database from being deleted accidentally.</p>
<p>Now at the very bottom, it has your estimated monthly costs. So we can see the cost of the database instance and also for the storage. Once you’re happy with all your options, simply click Create database. And now we can see that here’s our database instance, and we can see the status is Creating. And we have a message up here saying that the database might take a few minutes to launch. Okay, we now have a message that says the database has successfully been created. And it’s as simple as that.</p>
<h1 id="Amazon-DynamoDB"><a href="#Amazon-DynamoDB" class="headerlink" title="Amazon DynamoDB"></a>Amazon DynamoDB</h1><p>Hello and welcome to this lecture covering Amazon DynamoDB. Amazon DynamoDB is a NoSQL database, which means that it doesn’t use the common Structured Query Language, SQL. It falls into a category of databases known as key-value stores. A key value store is simply a collection of items or records, and you can look up data by using a primary key for each item or through the use of indexes.</p>
<p>Amazon DynamoDB is designed to be used for ultra high performance, which could be maintained at any scale with single-digit latency, making this a very powerful database choice used commonly for gaming, web, mobile and IoT applications to name but a few. Much like Amazon RDS, DynamoDB is also a fully managed service, taking many of the day-to-day administration operations out of your hands, giving you more time to focus on the business logic of your database. That’s one of the great things about Amazon DynamoDB, there’s no database administration required by us as a customer, no service to manage and nothing to back up. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> handles all of this for you. This makes the creation of a DynamoDB database very easy. All you have to do is set up your tables and configure the level of provision throughput that each table should have. Provision throughput refers to the level of read and write capacity that you want AWS to reserve for your table. You are charged for the total amount of throughput that you configure for your tables plus the total amount of storage space used by your data.</p>
<p>If we actually look at the configuration screen when creating a new DynamoDB database, as seen here, you can see that there are very few options required to create a new database. And in fact, in its simplest form, you can just provide a table name and a primary key, which is used to partition data across hosts for scalability and availability. You can then accept any remaining defaults and create your database, it’s as simple as that. DynamoDB tables are considered schemaless because there’s no strict design and schema that every record must conform to. As long as each item has an appropriate primary key, the item can contain varying sets of attributes. The records in a table do not need to have the same attributes or even the same number of attributes. This can be very convenient for rapid application development and if you want to add a new column to a table, you don’t need to alter the table, you can just start including the new field as an attribute when you insert new records. Likewise, you never need to adjust the data type for a column as DynamoDB generally isn’t interested in data types for individual attributes.</p>
<p>If when creating your DynamoDB database, you choose not to reset all the defaults, what other options exist? Let’s take a look. Unchecking the use default settings from the Table settings section provides you with the following. Firstly, you’ll be asked about secondary indexes, which allow you to perform queries on attributes that are not part of the table’s primary key. The default option provides no secondary index. However, you can add them here if required. DynamoDB lets you create additional indexes so that you can run queries to search your data by other attributes. If you’ve worked with relational databases, you’ve probably used indexes with those, but there are a couple of big differences in how indexes operate in DynamoDB.</p>
<p>First, each query can only use one index. If you want to query and match on two different columns, you need to create an index that can do that properly. Second, when you write your queries, you need to specify exactly which index should be used for each query. It’s not like a relational database that has a query analyzer, which can decide which indexes to use for our query. Here you need to be explicit and tell DynamoDB what index to use. DynamoDB has two different kinds of secondary indexes, global indexes let you query across the entire table to find any record that matches a particular value and by contrast, local secondary indexes can only help find data within a single partition key.</p>
<p>Following secondary indexes, you can modify the default settings applied to your table’s read&#x2F;write capacity mode. When you create a table in DynamoDB, you need to tell AWS how much capacity you want to reserve for the table. You don’t need to do this for disk space as DynamoDB will automatically allocate more space for your table as it grows. However, you do need to reserve capacity for input and output for reads and writes. Amazon charges you based on the number of read capacity units and write capacity units that you allocate. It’s important to allocate enough for your workload, but don’t allocate too much or DynamoDB could become prohibitively expensive.</p>
<p>By default, when you create a table in the AWS Console, Amazon will configure your table with five read capacity units and five write capacity units. There are two modes that you can choose from, provisioned and on-demand. Provisioned mode allows you to provision set read and writes allowed against your database per second by your application and is measured in capacity units, RCUs for reads and WCUs for writes. Depending on the transaction, each action will use one or more RCUs or WCUs. Provisioned mode is used generally when you have a predicted and forecasted workload of traffic. On-demand mode does not provision any RCUs or WCUs, instead they are scaled on demand. The downside is that it is not as cost effective as provisioned. This mode is generally used if you do not know how much workload you are expected to experience. Over time, you are likely to get more of an understanding of load and you can change your mode across to provisioned.</p>
<p>Once you have selected the provisioned mode, you will then have the opportunity to add configuration information relating to how your RCU and WCU are scaled as demand increases and decreases. As you can see, by entering your minimum and maximum provisioned capacity along with your target threshold utilization as a percentage, you can confidently rely on Amazon DynamoDB to manage the scaling operations of your throughput.</p>
<p>The last main point of the configuration allows you to set encryption of your tables, which is enabled by default for data at rest. Through the use of the key management service, KMS, you are able to select either a customer managed or AWS managed CMK to use for the encryption of your table instead of the default keys used by DynamoDB. For more information on CMKs and the key management service in general, please refer to our existing course found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Before I finish this lecture covering DynamoDB, I just want to cover some of its advantages and also what can be considered disadvantages. Some of the advantages of DynamoDB is that it’s fully managed by AWS, you don’t have to worry about backups or redundancy, although you’re welcome to set up these kinds of safeguards using some more advanced DynamoDB features.</p>
<p>As mentioned previously, DynamoDB tables are schemaless so you don’t have to define the exact data model in advance, the data model can change automatically to fit your application’s needs.</p>
<p>DynamoDB is designed to be highly available and your data is automatically replicated across three different availability zones within a geographic region. In the case of an outage or an incident affecting the entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>DynamoDB is designed to be fast, read and writes take just a few milliseconds to complete and DynamoDB will be fast no matter how large your table grows, unlike relational database, which can slow down as the table gets large. DynamoDB performance is constant and stays consistent even with tables that are many terabytes in size. You don’t have to do anything to handle this, except adjusting the provisioned throughput levels to make sure you’ve preserved enough read and write capacity for your transaction volume.</p>
<p>There are also some downsides to using DynamoDB too. As I just mentioned, your data is automatically replicated. Three copies are stored in three different availability zones and that replication usually happens quickly in milliseconds, but sometimes it can take longer and this is known as eventual consistency. This happens transparently and many operations will make sure that they’re always working on the latest copy of your data, but there are certain kinds of queries and table scans that may return older versions of data before the most recent copy. You need to be aware of how this works and you may need to adjust certain queries to require strong consistency.</p>
<p>DynamoDB’s queries aren’t as flexible as what you can do with SQL. If you are used to writing advanced queries with joins and groupings and summaries, you won’t be able to do that with DynamoDB. You’ll have to do more of the computation in your application code. This is done for performance reasons to ensure that every query finishes quickly and that complicated queries can’t hog the resources on a database server. </p>
<p>DynamoDB also has some strict limitations in the way you’re allowed to work with it. Two important limitations are the maximum record size of 400 kilobytes and the limit of 20 global indexes and five secondary indexes per table. There are other limitations that can be adjusted by contacting AWS customer support like the maximum number of tables in an AWS account.</p>
<p>Finally, although <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/demo-creating-dynamodb-database-2/">DynamoDB</a> performance can scale up as your needs grow, your performance is limited to the amount of read and write throughput that you’ve provisioned for each table. If you expect a spike of the database use, you’ll need to provision more throughput in advance or database requests will fail with a ProvisionedThroughputExceededException message. Fortunately, you can adjust throughput at any time and it only takes a couple of minutes to adjust. Still, this means that you’ll need to monitor the throughput being used in each table or you’ll risk running out of throughput if your usage grows.</p>
<h1 id="DEMO-Creating-a-DynamoDB-Database"><a href="#DEMO-Creating-a-DynamoDB-Database" class="headerlink" title="DEMO: Creating a DynamoDB Database"></a>DEMO: Creating a DynamoDB Database</h1><p>Hello, and welcome to this lecture. This is going to be a demonstration on how to quickly, and easily create a DynamoDB database. Now, first I’ll need to go to the database category, and here we can see <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/amazon-dynamodb-2/">DynamoDB</a>. Now I don’t have any DynamoDB databases sets up yet. So if you select create table, and you’ll be presented with this screen.</p>
<p>So first we’ll need to give it a table name, just call this my database and also a primary key. Now the primary key is essentially used to uniquely identify each item in the table. And the primary key is essentially comprised of a partition key. So let me just add one in. I’ll just call this product ID, and we can slate either a string, binary, or a number. I’ll leave that as a string. If we need to, we can also add in a sort key as well. And as we can see here the sort key simply allows you to search within a partition. Just remove that sort key.</p>
<p>Now essentially you can now create your table simply from providing that information, because this tick box here allows you to use lots of default settings that essentially fills in the rest of the configuration for you. So if you’re happy with your table name and primary key, with these default settings for your table, you can simply click create and it’s done. However, I want to uncheck the default settings so you can see the different configurable components used. So let’s take a look.</p>
<p>Now, firstly we have our secondary indexes, so you can add a secondary index, and these allow you to perform queries on attributes that are not part of the table’s primary key. Next, we have our read and write capacity mode, either provisioned or on demand. If we select the provisioned capacity mode, then we can select our read capacity units, and also our write capacity units.</p>
<p>Now scrolling down a bit further to auto scaling. We can set up auto scaling for our read and write capacity units. So when the read capacity gets to 70% utilization, we can scale up to a maximum of 40,000 units, and the same with the write capacity. So you can alter these figures if you need to, and change them to whatever values you need. As a part of that auto scaling process, DynamoDB needs an auto-scaling service link role to give it permission to do so.</p>
<p>Once you’re happy with your read and write capacity units, we can then scroll down to encryption at rest. Now by default encryption is enabled. The default option uses a key that’s owned by DynamoDB, and you are not charged for the use of any encryption keys in this default setting. However, you can use a KMS custom managed CMK, which is the CMK that you may have created, and you can select it from this box here, if you have any and enter the ARN, or you can use the KMS <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> managed CMK, which is this key here.</p>
<p>So it depends on the level of control that you want for the encryption key. First demonstration I’m just gonna leave it as the default. At the bottom here, you can add any text to a database if you’d like. So once you’re happy with the configuration, you simply click on create. As we can see the table is being created. And once it’s created, you can then use these tabs along the top to set up any alarms and review your capacity units set up your indexes, backups, etc, etc, etc. But for this demonstration, I simply wanted to show you how quickly and easy it is to set up and configure a DynamoDB table.</p>
<h1 id="Amazon-Redshift"><a href="#Amazon-Redshift" class="headerlink" title="Amazon Redshift"></a>Amazon Redshift</h1><p>Hello, and welcome to this lecture where I will look at Amazon Redshift. Amazon Redshift is a fast, fully-managed, petabyte-scale data warehouse. And it’s designed for high performance and analysis of information capable of storing and processing petabytes of data and provide access to this data, using your existing business intelligence tools, using standard SQL. It operates as a relational database management system, and therefore is compatible with other RDBMS applications. Redshift itself is based upon PostgreSQL 8.0.2, but it contains a number of differences from PostgreSQL. These differences are out of scope for this course, but for more information, please refer to the documentation <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">here</a>.</p>
<p>A data warehouse is used to consolidate data from multiple sources to allow you to run business intelligent tools, across your data, to help you identify actionable business information, which can then be used to direct and drive your organization to make effective data-driven decisions to the benefit of your company.</p>
<p>As a result, using a data warehouse is a very effective way to manage your reporting and data analysis at scale. A data warehouse, by its very nature, needs to be able to store huge amounts of data and its data may be subjected to different data operations such as data cleansing, which as an example, may identify, correct, replace or remove incomplete records from a table or recordset.</p>
<p>This can be expanded upon for the need to perform an extract, transform and load or an ETL job. This is the common paradigm by which data from multiple systems is combined to a single database data store or warehouse for legacy storage or analytics.</p>
<p>Extraction is the process of retrieving data from one or more sources. Either online, brick &amp; mortar, legacy data, Salesforce data and many others. After retrieving the data, ETL is to compute work that loads it into a staging area and prepares it for the next phase.</p>
<p>Transformation is the process of mapping, reformatting, conforming, adding meaning and more to prepare the data in a way that is more easily consumed. One example of this is the transformation and computation where currency amounts are converted from US dollars to euros.</p>
<p>Loading involves successfully inserting the transform data into the target database data store, or in this case, a data warehouse. All of this work is processed in what the business intelligent developers call an ETL job.</p>
<p>Now we have an understanding of what Amazon Redshift is. Let’s move on to looking at the architecture of the service and the components that is built upon.</p>
<p>Let me start with clusters and nodes. A cluster can be considered the main or core component of the Amazon Redshift service. And in every cluster, it will run its own Redshift engine, which will contain at least one database. As the name implies, a cluster is effectively a grouping of another component, and these being compute nodes.</p>
<p>Each will contain at least one compute node. However, if the cluster is provisioned with more than one compute node, then Amazon Redshift will add another component called a leader node.</p>
<p>Compute nodes all contain their own quantity of CPU attached storage and memory. And there are different nodes that offer different performances. For example, the following RA3 node types. Also, as you can see here, the dense compute node types.</p>
<p>The leader node of the cluster has the role of coordinating communication between your compute nodes in your cluster and your external applications accessing your Redshift data warehouse. So the leader node is essentially gateway into your cluster from your applications. When external applications are querying the data in your warehouse, the leader node will create execution plans, containing code to return the required results from the database.</p>
<p>If the query from the external application references tables associated with the compute nodes, then this code is then distributed to the compute nodes in the cluster to obtain the required data, which is then sent back to the leader node. If the query does not reference tables stored on the compute nodes, then the query will run on the leader node only.</p>
<p>Each compute node itself is also split into slices, known as node slices. A node slice is simply a partition of a compute node where the nodes memory and disk spaces split. Each node slice then processes operations given by the leader node where parallel operations can then be performed across all slices and all nodes at once for the same query. As I mentioned previously, compute nodes can have different capacities and these capacities determine how many slices each compute node can be split into.</p>
<p>When creating a table, it is possible to distribute rows of that table across different nodes slices based upon how the distribution case is defined for the table. For a deeper understanding on how to select the best distribution style, please see the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">here</a>.</p>
<p>When your Amazon Redshift database is created, you will of course connect to it using your applications. Typically these applications will be your analytic and business intelligence tools, that you’re running with your organization. Communication between your BI applications and Redshift, will use industry standard open database connectivity, ODBC. And Java database conductivity, JDBC drivers for PostgreSQL.</p>
<p>The performance that Amazon Redshift can generate is a huge benefit to many organizations. In fact, at the time of writing this course, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> currently boasts that it’s three times faster than other cloud data warehouses.</p>
<p>From a query perspective, Amazon Redshift has a number of features to return results quickly and effectively. Let’s take a look at a few of them.</p>
<p>Firstly, massively parallel processing. As highlighted in the previous section by associating rows from tables across different nodes slices and nodes. It allows the node leader to generate execution plans, to distribute crews from external applications across multiple compute nodes at once, allowing them to work together to generate the end result, which is an aggregated by the leader node.</p>
<p>Columnar data storage. This is used as a way of reducing the number of times the database has to perform disk I&#x2F;O, which helps to enhance query performance. Reducing the data retrievals from the disk means there is more memory capacity to carry out in memory processing of the query results. Result caching. Caching in general is a great way to implement a level of optimization.</p>
<p>Result caching helps to reduce the time it takes to carry out queries by caching some results of the queries in the memory of the leader node in a cluster. As a result, when a query is submitted, the leader node will check its own cache copy of the results and if a successful match is found, the cached results are used instead of executing another query on your Redshift cluster.</p>
<p>Amazon Redshift also integrates with Amazon CloudWatch, allowing you to monitor the performance of your physical resources, such as CPU utilization and throughput. In addition to this, Redshift also generates query and load performance data that enables you to track overall database performance. Any data relating to query and load performance is only accessible from within the Redshift console itself and not Amazon CloudWatch.</p>
<p>During the creation of your Redshift cluster, you can as an optional element, select up to 10 different IAM roles to associate with your cluster. This allows you to grant the Amazon Redshift principle, redshift.amazonaws.com access to other services on your behalf, for example, Amazon S3 where you might have a data lake. Accessing data within S3 will require a set of credentials to authorize Redshift access to S3. And the best way to do that is by using an IAM role. Therefore, if you intend to perform actions such as this when using your Amazon Redshift cluster, you might need to consider which access you need and what roles you will need to create.</p>
<p>To learn more about IAM and roles, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">here</a>. In the next lecture, I want to show you how to create a new Redshift cluster.</p>
<h1 id="DEMO-Creating-an-Amazon-Redshift-Cluster"><a href="#DEMO-Creating-an-Amazon-Redshift-Cluster" class="headerlink" title="DEMO: Creating an Amazon Redshift Cluster"></a>DEMO: Creating an Amazon Redshift Cluster</h1><p>Hello and welcome to this lecture. This is going to be a quick demonstration on how to set up an Amazon Redshift cluster. So as you can see, I’m in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console at the moment, and to find Redshift, we can scroll down to the database category and we can see Amazon Redshift here. So if we select on that, we’re then taken to this splash screen here. So I don’t have any Redshift clusters at the moment.</p>
<p>So to start with, all I need to do is to click on the orange create cluster button, and this takes us to the configuration page. And the first thing we need to do is give it a name. So I’m just going to call this my cluster. Then we can choose our node type. So we have the RA3 nodes and the dense compute nodes here. Now, AWS recommends the RA3 nodes due to the high performance and the managed storage aspect, or you have the dense compute nodes here.</p>
<p>Now, for this demonstration, I’m just going to select a dense compute node. Now, if you scroll down, we can select the number of nodes that we would like. As you can see, it ranges from one to 32, so we can scroll up or down. And also if you see in this configuration summary section the cost per month, and the total compressed storage will also change with the amount of nodes that I select. So there you can see it is scrolling up and down. I’m just going to leave it as two nodes.</p>
<p>Now I can scroll down to the database configurations. We can give the database a name, and also the port that it’s going to be using, and also a master username and password. So let me just enter a password. And then we have cluster permissions. Now, this is an optional step. So if you want your AWS Redshift cluster to interact with other AWS services on your behalf, for example, maybe Amazon S3, you might want to import data, then you can associate an IAM role that has access to S3 to allow that process to happen. But as I said, this is an optional component.</p>
<p>Now, at the very bottom here, we have additional configuration. Now, these are the default settings. So we have a default network, default backup options, maintenance, default security groups, and also a parameter group, as well. But if you turn off those default settings, then you can go through and modify any of those components. For example, network and security. You can select the VPC for it to run in. You can select the security groups that are associated with your clusters to define what resources can access it. You can also define a subnet group which defines what subnets that the clusters will be launched in and also any availability zones. You can also specify if you want any cluster traffic to purely route through your VPC and if you want your cluster to be publicly accessible or not. So there’s a few network and security features that you can change there.</p>
<p>Looking at database configurations, here you can select a parameter group if you have any configured, and you can also configure any encryption using AWS KMS, and if you want to use the default Redshift key, or if you want to use one of your own CMKs, for example, I have a CMK here in my account. For this demonstration, I’m just gonna disable encryption.</p>
<p>Under maintenance, you can set a maintenance window so that the day and time of the week that any maintenance will be carried out to your cluster. And also you can specify which cluster version you’d like, and you have three options. Either use the most current approved cluster version, use the cluster version before the current version, or use the cluster version with beta releases of new versions. I’ll just leave that as current.</p>
<p>Under monitoring, you can have CloudWatch alarms. So for example, you can create a new alarm for disk usage threshold when that reaches 80%, and then you can notify people via an SNS topic that you might already have configured. I’ll say no alarms. And finally, backup. And also you can specify your snapshot retention, which is how long you’ll keep the backups for. And finally, if you want to configure cross-region snapshot, you can either enable that or disable it. And this will back up your cluster to a different region. So if you enable it, you can then select an alternate region to where your cluster currently resides. I’m just going to disable that.</p>
<p>So there are the different options that are available, but I’m just going to select the defaults that it already suggested. And then once you’re happy with your settings, simply click create cluster. As we can see here now, it’s now creating our cluster. This might take a few minutes, so I’ll come back when that’s done.</p>
<p>Okay, as you can see, the cluster is now available. If we select the dashboard, then we can see that we have one new cluster in the Ireland region with two nodes, and we can see that it’s already taken an automated snapshot, as well.</p>
<p>So cluster overview here, so we can see a number of queries, any database connections, disk space used, CPU utilization. As you can see, there’s not much going on at the moment. We’ve simply just created it. If we have any alarms, and down here, any events, and also a query overview here. So I won’t go into any more detail than that.</p>
<p>This is just a very high-level, quick introduction on how to create an Amazon Redshift cluster. And that’s it.</p>
<h1 id="7Amazon-Relational-Database-Service"><a href="#7Amazon-Relational-Database-Service" class="headerlink" title="7Amazon Relational Database Service"></a>7<strong>Amazon Relational Database Service</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">Instance Types Performance</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">Course: When to use RDS Multi-AZ &amp; Read Replicas</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Shared Responsibility Model Security</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Lab: Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Lab: Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="9Amazon-DynamoDB"><a href="#9Amazon-DynamoDB" class="headerlink" title="9Amazon DynamoDB"></a>9<strong>Amazon DynamoDB</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Dat</a></p>
<h1 id="11Amazon-Redshift"><a href="#11Amazon-Redshift" class="headerlink" title="11Amazon Redshift"></a>11<strong>Amazon Redshift</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">Differences between Redshift and PostgreSQL</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">Selecting a Distribution Style</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Course: Overview of AWS IAM</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/" rel="prev" title="AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12">
      <i class="fa fa-chevron-left"></i> AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/" rel="next" title="AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14">
      AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-AWS-Database-Landscape"><span class="nav-number">2.</span> <span class="nav-text">The AWS Database Landscape</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Relational-Databases"><span class="nav-number">3.</span> <span class="nav-text">Relational Databases</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lectures"><span class="nav-number">3.0.1.</span> <span class="nav-text">Lectures</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#NoSQL-Databases"><span class="nav-number">4.</span> <span class="nav-text">NoSQL Databases</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Types-of-Managed-NoSQL-on-AWS-Part-1"><span class="nav-number">5.</span> <span class="nav-text">Types of Managed NoSQL on AWS - Part 1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Types-of-Managed-NoSQL-on-AWS-Part-2"><span class="nav-number">6.</span> <span class="nav-text">Types of Managed NoSQL on AWS - Part 2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Relational-Database-Service"><span class="nav-number">7.</span> <span class="nav-text">Amazon Relational Database Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-an-Amazon-RDS-Database"><span class="nav-number">8.</span> <span class="nav-text">DEMO: Creating an Amazon RDS Database</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-DynamoDB"><span class="nav-number">9.</span> <span class="nav-text">Amazon DynamoDB</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-a-DynamoDB-Database"><span class="nav-number">10.</span> <span class="nav-text">DEMO: Creating a DynamoDB Database</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-Redshift"><span class="nav-number">11.</span> <span class="nav-text">Amazon Redshift</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DEMO-Creating-an-Amazon-Redshift-Cluster"><span class="nav-number">12.</span> <span class="nav-text">DEMO: Creating an Amazon Redshift Cluster</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7Amazon-Relational-Database-Service"><span class="nav-number">13.</span> <span class="nav-text">7Amazon Relational Database Service</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#9Amazon-DynamoDB"><span class="nav-number">14.</span> <span class="nav-text">9Amazon DynamoDB</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#11Amazon-Redshift"><span class="nav-number">15.</span> <span class="nav-text">11Amazon Redshift</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
