<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Management (SAA-C03) IntroductionHello, and welcome to this course on AWS management fundamentals, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - A">
<meta property="og:type" content="article">
<meta property="og:title" content="AWS-Solution-Architect-Associate-Management-SAA-C03-43">
<meta property="og:url" content="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:description" content="Management (SAA-C03) IntroductionHello, and welcome to this course on AWS management fundamentals, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - A">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid6-3a3ff769-6937-455b-95ea-fe227962f0ae.png">
<meta property="og:image" content="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid4-e35d3347-e755-45e7-88d0-dc6c2fb51434.png">
<meta property="og:image" content="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid2-992c4663-2ed4-4f5d-89d1-33e7cfcd817a.png">
<meta property="article:published_time" content="2022-11-19T02:14:01.000Z">
<meta property="article:modified_time" content="2022-11-28T14:52:06.000Z">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid6-3a3ff769-6937-455b-95ea-fe227962f0ae.png">

<link rel="canonical" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>AWS-Solution-Architect-Associate-Management-SAA-C03-43 | Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AWS-Solution-Architect-Associate-Management-SAA-C03-43
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:14:01" itemprop="dateCreated datePublished" datetime="2022-11-18T22:14:01-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-28 10:52:06" itemprop="dateModified" datetime="2022-11-28T10:52:06-04:00">2022-11-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Management-SAA-C03-Introduction"><a href="#Management-SAA-C03-Introduction" class="headerlink" title="Management (SAA-C03) Introduction"></a>Management (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on AWS management fundamentals, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the various management services currently available in AWS that may be covered on the exam.</p>
<p>Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#x70;&#x70;&#x6f;&#x72;&#x74;&#64;&#x63;&#108;&#x6f;&#117;&#100;&#97;&#99;&#97;&#100;&#101;&#x6d;&#121;&#46;&#99;&#111;&#109;">&#x73;&#117;&#x70;&#x70;&#x6f;&#x72;&#x74;&#64;&#x63;&#108;&#x6f;&#117;&#100;&#97;&#99;&#97;&#100;&#101;&#x6d;&#121;&#46;&#99;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various management services in AWS in preparation for the exam.</p>
<p>The objective of this course is to provide a high-level introduction to the various management services in AWS that can assist with auditing, reporting, and monitoring within your AWS environments, including:</p>
<ul>
<li>Amazon CloudWatch,</li>
<li>AWS CloudTrail, and</li>
<li>AWS Config.</li>
</ul>
<p>You’ll learn about different approaches for managing multi-account environments in AWS using AWS Organizations and AWS Control Tower. We’ll take a look at AWS Systems Manager and see how it can be used to manage your applications and resources. We’ll then spend some time discussing logging in AWS, along with an in-depth discussion of logging options for AWS services including CloudWatch, CloudTrail, CloudFront, and VPC Flow Logs. And given the exam’s emphasis on designing cost-optimized architectures, we’ll delve into AWS cost management services including Cost Explorer, Cost and Usage Reports, and Budgets.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#117;&#112;&#x70;&#x6f;&#x72;&#x74;&#64;&#x63;&#108;&#x6f;&#x75;&#x64;&#97;&#99;&#x61;&#x64;&#101;&#109;&#121;&#x2e;&#x63;&#111;&#109;">&#115;&#117;&#112;&#x70;&#x6f;&#x72;&#x74;&#64;&#x63;&#108;&#x6f;&#x75;&#x64;&#97;&#99;&#x61;&#x64;&#101;&#109;&#121;&#x2e;&#x63;&#111;&#109;</a>. Thank you!</p>
<h1 id="What-is-Amazon-CloudWatch"><a href="#What-is-Amazon-CloudWatch" class="headerlink" title="What is Amazon CloudWatch?"></a>What is Amazon CloudWatch?</h1><p>Hello and welcome to this lecture which will provide you with a high-level overview of what Amazon CloudWatch is and does.</p>
<p>Amazon CloudWatch is a global service that has been designed to be your window into the health and operational performance of your applications and infrastructure. It’s able to collate and present meaningful operational data from your resources allowing you to monitor and review their performance. This gives you the opportunity to take advantage of the insights that CloudWatch presents, which in turn can trigger automated responses or provide you with the opportunity and time to make manual operational changes and decisions to optimize your infrastructure if required. </p>
<p>Understanding the health and performance of your environment is one of the fundamental operations you can do to help you minimize incidents, outages and errors. As a result Amazon CloudWatch is heavily used by those in an operational role and site reliability engineers. </p>
<p>There are a wide range of components to Amazon CloudWatch, making this an extremely powerful service. Let me now run through at a high level some of these features and what they allow you to do, including CloudWatch Dashboards, CloudWatch Metrics and Anomaly Detection, CloudWatch Alarms, CloudWatch EventBridge, CloudWatch Logs, CloudWatch Insights.</p>
<p>Using the AWS Management console, the AWS CLI, or the PutDashboard API, you can build and customize a page using different visual widgets displaying metrics and alarms relating to your resources to form a unified view. These dashboards can then be viewed from within the AWS Management Console.</p>
<p>Here is an example of the different types of widgets you can select to build your dashboard.</p>
<p>The resources within your customized dashboard can be from multiple different regions making this a very useful feature. Being able to build your own views, you can quickly and easily design and configure different dashboards to represent the data that you need to see from a business and operational perspective. For example, you might need to view all performance metrics and alarms from resources relating to a particular project, or a specific customer. Or you might want to create a different dashboard for a specific region or application deployment. The key point is that they are fully customizable to be designed how YOU want to represent your data.  </p>
<p>For more information of selecting the right chart type to visualize data, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/">https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/</a></p>
<p>Once you have built your Dashboards, you can easily share them with other users, even those who may not have access to your AWS account. This allows you to share the findings gathered by CloudWatch with those who may find the results interesting and beneficial to their day-to-day operational role, but don’t necessarily require the need to access your AWS account.</p>
<p>Metrics are a key component and fundamental to the success of Amazon CloudWatch, they enable you to monitor a specific element of an application or resource over a period of time while tracking these data points. For example, the number of DiskReads or DiskWrites on an EC2 instance, these are just 2 metrics relating to EC2 that you can monitor. Different services will offer different metrics, for example, there is no DiskReads for Amazon S3 as it’s not a compute service, and so instead metrics relevant to the service are available, such as NumberOfObjects, which tracks the number of objects in a specified bucket.</p>
<p>By default when working with Amazon CloudWatch, everyone has access to a free set of Metrics, and for EC2, these are collated over a time period of 5 minutes. However, for a small fee, you can enable detailed monitoring which will allow you to gain a deeper insight by collating data across the metrics every minute. In addition to detailed monitoring, you can also create your own custom metrics for your applications, using any time-series data points that you need, but be aware that when you create a metric they are regional, meaning that any metrics created in 1 region will not be available in another.</p>
<p>CloudWatch metrics also allow you to enable a feature known as anomaly detection. This allows CloudWatch to implement machine learning algorithms against your metric data to help detect any activity that sits outside of the normal baseline parameters that are generally expected. Advance warning of this can help you detect an issue long before it becomes a production problem.</p>
<p>Amazon CloudWatch Alarms tightly integrate with Metrics that I just discussed and they allow you to implement automatic actions based on specific thresholds that you can configure relating to each metric.</p>
<p>For example, you could set an alarm to activate an auto scaling operation, such as provisioning another instance if your CPUUtilization of an EC2 instance peaked at 75% for more than 5 minutes. You could also configure an alarm to send a message to an SNS Topic when the same instance drops back below the 75% threshold, causing it to come out of an ‘alarm’ state notifying engineers of the change. </p>
<p>For more information on SNS, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/">https://cloudacademy.com/course/using-sqs-sns-ses/</a></p>
<p>Speaking of Alarm states, there are 3 different states for any alarm associated with a metric, these being OK – The metric is within the defined configured threshold, ALARM – The metric has exceeded the thresholds set, and INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.</p>
<p>CloudWatch alarms are also easily integrated with your dashboards as well, allowing you to quickly and easily visualize the status of each alarm. When an alarm is triggered into a state of ALARM, it will turn red on your dashboard, giving a very obvious indication.</p>
<p>CloudWatch EventBridge is a feature that has evolved from an existing feature called Amazon Events. So if you have any prior experience working with CloudWatch Events then this will be fairly familiar to you.  </p>
<p>CloudWatch EventBridge provides a means of connecting your own applications to a variety of different targets, typically AWS services, to allow you to implement a level of real-time monitoring, allowing you to respond to events that occur in your application as they happen.  </p>
<p>But what is an event? Basically, an event is anything that causes a change to your environment or application.</p>
<p>The big benefit of using CloudWatch EventBridge is that it offers you the opportunity to implement a level of event-driven architecture in a real-time decoupled environment. </p>
<p>EventBridge establishes a connection between your applications and specified targets to allow a data stream of events to be sent. Currently, there is a wide range of targets that can be used as a destination for events as you can see here.</p>
<p>For the latest list of targets, please see the relevant documentation here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html</a></p>
<p>Let me provide a quick level overview of some of the elements of this feature, and these include Rules, Targets, and Event Buses.</p>
<p>So starting with Rules. A rule acts as a filter for incoming streams of event traffic and then routes these events to the appropriate target defined within the rule. The rule itself can route traffic to multiple targets, however the target must be in the same region. </p>
<p>Next, we have Targets. We saw a list of these just a few moments ago, so targets and where the events are sent by the Rules, such as AWS Lambda, SQS, Kinesis or SNS. All events received by the target are done os in a JSON format</p>
<p>Now finally, Event Buses. An Event Bus is the component that actually receives the Event from your applications and your rules are associated with a specific event bus. CloudWatch EventBridge uses a default Event bus that is used to receive events from AWS services, however, you are able to create your own Event Bus to capture events from your own applications. </p>
<p>CloudWatch Logs gives you a centralized location to house all of your logs from different AWS services that provide logs as an output, such as CloudTrail, EC2, VPC Flow logs, etc, in addition to your own applications.</p>
<p>When log data is fed into Cloudwatch Logs you can utilize CloudWatch Log Insights to monitor the logstream in real time and configure filters to search for specific entries and actions that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. </p>
<p>An added advantage of CloudWatch logs comes with the installation of the Unified CloudWatch Agent, which can collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. This metric data is in addition to the default EC2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. </p>
<p>There are now 3 different types of insights within CloudWatch, there are Log Insights, Container Insights, and Lambda Insights.</p>
<p>But what exactly are insights? Well as the name suggests, they provide the ability to get more information from the data that CloudWatch is collecting. So let’s look at each of these at a high level to understand the role that they perform, starting with Log Insights.</p>
<p>This is a feature that can analyze your logs that are captured by CloudWatch Logs at scale in seconds using interactive queries delivering visualizations that can be represented as bar, line, pie, or stacked area charts. The versatility of this feature allows you to work with any log file formats that AWS services or your applications might be using.</p>
<p>Using a flexible approach, you can use Log insights to filter your log data to retrieve specific data allowing you to gather insights that you are interested in. Also using the visual capabilities of the feature, it can display them in a visual way.</p>
<p>Much like Log insights, Container Insights allow you to collate and group different metric data from different container services and applications within AWS, for example, the Amazon Elastic Kubernetes Service, (EKS) and the Elastic Container Service (ECS). </p>
<p>In addition to the standard metrics collected for these services by CloudWatch, Container Insights also allows you to capture and monitor diagnostic data giving you additional insights into how to resolve issues that arise within your container architecture. This monitoring and insight data can be analyzed at the cluster, node, pod, and task level making it a valuable tool to help you understand your container applications and services.</p>
<p>As you may have guessed by now, this feature provides you the opportunity to gain a deeper understanding of your applications using AWS Lambda. Working on the principles as we have seen with the previous 2 insight features, it gathers and aggregates system and diagnostic metrics related to AWS Lambda to help you monitor and troubleshoot your serverless applications.</p>
<p>To enable Lambda Insights, you need to enable the feature per Lambda function that you create within Monitoring Tools section of your function:</p>
<p>This ensures that a CloudWatch extension is enabled for your function allowing it to collate system-level metrics which are recorded every time the function is invoked.</p>
<h1 id="What-is-AWS-CloudTrail"><a href="#What-is-AWS-CloudTrail" class="headerlink" title="What is AWS CloudTrail?"></a>What is AWS CloudTrail?</h1><p>Hello, and welcome to this lecture. In this lecture, I will explain the basic fundamentals of AWS CloudTrail to give you an overview of the service, before we look deeper at the inner workings revealing how the different elements work together.</p>
<p>So what is CloudTrail and what does it do? CloudTrail is a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface, from within the AWS Management Console, or even from a request made by another AWS service.</p>
<p>For example, when Auto Scaling automatically sends an API request to launch or terminate an instance, these API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file, which is then stored on S3. Each API call represents a new event within the log file.</p>
<p>CloudTrail also records and associates other identifying metadata with all the events. For example, the identity of the caller, the timestamp of when the request was initiated, and the source IP address. For greater management, new log files are typically created every five minutes, which are then delivered and stored within an S3 bucket that is defined by you during your CloudTrail configuration. This allows you to easily go back and review the history of all API requests made.</p>
<p>There’s also an option to have these logs delivered to a CloudWatch Logs log file as well. Having this association with CloudWatch enables custom metrics to be converted, to monitor specific API requests. Thresholds can be set against these metrics and when crossed the Simple Notification Service, SNS, can be triggered to notify your security teams to investigate. That, at a very high level, is the overall function of the AWS CloudTrail service.</p>
<p>Now let’s take a look at the CloudTrail architecture to understand where it can be implemented from an AWS region standpoint and which services can be supported. AWS CloudTrail is a global service with support for all regions.</p>
<p>In addition to this worldwide coverage CloudTrail also provides support for over 60 AWS services and features across a wide range of service categories. As you can imagine with this extensive coverage, CloudTrail can capture a vast amount of data if you have a multi-region, multi-service infrastructure environment deployed.</p>
<p>So armed with this information, what can you do with it? How can you use this data to help you manage and support your AWS infrastructure? Well there are a number of ways you can use the data captured by CloudTrail to help you enhance your AWS environment.</p>
<p>Firstly, it can be used very effectively as a security analysis tool. CloudTrail events provide very specific information about where an API call originated from and who or what initiated the request. As a result, if malicious activity was detected via irregular trends or restricted API call thresholds with the use of CloudWatch, then a number of security controls can be quickly implemented to prevent the user from causing additional damage.</p>
<p>Another common use for CloudTrail is to help resolve and manage day-to-day operational issues and problems. Using built-in filtering mechanisms it’s possible to quickly find who, what, and when a particular API was used, which could have potentially caused an outage or service interruption. This enables quicker root cause identification resulting in a speedy resolution. Appropriate actions could then be taken to ensure the incident does not reoccur in your environment. As API calls to add, modify, or delete resources are captured, CloudTrail can be an effective method of tracking changes to resources within your environment.</p>
<p>There is another AWS service that is specifically designed to audit and track changes to resources, which is called AWS Config, which CloudTrail interacts with. However, CloudTrail can be used to capture the actual API request and all associated data, which made the change. And if you’re not using AWS Config, then this at least provides some base level of monitoring and tracking.</p>
<p>From a governance and security legislation perspective, many certifications require the ability to recall and provide evidence of log files relating to specific changes to resources. CloudTrail provides all of this by default through the use of capturing events and writing them to a log file, which is then stored on S3. AWS has a great white paper on achieving compliance using CloudTrail entitled “Logging in AWS How AWS CloudTrail can help you achieve compliance by logging API calls and changes to resources.” The following <a target="_blank" rel="noopener" href="https://d1.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf">URL</a> will take you to that white paper.</p>
<p>If you need to be able to capture and track API requests within your AWS account, for any of these reasons mentioned, or perhaps for other reasons you may have of your own, then CloudTrail can do this for you and deliver the output as a log file into an S3 bucket of your choice.</p>
<h1 id="AWS-CloudTrail-Operations"><a href="#AWS-CloudTrail-Operations" class="headerlink" title="AWS CloudTrail Operations"></a>AWS CloudTrail Operations</h1><p>Hello, and welcome to this lecture. Where we are to discuss the different features and components of CloudTrail and how they work together to provide a customizable API call tracking and monitoring solution. </p>
<p>Firstly, let’s look under the hood of CloudTrail to see what makes up the core features and components that create the service.</p>
<ul>
<li>Trails. These are the building blocks of the service. You can create many different Trails containing different configurations related to API requests that you want to capture.</li>
<li>S3. S3 is used by default to store the CloudTrail log files and a dedicated S3 bucket is required during the creation of a new Trail.</li>
<li>Logs. Logs are created by AWS CloudTrail and record all events captured. A new log file is created approximately every five minutes and once processed, it is delivered to an S3 bucket as defined by its Trail configuration. If no API calls to be made, then no logs will be delivered.</li>
<li>KMS. The use of AWS KMS is an optional element of CloudTrail, but it allows additional encryption to be added to your log files when stored on S3.</li>
<li>SNS. SNS is also an optional component for CloudTrail, but it allows for you to create notifications. For example, when a new log file is delivered to S3 SNS could notify someone or a team via an email, or it could be used in conjunction with CloudWatch. When metric thresholds have been reached.</li>
<li>CloudWatch logs. Again, this is another optional component. For AWS CloudTrail allows you to deliver its logs to AWS as CloudWatch logs, as well as S3 for specific monitoring metrics to take place.</li>
<li>Events Selectors. Events Selectors allowed to add a level of customization to the type of API requests, you want the corresponding trails to capture.</li>
<li>Tags, Tags allow you to assign your own metadata to your Trail. For example, you could add a project or department tag indicating which project or department the trail relates to.</li>
<li>Events. For every API request that is captured by CloudTrail it is recorded as an event in a CloudTrail log file. API Activity Filters. These are search filters that can be applied against your API activity history in the management console for create, modify and delete API calls. These events are held in the management console for seven days. Even if the Trails itself is stopped or deleted.</li>
</ul>
<p>Okay, so I’ve now covered the different components that essentially build CloudTrail. Let me now introduce you to the process at a high level of how all of this fits together and in what order. The very first step is to create a Trail. If no Trail exists, then CloudTrail does not know what API cost to capture from which region and which services, global or otherwise. During this Trail creation, you will need to specify an S3 bucket for your log files to be delivered to you. This can be an existing bucket or new bucket, which can be created at this stage.</p>
<p>You will then have an option to encrypt your log files with the key management service KMS, if required. Also if required, you can configure the Simple Notification Service SNS to notify you when new log files are delivered. If you want to add another layer of security, integrity, then you can enable log file validation, which will ensure your logs have not been modified or tampered with since they delivered to S3. Your Trail is now ready to be turned on and created.</p>
<p>Once it has been created, you are able to make further configurational changes that are not available during the Trail creation itself. Upon selection of your new Trail you will have the option to configure CloudWatch logs, which allows you to deliver your cloud Trails log files to CloudWatch in addition to S3. This allows you to create CloudWatch monitoring metrics against specific API calls and will receive notification from SNS when custom thresholds are reached.</p>
<p>Another optional configurable element is that of CloudTrail Event Selectors. This allows you to specify the types of events, management or data that CloudTrail logs. Finally, at the last element of your newly created Trail configuration, there is the ability to add tags just as you would with other resources within AWS. At this point, your Trail is configured and actively recording API calls as per your configuration.</p>
<p>For every API call that matches the requirement of your Trail, it will be captured and recorded in a log file as an event. Each API call will be recorded as a new event. Once you’ve captured the data, you may need to find a particular event quickly, maybe for security reasons. This can be achieved using API Activity Filters, which can be found within the CloudTrail service from the management console.</p>
<p>So from a high-level perspective, we know how a Trail is configured, but what happens when an API matching a Trail is called upon by user or service? Let’s take a quick look.</p>
<p>A user or service calls upon an API. Next CloudTrail checks to see if this API call matches any configured Trails. If a match is found CloudTrail records the API as an event within its current log file. It also associates other identical metadata mentioned earlier. Eventually, the event with the log file will be delivered to S3 and possibly CloudWatch logs, depending on the Trail configuration. If it is sent to CloudWatch logs, the log file will be monitored by any configured metrics. When in S3, the log file will be stored and the default server-side encryption SSE, and unless KMS has been configured for increased security measures with the associated Trail. If any S3 lifecycle cycle rules are applied to the bucket, then over time, the log file may be archived to a different storage class or even glassier.</p>
<p>That essentially concludes the elements of CloudTrail and how they work together.</p>
<h1 id="What-is-AWS-Config"><a href="#What-is-AWS-Config" class="headerlink" title="What is AWS Config?"></a>What is AWS Config?</h1><p>Hello, and welcome to this lecture where we will talk about the AWS Config Service itself, what it is and what it does. So let’s get started. As many of you will be aware, one of the biggest headaches in any organization when it comes to resource management of IT infrastructure is understanding the following, what resources do we have? What devices are out there within our infrastructure performing function? Do we have resources that are no longer needed? And therefore, can we be saving money by switching them off? What is the status of their current configuration? Are there any security vulnerabilities we need to worry about? How are our resources linked within the environment? What relationships are there and are there any dependencies? If we make a change to one resource, will this affect another? What changes have occurred on the resources and by whom? Do we have a history of changes for this resource that shows us how the resource has changed over time? Is the infrastructure compliant with specific governance controls? And how can we check to ensure that this configuration is meeting specific internal and external requirements? And do we have accurate auditing information that can be passed to external auditors for compliance checks?</p>
<p>Depending on the size of your deployment with AWS, trying to answer some of these questions can be very time consuming and laborious. Some of this information can be captured via the AWS CLI by performing a describe or list against the specific resource. But implementing a system to capture those results and output them into a readable format could be very resource intensive. And of course, this will only help you with a small piece of the puzzle.</p>
<p>AWS is aware that due to the very nature of the cloud and its benefits, the resources within an AWS environment are likely to fluctuate frequently along with the configurations of the resources. The cloud by its very nature is designed to do so. And so trying to keep up with the resource management can be a struggle. Because of this, AWS released AWS Config to help with this very task. The service has been designed to record and capture resource changes within your environment, allowing you to perform a number of actions against the data that helps to find answers to the questions that we highlighted previously.</p>
<p>So what did AWS design AWS Config to do? Well, in a nutshell, AWS Config can capture resource changes. So any change to a resource supported by Config can be recorded which will record what changed along with other useful metadata all held within a file known as a Configuration Item, a CI. It can act as a resource inventory.</p>
<p>AWS Config can discover supported resources running within your environment allowing you to see data about that resource type. You can store configuration history for individual resources. The service will record and hold all existing changes that have happened against the resource providing a useful history record of changes. It can provide a snapshot in time of current resource configurations.</p>
<p>An entire snapshot of all supported resources within a region can be captured that will detail their current configurations with all related metadata. Enable notifications of when a change has occurred on a resource. The Simple Notification Service, SNS is used with AWS Config to capture a configuration stream of changes enabling you to process and analyze the changes to resources.</p>
<p>It can provide information on who made the change and when through AWS CloudTrail integration. AWS CloudTrail is used with AWS Config to help you identify who made the change and when and with which API. You can enforce rules that check the compliancy of your resource against specific controls.</p>
<p>Predefined and custom rules can be configured with AWS Config allowing you to check resources compliance against these rules. You can perform security analysis within your AWS environment. A number of security resources can be recorded. And when this is coupled with rules relating to security such as encryption checks, this can become a powerful analysis tool. And it can provide relationship connectivity information between resources.</p>
<p>The AWS Management Console provides a great relationship query, allowing you to quickly see and identify which resources are related to any other resource. For example, when looking at an EBS volume, you’ll be able to see which EC2 instance it is connected to. And it does all of this and presents the data in a friendly format. This is a lot of incredibly useful data that can be used across a range of different scenarios.</p>
<p>Now, unfortunately, at the time of writing this course, the AWS Config Service does not capture this information for all services, but it certainly captures data for the most common services and resources which you would want to hold information for. Services such as EC2, RDS, IAM, and VPC. And it’s great to see that within each of these, there are specific security resources that are covered such as security groups and custom IAM policies.</p>
<p>This makes AWS Config very useful when it comes to carrying out a security analysis. For more information on the latest resources that AWS Config supports. Please see the link on screen. AWS Config is region specific, meaning that if you have resources in multiple regions, then you will have to configure AWS Config for each region you want to record resource changes for.</p>
<p>When doing so, you are able to specify different options for each region. For example, you could configure Config in one region to record all supportive resources across all services within that region. And that at a predefined AWS managed Config rule that will check if EBS volumes are encrypted.</p>
<p>In another region, you could select to only record a specific type of resource such as security groups with no predefined rules allocated. Some of you may be wondering what if the service you want to monitor is not region specific such as IAM? well, in this case, there was a separate option to include Global Services, which IAM falls under.</p>
<h1 id="Key-Components-of-AWS-Config"><a href="#Key-Components-of-AWS-Config" class="headerlink" title="Key Components of AWS Config"></a>Key Components of AWS Config</h1><p>Hello, and welcome to this lecture. Where we will be taking a look at the components that make up the AWS Config Service and the functions that each of them carry in delivering the service. To understand how to get the most of the service, it’s important to understand how the services piece together and how it works. So let’s start by identifying the key components. Following this, we will then break each of them down to understand their role and how they sit within the service.</p>
<p>The following identifies the main components to the service. AWS resources, configuration items, configuration streams, configuration history, configuration snapshots, configuration recorder, config rules, resource relationships, SNS topics, S3 buckets and AWS Config permissions. Let’s now take a look at each of these and turn in more detail. Starting with AWS resources.</p>
<p>AWS resources are typically classed as objects that can be created, updated or deleted from within the AWS Management Console or programmatically, using the AWS CLI or supported STK. AWS Config records changes to supported AWS resources within a specific Region. For the current list of supported services and resources, please follow the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-resources">link</a> on screen.</p>
<p>A configuration item or CI as is known, is a key component of AWS Config. It is comprised of a JSON file that holds the configuration information, relationship information, and other metadata as a point in time snapshot view of a supported resource. All the information that AWS Config can record for resource, is captured within the CI. A CI is created every time a supported resource has a change made to its configuration in any way.</p>
<p>In addition to recording the details of the affected resource, AWS Config will also record CIs for any directly related resources to ensure the change did not affect those resources too. For example, if there was a rule change to a security group, additional rules would be added with new parts. AWS Config would record all CI information for that resource, but it would also give a CI information for any instances that were a part of that security group. The CIs would then be sent to a configuration stream, which we will cover next.</p>
<p>Because it captures so much data, these CIs are used by other features and components of AWS Config, such as configuration history. CIs are used to look up all the changes that have been made to a resource. </p>
<p>Configuration Streams. CIs are sent to an SNS topic to enable analysis of the data. Configuration snapshots. CIs are used to create a point in time snapshot of all supported resources. Following CIs, let’s move on to configuration streams. When a create, update or delete API call is made against a supported AWS Config resource, as we now know a new CI is created along with additional CIs for any resources that are related to the original modified resource.</p>
<p>These CIs are then sent to a configuration stream, and this stream is in the form of an SNS topic. However, this stream is also used by AWS Config to send information when other events occur such as when the configuration history for a resource was delivered to your account, when a configuration snapshot was started and delivered to your account, when the state of your resource compliance changes against any config rules that have been configured, when evaluation begins for rules against resources, and when AWS Config found to deliver notifications to your account.</p>
<p>The configuration history uses configuration items to collate and produce a history of changes to a particular resource. This allows you to see the complete set of changes made to resource over a set period of time. The information can be accessed either programmatically through the AWS CLI using the following command. You can also specify the resource type. So for example, if you wanted to look at the configuration history for a Subnet, you could enter the following into the AWS CLI. Or you could access the history via the AWS Management Console.</p>
<p>Additionally, AWS Config also sends a configuration history file for each resource type to an S3 bucket that is selected during the setup of AWS Config. This configuration file is typically delivered every six hours. Unlike our site, it contains all CI changes for all resources of a particular type. For example, there would be one configuration history file covering six hours for all RDS DB instance changes in one Region.</p>
<p>The configuration snapshot also uses configuration items in its production. The configuration snapshot will take a point in time snapshot on all supported resources configured for that Region. It will generate CIs for each resource in your AWS account for a specific Region, and this configuration snapshot can then be sent to an S3 bucket. Alternatively, this information can be viewed by the AWS Management Console.</p>
<p>Now let’s take a look at the configuration recorder. So this component of AWS Config can be seen as the engine of the service. This element is responsible for recording all of the changes to the supported resources within your account, and generating the configuration items. By default, the configuration recorder is automatically enabled and started when you first configure AWS Config. However, it is something that you can stop and then will start again at a later point. When you stop it, AWS Config will no longer track and record changes to your supported of resources.</p>
<p>When you first configure AWS Config in the AWS Management Console, you are asked which resource types you would like to record. This is essentially setting up and creating the configuration recorder, as this information is required to enable the configuration recorder to start. If you only select to record specific resource types, then it will capture all creates, changes and deletes for those resource types, and will create a CI record for each occurrence. However, the configuration recorder will still record all creates and deletes of supported resource types within that Region, but no changes to those resources will be recorded.</p>
<p>Also, if you stop and change your configuration recorder settings to, perhaps, remove certain resource types from being recorded, then all captured information for those resources up to that point will remain, and you will be able to view the history with all data from the previous CIs that have been captured.</p>
<p>AWS Config rules are a great way to help you enforce specific compliance checks and controls across your resources and allows you to adopt an ideal deployment specification for each of your resource types. Each rule is essentially a Lambda function, that when called upon, evaluates the resource and carries out some simple logic to determine the compliance result with the rule.</p>
<p>Each time a change is made to one of your supportive resources, AWS Config will check the compliance against any config rules that you have in place. If there was a violation against these rules, then AWS Config will send a message to the configuration stream by SNS, and the resource will be marked as noncompliant.</p>
<p>It’s important to note that this does not mean the resource will be taken out of service or it will stop working, it will continue to operate exactly as it is with its new configuration. AWS Config simply alerts you that there’s a violation and it’s up to you to take the appropriate action. These rules can be custom defined or selected from a predefined list of AWS managed rules that AWS has created on your behalf.</p>
<p>Being able to create your own rules, allows you to adopt best practices that you may have internally within your own enterprise or with other security best practices. By allowing AWS Config to monitor resources at this level, it adds another level of automation, helping to prevent misconfigurations made by human error being left, which could lead to a security risk or even worse, a breach. I highly recommend using config rules for maintaining security checks and configurations.</p>
<p>AWS have a number of predefined rules that fall under the security umbrella that are ready to use. For example, Rds-storage-encrypted, this checks whether storage encryption is activated by your RDS database instances. Encrypted-volumes, this checks to see if any EBS volumes that are in attached state, are encrypted. Root-account-mfa-enabled, this checks whether your root account of your AWS account requires multi-factor authentication for console sign-in. IAM-user-no-policy-check, this checks that none of your IAM users have policies attached.</p>
<p>Best practice dictates that permission should be provided by roles or groups. The great thing about these predefined rules is that you can also edit them to make subtle parameter changes as needed. As you can see, being able to ask either AWS Config to check your resources compliance with rules such as these are invaluable. And when you couple this with being able to create your own custom rules, the scope and potential of automating compliance is huge across your supported resources.</p>
<p>As a part of the CI for a particular resource, for example, in EBS volume, AWS Config identifies relationships with other resources from that resource. In this case, it might be the EC2 instance that the volume is attached to. This allows AWS Config to build a logical mapping of resources and how they connect. This mapping of relationships allows you to quickly jump to other linked resources within the AWS Management Console to view their configuration history and CI data. This is very useful if you’re trying to troubleshoot an issue and pinpoint where the source of an incident may be. For a full listing of available relationship types, see the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-relationships">link</a> on screen.</p>
<p>As we have already seen, an SNS topic is used as a configuration stream for notifications of various events triggered by AWS Config. You can have various end points associated to the SNS stream. Best practice in the case that you should use SQS and then programmatically analyze the results via SQS. The S3 bucket that was selected at the time of configuration, is used to store all that configuration history files that are generated for each resource type, which happens every six hours.</p>
<p>Also, any configuration snapshots that are taken are also stored within the same S3 bucket. The configuration details used for both SNS and S3 are classed as the AWS Config delivery channel, by which data can be sent to other services. When setting up AWS Config, you’re required to select an IAM role. This role is required to allow AWS Config to obtain encrypt permissions to carry out and perform a number of functions.</p>
<p>For example, AWS Config will need read only access to all the supported resources within your account so it can retrieve data for the configuration items. Also, we now know that AWS Config uses SNS and S3, both the streams and storage of the configuration history files and snapshots. So AWS Config requires the relevant permission to allow it to send data to these services.</p>
<p>That now covers the key elements of AWS Config. And so I hope it makes it clear as to how each part plays a role within the service. To reiterate, let’s take a bulleted point look at how it all comes together.</p>
<p>When you first turn on AWS Config, you will configure the elements required for the configuration recorder to begin capturing and recording data. AWS Config will then discover all of your supported resources based upon the details entered within the configuration recorder within that Region. When a create, change or delete against a resource is made, a CI will be created for this change, and a notification will be sent the configuration stream regarding the new CI. Also, following the CI, AWS Config will check its current config rules to evaluate if the changes made the resource noncompliant. If the evaluate state change for the resource, a notification will be sent to the configuration stream. Your config rules can also be configured to run periodically, and so that will run at a set given time period regardless if there have been any changes to resources. If a configuration snapshot is taken, AWS Config will create a point in time snapshot of the resources with new CIs and deliver this configuration to your specified S3 bucket within the configuration recorder. After six hours has passed from turning on AWS Config, a configuration history file will be created for each resource type, and again, sent to your specified S3 bucket. The configuration file will be sent every six hours from that point.</p>
<h1 id="AWS-Organizations"><a href="#AWS-Organizations" class="headerlink" title="AWS Organizations"></a>AWS Organizations</h1><p>Hello and welcome to this lecture where I will start this course by providing an overview of AWS Organizations as a foundation before focusing on the service control policies.</p>
<p>As businesses expand their footprint on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and utilize more services to build and deploy their applications, it will soon become apparent that the need for multiple AWS accounts is required to manage the environment and infrastructure effectively.</p>
<p>This multi-account strategy is beneficial for a number of reasons as your organization scales, and for example, multi-account strategies include cost optimization and billing, security and governance, management and control of workloads, resource grouping, and helping to define business units.</p>
<p>As you begin to expand with multiple accounts, it will become increasingly more difficult to manage them as separate entities. The more accounts you have, the more distributed your environment becomes and the associated security risks and exposures increase and multiply.</p>
<p>However, with AWS Organizations, it can provide a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization, which helps to maintain your AWS environment from a security, compliance, and account management perspective.</p>
<p>To understand how AWS Organizations operates, we first need to be aware of the hierarchy of the service’s components.</p>
<p>AWS Organizations uses the following components to help you manage your accounts: Organizations, Root, Organizational Units, Accounts, Service Control Policies.</p>
<p>An Organization is an element that serves to form a hierarchical structure of multiple AWS accounts. You could think of an organization as a family tree which provides a graphical view of your entire AWS account structure. At the very top of this Organization, there will be a Root container.</p>
<p>The Root object is simply a container that resides at the top of your Organization. All of your AWS accounts and Organizational units will then sit underneath this Root. Within any Organization, there will only be one single Root object.</p>
<p>Organizational Units (OUs) provide a means of categorizing your AWS Accounts. Again, like the Root, these are simply containers that allow you to group together specific AWS accounts. An organizational unit (or OU) can connect directly below the Root or even below another OU (which can be nested up to 5 times). This allows you to create a hierarchical structure as I mentioned previously.</p>
<p>Accounts. These are your AWS accounts that you use and create to be able to configure and provision AWS resources. Each of your AWS accounts has a 12 digit account number.</p>
<p>Service control policies, or SCPs, allow you to control what services and features are accessible from within an AWS account. These SCPs can either be associated with the Root, Organizational Units, or individual accounts. When an SCP is applied to any of these objects, its associated controls are fed down to all child objects. Think of it as a permission boundary that sets the maximum permission level for the objects that it is applied to.</p>
<p>Now we have an understanding of what AWS Organizations is exactly, what benefits can this bring to your AWS environment?</p>
<p>The primary benefit that this service brings is its ability to centrally manage multiple Accounts from a single AWS account, known as the master account. You can start by inviting your existing accounts to an Organization and then create new accounts directly from the Master Account.</p>
<p>Greater control of your AWS environment. Through the use of Service Control Policies attached to the Root, Organizational Units or individual accounts, administrators of the master account gain powerful control over which services and features—even down to specific API calls—that an IAM user within those accounts can use, regardless of the user’s identity-based or resource-based permissions. Consolidated Billing. The master account of your AWS Organization can be used to consolidate the billing and costs from all member AWS accounts. This allows for greater overall cost management across your individual AWS accounts. Categorization and grouping of accounts. By leveraging Organizational Units, you can segregate and group-specific AWS accounts together, applying different SCPs associated to each OU. For example, you may have a number of AWS accounts that must not have the ability to access any AWS Analytical services. In this case, you could place these accounts into a single OU and assign an SCP that denies this functionality.</p>
<h1 id="Implementing-AWS-Organizations"><a href="#Implementing-AWS-Organizations" class="headerlink" title="Implementing AWS Organizations"></a>Implementing AWS Organizations</h1><p>Hello and welcome to this lecture which will explain how to initially set up and configure AWS organizations. Setting up an organization is a very simple process that starts from a master AWS account. Your master account is a standard <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account that you have chosen to create the AWS organization. It’s best practice to use this AWS account solely as a master account, and not to use it to provision any other resources such as EC2 instances, et cetera. This allows you to restrict access to the master account at a greater level. The few users who need access to it, the better, and you need to do this because the master account carries certain administrative level capabilities such as being able to create additional AWS accounts within your organization, invite other accounts to join your organization, remove AWS accounts from your organization, and apply security features via policies to different levels within your organization.</p>
<p>Once you have selected your AWS account to be used as a master account, you can create an organization. From here, you have two choices when creating an organization type: enable all features or enable only consolidated billing. If you want to set up service control policies, then you need to select enable all features.</p>
<p>The second option allows you to control payments and manage costs centrally from that master account across all associated AWS accounts within the organization. When the organization is created, the master account can create organizational units for AWS account management as required. The master account can also invite other member AWS accounts to join the organization. During this invitational process, the account owner of these invited AWS accounts will receive an email requesting that their AWS account join the organization. Once the accounts have joined the organization, the master account can then move these accounts into the corresponding OUs that have been created and associate relevant service control policies with them.</p>
<p>Let me now show you via demonstration on how to create a new organization and invite an existing account to join it. Now I’m logged into my AWS management console in the AWS account that I want to be the master account, and the first thing I need to do is go to AWS organizations, which is under the management and governance category, and you can see, it’s just at the top here.</p>
<p>So if I go into organizations, and at the moment, I don’t have any organizations set up or created. So the first thing I need to do is click on create organization, and this gives you a quick, high-level screenshot just to explain what creating an organization does. So it provides single payer and centralized cost tracking, it lets you create and invite accounts, it allows you to apply policy-based controls, and it helps you simplify organization-wide management of AWS services.</p>
<p>Now, as I mentioned previously, there’s two options when you create your organization. You can only create it with all features enabled, which is what I just listed, or as you can see here, you can just create your organization to consolidate your billing features. With this demonstration, I’m going to create it with all features. So let’s go ahead and create our organization, and that’s effectively it. So it’s very easy to create your AWS organization to start with, and because this is a brand new organization, this is my master account, which is signified by this star here, and this is my account name, and my account ID.</p>
<p>So, to actually create the organization is very simple, but now I want to add another account as a member account, so let me go ahead and do that. So if I select add account, now I have two options here. I can invite an existing account or create a new account. Now I already have another AWS account, so I’m going to invite an existing account. Now I need to enter the email or account ID, so I’ll just paste in my account, and you can add any notes here, for example, please join my organization, and then you select invite.</p>
<p>Okay, now we can see that we have a request that’s been sent as an invitation. The status is currently open. So now the email address that was registered with this account will get an invitation and they must accept that invite into this organization. So let’s take a look and see if I got that email. So here we can see the email that’s been sent to the owner of that member account, and it says, Stuart would like to add your AWS account to their organization as a member account, and then it just gives some additional blurb about AWS organizations, but to accept the invitation, and to understand what features have been enabled, we need to click on this link here.</p>
<p>So if I select that link, and sign in to my account using my details and MFA code, then I can see that I have an invitation from AWS organizations. We can see the organization ID, the master account name, and the requested controls, which is enable all features. So here, I can either accept or decline and I’m going to accept. I just need to confirm the confirmation message about joining the organization.</p>
<p>Okay, now this member account is now a part of that organization. So if I go back to my master account now, I can see now that within my AWS organization of my master account, I have the CA demo account, which is the name of my other account, and we can see that it’s not a master because it hasn’t got the star whereas this account has the, this is the master account. So as you can see, it’s a very simple process to invite other accounts to your organization.</p>
<p>Now I also mentioned previously about organizing accounts and using organizational units. So if we select organize accounts, at the moment, we only have the root in here. So I can create the new organizational unit and assign each of these accounts into those. So, for example, let me create a new organizational unit called production.</p>
<p>Now I’m also going to create a second organizational unit called test. So let me create another one. At the moment, under root, we have our two accounts. So we have our master account and our member account here. Now I want to move my master account into the production organizational unit, just to make things a little more organized. So I can select the account, click on move, and then simply select where I want it to reside within the tree, and then click move, and we can see, it’s now been removed from the root location, and I want to do the same with the member account, but this time, I want to move that into the test OU. So now, if I click on production over here, this organizational unit, we can see the account that it has inside it, and again, if we go back to the root and click on test, we can see that we have the member account. So I just wanted to show you that quickly just to show you how you can easily and quickly organize your different AWS accounts.</p>
<p>Okay, and that’s the end of the demonstration.</p>
<h1 id="Securing-Your-Organizations-with-Service-Control-Policies"><a href="#Securing-Your-Organizations-with-Service-Control-Policies" class="headerlink" title="Securing Your Organizations with Service Control Policies"></a>Securing Your Organizations with Service Control Policies</h1><p>Hello and welcome to this lecture which will dive into Service Control Policies to understand how they can be used to secure your AWS Organization. SCPs are different from both identity-based and resource-based policies, which grant permissions to users, groups, and roles. However, SCPs do not actually grant permission themselves. Restrictions made within an SCP set a boundary of permissions for <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> accounts.</p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS, and EC2 via an identity-based policy. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2, despite having full access to S3. The SCP would serve to prevent that service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<p>So to be clear, an SCP does not grant access. They add a guardrail to define what is allowed. You will still need to configure your identity-based or resource-based policies to identities, granting permission to carry out actions within your accounts. If you want to use Service Control Policies to help you manage your security at an account level, then you need to ensure that you deploy AWS Organizations using the enable all features setting.</p>
<p>Before you start using SCPs, you first need to enable them from the root account of your organization. However, you need to ensure that you have the following permissions. From within your AWS Organizations console, navigate to the Policies tab and select the Service Control Policies option that will show the status of Disabled. Then, select the option to enable the SCPs as shown. At this point, SCPs will now be enabled at the root level of your organization and you can now begin to use Service Control Policies within your organization.</p>
<p>I now want to perform a demonstration showing you how to create a Service Control Policy and attach it to an account. In this demonstration, I will have two accounts; my master account called Stuart and another account called CA-Demo. In the CA-Demo account, I have a user, Alice, who has an IAM policy attached that allows full access to S3. From within my master account, I will create a new Service Control Policy denying access to the Amazon S3 service. I will then attach this new SCP to the CA-Demo account. I will then log in as Alice and try to access Amazon S3 to see the result. Let’s take a look.</p>
<p>Okay, so to start this demonstration, I’m logged in as the user Alice in this CA-Demo account which is my member account to my organization. And as I said previously, Alice only has access to Amazon S3. So if we take a look at S3, we should be able to get into the service without any issues. So we can get into the service which is great and also create a bucket as well, so a bucket for Alice. And there we have it. So this user Alice is able to access S3 and also create buckets as well.</p>
<p>Now what I want to do is swap back over to my master account of my AWS Organization, create a Service Control Policy to block access to Amazon S3 and then apply it to this account. So let’s do that next. Okay, so I’m now back in my master account and what I want to do is to create a new Service Control Policy. So from the AWS Organizations dashboard, if I click on Policies, now I already have my Service Control Policies enabled, if yours says disabled, simply select it and then you’ll have the option to enable it.</p>
<p>Once your Service Control Policies are enabled, if you select them, now we can create a policy. We have a default policy here which is FullAWSAccess which allows access to every operation and this is the default Service Control Policy that’s applied to the root of my AWS Organization. However, what I want to do is deny access to one of those services. So if I select create policy, I’ll just call this Deny S3, give it a description of deny access to S3, and then down here is where we can build our policy. So there’s different options.</p>
<p>Firstly, the statement and then the resource and any conditions. So to start with, we can see that over here our policy is in a deny state. Now if we wanted to allow access to a service, we’ll change that to allow. But I’m gonna leave that as deny and over here I wanna select my service that I want to deny so let me scroll down to S3, if I can just find it in the list, there we go, and I want to prevent all actions of S3. Now as we can see, it’s now updated this policy with the action of all actions to S3, but we don’t have the resource yet. So let’s now scroll down to add resource. So our service is S3 and our resource type would be all resources. So if I select add resource, we can now see that the policy has been updated again. So at the moment, it’s denying any action in S3 for all resources.</p>
<p>Now if I wanted to add a condition, then I could add any conditions in here with the condition key and qualifier, et cetera. For this demonstration, I’m not going to add any conditions.</p>
<p>Now I just need to create the policy. Okay, so that’s our policy created. Now although the policy has been created, it’s not actually attached to any organization unit or account thus yet. So let me go back to my accounts. Now if you remember from a previous demonstration, we had our member account, the CA-Demo account, within the Test OU. So I’m going to select that account, go across the Service Control Policies, and then we can see that we have this policy here that we just created and I want to attach that to this specific account. So if I select attach, we can now see that this account has two Service Control Policies, the FullAWSAccess which is filtered down from the root which allows access to all S3 resources, but we also have a Service Control Policy here that denies access to one of those services and the deny will always overrule an allow.</p>
<p>So now we’ve attached that Service Control Policy to the CA-Demo account which is the account that Alice is a part of. Let me now log back in as Alice into this account to see if I can now access S3 or if the Service Control Policy has been applied. So let’s take a look.</p>
<p>Okay, so I’m back in my CA-Demo account which is the member account. I’m logged in as Alice. So now if I go to S3, let’s see what happens. We have an error of access denied and we can’t see any buckets. So it looks as though that Service Control Policy has taken effect because I can’t access S3 at all. So let me see if I can create a bucket, if I just add in any name, and try to create, again we get an error of access denied. So we can see that the Service Control Policy has in fact now blocked access for Alice despite her having IAM permissions allowing her full access to S3. You would have noticed that in my list of SCPs from the demonstration I just carried out there was a pre-existing SCP there called FullAWSAccess and this was associated with the root object of my Organization, so how does the inheritance of SCPs work? Well, let’s take a look at an example.</p>
<p>Let’s suppose we had the following AWS Organization layout, with the FullAWSAccess SCP at the root. The objects within an Organization follow a parent-child relationship, with the Root being the parent to all other child objects. As you can see at the root level, we have an SCP that allows full access to AWS. I now want to establish what SCPs each of the five AWS accounts, highlighted in orange, is governed by. So looking at Account 1, looking from the root, we have the FullAWSAccess SCP. The next level down to Account 1 is the Dev OU. Now, this has four SCPs associated. However, these are number 2, 3, 5 and 6. So at this stage, only the services and features within these SCPs are allowed at this level.</p>
<p>Now if we go down further to Account #1, we have another OU, the Test OU, which again is governed by a list of SCPs, 3, 5 and 6. We can see at this level of the tree, SCP 2 has been dropped so this OU is now restricted to SCP 3, 5 and 6. Therefore, Account 1 which is a child of the Test OU is restricted and governed by the details set out in these three SCPs. Using this methodology, we can now look at the other accounts. So Account #2 is governed by SCP 2, 3, 5 and 6. Account #3 is governed by 1 and 6. And Accounts 4 and 5 are governed by SCP 4.</p>
<p>Let me now look at a different example, this time I’m going to remove the default SCP of FullAWSAccess at the root and replace it with custom SCPs as shown here. Now I’m going to assume in this scenario that every SCP shown has different service restrictions. So as you can see, at the root level this time we have four SCPs, number 1 through to 4, so let’s see how this affects the results this time for each account. So Account #1, looking from the root, we have SCPs 1 through to 4, the next level down to Account #1 is the Dev OU. Now, this also has four SCPs associated. However, these are number 2, 3, 5 and 6. As a result, this Dev OU is only controlled by SCPs 2 and 3. SCPs 5 and 6 are discarded as they are not a part of the parent relationship with the Root object, and 5 and 6 do not exist at any parent level.</p>
<p>Now if we go down further to Account 1, we have another OU, the Test OU, which again is governed by a list of SCPs, 3, 5 and 6. We already know that 5 and 6 are not allowed as they are not in the parent chain. However, SCP number 3 is. So SCP 3 exists from the root downwards in this OU. Therefore, Account #1 is restricted and governed by the details set out in SCP 3. Using this methodology, we can now look at the other accounts. So Account #2 is governed by SCP 2 and 3. Account #3 is governed by SCP 1. And Account #4 and #5 are governed by SCP 4.</p>
<p>Finally, before I end this lecture, please be aware of some of the characteristics of Service Control Policies. SCPs do not affect resource-based policies. They only affect principals managed by your accounts in your organization. SCPs affect all users and roles, in addition to the root user. However, the root user will still be able to change its own password including MFA settings, manage root access keys, and manage x.509 keys for the root user. If you disable SCPs in your organization, all SCPs are deleted and removed. Re-enabling SCPs again in the same organization will revert to the default SCP allowing FullAWSAccess. The following elements are not affected by SCPs: any actions performed by the master account, SCPs do not affect service-linked roles, and managing Amazon CloudFront keys.</p>
<p>That now brings me to the end of this lecture and to the end of this course. You should now have a greater understanding of how Service Control Policies can be used within your AWS Organization to help you centrally control the different levels of access between multiple accounts. If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="AWS-Control-Tower"><a href="#AWS-Control-Tower" class="headerlink" title="AWS Control Tower"></a>AWS Control Tower</h1><p>There comes a moment in every administrator’s life, when they see the ever-increasing burden of their users piling upon them, more and more accounts just keep joining the company, in ever quickening pace, how am I supposed to keep up with all this? The administrator screams to this guy as they create yet another developer account, trying desperately to wire up all the permissions as fast as possible.</p>
<p>The admin knows that these new database issues won’t fix themselves, so they’ll need to hurry, or how about the poor security auditor who is constantly having to look through production accounts to make sure that we’re following best practices, do they already have an audit log in created for them or will they have to bother the overworked administrator from before? Adding yet another task to the ever-growing pile, and even then is the other supposed to check every production account by hand to see if their EBS volumes are actually encrypted? Why isn’t there an automated way to check for all these things? The poor security person cries into their keyboard for the fourth time today.</p>
<p>And finally, when we have to keep track of all the log files from every security service that touches customer data, where does that all get stored? Are they on separate accounts for each of these production workloads? Do you aggregate everything into a locked vault somewhere? God save us from the crushing burden of all these text files.</p>
<p>Anyways, I think you get the point, there’s an enormous amount of work to consider when you’re administrating, securing or just simply creating an organization within AWS, there have also been a few ways over the years that AWS has recommended doing it, starting originally with just using IAM and hosting new users within your root accounts, creating policies, users and groups, and roles to divvy up permission, we were then given AWS organizations an extremely helpful service that allows us to administer large volumes of these accounts, organizations gave us ability to add accounts into groups and apply policies between them, these service control policies help to control and protect data and our users from portions of AWS, we didn’t want them to have access to, they had the power to override local IAM permissions, regardless of what that particular account actually allowed internally with IAM.</p>
<p>Well, we have now moved on to the next stage of this progress, AWS has evolved something new and I’d like to grab a little bit of your time today, to discuss probably the simplest and most powerful way to date to create, govern and administer large numbers of user accounts within AWS, and that would be with AWS Control Tower. What is AWS Control Tower? AWS Control Tower is a service that offers a larger and more controlled method of creating, distributing, managing, and auditing multiple accounts.</p>
<p>Originally these tasks were relegated to multiple different services that each dealt with an individual piece of the puzzle, some tasks like auditing didn’t even have a specific service to really help drive them forward, creating a large multi-account system from the ground up that deals with these scenarios might take an individual weeks to properly create, with AWS Control Tower we can manage all of these disparate services right within one area.</p>
<p>One of the first main topics we need to discuss for AWS Control Tower is the concept of landing zone What is a landing Zone? A landing zone is a multi-account architecture that follows the well-architected framework and is based around the ideas of security and compliance best practices. Your landing zone will be automatically created by AWS Control Tower and it is inherent part of the service, your landing zone is created from a series of best practice blueprints that help us setting up systems that deal with identity, federated access and overall account structure, these blueprints do the following on your behalf, they create a multi-cloud environment using AWS organizations, there are three organizational units that are provisioned here, the Root OU, this will be the parent OU that contains every other OU, within your landing zone, the core OU this OU contains the log archive and any audit member accounts, we generally refer to these accounts as the shared accounts and a custom OU, this OU another member OUs contain the actual working accounts that your users need to perform whatever duties they do within your AWS environment, for example your developer AWS accounts would sit within this OU.</p>
<p>The service also builds out two shared accounts, a log archive in the account which will be the place where all the logs will be sent between all accounts, it will store the logs of all API calls and resource configurations for every account within the landing zone and an audit account which has a restricted account that has been created to give your security and compliance team members read and write access to any account within your AWS landing zone.</p>
<p>From this account you’ll have programmatic access to review all other accounts, by way of a role that grants use of Lambda functions only, this account does not allow you to log into other accounts manually, which is good. Control tower will also provide identity management with the use of AWS single sign-on, default directory, this directory will house all of the AWS SSO users, you can also use it to define the scope or permissions available for each of those users. It will also provide federated access to those accounts, using AWS SSO directly, control tower then hooks up centralized logging from AWS cloud trail, and into its config, which is stored securely within AWS three and the logging account.</p>
<p>Finally it enables cross account security auditing using AWS IAM an AWS SSO to allow the audit account to perform routine checks as it wishes. In the end AWS Control Tower will have all of that automatically set up for you that is incredible, the amount of time it might take you to create all that by hand would probably be measured in weeks if done by a single person. The service can even create pre-configured groups such as your admins, users, and auditors, you can of course create more designations that provide different levels of access based on your needs.</p>
<p>Additionally, if you have your own active directory service already going, you can plug that into AWS SSO and configure it to work directly with your system which again can be a cloud-based AD or when you’re hosting on-premises, just make sure to use AD connector or the AD service. Safely managing your landings on resources, when you create your new landing zones using AWS Control Tower, there are large number of AWS resources that are created on your behalf, you need to be very careful about deleting or removing these pre-configured resources, if you were to destroy or tamper with these resources you can send control tower into an unknown state, AKA, it can break, so in very simple terms, do not modify or delete any of the AWS identity and access management roles that were created within the shared accounts, the auditing account and the archive account, otherwise bad things can happen.</p>
<p>Now, if you do so accidentally, it’s not the of the world later on, we’ll discuss how to fix these problems, additionally, it is very important that you do not disallow usage of any regions through service control policies or with AWS simple token service. Administration tips for setting up your landing zones. You should be setting up your landing zones in the region where you do most of your work, this region is known as your home region with that in mind make sure you deploy your new accounts from within that home region, if your architecture is multi-region again, keep it simple, put your landing zone and whichever region is the primary region, do not move the audit bucket or other buckets that were created when you launched control tower from your home region, this can cause issues down the line, and when you first launch your landing zones, AWS STS end points need to be activated in the management account in all regions supported AWS Control Tower if not, it might fail midway through the creation process.</p>
<p>Guardrails, even if AWS Control Tower only deployed the landing zones for us with all of its associated accounts and features, I would be happy, however, that is not all that AWS Control Tower helps deploy on your behalf, In addition to everything we’ve just covered, Control tower also deploys guardrails, guardrails is an appropriately named service that helps to keep all of your users accounts and everything under AWS is control tower and compliance with basic security regulations, overall, a guardrail is a fairly high-level security rule that provides continual oversight, they implement preventative or detective based policies that can help you govern your accounts, users, groups, and resources, the best part about these guardrails is that they’re written and meant to be understood in plain English.</p>
<p>Unlike other types of control policies be it service control policies, or IAM user policies, guardrails are very straightforward to understand, here are three simple examples, enable encryption at rest for log archive, enable access logging for log archive, disallow changes to CloudWatch logs, log group, just by reading these titles of the guardrails you already have a great understanding of what they’re supposed to do.</p>
<p>Now, of course, these are all built on top of existing AWS infrastructure and do have an associate SEP lying underneath them, but all of that is managed by AWS and they will keep that site up-to-date for you, here’s an example of what these look like in their normal form. Guardrails are applied to an entire organizational unit just like normal SCPS and therefore are also added to each account underneath that organizational unit, so that means that any accounts you add within your landing zone will be effective by the guardrails that you have in place, in general, guardrail’s there to help express your policy intentions.</p>
<p>Three types of guardrails, when using AWS was control tower we have three types of guardrails that we need to know about, first we have the mandatory set of guardrails, these are automatically added to your landing zone accounts upon creation and can not be disabled, in total there are 22 mandatory guardrails and you can check here for a full list of them and what they do. In general, though the mandatory guardrails are there to deal with basic security concerns, dealing with these shared accounts, to summarize them quickly, they help with enforcing logging, allowing the right services and all regions to facilitate logging, encrypting logs, stopping people from turning off your logs and stopping people from changing anything that AWS Control Tower set up.</p>
<p>Overall the mandatory guard rails don’t do anything super controlling and are defensive in nature. Second, we have a series of strongly recommended guardrails, they are designed to enforce some of the most common best practices for well-architected and multi-account environments, these guard rails are not mandatory and can be enabled and disabled as you please, these optional guardrails are not enabled initially, allowing you to choose what level of security and scrutiny you want for your environment.</p>
<p>Here are a few examples of the strongly recommended guardrails, disallowing creation of access keys for the root user, disallowing public access to Amazon RDS database instances, disallowing public right access to Amazon S3 buckets, there are a total of 13 strongly recommended guardrails in general they try to lock down, read, write, and control access to accounts and services that are commonly compromised.</p>
<p>Feel free to look here for more information about them. Finally, we have the elective guardrails, they continue to help lock down your accounts and can help keep track of the most commonly restricted actions with an enterprise environments, these guardrails are also not enabled by default, there are only five elective guardrails at this moment, so I’ll just name them quickly, disallow cross-region replication for Amazon S3 buckets, disallow delete actions on Amazon S3 buckets without MFA, disallow access to IAM users without MFA, disallow console access to IAM users without MFA and disallow Amazon S3 buckets that are not version enabled.</p>
<p>As you can see, they’re fairly common and highly recommended additions to the security of any architecture, just to finish up about guardrails, it’s important to reiterate that when you turn on these guardrails, they can and will create resources within your AWS account do not modify or delete these resources, otherwise it can set false flags for your guard rails. Provisioning counts for your users, we have spent a lot of time so far looking how control tower helps you set up a robust, secure and easy to audit environment through the use of shared accounts and guardrails, this next topic I wanna cover will explain how to actually add users accounts into the system in the safest and most scalable way possible.</p>
<p>AWS Control Tower has three methods for creating new member accounts, you can add new accounts directly through the enrolled accounts feature which lives directly inside AWS Control Tower, you can create new accounts through the account factory which is included as a part of AWS service catalog, finally, you can create new accounts from your management account using Lambda code and the appropriate IAM roles, I’m not a real fan of this option as it’s not streamlined to my liking, but it does exist and allows you to create an ad accounts programmatically.</p>
<p>Let’s take a moment to look at each of these scenarios to see what is gained or lost in each of these circumstances. Enrolling new accounts, the account enrollment feature of AWS Control Tower provides probably the most straightforward way for your administrators to add new accounts into an organization, these new accounts will be governed by control tower and any guardrails you may have set up.</p>
<p>One caveat for enrolling new accounts is that your landing zone must not be in a state of drift. Drift is where some of the assets within your landing zone have fallen out of compliance and must be updated, this can happen accidentally over time or might have occurred on purpose, to fix a time sensitive issue either way, once you have brought everything back into alignment you can create an account using this method.</p>
<p>To actually create an account, it’s fairly simple, navigate to the account factory page of AWS Control Tower, select enroll account, fill in the appropriate information under the create account section, this will include fields like account email, account name SSO username, and organizational unit, and finally click enroll account, the actual account creation might take up to several minutes to complete, if you do not have this option available make sure you are signed into an IAM user that has access to the AWS service catalog end user full access policy, if you’re still having issues make sure you’re not signed in as root.</p>
<p>Account factory, account factory allows your cloud administrators and your SSO end users to create new accounts within your control tower landing zone AWS refers to this method of account creation as the advanced account provisioning method, they recommend that you use the enrollment feature from earlier.</p>
<p>Account factory creates new accounts so the use of another service called ADSL service catalog, AWS service catalog is an organizational tool developed with the purpose of making provisioning and creation of IT tacks easier for both the end user as well as your IT admins, these stacks can include almost everything under the AWS sun such as easy to instances, databases, software and all the underlying infrastructure to create multi-tiered applications and architectures, service catalog allows your end users to select the content that they need from a list of pre-approved services that your IT or admin teams set up ahead of time, this helps to bring down those barriers of entry for content creation, as well as helping to keep best practices and system security a key component of any development.</p>
<p>Background information aside, to actually go about creating a new account within account factory, we have a few steps to follow, sign into your SSO user portal and from applications select AWS account and from your selection of accounts choose the management account, an AWS service catalog and user access, click on management console, make sure you’re in the correct region, this should be your AWS Control Tower main region search for and choose service catalog, click on the products list, select AWS Control Tower account factory, and then the launch button, now fill out the appropriate user information here, choose next until you get to the review section, now this is very important, do not define tag options and do not enable notifications, this will cause the account provisioning to fail. Review and launch, do not create a resource plan, this will also cause the account provisioning to fail, after that you’re all done.</p>
<p>This method will also take several minutes to complete. Using Lambda, is also possible to do all the above in an automated way, using Lambda, you can have your function create and provision account for you, you must do this for the management account that runs your control tower however, some notes, if you are automating control tower functions like account creation, your identity that is performing the work must have the AWS policy, AWS service catalog, and user full access, as well as the following permission.</p>
<p>This direction is fairly complex and might require a bit of back and fiddling to get working for your specific requirements that aside however, if you do really want to know how to get automatic account creation working just by uploading a CSV file then into an S3 bucket, please check out this blog link by AWS.</p>
<p>Unmanaging a member account, there might come a time after you’ve created an AWS account that need to release it from direct purview of AWS Control Tower, although this might be an infrequent issue, it’s fairly easy to accomplish, releasing an account or unmanaging it, as it officially called, can be done through AWS service catalog, simply log into a SSO user, that is part of the AWS account factory group, from there you can open service catalog, select provision products list, choose the name of the account that you wish to release, choose terminate, this doesn’t mean destroy, it just means removed from the organizational unit and your landing zone, thanks Amazon for making that clear and just give it a few minutes and the account will be unregistered, just to be clear however, this does not remove the users access to the account, it just unregistered it from control towers oversight, if you wish to clear SSO access to that account, you’ll need to change the settings and AWS SSO by changing the user email for that account.</p>
<p>Resolving and detecting drift within AWS Control Tower. When you create your initial setup within AWS Control Tower your landing zones, accounts, and all other organizations will be in compliance with your rules and guardrails that have been set up, as you work with and keep adding new accounts, identities and all the other good stuff to your AWS Control Tower system, you will, at some point experienced drift, some of the drift will be accidental, of course but other drifts might’ve been created on purpose to fix those time-sensitive issues.</p>
<p>AWS control tower is able to detect these abnormalities and lets you know how far from center your setup has officially drifted. Continually resolving drift is the best way to stay up-to-date and in compliance with both your internal corporate policies as well as with any governmental regulations you must comply with, you will receive your drift detection notifications automatically through Amazon SNS, these are aggregated within the audit account that was created in your landing zone.</p>
<p>Remember account admins should subscribe to these SNS drift notifications, even though the drift detection is automatic, you must resolve the problems manually through the console most of the time, luckily most of these drift issues can be repaired by literally pressing a button on the settings page through repair button to be specific, most drift can be taken care of at your leisure, but there are a few examples we call major drift that should be dealt with as soon as possible.</p>
<p>If you delete the organizational unit, originally named core you’ll not be able to perform any additional operations until you prepare it, if any of the IAM roles that check for drift are missing or inaccessible, you will see an error requiring you to repair the landing zone, these roles are the AWS Control Tower admin, AWS was control tower CloudTrail role and the AWS Control Tower stack set role. And finally, if you delete the OU originally named custom, your landing zone will be in a state of major drift but you will still be able to use control tower, only one non-core OU is required for control tower to operate, but you should still fix this.</p>
<p>Edge case guidance for organizations and AWS Control Tower. There’s practices that you should be aware of when working with AWS Control Tower, I have already spoken about a few of these as we’ve gone through the course, but I’ll restate the important ones for extra dramatic flare, do not update any service control policies through AWS organizations that are being managed by AWS Control Tower, if you do, it can throw off the detection systems within guardrails and leave it in an unknown state, this might require you to re-enable those guardrails in order to fix them.</p>
<p>AWS recommends that you instead create a new service control policy and attach those to the organizational units, instead of editing old ones, you should also be careful moving outside accounts into AWS Control Tower doing so will cause drift to occur that must be repaired, to fix this type of drift, simply update the account within the account factory, you can find a full breakdown of that process over here.</p>
<p>Nested OUs are not accessible from AWS Control Tower, the service only deals with top level OUs. And finally, if you rename an account or OU that was created by a control tower, you must repair your landing zone to see the new name displayed in control tower, and again, you just hit the press the button.</p>
<p>Comparisons, one final note I want to touch upon as we come to the end of this lecture is the comparison between ADFS control tower and AWS Security Hub, both of these services deal quite extensively with managing your environment and keeping your services secure, the main distinction that AWS has mentioned between the services is that AWS Control Tower is the primary destination for cloud administrators, helping you set up a secure and well-architected environment that sets your company off on the right path, AWS Security Hub is the primary destination for your security and compliance professionals, it allows you to see a comprehensive and timely analysis of your environments and gives you the option to take actions, based on these findings, these two services also sit on different sides of the security spectrum, AWS Control Tower has a more preventative approach, it tries to stop bad things before they happen through the use of guardrails and limiting your users while AWS security Hub exhibits a more detective-based approach providing reports and an analysis of your security posture allowing you to make decisions and root out possible vulnerabilities.</p>
<h1 id="AWS-License-Manager"><a href="#AWS-License-Manager" class="headerlink" title="AWS License Manager"></a>AWS License Manager</h1><p>Managing licenses in the cloud can become a headache for both asset managers and auditors. AWS License Manager has been designed to make the management and control of licenses with third party vendors such as Microsoft, SAP, Oracle and IBM when they are used both in the cloud and on-premises. It supports and tracks any software where the licensing agreement is set against virtual cores, VCPUs, physical cores, sockets, or a number of machines. AWS License Manager enables you to create license configurations which are made of multiple customizable rules that can be centered around the stipulations made by your different licensing agreements. These different rule types can include license counting type and this defines how your licenses are counted, by vCPU or physical core. Minimum and maximum allowed number of vCPUS or physical cores. This is dependent on the previous rule and the counting type but it essentially sets a threshold value for vCPUs or physical cores. License count, this specifies the number of licenses used within the license configuration. License count hard limit, when I hard limit is set it will block the launch of an instance that would make a breach of license amount. If you set a soft limit it will allow the launch of the instance however it will send a notification alert of the issue. Allowed tenancy, here you can specify which EC2 tenancy can consume a license with the configuration such as shared tenancy, which is the default, dedicated instance or dedicated host. </p>
<p>These rules are evaluated against your EC2 computer resources based on the software running on them to assess if your environment has reached its licensing limits on your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/highly-available-systems-sysops/domain-one-ec2/">EC2 instances</a>. AWS License Manager is currently supported and integrated with EC2 instances, dedicated instances, dedicated hosts, spot instances, spot fleets, and also auto-scaling groups. The customized rules help to minimize licensing breaches and depending on the configuration the EC2 instance can be prevented from being launched if there is a breach or it can send notifications to the appropriate team informing them of the limitations. AWS License Manager is integrated with AWS Systems Manager and AWS Organizations, allowing you to monitor your license requirements across multiple AWS accounts, plus on-premise environments. This allows you to monitor your licenses for your software vendors via a single account in the dashboard view. In addition to this, if you purchase resources from AWS Marketplace then you can also integrate Bring Your Own License to AWS License Manager as well. </p>
<p>So in essence, AWS License Manager provides a means of addressing, tracking, monitoring and managing licenses in a centralized location for on-premises and multi account AWS environments across multiple third party vendors using customized rules. </p>
<p>That brings me to the end of this lecture. Next I will be looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-compute-services-2019-reinvent-reminders/reinvent-c-amazon-elastic-inference/">Amazon Elastic Inference</a>.</p>
<h1 id="AWS-Service-Catalog"><a href="#AWS-Service-Catalog" class="headerlink" title="AWS Service Catalog"></a>AWS Service Catalog</h1><p>AWS service catalog is an organizational tool developed with the purpose of making provisioning and creation of IT stacks easier for both the end user as well as your IT admins. </p>
<p>These stacks can include almost everything under the AWS sun – such as EC2 instances, Databases, software, and all the underlying infrastructures to create multi-tiered applications and architectures. </p>
<p>Service Catalog allows your end users to select the content that they need from a list of preapproved services that your IT or Admin teams set up ahead of time. This helps to bring down those barriers of entry for content creation, as well as helping to keep best practices and system security a key component of any deployment. </p>
<p>Before service Catalog, your End User might have needed to ask their IT or Admin professionals to help them provision or determine what services were and were not within their scope of work. This could of course be achieved by limiting accessibility through IAM on a job role by job role basis. It is a bit tedious, however, and becomes a constantly changing battle, especially if you have many different roles within your company. </p>
<p>Now with AWS Service Catalog, we have the ability to browse through a list of ‘Products’ which are just a set of pre-approved services - and provision what we need from there. By setting up our system in this way, we can create and build with the full confidence that we as developers are creating solutions using only acceptable components that our security, administration, and leadership teams approve of. And on the Admin side, I’m not constantly being bugged with approval requests from every developer and department under the corporate sun. I think you can see the point, so let’s dive in a bit.</p>
<p>This is an important question because ‘products’ are the heart and soul of the AWS Service Catalog. Products are an IT service that you want to make available for deployment on AWS. </p>
<p>A product can consist of just a single AWS Resource or can be comprised of multiple items such as EC2 instances, Their associated EBS volumes, Database that you want them connected to, and all the monitoring capabilities you would come to expect from these services within the cloud.</p>
<p>A product can even be a package listed on the AWS Marketplace. For example, this could be helpful if you were using a database that AWS does not natively support but was available on the marketplace.</p>
<p>In the end, a product is a service. It can range from something as small as a single instance doing basic web hosting to an enormous multi-tiered web application.</p>
<p>Creating a product is actually very simple. AWS service catalog requires you to upload AWS Cloud Formation Templates, and from these templates, the service will add that entire stack into the catalog as a single product. Again that product can be something as small as a single EC2 instance, or a very large multi-tiered web application.</p>
<p>When uploading your products, you have the ability to add detailed product descriptions, any version information, and required support details and tags. These descriptions can be very helpful for your end users, allowing them to know who to contact if something goes wrong with the product.</p>
<p>Tags are very important for keeping track of cost. They can also be used to create metrics and analytics for your most commonly used product, Helping you to determine what your End Users are actually consuming. So, Make sure you appropriately tag your products during this phase. </p>
<p>You also have the ability to update any products you have already created. New versions of a product are added almost exactly how you built the initial product in the first place. Simply go to your product - click add new version - and upload your updated Cloud Formation Template to override the old product.</p>
<p>Once the new version is fully uploaded, and your product is published, your End User will have the option to update their running stacks to this version. You also have the option to deprecate old versions of a product. Your users will still have the ability to update but will not be able to provision more instances of that version.</p>
<p>Products can even be customized and have different deployments based on options you specify. Let’s say you wanted your product to use different ec2 instances based on the number of daily active users expected. When you set up your cloud formation template that describes your product, you can set predefined business-level input parameters. These parameters include questions you can ask your End Users when they select the product for deployment. Each question can have a predefined list of answers. Based on their answers to these questions you could completely change the architectural configuration of the product. That’s pretty Neat.</p>
<p>Now that we have a handle on what a product is, and how to create them, the next step within service catalog is to add these products into portfolios.</p>
<p>A portfolio is a collection of products with configuration information, that helps in determining who can use the products within. Each portfolio requires a name, description, and a product owner. That last one is very important because if something goes wrong with an available product it’s important to know who to send your complaints to. And just like with products, you can also tag each of your portfolios for all the normal good tagging reasons.</p>
<p>Portfolios can also be customized for each type of user within your organization, having individual controls and allowances per individual, group, or team. </p>
<p>When you are ready to publish your portfolios to your end users, you will need to add IAM users, groups, or roles to the portfolio from the console. This will allow them to actually see your finalized products within their Service Catalog.</p>
<p>You can also share portfolios between other AWS accounts, and give the administrators of those accounts the ability to add their own products to your portfolio. This could be useful when you have teams that operate independently, that each deal with creating their own products. You would want their product managers or admins in charge of when new versions are available and to whom they can be provisioned. All you need to do to allow sharing for your portfolio is specify the account id you want to share with (within the service catalog console) and provide the </p>
<p>One of the best features and really the whole goal of service catalog, is that you have full control over what your end users have access to. From an administrator perspective that is incredibly powerful and can help you maintain high levels of both security and credibility.</p>
<p>AWS Service Catalog allows you to apply constraints on the products within your portfolios. These constraints allow you to limit the scope and ability of your products based on pre-defined settings. They also allow you to have additional functionality, at least on the administrative side. When you apply a constraint, they become active as soon as you create them, however, they are not retroactive. </p>
<p>So, what can you do with a constraint? Well, there are a few types of constraints out there that we need to look at. First, we have the Launch constraint. This constraint specifies a specific AWS Identity and access management role that AWS service catalog will use when an End User decides to launch one of your products. By having a launch constraint, you are allowing the service to do the heavy lifting of creating the products instead of your user. This means that your users do not need to have all the permissions required to create your product, such as the ability to use cloud formation - which you probably didn’t want your new junior intern to have access to, for example. </p>
<p>With that in mind of course, the IAM role you apply to the product as a constraint needs to have the following permissions: AWS Cloud Formation, All the services used by that Cloud Formation Template, And read access to s3 bucket that houses the cloud formation template.</p>
<p>Notification Constraint. This constraint allows you to receive Amazon SNS notifications about stack events related to your product. These events are all created from deployment of the cloud formation template that specifies your product.</p>
<p>These are nice to have as an administrator because you can see what is being deployed and where. As well as being able to see if your products are deploying successfully or not. </p>
<p>Tag Update Constraint. This constraint specifies whether or not the user can update the tags on the given resources created by your product. This can allow for more specific tagging per group or user without you having to update the tags for them. However if you have good reasons to have the tags applied to the product or portfolio that are currently on there, maybe avoid letting the users have this ability.</p>
<p>Stack Set Constraints. This constraint gives you the option to configure where you want your products to launch. Specifically, you can control what regions and accounts you would like them available in. From there your End User can manage those accounts and determine where the product deploys and the order of those deployments.</p>
<p>Template Constraints. Template constraints allow you to limit what options that are available to your End User when they are provisioning your products. These options might include forcefully limiting the availability of certain ec2 instances based on the specific user group. You might need to do this if the parameter’s values in your cloud formation template are too broad for the target user group. In this situation, you can define a template constraint to narrow the band of available options.</p>
<p>This can be especially useful when you want your users to be able to use a certain product but to do so within the bounds of some compliance or regulation requirements set by your organization. These template constraints are bound to whatever portfolio they are in and do not cross over to other products in other portfolios.</p>
<p>You can also use template constraints to stop your users from inputting impossible, or obviously incorrect values. For example, you can create a rule that validates that a given subnet is in a particular VPC. This rule will be checked before creating or updating the stack, preventing CloudFormation from failing to create it.</p>
<p>Check out this example that makes sure the users are selecting an appropriate instance type for their workload.</p>
<p>Here we can see this constraint is setting the available instance type based on the environment the user is trying to set up. If they are trying to create a small test environment, the rules limit them to only using m1.small instances. However, if they are going to create a production environment, we don’t want the end users to be subjected to the slow speeds and underperformance of an m1.small, so let’s make sure they can only create m1.large instances.</p>
<p>After your End Users have created their applications using the products you have set up, they might have additional needs and requests. Some of these requests are repeatable and are general ‘day to day’ service level operations that need doing. For example, it would be quite common for someone to need a database backed up, but they don’t have the technical expertise or the trust level appropriate with being given access to the underlying RDS service.</p>
<p>This is where a Service Action can really shine. A service action is an End User available action - that you have explicitly given permission to use. Service actions allow you to give back some control to your end user, and helps to remove blockers from your operation. For this example, the service actions could be starting that RDS backup. </p>
<p>Service actions can enable your users to perform their own operations tasks: such as troubleshooting, running service level commands, or even request permissions to run additional products through service catalog. Service actions are also product-specific, so you can define exactly where you want them available. Behind the scenes, Service actions are based on AWS Systems Manager automation documents.</p>
<p>When working with service catalog, as well as with most aws services, pricing is of course a factor you should be aware of. There is a free tier for this service that lets you have up to 1000 API calls per month at no charge. When you start to exceed this number you will be charged at a rate of 1 cent per 14 API calls.</p>
<p>I think this is a pretty reasonable rate, as this is not a service that will be called all that often. Basically, you will only be paying for when people are thinking about provisioning new resources. Most of the time your users will be developing or actually using the product itself.</p>
<h1 id="Introduction-to-AWS-Systems-Manager"><a href="#Introduction-to-AWS-Systems-Manager" class="headerlink" title="Introduction to AWS Systems Manager"></a>Introduction to AWS Systems Manager</h1><p>Introduction to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/introduction/">AWS Systems Manager</a>. Welcome to this introduction to AWS Systems Manager to gain operational insights. In this segment, you will get an overview of AWS Systems Manager, including features and use cases. Systems Manager is a set of fully managed AWS services that enable automated configuration and ongoing management of systems at scale in a secure and reliable way across all your Linux and Windows instances running on Amazon EC2, your own data center or other cloud platforms. Its focus on automation enables configuration and management of systems where you can select the instances you want to manage and define the tasks you want to perform.</p>
<p>You can also define when modifications are to be applied by configuring a maintenance window. You can create and update system images, collect software inventory, apply system or application patches and configure Linux and Windows operating systems, also manage the state of your instances, all from the same console or the command line interface.</p>
<p>You don’t have to be concerned about setting up and managing different tools for different platforms. You also don’t need to configure secure shell keys, or secure shell or remote desktop ports or bastion hosts in order to establish connectivity to your instances. Systems Manager is built for cloud-type scalability, which uses agility and elasticity, allowing you to manage one or thousands of instances, no matter if they have long running or temporary workloads.</p>
<p>You also get AWS optimized native integration with the rest of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/systems-manager-is-a-good-fit-in-the-aws-tool-set/">AWS management tools</a>, such as Identity and Access Management for access control, CloudTrail for Auditing, CloudWatch events for event driven automation, and many other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> configuration and management tools.</p>
<p>There are no complex licensing models. Most of Systems Manager’s functionality is available at no charge. Systems Manager provides extensive building blocks and services where you can choose to build value of your own on top of these existing services.</p>
<p>In short, with Systems Manager, you can use automation to manage traditional and cloud workloads by performing essential setup, maintenance and management tasks, while maintaining complete visibility and control over your entire machine farm independent of operating system, location and number of instances.</p>
<h1 id="Systems-Manager-is-a-Good-Fit-in-the-AWS-Tool-Set"><a href="#Systems-Manager-is-a-Good-Fit-in-the-AWS-Tool-Set" class="headerlink" title="Systems Manager is a Good Fit in the AWS Tool Set"></a>Systems Manager is a Good Fit in the AWS Tool Set</h1><p>Systems Manager is a good fit in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Tool Set. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/introduction/">Systems Manager</a> fits well into your existing AWS configuration and management tool set. It provides you with the Systems Manager Explorer, which is a customizable operations dashboard that delivers a display of your operational data and work items for all your accounts and across regions. You can use Explorer to configure settings and preferences. You can customize the display using drag and drop and drill down capabilities to prioritize work items.</p>
<p>Finally, with Explorer, you can take action using automated runbooks if needed. Systems Manager also integrates with your Amazon CloudWatch dashboards and your Personal Health dashboard. You can now have a set of fully managed AWS Services that help you enable provisioning and operating of your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/systems-manager-requirements-and-building-blocks/">resources</a> at scale. If you’d like to know more about Amazon CloudWatch Dashboard, you could view our existing course at the link shown here on the screen.</p>
<h1 id="Managing-Resource-Groups"><a href="#Managing-Resource-Groups" class="headerlink" title="Managing Resource Groups"></a>Managing Resource Groups</h1><p>Systems Manager includes over 20 features and integrations, each with their own capabilities and functionality. Some of them are the Fleet Manager, Session Manager, Run Command, Parameter Store, Patch Manager, and State Manager, among others. Most of these features use Systems Manager documents to define the operations to be performed. They also use Maintenance Windows to define the date and time when those operations can take place. Together, they provide you a comprehensive dashboard and essential <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/systems-manager-is-a-good-fit-in-the-aws-tool-set/">tools</a> to set up and manage the life cycle of your instances. You can manage inventory and patch assets, run commands and manage desired state, and even securely connect to EC2 instances in private subnets.</p>
<p>In general, using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/introduction/">Systems Manager</a> entails grouping your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> resources, examining their relevant operational data via dashboards, and finally, take action to mitigate any issues reported. The instances to be operated can be selected using one of three general mechanisms. The first one is manually. This is where you select the instances that you want to register as targets individually, using the Systems Manager console. You can also use instance tags where you specify one or more tag key-value pairs to select the instances that share those tags. You can then save the results as a Resource Group to execute operations on the same set of instances in the future.</p>
<p>Finally, you can use Resource Groups where you can perform a query based on existing resource tags or choose an existing Resource Group that already includes the instances you want to target. Systems Manager operates on logical units of managed instances via Resource Groups. This is the most powerful way to define your targets for AWS Systems Manager to operate. In general, if you work across a range of different AWS resources that are related, AWS Resource Groups can help you organize them for better visibility and aggregation in terms of management, ownership and categories.</p>
<p>Resource Groups begin their life by defining common tags with key-value pairs describing the items in the categorization. A Resource Group is a collection of AWS resources in the same region that match a particular description. Resource Groups can be tag based, which represent a group of resources as being part of a development tier, production tier, a specific owner, a department, or dedicated for a particular project among many other possible categories. Systems Manager can also operate on Resource Groups that are based on CloudFormation stacks. These groups are resources created by the same CloudFormation stack in a particular region. The Resource Group will have the same logical structure as the stack. Systems Manager and Resource Groups allow you to create custom consoles that show organized and consolidated information about Resource Groups, and offer helpful visibility for operation and management.</p>
<p>As a default, the AWS Management Console shows resources organized by service category, as you may have already observed in the EC2 Management Console. The Tag Editor allows you to define tags and what will become a Resource Group. It allows for bulk editing and application of tags to resources in a specific region. The Tag Policy Editor can help enforce tagging across your resources in a particular account or the entire organization. You can manage Resource Groups and find the Tag Editor under the AWS Resource Group service in the Management Tools sections of your AWS Console. Also, as you provision resources on the console, a section of the provisioning will always permit you to define tags.</p>
<p>As you may have noticed, establishing the best practice of tagging your resources becomes essential in order for you to use and take advantage of the features of Systems Manager. As you <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/aws-systems-manager-requirements-and-building-blocks/">build</a> your fleet of instances, it is important to catalog these resources using tags. Later, it becomes significantly easier to group them and operate on them using Systems Manager.</p>
<h1 id="AWS-Systems-Manager-Requirements-and-Building-Blocks"><a href="#AWS-Systems-Manager-Requirements-and-Building-Blocks" class="headerlink" title="AWS Systems Manager Requirements and Building Blocks"></a>AWS Systems Manager Requirements and Building Blocks</h1><p>AWS Systems Manager, Requirements and Building Blocks. The SSM Agent. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/introduction/">AWS Systems Manager</a> requires an agent for its management service. The Systems Manager Agent is the software required to be installed and configured on all instances in order for them to be called managed instances.</p>
<p>A managed instance is an instance with the ability to communicate and be operated by Systems Manager. The agent executes and process tasks you specify through any of the Systems Manager features, like the Run Command. The agent is installed by default on the Amazon Linux AMIs, the AWS Windows AMIs, and available on the Amazon Linux repo. The agent is open-sourced and available on GitHub. You can install the agent on a physical server or a virtual machine in your data center or even another cloud provider. You can manage Windows Server 2003 or later, and Linux distributions like Amazon Linux, Ubuntu, Red Hat Enterprise Linux, SUSE, and CentOS.</p>
<p>Managed Instance Roles. A managed instance will require an Identity and Access Management role applied as an instance profile in order for Systems Manager to be able to interact with the agent and make the instance visible in the Systems Manager Fleet Manager console. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> provides pre-defined managed policies for Systems Manager. They usually have the acronym SSM as part of their name. One of them is called Amazon EC2 Role for SSM, which can save you time in the instance configuration. You can also create your own custom role if needed or use one of the many other SSM-related policies available. To register servers and virtual machines in your data center or other cloud providers outside the scope of Amazon EC2, you can create a hybrid activation and use the activation code and activation ID supplied to configure the agent and centrally manage your hybrid environment and EC2 instances from one location.</p>
<p>Fleet Manager Feature. Once you configure a managed instance, you can go to the Systems Manager console. And under the Node Management section, you will see the Fleet Manager feature. All your managed instances will be displayed in this console. Fleet Manager will give you visibility into the details of each managed instance, including Instance ID, Platform Type, Instance Type, Operating System name, IP Address, and the version of the SSM Agent that is installed among many other features. One interesting item about the Fleet Manager managed instance console is that under instance action, you can connect to the instance using the Session Manager feature of Systems Manager.</p>
<p>The Session Manager feature of Systems Manager is a fully-managed capability that lets you connect to any managed instance using an interactive browser shell login for Linux, Windows, and MacOS instances. It requires no open inbound ports and no need to manage bastion hosts or Secure Shell keys for connectivity to your instance. You also don’t need Secure Shell clients for Linux, or Remote Desktop Protocol clients for Windows when using Session Manager. Communication between Session Manager and instances uses Transport Layer Security version 1.2, or TLS 1.2 for short. Security of the communication can be increased using your own Key Management Service keys. Session Manager tracks all commands and output produced in a session, and also provides full logging and session auditing activity that can be dispatched to CloudTrail, CloudWatch, or an Amazon S3 buckets as a result. Session Manager can control which users can access specific instances by using Identity and Access Management policies. It works through the interactive browser shell or using the AWS Command Line Interface.</p>
<h1 id="The-Benefits-of-Logging"><a href="#The-Benefits-of-Logging" class="headerlink" title="The Benefits of Logging"></a>The Benefits of Logging</h1><p>Hello, and welcome to this short lecture, where I want to discuss a few of the different benefits that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> can bring you and your infrastructure. For some, many people consider logging an afterthought, something that is implemented after it’s too late. This is often the case where an incident or breach of security has occurred that resulted in a delay of resolution and safeguarding of your environment. In hindsight of these situations, logging would have been a great idea to have had running and implemented in the first place to rectify the event quickly and efficiently, or even prevent it from happening in the first place.</p>
<p>So how can logging help?</p>
<p>Generally, logs are created by services and applications which contain a huge amount of information, which is recorded and retained on persistent storage, to be reviewed and analyzed at any time that it might be needed. Some logs can be monitored in real time, allowing automatic responses to be carried out, depending on the data contents of the log. From an auditing perspective, these logs are invaluable. They often contain vast amounts of metadata, including date stamps, source information such as IP address or usernames, and this is especially true when you’re looking at CloudTail logs. These logs can be used to help you achieve specific compliance certifications that require evidence of traceable and auditable actions that have been carried out. </p>
<p>Being able to resolve an incident as quickly as possible is paramount within your organization. Whether it’s a priority one, two, or three, being able to gain as much insight into what happened just before and just after the incident can significantly reduce your time to resolution. Using logs to ascertain the state of your environment before and after and even during the incident provides clarity and enables you to detect where the incident occurred, allowing you to pinpoint your efforts in a specific area. Quicker resolution results in a better customer experience for your organization. </p>
<p>By monitoring the data within your logs, you’re able to quickly identify potential issues that you want to be made aware of as soon as they occur. By combining this monitoring of logs with thresholds and alerts, you are able to receive automatic notifications of potential issues, threats, and incidents, prior to them becoming a production issue. By logging what’s happening within your applications, network, and other cloud infrastructure, you are able to build a baseline of performance and establish what’s routine and what isn’t. By having this baseline, you are able to identify threats and anomalies easier through the use of third party tools and management services. </p>
<p>To have a thorough understanding of what’s happening within your infrastructure provides a huge benefit to your operational teams. Having an inside look of how your infrastructure is performing and communicating helps achieve the previous benefits that I’ve already discussed, and having more data about how your environment is running far outweighs the disadvantage of not having enough information, especially when it really matters to your business in the case of incidents and security breaches. </p>
<p>That now brings me to the end of this lecture. There are many more reasons as to why you should be capturing data that can be logged. But I just wanted to provide a few key points to you. </p>
<h1 id="CloudWatch-Logging-Agent"><a href="#CloudWatch-Logging-Agent" class="headerlink" title="CloudWatch Logging Agent"></a>CloudWatch Logging Agent</h1><p>Hello and welcome to this lecture where I shall explain what CloudWatch Logs are, how they work and how they are configured. As we know CloudWatch is a monitoring service that is used to collate and collect metrics on resources running on your AWS account allowing you to monitor their performance and respond to alerts that meet to find thresholds. In addition to this, Amazon CloudWatch is a powerful tool that allows you to collect logs of your applications and a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. </p>
<p>When data is fed into Cloudwatch Logs you are able to monitor the logstream in real time and set up metric filters to search for specific events that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. Let me explain the different components of this feature to give you a better understanding of how it all fits together. Starting with the Unified CloudWatch Agent. </p>
<p>With the installation of the Unified CloudWatch Agent you are able to collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. Interestingly this metric data is in addition to the D4EZ2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. The agent can be installed on a number of different operating systems and at the time of writing this course the operating systems versions supported are as follows. </p>
<p>To install the agent and configure it requires a number of different steps. To install the agent on your EC2 instances you need to perform the following, firstly you need to create a role and attach it to the instance with permissions allowing CloudWatch to collect data from the instances in addition to interacting with AWS systems manager SSM. You then need to download and install the agent onto the EC2 instance. And lastly configure and start the CloudWatch Agent. The most efficient way of completing this installation and configuration is with the use of the EC2 systems manager service known as SSM. You will need to create two roles. One role will be used to install the agent and also to send the additional metrics gathered to CloudWatch. The other role is used to communicate with the parameter store within SSM, to store a configuration file of the agent which then can be shared with other EC2 instances. </p>
<p>From a security perspective it’s best that only one of your EC2 instances has this permission to write to the parameter store. Once the agent configuration file is stored on SSM there is no need for other EC2 instances to do the same. In fact once the write has been completed, this role should be detached from the EC2 instance and the other role applied which simply allows the agent to send data to CloudWatch. </p>
<p>The role with the additional permissions for SSM needs to be configured as follows when creating a role. The options, ‘select the type of trusted identity’ needs to be ‘AWS service’. The option to ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. The ‘Attach Permissions Policies’ needs to be ‘CloudWatch Agent Admin Policy’ and the ‘Amazon EC2 role for SSM’. </p>
<p>The role that is simply used to install the agent and send data back to CloudWatch needs the following configuration, the ‘select type of trusted identity’ needs to be ‘AWS service’. The option ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. And finally under the ‘Attach Permissions Policies’ it needs to be ‘CloudWatch Agent Server Polic’y and ‘Amazon EC2 Role for SSM’. </p>
<p>Once your roles are created you can then associate the role shown in orange here, which I shall call ‘CloudWatch Agent Admin Role’ to your EC2 instance that will store the configuration file in the parameter store. </p>
<p>From the EC2 instance with additional permissions that will be saved in the configuration file with in the parameter store of SSM, you must then install the agent which can be done using systems manager or it can be downloaded from an S3 public link either for Linux or Windows. However as mentioned earlier I will explain how to do this via SSM in particular the run command function. As a prerequisite to the CloudWatch Agent installation you’ll need to verify that your EC2 instance has access to the internet to communicate with SSM and CloudWatch endpoints. In addition to this you must also have the SSM agent installed. For some AMI’s as stated on the screen the agent may already be installed. For more information on how to install or update your SSM agent on your EC2 instance please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">link</a>. Let me now perform a very quick demonstration to show you how to install the actual CloudWatch agent on an Amazon Linux EC2 instance using the SSM role command. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration 1"></a>Start of demonstration 1</h3><p>Okay so I’m within my AWS management console in the EC2 dashboard and as you can see I have an EC2 instance here named Logging Server and I have it in a VPC that has internet access. I have the SSM agent installed because it’s based on one of the latest Linux AMI’s comes by default with this, and I’ve also attached the CloudWatch Agent Admin Role which I discussed in the previous section so I’ve met all the prerequisites to install the CloudWatch Agent. </p>
<p>Now what I want to do is use the EC2 systems manager to install the CloudWatch Agent itself. So if I load up SSM which now has it’s own console and on the left hand side if I go down to the run command and click on run command to create a new command. What I want to select for the command document is the AWS configure AWS package. So if you just select the entry and then scroll down. And then next I need to select which EC2 instance that I want as the target. So which instance I’m going to install this package on and here we can see the Logging Server so I would just highlight that. And as you can see it’s added our EC2 instance already up there. </p>
<p>Now we come down to the command parameters and the action I want to perform is an install. The name of the package is Amazon CloudWatch Agent and I want to install the latest version. You can add some other comments here and some timeouts, for this demonstration I’m just going to leave those default. If you wanted to perform this same installation via the AWS CLI then you can click on the AWS command line interface command and then based on the above parameters you can simply cut and paste this command and run that command. But for this demonstration I’m going to use the run command within SSM. </p>
<p>So if I click on run we can see here that the command ID was successfully sent. It’s currently in progress and that will just take a moment to process and install the agent, then we should get a return of successful. So if I go back to the main dashboard of the run command we can see here that it was a success. This is the package that we just run using using this command ID, so now the agent is successfully installed. </p>
<p>As you can see here I have a previous command that failed so I just want to show you that worst on here. So if we select that command and go to view details we can drill down to understand why this failed. And if we scroll down to the bottom you can see here that it failed. Select the instance ID and view the output. Now step one, we can dive deeper to see why this failed. Now if we scroll down to the error itself and here it said it failed to retrieve the manifest and it could not find the latest version of this particular package. Now what happened here was I didn’t enter the correct package name. What I entered was CloudWatch Agent instead as we performed in the demonstration just now. The correct name is Amazon CloudWatch Agent. So it couldn’t find the package that was available and that was the reason why it failed. </p>
<p>However, going back to one that was successful, we can see here, we can drill down into this just to make sure everything was okay. Again here, success and we can view the output of this as well like we done with the failed one. And we can see here that it was all installed. It found all the files and it went through and processed everything okay. And that’s it, that’s how you install the CloudWatch Agent using the SSM run command. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration 1"></a>End of demonstration 1</h3><p>On your first instance you’ll need to create the CloudWatch Agent configuration file. Without doing so, you will not be able to start the agent. This file stores configuration parameters that specify which metrics and logs to capture on the instance which are then sent to CloudWatch. It can be created manually or by using a wizard. If you create it manually then you have a much wider scope for capturing elements that are not included within the wizard. Let me show you via another demonstration how to configure the agent using the wizard. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration 2"></a>Start of demonstration 2</h3><p>So I’m now on my Logging Server instance where we installed the agent and now need to configure it. So to configure it we’ll run this command here and this will launch the Amazon CloudWatch Agent configuration wizard. And it just goes through a number of questions before it completes so let’s take a look. So the first question it asks which operating system your using Linux or Windows, take the default choice of Linux. And if we’re going to try and collect logs on EC2 instances or On-Premises So it’s EC2 for us. </p>
<p>We then have a question if we want to monitor any host metrics such as CPU, memory, etc, I’m going to set a default of yes. And if we want to monitor CPU metrics per core and here additional CloudWatch charges may apply. For this demonstration I’m going to say no. We then have a question if we want to add any EC2 dimensions such as the instance ID or the instance type into our metrics if the information is available. Say yes. Then we have a question about how often CloudWatch will collect these metrics. Whether that’s one second, 10 seconds 30 seconds or 60. I’ll accept the default of every minute, 60 seconds and then asks which default metric config we want whether that’s basic, standard, advanced or none. I’ll accept the default of basic for this demonstration. It then just shows you an example of that configuration that you just selected. If you’re happy with that you can say one for yes or two for no. And then select a different default metrics config so I will just select yes for this demonstration, I’m happy with that. </p>
<p>Now it asks the question if we have an existing CloudWatch log agent configuration file that we want to import. At the moment we don’t but what we’re trying to do is get in the position of creating one to then import into the parameter store of SSM. So at this moment our default choice is no which is correct. It then asks if we want to monitor any log files on this EC2 instance. I’m going to say no just for this demonstration because all we are trying to do is configure the CloudWatch agent at the moment. Our next question asks if we want to store the config in the SSM parameter store, and here yes we do cause we want to upload this file to the parameter store to allow all other EC2 instances to connect to the SSM parameter store to download the configuration file to prevent us from doing this on each and every instance. So I’m going to say yes which is number one that we do want to store the config in the SSM parameter store. </p>
<p>It then asks what default name you want to give this configuration file. And it just gives you a little message there saying you should use the prefix of Amazon CloudWatch hyphen if you’re using any of the AWS managed policies. So I would go with their suggested name of Amazon CloudWatch Linux and you shouldn’t run into any issues there. It then asks you which region you want to store the config in the parameter store in and it gives a default choice. I’m just going accept that default. It then asks a question about which credentials should be used to send the config to the parameter store. I’m going accept the default credentials there which should have access and we now have a message that it successfully put the config to the parameter store Amazon CloudWatch-Linux. And that’s it, that’s the end of the wizard so it’s a very simple and quick and easy wizard. And now your CloudWatch agent configuration file is configured and it’s been uploaded to the parameter store so now any other EC2 instances can click to that parameter store and simply download it and have the agent running very quickly and easily. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration 2"></a>End of demonstration 2</h3><p>Now the configuration file is configured and successfully copied to the SSM parameter store I simply now need to start the agent and again I will use the systems manager service to complete this file with the run command. Let’s take a look. </p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration 3"></a>Start of demonstration 3</h3><p>Okay so the final stage is to start the agent and again I’m going do this from the AWS systems manager so I’m at the systems manager dashboard. Again I’m going use the run command so so it’s over on the left hand side here. Click on run command. Then click on the orange run command button and this will allow us to start a new command for the command document. What we need to look for is the following, which is Amazon CloudWatch Manage Agent. And here we have the command document here. So I’m going to select that. </p>
<p>Scroll down, we then need to select our target instance and we have our Logging Server here. Now the command parameters, for the action we want to select configure rather than stop because we’re going to select the configuration file that we uploaded to the parameter store first using the EC2 mode rather than On-Premises. The optional configuration source which is SSM which is where we stored the configuration file so we’ll leave that as SSM. Now in the optional configuration location we need to enter the name of the file that we stored it as. And if you can remember that was Amazon CloudWatch-Linux. So we’ll paste that in. </p>
<p>Now under optional restart we want that as yes because it will then start the agent once it’s pulled the information from SSM. If we scroll down to the bottom simply click on run. And now we have it, the command ID was successfully sent. If we go back to our dashboard we can see that it was a success. And here the document name was the CloudWatch Manage Agent. So the CloudWatch Agent will now be running on that EC2 instance, the Logging Server. And it’s as simple as that. Now we have our logs configured and log data of our EC2 instance is being sent to CloudWatch along with the additional metric information. It’s now possible to search for specific entries within the logs for points of interest. </p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration 3"></a>End of demonstration 3</h3><p>That now brings me to the end of this lecture on CloudWatch logs where I explained how CloudWatch can be used to centralize login for more EC2 instances or applications running on your instances by determining logs past configured by the CloudWatch Agent and log groups within CloudWatch. Coming up next I should be talking about CloudTrail logs.</p>
<h1 id="CloudTrail-Logging"><a href="#CloudTrail-Logging" class="headerlink" title="CloudTrail Logging"></a>CloudTrail Logging</h1><p>Hello, and welcome to this lecture focusing on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> capabilities and configuration of AWS CloudTrail. You should already be familiar with what CloudTrail is and what it does. However, to quickly summarize: It’s a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface: CLI, from within the AWS Management Console, Or even from a request made by another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service. For example, when auto-scaling automatically sends an API request to launch or terminate an instance. These API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the new log file. </p>
<p>CloudTrail also records and associates other identifying metadata with all events. For example, the identity of the caller, which can be the user or the account that made the API call, the timestamp of when the request was initiated, and the source IP address. The logs generated are the output of the CloudTrail service and they hold all of the information relating to the API calls that have been captured. So as a result, it’s important to know what you can do with these logs in order to maximize the benefit of the data they contain. </p>
<p>So, what is a log file and what does it look like? Log files are written in a JSON format. Much like access policies within IAM and S3. Every time an API is captured, it’s associated with an event and written to a log. And new logs are created approximately every five minutes or so, but they are not delivered to a nominated S3 bucket for persistent storage for approximately 15 minutes after the API was called. So if you expect to see the log file for an API called seven minutes ago, then you may not see the log as expected for potentially another eight minutes. The log files are held by the CloudTrail service until final processing has been completed. Only then will it be delivered to S3, and optionally, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a>, depending on your configuration of the trail. </p>
<p>When an event reflecting an API call is written to a log, a number of attributes are also written to the same event capturing key data about that call. As you can see from this example. Without going through every attribute here, I just want to point out some of the more interesting ones. These being eventName. This refers to the name of the actual API that was called. EventSource. This refers to the service as to which the API called was made against. EventTime. This was the time that the call was made. SourceIPAddress. This displays the source IP address of the requester who made the API call. This is a great piece of information when trying isolate an attacker from a security perspective. UserAgent. This is the agent method that the request was made through. Example values of these are: Signin.amazonaws.com and this is what we have in our example and it simply means that user made this request from within the AWS management console. Console.amazonaws.com, this is the same as the previous, however, if this was displayed, it would mean that the request was made by the root user of the account, and lambda.amazonaws.com, this is fairly obvious, this would reflect that the request was made with AWS lambda. UserIdentity. This contains a larger set of attributes that provides information on the identity that made the API request. Once events have been written to the logs and then delivered and saved to S3, they are given a standard name and format, as shown. </p>
<p>The first three elements of this naming structure are self-explanatory. The AccountID, Name of the Service delivering the log, CloudTrail, and the region that it came from. The next part relates to the date and time. The year, months, and days. The T indicates the next part is the time reflecting hour and minutes. The Z simply means that the time is in UTC. The UniqueString value is a random 16 alphanumeric character string that is simply used by CloudTrail as a unique file identifier to ensure that it doesn’t get overwritten with the same name of another file. Currently, the FileNameFormat is defaulted to json.gz which is a compressed GZ version of a JSON text file. While we are looking at structures, let me also talk about the bucket structure way your logs are stored. </p>
<p>You may feel that the logs are all stored in one folder within your S3 bucket. However, there is a lengthy but very useful folder structure as follows: Firstly, you have your dedicated S3 BucketName that you selected during the creation of your Trail. Next, is the prefix that is also configured during Trail creation and is used to help you organize a folder structure for your logs corresponding to different Trails. Following this, is a fixed folder name of AWSLogs. Followed by the originating AWS account ID. Then another fixed folder name of CloudTrail indicating which service has delivered the logs. And after that, the RegionName of where the log file originated from. This is useful for when you have Trails that apply to multiple regions. The last three folders show the year, month and day that the log file was delivered. As you can see, although there are multiple folders underneath your nominated S3 bucket, it does provide an easy navigation method when looking for a specific log file.</p>
<p>This folder structure comes into even greater use if you have multiple AWS accounts delivering logs to the same S3 bucket. Some organizations may be using more than one AWS account, and having CloudTrail logs stored in different S3 buckets across multiple accounts can be inconvenient in certain circumstances and require additional administration to manage. Thankfully, AWS offers the ability to aggregate CloudTrail logs for multiple accounts into a single S3 bucket belonging to one of these accounts. This is why there is an accountID folder within your S3 bucket. Please note that you are unable to aggregate CloudTrail logs for multiple AWS accounts into CloudWatch logs that belongs to a single AWS account. </p>
<p>So to have all your logs from your accounts delivered to just one S3 bucket is a fairly simple process with the end result allowing you to essentially manage all your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/monitoring-cloudtrail-cloudwatch/">CloudTrail logs</a>. Let’s take a look at how this solution is configured. Firstly, you need to enable CloudTrail by creating a Trial in the AWS account that you want all log files to be delivered to. Permissions need to be applied to the destination S3 bucket allowing cross account access for CloudTrail. And once permissions have been applied to your policy, you need to edit the bucket policy and add an additional line for each AWS account requiring access. Then you need to create a new Trial in your other AWS accounts and select to use an existing S3 bucket for the log files. When prompted, add the bucket name used in step one and when alerted, accept the warning that you want to use a bucket from a different AWS account. An important point to make here when configuring the bucket selection is to ensure that you use the same prefix as the one you used when you configured the bucket in the first step. That is unless you intend to edit the bucket policy to allow CloudTrail to write to the location of a new prefix you wish to use. When you have configured your Trail, click create and your new Trail will now deliver it’s log files to the S3 bucket in your AWS account used in the first step. Again, this is a great solution that allows you to essentially manage all of your CloudTrail logs in one single account and S3 bucket. However, there may be uses such as system administrators who manage the other AWS accounts where the logs have come from that might need access to data within these logs. So how would they gain access to the S3 bucket to allow them only to access their CloudTrail logs that originated from their AWS account? </p>
<p>It could be done quite easily by configuring a few elements within IAM. Firstly, in the master account, IAM Roles would need to be created for each of the other AWS accounts requiring read access. Secondly, a policy would need to be assigned to those Roles allowing access to the relevant AWS account logs only. Lastly, users within the requesting AWS accounts would need to be able to assume this Role to gain read access for their CloudTrail logs. The easiest way to show you how to configure the permissions required is by a demonstration while I shall perform the following steps: I shall create a new Role. Apply a policy to this Role to only allow access for AWS account B’s folder in S3. Show the Trust Relationship between AWS account A and B. I will then create a new IAM user in account B. And create a Policy and apply the sts:AssumeRole permissions to this user allowing them to assume the new Role we created in account A. So let’s take a look at how and where we apply these permissions. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so, as I just said the first thing we need to do is create a new Role in our primary account. So, if we go across to IAM, which is under Security, Identity and Compliance and then once that’s loaded, we need to go across to Roles and then Create New Role. So let’s give this Role a name. I’ll call it ‘Cross Account CloudTrail’ Click on Next Step. We then need to select a Role type, and what we want to do is select Role for Coss-Account access because we will allow users in another AWS account to access the log files in this primary AWS account. And this will set up the trust relationship between this account and then my secondary account. So, for that we will select this top option of providing access between AWS accounts you own. Then next, I’ll need to enter the secondary account ID that I want to create the trust relationship with. So I’ll just enter that number. </p>
<p>Okay, then after you have entered your account ID click on next step. And now we need to attach a Policy to this Role. Prior to this demo, I set up my own Policy and this allows cross-account access to read only from my secondary account to the bucket on this primary account. But I’ll explain this Policy in a few moments and I’ll show exactly what it contains. And from here click on next step and this is just a review of the Role. So we have the Role name, the ARN, the Amazon Resource name, the trusted entities. So this is the secondary account ID that I entered, and then the actual Policy and then the link that we can give to users in the secondary account to allow them to switch Roles. So, create Role. And there we go, the cross-account CloudTrail Role that we just created. So, let’s take a look at this.</p>
<p>Firstly, I’ll show the trust relationships. So, because we added a cross-account Role access and then we entered the secondary AWS account ID, we can see that this account is trusted by our primary account and that allows entities in this account to assume this Role. Now, I mentioned earlier that I previously set up a Policy with permissions in. So let’s take a look at that Policy. I named it Cross-Account read only for CloudTrail. So if I show the Policy, as you can see it. I made a very small Policy. Very simple. Now we have an effect of allow which will allow any S3:Get and any S3:List command, so essentially, read only access on this resource here specified by this line. Now this resource links to the bucket and folder where CloudTrail logs are delivered for our secondary account as you can see here. So, essentially, what this Policy does is allow read only access to any folders within the secondary account’s CloudTrail log folders. So this account won’t be able to access any other accounts CloudTrail logs, which is important. So, if we come out of this. So let’s just have a quick recap of what we’ve achieved so far.</p>
<p>So, so far, what we’ve done: We’ve created a Role in our primary account for our secondary account access. And we’ve also assigned an access Policy to this Role in order for the secondary AWS account to access the relevant folder in S3. So now what we need to do is assign a user in the secondary account and then apply the permissions to that user to enable them to assume the new Role in the primary account. So let’s go ahead and do that. </p>
<p>Okay, so I’ve now logged into the secondary account where I’ll need to create a new user and assign the correct permissions. So, to start with, I’m going to set up a permission Policy to assign to the user. So if I go down to Security, Identity and Compliance and select IAM. And then go across to Policies, and from here I want to create a new Policy. And I am going to create my own Policy, so I’m going to select the bottom option. I’m going to call this AssumeRoleforCloudTrail And description will be Assume role in primary AWS account. And for the Policy document, I’m going to paste in a Policy that I’ve already created. As you can see, it’s only a very small Policy again. And we have an allow effect that allows the Assume Role action from the security token service against the following resource, and this resource links back to a Role on our primary account where we created the Role cross-Account CloudTrail. </p>
<p>So this Policy will allow the user to assume this Role in the primary account. So let’s go ahead and create that Policy. Let’s validate it first. And then create. Now what we need to do is to assign a user to use that Policy. Now, I created a new user earlier prior to this demo. So, let’s just find our new Policy that we just created. And here it is at the bottom, AssumeRoleforCloudTrail. And I’m going to attach a user. And I’ve called our user CloudTrailUser1. And then attach Policy. And there we go. </p>
<p>So we now have one user attached to this Policy. So that’s all the actions and steps necessary to allow a user in a secondary account to access CloudTrail log files that have been delivered to an S3 bucket in a primary account. And it would do this by using the permission Policy that we just applied to that user to access the Role in the primary account. And that Role has a Policy attached that allows S3 read access to it’s own CloudTrail logs. </p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>CloudTrail allows you to enable a feature called Log File Integrity Validation. Which simply allows you to verify that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. This is typically used for security and forensic investigations where by the integrity of the log files are critical to confirm that they have not been tampered with in any way. </p>
<p>When a log file is delivered to an S3 bucket a hash is created for it by CloudTrail. A hash file is a set of characters that are unique that are created from a data source. In this case, the log file. The hashing algorithms used by CloudTrail are SHA-256. In addition for a hash for every log file created, </p>
<p>CloudTrail creates a new file every hour, called a digest file, which is used to help verify your log files have not changed. The digest file contains details of all the logs delivered within the last hour along with a hash for each of them. These files are stored in the same bucket as the key pair. When it comes to verifying the integrity of your log files, the public key of the same key pair is used to programmatically check that the logs have not been tampered with in any way. Verification of the log files can be achieved via a programmatic access and not via the console. Using the AWS CLI, this can be checked by issuing the following command. The folder structure for the digest is very similar to the CloudTrail logs, as you can see. But the digest files are clearly distinguishable by the CloudTrail digest folder. </p>
<p>That has now taken me to the end of this lecture. Coming up next, I’ll explain how you can use CloudTrail and CloudWatch together as a monitoring solution.</p>
<h1 id="Monitoring-CloudTrail-with-CloudWatch"><a href="#Monitoring-CloudTrail-with-CloudWatch" class="headerlink" title="Monitoring CloudTrail with CloudWatch"></a>Monitoring CloudTrail with CloudWatch</h1><p>Hello and welcome to this lecture where we will look at how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudtrail-logging/">AWS CloudTrail</a> interacts with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a> and SNS to create a monitoring solution. In addition to S3, the logs from CloudTrail can be sent to CloudWatch Logs, which allows metrics and thresholds to be configured, which in turn, can utilize SNS notifications for specific events relating to API activity. CloudWatch allows for any event created by CloudTrail to be monitored. This enables a whole host of security monitoring checks to be utilized. A great example of this is to be notified when certain API calls requesting significant changes to your security groups or network access control lists within your VPC. Other examples of these checks that are common within organizations are API calls relating to it starting, stopping, rebooting, and terminating EC2 instances. If instances are being created that shouldn’t be, then your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> cost could rise dramatically and quickly. Also, if instances are being rebooted or stopped, this could have a severe impact to your services if they are not configured in a high availability and resilient solution. Changes to security policies within IAM and S3. If changes are being made to your policies that shouldn’t be, access can be inadvertently removed for authorized users and access granted to unauthorized users, having a massive impact on operational services. Even a minor change to a policy can pave the way for an untrusted user to exploit the error. Failed login attempts to the Management Console. Monitoring failed attempts here can help to prevent unauthorized access at your environment’s front door. API calls that result in failed authorization. Not only does CloudTrail track successful API calls whereby the correct authorization was met by the authenticated identify, but it also tracks unsuccessful API requests, too, which would likely be due to the permissions applied. Special attention should be given to these unsuccessful attempts, as this could be a malicious user trying to gain access. However, it could also be a legitimate user trying to access a resource they should have access to for their role, but the incorrect permissions had been applied with their associated IAM policy. </p>
<p>To configure CloudTrail to use CloudWatch, you must first create a trail. Once your trail has been created, you can then configure it to use an existing CloudWatch Log group or have CloudTrail create a new one. Having CloudTrail create a new one for you is recommended if it is your first time doing this, as CloudTrail will take care of all of the necessary roles, permissions, and polices required. You may be wondering why roles and policy are required, so let me give you a high-level overview of the simple process that takes place when sending CloudTrail logs to CloudWatch. When a log file is created by CloudTrail, it is sent to your selected S3 bucket and your chosen CloudWatch Log group, assuming your trail has been configured for this feature. To allow CloudTrail to deliver these logs to CloudWatch, CloudTrail must have the correct permissions and these are gained by assuming a role with the relevant permissions needed to run two CloudWatch APIs. The first being CreateLogStream, and this enables CloudTrail to create a CloudWatch Logs log stream in the log group, and PutLogEvents, and this allows CloudTrail to deliver CloudTrail events to the CloudWatch Logs log stream. CloudWatch then delivers logs to the CloudWatch Logs. </p>
<p>When using the AWS Management Console, you can have the CloudTrail create this role for you, along with the correct policy. By default, the role is called CloudTrail_CloudWatchLogs_Role. For those that are curious, the policy for this role looks as shown. It’s important to point out that CloudWatch Log events have a size limitation of 256 kilobytes on the events that they can process. Therefore, any events that are larger than 256 kilobytes will not be sent to CloudWatch by CloudTrail. </p>
<p>Now that you have your logs with the associated events being sent to CloudWatch, you must then configure CloudWatch to perform analysis of your CloudTrail events within the log files. This is done by configuring and adding metric filters to the log within CloudWatch. These metric filters allow you to search and count a specific value or term within your events in your log file, which then allows for customizable thresholds to be applied against them. When creating these metric filters, you must create a filter pattern which determines what exactly you want CloudWatch to monitor and extract from your files. These filter patterns are usually fully customizable strings but as a result, a very specific pattern syntax is required. So, if you’re creating these for the first time, you must understand the correct syntax. </p>
<p>Just to reiterate what we have spoken about so far, I want to provide a demonstration on how to edit an existing trail to configure it to send logs to CloudWatch Logs. I will then configure a metric filter with the associated metric pattern, and finally, I will set up an SNS alert to notify me when a particular threshold is met. So, let’s take a look. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so what I need to start with is going into CloudTrail to edit an existing trail to enable CloudWatch Logs. So, if I go down to Management Tools and click on CloudTrail and then across to Trails, as you can currently see, under CloudWatch Logs log group, there’s no log group selected. So, if we go into the trail and then scroll down to CloudWatch Logs, click on Configure, and there we can get CloudTrail to automatically set up this group and it’ll create the necessary roles and permissions, etc. So, let’s call this CloudTrail&#x2F;Demo and then click on Continue. So, we’ve given it a name and here it just gives a message to say that for CloudTrail to deliver events and logs to CloudWatch Logs, it needs to assume a role with permissions to run two API calls, which are these two here. And if we go down into the details, you can see that the IAM role that it’s going to use is the CloudTrail_CloudWatchLogs_Role and we’ll ask it to create a new policy. And here’s the policy document. </p>
<p>So, go down to Allow, and then if we scroll down to our CloudWatch Logs section, you can now see that we have a log group created in CloudWatch called CloudTrail&#x2F;Demo. So, if we now go across to CloudWatch and if we click on Logs on the left-hand side here, we can see that we have our log group that was just created by CloudTrail, and it’s CloudTrail&#x2F;Demo. Now, if we go into our log group and select it, you’ll see this log stream, which is the incoming stream of events being sent from CloudTrail. Now, as we’ve only just started, there’s only a few events coming in here, so you might want to wait a few minutes before setting up your metric filters to give you more of a test pattern to search on. So, what I might do is just leave it a couple of minutes for some more events to start streaming in before we set up our metric filters here, just so we have something to search on. </p>
<p>Okay, so I’ve left it a few minutes, so let’s go back into the log group and you can see we’ve now got a couple of streams, and if we go into these, we can see there’s a lot more events. So, if we go back a couple of pages, back to our log group, now we need to create our metric filters to allow us to define what we want to search on within our logs. So, if we select the tick next to our log group and then go up to Create Metric Filter, and here within the metric filter, we need to define a filter pattern. Now, as explained earlier, filter pattern will define what we’re actually searching for within our logs. So, for this example, I’ll keep it fairly simple. I’m going to search for any API call that’s been made from my machine, so from my IP address. So, for that, I need to enter the following command, ( $.sourceIPAddress equals 2.218.11.188, which is my IP address. And now we can test to make sure that that filter pattern’s okay using this Test Pattern box here, and what that does, that’ll run this test filter on some log data we see from this log here and the output of that log is in this box here. So, all we need to do is click on Test Pattern and we can see at the bottom here that it found 47 matches out of 50 events in the sample log. So, we know that the syntax is okay for this filter pattern, so I’m going to go ahead and assign this metric. </p>
<p>And we can see up here that we’ve got our filter name and our filter pattern, and I’m going to create a new name space for this metric and I’ll call it Demo, and the metric name will be IPAddress. And then what we need to do is click on Create Filter. Now, as you can see, our filter has been created and we have the details in this screen here. Now, what we can do at this point is create an SNS alarm so it could be notified if a certain threshold was met. So, let’s go ahead and do that. </p>
<p>So, the first thing that we need to do is add a name, so I’m going to call this SourceIPAddress and description will be Too many calls from my IP. Now, I’m going to set this to be 30. So, whenever my IP address is used as a source IP address that is greater or equal to 30 times for one consecutive period over five minutes, then I want it to set to a state of an alarm. And I want to be notified, so I’m going to enter a new list, give this a new topic, SourceIPAddressAlarm, and I want that to be sent to myself. So, as we can already see, with the current data it’s got that it has already breached the alarm, but it has gone back down below, so we’ll see how this goes and we’ll create the alarm. And this is a message just to say that I need to subscribe to that AWS notification, and I can do that in just a few moments. So, if we go across to our Alarms, we can see that we have our source IP address alarm in the state of OK. </p>
<p>So, at the minute, it’s currently below the 30 threshold. As soon as it goes above that, it will alarm and I will get a notification. Now, over the past few minutes, I’ve just been having some activity within the Management Console, and as we can now see, we do have an alarm on our alert. We can see that it just crossed the threshold, and so, I’ve received an email notification to say that it is now in a state of alarm. And if we take a quick look at that email, we can see here that it was crossed with a data point of 33 and the threshold was 30. So, that is how you set up CloudTrail to use CloudWatch with the inclusion of SNS to create alarms against API activity.</p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="CloudFront-Access-Logs"><a href="#CloudFront-Access-Logs" class="headerlink" title="CloudFront Access Logs"></a>CloudFront Access Logs</h1><h2 id="Resources-Referenced"><a href="#Resources-Referenced" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">Web distribution log file format</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">RTMP distribution log file format</a></p>
<h2 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello, and welcome to this lecture focusing on the access logs generated by Amazon CloudFront. Amazon CloudFront is AWS’s content delivery network that speeds up distribution of your static and dynamic content through its worldwide network of edge locations. When you use a request content that you’re hosting through Amazon CloudFront, the request is routed to the closest edge location which provides it the lowest latency to deliver the best performance. When CloudFront access logs are enabled you can record the request from each user requesting access to your website and distribution. As with S3 access logs, these logs are also stored on Amazon S3 for durable and persistent storage. There are no charges for enabling logging itself, however, as the logs are stored in S3 you will be stored for the storage used by S3. </p>
<p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/introduction/">logging</a> process takes place at the edge location and on a per-distribution basis, meaning that there will not be data written to a log that belongs to more than one distribution. For example, distribution a, b, c, will be saved in a different log to that of distribution d, e, f. When multiple edge locations are used for the same distribution, a single log file is generated for that distribution and all edge locations write to the single file. </p>
<p>The log files capture data over a period of time and depending on the amount of requests that are received by Amazon CloudFront for that distribution will depend on the amount of log fils that are generated. It’s important to know that these log files are not created or written to on S3. S3 is simply where they are delivered to once the log file is full. Amazon CloudFront retains these logs until they are ready to be delivered to S3. Again, depending on the size of these log files this delivery can take between one and 24 hours. </p>
<p>When these log files are delivered they use a standard naming convention as follows. So let’s say for example you had the following settings. The bucket name was access-logs, the prefix was web-app-a, and you had the following distribution ID. Then your name and convention for the log would look something like this. Let me now show you a very simple demonstration on how to enable log in for your CloudFront distribution. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>So setting up access logs for your CloudFront distributions is very simple and easy to do. So let’s go into CloudFront. I’ll just select an existing distribution here, and then if you click on distribution settings and under the general tab you select edit, and then if we scroll down these settings here you’ll see a section where it starts referring to logging. And at the moment I have logging off. So to enable logging I simply click on on and then I select the bucket in S3 where I want the access logs to reside, so I’m going to select CloudFront Access Logs, which is an existing bucket I have set up for this. Now here I can add a log prefix if I want to, if I’ve got different distributions, etc. I’m just going to leave that as blank for this demonstration. And here we can have cooking logging on or off, which will log all cookie data within the request, and it’s as simple as that. And then once you’re happy with that you just click on yes to confirm your changes. And now any access requests that go via your CloudFront distribution will be logged via S3. And that’s it. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>To perform the demonstration that I just completed and to access the logs when they are stored, you will need specific permissions to the S3 bucket designated for logging. To enable the log in for your distribution, the user account activating that feature must have full control on the ACL for the S3 bucket, along with the S3 GetBucketAcl and S3 PutBucketAcl. The reason for this is that during the configuration process, CloudFront will use your credentials to add the AWS data-feeds account to the ACL with full control access. This is an account used by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> which will write the data to the log file and deliver it to your designated S3 login bucket. Therefore, if you’re trying to enable the login feature for your distribution and it’s failing, then you should check your access to ensure you have the required permissions. </p>
<p>Depending on the delivery type of your CloudFront distribution, either WEB or RTMP, the log output will vary. The number of fields within the log files differ between the two types. Web distributions have a total of 26 different fill types for each entry within the log, whereas the RTMP distributions only have 13. I won’t go through every single field explaining their purpose and use, however, I want to highlight a few points of interest starting with the web delivery type. These logs contain information which allow you to identify the following. The date and timestamp of the request of the user and which edge location received this request, source metadata of the requester including IP address details, HTTP access method of the request, such as PUT, DELETE, or GET, etc., the HTTP status codes of the request such as 200, the distribution domain name relating to the request, and the encryption and protocol data used in request such as SSL, V3, or AES256-SHA. For full information on each field and options please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">link</a>. </p>
<p>Now looking at the RTMP delivery type, the points of interest are as follows. Again, a timestamp of the request of the user and which edge location received this request, the source IP address of the requester, the event being carried out by the requester such as play, pause, or stop, and the URL of the page where your SWF file is linked to. Again, for full information on field data captured within RTMP logs you can view the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">here</a>. </p>
<p>One final feature of logging with CloudFront is cooking logging. If you enable this within your distribution, then CloudFront will include all cookie information with your CloudFront access log data. This is only recommended if your origin of your distribution points to anything other than S3 such as an EC2 instance as S3 does not process cookie data. </p>
<p>That now brings me to the end of this lecture covering AWS CloudFront logs. Coming up next I shall be looking at the logs generated at the network level within your VPC with the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC flow logs</a>.</p>
<h1 id="VPC-Flow-Logs"><a href="#VPC-Flow-Logs" class="headerlink" title="VPC Flow Logs"></a>VPC Flow Logs</h1><h2 id="Transcript-1"><a href="#Transcript-1" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture covering VPC Flow Logs. Within your VPC, you could potentially have hundreds or even thousands of resources all communicating between different subnets both public and private and also between different VPCs through VPC peering connections. VPC Flow Logs allows you to capture IP traffic information that flows between your network interfaces of your resources within your VPC. This data is useful for a number of reasons, largely to help you resolve incidents with network communication and traffic flow in addition to being used for security purposes to help spot traffic reaching a destination that should be prohibited. </p>
<p>Unlike S3 access logs and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/cloudfront-access-logs/">CloudFront access logs</a>, the log data generated by VPC Flow Logs is not stored in S3. Instead, the log data captured is sent to CloudWatch logs. Before creating your VPC Flow Logs, you should be aware of some of the limitations which might prevent you from implementing or configuring them. If you are running a VPC peered connection, then you’ll only be able to see flow logs of peered VPCs that are within the same account. Or if you are still running resources within the EC2-Classic environment, then unfortunately you are not able to retrieve information from their interfaces. And once a VPC Flow Log has been created, it cannot be changed. To alter the VPC Flow Log configuration, you need to delete it and then recreate a new one. </p>
<p>In addition to this, the following traffic is not monitored and captured by the logs. DHCP traffic within the VPC, traffic from instances destined for the Amazon DNS Server. However, if you decide to use and implement your own DNS Server within your environment, then the traffic to this will be logged and recorded within the VPC Flow Log. Any traffic destined to the IP address for the VPC default router and traffic to and from the following addresses, 169.254.169.254 which is used for gathering instance metadata, and 169.254.169.123 which is used for the Amazon Time Sync Service. Traffic relating to an Amazon Windows activation license from a Windows instance and finally the traffic between a network load balancer interface and an endpoint network interface. All other traffic both ingress and egress can be captured at a network IP level. </p>
<p>You can set up and create a flow log against three separate resources. These being a network interface on one of your instances, a subnet within your VPC, and your VPC itself. Obviously for option two and three, this will contain a number of different resources. As a result, data is captured for all network interfaces either within the subnet or the VPC respectively. I mentioned earlier that this data is then sent to CloudWatch logs via a CloudWatch log group. For every network interface that publishes data to the CloudWatch log group, it will use a different log stream. And within each of these streams, there will be the flow log event data that shows the content of the log entries. Each of these logs captures data during a window of approximately 10 to 15 minutes. </p>
<p>To enable your flow log data to be pushed to a CloudWatch log group, an IAM role is required for permissions to do so. This role is selected during the setup configuration of the VPC Flow Log. If your role does not have the required permissions, then your log data will not be delivered to the CloudWatch group. At a minimum, the following permissions must be associated to the role. In addition to this, you will also need to ensure that the VPC Flow Log service can assume that IAM role to perform the delivery of logs to CloudWatch. This can be achieved with the following permissions. </p>
<p>While on the topic of permissions, I want to also show you the required permissions for someone to review and access the VPC Flow Logs or indeed be able to create one in the first place. The following three EC2 permissions allows you to create, delete, and describe flow logs. These being ec2:CreateFlowLogs, ec2:DeleteFlowLogs, and ec2:DescribeFlowLogs. The logs:GetLogData permissions is used to enable you to list log events from a data stream. If you wanted to create flow logs, then you need to also grant the use of the IAM permission of iam:passrole which allows the service to assume the role mentioned previously to create these flow logs on your behalf. </p>
<p>Let me now show you how to create a flow log for an interface on an instance, a subnet, and lastly the VPC itself. </p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay so firstly I’m going to set up a VPC Flow Log for the running instance that we’ve used in a previous demonstration which was for the logging server. So what I need to do is go down to our network interfaces under network and security and select the ENI of the logging server. As you can see, it’s this bottom instance here. So if I select that interface, if I just drag this up a little bit, and we have three tabs here, details, flow logs, and tags. If we select the flow logs tab of this interface, we can see that there’s no flow log created as yet. </p>
<p>What we need to do is click on create flow log. Now we can select the filter for this flow log to only log either accepted requests or rejected requests so I’m going to select all so it gets accepted and rejected. We now need to select our role and I created a role earlier and I called that Flow-Logs-Role so that has the required permissions to push data to CloudWatch logs. And here we have the ARN of the role. The destination log group for CloudWatch, I set up a log group prior to this demonstration and I’ve just called this Flow-Logs. And then click on create flow log. And that’s it, it’s as simple as that. So now you can see for this eni interface here, we now have a flow log created. It gives it a flow log ID. Shows the filter which we have ALL here. Their destination log group. The ARN of the role and it’s currently active. So now any traffic going in and out of that interface on that EC2 instance will be captured and the data will be sent to the flow logs log group in CloudWatch. And let’s take a look at how you set up flow logs for a subnet. </p>
<p>So let’s go across to our VPC service. I have a couple of VPCs here and we’ll use our logging VPC. So if we go down to our subnets, and let’s select the public subnet for our logging VPC, now again we have the tabs for this subnet. We have the summary, route table, network ACL, etc, and we also again have the flow logs tab. Very simple process again. Click on create flow log. The same filters. Select the same role and the same log group. And then simply create flow log. And that’s now having the flow logs enabled on this particular subnet so all traffic going in and out of this subnet will be captured and sent to the flow logs log group. </p>
<p>And for the VPC, it’s very similar. So you simply select your VPC so we have our logging VPC here, again we have our flow logs tab. Create flow log. Select the role and the destination log group of flow logs and then create flow log and that’s it. So it’s very easy to set up your flow logs for your EC2 network interface clouds or your subnet or your entire VPC. And that’s it. </p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>Let’s now take a look at a record within one of these flow logs. When you access the logs, you will find each entry has the following syntax. These entries are defined as follows. Version, which is the version of the flow log itself. Account-id, this is your AWS account ID. Interface-id, this is the interface ID of which the log stream data applies to. Source address, this is the IP source address. Destination address, this is the IP destination address. Source port, this is the source port being used for the traffic. And the destination port is the destination port being used for the traffic. The protocol, this defines the protocol number being used for the traffic. Packets, this shows the total number of packets sent during the capture. Bytes, again this shows the total number of bytes sent during the capture. Start and end shows the timestamp of when the capture window started and finished. Action, this shows if the traffic was accepted or rejected by security groups and network access control lists. And the log-status shows the status of the logging through three different codes. OK, where data is being received by CloudWatch logs. NoData, this means there was no traffic to capture during the capture window. And SkipData, where some data within the log was captured due to an error. </p>
<p>One of the key fields from an incident response and troubleshooting perspective is the action field. For example, if you are troubleshooting an issue of traffic not being received by a particular resource, then you could check the VPC Flow Logs to see if the traffic is getting blocked at the subnet level by a network ACL. This will then allow you to review your entries within the NACL to make the changes that’s necessary from a security perspective. </p>
<h1 id="Overview-of-the-AWS-Health-Dashboard"><a href="#Overview-of-the-AWS-Health-Dashboard" class="headerlink" title="Overview of the AWS Health Dashboard"></a>Overview of the AWS Health Dashboard</h1><p><strong>Instructor: Carlos Rivas</strong></p>
<h1 id="Overview-of-the-AWS-Health-Dashboard-1"><a href="#Overview-of-the-AWS-Health-Dashboard-1" class="headerlink" title="Overview of the AWS Health Dashboard"></a>Overview of the AWS Health Dashboard</h1><p>The Health Dashboard is divided into 2 main sections:</p>
<ul>
<li>Events that affect everyone (top left), and </li>
<li>Events that affect your account’s resources, right below that.</li>
</ul>
<p>Let’s go over each one.</p>
<h2 id="Service-Health"><a href="#Service-Health" class="headerlink" title="Service Health"></a>Service Health</h2><p>First, you have <strong>Open and recent issues</strong>, this is where you can see current issues happening in the AWS Platform. More often than not, this option will show as disabled if there’s nothing of interest going on.</p>
<p><strong>Service history</strong> on the other hand, will show a historic view of issues. This is really helpful if something happened over a weekend or holiday and you want to get details about which services and regions were affected.</p>
<p>Let’s look at an example of a possible outage:</p>
<p><img src="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid6-3a3ff769-6937-455b-95ea-fe227962f0ae.png" alt="alt"></p>
<p>Each one of these tickets will have: A header, showing the latest status , in this case “resolved” and a short description of the issue. In this case, “Increased API Error rates.”</p>
<p>You want to pay close attention to the affected services, in this case it’s a list of 20 services and chances are, that if you were using one of these services when this event happened in the US-EAST-1 regions, your application would have experienced similar problems.</p>
<p>Information like this is useful to shorten troubleshooting times and also to consider multi-region solutions if your business suffers a significant impact by an issue like this, in this particular AWS region.</p>
<h2 id="Your-account-health"><a href="#Your-account-health" class="headerlink" title="Your account health"></a>Your account health</h2><p>If we switch over to <strong>Your account health</strong>, this is where the Health Dashboard becomes really useful, because it correlates AWS Global issues with the resources and service that you are currently using. This way, you can see if there’s any impact to your business.</p>
<p>For example, let’s say you are running an EC2 instance and it’s been running nonstop for 12 months… </p>
<p>You may go here under the <strong>Scheduled events</strong> tab ( or, you may get it in an Email from AWS) and see something like this:</p>
<p><img src="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid4-e35d3347-e755-45e7-88d0-dc6c2fb51434.png" alt="alt"></p>
<p>Essentially, this means that the physical hardware running your EC2 server may need to be taken down for repairs, upgrades or simply maintenance. – The solution is simple by he way: simply stop and restart your virtual EC2 instance and it will come online on a different physical computer, therefore allowing AWS to perform maintenance without further interruption to you or any other customers.</p>
<h2 id="Integration-with-EventBridge"><a href="#Integration-with-EventBridge" class="headerlink" title="Integration with EventBridge"></a>Integration with EventBridge</h2><p>It’s totally understandable if you don’t want to have to manually visit a web page to find out if there’s an outage affecting your AWS infrastructure, for this, there’s a solution: EventBridge.</p>
<p>EventBridge can be used to monitor and react to AWS Health Dashboard events, and take certain actions including:</p>
<ul>
<li>Sending a notification to the Ops team</li>
<li>Identifying affected resources, and</li>
<li>Executing custom lambda functions to perform pretty much any task, such as creating a Zendesk or JIRA ticket related to an AWS Scheduled maintenance event.</li>
</ul>
<p>We will be looking at this in more detail, but here’s a pattern to catch Events related to notifications, scheduled changes or issues sent to your account via the Health Dashboard.</p>
<p><strong>{</strong></p>
<p> <strong>“detail”: {</strong></p>
<p>  <strong>“eventTypeCategory”: [</strong></p>
<p>   <strong>“issue”,</strong></p>
<p>   <strong>“accountNotification”,</strong></p>
<p>   <strong>“scheduledChange”</strong></p>
<p>  <strong>],</strong></p>
<p>  <strong>“service”: [</strong></p>
<p>   <strong>“AUTOSCALING”,</strong></p>
<p>   <strong>“VPC”,</strong></p>
<p>   <strong>“EC2”</strong></p>
<p>  <strong>]</strong></p>
<p> <strong>},</strong></p>
<p> <strong>“detail-type”: [</strong></p>
<p>  <strong>“AWS Health Event”</strong></p>
<p> <strong>],</strong></p>
<p> <strong>“source”: [</strong></p>
<p>  <strong>“aws.health”</strong></p>
<p> <strong>]</strong></p>
<p><strong>}</strong></p>
<p>With this pattern in EventBridge you can quickly react to potential issues without human intervention and notify the right folks in order to decide what to do. Also note the Service filter here that includes AUTOSCALING, EC2 and VPC. This is important because if you are not using AWS S3 -for example- you don’t want to send out alerts if this service won’t impact you directly.</p>
<h1 id="Enterprise-Level-Services"><a href="#Enterprise-Level-Services" class="headerlink" title="Enterprise-Level Services"></a>Enterprise-Level Services</h1><p><strong>Instructor: Carlos Rivas</strong></p>
<h1 id="Enterprise-Level-Services-1"><a href="#Enterprise-Level-Services-1" class="headerlink" title="Enterprise-Level Services"></a>Enterprise-Level Services</h1><p>The Health Dashboard is quite useful as it is, specially for those using AWS Organizations. However, if you also subscribe to a Business or Enterprise level support plan, AWS takes it up a notch by providing access to the Heath API.</p>
<p>This enables you to perform lots of additional health-related tasks such as integration with 3rd-party applications, such as: Creating JIRA Tickets, sending notifications to Slack or MS Teams and more.</p>
<p>One feature that I find quite interesting is to be able to use the Open-Source tool called AHA – AWS Health Aware. This tool, created by AWS, depends on the Health API.</p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/mt/aws-health-aware-customize-aws-health-alerts-for-organizational-and-personal-aws-accounts/">https://aws.amazon.com/blogs/mt/aws-health-aware-customize-aws-health-alerts-for-organizational-and-personal-aws-accounts/</a></p>
<p>This tool will perform the monitoring for you and provide you with integrations and notifications of Events that are relevant to your accounts and overall AWS footprint.</p>
<p><em><img src="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid2-992c4663-2ed4-4f5d-89d1-33e7cfcd817a.png" alt="alt"><br>Image from</em> <a target="_blank" rel="noopener" href="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2021/09/04/aha-arch-single-region-1.png"><em>https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2021/09/04/aha-arch-single-region-1.png</em></a><em>.</em></p>
<h1 id="Bills-and-Cost-Drivers"><a href="#Bills-and-Cost-Drivers" class="headerlink" title="Bills and Cost Drivers"></a>Bills and Cost Drivers</h1><p>Welcome to the first live demo. As you can see here we are in the AWS Management Console. The most fundamental part is to get a good overview of the environment. Therefore you have to get a broad view of all the services, usage, and expenses from the past, and the present.</p>
<p>To set a base for our analysis, we first need to get some numbers. For this we will be using the billing dashboard which is also our main platform for gathering information on expenses. We will deal a lot with the billing and cost management dashboard alongside but the very first thing I like to investigate when I start with an unfamiliar account or environment is to check the last bills.</p>
<p>So to get there, you can either type bill here in the search field to get straight to the billing service or you can just go here over the menu to the billing dashboard. Here, you get a good first overview what is going on in your account. What are the most expensive services? How much have you spent so far? And what is the forecast? The forecast to the current month. But we will talk about this later.</p>
<p>So first thing here is the bill section here on the left side. I wanna get to overview from the last month, how much we spent last month and what were the biggest cost drivers. So as we are currently in January 2021, I go one month back to December 2020, here from the dropdown menu. And as you can see, I have a total amount, how much we spent last month.</p>
<p>I have like different information about taxes and payment summaries. And I can also get a CSV file from my whole bill or just printed as a PDF. So as you can see here we are in a master payer account, that means we’re using AWS Organizations which enables us to link other accounts to this account.</p>
<p>So all the costs that are caused by the under linked accounts are covered by this account that are collected here. You can see there are many accounts but let’s focus on this overview here. So what I wanna get here is the first overview. What are the most expensive services, or like what are the biggest cost drivers.</p>
<p>You can see we’re spending like small amounts, like compared to the total we’re spending small amounts on like different services. For example, here $34 for the API Gateway or 91 for CloudFront. As this is like just like a very very small percentage of our total amount, it would make no sense or like almost no sense to dive into deeper analysis here for CloudFront. It just would make no sense because like the amounts are here so small and this makes also like our whole analysis much easier because we know the only thing we have to focus is Elastic Compute Cloud because this is the biggest cost driver in this case. And when we go here, we can see, okay, we’re using like quite a lot of regions here.</p>
<p>So my first question would be, why do we need so many regions? And as you probably know, the regions in Asia are way more expensive than for example, the regions in Northern Virginia. So my first question would be, why do we have all these different regions? And as you can see this is like just my thinking process, how I would approach such an analysis or such an optimization process but we will talk about this later in detail. Let’s go through the next section.</p>
<h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p>AWS credits are applied to bills to help cover costs that are associated with eligible services. They are applied until they are exhausted or they expire. With credits AWS introduced a sort of reward system for particularly active users and developers. You can use them instead of spending money on certain services.</p>
<p>So you may ask yourself, “How do I get credits?” So, there are multiple ways. For example, developing and publishing a skill for Alexa or by attending webinars and events. As a startup, you can use the AWS Activate program or through the AWS Credit Program for Nonprofits.</p>
<p>In my case, let’s jump here to the credits section, I did a few AWS certifications, and for that I was rewarded by AWS with two $300 credit vouchers which is pretty cool. I used them here for my private account to play around with a few services that I was interested in and as you can see here, you can see a list of which these credits can be applied to. And there are almost all services that are available for AWS. And what’s quite funny is, the AWS cost explorer that can also cost money when you are using the API is not listed here. So, for the billing services and the cost services you can’t use these credits.</p>
<h1 id="Cost-Explorer"><a href="#Cost-Explorer" class="headerlink" title="Cost Explorer"></a>Cost Explorer</h1><p>In this section, we will take a look on how to use the AWS Cost Explorer, one of the most essential tools for our Cloud Financial Management within AWS. Moreover, the Cost Explorer is the very main tool for you to gather information and analyze all the costs and expenses in your environment. So let me introduce you to the tool itself.</p>
<p>To get there, you can go over to your Billing Dashboard. From here, go to the Cost Explorer, and just launch it. See, you’re getting guided to a whole new menu where you can start to analyze your costs. The Cost Explorer is a built-in tool that lets you get deeper insights into the costs and usage of your cloud environment.</p>
<p>With the help of the Cost Explorer, you will be able to identify trends, hunt down the biggest cost drivers and detect anomalies. And the best part, the Cost Explorer, in its base variation, does not cost anything. You can use it for free. However, take note that there can be additional costs when you use tools to request the API of the Cost Explorer.</p>
<p>Every API request costs about one cent, which could drive up your costs very fast when you fire up a lot of API requests. With the Cost Explorer, you can visualize your usage patterns over time and identify underlying cost drivers.</p>
<p>What we see here is the very first view that you get when you open the Cost Explorer. It is grouped by the different services that you use. You get a six-month overview, and the visualization type is bar, which means that the costs are stacked to each other in these kind of bars here. You can also change this to stacks or to lines, what doesn’t make too much sense in this case. So let’s stick to the bar view.</p>
<p>The legend here below shows you which service is illustrated by which color. In this example, we can see over the last six months our EC2 usage has increased quite a lot. If you scroll down a little bit you can see the detailed data table. Below the Cost Explorer here is the data table, which provides a deeper insight into the data seen in your chart. And you can also export this as a CSV file for further processing with a table tool like Excel if you wanna make forecasts or like further analysis. </p>
<p>You can also like, like I said we have the last six months here, you can also like scroll vertically to see all the services and cost drivers and you can scroll horizontally to see each month. It’s basically just a table.</p>
<p>You can choose the exact timeframe you want to see within Cost Explorer. By default, it’s on six months. So you can either select day by day that you wanna see or you can choose to auto select here to see for example like just last seven days, current month, three months, six months, one year, months to date, year to date, or three months forecast or 12 months forecast.</p>
<p>Let’s go for the last seven days here. So we can see how much we spend for the day different services day by day. We can also delete the group by filter here and get like monthly view. It makes no sense because we wanna see days. So you’re gonna select daily here and then we can see how much we spend day by day, if there’s, for example, an anomaly. We can see, oh wow, like way more than the other days. Why is that?</p>
<p>You can see we are here January 1st the first day of the month. And on the first day of the months, you usually have to pay for tags, reserved instances, and all that stuff that incurred like more costs than over the other days. And that could be the explanation for this anomally here.</p>
<p>And you may have noticed you can also go from a daily to an hourly view, but it is deactivated. I can’t select it. Why is that? The reason for that is super easy. You have to pay for the hourly view because you need like much more data records. So you have to enable it first. If you really wanna go in that much detail. I will show you how you can do this.</p>
<p>Hourly view, you have to go to preferences. And this is the point we are looking for, the hourly and resource level data. We can select this and save it. Here you can also see the cost that would occur. And we can see here one cent per 1000 usage records per month.</p>
<p>It depends how much EC2 instances because this just applies to EC2. It depends how much EC2 instances you’re running. Of course, the more instances you’re running the more usage records you are going to record and the more costs you incur, but 1 cent per 1000 records, in this case, it’s not that much. </p>
<p>So we wanna save it here, go back to the Cost Explorer and we can see, oh, the hourly view is available. We can only see last 14 days of usage was the hourly view enabled. In this case because I just enabled it, I won’t see anything because the data needs to be recorded first. So it will take a minimum of four days for Cost Explorer to collect enough data that the hourly view is available here.</p>
<p>About data grouping and filter. We started with the group function, with the grouped by functionality, you can segment your data based on a particular cost or usage. For example, you can choose a region to see in what region you spent the most money on which regions you spent money.</p>
<p>Let’s go with a bar so we can see, let’s take December. We spent the most money in Ireland followed by others because others mean we have like a lot of data here. Well, let’s go to December. We’ve used many different regions here and all the regions that we use that are not shown here are combined into the others bar. If you want to stripe your chart down to make it more granular, you should use the filter function located on the right.</p>
<p>Using the filter panel. You can refine your data set to include or exclude specific filtering dimensions and values. For example, let’s say we wanna only see costs that occurred in Frankfurt. We go to the filter section here search for the Frankfurt region, select it, applied it, then we can just see the costs that applied for the Frankfurt region. And we can use like many combinations here to dive deep within our costs.</p>
<p>For example, if we just wanna see the costs for our EC2 instances, just filter here for EC2, for our compute instances. Nothing else just compute instances.</p>
<p>So just our literal service and we filter it by instance type and then we can see what instances occurred the most. So for example, here, we have a G4DN which is like a graphical, a big graphical instance that is quite expensive in this case.</p>
<p>As you can see here on the right, there are quite a lot of filters that you can use and no worries, we won’t look at all of them, but some of them are quite interesting. Let’s for example talk about usage type, usage types are the unit that each service uses to measure the usage of a specific type of resource.</p>
<p>For example, if I wanna know how long my t2 micro instances we are running in the past, I would just type in here, t2 micro, oops micro, select a filter here. And I can see my t2 micro instances were running 370 hours in July and about 200 hours in August. And you can use this for many other services as well.</p>
<p>Usage type groups also is pretty cool, these are the kind of type of filters that collect a specific category of usage type filter and put it into one. For example, if I wanna know how long all my EC2 instances were running in the past, I would just search for EC2 running hours and I could see the amount all my EC2 instances were running over the last six months.</p>
<p>Another one that is quite useful if you wanna go in more detail is the filter for API operations. So let’s take S3 as an example, we can see here. We don’t have that much S3 costs here, just in December $188. But if I wanna know more about these costs, how these costs are structured, I can look for specific API operations that apply to S3.</p>
<p>For example, if I want to know how much my costs were for like reading files from my S3 three buckets, I would look up GetObject because this is the API operation for reading objects from S3. I apply this filter here and it can see the costs for GetObject from S3. That’s it, that’s how you’re gonna use API operations.</p>
<p>As you can imagine, there are a lot of API operations that are used within AWS and I can highly recommend to have a look at API reference documentation for each service because you can see here, this is like just for S3, all the API operations that you can use for S3. And just by looking at them, you can, for most of them, you can already like kind of guess what they’re doing. Like for example, here, create object, create bucket and you can like just look forward for the API operation that fits for your kind of analysis.</p>
<p>A little bit deeper into Cost Explorer and the advanced options that can also be quite useful. For example, show only untagged resources. This is a pretty important filter because we talk about tags already a lot and little spoiler here we will talk about them way more because this is such a super, super, super important topic. And if you set this filter here you will see all the resources that have no tags attached, super important to see that.</p>
<p>What is also quite cool is the show costs as here. The unblended filter here will be the best for the vast majority of AWS customers. This is the cost dataset presented you on the bill page like for the bills and for the invoices. And it’s also the default option for analyzing costs using AWS Cost Explorer or setting the custom budgets using AWS budgets.</p>
<p>The unblended costs represent your usage costs on the day that they are charged to you or in finance terms they represent your costs on a cash basis of accounting. For most of you this is the only data set that you will ever need. Okay?</p>
<p>Amortized costs are also quite interesting because the amortized cost is useful in cases in which it doesn’t make sense to view your costs on the day that they were charged or as many of finance owners say it’s useful to view costs on an actual basis rather than a cash basis.</p>
<p>This cost dataset is especially useful for those of you who have purchased AWS or reservations such as reserved instances or savings plans. Savings plans and reservations often have upfront or recurring monthly fees associated with them.</p>
<p>Recurrent fees for reservations are charged on the first day of the month that can lead to a spike on one day if you’re using unblended costs as your cost dataset. When you toggle over to amortized costs, these recurring costs, as well as any upfront costs, are distributed evenly across the month.</p>
<p>Armotised costs are a powerful tool if you seek to gain insight into the effective daily costs associated with your reservation portfolio, or when you are looking for an easy way to normalize costs and usage information when operating at scale.</p>
<p>And then there are also like, two more that can be quite helpful. And these are the net unblended costs and the net amortized costs. These are basically the same as the two that I just explained here but they also include discounts like the reserved instance volume discounts. Like these discounts are calculated into these costs.</p>
<h1 id="Reports"><a href="#Reports" class="headerlink" title="Reports"></a>Reports</h1><p>Let’s talk about reports. Reports can be super, super, super useful. For example, in this case, I build a specific view for EC2 instances running in Frankfurt, in the EU region on a monthly base, like on a base for December, in this case. And because I wanna use this view more often, I can save it as a report. And to do this, I just click here on save as. I give this one a title, EU-EC2. I save it, and that’s it.</p>
<p>By clicking here, on recent reports, I can see this report. Or I can just go under report overview and see all the reports that I have here. The reports with a lock here are default AWS reports, and the one without a lock are my own reports.</p>
<p>Some of the AWS reports can be pretty useful. For example, what I use quite a lot is the RI utilization and coverage, or the savings plan utilization and coverage, that basically shows you how effective or efficient your reservations are.</p>
<h1 id="Cost-and-Usage-Reports"><a href="#Cost-and-Usage-Reports" class="headerlink" title="Cost and Usage Reports"></a>Cost and Usage Reports</h1><p>The AWS Cost and Usage Reports or in short the CUR. The CUR is basically the most important thing to capture your AWS billing data. And the CUR is a pretty complex CSV file that stores all details about your cost and usage data of all AWS resources.</p>
<p>Enabling the CUR is super important because it’s the most granular and detailed mechanism to collect data for AWS costs and usage. It offers historical by-the-hour data that can offer clarity on trends and lead to a more accurate data-driven insight. And there’s no looking back. Until the CUR is enabled, you’re losing valuable data about your usage that is older than 12 months.</p>
<p>The CUR can get really big and in large corporations, it can easily get beyond five gigabyte and more with millions over millions of lines. So let’s see how to enable them.</p>
<p>When we are here in the AWS Management Console, we click here on the top menu on the billing dashboard and you can see here on the left menu for the Cost and Usage Report. By default it’s disabled so you need to enable it, enable it first and create a report. We give it a name. Let’s call it test. I would advise you to include the resource IDs because then, every resource get a unique resource ID.</p>
<p>You can enable the automatic refresh. Click Next and then you need to choose an S3 bucket where you put the file. This can either be existing an S3 or a new one. Then we set a path, a prefix pass task costs. You can select the time granularity here. Of course, the more granular the data are, the more data you’re going to produce.</p>
<p>We can create new report versions or we can override the existing ones and we can choose what kind of data integration we need.</p>
<p>And a little side note here maybe. You can export the files for Redshift or QuickSight usage. This will change the output format of the file to be readable for either Athena or Redshift and QuickSight. See, Athena is Parquet and Redshift or QuickSight is the CSV file that comes in a, in a zip file.</p>
<p>With Athena, Athena is a serverless service that allows you to analyze the data stored directly in Amazon S3 using standard SQL. And for that you need, as I just mentioned, the Parquet file While with Redshift and QuickSight, you can manage to see a SWI file as you would do it also like for example, for Excel.</p>
<p>Redshift is a so-called data warehouse service which you would use for querying big data sets with like multiple gigabytes or even up to petabytes. It can help you take wide insights of your own environment and for customers on a very large scale. And QuickSight is a business intelligence service that can combine data from literally any source into a dashboard. It helps you to visualize for any type of audience and it is much more visually driven than Athena or Redshift.</p>
<p>After you set up all these options here, you can click on Next and that’s basically it. You have now configured your Cost and Usage Report. But be aware that it may take up to 24 hours for the first report to be delivered. Also, expect some costs from S3 for storing the CUR data in your S3 bucket. But these costs are like very, very low. Maybe like, I don’t know, a few dollars per months or per year. Depends on how big your file gets of course.</p>
<h1 id="Budgets"><a href="#Budgets" class="headerlink" title="Budgets"></a>Budgets</h1><p>In this section, we will learn how to create a budget, to help manage running expenses. Budgets allow the user to get notified when costs or usage exceed a certain predefined amount. So let’s have look how to set up automatic notifications and actions with AWS Budgets.</p>
<p>So we get there by clicking here on the top menu on the billing dashboard. Here on the left, we have budgets, we click on it, and we can see that there is already a budget predefined here. I set this up in the past to get notified if my monthly budget threshold gets over $150 but let’s create a new budget.</p>
<p>So we can select four different kinds of budgets in this case or like in general. We have cost budgets based on actual costs, usage budgets based on usage like ours and budgets for reserved instances and savings plans. So let’s start with the cost budget.</p>
<p>So first we need to give it a name and we have to select a period in that we want to be notified. We will keep it here on a monthly period and we can also choose if this is a recurring budget or an expiring budget. Does that goes, for example, like just till April but we will stay with a recurrent budget.</p>
<p>So you can either choose if you want to set a fixed budget. Like for example, last month I had costs of $31. I can set this to $35 ‘cause this is quite close. And if I would like reach this threshold here I would get notified. You can also set a monthly budget planning.</p>
<p>For example, let’s say you have a snowboard rental and you know, there won’t be much users on your platform in the warmer month, like for example, in April till October. So you could set the budget here to like just 100 bucks. But you know, in the cooler months when there’s actually actually snow, you will have much, much more users. So you could set a budget here to 1000 bucks but let’s keep it simple and go with a fixed budget.</p>
<p>You have fear like also many other photos that you also know from the cost explorer demo. The last demo that I just showed you to get more information about the costs you had in the previous months. But let’s configure the thresholds.</p>
<p>You can define your budget thresholds and you can set it either to the actual costs like the cost that like actually occurred to a percentage, like an alert threshold for example, like 80%. So in this case, you would get an alert if 80% of the budget that we just defined in the last step, if we reach this threshold. That will be $28 in this case. And you could also set it based on forecast at cost but this is getting too complex.</p>
<p>Let’s keep it with the actual cost. So here, can you set up the notifications. I can like type in here, my email address and whenever I would reach the threshold I would get an email an alarm that I reached this threshold. And since October 2020, it is also possible to set different kinds of triggers for actions like budget actions. These are based either on identity and access management policies, service control policies or you can also target running instances like EC2 or RDS.</p>
<p>For example, you can choose to apply a custom denied EC2, run instance IAM policy to a user, to a group or to a role in your account once your monthly budget for EC2 has been exceeded. With the same budget threshold, you can configure a second action that targets specific EC2 instances, using a particular region. You can choose to execute actions automatically or make use of a workflow approval process before AWS Budgets execute a request on your behalf.</p>
<p>It’s possible to set up five budget thresholds with up to 10 actions for each threshold. IAM and SEP action type reset at the beginning of each budgeted period. Like in our case, monthly while actions target at a specific EC2 or RDS running instances will not reset.</p>
<p>So we’ve clicked here on the budget actions we activated it, and now we can choose an IAM role that allows budget actions to actually do something with the instances that we are going to define here. So let’s just take this open access role that have defined earlier and we can also choose what should happen.</p>
<p>We can say here if our threshold reaches the budget that we just defined, just stop all EC2 or RDS instances. This is of course, like quite radical step but if you are on a budget, well, you have to do what you have to do, right? So when you can choose if you want to stop EC2 or RDS instances, and you can also select a specific region where this should happen. And if I would click here on “Confirm budget,” the budget would be set. And if the threshold is reached, I would get an email and my EC2 instances would be stepped.</p>
<h1 id="Introduction-to-AWS-Budgets"><a href="#Introduction-to-AWS-Budgets" class="headerlink" title="Introduction to AWS Budgets"></a>Introduction to AWS Budgets</h1><p>Before the cloud, companies often had a fixed procurement process. Companies signed contracts upfront and understood how engineering workloads mapped to software and hardware. Because that process was so well understood, the costs associated with it were understood as well - which meant it was easier to track and control costs. </p>
<p>Now, with cloud computing, costs are variable. With variable usage, you gain speed - you can move quicker and procure the hardware and software you need faster. However, it’s now more difficult to understand the costs associated with that procurement. Often, it requires a change in the procurement process, which means application teams and finance teams need to work better together to determine how to improve planning and control costs. </p>
<p>And to do that, these teams need three things: </p>
<ol>
<li>They need to track AWS usage and costs, set appropriate budgets and receive alerts if they’re exceeding those budgets </li>
<li>They need to provide reports to business leaders and engineering managers to better inform future purchasing decisions and</li>
<li>they not only need to see this information, but they also need to take action and automate responses when they do exceed their budget</li>
</ol>
<p>This is where AWS Budgets comes into play. AWS Budgets has tools that map to each of these requirements. For tracking AWS usage and costs, or Savings Plan and Reserved Instance coverage, you can create a Budget.</p>
<p>For business reporting, you can use AWS Budgets Reports to disseminate information to the right people. And for taking action, you can use AWS Budgets Actions to automate responses if you go over your budget. </p>
<p>Let’s see how each of these tools work together at a high level. You’ll first define your budget, by specifying </p>
<ul>
<li>what you want to track, this could be cost - or how much you’re spending, service usage - how much you’re using, or coverage and utilization for Savings plans and Reserved Instances - are you getting the most out of your reservations </li>
<li>Then you will determine your budget amount, </li>
<li>and last, provide the scope of what this budget applies to - does it only apply to a particular project or service or does it apply to all resources in your account?</li>
</ul>
<p>For example, you can specify a cost budget with a $100 monthly spend as your budget amount that applies to all services in your account. </p>
<p>Then you configure an alert, by specifying a threshold. This threshold is where you specify when you want to be notified. For example, you may want to be notified once you spend 80% of your $100 budget. Once that threshold is reached, the alert will notify you through your choice of email, SNS topic or AWS Chatbot notification. </p>
<p>You can optionally also attach a Budget action to this alert. You can configure one of three automated actions: </p>
<ul>
<li>you can change IAM permissions, </li>
<li>change AWS Organizations permissions, </li>
<li>or stop EC2 or RDS instances.</li>
</ul>
<p>So going back to the previous example, if your alert threshold is met after you’ve spent 80% of your $100 budget, it will not only notify you but also trigger the action you selected automatically or with your approval. </p>
<p>Finally, to get a full report on all your budgets and their status, you can create a budget report and send it out to leadership or other interested parties. This will give them a high-level overview of the status of all budgets and enable them to plan for the future based on this data.</p>
<h1 id="Creating-a-Budget-Demo"><a href="#Creating-a-Budget-Demo" class="headerlink" title="Creating a Budget Demo"></a>Creating a Budget Demo</h1><p>In this video, I’ll create a cost budget to monitor the costs in my AWS account. To do this, I’ll search for Budgets in the AWS console, and click Create Budget. </p>
<p>The first thing I’ll select is the type of budget, I have four options: cost, usage, savings plans and Reserved Instances. I’ll select a cost budget.</p>
<p>From there, you can see my screen has been split. On one side, I’m selecting the options for my budget, and on the other side, it generated a chart from AWS Cost Explorer so I can view the historical cost data in my account. This account is fairly new, only a few months old, so it only has data from the past few months and you can see I generally stick around the $15 range monthly. </p>
<p>Using this data, I can gain a solid understanding of my costs and fill out the rest of my information to create my budget. First, I’ll choose a budget name, I’ll call it TakeMyFifteenDollars. Then I’ll select the time period. My options are daily, monthly, quarterly, and annually. I’ll choose monthly. </p>
<p>Then I can choose to have this budget set to recur after my budget period of a month, or I can have it expire after a certain period of time that I choose. I want this to be ongoing, so I’ll select recurring. </p>
<p>Then I select my threshold. I can either choose a fixed amount, such as $15. Or I can select a planned amount for each month in my budget period, in this case, a year. I can input these values manually, or I can let AWS calculate them based on a starting budget and a percentage of budget growth that I’m expecting.</p>
<p>So for example, if I own a consulting company, and I know the beginning of the year is typically tight budget wise, I can start with a low number - let’s say $100. And let’s say I know I’m going to grow at least 1% every month. AWS will use this information to fill in the data for my period of time, starting with $100 and ending with a compounded budget that accounts for my monthly 1% growth - in this case $111.57.</p>
<p>However, there may be times when you know your spending patterns will change over time, but you don’t know the exact percentage increase. So if you want to avoid having to update and maintain your budget yourself, you can choose the auto-adjusting option. </p>
<p>This, as the name suggests, will adjust your budget automatically based on spending patterns over a time range that you specify. You can choose to base your budget on the forecast for the current month, your bill last month, your average spend over 6 months, or a custom time period. </p>
<p>Since my account is new and I only have data for the past four months, I’m going to use the custom time period and select four months and click apply. Using this, it generates a budget amount for me - in this case it recommends $14 as my budget amount. </p>
<p>If AWS chooses to increase this budget threshold for me, it will notify me through email that it did so. And I can always make updates and edit this budget if I need. </p>
<p>Moving on to the budget scope, I can either choose to track costs from any services or filter based on specific resource attributes. With filters, you can get pretty advanced with how you configure your budgets. For example, EC2 instances for my development environment are often the most expensive part of my bill, so I might choose to add a filter to set a budget for my EC2 instances that are tagged with the tag value “dev”. </p>
<p>However, since I want to monitor costs for my entire account, I’ll choose all services. As far as advanced options, you can choose how to aggregate your costs for your budget. You can choose between unblended, blended, or amortized costs. Additionally, you can also specify whether your budget includes credits, discounts, taxes and more.  </p>
<p>I’m going to leave these options on the default settings and click next. From there, I can create my alert threshold where I can specify a percentage or an absolute value to trigger my alert. For example, I can choose to get notified when I reach the $10 value, or if I’ve spent 80% of my budget. Then I can choose if this is based on actual spend or forecasted. I’ll choose forecasted. If I’m forecasted to spend above 80% of my budget, it will notify me and then I can make changes before I’ve actually spent that money. </p>
<p>Then, I can notify myself through email, or choose to integrate Amazon SNS or Amazon Chatbot alerts. </p>
<p>I can also have multiple alert thresholds to receive additional notifications to have a better pulse on my spending at different times. I’ll leave it at one alert, click next, ignore the Budget actions section for this video, and then I can finally click create budget. And I’m done. From there, AWS will monitor all activity in my account, and send me an email notification if I’m forecasted to spend 80% of my $15 budget.</p>
<h1 id="Budget-Actions-Demo"><a href="#Budget-Actions-Demo" class="headerlink" title="Budget Actions Demo"></a>Budget Actions Demo</h1><p>While Budget alerts are helpful in terms of providing information and visibility, it’s often not enough to solve the spending problem. Typically, you will need to follow that notification with some action. These actions can be manual, such as sending out angry emails to users of your AWS accounts telling them to shut down unneeded resources. Or you can automate specific actions using Budget Actions. </p>
<p>There are three types of automated actions you can take once your budget alert is triggered:</p>
<ol>
<li>The first is IAM policies. With this action, you can choose to change the permissions of users and roles in your account. For example, once the alert is triggered, you may choose to decrease the level of permissions of your users or roles, by changing their policies to “read only policies” until you can figure out what’s going on with the budget. </li>
<li>The second is through Service Control Policies. This is a similar action that can help you change permissions at the AWS Organizations level or Organizational Unit level instead. For example, say your sandbox accounts have reached 80% of their budget, you can choose to limit the sandbox accounts permissions until resources are shut down. </li>
<li>And the third is by stopping EC2 and RDS instances by selecting the instances you want to stop once an alert threshold is crossed.</li>
</ol>
<p>For each of these actions, you can choose to apply the action automatically or through a manual approval process. If you choose the manual approval workflow, once your alert threshold has been reached, you will receive an email letting you know you have an action waiting for you. You can then login to the console and execute the action. If you choose to apply the action automatically, it will not wait for your approval and the action will be applied immediately. </p>
<p>So let’s say I’ve already started the process of creating a new budget, and I’ve already created an alert. Now I need to add on an automated response for this alert. To do this, I’ll attach this new action to the alert I’ve already set up by clicking “add action”. </p>
<p>From there, I’ll select an IAM role with appropriate permissions to run an action. This role uses an AWS-managed policy that has appropriate permissions to stop instances, and change permissions.</p>
<p>And then I can select which action to take. I’m going to choose to stop EC2 instances, as my account is just a sandbox and it’s the fastest way for me to save on cost. From here, I’ll choose the Region, which is us-east-1, and then I’ll select the instance I want to shut down. </p>
<p>Next, I can choose if I want this to happen automatically or go through a manual approval workflow. I’m going to choose the manual approval process, as I want to be extra safe and not shut down an instance I might need in the future. </p>
<p>And then I’ll click create budget. Now we’re finally done, but I’m going to wait some time to see what happens when my budget threshold has been exceeded. </p>
<p>When my alert is triggered, I get two notifications in my email. The first is a notification telling me that my budget has been exceeded. The second notification lets me know that an action is waiting for me in the console. Now I can go into the console to execute that action. Click on actions that require my approval. Scroll down to the actions section, and click the checkbox. And then click run action. Once I do that, I can go to the EC2 console, and check on my instance to see if it is in the stopped state. </p>
<p>Looks like it is, so now we know my action worked.</p>
<h1 id="Budget-Reports-Demo"><a href="#Budget-Reports-Demo" class="headerlink" title="Budget Reports Demo"></a>Budget Reports Demo</h1><p>Visibility into the performance of your budgets enables you to better plan and forecast for the future, and determine areas where the budget needs to increase or decrease. To get this visibility, you can use AWS Budget Reports. </p>
<p>Reports provide a high-level overview of the status of all your budgets and can help inform your decisions.</p>
<p>To create a budget report, you can login to the AWS console. Find the AWS Budgets service, and click on budget reports on the left-hand side. From here, you can click “create report” and then select the budgets you’d like to create a report for. </p>
<p>After that, you can select the frequency in which you’d like to receive these reports, whether that’s daily, weekly, or monthly. Keep in mind that each report costs $0.01. Then, you can specify the email addresses that you’d like to receive the reports. You can put up to 50 email addresses in this box but since I don’t know 50 people, I’ll just specify my own email address, <a href="mailto:alana.layton@cloudacademy.com">alana.layton@cloudacademy.com</a>.</p>
<p>Last, you can name your report and then click create report. Once you create this report, your reports will then be delivered to the email addresses you specified at the interval you selected.</p>
<p>Now let’s take a look at one of these emails and see what it looks like. This is an example of a report I created for two budgets in my account. The first budget called “CA Budget” tracks my daily spend. As you can see, I typically spend around $0.49 daily, and my current spend has exceeded that, at $.61. Additionally I have another budget called “EC2 Usage” that tracks my EC2 instance running hours. I budgeted 30 instance running hours, and currently I’m above that at 48 hours. </p>
<p>Now I know the status of each of my budgets. And it’s clear that I need to make some changes, as I have greatly exceeded my expected cost and usage. So, I’m off to go stop a few of my EC2 instances, which will reduce my running hours and my cost and hopefully my report will look better tomorrow. </p>
<p>See you next time! </p>
<h1 id="AWS-Cost-Anomaly-Detection"><a href="#AWS-Cost-Anomaly-Detection" class="headerlink" title="AWS Cost Anomaly Detection"></a>AWS Cost Anomaly Detection</h1><p>Everyone has a story about an AWS bill - and usually, it takes hearing just one of these stories for you to become obsessive about tracking your AWS spend. And while AWS budgets help with this paranoia, it’s not a perfect science. </p>
<p>There may be times when you exceed your budget, but it’s because of business growth. You’re using more and racking up additional costs to better serve your customers - which is a good thing. Or there may be times where you’ve set a budget and you have a spike that’s not normal for your business but it remains under the threshold, so you don’t get alerted…which isn’t a good thing. </p>
<p>This leads us to two fundamental truths: </p>
<ol>
<li>Every business hates surprise spikes in cost and, </li>
<li>Every business contextualizes these spikes in costs and determines if they are “good” or “bad” based on their own unique spending patterns</li>
</ol>
<p>Analyzing these cost spikes used to be a very manual process but now there are tools that can help us automate it, and one of those is called AWS Cost Anomaly Detection. </p>
<p>This service helps you gain an understanding of what is normal or not normal in terms of spending for your AWS accounts. And any time you have a random spike in spending that is deemed “not normal”, you can not only be alerted of that spike, but you can also investigate why it happened.</p>
<p>Like AWS Budgets, the first step in using Cost Anomaly Detection is to create an alert - which is called a cost monitor in this service. You can choose to evaluate your costs based on AWS service, or by linked accounts, cost allocation tags, or cost categories. </p>
<p>This choice is dependent on how you track your costs in your cloud environment. Lets use an example - say you’re in an organization that segments cost by project. You can do this in many different ways in AWS. For example, you may use AWS accounts to segment costs for each project or you may use tags, and tag resources based on a particular project or you may even create your own custom resource groupings using cost categories. Or perhaps you’re more interested in segmenting spend by AWS service, to monitor each service spend individually for your projects instead. So depending on how you filter your spend, you’ll choose the corresponding monitor to match. </p>
<p>When you create a cost monitor, you have to specify a threshold which will determine when the service sends you a notification. This threshold is defined as the difference between actual spend and your normal spend pattern. For example, let’s say you set your threshold at $25. And your normal spend is $50. When your daily spend reaches $75, which is $25 past your normal $50 spend, then you’ll be alerted of the anomaly. </p>
<p>Keep in mind this threshold only defines when to alert you and does not determine what an anomaly looks like for your account. And actually, you don’t have to define that anywhere - the service will define what an anomaly looks like based on your own spending patterns by using machine learning. </p>
<p>From there, you can choose to get notified as soon as the anomaly is detected, or in a daily or weekly summary. Instead of receiving a notification anytime an anomaly is detected, these daily or weekly summaries will consolidate all anomalies that occurred within that day or that week.</p>
<p>Once the anomaly reaches the threshold that you set, you can choose to get notified through email, SNS, or the AWS Chatbot service. </p>
<p>However, occasionally, there may also be times where there is an anomaly in your account but it doesn’t reach the threshold you set to be notified. This is where the detection history section of the Cost Anomaly Detection dashboard comes in handy. You can view your entire history of anomalies.</p>
<p>In detection history, you can inspect each anomaly in further detail. When inspecting an anomaly, you can take several actions, for example: </p>
<ul>
<li>You can view the anomaly in cost explorer to filter out details on a more granular level. </li>
<li>You can view the root cause analysis, which the service identifies as the “best guess” to what may have caused the spike. </li>
<li>And you can also submit assessments of the anomaly to better train the model to better learn your unique patterns of spending.</li>
</ul>
<p>In summary, Cost Anomaly Detection helps detect one time cost spikes and continuous cost increases. This service is technically considered a free service, so consider using this in conjunction with AWS budgets. This combination of services will provide you more visibility into your spending patterns and any strange cost spikes, enabling you to better plan for the future. </p>
<h1 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h1><p>The essence of a cost allocation strategy is the ability to tell how much is spent on which resource on which service. This type of visibility can be best achieved by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/introduction/">tagging</a> every single resource in your cloud. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> enables the user to put tags on every available resource. You can use tags for many things. But for this course, we’re just going to focus on how to use them for cost allocation. So let’s find out what tags actually are. </p>
<p>Tags provide the functionality to define metadata in the form of key and value pairs. These on the other hand are associated with the resources in a cloud account. Let’s have a look at this diagram. In this example, we’re looking at four resources. Don’t mind the details because it doesn’t really matter what kind of resources they are. These are just some key samples. So each of them has a tag, which goes by the key-value environment production. This one is true for every four of them. This means that all of these resources belongs to the production stage of our environment.</p>
<p>The next one distinguishes our resources between the frontend and backend, and basically tells us right away which resources host a front-end service and which hosts a back-end service. Typically, business tags such as cost center, business unit, or project are used to associate AWS costs with traditional financial reporting within an organization. However, a cost allocation report is not static and hence can include any tag. This allows customers to easily link costs to technical or security dimensions, such as specific applications, environments, or compliance programs.</p>
<p>With AWS Cost Explorer and Cost and Usage Report, AWS costs can even be viewed according to tags, providing even more insightful cost visualizations. AWS Cost and Usage report is otherwise known as AWS CUR. Just so you know what the Cost and Usage Report is, I wanted to drop in a short explanation. With the help of AWS Cost and Usage reports, you can track the monthly AWS costs and usage associated with your AWS account. The report includes items for each unique combination of product, usage type, and operation that is used in your AWS environment. It enables you to configure the AWS Cost and Usage report to show only the data that you want, using the AWS Cost and Usage API.</p>
<p>AWS Cost and Usage Reports contain the widest variety of cost and usage data. You can set up the CUR to collect billing data for any given period and push it into an Amazon S3 bucket to store it there for whenever you need it. You can get hourly, daily, or monthly reports. These contain the costs in detail and are sorted by product or resource. If tags are used properly, they are also listed in the report and can provide extra detail to your bill. A report is updated at least once per day and up to three times. They can be stored in an S3 bucket of your choice and be retrieved whenever needed, either manually or by using another service. </p>
<p>After you set up a cost and usage report, you receive the current month’s billing data and daily updates in the same Amazon S3 bucket. The data from the CUR forms the base for a detailed and complete cost analysis. It is often the main part for many business intelligence tools, like Athena and QuickSight, just to name a few. And CUR can also be reached by an API, which you can use for your custom scripts or individual needs.</p>
<p>So, in conclusion, with AWS CUR, you are able to store your report files in Amazon S3 buckets, update the report automatically, up to three times a day, make use of the AWS CUR API for automation or easier management through API calls, and use the CUR for in-depth analysis with business intelligence tools like QuickSight, Athena, and others. That’s about it for the AWS Cost and Usage Reports, so let’s continue. </p>
<p>Keeping cost allocation in mind, the best way to assign business-context details to specific resources is by using tags. Later on in the process, this enables you to carry out a more valuable analysis based on your cost data and facilitates company-specific decision making by a well-evaluated foundation of data. If you take bill analysis into consideration, tags can add business dimension and context to ease the allocation process. </p>
<p>Tags are used to identify which item or resource in your cloud is attributed to each of your business services. So you can always tell exactly which resources are used for which service in your company. Nevertheless, keep in mind that tags are only meaningful to their respective user or a customer. They literally do not have any semantic meaning. You can name them whatever you like and assign a value to them. However, when used correctly, they can help read and analyze your data and even automate your analysis with the right setup.</p>
<p>In AWS, you can manage tags in the service console or accessing the API through AWS CLI, although this limits you to only one resource at a time. If you want to add, edit, or delete tags on multiple resources, it is best to use a service, for example, the AWS Tag editor. Once you have tagged your resources, you can enable Cost Allocation Tags in the Billing and Cost Management sections. We will discuss how to tag and activate the Cost Allocation tags in a minute. One significant thing to note here is that tagging existing resources retroactively is pretty annoying. So make sure to tag your resources from the very beginning.</p>
<p>In the best case, policies can prohibit the deployment of new resources without the appropriate tags. But more about that later. If you want to analyze a cost report after the fact with unlabeled or poorly labeled resources, you will have a hard time understanding the exact usage of each resource, and will likely not be able to identify the exact costs and usage by resource. So, it is advised to start tagging resources as soon as possible and stay consistent with your tagging strategy.</p>
<p>Planning out a tagging strategy or a tagging standard is essential, and the best time to implement one is before a company launches its cloud resources. It’s best to keep tagging simple and easy to grasp. Don’t overdo it for the sake of it. After all, you want to gain visibility, not cause confusion. The best thing to do is to learn about predefined tags and choose the ones suitable for your business. It’s also advisable to adjust your tags to follow your KPIs once you determine them.</p>
<h1 id="Tagging-Best-Practices"><a href="#Tagging-Best-Practices" class="headerlink" title="Tagging Best Practices"></a>Tagging Best Practices</h1><p>We’ve picked a few best practice examples for you to apply for your business or organization. Let’s start with some common tags that are used by most organizations. Of course, these are just some ideas and you need to use tags that fit your business case. Some common examples include Cost Center or Business Unit tag, used to show where resource costs are allocated within the organization, and it also allows correct cost allocation within billing data.</p>
<p>Service&#x2F;Workload name tag. This shows which service the resource belongs to. Resource Owner tag. This is responsible for the resource. Simple Resource Name tag. This is something easier to read and to remember than the default tags. And Environment tag. It determines the cost difference between different environments. For example, dev, test&#x2F;stage, production. Check your cloud and see whether these tags can help you get started with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/tagging/">tagging</a>. Also make sure to check <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> pre-generated tags. They might save you some time.</p>
<p>Now let’s look at some tagging best practices. So, number one, align tags to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/aws-generated-cost-allocation-tags/">cost allocation strategy</a>. Before you start tagging, you should think of a general cost management strategy. Think of tags that help you to track and allocate expenses and make those tags align with your strategy. Next, tag everything. Tag as many resources as possible so that no resource is left untagged. Make this a rule. In fact, you can roll out policies in your cloud environment that will forbid launching resources without tags.</p>
<p>Next, find a purpose for each tag. Think of a certain use case before adding a tag. Otherwise you will have a hard time justifying your tags and you risk running into a mess of baseless tags. That now leads me onto the next point. Limit the number of tags you adopt. Find redundancies and overlapping tags and simplify them. There’s no point in releasing multiple tags that cover the same subject. Look for tags that might logically overlap. See where you might merge them and reduce the number of your overall tags. And keep it manageable. Obviously, the more tags you have, the more tags you have to deal with. Keep the number as low as necessary, but the information value as high as possible.</p>
<p>Next, consistency is key. Use a consistent naming convention. This helps to keep an overview and eases further processing. Giving your tags less abstract names, and instead naming them with descriptive terms also makes them easier to read. Automate tag management. Make use of tools like the AWS tag editor to automate your tagging. Avoid wasting time on repetitive tasks and use automation as much as possible. Set up policies to forbid launching untagged resources. This is an easy way to ensure that no new resources are slipping into your environment without a tag.</p>
<p>And finally, audit and maintain your tags. Make it a habit to review tags from time to time and verify their purpose. Tag maintenance is essential and should involve everyone on the team. So make it a recurring task for everyone and have everyone keep their eyes open for suggestions for improvement.</p>
<h1 id="AWS-Generated-Cost-Allocation-Tags"><a href="#AWS-Generated-Cost-Allocation-Tags" class="headerlink" title="AWS-Generated Cost Allocation Tags"></a>AWS-Generated Cost Allocation Tags</h1><p>The default tags in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> are basically cost allocation tags. They can be activated in the billing section, which we will explain in a minute. Cost allocation tags are special tags that are used by Cost Explorer and other services for allocation and visualization. So they can be explicitly used and displayed in the various views of various services. So take a look at the screenshot. Here we can see the Cost allocation tags section in AWS Billing. We can tell that CostCenter and Name are active and enabled as cost allocation tags, while the others are inactive. These inactive tags were once set for resources that are not in use any longer.</p>
<p>Once the cost allocation tags are activated, the console detects all tag keys used and suggests those for activation as cost allocation tags as well. Otherwise, they won’t show up in Cost Explorer charts. The term User Defined means that a user created these tags and that they are therefore custom tags. AWS generated tags were automatically generated, such as createdBy, the createdBy Amazon WorkSpaces Tag. These were created and applied to support AWS resources for the purpose of cost allocation. They can only be view in the AWS Billing and Cost Management console and reports. They do not appear anywhere else in the AWS console, including the AWS Tag Editor.</p>
<p>Also, note that in this Cost allocation tags section, all the tag keys that are and were used in the account will be shown here. However, if you don’t enable the Cost Allocation Tags option, you will not be able to evaluate certain views in the Cost Explorer and other services. For example, if I want to know what costs have been incurred for all resources with a specific cost allocation tag, then I won’t be able to select or see them.</p>
<p>To prevent you from failing to find your resources with those cost allocation tags, I will show you how to enable this option. Let’s see how you can enable AWS generated Cost allocation tags. First, you need to log in to your Master account as an IAM user with the required permissions. Next, type Billing into the search field and go to the Billing console. Select Cost Allocation Tags on the left side menu. And then, simply click on Activate to enable the tags.</p>
<h1 id="Data-Visualization-Services"><a href="#Data-Visualization-Services" class="headerlink" title="Data Visualization Services"></a>Data Visualization Services</h1><p>Let’s start by introducing the visualization tools and services that we have available to us. AWS provides a suite of tools to help you present and manipulate big data results. Amazon Athena is an interactive query service that makes it easy to analyze data that’s stored in Amazon S3 using standard sequel. Athena is server-less, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena uses Presto with ANSI SQL and works with a variety of standard data formats. They include CSV, JSON, ORC, Avro, and Parquet. </p>
<p>With Athena there is no need for complex ETL jobs to prepare your data for analysis. Now this makes it easy for anyone with SQL skills to quickly analyze large scale data sets. Another benefit is that Athena uses Amazon S3 as it’s underlying data store, so you’re data remains durable and highly available. Amazon Athena is integrated with AWS Glue, so you can use the Glue’s ETO capabilities to transform data or use the Glue data catalog, which is a powerful unification tool. </p>
<p>Now you could look to use this to create a unified MIDI data repository, for example, that could run across a number of data services. Amazon QuickSight makes it easy to build visualizations, perform ad hoc analysis, and quickly get business insides from your data. It has a number of pre-configured reports which take out the undifferentiated heavy lifting of creating visual reports. Let’s think through how QuickSight can be used. Data dashboarding is often a core requirement in business reporting, and a common-use case is business reporting on the data we might have stored in a data store.</p>
<p>Let’s envisage our data warehouse make up new data on a nightly basis from a number of different sources. So we need to ingest and transform that data quickly so the data is ready and consumable in the morning when the CEO and other business users come in and need to generate reports. So we might have a number of transformation jobs that put formatted and clean data into Amazon S3. </p>
<p>Now we use Amazon S3 as our data store, as it is highly durable, and Amazon Redshift can consume this data on multiple threads in parallel from each Redshift node. So data processing will be really fast. It is also for data on Amazon S3 to be consumed by other analytics, tools, or services if we add them. For visualizing analytics, we can use Amazon QuickSight or one of the many partner visualization platforms listed in the marketplace using the ODBC&#x2F;JDBC connection to Amazon Redshift. </p>
<p>Now the benefit of QuickSight is that it’s integrated into our AWS dashboard and our AWS account, and this is where reports and graphs can be viewed by the CEO and his staff. When we create visuals, the style and format of graphs is automatically selected by the QuickSight engine, which saves time and improves the quality of reports and visuals. QuickSight also makes it easy for business teams to create and share interactive graphs and reports as stories, and if we have any additional data sources added in the future, those can just simply be added as Amazon QuickSight database sources. Now once we are in QuickSight we can create visuals and scenes that provide information relevant to different business units or reporting agendas. </p>
<p>QuickSight also enables us to share graphs, reports, or business insights as creative stories. A story is a collection of interactive visuals that can be easily shared with other people. Now at the heart of the QuickSight service is the Spice engine. So QuickSight uses Spice which stands for Super fast Parallel In-memory Calculation Engine, and Amazon has developed this to run natively in AWS. So it has been from the ground up for the AWS cloud. </p>
<p>Now Spice uses a combination of data compression, columnar storage, machine code generation, and in-memory technologies enabled to the latest hardware innovations. Spice automatically replicates data for higher availability, and also enables Amazon QuickSight to support interactive analysis across a wide variety of AWS data sources. Now we don’t need to be an expert in how it all works, but what is does mean is that we can run interactive queries on massive data sets and get really fast results. Spice capacity is allocated by region. So the information displayed is for the currently selected region you have. You can see how much Spice capacity you are using and how much there is overall from the AWS console. </p>
<p>Currently each Amazon QuickSight account receives 10 gigabytes of Spice capacity per paid user, and that is allocated when you log into QuickSight for the first time. This limit will no doubt change over time so do check the current account limits. If space is a concern for your use cases. You get one free user per account, and the Spice capacity is pooled across all users for your Amazon QuickSight account. So each QuickSight account receives one gigabyte of Spice capacity. So if you have four users, say one free and three paid for, you’ll have 31 gigabytes of Spice capacity available. Now that can be utilized by any of the users in the account. </p>
<p>All of your default Spice capacity is allocated to your home region, and the other regions have no Spice capacity unless you choose to purchase some, okay? Now as your usage of QuickSight increases, housekeeping does become important, and you can release purchased spice capacity that you aren’t using to free up capacity. To free up Spice capacity, you delete any unused data sets that you haven’t ported into spice. </p>
<p>Now keep in mind purchasing or releasing Spice capacity only affects the capacity for that currently selected region. You can purchase up to one terabyte of additional Spice capacity per QuickSight account if you need it. If you do find yourself low on Spice Capacity, you can also choose the buy Spice alert that appears on the your data sets and create a data set pages in the console, and if you need more capacity than that, you can submit a limit increase request to AWS support following the AWS service limits instructions. A neat benefit with QuickSight is that it’s very easy to connect QuickSight to data sources. You can upload CSV or Excel files, ingest data from AWS data sources such as Amazon S3, Amazon Redshift, Amazon RDS, or Amazon Aurora, or Amazon Athena, and Amazon Elastic produce, which is Presto, and Apache, and Spark. </p>
<p>Now you can also connect to cloud or on premise databases such as MySQL, Sequel Server, and Postgres, and you can also connect to SAS applications like Sales Force. You can prepare data in any data set to make it more suitable for analysis. You can change field names or add a calculated field. You can also do Joins on database tables using structured query language or SQL. You’ll find it relatively limited if you need to do complex cascades or select into statements or on inner Joins. The Join interface doesn’t let you use any additional SQL statements to refine the data set. </p>
<p>A couple of points to remember, the target of the join has to be a Spice dataset for Joins. For both, datasets have to be based in the same sequel database data source, alright? So you cannot do Joins across two independent data sources. The fields used in the joins cannot be calculated fields. So if you’ve edited a date collect calculation or similar that can’t be part of your Joins statement. If you do need to run a lot of conditional logic then you might want to consider using Amazon Athena as it provides more functional support and flexibility in manipulating data. </p>
<p>You can use calculated fields to use common operators or functions to analyze or transform field data. You can use multiple functions and operators in a calculated field. So for example, you might use the format date function to extract the year from a date field and then the if else function to segment records based on that year. A calculated field has to be from the QuickSight data source, just FYI. A lot of common function types are supported as you can see from this visual on the screen. </p>
<p>Now Amazon QuickSight supports assorted visualizations that facilitate different analytical approaches. To create a visualization you start by selecting the data fields you want to analyze or drag the fields directly onto the visual canvas. We can do a combination of both. QuickSight will automatically select an appropriate visualization to display your data based on the data that you’ve selected. </p>
<p>Now it does this using a proprietary technology called Autograph. Autograph allows QuickSight to select the most appropriate visualizations based on the properties of the data such as cardinality and data type. Pretty clever. The visualization types are chosen to best reveal the data and the relationships in the most effective way. QuickSight is super-intuitive. It selects the display time that best suits the record types you have in your results set. </p>
<p>Now this is a real time saver if you just need to show results quickly. You can alter the visualization type to include fields or views you prefer by adding a field from the field selector. If you add a new view or field it automatically is added to the field types menu which is super-cool. If you add an additional sort field, the graph visual style automatically updates from a bar graph to a line graph to better represent your results set. Brilliant! You can also resort back to the default visualization at any time by clicking the menu option. </p>
<p>Okay, so let’s walk through a demo. So we choose a data set and then we choose create analysis. If we don’t have any data sets yet, we’ll create a new one by choosing new data set. Now at this point you’ll notice there’s an auto-save option up there in the menu bar. Auto-save is on by default when you’re working on an analysis. When it’s on you’re changes are automatically saved every minute or so. I’m not sure exactly what the timing is. When auto-save is off, your changes are not automatically saved, okay? </p>
<p>So that’s useful if you want to try out a different analysis or display style, or perhaps show a certain variation or view without changing your core analysis. The undo feature works when either auto-save mode is on or off. So you can undo or redo any change you make by using undo or redo from the application bar.</p>
<h1 id="AWS-Glue-Data-Catalog-Primer"><a href="#AWS-Glue-Data-Catalog-Primer" class="headerlink" title="AWS Glue Data Catalog Primer"></a>AWS Glue Data Catalog Primer</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="AWS-Glue-Data-Catalog-Primer-1"><a href="#AWS-Glue-Data-Catalog-Primer-1" class="headerlink" title="AWS Glue Data Catalog Primer"></a>AWS Glue Data Catalog Primer</h1><p>AWS Glue historically was only an ETL service. Since then, the service has turned into a suite of data integration tools. Now, AWS Glue is made up of four different services: </p>
<ol>
<li>Glue Data Catalog</li>
<li>Glue Studio</li>
<li>Glue DataBrew, and </li>
<li>Glue Elastic Views. Glue Elastic Views is out of scope for this content, so I won’t be talking about it in this lecture. If you’re interested in Glue Elastic Views, I will link a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-reinvent-2020-aws-glue-elastic-views-1209/">course</a> specifically for that topic.</li>
</ol>
<p>In this lecture, I’ll mainly focus on the Glue Data Catalog aspect of this service.  </p>
<h3 id="AWS-Glue-Data-Catalog"><a href="#AWS-Glue-Data-Catalog" class="headerlink" title="AWS Glue Data Catalog"></a>AWS Glue Data Catalog</h3><p>AWS defines the Glue Data Catalog as a central metadata repository. This means that it stores data about your data. This includes information like data format, data location, and schema. Here’s how it works: </p>
<p>You upload your data to storage like Amazon S3, or a database like Amazon DynamoDB, Amazon Redshift, or Amazon RDS. From there, you can use a Glue Crawler to connect to your data source, parse through your data, and then infer the column name and data type for all of your data. The Crawler does this by using Classifiers, which actually read the data from your storage. You can use built-in Classifiers or custom Classifiers you write to identify your schema. </p>
<p>Once it infers the schema, it will create a new catalog table with information about the schema, the metadata, and where the source data is stored. You can have many tables filled with schema data from multiple sources. These tables are housed in what’s called a database. </p>
<p>Note, that your data still lives in the location where you originally uploaded it, but now you also have a representation of the schema and metadata for that data in the catalog tables. This means your code doesn’t necessarily need to know where the data is stored, and can reference the Data Catalog for this information instead. </p>
<p>That’s it for this one. See you soon!</p>
<h1 id="ETL-with-AWS-Glue-Studio"><a href="#ETL-with-AWS-Glue-Studio" class="headerlink" title="ETL with AWS Glue Studio"></a>ETL with AWS Glue Studio</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="ETL-with-AWS-Glue-Studio-1"><a href="#ETL-with-AWS-Glue-Studio-1" class="headerlink" title="ETL with AWS Glue Studio"></a>ETL with AWS Glue Studio</h1><p>Hello and welcome to this lecture where I’ll be discussing AWS Glue Studio, which is one of the tools available in the AWS Glue ecosystem. AWS Glue Studio is where you create, submit and monitor your ETL jobs. </p>
<p>With AWS Glue Studio, every ETL job consists of at least three things: </p>
<ol>
<li><strong>A data source.</strong> This could be the Data Catalog, or a service like Amazon S3, Amazon Kinesis, Redshift, RDS, DynamoDB or another JDBC source. </li>
<li>Then, you need a <strong>transformation script</strong>. Glue will use the data from your source and process it according to the transformation script you write. You can write these in either Python or Scala. </li>
<li>Last, you need a <strong>target</strong>. Glue will export the output to a target of your choice, such as the Data Catalog, Amazon S3, Redshift or a JDBC source.</li>
</ol>
<p>Let’s look at Glue Studio in the Console. Here I am in the Job dashboard of the service. If I want to create a job, you can see there are many options to do so. However, they are categorized in one of two ways: I can either create a job programmatically or I can use a visual interface. </p>
<p>For example, if I click the visual with a blank canvas option and click create. I can then create graphical relationships between a source, transformation scripts, and a target destination.</p>
<p>Let’s build one quickly. I can use the Data Catalog as my source. For my transformation script, I’ll use a built-in script called Rename Field, that renames a key in my data set to another name. Then, I can output the transformation to an Amazon S3 bucket. I can additionally choose to update my Data Catalog or not. </p>
<p>While this is a pretty simple ETL job, you can create more complex relationships and graphs between services without coding at all, and Glue will generate the Apache Spark code for you behind the scenes. Note, that if you want a true no-code tool for creating ETL jobs, this won’t really provide you with that, as the built-in transformation scripts in Glue Studio are very limited. You only have about 10 options or so here. If you feel comfortable with coding, you can create custom transformation scripts in this interface using Python or Scala as well.</p>
<p>However, there are better places where you can develop your own custom scripts. For example, if I click back, you can see the other options for programmatically creating scripts, such as the </p>
<p>Spark script editor, the Python shell script editor, or the built-in Jupyter Notebook interface to create Python or Scala job scripts.</p>
<p>That’s it for this one - see you next time.</p>
<h1 id="AWS-Glue-DataBrew-vs-Glue-Studio"><a href="#AWS-Glue-DataBrew-vs-Glue-Studio" class="headerlink" title="AWS Glue DataBrew vs. Glue Studio"></a>AWS Glue DataBrew vs. Glue Studio</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="AWS-Glue-DataBrew-vs-Glue-Studio-1"><a href="#AWS-Glue-DataBrew-vs-Glue-Studio-1" class="headerlink" title="AWS Glue DataBrew vs. Glue Studio"></a>AWS Glue DataBrew vs. Glue Studio</h1><p>A few years ago, Glue released another transformation tool called Glue DataBrew. On the surface, DataBrew looks very similar to Glue Studio. So, what is Glue DataBrew?</p>
<p>Glue DataBrew is a true no-code service for transforming data. Here’s how it works: </p>
<p>You first <strong>upload your data</strong>. You can upload it directly to the service, or connect to other data sources like Amazon S3, Amazon Aurora, Amazon Redshift, Glue Data Catalog, or other JDBC Connections. It can additionally connect to AppFlow, Data Exchange and Snowflake. </p>
<p>Once you upload your data, you can preview your data in a visual interface. From there you can <strong>choose from hundreds of built-in transformations</strong>. Some of these transformations include formatting your data, modifying columns, working with duplicate or missing values, encoding data, and more. </p>
<p>Once you apply your transformation, you can <strong>store the output in Amazon S3</strong>. Note that Amazon S3 is the only place you can store your transformed data. </p>
<h3 id="Glue-DataBrew-vs-Glue-Studio"><a href="#Glue-DataBrew-vs-Glue-Studio" class="headerlink" title="Glue DataBrew vs Glue Studio"></a>Glue DataBrew vs Glue Studio</h3><p>So if both of these services provide transformations, function in similar ways, and if Glue Data Studio also provides some no-code options, which service do you use? </p>
<p>Well, there are a four main differences between the two that might help you distinguish when to use each service: </p>
<p>\1. No-Code vs Custom Code </p>
<p>Glue DataBrew is a no-code tool. Unlike Glue Studio, you can’t write your own custom code for transformations even if you wanted to. However, that means that DataBrew provides a lot more options for built-in transformations. DataBrew has over 250+ built-in transformations, while Glue Studio has around 10. These transformations are different as well. Glue Studio built-in transformations focus mostly on ETL, while DataBrew’s transformations mostly prepare data for machine learning.</p>
<h4 id="2-Different-Tools-for-Different-Users"><a href="#2-Different-Tools-for-Different-Users" class="headerlink" title="2. Different Tools for Different Users"></a>2. Different Tools for Different Users</h4><p>These services are meant for different audiences. Glue Studio is meant for ETL engineers and is focused on ETL itself, while Glue DataBrew is mostly for business analysts and data scientists that may not have coding experience. You don’t need specialized expertise to transform data with DataBrew. </p>
<h4 id="3-Programmatic-Creation-of-ETL-Jobs"><a href="#3-Programmatic-Creation-of-ETL-Jobs" class="headerlink" title="3. Programmatic Creation of ETL Jobs"></a>3. Programmatic Creation of ETL Jobs</h4><p>Both services provide a graphical interface for visualizing your transformations. Glue Studio, however, is the only option that provides programmatic opportunities for working with ETL through Jupyter notebooks and shell scripts. </p>
<h4 id="4-Data-Profiling"><a href="#4-Data-Profiling" class="headerlink" title="4. Data Profiling"></a>4. Data Profiling</h4><p>DataBrew has a profiling feature, which enables you to get statistics about your data. For example, with profiling, you can get information about how many rows you have in your data set or how many unique values you have in each column. Glue Studio does not have a data profiling feature. </p>
<p>That’s it for this one - see you next time!</p>
<h1 id="Amazon-EMR-vs-AWS-Glue-for-ETL"><a href="#Amazon-EMR-vs-AWS-Glue-for-ETL" class="headerlink" title="Amazon EMR vs. AWS Glue for ETL"></a>Amazon EMR vs. AWS Glue for ETL</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="Amazon-EMR-vs-AWS-Glue-for-ETL-1"><a href="#Amazon-EMR-vs-AWS-Glue-for-ETL-1" class="headerlink" title="Amazon EMR vs. AWS Glue for ETL"></a>Amazon EMR vs. AWS Glue for ETL</h1><p>In this video, I’ll be comparing Amazon EMR vs AWS Glue for ETL. Before we get deeper into the two services, it’s important to note that Amazon EMR does have multiple deployment options. You can use EMR on EC2, on EKS, or use EMR serverless. In this video, I’ll be focusing mostly on EMR on EC2, with some mentions to EMR serverless. </p>
<p>With that being said, our next fight for the evening is: AWS Glue taking on Amazon EMR. Who will win? </p>
<p>In one corner, we have Amazon EMR, a big data platform that’s designed not only for ETL, but also for machine learning and data analysis. </p>
<p>In the other corner, we have AWS Glue, a data integration service, that provides Glue Studio for ETL. It also includes a Data Catalog, Glue DataBrew for no-code transformations, and Glue Elastic Views. </p>
<p>Let’s look at these two services from three different perspectives</p>
<ol>
<li>Ease of use </li>
<li>Pricing</li>
<li>Limitations</li>
</ol>
<h3 id="Ease-of-Use"><a href="#Ease-of-Use" class="headerlink" title="Ease of Use"></a>Ease of Use</h3><p>Ease of use for any tool in AWS is often inversely related to control. Tools that AWS says are “ easy to use” generally provide the user with less control over the service. The same is true the other way around, tools that provide a lot of control are typically more complex to use. </p>
<p>You can see this clearly with EMR and Glue. For example, EMR on EC2 provides maximum control over the service. You can optimize, manage, and scale your cluster and compute nodes. You can take advantage of EC2 instance types, sizes, and pricing options such as Spot, Reservations, and Savings Plans. You can install a wide range of open source tools, such as Hive, Presto, HBase, Spark, and more to fit your use case. And you can choose how long you run your EMR cluster. It could be a longer running cluster that is available 24&#x2F;7, or it could be a transient cluster that’s provisioned, runs the proposed jobs, and then terminates soon after. You have control over all of it. </p>
<p>However, the freedom of choice can make the service more complex to manage. Configuring and maintaining the engine and the cluster can be a full time job. You may need to dedicate resources in your engineering teams to manage this underlying infrastructure.</p>
<p>With Glue, your choices decrease because it is serverless. You no longer get to manage the underlying EC2 instances and storage. All cluster, node, and engine maintenance disappears. From the infrastructure maintenance perspective, it is simpler. However, that also means you no longer get to choose EC2 instance types, sizes, or pricing options. And you also no longer get to choose from a range of open source engines. Glue can only run your ETL jobs in an Apache Spark environment. The other factor is that Glue terminates as soon as your job finishes executing, so support for longer-running clusters is not possible.</p>
<h3 id="Pricing"><a href="#Pricing" class="headerlink" title="Pricing"></a>Pricing</h3><p>The convenience of serverless is helpful, but there is a price for this convenience. Glue is, at face value, more expensive than EMR on EC2. This is a common tradeoff in AWS, where you have to decide if the convenience of not having to configure and manage a cluster is worth it. However, before you go with the cheaper option, you have to factor in additional costs with EMR, such as what it takes to maintain a cluster. You might need to factor in the cost of a cluster administrator into your total cost analysis. With all costs considered, you might even find that Glue may be cheaper in the long run. </p>
<p>Another factor to consider is that Glue terminates as soon as the job executes. With Glue, you only pay for the time it runs. If you have longer-running EMR clusters, you will pay for idle time where the cluster is sitting there, not performing any work.</p>
<h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>With Glue, there are three limitations you need to be aware of:</p>
<ol>
<li>It has a default limitation on how much CPU and RAM you can use for your jobs. The biggest worker type you can use currently has 8 vCPU and 32 GB of RAM. The number of workers you can scale up to in Glue by default is 100. So if you need more performance than Glue can provide to you, EMR is the better choice. </li>
<li>Glue is a true ETL service. While it can do light machine learning analysis and can be paired with Amazon Athena for data analysis, Amazon EMR outperforms Glue for both Machine Learning and with data analysis using engines like Presto. </li>
<li>Ultimately, if your workload requires any other engine other than Spark, you should use EMR.</li>
</ol>
<h3 id="EMR-Serverless-vs-Glue"><a href="#EMR-Serverless-vs-Glue" class="headerlink" title="EMR Serverless vs. Glue"></a>EMR Serverless vs. Glue</h3><p>EMR on EC2 vs Glue is a fairly straightforward comparison. However, the differences between EMR Serverless and Glue are a little less obvious. Both EMR Serverless and Glue require no infrastructure maintenance and are more expensive than EMR on EC2. The biggest difference is use case. Like EMR on EC2, you can use EMR Serverless for use cases beyond ETL. With Glue, while it does support light machine learning transformations, it is mostly considered an ETL tool. Because of this, Glue Studio offers more ETL tooling that EMR serverless doesn’t natively support, such as a graphical ETL interface, built-in scheduling, and the ability to build pipelines from Glue components. </p>
<p>If you already use EMR and have pre-existing Spark or Hive jobs, it may be worthwhile to consider running these jobs on EMR serverless. This lets you use a familiar tool without the maintenance of cluster management. </p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Ultimately, if you need flexibility with how you manage the engine or the underlying infrastructure, EMR on EC2 is best for you. Otherwise, if you need to run short-lived jobs that will run in an Apache Spark environment, Glue or EMR Serverless will save you time by managing the infrastructure for you.</p>
<h1 id="Orchestrating-ETL-Workflows"><a href="#Orchestrating-ETL-Workflows" class="headerlink" title="Orchestrating ETL Workflows"></a>Orchestrating ETL Workflows</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="Orchestrating-ETL-Workflows-1"><a href="#Orchestrating-ETL-Workflows-1" class="headerlink" title="Orchestrating ETL Workflows"></a>Orchestrating ETL Workflows</h1><p>ETL pipelines can be complex. You may be running multiple ETL jobs at once, at varying intervals of time (maybe hourly, daily, weekly), involving multiple AWS services. It’s important you have a service that not only triggers your pipeline to run, but also automates the movement between services, while also handling basic retry logic and error handling. </p>
<p>To do this, you can use orchestration services. There are three main orchestration services that can be used in combination with ETL services, like Amazon EMR and AWS Glue. These services are:</p>
<ul>
<li>AWS Data Pipeline</li>
<li>AWS Step Functions</li>
<li>And surprisingly, AWS Glue. AWS glue has it’s own orchestration tool called Glue Workflows</li>
</ul>
<h3 id="AWS-Glue"><a href="#AWS-Glue" class="headerlink" title="AWS Glue"></a>AWS Glue</h3><p>Let’s start with the simplest of the three options: Glue Workflows. Glue Workflows provides a visual editor to create relationships between your Glue components, such as your Triggers, Crawlers and your Glue ETL jobs. </p>
<p>For example, let’s say I create a Workflow. This Workflow will first start with a trigger. I can trigger based off a a schedule or an event. I want this Workflow to be triggered daily at 12:00. </p>
<p>Once the workflow is triggered, it will kick off a job to do some light pre-processing of the data. After that is successful, I’ll have a crawler crawl the optimized data set. Once the crawler finishes running, I can then run ETL on that data. </p>
<p>Glue will run this workflow every day at 12:00 without my intervention, completely automating my pipeline. </p>
<p>Glue Workflows has only one drawback: it is very simplistic and can only be integrated with Glue tools. If you use other AWS services within your pipeline, and not just Glue, consider using a service that has better service integration, such as Data Pipeline or AWS Step Functions. </p>
<p>There is no extra cost to Glue Workflows, however, you will pay for the Crawlers, the ETL jobs, and the Data Catalog requests that Workflows triggers on your behalf. </p>
<h3 id="AWS-Data-Pipeline"><a href="#AWS-Data-Pipeline" class="headerlink" title="AWS Data Pipeline"></a>AWS Data Pipeline</h3><p>Next, there’s Data Pipeline. It’s sole purpose is to coordinate data processing from one service to another without human intervention. </p>
<p>The service itself is pretty bare bones, and because of this, it’s very simple in nature. A data pipeline is made up of three core components: </p>
<ol>
<li><strong>Data nodes</strong>: these are storage locations where you house your input data and output data. Data nodes can be S3, Redshift, DynamoDB, RDS or a JDBC connection. </li>
<li><strong>Activities</strong>: this is the work that you want the pipeline to perform on your data. This could be a CopyActivity, that copies data to another location, it could be a SQL activity, that runs a SQL query on a database, or it could be an EMR activity, such as running an EMR cluster, or running a Hive query or Pig script on an EMR cluster.</li>
<li><strong>Preconditions</strong>: these are conditional statements that must be true before an activity can run. For example, you can check whether a data node exists, or run a custom shell script before your activity runs.</li>
</ol>
<p>Data Pipeline also has retry functionality built-in to the service. You can configure up to 5 retries per activity. The pipeline won’t report failure until it goes through the number of retries you set. The higher the number, the longer it will take. </p>
<p>While AWS DataPipeline is simple to get started with, you may find that there are some limitations. For example, DataPipeline has limited data sources. While you might be able to hack around this, you may want to consider using a service called AWS Step Functions for further AWS service integration. </p>
<h3 id="AWS-Step-Functions"><a href="#AWS-Step-Functions" class="headerlink" title="AWS Step Functions"></a>AWS Step Functions</h3><p>This leads us to the last orchestration service: AWS Step Functions. While AWS Step functions isn’t purpose-built for working with data, it does work well with most general workflows. This generic nature provides more flexibility to the user.</p>
<p>With Step Functions, you can integrate with far more services, such as AWS Lambda, API Gateway, Athena, and more. You can call over 200 AWS services from your Step Functions workflows. It additionally can support pipelines that use Amazon EMR and AWS Glue, whereas DataPipeline only supports EMR. </p>
<p>Step Functions coordinates the navigation among services in a serverless workflow and manages retries and errors. It is more robust than DataPipeline in terms of configuration, providing the ability to not only perform tasks, but also embed simple logic for execution in your pipeline. This enables you to make choices between multiple states, pass data between services, use parallel execution, and implement delays in your pipeline. </p>
<p>To get a feel of how it works let’s draw a quick example that uses Step Functions to orchestrate Glue ETL jobs. In this example, I upload my data to Amazon S3, which triggers Step Functions to run. Step Functions first signals to Lambda to validate my data in S3, to ensure that it is the right data type and schema. </p>
<p>If the validation is successful, the data is moved to a staging folder. If the validation fails, it moves to an error folder and sends you a notification using Amazon SNS. </p>
<p>For the successfully validated data, an AWS Glue Crawler runs to infer the schema. Step Functions then triggers a Glue ETL job to run, transforming the file into a different format. Once the glue job is complete, it stores the outputted data in the transformed folder in Amazon S3. I then receive an SNS message stating the ETL job has successfully finished. </p>
<p>This is just an example of what you can do with Step Functions. You can build far more complex ETL processes that include a wide range of AWS services and logic. </p>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>In summary, AWS Glue Workflows is great for creating workflows between different Glue Components. However, it’s not best if you need orchestration that includes other AWS services. </p>
<p>While AWS Data Pipeline is a simplistic way of getting started with building data pipelines on AWS, it does have rigid limitations on what services integrate with it. AWS Step Functions has far greater integration with AWS and provides more sophisticated logic when building a pipeline.</p>
<h1 id="Finding-Compliance-Data-With-AWS-Artifact"><a href="#Finding-Compliance-Data-With-AWS-Artifact" class="headerlink" title="Finding Compliance Data With AWS Artifact"></a>Finding Compliance Data With AWS Artifact</h1><p>Hello, and welcome to this lecture where I will be examining AWS Artifact, a free self-service portal that provides you with immediate access to AWS security and compliance reports. Within AWS Artifact, you also have the ability to view, download, accept, and terminate legal agreements between you and AWS at both the account and organization level.</p>
<p>So you may be asking yourself: why would I ever need to access the information in AWS Artifact? And as it turns out, there could be several reasons. For starters, you might be asked to provide evidence of the current or historical compliance of different AWS services used within your architecture as part of a required audit to ensure that your enterprise may continue to leverage the AWS cloud. And this audit could potentially extend out to include your suppliers as well. Or perhaps you just want to learn more about your responsibilities when it comes to complying with various regulatory standards such as Payment Card Industry, or PCI, or Service Organization Control, or SOC. After all, simply leveraging the AWS cloud does not guarantee that the systems you build within it will be fully secure or compliant. We’ll discuss this more in a moment.</p>
<p>AWS Artifact can be accessed directly from the AWS console by searching “Artifact.” From there, the AWS Artifact home page gives you options to view reports and view agreements, so let’s spend a little time discussing reports and agreements in more detail.</p>
<p>AWS Artifact Reports consist of AWS auditor-issued reports and include everything from ISO certifications to PCI and SOC reports.</p>
<p>These reports, known as audit artifacts, may be shared with auditors and regulators by creating IAM users with an associated identity-based policy that grants access only to the necessary reports. And these audit artifacts allow you to provide evidence of AWS security controls to ensure compliance with any applicable governance, regulations, or frameworks when architecting solutions in the AWS cloud. Now of course this is always done in accordance with the AWS Shared Responsibility Model, where AWS is responsible for the underlying security OF the cloud, but you remain responsible for your own systems’ and applications’ security IN the cloud. Now to learn more about the AWS Shared Responsibility Model, I encourage you to check out this resource. Consequently, the compliance reports provided within AWS Artifact pertain only to AWS and do not in any way certify the security or compliance of your own company, organization, or application. However, these audit artifacts can and should inform the security controls you choose to implement as part of your own cloud architecture and solution design.</p>
<p>In addition to security and compliance reports, AWS Artifact also allows you to view and execute legally binding agreements between you and AWS.</p>
<p>These agreements can be applied at the individual account level, or if you are signed in to the AWS console with the management account of an organization in AWS Organizations, you can also apply an agreement to all member accounts within your organization. One example of a commonly used agreement is the AWS Business Associate Addendum, or BAA, which governs your use of AWS services when storing personal health information, or PHI.</p>
<p>To accept an agreement, you must first accept the AWS Artifact non-disclosure agreement or NDA.</p>
<p>After you have accepted this NDA, then downloaded and reviewed the agreement, you may accept the agreement by checking a box acknowledging that you accept all of its relevant terms and conditions. Note that when accepting an agreement on behalf of all member accounts within an AWS Organization, you must also certify that you have the full power and authority to accept the agreement on behalf of every entity that either currently has, or may ever subsequently have, a member account within your organization at any point in the future.</p>
<p>So that’s how we can use AWS Artifact to not only view compliance reports and agreements but also to help ensure the solutions we architect in the AWS cloud remain secure and compliant with all necessary rules and regulations.</p>
<h1 id="What-is-AWS-CloudFormation"><a href="#What-is-AWS-CloudFormation" class="headerlink" title="What is AWS CloudFormation?"></a>What is AWS CloudFormation?</h1><p>Hello and welcome to this lecture where I am going to introduce you to the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudformation-introduction-infrastructure-code/introduction/">AWS CloudFormation Service</a> and some of the concepts of this service. When you first start using <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and begin getting to grips with some of the core services and features such as VPC’s, EC2’s, RDS, auto-scaling, and elastic load balancing for example. Then it is likely that you would have used specific dashboards within the management dashboard to configure them. The dashboards within the management console allow you to set up and configure the associated resources that you are interested in. Now, the configuration process generally consists of a number of steps where you are required to select different parameters. For example, when creating an EC2 instance you’ll be asked to select your AMI, the EC2 instance type, the type of storage and security groups etc. So when you start to create environments within AWS, for example you may create a Virtual Private Cloud with various subnets both private and public with Network Access Control Lists for security. And then on top of this you might deploy EC2 Instances, for example, web service in the public subnet and application service in your private subnet. And these in turn might connect to a RDS or DynamoDB Database which would also need to be configured and provisioned. You’ll also want to introduce autoscaling and elastic load balancers for higher variability. And this is great but what if you didn’t need to go through the process of configuring each and every resource through the management console selecting the appropriate parameters every time? Or even creating the same resources via the AWS CLI? Wouldn’t it be great if you created a script that created your entire environment for you? From the VPC to the Elastic Load Balancer’s and on top of that compiled the environment in such a way that you didn’t need to worry about dependencies as you would creating each individual resource? For example, you would normally have to configure your subnets after creating your VPC. Now, by using AWS CloudFormation you can provision all of your infrastructure resources that you require via a simple template in a YAML or JSON format. </p>
<p>CloudFormation performs these actions securely and across multiple regions and accounts if configured to do so. The template will describe all of your resources that you need, and their configurations without having to worry about service dependency. AWS CloudFormation will handle the order of deployment for you. You might be thinking If I have to describe and enter all of the configuration of my resources into a text file what true benefit does AWS Cloudformation have by doing it this way? Lets take a look at a few examples of where CloudFormation is extremely useful to you within your environment. </p>
<p>Security: As I explained previously your AWS resources can be provisioned and be deployed by configuring each service and component manually through a series of configurable screens. When carrying out these configurations once, five times, ten, or even twenty times the fact of human error will eventually come into play. And a mistake will be made that could lead to the resulting solution being compromised or vulnerable in some way. With AWS CloudFormation these repeatable steps can be tested, controlled, and rolled back should any issue arise. Once a template is considered error-free, the same resources can be deployed hundreds or even thousands of times without risk of errors. </p>
<p>Infrastructure Replication: AWS CloudFormation is a great tool to allow you to quickly and easily replicate your infrastructure within your AWS account. For example, lets say that you have deployed your application across a single region. Over time, the criticality of the application has increased, and you now need to deploy the same level of infrastructure and resources across multiple regions. With CloudFormation, this is easy with the use of deploying the same template in the other regions required. The alternative, would be to manually identify and understand all resources within the solution and then manually deploy them in the alternative region. Even then, it is likely that you would have missed some configuration. Having a template to deploy your resources enables the template to become the source of truth for your solution. </p>
<p>Simply Code: As the entire infrastructure is deployed via a scripted code It may make it easy for other members of your team or outside of the team to review and verify your code to ensure that it’s correct before deployment. Each author of the code can use their own code editor to construct the environment and implement a method of version control to help determine the latest templates in production. This will also help you to roll back to a previous version if required. </p>
<p>Notification and Automation: As AWS CloudFormation integrates with other management and automation serves it is easy to configure CloudFormation to notify you of the status of deployments through its integration with SNS . This could then provide you and your team the status of changes being made in CloudFormation. These SNS messages could also trigger an AWS Lambda function if you needed to bring another level of automation into your pipeline. </p>
<p>Sample Templates: AWS CloudFormation offers a number of sample templates to get you started off. Lets look at some common deployment options to save you having to create them from scratch yourself as a learning development tool. You can take a look and download these templates <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-sample-templates.html">here</a>. The template for the London Region alone contains sixty-three different templates. An example of some of these can be seen on screen. </p>
<p>That now brings me to the end of this lecture. Coming up next, I shall be looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudformation-introduction-infrastructure-code/components-of-cloudformation/">some of the components</a> that make up this service and what they are used for.</p>
<h1 id="Management-Summary"><a href="#Management-Summary" class="headerlink" title="Management Summary"></a>Management Summary</h1><p>You’ve made it through another section, it’s a great work. You’ve definitely covered the lion’s share of what you need to know in becoming a solutions architect. So, we’ve just covered topics, looking at AWS management services and concepts, and there’s a lot to take in. So, let me break down some of the core elements I believe are a must know for the exam.</p>
<p>So, let’s start with CloudWatch. Now, if you’re presented with any questions that relate to the health of resources or metrics, monitoring or logging, then it’s likely that CloudWatch will be in one of the answers available and for good reason. Now remember, CloudWatch is the go to service to understand the operational performance of your resources and applications. At its core, CloudWatch collects metrics from supported services and any custom metrics that you have added. And it can display these in a visual dashboard. It can help you to detect anomalies, review logs, trigger alarms, and also automate responses to help you optimize your infrastructure. So, it’s a great tool to help you maintain and monitor your environment.</p>
<p>So, for the exam, you need to be familiar with some of the features that it offers, as you might be asked how you could use CloudWatch to detect respond or to identify potential issues that arise in the performance of your infrastructure. So, let’s review some of the important features of the service. So, firstly, dashboards. Now, this is a customizable dashboard that lets you build a visual status of your resources using different types of widgets.</p>
<p>Now, the key point is that they are fully customizable, allowing you to design the dashboard how you need to represent your data. So, if you get a question asking how to best represent the status of many of your resources that perhaps relate to a specific project or region or resource type, then you can build a dashboard in CloudWatch for this. Next, you are 100% going to need to be aware of metrics from an exam point of view at least. If you see a question that mentions metrics, then you will almost definitely see something relating to CloudWatch. It’s CloudWatch metrics that enable you to monitor a specific element of an application resource using time series data points. So, you might see a question relating to EC2 IO Performance and what metrics you could check, perhaps disk reads or disk writes.</p>
<p>Now by default, metrics are collected every five minutes, but detailed monitoring can be enabled for a small cost, and this will collect metrics every minute. Now, what comes hand in hand with metrics is CloudWatch alarms, and these enable you to implement automatic actions based on specific metric thresholds. So, you might be given a scenario asking the best way to notify your engineering team when your EC2 instance reaches 75% CPU utilization.</p>
<p>So, what could you do to achieve this? Well, with CloudWatch, you could set an alarm that monitors for the CPU utilization metric, and then when it reaches 75%, trigger SNS to send an email to the engineering team automatically. So, remember that CloudWatch also has this integration with other services for alerting like SNS. Now, also be sure you’re aware of the different states of alarms as well, of which there are three. You have an OK state, an alarm state, and insufficient data state.</p>
<p>Now, remember CloudWatch EventBridge provides a means of implementing a level of real time monitoring. And this allows you to respond to events that occur in your application as they happen. And lastly, I just want to touch on CloudWatch Logs because these often come up in the exam one way or another. And in fact, logging is assessed at many different levels, which is why we focus on a few logging services in this course. So, what are some of the key things to remember with CloudWatch logs? Well, it acts as a central repository for real time monitoring of log data for different AWS services that provide logs as an output, such as CloudTrail EC2, VPC Flow Logs, etc. in addition to your own applications.</p>
<p>So, data is sent to a log stream within CloudWatch logs to differentiate between different logs. And you can filter for specific entries within these logs to help you identify potential issues. And you can also use the unified CloudWatch agent to collect logs and additional metric data, which is over and above the default metrics collected by CloudWatch against your EC2 instances. Now, this agent is best installed using EC2 Systems Manager known as SSM.</p>
<p>Okay, let’s leave CloudWatch logs there and move onto CloudTrail. Now, the key thing to know about CloudTrail is that it’s used to log, record, and track all API calls in your environment. If you remember that, you’ll be able to eliminate a couple of wrong answers if anything comes up about tracking API calls. Now, API calls are pretty much made for every action either made by you or made by another AWS service. They are all recorded with CloudTrail. And it’s a great tool for auditing because of this. Now, in addition to tracking the API it also tracks the user or service who initiated it, the time, date, and other metadata, such as source, IP address, etc. And where would CloudTrail send all this data? Well, to S3 of course as logs. Now S3 is used by many services for storing data and you probably know that by now anyway.</p>
<p>Now, a quick point worth mentioning is that CloudTrail logs can also be sent to CloudWatch logs for additional review, triggers, and automated responses that CloudWatch can provide like we’ve already talked about. So, this is another great integration between two services. Sometimes CloudTrail can be used as a security analysis tool, for example, identifying APIs that shouldn’t be called, or it can be used to assist with auditing as I mentioned previously.</p>
<p>Okay, so next up we looked at AWS Config. And I see CloudTrail and Config appear in the same question or the same set of answers for a particular question. So, it’s certainly worth noting the main difference between each of them and what each service is used for. So, let’s take a look at config. So, AWS Config is designed to record and capture resource changes within your environment. It’s a great service for helping you collate and review data about a specific resource type within your environment. Now you can check its configuration history to see all of the changes that have occurred on the resource since you first provisioned it, or you can see a snapshot in time of its current configuration.</p>
<p>Now, again, this also has integration with SNS and CloudTrail to offer automated notifications of any resource Config changes and which APIs triggered those. Now, one great benefit of AWS Config is its ability to implement Config rules which ensure that your resources are meeting a specific specification. Now, this is great if you get any questions that relate to compliance. So, let’s say the question was asking for ways of ensuring that your EFS file system were encrypted with KMS at all times, which was perhaps needed to meet specific regulatory requirements. What service could you use to help maintain this?</p>
<p>Well, the answer here would be AWS Config using managed rules. So, Config would assess your EFS file systems and alert you if an EFS file system was deployed without encryption. Now, this would allow you to correct the non-compliance that it’s met.</p>
<p>Okay, so we also looked at management from an AWS account level perspective as well,  and this focused on AWS organizations. Now this service is certainly mentioned in the exam, so make sure you know when you choose it and what it does and also some of the components that are used which make it a really effective account management service. And I’m going to tell you some of the main points to remember. Now the primary benefit that this service brings is its ability to essentially manage multiple accounts from a single AWS account known as the master account. Now, of course, by doing this, it helps to maintain security compliance and account management under a single umbrella.</p>
<p>Now there are two options to deploy AWS organizations, you can either deploy all with all features, which is the default, and this uses enhanced account management features, or just with consolidating billing features enabled. And this just gives a subset of features providing basic management tools enabling you to manage billing essentially across all of your accounts. So, the organization of your accounts essentially forms a family tree structure allowing you to group certain accounts with others. </p>
<p>So, know the difference between the root object, the organizational unit objects and also account objects as well. Now one benefit of organizations is that you can use service control policies or SCPs to control what services and features are accessible from within an AWS account or group of accounts. So, when a service control policy is applied to an organizational unit, all child accounts that fall under that OU will be under the same controls that are applied within the SCP.</p>
<p>Now, some people think SCPs as permission policies, however, they don’t actually give permissions, rather they just limit what permissions can be given within the corresponding account. So, they act as a permission boundary instead. So, if the SCP denied all S3 access, then no one in the associated account would be allowed to use S3. Even if their IAM permissions allowed it, it would be denied at the SCP level. So, if you get any questions relating to multi-account permissions and restrictions, then it’s likely that AWS organizations will be mentioned, specifically regarding service control policies.</p>
<p>Okay. Lastly, I just want to make sure that you know what a VPC Flow Log is and where you can use them. I’ve seen this topic come up on the exam a couple of times before. Now you might be asked how to monitor specific network traffic between your subnets or different interfaces within your infrastructure, and what would be the best solution to do this? Well, VPC Flow Logs will certainly help you here. You’ll need to have a basic understanding of what a VPC Flow Log is, what it can capture, and when they can be used to help you answer questions relating to this topic. </p>
<p>So, VPC Flow Logs capture all the IP traffic flowing between your network interfaces on your resources within your VPC, and then this log data is then sent to CloudWatch logs. Now, once the VPC Flow Log has been created, it can’t be changed. The only way you could change it would be to delete it and then recreate another. So, VPC Flow Logs can be configured against a network interface, one of your subnets in your VPC, or the VPC itself. So, just remember those three levels of application.</p>
<p>Okay, so we’ve now reached the end of this course, so a few things not to forget. So, if you need to monitor health of different resources, set alerts, gather metrics, then use CloudWatch. If you need to capture API calls being made across your AWS account, then AWS CloudTrail is your go to answer. If you need to monitor, manage, and assess the configuration state of your resource, then AWS Config can help you here with managed Config rules. If you have to set up management and security controls across a multi-AWS account level, then AWS Organizations is the service you need. And lastly, if you need to capture network traffic at an interface, subnet, or VPC level and review the logs in CloudWatch, then VPC Flow Logs should be at the forefront of your mind. Okay, that’s me done. Let’s take a step away from the keyboard and take a break before tackling the next section.</p>
<h1 id="2What-is-Amazon-CloudWatch"><a href="#2What-is-Amazon-CloudWatch" class="headerlink" title="2What is Amazon CloudWatch?"></a>2<strong>What is Amazon CloudWatch?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/course-introduction/">Data Visualization: How to Convey your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">List of metrics</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-sqs-sns-ses/">Using SQS, SNS and SES in a Decoupled and Distributed Environment</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">List of targets</a></p>
<h1 id="18CloudWatch-Logging-Agent"><a href="#18CloudWatch-Logging-Agent" class="headerlink" title="18CloudWatch Logging Agent"></a>18<strong>CloudWatch Logging Agent</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">Metrics collected by the CloudWatch Agent</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent Linux</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/windows/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent for Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">How to install&#x2F;update your SSM Agent</a></p>
<h1 id="21CloudFront-Access-Logs"><a href="#21CloudFront-Access-Logs" class="headerlink" title="21CloudFront Access Logs"></a>21<strong>CloudFront Access Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">Web distribution log file format</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">RTMP distribution log file format</a></p>
<h1 id="22VPC-Flow-Logs"><a href="#22VPC-Flow-Logs" class="headerlink" title="22VPC Flow Logs"></a>22<strong>VPC Flow Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/whats-new/2018/08/amazon-vpc-flow-logs-can-now-be-delivered-to-s3/">Flow Logs delivery to S3</a></p>
<h1 id="45Finding-Compliance-Data-With-AWS-Artifact"><a href="#45Finding-Compliance-Data-With-AWS-Artifact" class="headerlink" title="45Finding Compliance Data With AWS Artifact"></a>45<strong>Finding Compliance Data With AWS Artifact</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/resource/aws-shared-responsibility-model">AWS Shared Responsibility Model</a></p>
<h1 id="46What-is-AWS-CloudFormation"><a href="#46What-is-AWS-CloudFormation" class="headerlink" title="46What is AWS CloudFormation?"></a>46<strong>What is AWS CloudFormation?</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-sample-templates.html">AWS CloudFormation Templates</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-2-of-2-42/" rel="prev" title="AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-2-of-2-42">
      <i class="fa fa-chevron-left"></i> AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-2-of-2-42
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/11/18/AWS-Solution-Architect-Associate-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-44/" rel="next" title="AWS-Solution-Architect-Associate-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-44">
      AWS-Solution-Architect-Associate-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-44 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Management-SAA-C03-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Management (SAA-C03) Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-Amazon-CloudWatch"><span class="nav-number">2.</span> <span class="nav-text">What is Amazon CloudWatch?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-AWS-CloudTrail"><span class="nav-number">3.</span> <span class="nav-text">What is AWS CloudTrail?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-CloudTrail-Operations"><span class="nav-number">4.</span> <span class="nav-text">AWS CloudTrail Operations</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-AWS-Config"><span class="nav-number">5.</span> <span class="nav-text">What is AWS Config?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Key-Components-of-AWS-Config"><span class="nav-number">6.</span> <span class="nav-text">Key Components of AWS Config</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Organizations"><span class="nav-number">7.</span> <span class="nav-text">AWS Organizations</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Implementing-AWS-Organizations"><span class="nav-number">8.</span> <span class="nav-text">Implementing AWS Organizations</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Securing-Your-Organizations-with-Service-Control-Policies"><span class="nav-number">9.</span> <span class="nav-text">Securing Your Organizations with Service Control Policies</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Control-Tower"><span class="nav-number">10.</span> <span class="nav-text">AWS Control Tower</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-License-Manager"><span class="nav-number">11.</span> <span class="nav-text">AWS License Manager</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Service-Catalog"><span class="nav-number">12.</span> <span class="nav-text">AWS Service Catalog</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-AWS-Systems-Manager"><span class="nav-number">13.</span> <span class="nav-text">Introduction to AWS Systems Manager</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Systems-Manager-is-a-Good-Fit-in-the-AWS-Tool-Set"><span class="nav-number">14.</span> <span class="nav-text">Systems Manager is a Good Fit in the AWS Tool Set</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Managing-Resource-Groups"><span class="nav-number">15.</span> <span class="nav-text">Managing Resource Groups</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Systems-Manager-Requirements-and-Building-Blocks"><span class="nav-number">16.</span> <span class="nav-text">AWS Systems Manager Requirements and Building Blocks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-Benefits-of-Logging"><span class="nav-number">17.</span> <span class="nav-text">The Benefits of Logging</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CloudWatch-Logging-Agent"><span class="nav-number">18.</span> <span class="nav-text">CloudWatch Logging Agent</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-1"><span class="nav-number">18.0.1.</span> <span class="nav-text">Start of demonstration 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-1"><span class="nav-number">18.0.2.</span> <span class="nav-text">End of demonstration 1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-2"><span class="nav-number">18.0.3.</span> <span class="nav-text">Start of demonstration 2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-2"><span class="nav-number">18.0.4.</span> <span class="nav-text">End of demonstration 2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-3"><span class="nav-number">18.0.5.</span> <span class="nav-text">Start of demonstration 3</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-3"><span class="nav-number">18.0.6.</span> <span class="nav-text">End of demonstration 3</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CloudTrail-Logging"><span class="nav-number">19.</span> <span class="nav-text">CloudTrail Logging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration"><span class="nav-number">19.0.1.</span> <span class="nav-text">Start of demonstration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration"><span class="nav-number">19.0.2.</span> <span class="nav-text">End of demonstration</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Monitoring-CloudTrail-with-CloudWatch"><span class="nav-number">20.</span> <span class="nav-text">Monitoring CloudTrail with CloudWatch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-1"><span class="nav-number">20.0.1.</span> <span class="nav-text">Start of demonstration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-1"><span class="nav-number">20.0.2.</span> <span class="nav-text">End of demonstration</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CloudFront-Access-Logs"><span class="nav-number">21.</span> <span class="nav-text">CloudFront Access Logs</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Resources-Referenced"><span class="nav-number">21.1.</span> <span class="nav-text">Resources Referenced</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transcript"><span class="nav-number">21.2.</span> <span class="nav-text">Transcript</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-2"><span class="nav-number">21.2.1.</span> <span class="nav-text">Start of demonstration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-2"><span class="nav-number">21.2.2.</span> <span class="nav-text">End of demonstration</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#VPC-Flow-Logs"><span class="nav-number">22.</span> <span class="nav-text">VPC Flow Logs</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Transcript-1"><span class="nav-number">22.1.</span> <span class="nav-text">Transcript</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Start-of-demonstration-3"><span class="nav-number">22.1.1.</span> <span class="nav-text">Start of demonstration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End-of-demonstration-3"><span class="nav-number">22.1.2.</span> <span class="nav-text">End of demonstration</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview-of-the-AWS-Health-Dashboard"><span class="nav-number">23.</span> <span class="nav-text">Overview of the AWS Health Dashboard</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview-of-the-AWS-Health-Dashboard-1"><span class="nav-number">24.</span> <span class="nav-text">Overview of the AWS Health Dashboard</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Service-Health"><span class="nav-number">24.1.</span> <span class="nav-text">Service Health</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Your-account-health"><span class="nav-number">24.2.</span> <span class="nav-text">Your account health</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Integration-with-EventBridge"><span class="nav-number">24.3.</span> <span class="nav-text">Integration with EventBridge</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Enterprise-Level-Services"><span class="nav-number">25.</span> <span class="nav-text">Enterprise-Level Services</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Enterprise-Level-Services-1"><span class="nav-number">26.</span> <span class="nav-text">Enterprise-Level Services</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bills-and-Cost-Drivers"><span class="nav-number">27.</span> <span class="nav-text">Bills and Cost Drivers</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Credits"><span class="nav-number">28.</span> <span class="nav-text">Credits</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Cost-Explorer"><span class="nav-number">29.</span> <span class="nav-text">Cost Explorer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reports"><span class="nav-number">30.</span> <span class="nav-text">Reports</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Cost-and-Usage-Reports"><span class="nav-number">31.</span> <span class="nav-text">Cost and Usage Reports</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Budgets"><span class="nav-number">32.</span> <span class="nav-text">Budgets</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction-to-AWS-Budgets"><span class="nav-number">33.</span> <span class="nav-text">Introduction to AWS Budgets</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Creating-a-Budget-Demo"><span class="nav-number">34.</span> <span class="nav-text">Creating a Budget Demo</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Budget-Actions-Demo"><span class="nav-number">35.</span> <span class="nav-text">Budget Actions Demo</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Budget-Reports-Demo"><span class="nav-number">36.</span> <span class="nav-text">Budget Reports Demo</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Cost-Anomaly-Detection"><span class="nav-number">37.</span> <span class="nav-text">AWS Cost Anomaly Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tagging"><span class="nav-number">38.</span> <span class="nav-text">Tagging</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tagging-Best-Practices"><span class="nav-number">39.</span> <span class="nav-text">Tagging Best Practices</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Generated-Cost-Allocation-Tags"><span class="nav-number">40.</span> <span class="nav-text">AWS-Generated Cost Allocation Tags</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Visualization-Services"><span class="nav-number">41.</span> <span class="nav-text">Data Visualization Services</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Glue-Data-Catalog-Primer"><span class="nav-number">42.</span> <span class="nav-text">AWS Glue Data Catalog Primer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Glue-Data-Catalog-Primer-1"><span class="nav-number">43.</span> <span class="nav-text">AWS Glue Data Catalog Primer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Glue-Data-Catalog"><span class="nav-number">43.0.1.</span> <span class="nav-text">AWS Glue Data Catalog</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ETL-with-AWS-Glue-Studio"><span class="nav-number">44.</span> <span class="nav-text">ETL with AWS Glue Studio</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ETL-with-AWS-Glue-Studio-1"><span class="nav-number">45.</span> <span class="nav-text">ETL with AWS Glue Studio</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Glue-DataBrew-vs-Glue-Studio"><span class="nav-number">46.</span> <span class="nav-text">AWS Glue DataBrew vs. Glue Studio</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#AWS-Glue-DataBrew-vs-Glue-Studio-1"><span class="nav-number">47.</span> <span class="nav-text">AWS Glue DataBrew vs. Glue Studio</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Glue-DataBrew-vs-Glue-Studio"><span class="nav-number">47.0.1.</span> <span class="nav-text">Glue DataBrew vs Glue Studio</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Different-Tools-for-Different-Users"><span class="nav-number">47.0.1.1.</span> <span class="nav-text">2. Different Tools for Different Users</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-Programmatic-Creation-of-ETL-Jobs"><span class="nav-number">47.0.1.2.</span> <span class="nav-text">3. Programmatic Creation of ETL Jobs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-Data-Profiling"><span class="nav-number">47.0.1.3.</span> <span class="nav-text">4. Data Profiling</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-EMR-vs-AWS-Glue-for-ETL"><span class="nav-number">48.</span> <span class="nav-text">Amazon EMR vs. AWS Glue for ETL</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Amazon-EMR-vs-AWS-Glue-for-ETL-1"><span class="nav-number">49.</span> <span class="nav-text">Amazon EMR vs. AWS Glue for ETL</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ease-of-Use"><span class="nav-number">49.0.1.</span> <span class="nav-text">Ease of Use</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pricing"><span class="nav-number">49.0.2.</span> <span class="nav-text">Pricing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Limitations"><span class="nav-number">49.0.3.</span> <span class="nav-text">Limitations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EMR-Serverless-vs-Glue"><span class="nav-number">49.0.4.</span> <span class="nav-text">EMR Serverless vs. Glue</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">49.0.5.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Orchestrating-ETL-Workflows"><span class="nav-number">50.</span> <span class="nav-text">Orchestrating ETL Workflows</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Orchestrating-ETL-Workflows-1"><span class="nav-number">51.</span> <span class="nav-text">Orchestrating ETL Workflows</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Glue"><span class="nav-number">51.0.1.</span> <span class="nav-text">AWS Glue</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Data-Pipeline"><span class="nav-number">51.0.2.</span> <span class="nav-text">AWS Data Pipeline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AWS-Step-Functions"><span class="nav-number">51.0.3.</span> <span class="nav-text">AWS Step Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-1"><span class="nav-number">51.0.4.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Finding-Compliance-Data-With-AWS-Artifact"><span class="nav-number">52.</span> <span class="nav-text">Finding Compliance Data With AWS Artifact</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-AWS-CloudFormation"><span class="nav-number">53.</span> <span class="nav-text">What is AWS CloudFormation?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Management-Summary"><span class="nav-number">54.</span> <span class="nav-text">Management Summary</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2What-is-Amazon-CloudWatch"><span class="nav-number">55.</span> <span class="nav-text">2What is Amazon CloudWatch?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#18CloudWatch-Logging-Agent"><span class="nav-number">56.</span> <span class="nav-text">18CloudWatch Logging Agent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#21CloudFront-Access-Logs"><span class="nav-number">57.</span> <span class="nav-text">21CloudFront Access Logs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#22VPC-Flow-Logs"><span class="nav-number">58.</span> <span class="nav-text">22VPC Flow Logs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#45Finding-Compliance-Data-With-AWS-Artifact"><span class="nav-number">59.</span> <span class="nav-text">45Finding Compliance Data With AWS Artifact</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#46What-is-AWS-CloudFormation"><span class="nav-number">60.</span> <span class="nav-text">46What is AWS CloudFormation?</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
