<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/32/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/32/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:24" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:24-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:39:44" itemprop="dateModified" datetime="2022-11-20T19:39:44-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:22" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:22-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:36:24" itemprop="dateModified" datetime="2022-11-20T19:36:24-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to “Managing Your Google Cloud Infrastructure”. I’m Guy Hummel and I’ll be showing you how to keep your cloud systems running well.</p>
<p>This course is about how to maintain your cloud infrastructure after you have implemented it. Although you can set up Google Cloud to automate many operations tasks, you will still need to monitor, test, manage, and troubleshoot it over time to make sure your systems are running properly.</p>
<p>To get the most from this course, you should know the fundamentals of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, such as how to create virtual machine instances and use Cloud Storage. If you need a refresher, then you can take our Google Cloud Platform: Fundamentals course.</p>
<p>You should also have experience with performing operations tasks, especially working with Linux. It’s also helpful to have some programming experience, although just knowing the basics of a typical programming language should be enough.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>We’ll start by going through all of the components of the Operations suite to monitor, log, report errors, debug, and trace your applications.</p>
<p>Then I’ll show you how to test your infrastructure to see how it performs under difficult conditions, including heavy load, instance failures, and cyber attacks.</p>
<p>After that, you’ll see how to get data into Cloud Storage and then how to keep it under control with lifecycle management. You’ll also learn how to optimize your Cloud SQL and Cloud CDN configurations.</p>
<p>Finally, we’ll wrap up with how to troubleshoot instance startup failures, SSH errors, and network issues.</p>
<p>If you’re ready to learn how to tame your Google Cloud infrastructure, then let’s get started.</p>
<h1 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h1><p>After you’ve implemented your infrastructure in Google Cloud Platform, the first thing you’ll want to do is set up a monitoring system that will alert you when there are major problems. The easiest way to do this is to use Cloud Operations (formerly known as Stackdriver), which is Google’s powerful monitoring, logging, and debugging tool.</p>
<p>To get to it, select “Monitoring” from the console menu. The first time you bring up Monitoring in a project, it’ll take a while to set up. I’ll fast-forward.</p>
<p>Here’s where you’ll find the instructions for installing the Monitoring Agent (and the Logging Agent). You don’t need to install the agent to be able to use Monitoring, but if you do install it, you can get more information about an instance, such as about the third-party software running on it. We don’t need to install it yet, so we’ll leave that until later.</p>
<p>Suppose you want to monitor a web server and get notified if it goes down. First, you need to create an Uptime Check. </p>
<p>For the title, let’s call it “Example”. Click “Next”. Since we want to check if a web server is up, leave the Protocol as HTTP and the Resource Type as URL.</p>
<p>For the hostname, I’m going to put in the IP address of an instance I have that’s running a web server. Leave “Check Frequency” set to 1 minute. And click “Next”. We can leave the Response Validation with the defaults, so click “Next” again.</p>
<p>This is where we set up an alert for when the uptime check fails. First, we specify how long a failure has to last before it’ll trigger an alert. Let’s leave it set to 1 minute. We should also tell it how to send alert notifications. Click the dropdown menu, and then click “Manage Notification Channels”. You can be alerted by email, text message, or a variety of other options, such as Slack. We’ll get it to send an email when the web server is down. Click “Add New”, and enter your email address and your name.</p>
<p>Okay, now close this tab in your browser, and go back to the previous one. Click Refresh and select your email. Now click the “Test” button. Since the web server at that address is up, it came back right away.</p>
<p>Now I’m going to stop Apache on the instance that’s running the web server and test it again. This time, the connection failed, as expected. Click the Create button.</p>
<p>To see the results of the uptime check, go to the menu on the left and select Uptime Checks. It’ll take a while before it runs for the first time, so don’t worry if you don’t see anything in the dashboard right away. I’ll skip ahead to when the uptime check has run. OK, now you can see it’s showing that the web server’s down. After a little while, it’ll send a notification email. Here’s what it looks like.</p>
<p>Now I’ll start Apache up again and see if the alert policy sees it. I’ll just skip ahead a few minutes. Yes, it sees that the web server is up now.</p>
<p>If you want to see data graphically, then click on Dashboards. It provides default dashboards for many Google services. To create your own, click Create Dashboard. I’ll call it “Example Dashboard”.</p>
<p>Now click the “Add Chart” button. In this search field, type “URL”. There’s the resource type we need. It’s called “Uptime Check URL”. It gives us a few different metrics to choose from. The obvious one to choose is “Check passed”, but to make things more interesting, let’s choose “Request latency”. Click Save and the graph will be added to your dashboard.</p>
<p>This graph shows the network latency between each region and the web server. You can see when the web server was down, but it gives us more information than that. This network latency data from different locations around the world can be quite helpful, especially if some of your users are reporting slow performance.</p>
<p>Note that you’ll need to refresh this page to see the latest data. You can turn on auto-refresh if you want.</p>
<p>Suppose you’d like to get more information about the instance where the web server is running, such as the CPU load. Let’s create another chart.In the search field, type “cpu load”. Let’s select the 1-minute version. For the resource type, select “VM instance”. Notice that you can even monitor Amazon EC2 instances.</p>
<p>You’ll notice that the chart is blank. That’s because we need to install the Monitoring Agent to get CPU data.</p>
<p>Go back to the Overview, and then click this link to bring up the installation instructions. The instructions are different depending on which Linux distribution you’re running on your instance. I’m running Debian, so I need to follow these instructions. I’ll fast-forward to the point after I’ve run all of the install commands.</p>
<p>While we’re here, let’s install the logging agent too. You can find a link to the documentation in the GitHub repository I created for this course. The link to the GitHub repository is at the bottom of the Overview tab below this video.</p>
<p>Installing the logging agent will prepare this instance for the next lesson when we use Cloud Logging. I’ll fast-forward again.</p>
<p>Now let’s go back and see what happened to our chart. Okay, now there’s a line showing the load average, so it worked.</p>
<p>If you’ve been following along using your own account, then you should go back and delete the monitoring you set up. First, go to the Alerting page from the menu, and then delete the policy. You have to do that before it will let you delete the uptime check. Okay, now go to “Uptime Checks” and delete the one you created. Finally, go to Dashboards, click on the one you created, and delete it.</p>
<p>That’s it for this lesson.</p>
<h1 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h1><p>Looking at real-time monitoring is great, but there will be many times when you’ll want to look at what happened in the past. In other words, you need logs.</p>
<p>For example, suppose I wanted to see when a VM instance was shut down. Compute Engine, like almost every other Google Cloud Platform service, writes to the Cloud Audit Logs. These logs keep track of who did what, where, and when.</p>
<p>There are three types of audit logs. Admin Activity logs track any actions that modify a resource. This includes everything from shutting down VMs to modifying permissions.</p>
<p>System Event logs track Google’s actions on Compute Engine resources. Some examples are maintenance of the underlying host and reclaiming a preemptible instance.</p>
<p>Data Access logs are pretty self-explanatory. They track data requests. Note that this also includes read requests on configurations and metadata. Since these logs can grow very quickly, they’re disabled by default. One exception is BigQuery Data Access logs, which are not only enabled by default, but it’s not even possible to disable them. Fortunately, you won’t get charged for them, though.</p>
<p>In the console, select “Logging”.</p>
<p>There are lots of options for filtering what you see here. You can look at the logs for your VM instances, firewall rules, projects, and many other components. You can even send logs from other cloud platforms like AWS to here. You just need to install the logging agent on any system that you want to get logs from.</p>
<p>This is a great way to centralize all of your logs. Not only does centralizing your logs make it easier to search for issues, but it can also help with security and compliance, because the logs aren’t easy to edit from a compromised node.</p>
<p>In this case, we need to look at the VM instance logs. You can choose a specific instance or all instances. I only have one instance right now, called instance-1. Since we installed the logging agent on instance-1 in the last lesson, there are already some log entries for it. </p>
<p>Here you can choose which logs you want from the instance, such as the Apache access and error logs. I could set it to “syslog” since that’s where the shutdown message will be, but I’ll just leave it at “All logs” because sometimes you might not know which log to look in.</p>
<p>You can also filter by log level, and for example, only look at critical entries. I’ll leave it at “Any log level”.</p>
<p>Finally, you can change how far back it will look for log entries. I’ll change it to the last 24 hours.</p>
<p>OK, now I’ll search for any entries that contain the word “shutdown” so I can see if this instance was shut down in the last 24 hours.</p>
<p>If you need to do really serious log analysis, then you can export the logs to BigQuery, which is Google’s data warehouse and analytics service. Before you can do that, you need to have the right permissions to export the logs. If you are the project owner then, of course, you have permission. If you’re not, then the “Create Sink” button will be greyed out, and you’ll have to ask a project owner to give you the Logs Configuration Writer role.</p>
<p>First, click the “Create Sink” button. A sink is a place where you want to send your data. Give your sink a name, such as “example-sink”. Under “Sink Service”, you have quite a few options, such as BigQuery, Cloud Storage, or a Custom destination. We’ll choose BigQuery. </p>
<p>Under “Sink Destination”, you have to choose a BigQuery dataset to receive the logs. If you don’t have one already, then click “Create new BigQuery dataset”. Give it a name, such as “example_dataset”. Note that I used an underscore instead of a dash because dashes are not allowed in BigQuery dataset names. Now click the “Create Sink” button.</p>
<p>It says the sink was created, so let’s jump over to BigQuery and see what’s there. Hmmm. It created our example dataset, but it doesn’t contain any tables, which means it doesn’t have any data. That’s weird, right? Well, it’s because when you set up a sink, it only starts exporting log entries that were made after the sink was created.</p>
<p>OK, then let’s generate some more log entries and see if they get exported. I’ll restart the VM, which will generate lots of log entries. Okay, I’ve restarted it. Now if we go back to the Logging page, do we see the new messages? Yes, we do.</p>
<p>Now let’s go back to BigQuery and see if the data’s there. Yes, there are two tables there now. Click on the syslog table. Now click the “Query Table” button. To do a search in BigQuery, you need to use SQL statements, so let’s write a simple one just to verify that the log entries are there.</p>
<p>Thankfully, it already gave me the skeleton of a SQL statement. I just need to fill in what I’m selecting. I’ll put in an asterisk to select everything, but I’ll restrict it by using a WHERE clause with the column name “textPayload” (which is the column that contains the text in the log entry)…”LIKE ‘%shutdown%’”. The percent signs are wildcards, so this SQL statement says to find any log entries that have the word “shutdown” in them somewhere.</p>
<p>Now we click the “Run” button…and it returns the matching log entries. If we scroll to the right, then we can see the textPayload field and it does indeed contain the word “shutdown” in each of the entries.</p>
<p>Of course, we did exactly the same search on the Logging page and it was way easier, so why would we want to go through all of this hassle of exporting to BigQuery and writing SQL statements? Well, because sometimes you may need to search through a huge number of log entries and need to do complicated queries. BigQuery is lightning fast when searching through big data, and if you build a complex infrastructure in Google Cloud Platform, then the volume of log data it will generate will easily qualify as big data.</p>
<p>Since we don’t want our example sink to keep exporting logs to BigQuery and incurring storage charges, let’s delete what we’ve created. On the Logging page, click on “Logs Router” in the left-hand menu, then select the sink and delete it.</p>
<p>We should also delete the BigQuery dataset, so go back to the BigQuery page, select the dataset, and click “Delete Dataset”. It wants you to be sure that you actually want to delete the dataset, so you have to type the dataset name before it will delete it.</p>
<p>One concern that you or your company may have is how to ensure the integrity of your logs. Many hackers try to cover their tracks by modifying or deleting log entries. There are a number of steps you can take to make it more difficult to do that.</p>
<p>First, apply the principle of least privilege. That is, give users the lowest level of privilege they need to perform their tasks. In this case, only give the owner role for projects and log buckets to people who absolutely need it.</p>
<p>Second, track changes by implementing object versioning on the log buckets. The Cloud Storage service automatically encrypts all data before it is written to the log buckets, but you can increase security by forcing a new version to be saved whenever an object in a log bucket is changed. Unfortunately, this won’t prevent an owner from deleting an incriminating object, which is why you need to keep tight control on which users are given the owner role.</p>
<p>Third, you could add more protection by requiring two people to inspect the logs. You could copy the logs to another project with a different owner using either a cron job or the Cloud Storage Transfer Service. Of course, this still won’t prevent an owner in the first project from deleting the original bucket before the copy occurs or from disabling the original logging.</p>
<p>So the bottom line is that a person with the owner role can get around just about anything you put in place, but you can make it nearly impossible for someone without the owner role to change the logs without you knowing about it.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Error-Reporting-and-Debugging"><a href="#Error-Reporting-and-Debugging" class="headerlink" title="Error Reporting and Debugging"></a>Error Reporting and Debugging</h1><p>So far, we’ve been looking at alerts and log messages from system software. Now it’s time to look at how to get error information from your applications. That’s where the Cloud Error Reporting service comes in. To show you how this works, I’m going to install Google’s Hello World application in App Engine and then get it to generate an error.</p>
<p>Normally, you’d use your own workstation for the development environment, but to simplify this demo, I’m going to use Cloud Shell. The nice thing about Cloud Shell is that it already has all of the packages installed that you need. When you do want to write and test Java code on your own workstation, remember that you need to install the Google Cloud SDK, the Java SE 11 Development Kit, Git, and Maven 3.5 or greater on your system.</p>
<p>First, open Cloud Shell. Next, get a copy of the Hello World application with this “git clone” command. Then go into the directory where the app is.</p>
<p>Now use the local development server to make sure the app works. To see if it’s working, click the “Web Preview” icon, and select “Preview on port 8080”. You should see a “Hello world!” message. OK, it’s working, so let’s stop the development server and upload the application to App Engine. You can stop the development server with a Ctrl-C.</p>
<p>Now use the “gcloud app deploy” command to upload it to App Engine. If you’re doing this yourself, then it may look slightly different than mine because I’ve already configured App Engine. If this is your first time deploying to App Engine, then it will likely ask you to choose a region. OK, it’s done deploying.</p>
<p>There are a couple of ways to test it. If you’re not using Cloud Shell, then you could do a “gcloud app browse”, which is pretty handy. Since we are using Cloud Shell, we’ll have to go to this URL. There’s “Hello World!” again.</p>
<p>Now, in order to see an error on the Error Reporting page, we need to generate an error. Let’s edit the code and mess something up. I’m going to add a line that I know will cause a problem. This’ll throw an exception because you can’t divide a number by zero.</p>
<p>Now run “gcloud app deploy” again. When it’s done, bring the app up in your browser again. This time it gives you a big error message, which is actually what we want, for once.</p>
<p>Let’s see if Error Reporting picked it up. The Google Cloud Console shows errors on the main dashboard, so you don’t have to go to the Error Reporting page to see them. I’ll refresh the page. There it is. Click on “Go to Error Reporting” to see what it shows. If you click on the error, you’ll see more details, including the stack trace. You can even click on the line where the error occurred and it will take you into your source code in the Debugger. However, in this case, the line it’s showing at the top of the stack trace is not in our code. We can click on this line, though, which is in our code, and it should take us there.</p>
<p>The Debugger is a great tool that you can use whether an error occurred or not. Let’s put a more subtle problem in the code and see how we can use the Debugger to figure out what’s wrong.</p>
<p>Suppose we want to check the operating system running our app, and if it’s Ubuntu, then we’ll print “Ubuntu rocks!”</p>
<p>First, we have to fix the bug that we introduced previously, so we’ll remove that line. We have to go back to the editor to do that. Now I’ll add the new code. Even if you’re not familiar with Java, this is pretty straightforward. It gets the name of the operating system, then it checks to see whether it’s equal to Ubuntu or not, and if it is, it says, “Ubuntu rocks!”, and if it isn’t, it says, “Hello world!”.</p>
<p>Now we’ll upload it to App Engine again. OK, now we’ll refresh the webpage. And it says “Hello world!” again, not “Ubuntu rocks!” That might be because the underlying operating system isn’t Ubuntu, but let’s go back to the Debugger and see if that’s the reason.</p>
<p>You’ll notice that this is still the old version of the file. First, refresh the browser. It’s still showing the old version. To get to the new version, you have to click on this drop-down menu and select the right one. The latest version should say 100% at the end. Sometimes you have to tell it where the source code is. Find “App Engine” in the list, and click the “Select source” button.</p>
<p>Now find the file. You’ll see some text on the right-hand side that says to click a line number to take a snapshot of the variables and call stack. It also points out that taking a snapshot does not stop the running application, which is good to know.</p>
<p>Click in the left-hand gutter on the line just after the “osname” variable is set. Now that the snapshot point is set, we can refresh the webpage and trigger the snapshot. If we go back to the Debugger tab, you’ll see that it’s showing the variables and call stack on the right-hand side. There’s “osname”. It’s set to “Linux”, not anything more specific. I guess it doesn’t know the specific distribution of Linux that’s running, so let’s change our code to check for Linux instead.</p>
<p>And deploy the new version. Now refresh the webpage. It worked!</p>
<p>Let’s go back to the main Error Reporting page and I’ll show you a couple of other things. First, if you’re sitting on this page watching for errors in real-time, then you should click the “AUTO RELOAD” button, which will refresh the page every 5 seconds. If you don’t want to hang around here and just want to get an email when an error occurs, then click the “Turn on notifications” button.</p>
<p>Alright, that’s it for this lesson.</p>
<h1 id="Tracing-and-Profiling"><a href="#Tracing-and-Profiling" class="headerlink" title="Tracing and Profiling"></a>Tracing and Profiling</h1><p>In the last lesson, we looked at how to debug errors in your application, but what do you do if your application is working properly but performing too slowly? That’s what Cloud Trace and Cloud Profiler are used for. Cloud Trace shows you the latency of each application request. That is, it tells you how long each request takes.</p>
<p>The Trace List is probably where you will spend most of your time. It shows you all of the traces over a specific period of time in this cool graph. It is set to “1 hour” right now, but we can change that to give a longer view. Each one of these dots is a trace of an individual request to the application. If you click on one of the dots, it brings up two more panes underneath. The Waterfall View shows what happened during the request. The first bar shows the total end-to-end time, which was 215 milliseconds in this case. The bars underneath show the time it took to complete calls performed when handling the request. In this case, we have one bar for an HTTP GET request.</p>
<p>Of course, this timeline would be a lot more useful if we were running a more complex application with multiple calls so you could see which ones were taking the most time. Each of those calls would have a bar on this chart. The Hello World application is about the simplest application possible, so you’ll just have to use your imagination here.</p>
<p>Analysis reports show you the latency distribution for your application and also attempt to identify performance bottlenecks, which is a great feature. You have to have at least 100 traces before you can run a report, though.</p>
<p>If you’re running your applications in App Engine, then it’ll automatically capture and submit traces, but if you want to trace code that’s running outside of App Engine, then you’ll have to either add instrumentation code to your applications using the Trace SDK or submit traces through the API.</p>
<p>Cloud Trace shows you which requests take the longest to run. Once you’ve determined which requests might need to be optimized, you can use Cloud Profiler to see which parts of the code for those requests are using the most CPU and memory.</p>
<p>To use Cloud Profiler, you have to add instrumentation code to your application even if it’s running in App Engine. Google has provided a sample application called shakesapp that includes this instrumentation. It’s written in the Go language. Here’s what it looks like in Cloud Profiler. This is called a flame graph, and it can be a bit confusing until you know how it works.</p>
<p>Since CPU time is selected, the bars represent the CPU time taken by each function. I ran the application seven times, so these results show the average of those seven runs. The first bar is for the entire application, which took about 13 seconds of CPU time, on average.</p>
<p>The bars underneath are color-coded according to the package they’re in. Most of these functions are part of the standard libraries for the Go language. The ones that are part of the actual application, shakesapp, are dark green in this graph. The first one just calls the second one, so the second bar is the one that matters. It calls a Go language function called MatchString. This single function takes up 58% of the CPU time for this application, so we might want to see if there’s a more efficient way to perform this operation.</p>
<p>Okay, that’s it for the Cloud Operations suite. Before we go, you might want to delete your application, so it doesn’t incur any more charges. Go to App Engine and then go to Settings. Click “Disable application”. It will ask you to type the app’s ID before you can click “DISABLE”. This doesn’t delete the application, but it does stop it from serving requests. To start the application up again, you can just click “Enable application”.</p>
<p>If you want to permanently delete the application, then you’ll have to delete the project it’s associated with, which you can do in the “IAM &amp; Admin” page. Be aware that if you delete a project, you will never be able to use that project ID again. That is, you won’t be able to create a new project with the same ID.</p>
<p>That’s it for this lesson.</p>
<h1 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h1><p>So far, we’ve been talking about monitoring and debugging your applications in production, but you’ll also need to test your application and infrastructure to see how it will perform under different conditions.</p>
<p>There are at least three types of tests you should run: load tests, where you stress your application with a heavy load. Resilience tests where you see what happens when various infrastructure components fail and vulnerability tests where you see if your application can withstand hacker attacks.</p>
<p>Ideally you should run load tests before you put your application into production. Your test should be designed to simulate real world traffic as closely as possible. You should test at the maximum load you expect to encounter which can admitingly be difficult to predict for some applications but hopefully you’ll have a reasonably good idea of how much traffic you’re likely to get. You should also measure how your Google cloud costs increase as the number of users increases.</p>
<p>If you’re expecting a wide variation in how much traffic you get then you should also test how your application performs when traffic suddenly increases.</p>
<p>Resilience testing is similar to disaster recovery testing because you’re testing what happens when infrastructure fails but the difference is that in resilience testing you’re expecting your application to keep running with little or no downtime.</p>
<p>One common testing scenario is to terminate a random instance within an autoscaling instance group. Netflix created software called Chaos Monkey that automates this sort of testing. If your application in the autoscaling instance group is stateless, then it should be able to survive this sort of failure without any noticeable impact on users.</p>
<p>Since cyber attacks are extremely common these days, your organization should put processes in place to test the security of your applications. Here are a few important ones:</p>
<p>First, ideally your software development team should have a peer review process with developers checking each other’s code for security flaws. Second, you should integrate a static code analysis tool such as HP Fortify into your continuous integration continuous deployment pipeline to automate security checking.</p>
<p>Third, at least once a year you should run penetration tests on your applications and infrastructure to see if they’re vulnerable. You can either do this yourself or contract a third party to do it. Other cloud providers typically require that you request permission before you perform penetration testing on your cloud infrastructure. Surprisingly Google does not require that you contact them.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a> also provides a useful tool called the Web Security Scanner. This service connects to the base URL of your application and follows all of the links in it while scanning for vulnerabilities, such as cross-site-scripting, mixed content, and outdated libraries. It can scan applications hosted in App Engine, Compute Engine, and Google Kubernetes Engine.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Storage-Management"><a href="#Storage-Management" class="headerlink" title="Storage Management"></a>Storage Management</h1><p>Once you’ve set up your Cloud Storage buckets and applied the right security settings to them, you’ll still need to manage them on an ongoing basis.</p>
<p>One of your first tasks will likely be to get data into your Cloud Storage buckets. If you need to upload data from an on-premise location, then you have three options:</p>
<ol>
<li>The Cloud Storage console</li>
<li>gsutil, or</li>
<li>Offline media import &#x2F; export</li>
</ol>
<p>The easiest way is to click on a bucket in the Cloud Storage console and then click “Upload Files” or “Upload Folder”. You can even view the uploaded file from the console if it’s the type of file that a web browser knows how to display.</p>
<p>The second way is to use the “gsutil” command. For example, to upload a folder called “example-folder” from your desktop, you would use “gsutil cp -r Desktop&#x2F;example-folder” and then put “gs:&#x2F;&#x2F;” and the name of the bucket. Now, if we go back to the Cloud Storage console, we can see the uploaded folder.</p>
<p>If you have a slow or expensive Internet connection, then you may want to use the third option, which is to ship your data on offline media. One way is to send hard disks, storage arrays, tapes, or other media to a third-party provider, such as Iron Mountain, and let them upload your data. </p>
<p>Another way is to use a Transfer Appliance supplied by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a>. Here’s how it works. You submit a request for a Transfer Appliance, which Google then ships to you. When you receive it, you install it in your data center and transfer your data to it. Then you ship it back to Google so they can upload it for you.</p>
<p>There are still more steps to perform, though, because the data on the Transfer Appliance is encrypted. First, Google uploads your encrypted data to a staging bucket in Cloud Storage. Next, you have to launch and configure what’s called a rehydrator instance. Then you run a rehydrator job on the instance, which will decrypt the data and copy it to a Cloud Storage bucket of your choosing. Finally, you delete the instance and send Google a request to erase the data from the Transfer Appliance and the staging bucket.</p>
<p>If you need to transfer data from another cloud provider, then you can use the Google Cloud Storage Transfer Service. I’ll show you how it works.</p>
<p>Click “Transfer” in the left-hand menu. Then click the “Create transfer” button. Under “Select source”, there are three options: you can transfer from another Google Cloud Storage bucket, an Amazon S3 bucket, or from a URL. I’m going to transfer from another Cloud Storage bucket to simulate transferring from another cloud provider because it works the same way.</p>
<p>Click the Browse button and select the bucket. Then say which files to include in the transfer by specifying their prefix, such as “example”. You can also exclude files in the same way. Another option is to specify that you only want files modified recently, say in the last 24 hours. Of course, if you want to copy the entire contents of the bucket, then you don’t need to put in any filters.</p>
<p>Now choose the destination bucket. There are also some options for how it handles overwrites and deletions. By default, an object will only be overwritten when the source version is different from the destination version. Also by default, no objects will be deleted from either the source or the destination. I’ll just leave it with the default settings.</p>
<p>Then you specify when you want the transfer to happen. You can either run it now or schedule it to run at a particular time every day. I’ll just run it now. The transfer is going to take a little while, so I’ll fast forward until it’s done. It’s a bit tedious to transfer files manually like this, which is why scheduled transfers are so nice. For example, you could have it run automatically every day to check for new files in the source bucket and transfer them to the destination bucket. Alternatively, you could do this with a cron job that runs the gsutil command, but it’s much cleaner to do it this way.</p>
<p>If you have any experience with managing data, then you know that data just keeps growing and growing over time, and if you don’t implement something to keep that growth under control, then you will either run out of storage space or have runaway costs. Since Google Cloud has nearly unlimited storage resources, that means you could easily have runaway costs.</p>
<p>To prevent that, you can use object lifecycle management. There are two ways to control costs using lifecycle management. Based on age, creation time, or number of newer versions, either:</p>
<ul>
<li>Delete objects, or</li>
<li>Move objects to a cheaper storage class, such as Nearline or Coldline Storage</li>
</ul>
<p>You can manage object lifecycle policies through the Cloud Storage console, the “gsutil” command, or the Google API Client Libraries. I’ll show you how to do it with gsutil.</p>
<p>First, you need to create a lifecycle config file that contains the rules you want to set. Here’s an example in JSON format:</p>
<p>{</p>
<p>“lifecycle”: {</p>
<p> “rule”: [</p>
<p> {</p>
<p>  “action”: {“type”: “Delete”},</p>
<p>  “condition”: {</p>
<p>   “age”: 365,</p>
<p>   “isLive”: true</p>
<p>  }</p>
<p> },</p>
<p> {</p>
<p>  “action”: {“type”: “Delete”},</p>
<p>  “condition”: {</p>
<p>   “isLive”: false,</p>
<p>   “numNewerVersions”: 3</p>
<p>  }</p>
<p> }</p>
<p>]</p>
<p>}</p>
<p>}</p>
<p>The first rule says to delete any live object older than 365 days. This might seem like a short time to live, but it’s not as draconian as it looks because it won’t completely delete the object if you’ve enabled versioning on the bucket. It will archive it instead. However, if you don’t have versioning enabled, then a Delete action will completely delete objects matching the condition and there will be no way to get them back. This is why you should test your lifecycle rules on test data before applying them to production data.</p>
<p>The “isLive” parameter only matters if you’ve turned on versioning. If an object is live, it means that it’s the most current version of that object. If it’s not live, then it is one of the archived versions of that object.</p>
<p>Let’s see if versioning is enabled on this bucket. I’ll use the gsutil command to check. It says it’s suspended. What the heck does that mean? It means versioning is disabled. I don’t know why they say suspended instead of disabled.</p>
<p>To enable versioning, we just need to change “get” to “set on”. I have a file in the ca-example bucket called “examplefile” [Show that]. Now, I’ll upload a different version of “examplefile” and see if it archives the old version. OK, it’s uploaded, now I’ll run the “gsutil ls -la” command on that file. Yes, there are two versions of it now and they have different dates and sizes. Note that if you do an “ls -l” without the ‘a’ flag, then it won’t show the different versions, so make sure you include the ‘a’ flag.</p>
<p>OK, let’s get back to the lifecycle policy. The second rule says to delete any object that has at least 3 newer versions of itself, including the live version. Unlike the first rule, this one really will delete the object because when you delete an archived object, it gets deleted forever. Although this rule has an explicit condition that the object must not be live, you don’t actually need to put in that condition because if an object has three newer versions, then it can’t be live. It has to be an archived version.</p>
<p>So, looking at the big picture for these two rules, there are two possible scenarios, depending on whether versioning is enabled or not. If versioning is enabled, then the first rule will archive any object that is more than one year old, and the second rule will delete any object that has at least three newer versions of itself. If versioning is not enabled, then the first rule will delete any object that is more than one year old, and the second rule will not do anything.</p>
<p>Suppose that instead of deleting objects older than one year, you’d like to send them to Nearline Storage, which is significantly cheaper, and send objects in Nearline Storage that are older than 3 years to Coldline Storage, which is even less expensive.</p>
<p>Here’s a lifecycle config file that will implement this policy.</p>
<p>{</p>
<p>“lifecycle”: {</p>
<p> “rule”: [</p>
<p> {</p>
<p>  “action”: {</p>
<p>   “type”: “SetStorageClass”,</p>
<p>   “storageClass”: “NEARLINE”</p>
<p>  },</p>
<p>  “condition”: {</p>
<p>   “age”: 365,</p>
<p>   “matchesStorageClass”: [“MULTI_REGIONAL”]</p>
<p>  }</p>
<p> },</p>
<p> {</p>
<p>  “action”: {</p>
<p>   “type”: “SetStorageClass”,</p>
<p>   “storageClass”: “COLDLINE”</p>
<p>  },</p>
<p>  “condition”: {</p>
<p>   “age”: 1095,</p>
<p>   “matchesStorageClass”: [“NEARLINE”]</p>
<p>  }</p>
<p> }</p>
<p>]</p>
<p>}</p>
<p>}</p>
<p>The first rule moves objects that are older than one year from Multi_Regional Storage to Nearline Storage. </p>
<p>The second rule says that if an object is in Nearline Storage and it is at least 1,095 days old (which is 3 years), then it should be moved to Coldline Storage.</p>
<p>To apply this lifecycle policy, you type “gsutil lifecycle set”, then the name of the config file, which is “lc2.json” in this case, and then the URL for the bucket, which is “gs:&#x2F;&#x2F;ca-example” in this case. Again, make sure you apply a lifecycle policy to test data before you put it into production or you risk losing valuable data.</p>
<p>Once you’ve set up a lifecycle policy, you can monitor what it’s doing in two ways:</p>
<ul>
<li>Expiration time metadata</li>
<li>Access logs</li>
</ul>
<p>To see the metadata for an object, use “gsutil ls -La” on the object. I’ll just show you the first dozen or so lines of the output because it prints all of the access control list information, which we’re not interested in right now. The output from this command may or may not contain expiration time metadata (and it doesn’t for this file), but the lifecycle policy should add that metadata when it knows the date and time that an object will be deleted.</p>
<p>Bear in mind that updates to your lifecycle configuration may take up to 24 hours to go into effect. Not only will it take up to 24 hours before your new rules kick in, but your old rules may still be active for up to 24 hours, so if you discover a mistake in your rules, that bug can still be active for another 24 hours after you fix it, which is another reason why testing your rules on test data first is so important.</p>
<p>Expiration time metadata is useful to see when your lifecycle policy is planning to delete an object, but it won’t show any of the other potential operations, such as moving an object to another storage class. If you want to see all of the operations that your lifecycle policy has actually performed, then you can look at the logs.</p>
<p>If you haven’t already set up access logs for your bucket, then here are the commands you need to use. First create a bucket to hold the logs. Remember to change “ca-example-logs” to your own log bucket name.</p>
<p>gsutil mb gs:&#x2F;&#x2F;ca-example-logs</p>
<p>Then you have to give Google Cloud Storage WRITE permission so it can put logs in this bucket.</p>
<p>gsutil acl ch -g <a href="mailto:&#x63;&#108;&#111;&#x75;&#100;&#45;&#x73;&#x74;&#x6f;&#x72;&#97;&#103;&#x65;&#x2d;&#x61;&#110;&#97;&#108;&#x79;&#116;&#x69;&#x63;&#x73;&#64;&#x67;&#x6f;&#x6f;&#103;&#108;&#x65;&#x2e;&#x63;&#111;&#109;">&#x63;&#108;&#111;&#x75;&#100;&#45;&#x73;&#x74;&#x6f;&#x72;&#97;&#103;&#x65;&#x2d;&#x61;&#110;&#97;&#108;&#x79;&#116;&#x69;&#x63;&#x73;&#64;&#x67;&#x6f;&#x6f;&#103;&#108;&#x65;&#x2e;&#x63;&#111;&#109;</a>:W gs:&#x2F;&#x2F;ca-example-logs</p>
<p>Next, you can set the default object ACL to, for example, “project-private”. You don’t have to do this, but it’s a good idea for security purposes to keep your logs private.</p>
<p>gsutil defacl set project-private gs:&#x2F;&#x2F;ca-example-logs</p>
<p>And finally, you enable logging with the “gsutil logging set on” command.</p>
<p>gsutil logging set on -b gs:&#x2F;&#x2F;ca-example-logs gs:&#x2F;&#x2F;ca-example</p>
<p>Once you have logging set up, then you can go into your logging bucket in the console and the access logs will show up there after the lifecycle policy has made changes.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Cloud-SQL-Configuration"><a href="#Cloud-SQL-Configuration" class="headerlink" title="Cloud SQL Configuration"></a>Cloud SQL Configuration</h1><p>Suppose you’ve been running a Cloud SQL for MySQL instance for a while, and it’s starting to run out of storage space. There are a few ways you could deal with this. If the database contains lots of old data that you can delete to free up space, that might be a good option. If the extra usage is caused by a temporary spike, and the data will be deleted soon anyway, then you might want to leave the storage capacity at its current level. You have to be careful, though, because if the instance runs out of space, it could go offline.</p>
<p>But you also have to be careful about increasing the instance’s storage capacity because once you increase it, the change is permanent, and you can never decrease it. So you could be paying extra money every month for space that you don’t need. In most cases, though, you’ll want to increase the storage space to avoid problems.</p>
<p>The first way to increase the capacity is very simple. You can just edit the instance’s configuration and change the storage capacity setting. However, if you think you’ll need to increase the storage space again in the future, then you might want to consider enabling the “Automatic storage increase” setting. Note that this option is enabled by default now, so you’d only have to turn it on if you disabled it when you created the instance.</p>
<p>You should also consider setting the “Automatic storage increase limit.” Since you can never decrease the storage capacity of an instance, you may want to prevent a runaway increase by setting a limit. By default, it’s set to 0, which means there’s no limit other than the maximum that’s available for the machine type of the instance.</p>
<p>While we’re on the subject of defaults, there’s another setting you may want to consider changing. Every few months, Google applies updates to each Cloud SQL instance. This requires a reboot that typically takes only a few minutes, but you’ll probably want to set a maintenance window for when Google will do the update. You can specify a 1-hour window on any day of the week. If you don’t set a window, then by default, Google can perform the update at any time.</p>
<p>Amazingly, you can change almost all of the configuration settings for a Cloud SQL instance after you’ve created it, even the machine type and the zone! The instance will go offline for a few minutes when you change the machine type or the zone, though.</p>
<p>The only settings you can’t change after you’ve created an instance are the instance ID, the region, the MySQL version (which can be set to 5.6, 5.7, or 8.0), and the storage type (that is, either SSD or HDD). Also, once you’ve configured a private IP address for an instance, you can’t remove it.</p>
<p>Another issue you might run into is called replication lag. To achieve high availability for a Cloud SQL instance, you need to create a failover replica. With this configuration, every write operation on the primary database is also made on the replica. Normally, there’s only a slight delay before the update is performed on the replica, but there are times when the replica can fall behind, and there’s a significant lag before updates are made.</p>
<p>Although the replica will not lose any of the updates, a significant lag can cause a failover to take longer if the primary fails. If the replication lag becomes too high, then Google’s SLA will no longer be valid, so it’s important to address this issue.</p>
<p>To make sure you’ll know when replication lag occurs, you can set up an alert. You just need to configure Stackdriver to monitor the seconds_behind_master metric.</p>
<p>When replication lag is caused by a temporary spike in database updates, the replica will eventually catch up, and you don’t need to take any action. If the lag is continually high, then you could try recreating the replica. If that doesn’t work, then you may need to add RAM and disk to the replica. If that still doesn’t solve the problem, then you’ll likely have to shard your database into multiple instances to spread out the load.</p>
<p>And that’s it for Cloud SQL configuration.</p>
<h1 id="Cloud-CDN-Configuration"><a href="#Cloud-CDN-Configuration" class="headerlink" title="Cloud CDN Configuration"></a>Cloud CDN Configuration</h1><p>As you probably know, Cloud CDN is a service that caches your web content in Google’s delivery network. Then when a user goes to your website, they retrieve your content from the nearest CDN location rather than from your web server.</p>
<p>Cloud CDN is a great service for serving content to your users faster, but it does complicate things, so you may need to change some settings to optimize the cache. Here’s the first problem. When you update your content, how do you make sure the cache gets updated, too, so your users get the latest content? This might sound like a simple problem, but there are several different approaches to dealing with it.</p>
<p>First, you need to set an appropriate expiration time. If you have content that changes frequently, then you should set a short expiration time so users won’t get stale content from the cache. For example, stock prices change rapidly, so you’d set a very short expiration time for them.</p>
<p>You wouldn’t want to set a short expiration time for everything, though. For example, if you set a short expiration time for your company’s logo, then your users would frequently get cache misses, and the logo would have to be retrieved from the web server, which would defeat the purpose of using a CDN. And if your company changes its logo at some point, it wouldn’t be a big deal that it would take a while before your customers see the new one.</p>
<p>Another approach for dealing with content that changes infrequently is to use versioned URLs. The idea is that if you change the name of a piece of content, then it won’t be in the cache, so users will always get the latest version.</p>
<p>Here are three different ways to use versioning. You could add a query string with a version number in it. You could add a version number to the filename. Or you could add a version number in the path.</p>
<p>You might be wondering why you can’t just remove stale cache entries directly rather than relying on expiration times and versioning. Well, you can. It’s called invalidation, but you should only use it as a last resort because <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a> charges for invalidations and also enforces rate limits on how many invalidations you can do at a time.</p>
<p>Now that we’ve covered ways to prevent cache entries from becoming stale, we should also cover the opposite problem—how to reduce the number of cache misses. As I mentioned earlier, if the content that users need is often not in the cache, then response times will be slower. Aside from setting the expiration time appropriately, another way of improving the cache hit ratio is to use custom cache keys.</p>
<p>By default, Cloud CDN creates each cache key using the full URL, but this can be inefficient. For example, many sites serve up the same content regardless of whether the URL contains http or https. If you cache a separate copy of each piece of content for each protocol, then there would be a lot of cache misses. By using a custom cache key that doesn’t include the protocol, you’d only have one copy of each piece of content in the cache for both protocols, so you’d have way fewer cache misses.</p>
<p>Why? Suppose you have a web page that hasn’t been accessed for a while, so the cached version of that page has expired and is no longer in the cache. Then suppose a user browses the http version of that page. There’ll be a cache miss, and the page will be retrieved from the web server into the cache. When another user browses the https version of that page, there will be a cache miss again if the cache keys are based on the full URL. But if you use a custom key without the protocol, then the request for the https version of the page will result in a cache hit.</p>
<p>Similarly, you can create custom cache keys that leave out the host. If you have multiple copies of your website on different hosts, then it would make sense to only have one copy of your content in the cache.</p>
<p>Dealing with the query string can be a bit more complicated. If the content should always be the same for a URL regardless of what’s in the query string, then that’s easy. You can just create a custom cache key that leaves out the query string. But if certain parts of the query string will result in different content being retrieved, then you need to specify which parts of the query string to include in the cache key.</p>
<p>Note that you can leave out any combination of protocol, host, and query string when you create your custom cache keys.</p>
<p>And that’s it for Cloud CDN configuration.</p>
<h1 id="Instance-Startup-Failures"><a href="#Instance-Startup-Failures" class="headerlink" title="Instance Startup Failures"></a>Instance Startup Failures</h1><p>What can you do if your VM instance fails to boot up completely? You can’t use SSH because the SSH server isn’t running yet. If you’re running the VM on your desktop, then you could look at the console. But how do you do that for a <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud</a> instance? Luckily, there’s a solution. You’d look at the serial port.</p>
<p>By default, you can see the output of the serial port by clicking on the instance, and then at the bottom of the page, you can click the view serial port button. This might be enough information to help you troubleshoot your problem, but it many cases, you’ll need to interact with the VM to see what’s going on. You’ll notice that there’s a button called connect to serial port, but it’s grayed out. How frustrating. To enable interactive access, you need to add meta data to the instance. This isn’t a terribly user friendly way of enabling a feature, but it’s actually not too difficult.</p>
<p>First you have to decide whether you want to enable interactive access for an individual instance or for an entire project. If you enable it on individual instances, then you’ll have to enable it manually for every instance. For convenience, you might want to enable it for an entire project, but there is a higher security risk enabling serial port access for all of your instances because there is currently no way to restrict access by IP address. So hackers could try to break in to any of your VMs through the serial port. It wouldn’t be easy though, because they’d need to know the correct SSH key, username, project ID, zone, and instance name.</p>
<p>To enable interactive access to an individual instance, you can use this gcloud command gcloud compute instances add dash meta data. Now put in the instance name, which is instance dash one in my case, and then dash dash meta data equals serial dash port dash enable equals one.</p>
<p>Now when I refresh the page, the connect to serial port button lights up. If I click on it, then it brings up another window where I can interact with the serial console.</p>
<p>By the way, if you’re connecting to a Windows instance, then you’ll need to go into the drop down menu and select port two.</p>
<p>If the serial port output showed that you have a problem with the file system on your boot disk, then you can attempt to fix it by attaching the disk to another instance.</p>
<p>First, delete the instance, but be sure to include the keep disks option. Notice that it still gives me a warning about deleting disks, even though I used the keep disks option. That’s normal.</p>
<p>Then create a new instance. I’ll call it debug dash instance.</p>
<p>Now attach the disk that we saved from the original instance. Notice that by default the name of a boot disk is the same as the name of the instance, instance dash one in this case. You can also add the device name flag so it will be obvious which device corresponds to this disk, which will be helpful in a later step.</p>
<p>Then SSH into the new instance.</p>
<p>Now you need to find out the device name for the debug disk. Look in the dev disk by ID directory.</p>
<p>Remember when I mentioned that naming the disk device would be helpful? You can see that the debug disk is SDB. The file system is on the first partition, or part one. So the device name we need to use is SDB One. Now you can run an fs-check on it.</p>
<p>Of course, I’m doing this on a good disk, so fs-check doesn’t see any problems. But if this disk had come from an instance that couldn’t boot properly, then there’s a good chance that an fs-check would find lots of problems.</p>
<p>Let’s pretend that fs-check had to clean the file system and it was successful. After that, you should verify that it will mount properly.</p>
<p>You should also check that it has a colonel file. It does, but before you celebrate, you should check one more thing, that the disk has a valid master boot record.</p>
<p>It printed out information about the file system, so this disk is good to go. Now you would create a new instance and use this disk as its boot disk.</p>
<p>That took a bit of work, but it was relatively straightforward. For a tougher challenge, try the next lesson where we tackle SSH errors.</p>
<h1 id="SSH-Errors"><a href="#SSH-Errors" class="headerlink" title="SSH Errors"></a>SSH Errors</h1><p>We’ve covered what to do when your instance won’t boot at all, but what if it boots, at least partway, but you can’t connect to it using SSH?</p>
<p>When you can’t connect with SSH, then a feeling of helplessness can set in. But, don’t worry, there are quite a few things you can do to resolve the issue.</p>
<p>Before we get into how to troubleshoot the “Connection failed” error you see here, let’s go over how to deal with the “Permission denied” error. This error can occur for obvious reasons, such as not using the right flags on your ssh command when you try to connect, but there are also some potential issues that are specific to Google Cloud Platform.</p>
<p>To understand why, you need to know the different ways you can configure SSH connections on GCP instances. First up is “OS Login”, which is the preferred method. It lets you control access by using IAM roles. The advantage of using IAM roles is that they’re the standard way to control access to all other resources on GCP, too. OS Login can also manage your public SSH keys for you, and it even supports two-factor authentication.</p>
<p>If you can’t use the OS Login method for some reason, then you can add public SSH keys to metadata. One easy way to do it is to add project-wide SSH keys by putting them in a project’s metadata. Any user who has an SSH key in a project’s metadata can connect to any instance in that project with the exception of instances that specifically block project-wide SSH keys. The other way is to add SSH keys to the metadata of each instance. This gives you more fine-grained control of who can access individual instances, but it takes a lot more effort to manage.</p>
<p>Okay, so how might these different configurations lead to a “Permission denied” error? There are several different ways. For example, OS Login disables SSH keys in metadata, so if you try to connect to an OS Login-enabled VM using a key in metadata, you’ll get an error. The opposite configuration also generates an error. That is, if you try to use a key stored in an OS Login profile to connect to a VM that doesn’t have OS Login enabled, then it won’t work. Another scenario is if you try to use a project-wide SSH key to connect to a VM that has project-wide keys disabled.</p>
<p>All right, now let’s get back to the “Connection failed” error. Here’s how to troubleshoot it.First, check your firewall rules. By default, your network will contain a firewall rule that allows SSH traffic, but you should make sure that the rule wasn’t deleted or modified. The easiest way to check is to go into the Networking section of the Google Cloud console and click on Firewall Rules. There should be a rule called “default-allow-ssh” that allows traffic on tcp port 22. The source should be all zeroes (meaning it will allow SSH requests from any IP address) and the target should say “Apply to all” targets.</p>
<p>If you don’t have this rule, then you can easily create it. Click “Create Firewall Rule”. You don’t have to name it “default-allow-ssh”, but you probably should, just so it follows the convention of network name (that is “default”), allow, and protocol (that is, “ssh”). Change the priority to 65534. Leave “Direction of traffic” as “Ingress” and “Action on match” as “Allow”. Change “Targets” to “All instances in the network”. Leave the “Source filter” as “IP ranges”. In the “Source IP ranges” field, put 0.0.0.0&#x2F;0. And put “tcp:22” in the “Protocols and ports” field. Now click the “Create” button and that’s it. Let’s try connecting with SSH again and see if that fixed it. Great, it did.</p>
<p>If you didn’t have a problem with the firewall rules, but you still can’t get in with SSH, then try connecting to port 22 manually and see if the SSH server responds. Use the “nc” command on the external IP address of your instance and then specify port 22 . If you see the SSH banner, then you know that the network connection is ok and the SSH server is running. If you don’t see the banner, then it could be a problem with either the network connection or the SSH server.</p>
<p>Next, try accessing the serial console, which I showed in the previous lesson. Look at the serial port output to see if that will tell you why you can’t connect using SSH. </p>
<p>The next thing to check is if there’s a problem with your account. Try connecting with another username, like this.</p>
<p>You can put in whatever username you want and the gcloud tool will update the project’s metadata to add the new user and allow SSH access. I’m going to call it “newuser”.</p>
<p>If that didn’t work and your instance boots from a persistent disk (which is the case by default), then you can detach the persistent disk and attach it to a new instance. Of course, this will take down your existing instance, so if the instance is serving production users properly and you don’t want to cause an outage, then skip this procedure and go to the next one.</p>
<p>First, delete the instance and be sure to include the –keep-disks option.</p>
<p>Then create a new instance (I’m going to create it with the same name as the original one) and attach the disk that we saved from the original instance. Also add the “auto-delete&#x3D;no” option so the disk isn’t automatically deleted when you delete this instance.</p>
<p>Now SSH into the new instance. That worked. If your instance is serving production users properly and you don’t want to cause an outage, then you can follow this procedure instead of the one I just did.</p>
<p>First, create an isolated network that only allows SSH connections. This is because you’re going to clone your production instance and you don’t want the clone to interfere with your production services.</p>
<p>Now add a firewall rule to allow SSH connections to the network. Then create a snapshot of the boot disk. Now you can create a new disk with the snapshot you just created. Then create a new instance in the new network. Now attach the cloned disk.</p>
<p>By the way, Google recommends that you don’t give this instance an external IP address, but that makes it much more difficult to connect to it. Considering that the instance is sitting in a network that blocks everything except SSH requests, it shouldn’t cause any disruption to your production services even though it has an external IP address.</p>
<p>Now SSH into this instance. Although the cloned disk is attached to this instance, it isn’t mounted anywhere, so you’ll have to do that manually.</p>
<p>First create a mount point. Then see what the device name of the disk is. The boot disk is disk 0, so the extra disk we attached has to be disk 1. Then mount the filesystem.</p>
<p>Now you can finally try to debug why you can’t connect to the original instance using SSH. You can look through the logs, for instance.</p>
<p>That was a pretty complicated process, but that’s the sort of thing you have to do if you don’t want to disrupt your production service while you’re debugging it. Here’s a summary of what I just showed you. First, create an isolated network that doesn’t allow any connections. Then add a firewall rule to allow SSH connections. Next, create a snapshot of the boot disk. After that, you can create a new disk from the snapshot. Then create a new instance in the new network, and attach the cloned disk to it. Finally, SSH into the instance, and mount the disk so you can inspect it.</p>
<p>If you still can’t find the reason why your instance won’t accept SSH connections and you are able to restart the instance at some point, then you can use a startup script to gather information. If you’re not sure what to put in the startup script, then you can use one provided by Google.</p>
<p>The script will run the next time the instance boots, so when you’re ready, you can run <code>gcloud compute instances reset instance-1</code>. The script sends its output to the serial port, so look there for the debugging info.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Network-Traffic-Dropping"><a href="#Network-Traffic-Dropping" class="headerlink" title="Network Traffic Dropping"></a>Network Traffic Dropping</h1><p>Another problem you might run into is network traffic dropping between an instance and other systems. The first thing to check is the firewall rules for the network the instance is in. The default network has firewall rules that allow http, https and a few other protocols. If any of those rules were deleted, that could be the reason for your network issues.</p>
<p>If instead of putting the instance in the default network, you put it in another network that you created, then make sure you created the appropriate firewall rules. When you create a new network, it doesn’t come with any firewall rules. So you have to create some to allow any traffic.</p>
<p>Another potential reason for network traffic dropping, is the idle connection timeout. Idle TCP connections are disconnected after ten minutes. If your instance initiates or accepts long lived connections, then you should adjust the TCP keep alive settings. This will prevent connections from being dropped by the idle timeout.</p>
<p>Since the timeout is ten minutes, you need to set the TCP keep alive to something less than ten minutes so connections will never be idle for that long. You need to set the keep alive on either the instance or the external client, depending on which side initiates connections.</p>
<p>Be aware that you should only set a TCP keep alive if you are having problems with connection timeouts. If you are not having that sort of problem, then setting a TCP keep alive will just increase network traffic for no good reason.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to manage your Google Cloud infrastructure. Now, you know how to monitor and debug using stack driver, test your infrastructure under challenging conditions, manage your storage while keeping costs under control, and troubleshoot issues with instance and networks.</p>
<p>To learn more about <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know on the Cloud Academy Community Forums. Thanks, and keep on learning.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Optimizing-Google-BigQuery-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:21" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:21-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:37:12" itemprop="dateModified" datetime="2022-11-20T19:37:12-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to the “Optimizing Google BigQuery” course. I’m Guy Hummel and I’ll be showing you how to get the most from this big data service.</p>
<p>BigQuery is an incredibly fast, secure, and surprisingly inexpensive data warehouse, but there are ways to make it even faster, cheaper, and more secure.</p>
<p>To get the most from this course, if you don’t already have experience with BigQuery, then please take my Introduction to BigQuery course first.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>We’ll start with how you can reduce the amount of data that’s processed by your queries, which will also lower your costs. Then we’ll go into more depth on partitioned tables, which is one method for reducing processing requirements.</p>
<p>Next, we’ll talk about ways to speed up your queries, including denormalizing your data structures and using nested and repeated fields.</p>
<p>Finally, we’ll wrap up with how to use roles and authorized views for access control.</p>
<p>If you’re ready to learn how to get the most out of BigQuery, then let’s get started.</p>
<h1 id="Reducing-the-Amount-of-Data-Processed"><a href="#Reducing-the-Amount-of-Data-Processed" class="headerlink" title="Reducing the Amount of Data Processed"></a>Reducing the Amount of Data Processed</h1><p>BigQuery is relatively inexpensive to use. But if you process large amounts of data, your costs can add up quickly. At first, it looks like streaming and storage would be your highest costs. That would be true if you didn’t run many queries, but most organizations use BigQuery to run lots of queries. After all, query is even part of the name, so that must be what people use it for, right?</p>
<p>Although the streaming and storage costs are higher than the query costs on a per gigabyte basis, you only get charged for streaming once and you only get charged for storage once a month. With queries, on the other hand, you get charged every time you run a query, which can be hundreds, or even thousands, of times a month, so reducing how much data is processed by your queries is usually the best way to reduce your costs. Another benefit is that it often speeds up your queries too.</p>
<p>To see how to optimize our queries, let’s use some stock exchange data that Google makes available on Cloud Storage. First, create a dataset where you can put the tables. Click the down arrow next to your project name and select “Create new dataset”. Let’s call it “examples”.</p>
<p>Now create a table in the examples dataset. Change the Location to Google Cloud Storage. Here’s the path to the first stock exchange file.</p>
<p>There will be quite a bit of text to enter in this course, especially when we start writing queries, so if you don’t want to type everything yourself, you can copy and paste from a <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/optimizing-bigquery/">readme file</a> I put on Github. It’s at this URL.</p>
<p>Now call the Destination Table “gbpusd_201401”. For the Schema, click “Edit as Text” and put this in. Make sure you get rid of the text that was in this box already. Click “Create Table”…and it takes a little while to upload, so I’ll fast forward. I’m going to fast forward quite a few times throughout the course, so if some operations seem to take longer when you run them, it’s probably because of that.</p>
<p>Then go through the same process for the second file.</p>
<p>OK, now we can run a simple query to get a baseline before we try to optimize it. Click “Compose Query”. Then, before you do anything else, click on “Show Options” and uncheck the “Use Legacy SQL” box. I don’t recommend using Legacy SQL because you have to learn some non-standard syntax to use it. Now click “Hide Options” so it doesn’t clutter up the page. OK, click on the first table and then click “Query Table”. Let’s select star so we see how much data gets processed when we read all of the data. Let’s also get rid of the “LIMIT 1000” to make sure it’s reading the whole table.</p>
<p>Before you run it, see how much data it says it’s going to process. Click “Open Validator”, which is the green checkmark. It says it will process 46.4 meg. Now click “Run Query”. It warns us that we’ll be billed for all of the data in the table, even if we use a LIMIT clause. We actually want to read all the data in the table this time, so ignore the warning. Click “Run query”…and after a few seconds, it comes back and confirms that it processed 46.4 meg.</p>
<p>There are two basic approaches to reducing how much data is processed by a query. You can reduce the number of rows that are scanned and you can reduce the number of columns that are scanned.</p>
<p>One obvious way to try to reduce the number of rows processed is by using a LIMIT clause, but that warning message we just got said that we’d be billed for all the data in the table even if we use a LIMIT clause. Let’s try it to see if that’s true.</p>
<p>Add a “LIMIT 10” clause to the end of the query.</p>
<p>It still says this query will process 46.4 meg, but let’s run it anyway. And the warning message comes up again. Click “Run query”…it takes a few seconds, and it says that it processed the whole 46.4 meg again. It did run slightly faster, but that’s probably not because of the LIMIT clause. The processing time varies, even for the exact same query.</p>
<p>The bottom line is that we can’t reduce the number of rows scanned by using a LIMIT clause, so we’ll have to look at other approaches. I’ll show you how to do that later, but first let’s try limiting the number of columns scanned.</p>
<p>The obvious way to do that is to only select specific columns instead of doing a SELECT star. For example, on this table, instead of selecting all 5 columns, let’s just select 2 of them. Replace the star with “time, bid”.</p>
<p>That only processed 19.5 meg, so it actually worked. You probably also noticed that it didn’t give us a warning message this time. That’s because it doesn’t give you a warning when you select specific columns instead of selecting star.</p>
<p>Let’s go back to trying to reduce the number of rows processed. Have a look at the data by going to the Preview tab. Do you see a pattern? The data is sorted in ascending order by the time column. Maybe we could try to use the BETWEEN operator to only select rows that are between two dates. You can copy and paste this query from the Github file.</p>
<p>Since the validator accurately predicts how much data will be processed by a query, we don’t even have to run this to know that it won’t reduce the number of rows scanned because it says it will process 19.5 meg, which is how much it processed without the BETWEEN operator. It makes sense, when you think about it, because there’s no way for BigQuery to know that all of the data is properly sorted, so it still has to read the entire table.</p>
<p>Is there anything we can do to reduce the number of rows processed? Yes, there are a couple of ways, but they’re not easy. The first way is to put your data in separate tables. That’s kind of a brute force way of limiting your queries. It still scans the whole table, but since you only put some of your data in the table, that automatically limits how much data is processed.</p>
<p>The two tables we uploaded are actually split that way because there’s one for each month. The first is for January 2014 and the second is for February 2014. So all of the queries we’ve run so far have only been on the January data. If, instead, all of the data for 2014 were in one file, then we would have processed about 12 times as much data on every query.</p>
<p>In a normal relational database, breaking your data into tables like this is not recommended. Instead, you would use an index. But BigQuery doesn’t support indexes, so you can’t do that.</p>
<p>If you break your data into multiple tables, then how can you run queries across the tables when you need to? For example, what if you needed to run a query across all of the 2014 data? Would you have to write a long query with a UNION of all 12 tables? No, because fortunately BigQuery supports using wildcards in your table references.</p>
<p>Here’s how you would run a query across the two stock market tables. Let’s select the earliest time and the latest time from the tables. To query both tables, you just need to put in a star as the last character of the table name, which will match both 1 and 2. The wildcard has to be the last character of the table name.</p>
<p>When you run the query…you’ll see that it took the mintime from the first table because it’s in January and it took the maxtime from the second table because it’s in February.</p>
<p>You’ll recall that I mentioned that there are two ways to reduce the number of rows processed. I’ll tell you how to use the other method in the next lesson.</p>
<h1 id="Partitioned-Tables"><a href="#Partitioned-Tables" class="headerlink" title="Partitioned Tables"></a>Partitioned Tables</h1><p>In the last lesson, I showed you how you could break your data into tables to limit how much data your queries process. But that seems kind of awkward, doesn’t it? There are, in fact, quite a few disadvantages to this approach. First, you have to get your data into all of these tables somehow, probably by writing a program. Second, the more tables there are in your query, the worse your query performance is. And third, you can’t reference more than 1,000 tables in a single query. You might think you’re not likely to ever hit that limit, but it’s not as unlikely as it sounds. If you collect a lot of data every day, then you may want to have a separate table for every day. Then if you had 3 years worth of data, that would be more than 1,000 tables.</p>
<p>BigQuery has a solution for this called partitioned tables. The way it works is you put all of your data in one table, but in a separate partition for every day. Then you can limit your queries to particular days and it won’t scan the rest of the table.</p>
<p>Of course, you still have the challenge of how to divide your data into the right partitions, just like you would with dividing your data into separate files with the other approach. If you’re starting fresh and want to stream new data into a table every day, then it’s very easy. You can create an ingestion-time partitioned table, and when you stream or upload data to it, BigQuery will automatically put it in the partition for that day. It also creates a pseudo-column called _PARTITIONTIME that contains the date for each data record.</p>
<p>It wouldn’t be very useful to upload our existing data to an ingestion-time partitioned table, though, because all of the data would go into today’s partition regardless of what date was in each record.</p>
<p>The solution is to create a time-unit column-partitioned table. You just need to tell BigQuery which column contains the date, and it will put each data record into the right partition.</p>
<p>We can copy the data from our existing table and put it into a new partitioned table in a single command. You can copy it from the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/optimizing-bigquery/">repository</a>.</p>
<p>We use “bq query” because we’re going to query the existing table to get the data. We set the “use_legacy_sql” flag to “false” so it’ll use standard SQL. Then we use the “replace” flag, which isn’t strictly necessary, but if you need to run the command again, then it will overwrite the destination table instead of appending to it. Then we tell it what to call the destination table. I used the same name as the original table except with a ‘p’ for ‘partition’ at the end. To make this a partitioned table, we use the time_partitioning_field flag and set it to the column that contains the date, which is called “time” in the original table. Finally, we run a “SELECT *” to get all of the data from the original table.</p>
<p>By the way, if you wanted to partition the data based on a different unit of time than a day, you’d need to add the “time_partitioning_type”, which can be set to day, hour, month, or year. If you don’t set it, then it defaults to “day”.</p>
<p>I’m going to run the command from Cloud Shell, which is right here. I need to authorize it.</p>
<p>Alright, it’s done. You’ll notice that I recorded this demo after Google changed BigQuery’s user interface, so that’s why it looks different.Now let’s have a look at the partitioned table it created. It looks exactly the same as the original table. The only indication that there’s anything different about it is this note up here saying that it’s a partitioned table.</p>
<p>Let’s say you wanted to retrieve records only from January 9th and 10th. Here’s the SQL to do that.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT time, bid</span><br><span class="line">FROM examples.gbpusd_201401p</span><br><span class="line">WHERE time</span><br><span class="line"> BETWEEN TIMESTAMP(&#x27;2014-01-09&#x27;)</span><br><span class="line"> AND TIMESTAMP(&#x27;2014-01-10&#x27;)</span><br><span class="line">ORDER BY time ASC</span><br></pre></td></tr></table></figure>

<p>It’s basically the same as a query we tried to run earlier except that it’s querying the partitioned table instead of the original one. What a difference that makes. It says it will only process 1.3 meg. When we tried to do it with the original table, it said it would process 19.5 meg. Let’s run it to make sure it works. Yes, it only processed 1.3 meg.</p>
<p>Before we move on, I should mention another really useful feature of partitioned tables. If you have more data being added to a table every day, you may want to delete the oldest data at some point. You can configure a partitioned table to do this automatically by setting a partition expiration time. For example, you could say that each partition will be deleted after it’s 12 months old. This would ensure that you only have the latest 12 months worth of data in your table. You can also set an expiration time on an entire table if you want.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Denormalized-Data-Structures"><a href="#Denormalized-Data-Structures" class="headerlink" title="Denormalized Data Structures"></a>Denormalized Data Structures</h1><p>Now you know how to reduce the amount of data BigQuery processes, which will reduce your costs, but it won’t necessarily reduce how long it takes your queries to run. There are different strategies for doing that, but we might have to break some database structure rules.</p>
<p>One of the core principles of good schema design is normalization, which is basically to remove redundancy from data. Some of you are probably saying, “That’s a dramatic oversimplification of the concept, and removing redundancy is more of a result of normalizing data.” You’re right, of course, but for our purposes here, the lack of redundancy is what matters. I should mention, though, that not only does normalization reduce the amount of space taken up by the data, but it also improves data integrity, because if the same information is stored in multiple places, you could end up with inconsistencies between the copies.</p>
<p>Having said all that, in BigQuery, you may want to do the unthinkable and denormalize your data and add redundancy into it. Why would you do that? Well, because it can make your queries much faster, and because the downsides of having redundant data are not a problem for what we’re doing. First, data integrity is not an issue because BigQuery isn’t a transaction processing system. We’re just using it for reporting, not for recording transactions, so we’re not going to create inconsistencies in the data, since we’re not going to modify the data at all.</p>
<p>Second, the extra cost associated with storing and querying extra copies of data is often outweighed by the much faster query times. Why does denormalization make queries so much faster? Because when you query a normalized database, you will often have to join multiple tables together, but joining tables is a very time-consuming operation. It’s not very noticeable when the tables are small, but as the tables get larger, the time required to join them increases exponentially.</p>
<p>This performance comparison done by Google shows just how much of a difference it can make. The slope of this line gets so steep that it would take a ridiculously long time to join tables that have billions of rows.</p>
<p>Let’s go through an example of how to denormalize a data structure. We’ll use some MusicBrainz data, which is a freely available encyclopedia of music. Here are three data files to import into tables.</p>
<p>Create a new table in the examples dataset. Change the Location to Google Cloud Storage and paste the Data File URL. Then change the File format to JSON. Call the table “artist”. Under the Schema, click “Edit as Text”, then paste in the contents of the schema file…and create the table.</p>
<p>Now do the same for the other two tables. I’ll show the process, but I’ll speed it up so it all happens in 15 seconds, so hang on.</p>
<p>OK, here’s how these three tables are organized. They’re normalized, so there isn’t any redundant data. Now we’re going to take columns from these three tables and combine them into one table. We’re going to join these tables and save the results so that when we need to run queries in the future, they don’t have to go through the join step first, which will save a lot of time. We’re doing an inner join on the three tables using the artist. The resulting table will have lots of duplicate information, such as multiple copies of an artist’s name and multiple copies of a recording’s name.</p>
<p>Click Show Options and set the Destination Table to “recording_by_artist”. While you’re here, make sure the “Use Legacy SQL” box is not checked. Alright, now run the query, which should take about 30 or 40 seconds.</p>
<p>Since this query took a long time, it says, “We can notify you when long-running queries complete.” That could be helpful in the future, so click “Enable notifications”. Then you have to click Allow.</p>
<p>Let’s see how much extra space it takes up. The recording table takes up about 1.5 gig…and the other two tables…bring the total up to about 1.7 gig. The denormalized table takes up about 2.3GB, so it’s about 35% bigger, but the speed gains would be worth it if we were dealing with larger tables.</p>
<p>Let’s get rid of the Destination Table option so we don’t accidentally overwrite it. This table only has about 18 million rows, so the speed gain won’t be very noticeable. One benefit is much simpler queries, though. Compare this…with this.</p>
<p>Both queries do exactly the same thing.</p>
<p>You can see why this table takes up more space. The name “Red Elvises” is repeated many times. In the original tables, it only had the artist’s ID for each recording, and the artist’s name was only listed once in the artist table.</p>
<p>By the way, you probably wouldn’t use the web interface to denormalize data unless it’s a one-time conversion. If you need to denormalize data on a regular basis, then you would want to automate the process using Cloud Dataflow or something similar.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Nested-Repeated-Fields"><a href="#Nested-Repeated-Fields" class="headerlink" title="Nested Repeated Fields"></a>Nested Repeated Fields</h1><p>Denormalizing data might seem like a pretty clunky way to increase query performance. Is there a way to improve performance without having to add lots of redundant data? Yes. BigQuery supports something called “nested repeated fields”. This is a way to combine data into one table without redundancy.</p>
<p>Google has provided a very small data file to demonstrate this concept. First, open it in your browser and then right-click and do a Save As.</p>
<p>Then upload it into a new table. Change the file format to JSON. Call it “persons_data”. Now open the schema file…and copy the contents here. It’s pretty hard to understand the schema by looking at this, but it will be easier to see once you’ve created the table.</p>
<p>Now have a look at the schema. Some of the fields have a dark gray background, like phoneNumber. And under phoneNumber, you’ll see that areaCode and number look like subfields.</p>
<p>It gets more interesting with the children and citiesLived fields. Not only do they have nested fields under them, but they are also repeated fields, so each person can have multiple children and citiesLived.</p>
<p>Let’s go to the Preview to see what the data looks like. This is pretty different from a normal record because it takes up two rows and there are blanks at the beginning of the second row. One thing that can be confusing is that it looks like the child Jane lived in Seattle in 1995 and the child John lived in Stockholm in 2005 because in a normal record, all of the pieces of data on one row make up the entire record. But with repeated records, that’s not the case.</p>
<p>It’s easier to see if you look at the next record. First of all, either Mike Jones has a really hard time deciding where he wants to live or there’s something wrong with this data. Anyway, here it’s obvious that the children fields are not directly related to the citiesLived fields. Another way to see this is to look at the JSON representation of this record. Click on the JSON button. It’s not as easy to read as the table view, but it makes it very obvious how the record is organized.</p>
<p>To refer to a nested field, you can use the dot notation that it shows in the schema. For example, to get a list of all of the people in the table and their phone numbers, you would run this.</p>
<p>You refer to the phone number as phoneNumber.number. You don’t need the “LIMIT 1000” because there are only 3 records in this table. It doesn’t hurt to leave it there, but I prefer to keep unnecessary clutter out of my queries.</p>
<p>If you want to refer to a nested repeated field, though, then it takes a bit more work. Let’s say you want to see who has lived in Austin. It’s giving us an error because it doesn’t recognize the “place” field. That’s because it’s a nested repeated field, so you have to use the UNNEST function. First, put in a comma after the table name. Then unnest citiesLived. You’ll notice that the error went away because now that citiesLived is unnested, we can refer to the field as just “place” instead of citiesLived.place. Let’s also select place, so we can verify that the query is working properly.</p>
<p>It came back with the correct results because Anna Karenina and Mike Jones both lived in Austin, but John Doe didn’t.</p>
<p>What happens if you don’t specify the WHERE clause? Will it print all the places for each person?</p>
<p>Yes, it does, although it repeats the person’s name beside every city, unlike what we saw in the Preview tab. The records also come up in random order, because we didn’t specify an ORDER BY clause.</p>
<p>What if you do a SELECT *? Will it look like what we saw in the Preview tab?</p>
<p>Yes, it looks like the Preview tab…or does it? There are 9 rows in the result, but there are only 3 in the table. What’s going on? It’s because we left UNNEST(citiesLived) in the query. Now it’s very clear what the UNNEST does. It creates two new columns with the contents of the nested fields. And since our query says to select from the table and from UNNEST(citiesLived), it’s actually doing a JOIN. That’s what the comma between the two means. In fact, you could replace the comma with JOIN and it would work the same. There are 9 citiesLived records in total, one for each city that each person has lived in, which is why there are now 9 records in the result.</p>
<p>OK, so if we remove UNNEST(citiesLived), will it look like the Preview tab?</p>
<p>Ignore the warning. Yes, it does, although the records are in random order, of course.</p>
<p>In this example, we uploaded a JSON file that already contained nested repeated data. If you want to do this with your own data and it’s not already in that format, then you’ll have to create it yourself. There are many ways of doing this, such as using Cloud Dataflow, but that’s outside the scope of this course.</p>
<p>If you find that you frequently need to unnest the same repeated fields in your queries, then you might want to create a view.</p>
<p>Here’s a really simple example. Open the query where you selected the person’s name and place, but didn’t use a WHERE clause.</p>
<p>Then click Save View. For the table name, use cities_by_person. You’ll notice that the icon to the left of the name is different from the other ones. That’s because a view isn’t a table, even though it asked us to put in a table name when we created it. If you click on the view, you’ll see that the schema looks like a table’s schema, but if you click on Details, you’ll see that it’s quite a bit different.</p>
<p>In fact, it doesn’t store any data. It just stores the query. So when you use a view in your queries, it actually runs the query again. So why would you want to use a view if it just runs the query again anyway? Well, it can make your queries less complicated. Here, I’ll show you.</p>
<p>First, replace the table name (persons_data) with the name of the view (cities_by_person). Now you can remove UNNEST(citiesLived). And remove the comma too. Let’s put in a WHERE clause to make it a useful query. Let’s search for all of the people who have lived in Stockholm.</p>
<p>OK, I realize that it probably wasn’t worth the effort of creating the view, and changing our queries to use the view, all to make the queries a little simpler, but if you were regularly running queries with multiple levels of UNNESTs, then it might be handy to create a view.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Access-Control"><a href="#Access-Control" class="headerlink" title="Access Control"></a>Access Control</h1><p>Data can be one of the most precious resources an organization owns. So it’s important to keep tight control of not only who’s allowed to read your data, but also who’s allowed to modify or delete it. That might seem pretty basic, but doing this with BigQuery is more complicated than it sounds.</p>
<p>BigQuery provides several layers of access control. The top layer is primitive roles, which act at the project level. They’re administered through IAM, the Identity and Access Management system.</p>
<p>There are three primitive roles: owner, editor, and viewer. When you add a member to a project, you can assign them one of these roles, which will apply to all Google Cloud Platform services, not just BigQuery. A viewer can view all datasets and run jobs (such as queries). Editors have viewer permissions, but can also modify or delete all tables. They can’t modify datasets, but they can create new datasets. An owner has editor permissions, but can also delete all datasets and see all jobs for all users in the project.</p>
<p>Primitive roles are fine as long as you have these very simple access control requirements:</p>
<ul>
<li>First, each user can have the same permissions for all GCP resources in a project, not just BigQuery. For example, if a user has the Editor role, then they’ll be an Editor not only for BigQuery, but also for Google Cloud Engine instances or any other resources in the project. If you only have BigQuery enabled in this project, then this isn’t an issue.</li>
<li>Second, each user can have the same permissions for all datasets in a project. For example, if each team has owner permissions for their team’s datasets, then they’ll also have owner permissions for other team’s datasets in this project.</li>
<li>Third, you don’t need to separate data access permissions from job-running permissions. For example, if a user has permission to view data in a project, then they can also run queries on that data.</li>
</ul>
<p>If all of these requirements are true for a project, then primitive roles are a good solution, but otherwise, you’ll need to use predefined roles.</p>
<p>There are six predefined roles. The dataViewer, dataEditor, and dataOwner roles are essentially the same as the primitive roles except for two things: First, you can assign these roles to users for individual datasets, and second, they don’t give users permission to run jobs or queries. Those permissions can be granted through the user and jobUser roles. A jobUser can only start jobs and cancel jobs. A user, on the other hand, can perform a variety of other tasks, such as creating datasets. The admin role gives all permissions.</p>
<p>Here’s how you can use predefined roles to give more fine-grained access than primitive roles in each of these situations:</p>
<ul>
<li>To give a different level of access to BigQuery than to other GCP resources in a project, use BigQuery roles, such as BigQuery Data Editor. Then also use predefined roles for any other GCP resources they need to access, such as App Engine Admin.</li>
<li>To give a user or group a different level of permissions for a dataset in a project, click the down arrow next to the dataset and select “Share dataset”. Then add a user or group by putting in their email address. Then select one of the three roles (owner, editor, or viewer).</li>
<li>To give job-running permissions without giving data access permissions, select either BigQuery User or BigQuery Job User. In most cases, you should give BigQuery User because it lets the user list datasets and tables as well as create new datasets. Job Users can only run jobs.</li>
</ul>
<p>You might be wondering why you would want to separate data access permissions from job-running permissions because it would seem like most users would need both. That’s true in many cases, so remember that you have to assign both types of roles to users in order for them to look at the data and run queries (unless you’re using primitive roles). But there are situations when you might want to give only one or the other. For example, if you have an application that monitors the size of your tables, then you might want to assign only the BigQuery Data Viewer role to its service account. That way, even if the application developers accidentally make a change to the program that would cause a query to run, it will be disallowed. Why would that matter? Because queries incur a cost and a program could potentially run up some hefty charges if there’s a bug.</p>
<p>So far, I’ve only shown you how to set permissions at the project and dataset levels. There’s a way to set permissions at the table level and even to particular data within a table, but it’s a much more complicated process. The only way to do it is to use something called an authorized view.</p>
<p>We already created a view in the last lesson. What’s different about an authorized view is that it allows users to access the results of a query without giving them access to the tables that were queried. So, for example, if you didn’t want a particular group of users to have access to certain columns in a table, you could run a query that didn’t include those columns, and then save the results as an authorized view for that group.</p>
<p>To make this work, you need to perform four steps:</p>
<ul>
<li>First, create a separate dataset to store the view. You need to do this because if you were to put the view in the same dataset as the original tables, then the group would be able to access the tables too and not just the view.</li>
<li>Second, create the view in the new dataset.</li>
<li>Third, give the group read access to the dataset containing the view.</li>
<li>Fourth, authorize the view to access the source dataset.</li>
</ul>
<p>This assumes that you’ve already given the group permission to run queries in the project.</p>
<p>Let’s say you wanted to give a group called “team1” access to only the name, age, and gender fields in the persons_data table. Before I start, I should mention that if you don’t currently have permission to assign roles to other users, then you won’t be able to do all of the steps I’m about to show you.</p>
<p>OK, first create the new dataset. Call it “shared_views”.</p>
<p>Then, run a query to select the name, age, and gender fields from the table.</p>
<p>Now click “Save View”. Change the dataset to “shared_views” and call the table “persons_view”.</p>
<p>Then click the down arrow to the right of shared_views and select “Share dataset”. In the menu on the left, select “Group by e-mail” since we’re giving permission to a group, not a user. Then I’ll fill in the email address of the team1 group. If you have a group you can assign, then use that one. Leave the permission on “Can view” and click Add. You might want to uncheck “Notify people via email” if you’re testing this with an actual group of people. And click Save Changes.</p>
<p>Now that team1 has read access to the view, the only thing left to do is to give the view read access to the persons_data table. We need to do this because the view takes on the permissions of the person using it, and since team1 doesn’t have access to the persons_data table, they’d get an error if they tried to use this view.</p>
<p>Now we’re finally at the point where we’re going to turn this into an authorized view. In the menu next to the examples dataset, select “Share dataset”. In the menu at the left, select “Authorized View”. Then click “Select View”. Change the dataset to “shared_views” and put “persons_view” for the table. Click OK, click Add, and save the changes.</p>
<p>Now users in team1 will be able to run queries on this view, even though they don’t themselves have access to the persons_data table.</p>
<p>Before we go, you’ll probably want to delete everything you’ve created in this course. Fortunately, that will be very easy. Click the menu next to examples and select “Delete dataset”. Type in “examples” to confirm that you want to delete the dataset and all of the tables in it. Now do the same thing for the shared_views dataset.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to optimize BigQuery. Now you know how to reduce the amount of data processed by queries, speed up your queries through denormalization, and use access controls on projects, datasets, and tables.</p>
<p>To learn more about BigQuery, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know in the Comments tab below this video. Thanks and keep on learning!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:19" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:19-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:41:16" itemprop="dateModified" datetime="2022-11-20T19:41:16-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-BigQuery-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-BigQuery-17/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Introduction-to-Google-BigQuery-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:18" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:18-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:37:24" itemprop="dateModified" datetime="2022-11-20T19:37:24-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-BigQuery-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-BigQuery-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to “Introduction to Google BigQuery”. My name’s Guy Hummel, and I’m a Google Certified Professional Cloud Architect and Data Engineer. If you have any questions, feel free to connect with me on LinkedIn and send me a message, or send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>This course is intended for anyone who’s interested in analyzing data on Google Cloud Platform.</p>
<p>To get the most from this course, it would be helpful to have some experience with databases. It would also be helpful to have some familiarity with writing queries using SQL, but it’s not a requirement.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>To save you the trouble of typing in the URLs and commands shown in this course, I’ve created a GitHub repository with a readme file that contains all of them. The link to the repository is at the bottom of the course overview below.</p>
<p>We’ll start by running some basic queries and saving the results.</p>
<p>After that, I’ll show you how to load data into BigQuery from files and from other Google services.</p>
<p>Then, you’ll see how to stream data into BigQuery one record at a time.</p>
<p>Finally, we’ll wrap up with how to export data from BigQuery.</p>
<p>But first, I’ll give you a quick overview of why you’d want to use BigQuery.</p>
<p>Data warehouses have been around for decades. When databases first became popular, they were primarily used for transaction processing (and that’s still the case today). But managers also needed to analyze data and create reports, which is difficult to do when the data resides in numerous databases across an organization. So data warehouses were created to collect data from a wide variety of sources, and they were designed specifically for reporting and data analysis.</p>
<p>If data warehouse technology has been around for so long, why did Google release BigQuery, and why would you use it instead of a more established data warehouse solution? Well, for two main reasons: ease of implementation and speed.</p>
<p>First, building your own data warehouse can be expensive, time consuming, and difficult to scale. With BigQuery, on the other hand, the only thing you have to do to get started is load your data into it. And you only pay for what you use, so you don’t need to spend a lot of money building capacity to handle peak periods.</p>
<p>Second, even if you do build your own high performance data warehouse, it will probably never be as fast as BigQuery because BigQuery can process billions of rows in seconds. This speed is especially valuable if you need to perform real-time analysis of streaming data, such as from online gaming systems or Internet of Things sensors.</p>
<p>Okay, now if you’re ready to learn how to crunch big data with ease, then let’s get started. We’d love to get your feedback on this course, so please give it a rating when you’re finished.</p>
<h1 id="Running-a-Query"><a href="#Running-a-Query" class="headerlink" title="Running a Query"></a>Running a Query</h1><p>To open BigQuery, go to the Google Cloud Platform console, then find BigQuery in the menu. Alternatively, you can type “bigquery” in the search bar, which is probably easier.</p>
<p>Suppose you wanted to see which US state had the most babies with the same name in one year. There’s a public dataset with baby name data available on BigQuery. If you look under bigquery-public-data, you’ll see one called “usa_names”. If you click on it, you’ll see two tables that are almost the same. We’ll use the first one.</p>
<p>When you click on the table, it brings up the schema. If you click on the Details tab, it’ll show you a description of the data in the table. Let’s make this bigger. If you click on the Preview tab, it’ll give you a sample of the data. If you click the “Query Table” button, it will even give you the skeleton of a SQL query.</p>
<p>However, the query that it put in isn’t complete. You can tell because the Validator circle at the bottom right is a red exclamation point, which means there’s a problem. To see why, click on it and open the Validator. It says the “SELECT list must not be empty”. Let’s “SELECT *” from the table, which, if you’re not familiar with SQL, means select everything. Now the exclamation point has turned into a green check mark, so it’s a proper query.</p>
<p>We should sort the results with the biggest number at the top, so use “ORDER BY number DESC” (for “descending”), and then, since we only need to see the top results, let’s put in a LIMIT of 10. This line is quite long now, so if you want to make it easier to read, select the “Format” option. That’s better.</p>
<p>Now click the “Run” button. It only takes a few seconds to run.</p>
<p>The top result is Robert in New York in 1947, with 10,025 occurrences. You might be wondering if we did something wrong with this query because all of the top 10 names are boy’s names. Let’s look only for girl’s names and see what happens. Add “WHERE gender &#x3D; ‘F’”. Remember to put quotes around the F.</p>
<p>Now it makes sense why we only saw boys’ names before. The highest number of occurrences for a girl’s name was “Mary” in Pennsylvania in 1918, with 8,184 occurrences. Although that’s a lot of Marys, there were 9,054 Roberts in New York in 1951 and that was the 10th highest number of occurrences, so no girl’s names showed up in the top 10.</p>
<p>Before we run any more queries, let’s see how much this is costing us. I’ll cover that in the next lesson.</p>
<h1 id="Pricing"><a href="#Pricing" class="headerlink" title="Pricing"></a>Pricing</h1><p>There are two components to BigQuery pricing: storage and queries.</p>
<p>BigQuery’s storage charges are incredibly cheap. It costs two cents per gigabyte per month, which is the same price as Cloud Storage Standard. What’s even better is that if you don’t edit a table for 90 days, then the price for that table drops to one cent per gig per month until you modify the data in the table again. That’s as cheap as Nearline Storage! In fact, it’s even cheaper because when you read data from Nearline Storage, there is a one-cent per gig charge. With BigQuery storage, you aren’t charged for reading data at all.</p>
<p>Since we’ve only been using public datasets so far, there won’t be any storage charges.</p>
<p>The only other charge is for queries. (There’s also a charge for streaming data to BigQuery in real-time, but that doesn’t apply to these examples and I’ll cover it in another lesson.) For queries, the first terabyte per month is free. After that, it costs $5 per terabyte, which is half a cent per gigabyte. Wait a minute, didn’t I just say that you aren’t charged for reading data from BigQuery storage? Yes, that’s true because BigQuery charges query fees regardless of where you read the data from. For instance, if you query a dataset that’s in Cloud Storage, then you get charged at the same rate that you would from querying a dataset in BigQuery storage, so the charge isn’t for reading – it’s for processing.</p>
<p>For high-volume customers, there’s also flat-rate pricing, but it’s only worthwhile if you spend at least $2,000 per month. It only applies to query costs and not storage, which is still separate.</p>
<p>To see how much data is processed by a query, look in the Validator message area. Since there isn’t an error in the syntax, now it’s showing how much data would be processed by the query above. In this case, it’s 163 MB. Considering that the first terabyte of processing in a month is free, this won’t cost us anything, but even if we were already over the 1 terabyte mark this month, it wouldn’t cost much. How much? Less than a tenth of a cent. I’d say that’s pretty reasonable.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Saving-Query-Results"><a href="#Saving-Query-Results" class="headerlink" title="Saving Query Results"></a>Saving Query Results</h1><p>Now that you know how inexpensive it is to store data on BigQuery, you may want to save some of your query results there too.</p>
<p>It’s very easy to do that. First, though, since you don’t have a dataset to put tables in, you’ll need to create one. Click your project name and then click “Create Dataset”. You need to give it a name, so call it “babynames”. That’s the only thing you have to specify in this dialog box, but if you want to, you can set the location, which you might want to do for compliance reasons. You can also set an expiration time so that a certain number of days after a table has been created, it will be automatically deleted. You would do this if it’s temporary data that you don’t want to have to remember to delete later. Now click “Create dataset”, and you should see your new dataset show up under your project name.</p>
<p>Great, now we’re ready to run a query. Let’s open one of our previous queries, so we don’t have to type a new one in. If the list of your previous queries isn’t already showing, then click “Query History”. Click the button to the right of your first query to bring it up again.</p>
<p>Now before you re-run this query, select “Query settings” from the menu…and then select “Set a destination table for query results”. It has already set the right project and dataset name. Now we just need to tell it which table to use. If the table doesn’t already exist, it’ll create it, so you can type in whatever table name you want. Let’s call it “babynames_top10”. Click Save and then click “Run”.</p>
<p>Now if you click on the babynames dataset, you’ll see that it created the “babynames_top10” table. Then click the “Preview” tab and you’ll see the results from the query.</p>
<p>What if you decide to save the results to a table after you’ve run the query? That’s easy too. Click “Query History” and click on the first query. We didn’t specify that the results should be saved to a table when we ran this query the first time, but if you scroll down, you’ll see that it saved the results to a temporary table. Click that to see the table. Now you can click “Copy Table” and specify where you want to save it. You have to select the dataset first and then type in a new table name.</p>
<p>One more thing to be aware of is that if you don’t specify a destination table and it puts the results in a temporary table, the temporary table stays in cache for about a day. So if you run the query again within 24 hours, it’ll retrieve the cached copy and you won’t be charged for the query.</p>
<p>However, if you run a query again and specify a destination table, like we just did, then it won’t read the data from cache. So let’s run it again without the destination table option.</p>
<p>If you go to the “Job information” tab, you can see that it didn’t process any bytes because the results were cached. Also, the duration of the query was zero seconds because it just retrieved the results from cache rather than running the actual query. Of course, since we could have just looked at the results in the temporary table, there may not seem to be a lot of point in re-running the query. That’s probably true if you’re using the web interface, but if you’re using the bq command or the BigQuery API, then it might be useful in some cases.</p>
<p>Before we go, let’s get rid of the table we created. Click on the dataset name, and then click “Delete Dataset”. This will delete the dataset and all of the tables in it, so you have to type the name of the dataset before it’ll delete it. </p>
<p>Okay, that’s it for this lesson.</p>
<h1 id="Loading-Data"><a href="#Loading-Data" class="headerlink" title="Loading Data"></a>Loading Data</h1><p>It’s great having public datasets in BigQuery that you can use, but what about analyzing your own datasets? How do you get them into BigQuery? There are many ways to do this. If your data is already in another Google service, then there’s usually a way to get it into BigQuery, although sometimes it requires an intermediate step. Some of the most commonly used sources are Cloud Storage, Google Drive, Firestore, Cloud Operations, Bigtable, and Google Analytics.</p>
<p>If your data isn’t in a Google service, then you can upload it to BigQuery through the web interface, the command line, or the API. Uploading through the web interface is the simplest, although it does have limitations.</p>
<p>Suppose you want to upload a dataset of the number of occurrences of all baby names across the US for a particular year. Since you don’t already have a copy of this dataset, you’ll need to download it from the Social Security website. The URL is in the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">repository</a> for this course. Then unzip the file. You can see that it contains one text file for each year from 1880 to 2019. Let’s have a look at the 2019 file.</p>
<p>It’s a very simple file, with a name, a gender, and the number of occurrences of that name for babies born in 2019. Although the filename extension is “txt”, it’s actually a comma-separated values file (or CSV). BigQuery can upload files in 5 formats: CSV, JSON, Avro, ORC, or Parquet.</p>
<p>Before you can upload a file, you need to create a dataset and table in BigQuery to hold it. Let’s call it “babynames”.</p>
<p>Now you need to create a table in that dataset, so click the “Create Table” button. Change the Source to “Upload”. If you click on the menu, you’ll see that you could also choose Cloud Storage, Google Drive, or Cloud Bigtable. If your data were in one of those three services, then you could load it into BigQuery from here or you could even leave the data where it is and make it an external data source, also known as a federated data source. However, the performance is usually slower when you query an external data source than if the data resides in BigQuery storage, so it’s often better to copy the data into BigQuery instead.</p>
<p>Okay, back to the task at hand. Set the Location to “Upload” and click the “Browse” button. Then select the “yob2019.txt” file. Change the file format to CSV. Now you need to give your table a name. Let’s call it “names_2019”.</p>
<p>You’ll notice there’s an option to automatically detect the schema and input parameters. That’s often a very handy feature because it saves you from having to enter the schema manually. Let’s see if it works with this data file.</p>
<p>It got an error. The message is a little bit cryptic, but here’s what happened. When BigQuery tries to detect the schema, it only looks at the first 100 records. In this file, the first 100 records all have ‘F’ in the second column. BigQuery assumes this means that the second column can be either an ‘F’ for “False” or a ‘T’ for “True”, so it sets this column to Boolean. When it tries to upload records with ‘M’ in that column, it gets an error because it’s expecting an ‘F’ or a ‘T’.</p>
<p>In situations like this, you have to specify the schema manually. Fortunately, it’s pretty easy in this case. First, I’ll redo everything except the schema.</p>
<p>Okay, now instead of asking it to auto-detect the schema, click “Add field”. The first field is the name, so type “name”. It’s a string, so the type is set correctly. The mode is set to “Nullable” by default, which means that this field can be empty for some records. If we wanted to make sure that no records were missing the name, then we would set the mode to “Required”. This file isn’t missing any names, but let’s leave it as Nullable anyway.</p>
<p>Now we’ll add the second field. Call it “gender”, and leave it as a string. The third field is the number of people who have this name, so call it “count”. It’s a whole number, so set the Type to “Integer”. Okay, now click “Create table”. It only takes a couple of seconds to upload all of the data into the table.</p>
<p>All right, go to the table and click on the Preview tab to see a sample of the data. That looks right. At this point, you could run queries on this table just like you did with the table in the public dataset.</p>
<p>Okay, that was all pretty easy, but remember when I said that the web interface has limitations? One big limitation is that you can only upload files that are 10 megabytes or less in size. There are lots of data files that are bigger than that. Here’s an example. It’s a 35 meg file that contains over 200,000 questions and answers from the game show, Jeopardy. You can find the URL for this file in the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">repository</a>. If you try to load it using the web interface, it’ll give you an error.</p>
<p>In cases like this, you need to use the command line. Alternatively, you could upload it to Cloud Storage first and then move it to BigQuery, but that would be a bit of a hassle, so it’s better to use the command line.</p>
<p>If you haven’t used any of Google Cloud’s command line tools yet (that is, gcloud, gsutil, kubectl, or bq), then you’ll have to install the Google Cloud SDK. The installation instructions are at this <a target="_blank" rel="noopener" href="https://cloud.google.com/sdk/docs/install">URL</a>. If you need to install the SDK, then pause this video while you do that. .</p>
<p>Okay, first you need to create a table to put data in, which we could do through the web interface again, but let’s use the command line for that too. Before we create the table, we need to have a dataset to put the table in. If we had used a more generic name for the previous dataset, instead of “babynames”, then we could have created a table for the Jeopardy data in that dataset too. Let’s not make that mistake again. What should we call it? Maybe we should put all of the data that we downloaded from the internet in it. We could call it “downloads” or something like that, but maybe we should just call it “public”.</p>
<p>The command for all BigQuery operations is “bq”. To create a dataset, type “bq”, and then “mk” for make, and then the name of the dataset, which is “public” in this case.</p>
<p>Okay, the dataset was created. Now we need to create a table. Let’s call the table “jeopardy”. There are a couple of ways to create the table. You could create an empty table and then upload the data into it or you could do it all in one step, which is usually easier.</p>
<p>To upload a file, type “bq load”, then the autodetect flag, which tells it to automatically detect the schema so you don’t have to specify the schema yourself. Then type the name of the table you want to load it into. If you haven’t set a default dataset, then you also need to specify the dataset name. In this case, you would type “public.jeopardy”. Then type the filename you want to upload, which is JEOPARDY_CSV.csv. If you’re not in the directory where the csv file resides, then you’ll have to put in the pathname to that file. It’ll take a little while to upload the file.</p>
<p>By the way, another reason to use the command line instead of the web interface is if you need to upload lots of files at the same time. With the bq command, you can put an asterisk in the filename, which will act as a wildcard and upload all matching files.</p>
<p>Let’s have a look in the web interface again to make sure the file uploaded properly. You have to refresh the page first so you can see the updates. Great, there’s the “public” dataset and there’s the “jeopardy” table. Click on the jeopardy table, and then go to the Preview tab. It looks good. Did you notice that the column names are actually descriptive? They have names like “Show Number” and “Category” instead of generic names like “string_field_0”. That’s because the first line of the csv file listed the field names.</p>
<p>On a different topic, you might be wondering why we didn’t just rename the “babynames” dataset to “public” instead of creating a new dataset. Well, that’s because you can’t rename a dataset in BigQuery. The only way to do it is to create a new dataset, copy all of the tables from the old dataset to the new dataset, and then delete the old dataset and its tables. So choose your dataset names carefully.</p>
<p>Let’s move the names_2019 table from the babynames dataset to the public dataset. Click on it, then select “Copy Table”. Now change the destination dataset to “public” and call the destination table “babynames_2019”. Click Copy. It takes a few seconds. Now click on the “babynames” dataset, and click “Delete dataset”. This will delete the dataset and all of the tables in it, so you have to type the name of the dataset before it will delete it. There, it’s done.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Streaming-Data"><a href="#Streaming-Data" class="headerlink" title="Streaming Data"></a>Streaming Data</h1><p>So far, we’ve run queries on public datasets and on pre-existing data that we loaded into BigQuery. But there’s another way to get data into BigQuery – streaming, where you add data one record at a time instead of a whole table at a time. One example is if you need a real-time dashboard that gives you an up-to-the-minute (or even up-to-the-second) view of your data.</p>
<p>One common way to stream data into BigQuery is to use Cloud Dataflow, but that’s the subject of another course. You can stream data directly into BigQuery by calling the API in your software. I’ll show you an example using the API Explorer, which lets you make an API call from a nice interface in your browser.</p>
<p>Before we do that, we need to create a table to stream the data into. We should also create a new dataset because we’re not going to put public data in the table, so the public dataset isn’t the right place to put it. Let’s call it “streaming”. Now create the table. This time, we’re going to leave it as “Empty table” because we’re not loading any data into it right now. Let’s call the table “stream”.</p>
<p>For the schema, we could put in pretty much anything, since we’re just going to stream some test data, but let’s have at least two fields. Create a string field called “greeting” and a string field called “target”. And click “Create table”.</p>
<p>Okay, now we’ll go to the BigQuery API documentation. The URL is in the GitHub repository for this course. We’re going to use the “tabledata.insertAll” method to stream data into the table.</p>
<p>On the right-hand side is a form you can fill out to call the API. First, put in your project ID (which will be different from mine, of course). You can get your project ID from BigQuery. Then put in “streaming” for the datasetId, and “stream” for the tableId.</p>
<p>Now, in the “Request body” text box, click the plus sign and select “rows”. This saves you from having to type in everything yourself. It’s especially helpful with the brackets because we’re going to have quite a few nested brackets in a minute. Now click the plus sign under “rows” and select “[Add Item]”. It put in a couple more brackets. Then click the plus sign between those and select “json”.</p>
<p>Now we finally need to type something. This is where we say what data we want to put in the row we’re adding to the table. If you’ll recall, the first field is called “greeting”, so type that between quotes after the curly bracket. Then type colon quote Hello quote comma. Hit Enter. We called the second field “target”, so type quote target quote colon quote world quote.</p>
<p>All right, that’s a complete record, so the request body is complete. It added a bunch of junk at the bottom for some reason, so just delete those lines. Good, the errors went away.</p>
<p>Now click the “Execute” button. Okay, the return code is 200 and it’s colored green, which means the API call was successful. It also didn’t return an error, so it looks like it worked. Let’s see.</p>
<p>Go back to the BigQuery page and click on the “stream” table. Click the “Preview” tab. Great, there’s our “Hello world” message that we just sent.</p>
<p>Although that was a quick way to show you how streaming works, you likely won’t be calling the API directly when you write your own streaming code. Instead, you should use the BigQuery client libraries. </p>
<p>Google provides client libraries for C#, Go, Java, Node.js, PHP, Python, and Ruby. There’s a different way of doing a streaming insert for each language, but it’s easier than writing the API call yourself. If you need to write code to stream data into BigQuery, have a look at the documentation for your language.</p>
<p>Oh, and one more thing. Remember in the pricing lesson when I said that there’s a separate charge for streaming? Well, it’s 5 cents per GB to do streaming inserts. That actually makes it the most expensive BigQuery operation. Loading data in any other way is free, querying data costs half a cent per gig, and storing data costs 2 cents per gig, at most. But if you need to do up-to-the-minute analysis of streaming data, then 5 cents a gig is still pretty cheap.</p>
<p>And that’s it for this lesson. </p>
<h1 id="Exporting-Data"><a href="#Exporting-Data" class="headerlink" title="Exporting Data"></a>Exporting Data</h1><p>Sometimes you need to export data from BigQuery, such as when you want to use third-party tools on the data. Exporting is pretty easy, but there is only one place you can export the data to and that’s Cloud Storage. So if you want to export data anywhere else, you have to export it to Cloud Storage first, and then download it from there.</p>
<p>Let’s do that with the babynames data. Click the table name, then select “Export to GCS” from the “Export” menu. It supports CSV, JSON, and Avro formats. Just for something different, let’s select JSON. Then you can choose whether to compress it or not. GZIP is the only compression option. This is a pretty small file, so let’s not bother compressing it.</p>
<p>Now you need to specify the Cloud Storage location where you want to save the file. I’ll put it in my ca-example bucket, but you’ll have to put it somewhere else, of course. Make sure you have write access to whatever bucket you specify. You also have to put in the filename. I’ll call it babynames_2019.json. Then click the Select button…and the Export button.</p>
<p>It tells you that it started an export job. It doesn’t take long to finish. Now if we go to the Cloud Storage bucket, we can see that the file was created. If you click on the filename, you can download it to your computer.</p>
<p>Now if you open it up, you’ll see the data in JSON format, which looks far different from the CSV file we originally uploaded into the table.</p>
<p>If you want to see a history of the exports you’ve done, click on Job History. If you click on one of the jobs in the list, it’ll give you more detail. Notice that it also lists other types of jobs, such as when we loaded the data originally. You can even re-run a load job from here if you want.</p>
<p>You can also use the bq command to run an export job, but the option is called “extract” rather than “export”. That is, you use the “bq extract” command.</p>
<p>As you’ve seen, exports are quite straightforward, but they get a little more complicated if you need to export more than a gig of data. Here, I’ll show you what happens. Take a look at the games_wide table in the baseball dataset. It’s 1.76 gig. I’ll try to export it, and I’ll even compress it, so hopefully the exported file will be less than one gig. </p>
<p>It gives us an error. It says that it’s “too large to be exported to a single file. Specify a uri including a * to shard export.” That second sentence is a little cryptic, isn’t it? What it means is you have to include a wildcard so it knows to export the data in multiple files.</p>
<p>Here’s how to do that. I’ll export it again, and this time, I’ll put an asterisk after “games”. You can put the wildcard anywhere in the path except for the bucket name, but putting it just before the first file extension is usually the best place to put it as you’ll see in a second.</p>
<p>It’s running this time, which is a good sign. It’ll take a lot longer, so I’ll fast forward. Now it’s done, so if I go back to Cloud Storage, you’ll see two new files with long numbers in them. BigQuery simply appends numbers starting from 0 and goes up by one for every file, but it puts in 12 digits with all zeros at the beginning, just in case it needs to split the data into a lot of files.</p>
<p>Also notice that the sum of those two files is nowhere near one gig. It’s less than a hundred meg. That’s because BigQuery looks at the size of the source data rather than estimating the size of the destination file when it decides whether you have to split it into multiple files or not.</p>
<p>Okay, if you don’t want to incur ongoing storage charges for the data you’ve saved in this course, then you should remove it. First, delete the files you exported to Cloud Storage. Select all three files, and then click “Delete”. Now go back to BigQuery, and delete the public dataset you created. You have to type “public” to confirm the deletion. Then delete the streaming dataset. There, now everything you loaded or exported in this course should be gone.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I hope you enjoyed learning how to use BigQuery. Now you know how to load data, run queries and save the results, stream data one record at a time, and export data. Let’s do a quick review of what you learned.</p>
<p>BigQuery’s advantages over on-premises databases are ease of implementation and speed.</p>
<p>If you want to import data that’s already in another Google service, then there’s usually a way to get it into BigQuery, although sometimes it requires an intermediate step. It’s also possible to query data in certain Google services without importing it into BigQuery. However, the performance is usually slower when you query an external data source than if the data resides in BigQuery storage.</p>
<p>If your data isn’t in a Google service, then you can upload it to BigQuery through the web interface, the command line, or the API. One limitation of the web interface is that it can only upload files that are 10 megabytes or less in size. The command-line tool for all BigQuery operations is “bq”.</p>
<p>When you’re uploading data, BigQuery includes an option to automatically detect its schema, but it doesn’t always work, so you may have to specify the schema manually.</p>
<p>Another way to get data into BigQuery is streaming, where you add data one record at a time instead of a whole table at a time. This is most useful for real-time applications. Although there’s no cost to upload data to BigQuery in bulk, it does cost money to stream data into BigQuery. To add streaming code to your applications, it’s easiest to use Google’s BigQuery client libraries, which are available for many different languages.</p>
<p>BigQuery stores data in tables, and each table must be part of a dataset. You can’t rename a dataset in BigQuery.</p>
<p>When you run a query, if you don’t specify a destination table, it puts the results in a temporary table. This temporary table stays in cache for about a day. So if you run the query again within 24 hours, it’ll retrieve the cached copy, and you won’t be charged for the query.</p>
<p>Cloud Storage is the only place where you can export data from BigQuery. If you need to export more than one gig of data, then you have to shard the data into multiple files by including an asterisk in the destination filename. You can also use an asterisk when you’re uploading files.</p>
<p>To learn more about BigQuery, you can read Google’s online documentation. You can also try one of the other BigQuery courses on Cloud Academy.</p>
<p>Please give this course a rating, and if you have any questions or comments, please let us know. Thanks!</p>
<h1 id="5Loading-Data"><a href="#5Loading-Data" class="headerlink" title="5Loading Data"></a>5<strong>Loading Data</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">Course GitHub repository</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.google.com/sdk/docs/install">Google Cloud SDK Installation Instructions</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:16" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:16-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:37:48" itemprop="dateModified" datetime="2022-11-20T19:37:48-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Google-Cloud-Firestore"><a href="#Google-Cloud-Firestore" class="headerlink" title="Google Cloud Firestore"></a>Google Cloud Firestore</h1><p>Hello and welcome, this is Introduction to Cloud Firestore with App Engine, and in this lesson, we’ll be exploring the basic functionality of Cloud Firestore and how it’s used with App Engine. By the end of this lesson you should be able to: explain the purpose of Cloud Firestore, explain the relationship between Cloud Datastore and Cloud Firestore, explain when to choose Datastore Mode, and explain how to use Datastore with App Engine. </p>
<p>Most applications need a place to store queryable data, and fortunately, there is no shortage of databases to help. However the choice of database is not always a simple one, it depends on multiple factors including but not limited to the type of data being stored, how the data will be consumed, et cetera. Technical limitations are another concern. For example, will the database be able to support the amount of traffic? </p>
<p>No matter what kind of database you’re using, if it can’t support the workload that you intend to use it for, it’s not all that useful. Remember, App Engine can keep scaling up to support the demand and that means the database needs to as well, and that’s why Google created Cloud Datastore and paired it with App Engine. Now, that doesn’t mean it’s the only database you can use though it was designed to be a good likely pairing for many apps. So, if you’re interested in learning more, then let’s get started. </p>
<p>Google released Datastore in 2013 to serve as App Engine’s database, and for years that is how it has been. Datastore is a highly scalable, fully-managed, no-SQL document database. Supporting both eventual and strong consistency, it supports transactions and it offers a SQL-like query language called GQL. It stores data as properties of an entity with support for multiple data types, and it categorizes entities based on a developer supplied Kind. </p>
<p>To make entity lookups perform quickly, entities include a Datastore property named a Key which is a unique ID, and to allow entities to be queried, Datastore allows developers to create indices based on the properties for which we want to filter. </p>
<p>I mentioned queries being similar to SQL, they’re similar but not exact, and there are some limitations for which I’ll include a link App Engine is designed to run web apps and mobile backends which are broad categories with a wide range of storage requirements. There’s really just no single option that’s going to work for all workloads. If we were to compare a mobile chat application with a mobile news feed, both of them have very different storage needs. The same goes for web applications, let’s say a brochureware application is going to have different storage needs than a site such as Twitter. </p>
<p>In 2014, Google acquired a realtime database called Firebase. Realtime databases are used for data which is always changing and require processing to happen very quickly. Shortly after that acquisition, Google started building Cloud Firestore. It took the best parts of Firebase, it took the best parts of Datastore, and it smashed them together into a single service. </p>
<p>Now, the purpose of Cloud Firestore is to serve as the next generation no-SQL database for <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud</a>. This is where we get a bit tangled. Cloud Firestore provides one of two operating modes that are called Native Mode and Datastore Mode. Using Cloud Firestore in Datastore Mode is the next generation of Cloud Datastore. It supports Datastore’s API and it uses Firestore’s data storage layer which removes some of the previous Datastore limitations. </p>
<p>Now, currently the original Datastore and Firestore in Datastore Mode are two different products, however, Google will be moving users over to Firestore seamlessly over time. Using Firestore in Native Mode is not the next generation of Firebase, at least not publicly, at least not in this moment. Using Cloud Firestore in Native Mode is similar to Firebase, though there are some implementation changes. </p>
<p>So with all of this, the logical question is, how do I know which mode to use? Here’s some general advice, now, it’s not universally applicable, though it is a good starting place. Datastore or Firestore in Datastore Mode are intended for server workloads, meaning a service-side application interacts with a database. Now, this is exactly what App Engine does, so if you’re building with App Engine and you need a schema list database, then you’ll probably wanna use Firestore in Datastore Mode. Firestore in Native Mode is similar to Firebase, which is basically an application platform and doesn’t require developers to create service-side applications. </p>
<p>Firestore Native and Firebase both provide STKs which allow read and writes to documents for which users have access and all without a server, and if you need some sort of backend functionality, it also supports running Cloud functions based on event triggers which include, when a document is created, deleted, updated, et cetera. So Firestore in Native Mode is very cool, but it’s not the focus of this lesson, so we’re not going to go into detail, this lesson is really focused on using Datastore Mode with App Engine, though if you wanna learn more, I’ll include some links for further reading. </p>
<p>Before moving on, there are some things to commit to memory, each project can use Firestore in one mode only. So if you select Datastore Mode, that’s the mode for your project and vice versa with Native Mode. Also, and importantly, if you create an App Engine app inside a project, it’s going to automatically choose Datastore Mode, so be careful with that, if you really don’t wanna Datastore Mode, don’t create an App Engine app in that project first. </p>
<p>Alright, with all of this out of the way, how do you actually use Datastore with an App Engine application? Due to the ever-evolving nature of App Engine, there have been different methods for interacting with Datastore which varied by runtime. Currently, Google recommends the use of the Google Cloud Client Libraries. Google provides these libraries for different runtimes, and they allow engineers to interact with Datastore in the language that they’re familiar with. If for some reason you’re using a language that does not have a supported library, you can always fall back to the Rest API and use that directly, it just requires a bit more effort on your part. </p>
<p>Before wrapping up, let’s check out a demo of Cloud Datastore in the console. I’m here on the Datastore Dashboard, I’ve already enabled the Datastore API and I have some sample entities which store some dummy data. Notice here I have a Kind called EmailEvent and there are four entities. This Entities page is where we can see the different entities for a specific Kind which is specified in the dropdown. And if we click on Create it will open a form where we can add a new entity. </p>
<p>The Namespace is used to partition data, an example might be to specify a company name which would allow for a multitenant app, and if you don’t specify a namespace that’s fine, the default is used, just know, they can’t be changed once they’re set. Datastore uses the concept of a Kind to categorize an entity and that makes it easier to query specific types of entities The Key is used to look up an entity quickly, notice these properties here, these are added by Datastore to make it easier for us to populate this, it knows that this is an EmailEvent Kind so it’s given us the properties that exist on some of the other entities. </p>
<p>This is just a nice feature from the user interface to make it easier to enter our data, remember, this is a schema list database, so we don’t have to define specific properties for even the same Kind, I could just remove all of these and have a totally different property for one EmailEvent than I do for all the rest. I can’t imagine a use case in which you’d want to do that, but it is possible. </p>
<p>The query language that Datastore supports is called GQL and again, and it resembles SQL up to a certain point, so if we start by typing select star from followed by the name of our Kind, we can see this returns all EmailEvent Kinds with all the properties, though you could also specify properties to return and that way, you can get just the data you need. </p>
<p>OK, let’s stop here and see how we did. The purpose of Cloud Firestore is to serve as the next generation no-SQL database for Google Cloud. To get there, it took the lessons learned from Datastore and Firestore, probably other stuff internally as well, and put them all into a single product. Cloud Datastore is currently both its own service and a mode of Firestore, where Firestore in Datastore Mode is the latest generation of Datastore and will replace Datastore. Datastore Mode is intended for server workloads and if you’re pairing with App Engine, then it’s likely a good choice. To interact with Datastore or Datastore Mode in software, we would use the Google Cloud Client Libraries, though we could always use the Rest API directly if we needed to. </p>
<p>Alright, that’s going to do it for this lesson, I hope this has been helpful, I hope it’s filled in a few of the gaps, and I will see you in the next lesson.</p>
<h1 id="Google-Cloud-Datastore"><a href="#Google-Cloud-Datastore" class="headerlink" title="Google Cloud Datastore"></a>Google Cloud Datastore</h1><p>In this lesson, we’ll dive deeper into Cloud Datastore. We’ll cover queries and indexes and entity groups and transactions.</p>
<p>Let’s start with queries. A query can specify a kind and then zero or more filters and zero or more sort orders. We can filter on properties, keys and ancestors. Filters are basically pretty simple. Here’s an example in Python outside of the context of an actual application. We define the q variable as a query for the person kind, and then we filter it on the person name &#x3D; John, and then we can add a sort order by calling the order method, so we say order and then we specify the name, and then if we add a hyphen in front of it, it makes it descending, and we can query on ancestors as well with something like this. We can use an ancestor query specifying ancestor &#x3D; and then the key.</p>
<p>Let’s check out an example from our actual application. If we look at the images.py file, we can see that we’re using a class method called for category to fetch all of the images for a given category. It uses an ancestor key as a query filter and this allows us to get all of the images that belong to the category that was passed in. So if we were to break down this code, it would translate into something like we get the key, based on the urlsafe key, and then we use that to query all of the images that have that category as an ancestor. We sort by the created on date, descending, and then we take the last 20 results. It’s a fairly simple-to-use API, but it’s very powerful.</p>
<p>With a traditional relational database, we use indexes to improve performance. Due to the design of Datastore, we use one or more indexes for any query we run. With Datastore, there are basically two types of indexes. We have single property indexes and composite indexes. Single property indexes are automatically created for us which means each individual property is indexed, allowing us to query it. Now, these indexes take up space, which means there’s cost attached to it, so we also have the ability in our code to say index &#x3D; fault. This will allow us to skip indexing properties that we won’t ever be querying. This is going to save us money.</p>
<p>Okay, there are some limits on the queries we can run with single property indexes. We can use equality filters on one or more properties, which is a merged join, so something like first name &#x3D; Bob and last name &#x3D; James, this works because even though we’re querying on two properties, we’re using an equality filter so it’s merging the results, and we can use inequality filters on one property, such as first name &gt;&#x3D; to the letter B and first name is &lt; the letter C, and only one sort order can be defined on a single property query. Now, if we want to query on multiple properties, we can create a composite index. We can create it manually using the if find YAML syntax, or we can run our queries on the development server and it’s going to generate an index.yaml file or a datastore-index.xml file for Java.</p>
<p>Let’s check out our index.yaml file. Right here at the top it says auto-generated and that’s because when we run any code that runs a query against the development version of Datastore, it builds the index.yaml for us. That way when we deploy, App Engine knows what indexes it needs to build. Here’s an example of what a composite index might look like. We have a last name and a first name and they’re both ascending.</p>
<p>For multi-valued properties, like our tags, it looks similar except an index entry gets created for every value of a property. We can query multi-valued properties if at least one value matches the filters. We saw that when we checked out the tags on our images in a previous lesson. It’s considered best practice that we don’t index very long strings. Instead, we should be using the Search API which gives us Google-like search capabilities. Also, we should clean up old indexes using the appcfg vacuum_indexes command, and if we have properties that shouldn’t be indexed, maybe something like a very long string as we just mentioned, we can flag them as not indexed with the indexed &#x3D; false.</p>
<p>We’ve talked about how Cloud Datastore is fast and efficient for querying, but why is that? It’s because we use the indexes to shift the cost of querying to upfront when the index is created, so sometimes it’s going to take a little while for the indexes to initially build if we have very large data sets, though once an index is built, then querying them is very fast. Let’s talk about consistency with Datastore. We’ve talked about eventual and strong consistency a few times.</p>
<p>The difference is basically that for strong consistency, the data we read is the last data that was written, and with eventual consistency, the data that we read may not be the last data written. Eventual consistency is great for when we don’t need anything critical. This can be things like a blog post, and we’d use strong consistency when it’s vital to see the latest updates. Now, this can be for things like the price of a product in our catalog. If we need strong consistency, we have a few options. We can use an ancestor query. We can fetch an entity using the get method on a Key, or we can use a transaction. We’ve talked about the first two throughout our discussion on Datastore. However, we haven’t talked about transactions, so let’s dive into that a bit more.</p>
<p>We can use transactions to gain strong consistency. Let’s say that we wanted to update a property and in this example, it’s the amount of tickets available for a conference. So, we’re able to ensure that if this executes successfully, any future queries will have this data. The ndb library makes it easy to perform transactions with this transactional decorator, along with the other methods that we can find in the API documentation. Now, sometimes we’re going to need to work with entities that are not part of the entity group we’re using.</p>
<p>Let’s say we have two bank account entities that are not part of the same group, and we want to transfer funds from one to another. We want to be able to ensure strong consistency with something like this since eventual consistency could result in something like withdrawing more money than we should have available or not being able to withdraw enough money that we should have. For something like this, we can use cross-grouped transactions to ensure strong consistency. We still use the same transactional decorator. However, we set the xg parameter to true.</p>
<p>There are some best practices for transactions. First, because entity groups can only be written to once per second, we need to consider the design of our entity groups in advance. Next, an entity group’s relationships are immutable, so if we need to make a change to the relationship, we need to delete the entities and recreate them with the new relationships. Also, we have a 60-second time-out on transactions. This is intended to reduce the chances that an entity is edited in another transaction during that same time. Finally, inside of transactions, the only type of query we can run is an ancestor query. So, we may need to fetch data outside of the transaction and pass it off to the code that’s going to be running that transaction.</p>
<p>All right. Let’s summarize what we’ve covered in this course. A query can specify a kind, and then zero or more filters and zero or more sort orders.</p>
<p>We can filter on properties, keys, and ancestors. With Datastore, there are basically two types of indexes. We have single property indexes and composite indexes.</p>
<p>Datastore supports strong and eventual consistency, and we can use ancestor queries calling the get method of a Key and transactions for achieving that strong consistency, and transactions can also be cross-grouped to allow us to support strong consistency for disparate entity groups.</p>
<p>Thanks for taking the time to watch this course. For Cloud Academy, I’m Ben Lambert. Thanks for watching.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:14" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:14-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:39:20" itemprop="dateModified" datetime="2022-11-20T19:39:20-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:13" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:13-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:36:10" itemprop="dateModified" datetime="2022-11-20T19:36:10-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Greetings, welcome to Cloud Academy’s course on managing Google Cloud Kubernetes and App Engine Resources. I’m delighted to have you join me in what is bound to be an educational and exciting adventure into the world of Google Cloud development.</p>
<p>First, I’ll let you know a bit about myself, before I get into the course outline. My name is Jonathan. I’m one of the course developers with Cloud Academy. I’m a former high school teacher turned technical consultant specializing in DevOps and data engineering. It’s a pleasure to get back into the world of teaching, only now talking about technology.</p>
<p>This course is designed to be very practical. It is meant for technology professionals, developers, data architects, CTOs, etc. With the goal of helping them get a solid understanding of how to build infrastructure using Google Cloud services. This course will also help you to prepare for the Google Associate Cloud Engineer Certification exam.</p>
<p>So you may be wondering what the prerequisites are for a course like this. What do you need to already know to be successful in this class? Well a few things. For one, you should have some familiarity with Google Cloud Platform. This course assumes you know basically what GKE and App Engine and Compute Engine are. You should also be comfortable working with command line tools and in a web console. You don’t need to be an expert programmer you don’t need years of experience working with GCP or AWS or something similar, however, general knowledge of what cloud providers do is helpful as this course will not cover rudimentary concepts such as, “What is a VM?”</p>
<p>So now let’s talk learning objectives. In this course, there are two main takeaways related to Google Cloud Platform. Number one, the student will learn how to manage and configure GCP Kubernetes Engine resources. And number two, the student will learn how to manage and configure GCP App Engine resources.</p>
<p>In short, this course covers GCP compute services related to Kubernetes and serverless architectures, specifically GKE, and App Engine. This might seem familiar if you took an earlier course, Planning and Configuring a Cloud Solution, which also covers these topics but the difference here is that the learning objectives in that earlier course was just deploying and configuring, whereas the material was more of an overview because of that.</p>
<p>In this course, we’re going to go deeper. If you have zero familiarity with GCP we definitely recommend taking the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/planning-configuring-google-cloud-platform-solution/introduction/">Planning and Configuring</a> course first. That will more gently introduce you to these compute services. This course, by contrast, will assume that you have a basic understanding of the different services and will instead serve as a deep dive on how to actually manage these systems. By the end of this course, you should be truly ready to be responsible for a GCP environment, specifically App Engine and Kubernetes Engine. You’re not just clicking through a wizard and hoping for the best.</p>
<p>So, one last thing before we start, I want to encourage everyone to leave feedback. Email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> if you have any questions, comments, suggestions, concerns. We always appreciate people taking the time. And so now without further ado, let’s get started.</p>
<h1 id="Section-One-Introduction"><a href="#Section-One-Introduction" class="headerlink" title="Section One Introduction"></a>Section One Introduction</h1><p>Welcome to section one. In this quick introduction lesson, we’ll go over the learning outcomes for this first section of the course. Our focus will be on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> Kubernetes Engine. So there are two high-level goals for this section. First, we wanna make sure you understand Kubernetes generally. Whether it is inside GCP or with another cloud provider, there are certain concepts you need to be familiar with to utilize Kubernetes effectively. Second, we will go over GCP Kubernetes Engine’s specific tools for implementing a Kubernetes application and we’ll do this in two lessons, the first focusing on creating a cluster and setting up an application container, and then, the second lesson, we’re focusing on more fine-grained Kubernetes configuration. We’ll drill down into topics like pods, nodes, services, deployments, and stateful applications, so on and so forth. Finally, we’ll end the section with a practical demonstration. We’ll do a video walkthrough on how to utilize GKE starting with nothing and ending with a running app in a GKE cluster. So if you’re ready, let’s dive in.</p>
<h1 id="Kubernetes-Concepts"><a href="#Kubernetes-Concepts" class="headerlink" title="Kubernetes Concepts"></a>Kubernetes Concepts</h1><p>Review of Kubernetes Concepts. If you’re coming from the more introductory, “Planning and Configuring” <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> course, then you recall its basic overview of GKE. The goal there was really to just show how it works with enough information to get you started. In this course, we want to get you ready to be responsible for maintaining a GKE cluster in a real production environment. So this will require a deeper dive into how GKE works, and before we do that, we want to ensure you have a deep understanding of Kubernetes concepts. If you’re already are a Kubernetes expert and you wanna just dive into GKE, feel free to skip this lesson.</p>
<p>So let’s start by talking about what Kubernetes is at a high level. Stated most succinctly, Kubernetes is a system for orchestrating containerized applications. So if you are packing your software application using something like Docker, and wish to deploy and manage those Docker containers, Kubernetes gives you the tools to abstract away most hardware and networking considerations to make managing your system much easier.</p>
<p>Consider the basic resources needed to deploy a Dockerized application. The Docker container needs a server to run on, so that means compute resources. It needs a certain amount of CPU and memory. It also needs network access, port configuration, firewalls, load balancing, DNS configs. The app may need to talk to other backend services, or it may be part of a larger system of microservices involving several other Docker apps. Managing all of this complexity manually is very difficult. We’d need to create all sorts of scripts and documentation for each server, each network setup, each application’s hardware needs, config files, etc., etc. Kubernetes, like other orchestration frameworks such as Docker Swarm or Mesos Marathon, it makes all of this work much easier.</p>
<p>It starts with the concept of a cluster. A Kubernetes cluster is a complete set of resources for an application environment. Hardware resources. So in general, this will be confined to a single data center and will comprise a number of servers and network interfaces. Storage is also a possible resource here as Kubernetes can create ephemeral and persistent volumes. The servers, whether physical machines, VMs running in EC2, or Google Cloud Compute, or somewhere else, they are referred to as nodes. Servers are nodes. A Kubernetes cluster may, for example, run on three nodes, three virtual servers, that will host your container-based applications. Having multiple nodes grants redundancy in case of hardware failure and it also makes scaling up or down easier. Each node will run a kubelet, a lightweight Kubernetes agent, that allows it to communicate with other nodes and stay in sync regarding cluster configuration and help.</p>
<p>A Kubernetes cluster with just nodes, however, is not really doing anything useful. In order to run a container, the cluster must have access to an image repository and it has to create pods. Now, a pod is a basic workload unit in Kubernetes. Often, a pod is an instance of a single container, however, it can also be comprised of multiple containers. A pod will also have a unique IP address within the network as well as storage resources based on its config. This is the smallest unit of what we might think of as a microservice in a software as a service architecture. So for example, you might have a simple stateless Python app run as a pod in your Kubernetes cluster. And we’ll dig in deeper on the container configuration and cluster setup in the next section.</p>
<p>So now, with a basic understanding of clusters, nodes, and pods, there are only two other core concepts of Kubernetes that you need to really get started working with GKE. And these two are Controllers and Services. Now, we’ll link to the <a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/">Kubernetes documentation</a> if you wanna really go deeper on all the other terminology. That’s a bit out of scope for this class, we’re just doing a basic conceptual overview.</p>
<p>But let’s diagram there, let’s start with Controllers. You can think of a Kubernetes Controller as a <em>control loop</em>. This is a basic CS concept. It’s a tool for maintaining a certain desired state. In Kubernetes, this refers to an API that manages a pod or set of pods by preserving a pre-set configuration. So there are actually a few different types of Controllers. One of the most basic is the ReplicaSet, which simply guarantees that a certain number of copies of a given pod are kept running. So for example, let’s say we have our stateless Python app running as three pods across the nodes in our cluster. If they are running as a ReplicaSet, then Kubernetes will make sure that three pods are always running. If there is a failure, a crash, an error of some kind that causes a pod to die, the ReplicaSet controller will try to bring up a replacement.</p>
<p>Now, there are other Controller types for different purposes. There is the DaemonSet controller which is meant to guarantee a specific distribution of pods on each node. This controller is useful for hardware monitoring, logging applications that need a one-to-one mapping to servers for whatever reason. There’s also the StatefulSet controller designed for stateful applications. This controller provides guarantees about pod ordering, uniqueness, and stable storage. Finally, there is the Deployment Controller. This is designed to work with other Controllers, such as ReplicaSets. The Deployment Controller, as the name implies, is designed for declarative updates to a set of pods. It handles transitioning a set of pods from its current state to a defined desired state.</p>
<p>So there’s more we could add about Controllers but this is enough to get the basic idea. Again, don’t hesitate to dig through the Kubernetes documentation for more details about each specific Controller if you’re interested. But for now, let’s move on and talk about Services.</p>
<p>Services are very important. Services are Kubernetes’ way of exposing pods to external networks including the public internet. So if we wanna make our application reachable from a browser, we’re going to need a Service to set up the IP address and DNS name. The basic configuration needed is a network protocol, such as TCP as well as ports and some metadata such as a Service name. Now as with Controllers, there are a few different types of Services. There are ClusterIP services that only expose an internal IP address, suitable for apps that don’t need to be accessed from the public internet. Then you have NodePort services that expose the node’s IP address on a specific port. This might make sense depending on your firewall setup. You may not want your Kubernetes nodes, the actual VMs, to expose their IP addresses, even if its only on a specific port. And then, you also have LoadBalancer and ExternalName services, both of which work more closely with your cloud provider. The former, the LoadBalancer, it works with your provider’s LoadBalancer resources to expose a set of pods while the latter, the ExternalName type returns a CNAME record based a DNS name of your choosing.</p>
<p>So Controllers let us turn sets of running containers into resilient, updateable applications with predictable behavior. And Services let us control access to those applications by publishing them in various ways. Now, if you understand these five terms, cluster, node, pod, controller, service, then you now know enough Kubernetes to get your hands dirty. In the next short lesson, we’re gonna do just that. We’ll review setting up a cluster and preparing a container for deploy using GKE. It’s gonna be a blast. We’ll see you there.</p>
<h1 id="Cluster-and-Container-Configuration"><a href="#Cluster-and-Container-Configuration" class="headerlink" title="Cluster and Container Configuration"></a>Cluster and Container Configuration</h1><p>Welcome to part three. In this lesson, we’re gonna cover just two things; launching a cluster and setting up a Docker image for deploy as a pod in the cluster. We’ll go over both the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> console approach and the CLI, which can be used from your local computer or a remote shell environment or the GCP cloud shell tool from the browser, so let’s get right into it.</p>
<p>Now, recall that in order to have a cluster, we need to have nodes, which are just virtual machine instances. In the early days of Kubernetes, we would have to manually create those instances, install the Kubernetes agents, known as kubelets, and then, run some commands to wire it together. Now, with GKE, we can get a cluster up and running much faster. In the web console, it’s as simple as clicking on the Kubernetes Engine menu and then, selecting the blue Create Cluster to kick off the setup wizard. I recommend going through this sort of hand-holding wizard approach your first time to get a sense of what configuration is available. We can set a name, a number of nodes, a machine type, a geographic region, and some other config options. GKE has some really nice defaults and it has options for things like CPU-intensive or memory-intensive applications. Once you finish running through it, you’ll have a running Kubernetes cluster with some set of nodes.</p>
<p>Now, we should also take a moment to talk about node pools. Node pools are groups of nodes within a cluster that share the same configuration. This is a GKE-specific feature. It’s an extension of Kubernetes functionality essentially. When we create a cluster in GKE, it will by default create a set of nodes known as the default node pool. We can then just work with that node pool or we can add additional node pools with different configuration. So for example, we might want to have a set of nodes dedicated to in-memory caches, so we create some instances with lots of RAM and a particular network config, and that could be our cache node pool. Each node pool can use distinct virtual machine images, distinct instance type, and storage options.</p>
<p>So you can create node pools in the web console or using the CLI tools with the gcloud container node-pools command. With the console, you can also upgrade or delete node pools as well. So to do this with the console, simply go into the Kubernetes Engine menu and click Edit on the cluster you wanna change. There should be a node pools section with expandable menus. Click on the one you want to change, set the size value to the desired count, and click Save.</p>
<p>So that’s node pools. Now that we understand the makeup of clusters a little bit more, let’s go ahead and create one. The command to instantiate our basic cluster is as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud container clusters create cluster-name</span><br></pre></td></tr></table></figure>

<p>Now, this will just spin up a minimal cluster with default values and a specified name. You’ll likely have to at least add a –zone argument if you don’t have a default geographic zone configured. The container clusters create command actually has a lot of optional flags to configure all of the things we saw in the web console, stuff like machine type and region, number of nodes, etc.</p>
<p>Now, after running this command, at this point, all we have is a Kubernetes cluster running with no actual application. Now, we can launch an application in container form by using the deployment controller, that is by creating a deployment with a command like so. We do:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment app --image=$ImageRepo:$Tag</span><br></pre></td></tr></table></figure>

<p>This will create a running application here, named app, using an image of our choosing. So before we can run this, we need to deal with that <code>--image</code> argument. Kubernetes needs the location of your container registry, as well as the tag. Now, the container registry, it doesn’t actually have to be in Google Cloud. It could be Docker Hub or AWS ECR or somewhere else. As long as we can provide GKE with the right URL and credentials, we can access our images from anywhere.</p>
<p>Google Cloud, however, it does have a container registry service. In the console, all you have to do is enable it. You enable the Container Registry API and you can use it. It’s just a couple of clicks. We’ll have a documentation link <a target="_blank" rel="noopener" href="https://cloud.google.com/container-registry/docs/quickstart">here</a>. And when you push Docker images to the registry, you should see them in the UI. You can click on the image’s button in the Container Registry section and then, select individual images to see details. This is a really convenient way to check names, tags, dates, other information about your images.</p>
<p>So in our video demo, we’ll run through the Docker commands for building images, logging in, and pushing them to the repo. For our purposes here, we can use one of GCP’s sample applications for the image argument and we can run it as so:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0</span><br></pre></td></tr></table></figure>

<p>So that’s the hello app, tag 1.0, we run this and with that, we have deployed a container application to our GKE cluster. We can see cluster health in the console or by running kubectl or gcloud commands, and congratulations, you’ve got something running. However, this is just the start of the fun. We need to go a bit deeper. We need to understand what we just did.</p>
<p>In our next lesson, we’ll learn more about controllers, services, and pods. It’ll be a blast. See you there.</p>
<h1 id="Working-with-Pods-Services-and-Controllers"><a href="#Working-with-Pods-Services-and-Controllers" class="headerlink" title="Working with Pods, Services, and Controllers"></a>Working with Pods, Services, and Controllers</h1><p>Working with pods, services, and controllers. In the last lesson, we reviewed how to spin up a cluster and execute a simple deployment. By the end of it you had a simple app running, however, it probably all seemed a bit magical if you’re not already very familiar with GKE and Kubernetes. In this lesson, we’re going to break things down further to ensure you know what is really going on under the hood.</p>
<p>So let’s start by talking about the pod we launched. Remember this is our workload, usually defined by a single container taken from an image repository. We used a sample image from a public <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> repository. We deployed it by running a create deployment command using kubectl, the Kubernetes command line tool. The deployment is the controller for a given pod or set of pods.</p>
<p>So why did we use a deployment controller at all? Why bother using a controller in the first place? Couldn’t we just generate pods without that construct as overhead to deal with? Well, technically yes, we could just create pods, but this is bad practice. In general, with Kubernetes, we always want to use controllers when creating pods. This is, as the name implies, meant to give us more control. It reduces our maintenance and monitoring workload considerably to have a controller responsible for the state of pods. It lets us ensure that pods are healthy, that we have the right number, that the right networking is there, the right configuration, etc., etc. In general, in Kubernetes, we want to work with the highest level of abstraction possible, so we prefer to work with controllers instead of individual nodes or pods.</p>
<p>So, for now, we can get a bit more information about the pods that are running, by running this command: <code>kubectl get pods</code>. This will spit out some basic information about pods running in a cluster. We should see 1&#x2F;1 running, meaning the cluster expects one pod and sees one pod. We see its age, we see its status, we see the number of restarts. We can now add more pods to this cluster a few different ways. We can use the web console by navigating to Kubernetes Engine UI. And there we just click on the Workloads button and from there we can click on Deploy to launch more pods. There is a default nginx pod we can do just to test that out. And by default, this will create three pods out of the same container. And again it will do this by using the deployment controller.</p>
<p>We can get information about our deployments by running this command: <code>kubectl get deployments</code>. This will show us our available deployment controllers. Here we aren’t seeing nodes or pods, but rather deployments, which are kind of an abstraction level higher. So we will see the hello-server deployment which you might recall was the name we gave the deployment in the command. And also remember that a deployment is a type of controller, and crucially it can work with other controller types to both execute updates and ensure that our apps are in a desired healthy state. In this example, our pods are also running using the ReplicaSet controller.</p>
<p>So we can actually see this by running a command: <code>run kubectl get replicasets</code>. And we should see both the hello-server and the optional nginx-1 application if you launched it. They should both pop up since they’re both ReplicaSets. Now if we were to run a similar get command for DaemonSets or StatefulSets, <code>kubectl get daemonsets</code>, or something like that, it will return nothing, because the pods we have launched so far do not use those controllers.</p>
<p>ReplicaSets work really great for stateless apps that are easily distributed across a set of nodes. Recall that DaemonSets are designed for scenarios when we want to ensure that all or a specified set of nodes run a copy of the pod. And when we need that mapping of pods to nodes. When we care about hardware-level mapping, basically, that’s when we use a DaemonSet, and when we care about state, pod deployment order and persistent storage, then we will want to use a StatefulSet.</p>
<p>So now let’s talk a little bit about services. If we run <code>kubectl get services</code>, the only thing that should come up is something named Kubernetes, initially. This is the default service for the cluster that is unrelated to the pods that we have created. So how do we go about exposing our pods running as ReplicaSets to the public internet? Well to do that we need to create services, and this will create generating some additional config. We can create that config as a YAML and run a <code>kubectl apply,</code> run that command to execute that config in the cluster. We could also run a <code>gcloud</code> or <code>kubectl</code> command with some arguments. Or, easiest of all, we can go into our GKE console and click on Workloads. And from there we just click on the pod we care about, for example hello-server, and then just click the blue Expose button to generate a service. Once we do this it will ask for a port mapping before assigning an external IP. Now by default GKE will create a load balancer type service, but recall that there are other options here such as a ClusterIP or a NodePort service. </p>
<p>So pat yourself on the back. You now know enough Kubernetes and GKE to really kick some butt. You should have a sufficient grasp of pods, services, controllers to not only spin up a cluster, but also to deploy resilient services using appropriate controllers. To really lock in our understanding though, we need to see all of this in action. That’s what the next section, the video demonstration, is designed to do. See you there.</p>
<h1 id="Kubernetes-Demo"><a href="#Kubernetes-Demo" class="headerlink" title="Kubernetes Demo"></a>Kubernetes Demo</h1><p>Welcome to our video demonstration on GKE in GCP. That’s Google Kubernetes Engine in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>. In this demo, we’re gonna do a few fun things. First, we’ll create a basic application package, using Docker, and then upload it to a registry. Second, we’ll create a GKE cluster and deploy our app into it and see it running. And then finally, we’ll expose the app to the internet before scaling it up and practicing deploying a new version of the app, an upgrade essentially. So we’ll do most of these things using the GCP Cloud Shell. And also, maybe occasionally the Web Console a bit. So feel free to follow along at home in your own account.</p>
<p>Now, this tutorial comes mostly from GKE’s own documentation. So we’ll <a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app">link</a> that below. You’re welcome to just go and look at that. It’ll help if you have questions. If you do plan to follow along, be sure that the Kubernetes API is enabled. You can do this by just going into Kubernetes Engine on the console. Or you can do it if you’re running from your local laptop or something, you can run the command:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud components install kubectl</span><br></pre></td></tr></table></figure>

<p>And it should just set it up for you. So make sure that’s ready to go. So let’s get started. Okay, so to start, we need a Docker image with our application code, we need to prepare that. So we could build our own Dockerfile from scratch and using our own app code and test everything, but we’re gonna be lazy. Instead, we’re just going to save some time by taking one of GCP’s test projects. So we can just clone that using git, like so. And once we have it cloned, we’ll just cd into the directories, we wanna use their hello-app, and what we need to do after that, so it’s actually four commands, we clone it, we cd into the directory, we’re gonna export a project ID environment variable, and then we’re gonna do the Docker build.</p>
<p>So we’re gonna do <code>export PROJECT_ID=[PROJECT_ID]</code>, which we can get right from the console. And then we’re gonna do a Docker build of that. So the only thing you might have to look up is the project ID. You can get it right from the console if you don’t know it. And there’s other ways to get it. We’ll have a <a target="_blank" rel="noopener" href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">link</a> for that. But once that’s done, you’ll build your image and you’ll have it locally in your Cloud Shell environment. </p>
<p>Now what we have to do after that is, so you can see it building, just give it another sec. But what we have to do after it’s finished building, see it’s finished right here, is we have to push the image to a Docker registry. Because we can’t play from our shell environment, we need it in Google Cloud registry. And that we can do pretty quickly. We can do that with just two commands. We need to first run this. This is <code>gcloud auth configure-docker</code>, and what this will do is it will set the right credentials for a Docker push command, for Docker to push the image to our container registry. And then we just run the Docker push command, which will take the image that we just built, and it will push it to our Google Cloud container registry with the ID that we set, and with the tag that we set, we have hello-app tagged with the v1, it’s our version one. So just give this a sec, it needs to execute. And there you go. Well, it was already there, but it’ll go for the first time, you’ll see it. </p>
<p>Okay, so now that we have the image in the Docker in the container registry, we need to deploy it somewhere, right? We need a cluster to deploy it too. So how can we do that? How can we get a GKE cluster running? There are a few ways to do this. But the simplest way, if we don’t already have a cluster running, is to use the <code>gcloud container clusters create</code> command. Now before we run that, we’re going to set the project ID in our environment.</p>
<p>Okay. And so after that, we have to set our zone, our region where we’re going to actually deploy or create the cluster. So we’ll do US east1-b. And so these two configuration options, the project ID and the zone, this just tells Gcloud what project to associate the cluster with, and where geographically to create it.</p>
<p>So once we have that configuration set, we can run the <code>container clusters create</code> command. Now we see a couple of warnings here, just issue going on around this time, November 2019. But after a while, it’ll take a few minutes, we will see the cluster come up. And it takes a minute because it has to actually create the VM instances that will host the Kubernetes infrastructure and it has to set up some networking. But once it completes, you should be able to see the instances with a gcloud list command. So eventually the cluster will come up, it’ll be finished, it’ll be in a healthy state.</p>
<p>We can see the instances if we run a <code>compute instances list</code> command. So you can see these two here are the two instances we created for GKE hello-cluster. And we can also see it from running a container clusters list. So we should see our cluster come up, and we should see it’s in status running two nodes. So that should be good. So now that we have a cluster running, and we have an image that’s been uploaded, how do we go about deploying that image into our cluster, right? It must be pretty involved, we have to set up deployment configuration, we have to download the image, we have to run it, we have to sync everything, you know, probably requires a lot of work, right? Well, no, actually, it’s pretty simple. We can do the deployment with a single command. We run <code>kubectl create deployment</code> like so, we pass a couple of arguments. We’re gonna call it hello-web, that’s the name of our deployment. And we’re going to pass in the image we wanna use with –image. And that’s our gcr.io. That’s the cluster, the container repository for GCP. And we’ll put in our project ID and the name and tag we had.</p>
<p>So this literally does everything, this one command. We’re leaning on the deployment controller functionality to do most of the legwork. So we’re telling Kubernetes to create a pod using that image. So we can actually see if we do <code>kubectl get pods</code>. And we’ll see there’s our, the pod, the image running in the deployment. And it’s going to create the deployment controller. That’s what keeps everything running. And it’s what moves us from the empty cluster to the desired state. It’ll keep things that way until we make a change.</p>
<p>So now let’s have some fun. Let’s go ahead and make a service. Let’s expose our app to the public internet. We can do that also with a single command. We’re just gonna do <code>kubectl expose deployment hello-web</code>. So this is similar in that we are working with the deployment controller again. So please note that we use expose on the deployment, not on the pod. The deployment is responsible for the overall app configuration. And as far as best practices go, in general with Kubernetes, we wanna work at the highest level of abstraction possible.</p>
<p>So we wanna work with controllers instead of pods or nodes. So in this command, we tell Kubernetes to create a service, a load balancer type service, set to accept traffic on port 80. And that is going to target, it’s gonna be route trafficked to port 8080 for the container. So we can actually see that as well, we can do <code>kubectl get service</code>. And we should see there’s our service running. There’s actually two. There’s the Kubernetes kind of default service here. And then there’s the new one, the load balancer one right here. And we can see it has an external IP address. And we can see if we can hit that. And we can, there is our application. Hello world, version one, host, yada, yada, yada. So pretty cool. We’ve got a running service exposed to the internet.</p>
<p><a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app">https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects</a></p>
<h1 id="Section-Two-Introduction"><a href="#Section-Two-Introduction" class="headerlink" title="Section Two Introduction"></a>Section Two Introduction</h1><p>Welcome to section two. In this second half of the course, we’ll be focusing on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a>‘s App Engine service. Since this course assumes you already have some familiarity with App Engine, we won’t spend time explaining what App Engine is, and instead, we’ll dive right into spinning up a basic app in the very next lesson. After that, we’ll get into some of the critical management components. First with a lesson on configuring application traffic, and then after that with a lesson on autoscaling. As with the previous section, we will end with a video demonstration, walking through the entire setup and deployment process. So if you’re ready, let’s dive in.</p>
<h1 id="Creating-a-Basic-App-Engine-App"><a href="#Creating-a-Basic-App-Engine-App" class="headerlink" title="Creating a Basic App Engine App"></a>Creating a Basic App Engine App</h1><p>Creating a basic App Engine app. This lesson will be a walkthrough of how to create a simple Python service using App Engine. Our focus will be on using the command line tools instead of the web console. While the console is nice for beginners thanks to its setup wizards and friendly UI, the terminal is really where our understanding is properly crystallized, so without further ado, let’s dive in.</p>
<p>Now, first, we need a proper shell environment. As before, we can work from any computer that has the Google Cloud SDK installed. We could connect to a remote server using SSH and install the gcloud tools there or perhaps easiest of all, we could just use the GCP Cloud shell tool from the console, right from our browser.</p>
<p>Now, whichever you choose, the first thing you’ll need to do from the command line is create your App Engine project. This is basically the highest level abstraction for your app. App Engine apps are tied to a project in your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> account, so you will need your project ID to run the app create command. You can get your project ID from the web console or from running <code>gcloud projects list</code>. So the command for creating your App Engine project will be <code>gcloud app create --project=$PROJECT_ID</code>, so this will create the project.</p>
<p>Now, to verify, you run <code>gcloud app describe</code>. So congratulations, we’re done, right? Well, not quite yet. We don’t just want some empty App Engine framework with no actual service running. We want to deploy something, so for this lesson, we’re going to just use the sample Python app from GCloud’s own documentation. Now, if you’re not very familiar with Python development, I recommend using the GCP Cloud shell, as it is a standardized environment, so you won’t have to worry about Python version or library dependencies. To set up the needed gcloud components, we start by just running this command:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud components install app-engine-python</span><br></pre></td></tr></table></figure>

<p>That will take care of the environment configuration for Python, and then, after that, we need to actually get the code. We need to clone that from GitHub, so we’ll do a Git clone. The <a target="_blank" rel="noopener" href="https://github.com/googlecloudplatform/python-docs-samples/">URL</a> will be posted below. And so, now, we have the code that we intend to deploy. The actual hello world app we want is in the following directory <a target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/appengine/standard/hello_world">here</a>, it’s under python-docs-samples&#x2F;appengine&#x2F;standard&#x2F;hello_world. And then, if you wanna try it out and see what it does, you can actually run this command here, <code>dev_appserver.py app.yaml</code>.</p>
<p>Now, this will start the app server and run the service. Now, if you’re doing this from the GCP Cloud shell, all that you’re gonna see is some output showing that it’s running. This is a decent test, but it’s not really what we want. What we’d really like is to see is the app running in a browser and we can’t do that from running it on localhost in Cloud Shell, at least not trivially. If you happen to be doing this from your laptop or desktop, you can browse to the app. Using a browser, you can go to <a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080</a> and you should see your output.</p>
<p>Now, either way, we still need to actually deploy this thing, right? We need to make it into a real web service reachable from the public internet. Now, fortunately, this is pretty easy to do since it’s a completed project. All we have to do is, in that same hello_world directory, we run <code>gcloud app deploy</code>. This will use the app.yaml file in that directory for instructions, and then, actually deploy it to the public internet.</p>
<p>So to actually see it from the browser, you run <code>gcloud app browse</code>, and this will give us the URL that we can navigate to to see the app running. Now, for more fun, go ahead and look at the main.py code to see what the app actually does. Feel free to edit it. You can make it output a different message if you’d like. So with that, we can end our first lesson. You now know the basics of deploying an app in App Engine using the shell. This is, of course, a very simple case. We’ll need to go a bit deeper on the actual App Engine configuration, so that you can build confidence for using it in different scenarios. We’ll start on that in our next lesson. See you there.</p>
<h1 id="Configuring-Application-Traffic"><a href="#Configuring-Application-Traffic" class="headerlink" title="Configuring Application Traffic"></a>Configuring Application Traffic</h1><p>Section Two Part Three: Configuring Application Traffic. Welcome to Part Three. Now that we’ve gotten our feet wet with App Engine and actually deployed a basic app using the command line, let’s go a bit deeper by looking at how configuration works. Our goal in this lesson is to understand how to configure traffic to different parts of our App Engine environment. However, before we do that, we should review a bit about App Engine generally just to set the foundation.</p>
<p>So recall that there are two types of App Engine environments, Standard and Flexible. The core difference between the two is that the Flexible environment gives you direct control over your application runtime via docker files. You can enable root access to the underlying VM instances. So overall, this is a better option if you have a more unique environment use case, hence the name Flexible. Now the main trade off is that the Flexible environment is slower and it’s less resilient. Instances take minutes to deploy and start up and the instances are automatically restarted more frequently by GCP. You also don’t get access to all the same App Engine API, such as the users and images APIs, which are useful for scripting an environment automation. Another difference is that the Standard environment has a bit more flexibility with its autoscaling options. Now we’ll go into that in the next lesson.</p>
<p>But generally speaking, if you’re using App Engine to go serverless, you’re probably gonna wanna use the Standard environment for its greater speed, reliability, and feature richness. The Flexible environment is more for niche use cases. For most of the remaining lesson and demo content, we’ll be making use of the Standard environment. But keep in mind these differences with the Flexible environment in case you have need of it.</p>
<p>Now whichever environment you use, you’re going to configure your app with YAML files. Only one YAML file is absolutely required, and that is the app.yaml file. This is for application level settings. An application may be made up of multiple services. And these are configured in service.yaml files. You can have more than one service.yaml file in your applications root directory. Now for a fairly simple app, this is okay. But for something more complex, it is better practice to have separate subdirectories for each service.</p>
<p>So here’s a sample app.yaml file. This is the one for the Python app we launched in the last lesson. One line, “r<code>untime: python 37</code>“. As you can see, our app.yaml file is pretty simple. The only thing app.yaml really needs is runtime, is to specify a runtime and potentially a version parameter.</p>
<p>Now if we wanted to, we could break this out into a separate service by using a service.yaml file. And it would look pretty similar, with the only difference being that the service.yaml file generally starts with a field called “service”. So it would look something like this.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">service: python-service</span><br><span class="line">runtime: python37</span><br><span class="line">api_version: 1</span><br><span class="line">threadsafe: true</span><br></pre></td></tr></table></figure>

<p>We have a service name, runtime. And we can also put in an API version, and some other config. Now a more complex app might have several of these service.yaml files to create microservices, to create a microservices architecture within App Engine. Now aside from this, there are five other optional configuration files that can help you to extend your app’s overall functionality. And these are dispatch.yaml, queue.yaml, index.yaml, cron.yaml, and dos.yaml.</p>
<p>So real quick, let’s go through them. Dispatch.yaml is for overriding routing rules. You set this file in your application root directory and you use it to route incoming requests to specific services based on specific paths or host names in the URL.</p>
<p>Queue.yaml is for configuring push and pull task queues. This lets us define retry parameters, such as minimum back off time in seconds or, and a maximum task age, for example. We won’t be doing any queue-based services in this course. But we’ll link to the documentation. If it’s relevant to your use case, definitely take a look at this, queue.yaml.</p>
<p>Now dos.yaml is a security feature that lets you blacklist IP addresses or subnets to protect your application from DOS attacks.</p>
<p>And then cron.yaml lets you define scheduled tasks. You set a schedule, such as every 24 hours, or every Monday at six p.m. and then give it a URL for a task definition, much like a typical cron job. The task definitions should live in the specified path. Now, this is very handy for maintenance, monitoring, and other standard automated tasks that you might want to configure and have in one easy to work with location.</p>
<p>And then finally, there is the index.yaml. Now you may have noticed that in our simple Python app from last lesson, an index.yaml file was automatically generated. This file is a reference of properties on various application entities. For simple applications, you will not have to manually edit this file at all. App Engine will automatically update it for you.</p>
<p>So, now that we have some strong background on App Engine configuration and environments, let’s talk about traffic management. There are a few use cases that we need to consider. As mentioned above, we know that we can control service routing by using the dispatch.yaml file. But what about running multiple versions of a service for A&#x2F;B testing or migrating traffic to a single specified version?</p>
<p>To do these two things, we need to talk about traffic splitting and traffic migration. Splitting refers to taking a percentage of our traffic and directing it to one or more distinct versions of a service within our app. Migration refers to the opposite process, moving traffic that’s split among different versions to a single specified new version. So let’s start by talking about splitting. The first thing to know is that traffic splitting is automatically applied if the URLs in your service do not specify an app version. So if you have multiple service.yaml files with different versions of the same service and the URL in your app.yaml file do not have version parameters at all, then traffic will be split automatically and randomly. If you wish to be more precise, you can explicitly enable traffic splitting in the console or by using G Cloud CLI tools or the GCP API.</p>
<p>So for example, if we did this in the console, we would just go to the application page in App Engine and select the versions we want to split. Click on Split Traffic, and then just put in the percentage each service should receive.</p>
<p>Now we could do the same thing with the G Cloud CLI tool with a command like this. Here we have “<code>gcloud app services set-traffic</code>“ and a number of flags here. Now if you look at these flags here, there’s one option you really need to note. It’s the one that says “IP_OR_COOKIE” options in the dash dash split by flag. We have to tell App Engine whether to split traffic using IP address splitting or cookies.</p>
<p>IP address splitting is easier to do. It will just make App Engine hash the request’s IP address from zero to 999 and then route based on whatever random value it gets. This isn’t as precise as cookie routing, though, because of how IP addresses tend to be somewhat ephemeral, particularly from cellphone traffic and from the public internet. IP address splitting is also bad for traffic coming from internal <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> services because those services tend to utilize a small set of internal IP addresses. That will get stuck to the same version of your app.</p>
<p>So for better precision, you should use Cookie-based splitting. The trade-off here is that it will take a bit more setup because the application will look for a specific HTTP request header. So you may need to make a code change in your app to deal with this, but it will help ensure you get a precise split for traffic.</p>
<p>And then finally, let’s talk a little bit about traffic migration. In the Standard App Engine environment, we can choose to migrate traffic either immediately or gradually. In the Flexible environment, we can only do it immediately. There’s no option for gradual migration. When we do an immediate migration, you’ll generally see a spike in latency, as it causes all instances of the older version of your service or services to shut down. For a latency-sensitive application, this could be a deal-breaker, as you could see requests hang or it can drop as the traffic is rerouted.</p>
<p>For the Standard environment, the solution is to use gradual migration. This is configured in your app.yaml file with this one setting here, “<code>inbound_services: - warmup</code>“. And with that, you are informed and ready to deal with migration, splitting, and deployment in the App Engine world.</p>
<p>Phew, okay, so we made it through a deep dive into App Engine environments and configuration and traffic. You’re almost ready to really do some damage. In the next short lesson, we’ll talk about autoscaling and deployment. If you’re ready, let’s get going. Cheers.</p>
<h1 id="Autoscaling-App-Engine-Resources"><a href="#Autoscaling-App-Engine-Resources" class="headerlink" title="Autoscaling App Engine Resources"></a>Autoscaling App Engine Resources</h1><p>Autoscaling App Engine resources. To understand how scaling works in App Engine, it’s important to first remember that even though we think of App Engine as serverless, in reality, all of our code must run on virtual instances somewhere within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a>. So when we talk about scaling, we’re talking about scaling that underlying hardware, the virtual instances of GCP.</p>
<p>Now, first and foremost, we have to be aware of instance types. From App Engine’s perspective, there are two types of instances: dynamic and resident. Dynamic instances automatically shut down based on usage, while resident instances will run all the time, no matter what. The former is better for cost optimization, while the latter can improve app performance since you won’t have to wait for instances to restart.</p>
<p>Next, we should talk about scaling types. There are actually three types of scaling in App Engine: Automatic, Basic, and Manual. Automatic scaling is the most common type. It will create dynamic instances based on specific metrics, such as response latencies or request rate, and you can also specify a minimum number of idle instances that will run as resident instances to maintain a baseline.</p>
<p>So next, there’s manual scaling. As you can guess, this is done manually. This uses resident instances only. When set, App Engine will continuously run these instances irrespective of load. You can use manual scaling for temporary load spikes or performance tests.</p>
<p>And then, finally, there is what is known as basic scaling. When your application receives requests, dynamic instances come up. When the app becomes idle, all dynamic instances shut down, so, obviously, this is not suitable for latency-sensitive continuously running services. It is great, however, for intermittent work that is not latency-sensitive, such as the occasional user-initiated script or batch job or service. Basic scaling is not available in the flexible App Engine environment, so do keep that in mind.</p>
<p>And congratulations, that’s it, you’ve made it through all three of our rigorous lessons on App Engine. You have enough theory to launch a billion-dollar startup. Now, let’s go and put that theory into practice. Let’s have some fun. We’re gonna do a video demonstration where we will take our Hello World app to the next level. See you there.</p>
<h1 id="App-Engine-Demo"><a href="#App-Engine-Demo" class="headerlink" title="App Engine Demo"></a>App Engine Demo</h1><p>Okay, welcome to our video demonstration for GCP App Engine. Now we’re gonna have some real fun in this little section here. We’re gonna actually play around a bit with a real app in the standard App Engine environment. Now, if you made it through the earlier four parts without too much trouble, you should have no problem following along with this demo, though, we do encourage students to actually try things out in their own <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> account, if possible.</p>
<p>So, there are gonna be basically three parts to this demo. First, we’ll quickly spin up a sample app in the standard environment. This will be identical to what we showed in part two, very quick. And then secondly, we will practice traffic splitting and migration by creating a second version of the service. So, that we’ll walk through how to configure traffic basically. And then finally, we’ll check out some scaling functionality, we’re gonna manually scale the app up and we’ll also go through the automatic and basic scaling system. We’ll just show how those are configured and that’s basically it.</p>
<p>So let’s get started. Welcome to our video demonstration of App Engine on GCP. We’re gonna have some real fun now. We’re gonna actually play around with a real app in App Engine using the standard environment. Now, if you made it through the earlier four parts without too much trouble, you should have no problem following along with this demo, we are going to encourage students to actually try things out in their own GCP account, if possible, but of course, feel free to just watch along. So, this demo has basically three parts.</p>
<p>First, we’ll review the how to spin up a sample app in the standard environment. This will be identical to what we showed in part two, we already have an app up and running for that. Secondly, we’ll practice traffic splitting and migration by creating a second version of the service. So, we’ll see the traffic shift. And then finally, we will check out some of the scaling functionality by manually scaling the app and walking through the automatic and basic scaling configuration systems. So that’s the basic plan.</p>
<p>So, now, let’s just review how we deploy a basic app. We start by ensuring that we have our project ID set in the environment. So, you’re gonna do an export project ID equals whatever your project name is. Here, it’s Test Project CA555. You can get your project ID and organization ID all from the dashboard pretty easily. So we’ll export that and what we need to do is get the actual App Engine code. We’ll just clone the sample project and that is from Google’s GitHub <a target="_blank" rel="noopener" href="https://github.com/googlecloudplatform/python-docs-samples/">directory</a>. So I do a git clone of that and then we will need to go into the directory with the App Engine configuration. So we’ll do this and that’ll take us into the Hello World app in standard App Engine, Python 3.7. Be sure to do the Python 3.7 version, there’s also a Python 2 version, but that’s pretty deprecated now.</p>
<p>So, once you’re there, you should see in the directory that it has the app.yaml, the main.py. That’s the actual Python code that we’re gonna run and a requirements file and a test file there. The app.yaml is what we’re interested in. That’s gonna, you know, be the actual configuration for App Engine. The only thing here now is the runtime definition, Python 3.7. And then the actual… The deployment command is pretty straightforward. We’re just gonna do gcloud app deploy to launch our app.</p>
<p>Now this one is already up. So it’ll go through it pretty quickly, but the very first time that you do this deploy, it might take a little while. You know, once it finishes, we should be able to go to the URL and see the code actually running. So, let’s just give this a second to complete. Okay and once that has completed, we can run this command, gcloud app browse to actually see the application in the browser. Sometimes you might see this error, did not detect your browser, go to this link instead, we can click on it and we can see it return “Hello world,’ which is really all the code is doing, we go into the main.py and we can see that it’s really just a server returning “Hello world.” That’s all it is.</p>
<p>So our app is up and running. Just like that, we have a basic app running in GCP using App Engine. So next, what we wanna do is to test how to do traffic splitting and migration and in order to do this, we’ll need a second version of our service and doing this is gonna require a little bit of file manipulation. Specifically, we’re gonna need to create two directories with the same service.yaml file and we’ll make a minor change in the Python code so that we can see that there are in fact, two different versions of the service running.</p>
<p>So, let’s go about creating those two versions of the service. The way we’re gonna do that is we’re going to create two directories. We’re gonna create a v1 and a v2 directory and then we’re, you know, we’ll see that we have them in the same directory as all of our code right now. So what we wanna do is copy all of our code into those directories. So we have a v1 and we will have everything in v2.</p>
<p>Now, it is important to copy everything. If you try to copy only the yaml and the Python code, it will fail. The requirements that TXT file is very important. If you look at it, you’ll see that it is what tells App Engine to install Flask and what version of it. So if you forget that, it’ll try to deploy an app, but it will be broken because it won’t have Flask. So make sure you do that. So, if we go into, let’s say, v2. and we look at our code here, what we’re gonna need to do is change the app.yaml file into a service.yaml file. So we can just change it to service.yaml. And then to make it its own… To make it a little bit more unique… Oh, so yeah, we keep the same configuration here, but to make the code different, we’re gonna go into the main.py and we’re gonna have it return something different. So instead of “Hello world,” we’ll say this is the other version. Keep it very clear.</p>
<p>So, now we can actually deploy this as another version of the service by doing a similar command, we’ll do gcloud app deploy and this time we’ll pass in service.yaml. And, again, it’ll use the same… It’s the same basic default service, but now it’ll have another version with this different code. So hit the deploy, we confirm, it’ll take a minute to come up. So let’s just wait for that.</p>
<p>Okay, so now that that has finished deploying, we can see again, gcloud app browse if we wanna get the URL for the service, but before we do that, just to verify that we have two versions running, in the console, we can check and see this. We can see it in two places, in the services page, there should be a two under the number of versions there. So we can click on that or we can click on versions and we can reload this. Sorry, here. We can see that if we refresh it, there are two versions here. One of them is getting 100% of the traffic, the other is not.</p>
<p>Now, if we do check the versions here, this is the one getting 100% of the traffic. If we click on it, it’ll return the service output, which is the other version, the new one instead of “Hello world.” If we click on this one, this is the older one that outputs “Hello world.” Now, this is not the official service URL. This is not the app URL. This is just for those individual versions of the service. If we look at the app URL, gcloud app browse, we will get this app URL, testprojectca555.appspot.com and it’ll give us this is the other version. It’ll do that because 100% of the traffic is going to the new version that we just deployed. You can see it has a more recent timestamp. So, the console gives us these two URLs to see the two versions of the service, but the service itself, remember the traffic allocation is what determines what a user will see.</p>
<p>So now we have to think about splitting the traffic and migration. So that’s gonna take a little bit of work, but nothing too tricky. So, let’s start by talking about traffic splitting. If we wanna do traffic splitting, we can do that from the command line, but it’s a little bit clearer in demo if we use the console. So, what we’re gonna do is click on traffic split here and remember that there are IP address and cookies splitting types. For when you need a very precise split, you use the cookie type, though it takes a little more configuration. The IP address one is a little bit simpler. There’s also this random option here, you can see. So we’re gonna pick that and we’re gonna click on, you know, the other version of the service here, these are the two versions running and we can decide what percentage we want. Let’s say we want 50&#x2F;50. So half the time, it should randomly choose the “Hello world” output and half the time it should choose the other one.</p>
<p>So we click save, it’ll update our traffic settings, just give it a moment to execute. Okay, that should be done. Now we go back to our Services page. Still, there are two versions there. Go to the versions page, we can see there’s a 50&#x2F;50 split. So, if that’s the case, we can test this pretty clearly by going to the app URL and half the time we should get hello world, half the time we should get this is the other version. So let’s click on it. There’s “Hello world.” Reload. This is the other version. Reload. “Hello world.” Reload. Well, there’s “Hello world” again, but as you can see, it’s randomly choosing 50&#x2F;50 between the two versions of the app. So we have the two versions of the service. So we have successfully split our traffic and the nice thing is that migrating back to a single version is similarly very easy.</p>
<p>We can do it through the web console with just a few clicks. What we do is we click on the version that we wanna migrate to, let’s say the newer one, we wanna get that back to 100% of the traffic, all you do is click migrate traffic and confirm. And then in a matter of seconds, 100% of the traffic will go to the newest version of the service. Executed, now we’re back at 100%. We should only get this is the other version now. If we reload it, we can see that that’s what’s happening. Right, there’s three times, so. So there you go, splitting and migration, pretty straight forward.</p>
<p>So let’s do some scaling. Scaling, there’s a few different versions of scaling within App Engine. In the standard environment, the easiest one to test since it won’t require any traffic manipulation, it would be manual scaling and manual scaling, we can configure at the level of individual services by just going to our service.yaml and what we’re gonna do is add a little bit of configuration.</p>
<p>Now one thing to note, another reason we’re showing manual scaling is that automatic scaling is enabled by default in the standard environment. There’s also the basic scaling option, but for demonstration purposes, this is actually much neater, much cooler. So how do we add scaling? Well, what we do is we add the manual scaling config to our yaml here, and we’re gonna set that to instance count of, let’s say five. Let’s say we want five instances backing our app. So what we do is we edit that and we redeploy it and we should see it. Oh. Actually, we have a light slight error there. We wanna make that into instances. Sorry. And then that should come up with five instances behind our app. So let’s just give that a sec to run and of course, be careful about the name of your configuration, instances not instance.</p>
<p>Okay, so the deploy completed and as we can see, a third version of the service came up because we did the app deploy again and this version will have five instances of traffic behind it, five instances behind it to manage the traffic. We can see here, they all just came up around the same time. So, the scaling is actually pretty fast. You’ll recall from the lesson that there is an option for more gradual scaling if you want or gradual migrations, but we’re doing just immediate changes here with the deploy command.</p>
<p>Now we can also migrate back with the same method by just deleting that configuration. We can go into service.yaml and just kill that and then just deploy it again. And again, by default, you know, we create a new service and all traffic goes to that… A new version of the service and all traffic will be split toward that new version. Now that’s a default that you can override by changing the splitting options or by migration.</p>
<p>So, let’s see if the instances go away for this new version. We’re doing our update, let’s give that a second to run. Okay, so the update completed. So now, again, if we go to our versions here, we’ll refresh, we’ll see there’s a fourth version now. It’s getting all the traffic and if we look at our instance count, there are no instances. They’re all gone because the scaling config was deleted.</p>
<p>So we can see, it’s really that simple to manually adjust the scaling with your App Engine services. So that’s basically it, congrats. That’s it for our App Engine demo. We’ve done quite a bit in just a few minutes. We spun up a basic app, we created multiple versions, we split traffic, we ran a migration, we scaled everything up and down, pretty cool stuff. You should have a good basic understanding. Thanks for toughing it out and cheers.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>So congratulations! You made it! Give yourself a pat on the back because it has been a long and tough ride. We went through a lot of material so before we start celebrating let’s take a minute to briefly review what we’ve learned.</p>
<p>By completing this course you should have a very thorough understanding of how to configure and manage GCP’s App Engine and Kubernetes Engine services. You should be ready to spin up a cluster in GKE, create a Kubernetes deployment, and set up an autoscaling in App Engine with a basic app. Basically you should be ready to set up or take over administration of a <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> compute environment. For clarity’s sake, let’s review the two learning objectives.</p>
<p>Number one: Learn how to manage and configure GCP Kubernetes Engine resources. We did this in section one. We not only covered how to manipulate cluster configuration using the web console and SDK, but we also reviewed basic Kubernetes concepts just in case you aren’t familiar with them.</p>
<p>Number two: learn how to manage and configure GCP App Engine resources. This was done in section two. We focused on setting up App Engine standing environment and autoscaling to help you establish that truly serverless experience.</p>
<p>With these two learning outcomes solidified in your mind, you should now be ready to dig in and build your own applications from scratch. You should also have the conceptual foundation necessary to branch out to other infrastructure platforms if you’d like. Go you!</p>
<p>Now that you are done I’d like to end by inviting you to send feedback about the course to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. We really appreciate your comments, questions, suggestions, really anything. Congratulations again on fighting through the whole course and good luck in your future endeavors.</p>
<h1 id="3Kubernetes-Concepts"><a href="#3Kubernetes-Concepts" class="headerlink" title="3Kubernetes Concepts"></a>3<strong>Kubernetes Concepts</strong></h1><p><a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/">Kubernetes documentation</a></p>
<h1 id="4Cluster-and-Container-Configuration"><a href="#4Cluster-and-Container-Configuration" class="headerlink" title="4Cluster and Container Configuration"></a>4<strong>Cluster and Container Configuration</strong></h1><p><a target="_blank" rel="noopener" href="https://cloud.google.com/container-registry/docs/quickstart">Container Registry documentation</a></p>
<h1 id="6Kubernetes-Demo"><a href="#6Kubernetes-Demo" class="headerlink" title="6Kubernetes Demo"></a>6<strong>Kubernetes Demo</strong></h1><p><a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app">GKE Documentation</a></p>
<h1 id="8Creating-a-Basic-App-Engine-App"><a href="#8Creating-a-Basic-App-Engine-App" class="headerlink" title="8Creating a Basic App Engine App"></a>8<strong>Creating a Basic App Engine App</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/googlecloudplatform/python-docs-samples/">Python Docs samples</a></p>
<h1 id="11App-Engine-Demo"><a href="#11App-Engine-Demo" class="headerlink" title="11App Engine Demo"></a>11<strong>App Engine Demo</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/googlecloudplatform/python-docs-samples/">Google GitHub directory</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:11" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:11-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:41:32" itemprop="dateModified" datetime="2022-11-20T19:41:32-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:10" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:10-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:40:34" itemprop="dateModified" datetime="2022-11-20T19:40:34-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/31/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/31/">31</a><span class="page-number current">32</span><a class="page-number" href="/page/33/">33</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/33/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
