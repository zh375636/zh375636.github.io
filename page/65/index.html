<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/65/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/65/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Implementing-Continuous-Delivery-on-Azure-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Implementing-Continuous-Delivery-on-Azure-14/" class="post-title-link" itemprop="url">AZ-400-Implementing-Continuous-Delivery-on-Azure-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:47" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:47-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 12:25:20" itemprop="dateModified" datetime="2022-11-27T12:25:20-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Implementing-Continuous-Delivery-on-Azure-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Implementing-Continuous-Delivery-on-Azure-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to Implementing Continuous Delivery. My name’s Matthew Quickenden and I’m going to be guiding you through understanding how to configure and implement continuous delivery with Azure DevOps. If you have any questions, feel free to connect with me on LinkedIn or send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>This course is intended for people who want to become a Microsoft certified Azure DevOps engineer, or who need to combine people, process and technologies to continuously deliver products and services to meet business objectives. While it is very useful to have experience with Agile processes and practices, having a basic understanding will be okay. Having some experience in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-azure-services/">Azure administration</a> will be useful during this course, but not essential. DevOps is still considered a very new practice and students who may not have any experience or background in Agile, but are just interested in learning, will be able to benefit from this course.</p>
<p>By the end of this course, you should be able to decide on a release strategy and implement a release pipeline with various checks and triggers to deploy artifacts. Your feedback on this course is important so give it a rating when you’re finished. We have lots to cover so let’s get started.</p>
<h1 id="What-is-Continuous-Delivery"><a href="#What-is-Continuous-Delivery" class="headerlink" title="What is Continuous Delivery?"></a>What is Continuous Delivery?</h1><p>You will hear continuous delivery is often combined with the term CI&#x2F;CD or continuous integration, continuous delivery, but what is it? It was born as part of a manifesto to software development but has evolved over time and is used widely in various different ways. Wikipedia defines continuous delivery as “An approach in which teams produce software in short cycles, ensuring that software can be reliably released at any time and when releasing the software, doing so manually. It aims at building, testing, and releasing software with greater speed and frequency. The approach helps reduce cost, time, and risk of delivering changes by allowing for more incremental updates to an application in production. A straightforward and repeatable deployment process is important for continuous delivery.”</p>
<p>Continuous delivery is made possible through the implementation of processes enabling a trusted and repeatable deployment pipeline. While manual <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-continuous-delivery-on-azure/triggers-approvals-and-release-gates/">triggers</a> are often used to move through these different stages as a pipeline matures and the process itself gains trust, many of these triggers become automated, which we will look at during <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-continuous-delivery-on-azure/introduction/">this course</a>.</p>
<h1 id="Designing-a-Release-Strategy"><a href="#Designing-a-Release-Strategy" class="headerlink" title="Designing a Release Strategy"></a>Designing a Release Strategy</h1><p>Before we get started with some hands-on exercises, we need to cover some general concepts. A release is a package that holds a versioned set of artifacts specific to your release pipeline. All the information needed to execute the tasks are stored in the release pipeline and are included from the stages, variables, triggers through to approvers and release gates. There can be multiple releases for one release pipeline.</p>
<p>An important distinction is understanding the difference between the release and the deployment. A release is talking about implementation of a new feature or a change to an existing system in the way it works. A deployment is pushing the code into a new environment. This distinction is also referred to as separating your functional release (or feature release) from your technical release (deployment to an environment). These two things often get mixed together because they are tightly coupled together in the release process.</p>
<p>Many of the tools and programs you will use do not exist within the Azure DevOps environment. To access these services, you will need to create services connections. You may have already created some of these but here is a list of commons service connections. Each service connection will have the specific parameters needed to create connect to that service.</p>
<p>An artifact is content required to deploy your application or infrastructure. The process is to build, collect, or create the content once and then release or deploy that same content over and over again. This is done by ensuring that the base artifacts don’t change, if it changes how can we guarantee a consistent result? Here is the list of artifact sources and depending on your purpose or project you may choose different sources, there is no one right answer.</p>
<p>Looking over these sources there are some general types we can consider. There are builds where a build pipeline creates a package that is versioned and stored where the contents can’t be changed. There are version control solutions, where the release pipelines pull code and packages directly from source control. There are container registries, that allow you to pull a specific version of a container helping with deployment consistency.</p>
<p>This list will obviously change over time, but the important thing to remember is the goal of this style of deployment is to create a consistent deployment that you can rely on. To achieve this, the source artifact you need to select needs to be stable. The only thing that should be changing when you deploy to a new environment is the configuration. The ‘thing’ you are deploying should never change.</p>
<p>An important consideration is what type of auditing do you need to provide for these source artifacts? As more layers are added, it often becomes more important to know what line of code changed and who checked in the code, or merged that branch. There are many tools and techniques out there to help with code in relation to version control but each organization needs to implement their own process around what they need to do and what to release.</p>
<p>Environments define collection of resources such as Kubernetes namespaces, Web Apps, VMs, or databases that can be targeted by deployments from a pipeline. Typical examples of environments include Dev, Test, Staging, and Production. Some benefits of using environments are that you can view deployment history by environment, users permissions, resource health, or traceability for tracking work items or tasks against an environment.</p>
<p>A deployment stage is a logical representation of somewhere you want to release your artifacts to, some easy examples are dev, test, or production. A key aspect of a stage is the boundaries of a stage. Each stage should be independent from each other, meaning you can release code independently to that stage. Depending on what you’re deploying, this will also help define your boundaries.</p>
<p>With modern cloud platforms and PaaS services we can now define infrastructure as code or IAC. We can now define infrastructure that can be versioned and deployed along with the applications, for example, using ARM templates to build out network load balancers, virtual machines in Azure or even Kubernetes using AKS. Leveraging this power means a deployment stage can be completely torn down and rebuilt to ensure the source artifacts or package truly contain all the content and that our release contains all the configuration to ensure the solution is fully deployable.</p>
<p>Now that we have covered some fundamental concepts, let’s take a look at the prerequisites needed to work through our lab. We are going to create one pipeline, containing four stages, we’re going to use a GitHub project for our <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> infrastructure as code, we’re going to use an HTML project as the website. By the end of this lab, we will be able to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-continuous-delivery-on-azure/lab-01-prep-work/">trigger a deployment</a> simply by checking in a change to the HTML code or completing a pull request.</p>
<h1 id="Lab-01-Prep-Work"><a href="#Lab-01-Prep-Work" class="headerlink" title="Lab 01: Prep Work"></a>Lab 01: Prep Work</h1><p>The goal of this lab series is to leverage two public projects in our own environment. One project will be used to create the infrastructure in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>, in this case, we’re going to be using web apps. The other project will be an HTML webpage. The focus here is on the deployment pipeline. We’re gonna take a little bit of time and just get things set up.</p>
<p>So, a couple of prerequisites. You’re gonna need a GitHub account. You’ll need an Azure DevOps account, which should be free to sign up. You’ll need an Azure account, and you should have an editor installed with Git on your local machine. In this case, I’ve got Visual Studio Code, which I would recommend for this demo.</p>
<p>First thing, once we’ve got our Azure DevOps, you should have an organization setup. And if you don’t already have a project, we’ll create one. So, you may have one from previous sessions. We’re gonna use version control Git and select through different work items. In this case, I’ll use Agile, and create. So, here we have our boards, repos, and the pipelines.</p>
<p>The first thing we’re gonna need to do is create some service connections under project settings, or you can go down to service connections, and create a service connection. Here we can see the variety of different systems we can connect to. In this case, our first connection will be to the Azure Resource Manager, or ARM. So we’ll give this “MyAzure” and allow pipelines. Select your subscription. You’ll need to log in, just to ensure you have authentication. It helps if you’re already signed in, and it will authorize Azure DevOps to use the Azure Resource Manager.</p>
<p>Now that we have that connection, we’ll create a new connection to GitHub. Next here. And we’re just going to grant authorization to Azure pipelines and authorize. You do have other connection options, but for this case, this is the easiest. And we’re just going to call this GitHub. So you can have access to different users with different repositories. So we now have two service connections created in our Azure DevOps project.</p>
<p>Next, we need to create a repo where we’re going to import our HTML project. So this GitHub connection we’re going to get the infrastructure from, if we go to repos and files, and we’re gonna chose to import a repository. We need to go find the repository we’re gonna import. So let’s go over here and the project you’re looking for is called the <a target="_blank" rel="noopener" href="https://github.com/azure-samples/html-docs-hello-world">Azure-Samples&#x2F;html-docs-hello-world</a>. If we search for that, we should find that project here, it’s a public project.</p>
<p>If we scroll down, you should see this clone or download and it will give you this connection here. So let’s take that back to our file repo and we’re gonna import this html-docs-hello-world. So if we do import, that will process that request and copy all those files into a repository inside our own Azure DevOps space. So this is part of the CI area, the continuous integration. So we’re not gonna cover a lot of this, I’ll assume you already know something about it. But we will go through enough just to use it.</p>
<p>Now that we have a copy of the project in Azure DevOps, we need to clone it to our desktop. And this is where Visual Studio Code comes in, it’s connected easily with this link here. You open the page, we’re gonna select a repository for this. So I’m just gonna do Documents, Labs, and select repository. That will then copy a set of those files to my local system and I can open them.</p>
<p>You can see the same project that was available in my Azure DevOps repo is now available on my local system. I need to create a new branch. There are multiple ways to do this. You can do that through visual studio code, through the Repo. Here we can say, create new branch, provide branch name, dev. Enter. And we can see we have now that changed to sync cloud. So if we push that change into the cloud, and we go have a look at our branches, we can see we have a Dev branch.</p>
<p>That’s the prerequisites set up, we have two service connections to Azure, to GitHub, we’ve also imported a GitHub project into our own Azure Repo, created a Dev branch and we have that code synchronized on our local system in visual studio code.</p>
<h1 id="Triggers-Approvals-and-Release-Gates"><a href="#Triggers-Approvals-and-Release-Gates" class="headerlink" title="Triggers, Approvals, and Release Gates"></a>Triggers, Approvals, and Release Gates</h1><p>Triggers, Approvals and Release Gates. Once you have defined different types of stages, next you need to consider how the package (a versioned set of artifacts) will be moved through the stages. To release the package from one stage to the next, we can use triggers. There are different types of triggers we can use: Continuous deployment Triggers, Scheduled Triggers, Manual Triggers.</p>
<p>Here is a screenshot of the visual representation of the Azure DevOps Release pipeline. The artifacts are in green, pre-deployment conditions, triggers, and approvals are in purple, and post-deployment approvals are in orange. Scheduled release triggers occur as we would expect with a scheduled task. You can add multiple schedules to one trigger. For instance, perhaps you want to refresh your test environment each night with the code that’s currently in development.</p>
<p>Artifact Push&#x2F;Pull Triggers. We can trigger a release based on the source control actions. Here we have a simple Git project showing push and pull actions. In our pipeline, we can create triggers that initiate a new release based on the action of a Git push or the merging of a pull request.</p>
<p>Stage triggers help move and deploy the artifacts through the various stages of a release. This can be after the release, after a stage, or we can create them as manual triggers requiring user interaction. There are additional options such as auto-deploy triggers. You can base these on post-deployment conditions or a particular event. We can also add approvals. Approvals can give you additional control over the start or completion of a deployment pipeline. Each stage has a pre-deployment and post-deployment condition. One of these conditions can include waiting for users to manually approve or reject a deployment. There are various options around this and you can configure the pipeline to continue with the deployment if no response is received during a period of time.</p>
<p>You can enable gates at the start of a stage, in the pre-deployment conditions, or at the end of a stage in the post-deployment, or both. There are a few types of gates available out of the box as we can see here: Azure Machine Learning, Check Azure Policy, Invoke Azure functions, REST API, Query Azure Monitor alerts, or Query work items. You can add more than one test to a gate, you can add additional tests found on the marketplace or even create your own.</p>
<p>An example of a release gate could be a test that the website responds to a query in a certain amount of time and that there are currently no open issues or bugs against this version of the website. Once you have your tests configured, you have to define the evaluation criteria that define whether or not a gate is marked pass or fail. This includes the time between the re-evaluation of a gate or the sample interval and the timeout after which a gate will fail. </p>
<p>Here are two examples from the Microsoft documentation. The following diagram illustrates the flow of a gate evaluation where, after the initial stabilization delay period and three sampling intervals, the deployment is approved. We can see from the history that the first gate failed twice and was finally approved when all three gates passed. The next diagram illustrates the flow of a gate evaluation where, after the initial stabilization delay period, not all gates have succeeded at the sampling interval. In this case, after a timeout period expires, the deployment is rejected. We can see gate one failed twice while gate two was passing and then gate one passed and gate two failed. Not all three gates succeeded at the same time.</p>
<h1 id="Lab-02-Set-Up-Stages"><a href="#Lab-02-Set-Up-Stages" class="headerlink" title="Lab 02: Set Up Stages"></a>Lab 02: Set Up Stages</h1><p>Now that we have our prerequisites done, we can come down to our pipelines and releases and add a new pipeline. There are some templates here to help you get started with the tasks. In our case, we’re just gonna use the blank pipeline, and we need to bolt on our two artifacts. One, we have the HTML code, and two, we have the Azure ARM templates. So we’re gonna pick an artifact and we’ll use the Azure repo first.</p>
<p>If we choose the drop-down project CDLab, we’ll see our hello world. I’ll select the master branch and latest version of that code. An alias is created underscore in front of the name, and we have our first artifact.</p>
<p>Next, we’re gonna add again and we’re gonna choose Github. And now we can see the service connections we created earlier are available here. We wanna search for a repository, as if we were in Github. So this is now using the Azure quickstart templates. If I use the ellipses, I can search partial names and find the projects I need. From the drop-down, we’ll choose master and latest version.</p>
<p>We see the alias is created with an underscore and we have our first two artifacts. Now we’d like to create a development environment, so we’re gonna push these two artifacts through four stages. There’s gonna be development and test and pre-prod and production. Right now, we’re just gonna focus on doing the development and test stages. So we’ll create a new stage, empty, and we’ll call it development. From the development stage, we wanna add another stage called test and, by default, it adds it in order. We can look at the pre-deployment conditions here for the test stage and if we look at the visual representation of what’s occurring, if we change our option here for select trigger for the stage after release or manual, we can see that line disappears.</p>
<p>In our case, we are deploying after a stage and we’re deploying after the development stage. You can the different choices here in the dropdown. We just need to give ourself a name. And we can save this as our first revision. We now have our first pipeline. If we choose Create Release, we’ll get a new release option and we can click Create. We’ll see there’s Release-1 and we can also go view that release here under View Releases. And here’s a history of our releases. So we can see the development environments currently in progress and there’s a test environment to go.</p>
<p>If we dive into this release, we get a more visual representation of what’s happening and we can look into the logs of what the agents are doing and the tasks that they’re executing. So currently it’s downloading deltas. However, we had to trigger that manually, which is not really what we wanna do. We wanna automate this process. So while this is underway, we’re gonna go click edit our pipeline again and we’re gonna add some triggers.</p>
<p>What we want to do is when we push code or pull code into our HTML branch, we wanna <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-continuous-delivery-on-azure/creating-a-release/">create the releases</a> for our development environment and production, so we want a pull request we’ll push into production and a development request we’ll push through the development and test environments. We’re also gonna create a schedule for the test so every Monday to Friday at 3 a.m. the test environment will be updated with the latest development code.</p>
<p>If we click on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-continuous-delivery-on-azure/triggers-approvals-and-release-gates/">continuous deployment trigger</a> for html-docs-hello-world, we can enable both the pull request and push, and we’ll create a filter for the master branch on the pull request. We’re given a warning here which shows us that zero out of two stages are enabled. That’s fine. We’ll close that. In the pre-deployment conditions for the development environment, we do wanna add an artifact filter. So we don’t wanna update the development branch with master code, so we’ll pick our html-docs-hello-world and we’ll put the filter in there for dev. In the pre-deployment conditions for test, we’re gonna add a schedule, and Monday to Friday 3 a.m. Close that and we’ll save. So we’ll say Added Triggers.</p>
<p>If we bring up our Visual Studio code that we had connected earlier, we’re gonna just change this name and we’ll just say a tiny hello world application. Save. You’ll need to commit that change, and synchronize it to the cloud. So now, as that synchronizes to the cloud and we come back and look at our release history or our releases, we can see a second release was created and it’s kicking off now. If we look at that release, we can actually see here this artifact is the thing that triggered the release and we’re running again. I’ll just pause this while it downloads the artifacts and we’ll see what happens with the test environment.</p>
<p>We can see the development environment’s succeeded and our test environment is scheduled. We could choose to deploy that manually. For now, we’ve completed our first successful trigger to push code through our pipeline.</p>
<h1 id="Creating-a-Release"><a href="#Creating-a-Release" class="headerlink" title="Creating a Release"></a>Creating a Release</h1><p>Next, we’re going to explore the specifics of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-continuous-delivery-on-azure/lab-03-creating-a-pipeline/">creating a release pipeline</a>. Variables allow a pipeline to be dynamic, meaning we can feed data from one task into the next task. It means we can reuse tasks, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-continuous-delivery-on-azure/lab-02-set-up-stages/">stages</a>, and pipelines for multiple deployments. We can scope variables to different parts of the release, for instance, we can set up a set of variables and scope it to the development stage and then create another set of variables with the same name for a production stage.</p>
<p>You can use variable groups to store values that you want to take control of, and make them available across multiple pipelines. Azure DevOps also has a number of predefined variables as shown here. Here’s a brief look at a few items accessible from the agent variable. The Microsoft documentation will help you explore what these predefined variables are and what you can use them for. It is highly likely that in your pipeline you’ll need to pass through secure data. This could be an SSH key, passwords, API secrets, ClientID or even configuration settings. Variables have a built-in option to store secrets securely. It is important to use this flag as shown here in the screenshot as it means the secrets will be hidden while being used, for example, in the output of log files.</p>
<p>Another option is to store your secret in Azure Key Vault. If you create a Key Vault in Azure and populate a secret, then you’ll need to authorize Azure DevOps to retrieve the secrets from the Key Vault itself. Once authorized, you’ll be able to add secrets directly from Key Vault to Azure DevOps variable groups as we can see here on the screenshot. And now we can see the secret in Azure DevOps.</p>
<p>A pipeline contains tasks which can be executed repeatability often with parameters and variables used to drive dynamic execution. There are many standard tasks that come out of the box and there’s a growing list of tasks available on the marketplace. There are also generic shell tasks which will let you execute your own custom script like PowerShell or SSH tasks. You can browse marketplace tasks from within the Azure DevOps console. If you select Add, you’ll be walked through the process of installing that task into your DevOps environment.</p>
<p>Task groups allow you to group a series of tasks, already defined, into a single reusable task. This task group can be added to the pipeline, just like any other task. You can define configuration variables, allowing the task to be reusable. A major advantage of task groups is that it allows groups of tasks to be centrally managed, reducing the complexity involved of managing this sequence at scale.</p>
<h1 id="Lab-03-Creating-a-Pipeline"><a href="#Lab-03-Creating-a-Pipeline" class="headerlink" title="Lab 03: Creating a Pipeline"></a>Lab 03: Creating a Pipeline</h1><p>So now that we’ve created our skeleton of the release pipeline, we want to populate those with tasks that will deploy the web application. The first step in this process, though, is we’re gonna create some variables groups. So we go to variable group, add variable group, we’re gonna call this VarDev. And we’re gonna add three variables to this. We’re gonna add the ResourceGroupName. We’re gonna add the SlotName, and the WebAppName. Resource group, we’ll call WebAppDev. The slot, we already have a production slot so we’re gonna add one for development, and the web app needs to be unique within Azure itself. So I’ll just use my initials, the date, mywebapp and add dev on the end and save.</p>
<p>To save time, we’re gonna clone this variable set and we’ll call it VarProd. And take the dev off here, we’re gonna call this PreProduction or PreProd. And take the dev off the end, save. If we go back to the library, we can see we’ve created two, VarDev, VarProd, and we can now go to our release. Here’s our pipeline and the releases we have so far. If we edit that release and go to variables, first thing we’re gonna do is add ourself a single variable for the pipeline release called Region. So that’s the Azure region we’re gonna deploy to. In this case, I’m in West US and that’s where we’re gonna deploy it. We go to the variable group, we can link a variable group. And we’re gonna link VarDev to the stages rather than the release for test and development.</p>
<p>So now we can return to the pipeline and add some tasks. If we click on the view stage tasks here, we’ll add our first task, and we’ll search for Azure resource group deployment and add. If we click the task itself we can choose the subscription which we’ve already set up with the service connections. The resource group, we wanna use the variables, which was the dollar ResourceGroupName. And it is case sensitive so make sure you get that right. We can have Region in here and linked artifacts. In this case, we wanna use the Azure deployment template. So if we go look just briefly at the GitHub project for the quickstart template <a target="_blank" rel="noopener" href="https://github.com/Azure/azure-quickstart-templates/tree/master/quickstarts/microsoft.web/webapp-basic-windows">101-webapp-basic-windows</a>, we can see we’ve got the azuredeploy.json and the parameters file.</p>
<p>If we look at the Azure deploy file, we can see we’ve got a WebAppName, so we need to be case sensitive of that. And the rest of these parameters we’re gonna pick up through the default parameters. So if we use the ellipsis, we can see our two, our source artifacts, and we have the hello world from our Azure Repo and the GitHub quickstart template.</p>
<p>If we open this up and scroll down, 101-webapp-basic-windows, expand that and choose the azuredeploy.json file. So that’s the template we wanna use. Now we wanna do the same thing and pick out the parameters file. Scroll down. Parameters. In our case, we want to give a parameterized, our parameter for the web application name to this template. But instead of creating our own parameter file, we’ll just override one particular parameter. So that’s the parameter we wanna override. So we’re just gonna do a dash, parameter, dollar, WebAppName. And there are other deployment options there, we can choose the names and outputs and other things. We’re not gonna touch any of that at the moment. We’re gonna add our next job, which is an archive job.</p>
<p>We’re looking for the HTML docs root directory. So we don’t need all the Azure quickstart templates, that’s really large, we just wanna zip up the docs file there. We’re also gonna un-tick this prepend root. We don’t want the root name on that, we just wanna deploy directly to it. If we have any trouble, we can use the verbose output, so let’s tick that on. The next task we wanna add is Azure PowerShell. You have a choice here of using a file path for a script or we’re gonna use some inline code. I’ve already typed this out to save some time, but we’ll review it. Pick your service connection. So the code here is:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">New-AzWebAppSlot -ResourceGroupName $(ResourceGroupName) -Name $(webAppName)-webapp -Slot $(SlotName) -AppServicePlan AppServicePlan-$(WebAppName)</span><br></pre></td></tr></table></figure>

<p>which is what the template does, so it actually adds <code>-WebApp</code> to the name and uses the app’s service name for that plan. So we’ll save that there.</p>
<p>We just need to come down, we’ve still got something wrong, it wants to know what version of PowerShell. So we’re just gonna use the latest version that’s installed. We’re gonna add another task which is the Azure app service. Deploy. Again, pick the service connection. I’m gonna copy these parameters in to save some time. So <code>$(webAppName)-webapp</code>. We wanna deploy to a slot, right. Paste in the ResourceGroupName here and the SlotName we want is again the parameter we’ve already created.</p>
<p>Now, in this case, we can see what’s next, we’re looking for the package or folder. So that’s what we’re creating in this archive file. So if we look at this output here, to keep things simple, we’re gonna call this the WebAppName. And we’re just gonna copy that parameter as it is into our app service package deployment. So in our staging directory, these are one of these predefined variables that exist within Azure DevOps. And we have one last task here, select deployment method. So it will try to auto-detect. In our case, we just wanna use the zip deploy. So we’ve gone and created the zip file, we’ve told it where to find that zip file, and we’ve told it how to deploy it. Fingers crossed, that should be the dev stage complete. So we’ll save that.</p>
<p>Next, we wanna go and create the tasks for test. In the tasks for test, we’re gonna do a swap application slots. So under app service manage, if we add that. We’re gonna pick our Azure subscription again. We’re gonna pick the app service name. So that’s our $(WebAppName)-webapp, $(ResourceGroupName), SlotName, which in our case was dev and we wanna swap that with production. So for our test environment, we’re moving the development stage into the test stage which is the production version of this web application. And we’ll click save.</p>
<p>So to kick this off we can commit some code again. So we’ll call this My World HTML app. My code. Save all and commit. And let’s push that change.</p>
<p>If we go look at our releases, we now see Release-6. There’s some ones I deleted in between that. And we’ll watch the development. If dive into the release itself we can see that job’s being initialized, artifacts being downloaded. We can also actually look at the pipeline again while we’re waiting for that.</p>
<p>Under the test task itself, if we scroll down here, we can see artifacts to download. So for that environment, we don’t actually need to download all this content, so that’s a very large file. So we’re just gonna un-tick these two and that should speed up our deployments. So we’ll save that and just say remove artifacts. So that’s not gonna happen till next release, which is fine because that will still help us through this version. So we’re still deploying. Waiting for output.</p>
<p>If we go over to our Azure subscription. So we’re in the Azure portal. Refresh that, we can see the web apps here. And then currently we have no resources inside it but we have one resource group template being deployed. And here, here’s the deployment name, date, time. And we can dive in and start looking a little bit closer at all these resources. I’m going to pause the video at this point and return once this web application has deployed into this resource group.</p>
<p>So let’s have a look at our environment. We can see the web service template finished deploying. If we go to the resource group we can see our AppServicePlan, we can see the dev slot here, we can see the application itself. So we’ve deployed into the dev slot. If we open up that slot there and then we do browse, we can see the static sample HTML which is the page we should be seeing.</p>
<p>If we return to our deployment, we can see we’re not scheduled to go into the test environment yet. It’s waiting. If we go back to the web app and look at our production slot and browse, we’ll get the default template here. If we return to this piece and choose to deploy manually, so rather than waiting for that scheduled time, we’re just gonna say manual kick off and we’re gonna deploy. That job is now queued.</p>
<p>While we’re waiting for that to complete, we’ll come back and have a look at the logs for this task itself. And we can see the various initialize job, download artifacts, here we created the resource group, it gives you the times of how long everything’s taken, can see the inline PowerShell script we used to create the slot and the application deployment, and the job’s been finalized.</p>
<p>If we return to the release, we can see now this is the downloading artifacts. So because we changed that artifacts before this release, that change isn’t in effect in this release v6. So we’re gonna have to wait for that to download, so you see how removing those artifacts that we don’t use will speed up this deployment time. Here we can see we’re up to the swap slots. It’s running.</p>
<p>So we’ve now got the -dev website, which is now empty because we have swapped the application into the new site. And running in our production slot for the dev environment is our sample application. So we’ve successfully created and executed our first pipeline task and moved a web application and deployed the infrastructure for it using parameters and a continuous deployment trigger.</p>
<p>As a final task, you can change the code here and make sure you change the header. I’ve just executed the change on the wrong title, so we’ll change the header this time and we’ll commit that change, so I’ll save that. Welcome to DevOps. And we’ll push that change there, so it’s gonna initiate a new release. And what we’re gonna see is into the dev slot we’re gonna see the Welcome to DevOps header change and our production or test version of the application will have the old version of the website. So we’ll minimize this and there’s my previous release I pushed into the… Didn’t make the change in the right heading. I’ll pause the video and we’ll just wait and see how that goes. So that release has completed. If we go refresh the main website, we can see we’ve still got our HTML code there. And if we refresh that site, we can see in the development slot we now have Welcome to DevOps.</p>
<h1 id="Creating-a-Release-Agents"><a href="#Creating-a-Release-Agents" class="headerlink" title="Creating a Release: Agents"></a>Creating a Release: Agents</h1><p>To execute your pipeline, you need to use agents. Agents are the workhorse of Azure DevOps. When a task in a pipeline needs to be executed, a job is created and submitted to an agent queue to be completed by an agent. Microsoft provides hosted agents which live in the cloud and need to be able to access public endpoints like AWS or <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>. There is a variety of hosts you can select. Here is the current list. This will, of course, evolve over time. Depending on your pipeline, you may want to execute tasks at different locations, for instance, if you’re using a cloud-hosted Azure DevOps instance but want to execute your pipeline tasks on your on-premise servers, the cloud agents have no line of sight to your target systems; you’ll need to install a self-hosted agent. These agents will be able to talk to the cloud-hosted Azure DevOps and to the on-premise target servers.</p>
<p>If you need to execute these tasks in a private workspace, the Azure DevOps agent can be installed on a variety of platforms from Windows OS to macOS, Linux, and Docker, allowing for integrations into many different of environments. Agent pools allow you to group and create security boundaries for your agents. They are scoped to your DevOps organization, and agent pools can be shared across multiple teams and projects. This is because sometimes the infrastructure you’re deploying to is used by multiple teams.</p>
<p>A queue is used to collect jobs for an agent pool, which can be backed by one or many agents. When you create your release, you can specify what queue you want to use. Queues are scoped to your team project collection, this is so that you can share them across multiple team projects.</p>
<p>In your release pipeline, you can organize groups of tasks into jobs. Every build or deployment pipeline has at least one job, but can also be made up of many jobs. A simple example could be you have a mixed OS environment and you have tasks you need run on a Windows machine and tasks you need run on a Linux machine. In this case, you would define multiple release jobs, targeting different agents, to help achieve the desired deployment result.</p>
<p>Azure DevOps has the ability to run server jobs, which run tasks on the DevOps server itself. This is also referred to as agentless jobs. Agentless jobs do not require an agent to execute the task. The list of tasks that can be executed this way are small and contain tasks like manual intervention or REST API tasks.</p>
<p>Containers are becoming more common in today’s landscape. With a container, you can leverage existing containers built by vendors or by the community or by yourself. When you specify a container to run your tasks, the tasks will be executed inside that container.</p>
<p>Agents can be grouped together into deployment groups. Deployment groups are logical collections of agents that exist within an environment, for instance, Dev, Test or Production. Tasks that are targeted at a deployment group can run on some or all of the target servers, depending on the arguments you specify.</p>
<p>The last thing to cover in regards to jobs are Multi-configuration, and Multi-Agent. Typically, a task will run on a single agent, however, with multi-configuration, you can run the same set of tasks with different configurations. For instance, deploying the same website to different regions in Azure. With multi-agent, you can run the same set of tasks many times on a number of different agents. A good example would be a performance test, where you can run the same 10 tests on 20 different servers to validate the results.</p>
<h1 id="Ensuring-Release-Quality"><a href="#Ensuring-Release-Quality" class="headerlink" title="Ensuring Release Quality"></a>Ensuring Release Quality</h1><p>In some GitHub projects, you may have noticed these badges. The badges can define the states and different aspects of a project. These tags can be generated by Azure DevOps. This can help generate a view of what state various parts of the project are in, regarding build, deployments, release or even compliance.</p>
<p>We’ve already looked at release gates. If we explore this idea further, we can use release gates for two different purposes. The type we discussed earlier was using gates for automatic approval of a release. We can also use a release gate as a quality check. This could mean we create a query against work items that are looking for any outstanding bugs, issues, incidents against this release version. We could also do it for security scanning the code base or scanning artifacts like containers for security compliance.</p>
<p>Automating tests is another challenge when we start looking at incorporating it into continuous delivery pipelines. To automate testing, this chart has been designed and can help us define tests into four quadrants that helps us define what test we can automate. Tests in quadrant one are designed at the lowest level to test specific functionality, generally allowing them to be automated easier than some other tests. Quadrant four focuses on tests that require tooling to help, things like load testing, measuring performance, or security analysis. Tests in quadrant two and three move into the more complicated tests that are generally harder to automate or that simply require user interaction.</p>
<p>Documentation can have a number of different purposes from design, tracking work items, logging issues and bugs, installation, configuration, through to product operational manuals. There are a variety of approaches to this. One solution might not fit all use cases and you may need to use a combination. When we introduce more automation using release pipelines and increase the number of changes in an environment, we need to ensure that supporting documentation stays in sync.</p>
<p>Documentation can describe the changes that are present and being released into each environment and also needs to be in step with the product or environment changes, including a manual of how these new features and functionalities work. Manuals or documentation that we release together with the product should be treated as source code also.</p>
<p>A common method is using text files sometimes written using markdown syntax that are stored in the same directory or repository as the code. Another option is to create a new repository like a wiki. There are also wikis in Azure DevOps. In addition, we can use services like SharePoint or third-party solutions like Confluence by Atlassian. Another option is using work items to store and capture release notes. Depending on how you track your tasks, you could have bugs, tasks, user stories or other items which you can extract data from a particular field to cobble together a release notes text block allowing the task of extracting this text to be fully automated.</p>
<p>Now that we have covered ensuring the quality of a release pipeline, let’s return to the lab and look at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-continuous-delivery-on-azure/lab-04-clones-stages/">cloning</a> and finishing off our lab environment.</p>
<h1 id="Lab-04-Clones-Stages"><a href="#Lab-04-Clones-Stages" class="headerlink" title="Lab 04: Clones Stages"></a>Lab 04: Clones Stages</h1><p>The next task we want to do is clone the stage tasks. So we’re just gonna clone this one and if we go into this task, we’re gonna pull this PreProd. We’re also gonna change the pre-production conditions, so this one’s actually after release, and we enabled the pull request previously on this one, so this is a pull request deployment, so we can turn that on for this particular stage, and we need to apply the artifact filter for html-docs, and this is for the master branch.</p>
<p>We’ll close that, and we need to clone the Test environment to be our production, so we’re gonna swap this slot with the PreProduction slot. So this is called Production, and we want this to be after the stage PreProd, and for Production, we don’t want it to be scheduled, we actually want to get some approvals, so we need a responsible person before we go through. There are other options here around the user can’t approve their own request. For our purposes, this is fine.</p>
<p>Next, we’ll need to change the variable scope. So at the moment, we’ve connected VarDev into these other environments we’ve just created. So we’re gonna change the scope of that and untick it for PreProd and Production, and we’re going to link ourselves with the VarProd, particular stages, and PreProduction, and Production. So we’ve now mapped to a different set of parameters to our secondary pipeline.</p>
<p>So let’s kick this one off. We’re gonna create the release manually and we’re gonna say here, select stages for manual trigger, so we’re gonna select PreProd, we’ll go to the release, and we can see now that the artifact doesn’t meet the conditions, but we can deploy this one. Job waiting. So we’ll let that go, and in the meantime, we’re gonna add some badges to our HTML pages to the… We’re gonna add some badges to the code, and if we go into options, under integrations, we can see here, we can say enable the deployment stage badges. Now each one of these is the actual badge for Development, Test, Production, PreProduction, and we can put that into our Visual Studio code.</p>
<p>Let’s copy the Development badge, and we’ll bring up Visual Studio Code. We’re gonna make this change in the markdown file, so let’s bring the markdown file back, and put the status at the top. We can see, just used markdown syntax and put in the pasted link from each of the different badges for each of the releases. We can save that and commit that code. And push that into dev branch. Save it first, come into our files section, and we’ll look at the html-docs for the dev branch, and look at the markdown syntax, we can see we’ve got three badges, never deployed, succeeded, succeeded, and I’ve got a syntax mistake here. Brackets the wrong way around. Go.</p>
<p>So by enabling those deployments, we can now see the different states in our markdown page, in our Github Repo, and we can obviously see that the PreProduction environment we manually kicked off deployed, now we’d like to do one last thing and just test if that pull request will work. So we can do that from within here. If we go to our Branches, we can see we’ve got the just now and our master branch which hasn’t changed. So if we go to our Pull requests, we’re gonna create a new pull request, dev into master, Pull trigger test. You can associate different items to see what’s changed. In this case, it’s gonna push everything we’ve done; we’re just gonna complete the request.</p>
<p>It’s important here that for this example, we’re not gonna delete the branch, so this is a nondestructive branch change, and complete the merge. If we return to our Pipeline Releases, we can see now we’ve got the pull request here, merge, and Release-14’s been created from the master. So again, we can see the trigger that effects this, we can see this artifact condition hasn’t been met because it’s not from the dev branch, whereas the artifact condition here has been met.</p>
<p>We still have an existing deployment in the background, so let’s go back to… Last deployment is down here. So that’s the one we triggered manually. We’ll have to wait for these to finish before we can test the release into production. Here we can see that the PreProduction’s kicked off. If we look at our Azure portal, at our Resource groups, we can see we’ve got a new resource group for a production version of this application. So we’ve got the PreProd slot, the production app. So we can see the PreProd deployment finished, and we’re now pending an approval for Matthew. So I can simply approve that task, go for production.</p>
<p>Before we do that, if we go to our webapp and look at the PreProd, and do a browse, we’ve got a Welcome to DevOps, and we go back to the webapp and our production slot, and browse. There’s nothing there. Go for production, Approve. It is now kicked off. We’ll give it a few minutes. You see in this case it isn’t downloading those artifacts now that we’ve taken that step out, so initialize the job and then swap the slots. So we see production has succeeded. We refresh this, we have Welcome to DevOps, and our PreProd environment will be blank. So I hope this walkthrough has been helpful and given you a good base to learn some of the other aspects of Azure DevOps.</p>
<h1 id="Implementing-Deployment-Patterns"><a href="#Implementing-Deployment-Patterns" class="headerlink" title="Implementing Deployment Patterns"></a>Implementing Deployment Patterns</h1><p>A deployment pattern refers to the process which you move an application, or piece of code, into production. Traditional methods used to involve large changes, less often. In the past two decades, the speed of IT has given rise to a business that needs to be more agile and respond fast to customer needs. This means they need to be able to handle changes and push out changes in infrastructure and applications faster. Organizations are learning to incorporate change on a regular basis—monthly, weekly, daily or even hourly—into their business. In this last section, we’re going to review some modern deployment patterns.</p>
<p>We have all likely heard of environment names such as dev, test, dev&#x2F;test, QA, pre-prod, and production. Depending on the size of your budget, you can see more or less of these environments. A common problem with these in the past has been keeping these environments consistent. Often, they are used for many different purposes, testing different technologies and applications all in one place. This can lead to configuration drift, making it harder to guarantee that testing in one environment is a true representation of what will happen moving into the next environment.</p>
<p>As application development has changed, the way we deploy and test applications has also changed: being able to utilize platforms as service, or PaaS, also the advent of containers, even through to infrastructure as code. All these types of things can be versioned and related to artifacts and stored and released using agile-related deployment patterns. Let’s have a look at some of these modern patterns.</p>
<p>Blue-green deployment is a technique that can reduce downtime and risk by running two identical production environments called Blue and Green. At any one time, only one environment is live, with the live environment serving all the production traffic. Often, a router is used to redirect the traffic. Code is released to the Blue environment and traffic is redirected from Green to Blue. This can allow you to ensure the new environment is truly running before flipping the switch to redirect the traffic without interrupting the user’s experience. It also has the advantage that if you need to roll back the change you can redirect the traffic back to the last known good deployment very easily.</p>
<p>A canary release means that most users will continue to use the known good application (in this case, v1). A small number of users are given access, or are redirected, to a new version of the application (in this case, v1.1). There are various ways you can filter these users to the canary release, through the user login IDs, a cookie, source IP address. The method itself will be dependent on the type of application and how you are filtering these users through. The impact of the change is small and if there are any issues, changes can be reversed or corrected quickly with minimal impact to the larger user base.</p>
<p>The next pattern has a couple of different names: feature toggles, feature flag or feature switch. It is the concept of adding a flag to a feature so that it can be enabled, or disabled, to a specific user or group of users, in the backend or frontend of the application. This allows a feature to be tested even before it is completed and ready for release. Feature toggles are used to hide, enable, or disable the feature. If we consider <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> itself, you may have seen video or demonstrations of Azure private preview features. Microsoft often invites people, or companies, into these preview groups before a feature goes GA. This allows them to test and work to refine that feature with a customer before it goes live.</p>
<p>Progressive exposure deployment is where new software is exposed to a subset of users that gets extended gradually over time. A conceptual illustration here shows the inner circle called canaries, followed by early adopters, and finally, the users sphere representing the remaining user base. This is often referred to as the impact, or blast radius, where the release is evaluated and observed before exposing them to more users. One of the terms that is often used in DevOps and Agile processes is, “Fail early and fail fast.” What this means is the sooner we know there is an issue, the sooner we can correct it. However, with some of these programs, they are so large that testing every aspect of a system is virtually impossible.</p>
<p>An example of this is Windows as a service. Windows has millions and millions of lines of code. Microsoft relies on their user base and customers to implement and test these new features against existing systems to make sure they continue to function as expected. We can see how the impact on the user base is affected over time. An advantage of this method is that users know they are in an early adoption program and they are often looking for, or expecting to see, issues.</p>
<p>A&#x2F;B testing, also known as split testing or bucket testing, is a method of comparing two versions of a webpage, or app, against each other to determine which one performs better. A&#x2F;B testing is mostly an experiment where two or more variants of a page are shown to users at random, and statistical analysis is used to determine which variant performs better for the given conversion goal. Imagine a shopping network testing a new checkout page to encourage users to complete their purchase. You can analyze the percentage of users that used version A of the website and purchased their items in the shopping cart, versus version B.</p>
<p>Dark launching is the process of releasing production-ready features and released to a subset of users gradually, in order to get the user feedback, or test the application or infrastructure performance. Features are wrapped in a way where they can be toggled which is used to control who gets the new features and when. Dark launching is a popular method to test new features gradually, by releasing them to small groups of users over time. This way, they can see how users respond to the changes and make adjustments before continuing. They’re called dark launches because these changes typically aren’t publicly announced. These changes are released gradually to 1%, 5% 20%, and so on. If there are any issues with these new features, the existing systems are still in place and it’s easy to roll back the change to fix the problem.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This brings us to the end of the course. I hope you found the content useful, and you found the demo gave you a good experience of using Azure DevOps to create and implement a release pipeline. Azure DevOps is a powerful and flexible tool, and I wish you luck using it now and in the future. Thank you for taking the time to watch this course, please don’t forget to give this course a rating.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Continuous-Integration-using-Azure-Pipelines-in-Azure-DevOps-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Continuous-Integration-using-Azure-Pipelines-in-Azure-DevOps-13/" class="post-title-link" itemprop="url">AZ-400-Continuous-Integration-using-Azure-Pipelines-in-Azure-DevOps-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:46" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:46-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 11:16:54" itemprop="dateModified" datetime="2022-11-27T11:16:54-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Continuous-Integration-using-Azure-Pipelines-in-Azure-DevOps-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Continuous-Integration-using-Azure-Pipelines-in-Azure-DevOps-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Implementing-a-Build-Strategy-for-Continuous-Integration-With-Azure-DevOps-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Implementing-a-Build-Strategy-for-Continuous-Integration-With-Azure-DevOps-12/" class="post-title-link" itemprop="url">AZ-400-Implementing-a-Build-Strategy-for-Continuous-Integration-With-Azure-DevOps-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:45" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:45-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 11:15:58" itemprop="dateModified" datetime="2022-11-27T11:15:58-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Implementing-a-Build-Strategy-for-Continuous-Integration-With-Azure-DevOps-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Implementing-a-Build-Strategy-for-Continuous-Integration-With-Azure-DevOps-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to the Implementing a Build Strategy for Continuous Integration With Azure DevOps course.</p>
<p>My name is Cory W. Cordell and I’ll be your instructor for this course. I’ve worked as a DevOps engineer and architect on both greenfield and existing DevOps integrations for small to large companies across the globe in an effort to establish DevOps culture and best practices.</p>
<p>I’ve also worked with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps for the past four years and have experienced firsthand its trials and growth as it continues to develop into a robust end-to-end planning and automation tool.</p>
<p>I look forward to accompanying you along your endeavor to learn more about Azure DevOps.</p>
<p>If you have a desire to utilize Azure DevOps for your CI builds, this is a course for you.</p>
<p>This course is designed to teach you about some of the Azure DevOps concepts in order to optimize end-to-end continuous integration.</p>
<p>In this course, you will learn triggering builds for various use cases, making parallel build work for you, and setting up automated builds.</p>
<p>To follow along in this course you will need an active Azure DevOps account, a good understanding of the software development lifecycle, to know how pull requests work, an understanding of continuous integration and continuous development best practices, knowledge of creating an Azure DevOps Pipeline, familiarity with Azure Pipelines YAML files, and Azure Pipelines stages, jobs, tasks, and conditions.</p>
<p>If you need help for any reason, please contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. After completing, don’t forget to rate the course.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-tools/">Build Tools</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-triggers/">Build Triggers</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/parallel-builds/">Parallel Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/hybrid-builds/">Hybrid Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Build-Tools"><a href="#Build-Tools" class="headerlink" title="Build Tools"></a>Build Tools</h1><p>In this section, we will take a look at some popular build tools and then some recommendations on how to choose a build tool.</p>
<p>Build tools are invaluable when it comes to automating repeatable processes associated with a continuous integration. There are several options to choose from and the number of options is increasing.</p>
<p>The first question that may come to mind is what is the best build tool? That depends a lot on the use case and preference. No build tool is perfect but considering the tools that check off the majority of your criteria is a good place to start. Perhaps the most defining characteristic of a build tool is whether it is entirely cloud-based or can be hosted on-premise.</p>
<p>Let’s take a quick look at some of the more popular build tools out there today.</p>
<p>Jenkins has been around a long time and offers the ability to run the tool just about anywhere even as a docker container on your local system as we’ll see later. Jenkins has a portal base interface and has the option of using declarative style or pile-on definitions via Groovy.</p>
<p>Bamboo is an Atlassian product which is a paid platform. It has good integration with some of their other popular tools, Jira and Bitbucket.</p>
<p>Circle CI offers options between a cloud-based platform or on-premise and has good support.</p>
<p>Drone is a relatively new tool with some innovative features such as easy plugins.</p>
<p>Azure DevOps is truly an end-to-end dev ops tool. It also provides a lot of integration with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> services as well as planning and testing frameworks.</p>
<p>There are many more build tools out there. I urge you to explore and see what works best for your use cases.</p>
<p>Let’s go over some guidelines that may help you pick a build tool.</p>
<p>When it comes to build tools, like most things, it isn’t necessarily all about the tool but also about what else these tools have to offer. Take into consideration the peripheral features of the tool.</p>
<p>Do you need a build tool that is quick to learn so that other teams can come up to speed quickly? Or do you need something that is more robust with plenty of options and flexibility at the expense of a greater time investment?</p>
<p>Also consider professional support. It may be well worth the investment.</p>
<p>There are many cloud-based CI tools but if you need something that works on-premise then make sure to factor that in.</p>
<p>Greenfield DevOps or revamping may be a good time to consider a tool with top planning integration.</p>
<p>It may also be beneficial to consider the monetary investment or at least try a tool for free in order to make a better-informed choice.</p>
<p>Whatever the case, it’s up to you to determine the requirements and find a tool that satisfies as many of those as possible.</p>
<h3 id="Lectures-1"><a href="#Lectures-1" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-triggers/">Build Triggers</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/parallel-builds/">Parallel Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/hybrid-builds/">Hybrid Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Build-Triggers"><a href="#Build-Triggers" class="headerlink" title="Build Triggers"></a>Build Triggers</h1><p>Build triggers initiate builds when some designated action or actions are performed and any required criteria is met.</p>
<p>We will cover triggers in detail over the next several minutes. For context, here is a trigger example. The snippet shows all of the types of triggers that can be defined in the azure-pipelines.yml file, branch triggers, tag triggers, path triggers, and pull request triggers.</p>
<p>Triggers are evaluated before the build runtime since they determine if the build is run at all. Thus, variables and other event data are not available when the build is triggered.</p>
<p>There are two categories of triggers, automated and manual.</p>
<p>Automated triggers are event or schedule based and are essential when automation is desired. Automated triggers are usually accompanied by supporting criteria, such as a specific branch or directory being updated.</p>
<p>Manual triggers are a way to start a CI build process without using an automated trigger. Manual triggers can be performed at will and without meeting automated trigger criteria. Builds can be manually started from the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps Pipelines portal, the Azure CLI, or the Azure DevOps API.</p>
<p>Automated build triggers can be further refined into types:</p>
<ul>
<li>Repository triggers start a build based on a repository commit, or a pull request being initiated or updated.</li>
<li>Schedule triggers can be planned to start builds at a predetermined time or interval.</li>
<li>Pipeline triggers can kick off another pipeline upon completion with optional criteria such as successful or failed.</li>
</ul>
<p>Triggers can also have criteria defined that will further refine when a trigger kicks off a build:</p>
<ul>
<li>Branch or tag is used to include or exclude branches and&#x2F;or tags that can trigger the build.</li>
<li>Path is used to include or exclude a directory or file path that can trigger a build.</li>
<li>Pull request is used to specify a list of branches as the pull request target to include or exclude to determine whether to kick off a build when a pull request is opened or updated.</li>
</ul>
<p>There are also some additional options that can be associated with triggers:</p>
<ul>
<li>Batch is a boolean value indicating whether to start another build while another is running for the same branch.</li>
<li>None will disable any automatic triggers, making the only way to run a build, manually.</li>
</ul>
<p>There are some important distinctions between what triggers can be used in conjunction with the resources being used as the trigger. Most notably are the differences between the pull request triggers when used with the various version control systems and Git hosts.</p>
<p>For instance, Azure Git Repositories use branch policies to control pull request triggers. Pull requests do not apply to Team Foundation Version Control.</p>
<p>Next, we will look at strategy around using build triggers. Careful design of trigger strategy can greatly impact overall goals. What does that mean exactly?</p>
<p>Begin by asking a simple question, when should this build run? Most commonly, the answer is when the code changes.</p>
<p>Another often-used trigger is when a pull request is initiated or updated. This helps ensure that code passes checks prior to merging to a main branch, provided the pipeline is set up properly.</p>
<p>A somewhat less common scenario is to start a build at a certain time or interval. This could be used to run daily tasks or to bulk build all repository changes at the end of day.</p>
<p>It is also possible to trigger a loosely coupled build based off of the completion of another pipeline. A good example would be to have a build that serves as a base for other builds, and then all upstream builds could use the dependency in a new build.</p>
<p>Whatever the need, there is a way to automate the CI build to ensure code quality and availability is always on par.</p>
<p>Branch triggers are the most common type of repository trigger. Triggers are specified in the azure-pipelines.yml file with the keyword trigger. Branch triggers specify which branches should start a build when updated.</p>
<p>The default configuration for a branch trigger is all branches. This means that a push to any branch will start a build for the branch. This will also trigger on branches when they are first created. For example, if a new branch is created from master on the git host or locally and pushed, a build will be started for the new branch.</p>
<p>This default behavior can also explicitly be defined using an asterisk. Since an asterisk is a reserved character in YAML, any text beginning with an asterisk will need to be quoted.</p>
<p>Trigger definitions are defined as an array of branches. This allows multiple branches to be included with this syntax.</p>
<p>This simplified syntax works only when we only want to include branches. More verbose syntax can be used to configure more options.</p>
<p>Adding the keyword branches under triggers and then include under branches is equivalent to what we just had.</p>
<p>The exclude keyword can also be used in the more verbose syntax. This works well when all branches except the ones listed should trigger a build.</p>
<p>Include and exclude can be used in conjunction, but specifying either include or exclude, implies that any branches not listed are excluded if include is used, and included if exclude is used in the filter.</p>
<p>For instance, if master, dev, and feature branches exist and only the master branch is listed under include, then dev and feature branches are implicitly excluded. The same goes for master being listed under exclude, dev and feature branches are implicitly included.</p>
<p>Using include and exclude appropriately allows us to use the shortest path, but it also allows for flexibility when used in conjunction with wildcards. Wildcards allow specifying branches where more than one possibility may exist.</p>
<p>There are two wildcards, an asterisk is used for zero or more characters, and a question mark, a single character.</p>
<p>It’s a common practice to name all feature branches with the prefix feature&#x2F;. All branches that begin with the prefix feature&#x2F; can be specified using an asterisk wildcard.</p>
<p>The question mark can be used to stand in for a single character. For instance, if there are multiple branches that begin with wip- followed by a single digit then the filter could match on wip-?.</p>
<p>The wildcards can also be used in the middle or the beginning of a branch name. Just remember that the text will need to be quoted if beginning with a special character.</p>
<p>Let’s look at more advanced use case where both include and exclude are used with wildcards. All feature branches are included in the branch triggers except the excluded branches that match features&#x2F;wip-?-signin-form-*.</p>
<p>Tags can also serve as triggers. They can be specified under the branches keyword and included or excluded just like branch names using Git syntax for tags.</p>
<p>As an alternative, tags can be included and excluded under their own keyword at the same level as branches without the Git syntax.</p>
<p>In either case, the same rules apply to using wildcards with tags as they do with branches.</p>
<p>By default, tags do not trigger pipelines. So, they must have a definition, or the default behavior will stand. Being able to include or exclude branches as triggers is a powerful feature, but it doesn’t allow for filtering across branches. It is very useful to be able to trigger or not trigger a build based on what files were changed.</p>
<p>For instance, triggering a build when only the README file is updated may be a waste of resources. The README is at the root of the repository, so it can be added without a directory path. Paths are also case sensitive.</p>
<p>Now changes to the README on any branch, will not trigger a build, however, if any files outside the exclusion are changed in the same push, then the build will still trigger, and that’s more often what we want.</p>
<p>The default behavior for paths is to include the root of the repository. This is equivalent to using the asterisk wildcard under include. This means that any files that change will trigger a build, provided that the branch requirement is met.</p>
<p>In fact, one requirement of using paths is that branch triggers are defined.</p>
<p>Entire directories can also be included or excluded. For example, a directory called development can be added to the include path.</p>
<p>Wildcard rules are different for paths than that of tags and branches. An optional asterisk can be added to the end of the path, however, it is equivalent to just specifying the directory. The asterisk is only allowed to be the last character and question marks are not allowed at all.</p>
<p>Include and exclude can be used together. A path can be excluded and child paths included since the path reaches deeper into the file structure. For instance, a directory named development can be excluded but its subdirectory named main can be included.</p>
<p>Branches, tags, and paths can be used together but special consideration must be applied.</p>
<p>Tags and paths are evaluated as an or when used with branch filters, thus, if a branch is a trigger and either a tag or path is included, either implicitly or explicitly, then the build is run, provided the criteria is met.</p>
<p>For instance, if the branch master is updated with tag version 3.1, then the build is run regardless of the path criteria. The same goes for the development&#x2F;main path being met and the tag criteria not.</p>
<p>In another scenario, if the master branch is updated and neither the path or the tag criteria is met then the build will not run.</p>
<p>Lastly, in the case that the branch criteria is not met, like the test branch is updated, then the build will not run because the test branch is excluded regardless of any tag or path criteria. </p>
<p>Paths defined for Azure DevOps Repos must be done in the portal. Although GitHub is being used for this course, it is worth showing the Azure path configuration.</p>
<p>I’ll traverse over to the Branches page for the project git repository that was created with the project. Clicking on the Options menu and then selecting the branch policies item will take us to the branch policies for the master branch on this repository. A build policy can be added to the build validation. Path filters can be added to the Path filter textbox. All paths are relative to the root, just like in the YAML. Separate paths with a semicolon and exclude paths with the exclamation point before the path. Remember that paths are case sensitive.</p>
<p>I can’t save the changes because a build pipeline is required in which to apply the policy. Since I have not made a pipeline that utilizes the Azure Git repository, then I won’t be able to save my changes. This is something to be aware of before trying to define paths.</p>
<p>Pull request triggers are used to start a build when the pull request target branch matches the PR trigger criteria. Pull request triggers and normal triggers can be defined in the same Azure pipelines file, but it isn’t required.</p>
<p>Pull request triggers help ensure that the branch intended to be merged with the target branch doesn’t break the build. This is done by merging the branch into the target branch outside of the git host, and then running that through the build process. By default, if the build does not fail then it will be deemed as valid, and can be permanently merged to the target branch, provided that review criteria is met.</p>
<p>As a safeguard against allowing non-validated code to be merged, any updates will invalidate the pull request build. In regard to the build, it will need to be run again in order to become valid.</p>
<p>The PR trigger structure is very similar to the normal trigger structure.</p>
<p>By default, all pull requests are triggers. This is equivalent to placing an asterisk under the pr keyword.</p>
<p>A PR can be set to include or exclude branches and paths but not tags, and the same wildcard rules also apply.</p>
<p>One difference between normal triggers and pull request triggers is the autoCancel boolean.</p>
<p>The default autoCancel value is true, which will invalidate any builds where the code has changed. The autoCancel keyword can be specified with a false value so that changes to files after the pull request is opened won’t invalidate the build. In other words, if the pull request build passes and then changes are made to the pull request code, then the source branch will still be able to merge with the target. Leaving this as the default will help ensure the integrity of the target branch.</p>
<p>For reviewers of pull requests, it may be convenient to perform some actions within the pull request’s detail page, such as manually running a build by issuing a comment on the pull request.</p>
<p>For example, a command to run a pull request’s associated Azure Pipeline can be accomplished with &#x2F;AzurePipelines run in the pull request’s comments. &#x2F;azp can be used in place of &#x2F;AzurePipelines for brevity.</p>
<p>There are a few other commands and use cases that I urge you to explore.</p>
<p>Unfortunately, these are specific to the Git host, so if available and you would like to use them, then further research is warranted.</p>
<p>It may be that a build should never be run automatically.</p>
<p>Specifying trigger none will disable automatic triggers, effectively making this pipeline a manual only start regardless of the branch, tag, or path.</p>
<p>Pull request triggers can also be set to none, so regardless of the pull request target, the build will not start automatically.</p>
<p>You may have foreseen an issue where starting a build every time code is pushed can tax resources. This is especially wasteful when not all code pushes need to run through the build process.</p>
<p>There are a couple of ways to mitigate this scenario, skip comments and batch builds.</p>
<p>Builds can be skipped by adding special text to the commit comment. Here are the possible variations.</p>
<p>For instance, within the message for the git commit, adding the bracketed skip ci text will skip the build.</p>
<p>This is very useful for work in progress, also known as WIP, pushes.</p>
<p>Another way to take better advantage of resources is to batch them. Normally, each push that matches the trigger criteria for a branch starts a new build. This is great for individualized feedback but if pushes are made in rapid succession, it could fill the build queue, depending on the number of agents and the build time required.</p>
<p>Builds on the same branch can be batched to conserve resources by using the keyword batch. By default, batch is false. Specifying a value of true will enable batch builds.</p>
<p>Once enabled, if a build for a branch is not in progress, then one will be started, provided the branch trigger criteria is met. If code is pushed again before the prior build completes, then the build will not start until the prior one finishes. So far, this may be a disadvantage if more than one agent is available. The potential resource conservation comes into play when more pushes are made before the initial build completes.</p>
<p>Once the blocking build has completed, then all pushes that have accumulated for the branch will be run together. Effectively, this will run the last push on the branch.</p>
<p>This may not be the desired behavior if build feedback is needed for each push. Using skip ci may be a better option in this case.</p>
<h3 id="Lectures-2"><a href="#Lectures-2" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-tools/">Build Tools</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/parallel-builds/">Parallel Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/hybrid-builds/">Hybrid Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Parallel-Builds"><a href="#Parallel-Builds" class="headerlink" title="Parallel Builds"></a>Parallel Builds</h1><p>Proper utilization of multi-agent or parallel builds is one of the most effective optimizations that can be performed. With careful strategy, significant time can be saved. However, as often the case, such efficiency comes at the cost of greater complexity and more stringent requirements. In this section, we will cover the criteria that needs to be met to enable parallel builds and the application of parallelism in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps pipelines.</p>
<p>Azure Pipelines wants to run builds in parallel and will do so unless the conditions are not agreeable.</p>
<p>Let’s look at what criteria must be met to allow parallelism.</p>
<p>Since build agents do the heavy lifting, more than one build agent is required if parallelism is desired. In fact, not only can’t a build be optimized to run in parallel, only one build or deployment can be run at one time with a single agent.</p>
<p>In relation to having more than one agent for an account, more than one agent must be available for the build to run in parallel. For example, an account could have five agents, but if four are busy running other builds or deployments, then only one is available for the build.</p>
<p>Also, the Azure Pipelines YAML file must be structured to run in parallel. This means that the build steps must be able to be broken into distinct jobs.</p>
<p>Furthermore, the number of jobs that can be run in parallel are limited by the availability of agents to run those jobs. For instance, if a build has four jobs that can be run concurrently, but only three agents are available, then only three jobs will be run in parallel at one given time unless another agent becomes free.</p>
<p>Given that jobs run in parallel when the agent availability criteria is met, breaking the pipeline down into multiple jobs is the only direct correlation between the Azure Pipelines’ YAML and parallelism.</p>
<p>Simply adding more jobs, each with at least one step, is enough to support multi-agent builds for a pipeline.</p>
<p>Adding more than one stage will also accomplish parallelism since, inherently, a stage has at least one job.</p>
<p>Stages and jobs can be added to further optimize the pipeline where it makes sense to do so.</p>
<p>There are often times when parallelism isn’t desired. This could be due to a dependency or the overhead of running a job in parallel.</p>
<p>Simply adding the dependsOn syntax under the job will prevent the job from running before the dependency. For example, adding dependsOn under the test job and specifying the validate job will ensure that the validate job is completed prior to running the test job.</p>
<p>The same can be accomplished when using stages. Just make sure the syntax is at the proper level.</p>
<p>Typically, the dependsOn syntax is placed close to the top of the job or stage to make the code logic more clear.</p>
<h3 id="Lectures-3"><a href="#Lectures-3" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-tools/">Build Tools</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-triggers/">Build Triggers</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/hybrid-builds/">Hybrid Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Hybrid-Builds"><a href="#Hybrid-Builds" class="headerlink" title="Hybrid Builds"></a>Hybrid Builds</h1><p>Hybrid builds make it possible to perform parts of a build on one or more platforms. This may be due to security requirements, proprietary software, resource allocation, tooling availability, compliance requirements, optimization strategy, or any multitude of other reasons.</p>
<p>For example, security concerns may prevent building sensitive code outside of private infrastructure. However, the cloud would be beneficial for platform testing. With hybrid builds, application code can be built on-premise and the artifact pushed to storage. From there, public infrastructure can be utilized for further testing and deployment.</p>
<p>Having a good design is paramount with any build, but hybrid builds require more planning due to the complexity of using more than one platform.</p>
<p>Isolating what parts of the build should be done on which platform is a good first step. The primary consideration for segregation is the level of interdependence. Each build will need to achieve a finish state that results in some complete action or artifact.</p>
<p>For example, a scenario where an application pipeline needs to start on-premises for security reasons but for optimization, the build process utilizes the public cloud to perform testing and deployment is a good use case. The build process starts the application build on-premises and produces an artifact as a docker image. The image is pushed into a private docker registry. From there, the public cloud is engaged to do integration, stress, and platform testing. If successful, the docker image is promoted for deployment.</p>
<p>Breaking down this process and identifying definite stages of transition is a good start. The other consideration is the trigger to start the testing process. For this example, there are multiple options for triggering the testing phase. The docker registry could be set to initiate upon a new push; a specific branch, tag, or comment could be used in conjunction with a Git repository; or triggering the pipeline directly using an API call at the end of the build phase.</p>
<p>There are many ways to accomplish hybrid builds. One such way is to use a pipeline tool, like Jenkins, on-premise. The tool can be configured to trigger on a push to a specific branch or branches. The result from that build could then further be processed in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps.</p>
<p>Let’s look at how this may be implemented. I’ve created a Jenkins file with some mock stages and steps to illustrate how a build might be configured. The Jenkins file will test the code, build the artifact, and then store the artifact. The last stage is to kick off any downstream automation.</p>
<p>In this case, an Azure pipeline’s kickoff is being locked. Typically, any failure at any point would result in a terminated failed build. This pipeline is counting on that behavior. The downstream automation should never occur if an error exists prior to this stage. The trigger for the succeeding build is usually the most technical part of a hybrid build. Careful consideration must be given in order to execute the handoff properly. Here, the Azure DevOps API or the Azure CLI could be used to kick off the Azure pipeline’s build.</p>
<p>Let’s look at the Azure Pipelines file to round out this hybrid design. The Azure Pipelines file contains the trigger specification, the pool, and the testing stage with parallel capable jobs for integration and stress testing. Of course, this is only an example. Much more can be done here.</p>
<h3 id="Lectures-4"><a href="#Lectures-4" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-tools/">Build Tools</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-triggers/">Build Triggers</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/parallel-builds/">Parallel Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Automation build tools are extremely powerful but can be complex. Putting forth the effort in learning to use <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps and developing the ability to design and implement effective and efficient pipelines to automate tasks will pay dividends.</p>
<p>Let’s do a brief review of what was covered and pull the individual pieces together.</p>
<p>A good place to start is with build tools. There are many pipeline tools available today. Each has associated characteristics that may or may not be desirable. Some can be run on-premises. Others may have professional support. Whatever criteria must be met, due diligence will need to be done to find one that fits your use case the best.</p>
<p>Azure DevOps is a robust, end-to-end planning, testing, and pipeline tool. Azure DevOps Pipelines subscribes to the configuration as code convention by using an Azure Pipelines YAML file to declare the build criteria and implementation.</p>
<p>Let’s take a look at an Azure Pipelines YAML file that has been prepared.</p>
<p>At the top of the file, we can find the trigger declaration. The pipeline is configured to trigger on the master and release branches. Specifying branches to include overrides the default of including all branches.</p>
<p>Triggers allow builds to be automatically started or excluded from starting based on matching criteria.</p>
<p>The branches syntax is not required if simply including branches to trigger on, however, it is required when options are desired such as exclude for branches and the use of paths and tags. </p>
<p>Wildcards can be used for pattern matching. The asterisk matches zero or more characters and the question mark, which is not used in this example, matches a single character.</p>
<p>By using include, implicitly all other branches not listed or matched are excluded. The same goes for exclude when used. All other branches not listed are implicitly included.</p>
<p>Paths allow file paths to be included or excluded. Any specified path also encompasses children of the specified directory, if specifying a directory. The default is to include root but this declaration has excluded development. By reaching deeper into the file structure, the directory main under development can be included. Everything else under development is excluded. </p>
<p>Tags can also be specified either under branches using git syntax or under the tags object. All tags are included here, which is converse to the default.</p>
<p>Keep in mind that branch conditions must be a match for the build to run and if tags or paths are specified, one of those must have criteria that matches for the build to run.</p>
<p>Pull request triggers allow branches and paths to be specified. This only applies to builds when pull requests are involved. Branches that are the target of the pull request, meaning the branch being merged into, can be specified to include or exclude. Although wildcards are not used here, they are very much applicable.</p>
<p>Paths are specified to include or exclude as well. The pull request paths are not required to be the same as the ones specified under normal trigger paths. It just happens to be the case in this example. The default is to include all pull request branches and the root path. Tags are not allowed in the pull request trigger specification.</p>
<p>Further down the file are the declared steps to be run. These steps can be logically grouped under jobs. Those jobs can be further grouped into stages. By default, every pipeline has one stage and one job. Although not the case in this example, if only one stage and job are needed then the syntax to explicitly define those can be omitted.</p>
<p>Here, there are two stages: validate_and_test and build_and_archive. Since there are multiple jobs, this would run in parallel if more than one agent is available and the dependsOn syntax wasn’t used to force the builds_and_archive job to wait for the validate_and_test job to successfully complete.</p>
<p>The dependsOn syntax can be used for jobs or stages. It is useful for not only establishing dependencies but to also optimize the build. For example, there may be a case where a job could be run in parallel but the overhead of doing so is a detriment to the build. In that case, forcing the build to be sequential would conserve resources.</p>
<p>Chain of result is also a consideration. Build steps typically use the results from prior steps to operate upon. Using more than one build agent means that jobs would either need to be completely independent or some other mechanism must be used to provide context from one job to another.</p>
<p>For example, the initial job for a pipeline tests and builds code, the next step may be to run integration tests on the built code. That could be done within the same job or it could be done in a separate job. If done in a separate job then the built application artifact would need to be made available to the downstream jobs. This could be easily done by storing the artifact at the end of the initial build job and then retrieving the artifact for each successive build.</p>
<p>In fact, often, not only does the code artifact need to be stored, configuration details and metadata need to be promoted. Artifact storage is certainly applicable to these details also. This data can be written to one or more files and then stored as one or more artifacts for later consumption.</p>
<p>These principles also apply to hybrid builds since hybrid builds are essentially build jobs performed across multiple platforms.</p>
<p>Hybrid builds can be started on one platform and then finished on another platform, such as using Jenkins to build the artifact and then using Azure DevOps to thoroughly test the artifact. </p>
<p>The build process needs to be broken into distinct jobs, while keeping in mind the criteria that needs to be met that prompted the utilization of hybrid builds.</p>
<p>There are several reasons for hybrid builds. This may be due to security requirements, proprietary software, resource allocation, tooling availability, compliance requirements, optimization strategy, or any multitude of other reasons.</p>
<p>One of the most important design items for hybrid builds is how the builds will be triggered. Most commonly this is done with hooks, Git triggers, or API calls.</p>
<p>Careful consideration must be given to what criteria defines a trigger, when it should trigger, how it should trigger, and what actions, if any, need to be performed upon failure of the build at critical points. This will ensure that the build process runs smoothly between platforms.</p>
<p>By understanding the advantages and limitations of build tools and applying the principles and techniques covered with build triggers, parallel builds, and hybrid builds, you will be well on your way to providing a foundation for a secure, repeatable, auditable, and complete continuous integration solutions for your projects.</p>
<h3 id="Lectures-5"><a href="#Lectures-5" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-tools/">Build Tools</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/build-triggers/">Build Triggers</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/parallel-builds/">Parallel Builds</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-a-build-strategy-for-continuous-integration-with-azure-devops/hybrid-builds/">Hybrid Builds</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Designing-an-Infrastructure-and-Configuration-Management-Strategy-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Designing-an-Infrastructure-and-Configuration-Management-Strategy-11/" class="post-title-link" itemprop="url">AZ-400-Designing-an-Infrastructure-and-Configuration-Management-Strategy-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:43" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:43-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 11:10:30" itemprop="dateModified" datetime="2022-11-27T11:10:30-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Designing-an-Infrastructure-and-Configuration-Management-Strategy-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Designing-an-Infrastructure-and-Configuration-Management-Strategy-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hi there. Welcome to Designing an Infrastructure and Configuration Management Strategy. My name is Thomas Mitchell and I’ll be taking you through this course.</p>
<p>I’m an Azure instructor at cloud Academy and I have over 25 years of IT experience, several of those with cloud technologies. If you have any questions about this course, feel free to connect with me on LinkedIn, or send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>This course is intended for those who are preparing for the <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/az-400-exam-prep-microsoft-azure-devops-solutions-1-1368/">AZ-400</a> exam. To get the most from this course, you should have a basic understanding of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a> and of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/devops/">DevOps</a> concepts.</p>
<p>We’re going to start things off by looking at hosting infrastructure. We’ll take a look at Infrastructure as a service options, platform as a service options, functions as a service options, and at some modern native app options.</p>
<p>We’ll then take an introductory look at Infrastructure as Code. You’ll learn what infrastructure as code means and you’ll learn about several tools and technologies that are used to deploy and manage infrastructure as code.</p>
<p>Later on, we’ll touch on some of the more common infrastructure as code tools and technologies. You’ll learn about things like Terraform, Azure Resource Manager, Chef, Puppet, and more.</p>
<p>After covering the different infrastructure as code tools and technologies, we’ll get into technical debt and templates, where we will look at what technical debt is and how to deal with it in templates.</p>
<p>Coming down the home stretch, you’ll learn about transient infrastructure and its role in the delivery lifecycle.</p>
<p>We’ll wrap things up by covering the mitigation of infrastructure state drift.</p>
<p>By the time you finish this course, you should have the foundational knowledge that is required to design an Infrastructure and Configuration Management Strategy.</p>
<p>We’d love to get your feedback on this course, so please give it a rating when you’re finished. If you’re ready to learn about designing an Infrastructure and Configuration Management Strategy, let’s get started.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/hosting-infrastructure/">Hosting Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/an-intro-to-infrastructure-as-code/">An Intro to Infrastructure as Code</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/infrastructure-as-code-iac-technologies/">Infrastructure as Code (IaC) Technologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/technical-debt-templates/">Technical Debt &amp; Templates</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/transient-infrastructure/">Transient Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/mitigating-infrastructure-state-drift/">Mitigating Infrastructure State Drift</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Hosting-Infrastructure"><a href="#Hosting-Infrastructure" class="headerlink" title="Hosting Infrastructure"></a>Hosting Infrastructure</h1><p>Hello and welcome to Hosting Infrastructure. In this lesson, I’m going to introduce you to the main compute options that are available for hosting your infrastructure. We’ll take a look at Infrastructure as a service options, platform as a service options, functions as a service options, and at some modern native app options.</p>
<p>Let’s start with the easy one, infrastructure as a service, because, really, there is only one main compute option available as infrastructure as a service – and that’s the virtual machine option.</p>
<p>Azure virtual machines are pretty self-explanatory. They allow you to deploy and manage virtual servers that are connected to an Azure Virtual Network. </p>
<p>Under the platform as a service umbrella, we have the azure app service, azure container instances, and azure cloud services.</p>
<p>The Azure App service is a managed offering. You’d typically use the azure app service to host web apps, mobile app back-ends, and automated business processes. You can also host RESTful APIs on the azure app service.</p>
<p>Azure Container Instances is another platform as a service offering that you can use to host your infrastructure. This service provides the fastest and easiest way to run containers in <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/az-400-exam-prep-microsoft-azure-devops-solutions-1-1368/">Azure</a>. It allows you to do so without needing to provision any virtual machines.</p>
<p>Azure Cloud services is yet another managed platform as a service offering. This service is designed for hosting and running cloud applications.</p>
<p>In the functions as a service bucket, we find azure functions and azure batch. </p>
<p>Azure Functions is a managed FaaS service that allows you to run small pieces of code, which are called functions, without the need to deal with application infrastructure. When you leverage Azure Functions, you get the benefit of all the up-to-date servers you need to run and scale your applications. Azure handles the maintenance and updating of the underlying servers.</p>
<p>Azure Batch is another functions-as-a-service offering. This service is typically used to run large-scale parallel and HPC applications. What Azure Batch does is create and manage a pool of underlying VMs (or compute nodes) and then installs the applications that you want to run. It then schedules jobs to run on the compute nodes. You don’t have to worry about clusters or installing job scheduler software. What you do, instead, is use Batch APIs, tools, command-line scripts, or even the Azure portal to configure, manage, and monitor your jobs.</p>
<p>Some modern native cloud app options that are available include microservices, azure service fabric, and the Azure Kubernetes service.</p>
<p>Microservices are used to provide massive scale and distribution, while Azure Service Fabric is a distributed systems platform that can run in Azure or on-prem. It’s essentially an orchestrator of microservices that are running across a cluster of machines. The azure service fabric allows you to more easily package, deploy, and manage scalable and reliable microservices and containers.</p>
<p>AKS, or Azure Kubernetes Service, is another modern native cloud app option. Leveraging AKS allows you to create, configure, and manage cluster of virtual machines that are preconfigured to run containerized applications.</p>
<p>Because there are so many ways to host your application code in Azure, choosing the right option can be challenging. Lucky for you, you have this handy-dandy flowchart that can tell you which compute service you should use to host your application. </p>
<p>As you can see, there are lots of factors to consider – especially when every application has its own unique requirements.</p>
<p>When choosing a hosting platform, be sure to perform a detailed evaluation of your needs. Be sure to consider things like necessary feature sets, service limits, and associated costs. You might also want to factor in SLAs and regional availability as well.</p>
<p>As a matter of fact, if the application that you need to host consists of multiple workloads, you may even want to evaluate each of those workloads separately. As a result, some hosting solutions may consist of more than one compute service.</p>
<h3 id="Lectures-1"><a href="#Lectures-1" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/an-intro-to-infrastructure-as-code/">An Intro to Infrastructure as Code</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/infrastructure-as-code-iac-technologies/">Infrastructure as Code (IaC) Technologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/technical-debt-templates/">Technical Debt &amp; Templates</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/transient-infrastructure/">Transient Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/mitigating-infrastructure-state-drift/">Mitigating Infrastructure State Drift</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/conclusion/">Conclusion</a></li>
</ul>
<h1 id="An-Intro-to-Infrastructure-as-Code"><a href="#An-Intro-to-Infrastructure-as-Code" class="headerlink" title="An Intro to Infrastructure as Code"></a>An Intro to Infrastructure as Code</h1><p>Hello and welcome to an intro to infrastructure as code. In this lesson we are going to take a look at what infrastructure as code means and what it offers.</p>
<p>So, what exactly is infrastructure as code? In a nutshell, infrastructure as code is the practice of deploying and managing infrastructure, such as networks, VM’s, and other resources in a descriptive model. This is done through the use of versioning, which is not unlike the versioning that DevOps teams use for source code.</p>
<p>In much the same way that a specific set of source code produces the same outcome within an application, whenever the source code is run in an infrastructure as code model, that code will produce the same environment every time it is applied. Because infrastructure as code mitigates environment drift in the release pipeline, it is a key DevOps practice that is typically used in conjunction with continuous delivery.</p>
<p>Infrastructure as code eliminates the need to manually maintain deployment environment settings, which, by the way, usually results in snowflake environments. Snowflake environments are individual environments that each require a unique configuration that is difficult or even impossible to be reproduced automatically. The inconsistencies that arise as a result of snowflake environments will often cause issues during deployments. These snowflake environments also create administration and maintenance headaches because each infrastructure requires manual processes that are not only difficult to track, but also result in errors. Infrastructure as code solves these problems.</p>
<p>A key principle of infrastructure as code is Idempotence, which refers to the concept that a specific deployment command will always set up the target environment in the same configuration, regardless of the starting state. In other words, Idempotence is the ability to run a specific process or command that produces the same exact result each time. You can achieve idempotency in a couple different ways. You can achieve it by automatically configuring an existing target or you can achieve it by discarding the existing target and creating a fresh environment to start from.</p>
<p>Idempotence is important when trying to create consistently configured environments. To achieve this, the DevOps team makes changes to the environment description and version of the configuration model. The configuration model typically exists in a JSON file. The configuration model is executed by the release pipeline, which in turn, configures the target environment. This means that if a change to the target environment is needed, the DevOps team actually makes the change to the source, which is then rerun to produce the changes on the target end.</p>
<p>DevOps teams will often use infrastructure as code early in the development cycle to produce environments that mirror production. These environments are then used to test applications.</p>
<p>Leveraging infrastructure as code rather than deploying environments manually allows teams to provision multiple test environments on an as-needed basis. Infrastructure as code also allows for rapid and scalable deployment delivery. Infrastructures that are deployed using infrastructure as code are easily repeatable and do not suffer from many of the runtime issues or configuration drift that manually deployed infrastructures suffer from.</p>
<p>The too long didn’t read version of infrastructure as code is pretty simple. It is used to deploy consistently configured infrastructure.</p>
<h3 id="Lectures-2"><a href="#Lectures-2" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/hosting-infrastructure/">Hosting Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/infrastructure-as-code-iac-technologies/">Infrastructure as Code (IaC) Technologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/technical-debt-templates/">Technical Debt &amp; Templates</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/transient-infrastructure/">Transient Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/mitigating-infrastructure-state-drift/">Mitigating Infrastructure State Drift</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Infrastructure-as-Code-IaC-Technologies"><a href="#Infrastructure-as-Code-IaC-Technologies" class="headerlink" title="Infrastructure as Code (IaC) Technologies"></a>Infrastructure as Code (IaC) Technologies</h1><p>Hello! Welcome to Infrastructure as Code technologies! Now that you have an idea of what infrastructure as code is, let’s take a look at some of the more common infrastructure as code tools and technologies.</p>
<p>Terraform is an IaC tool that allows you to provision infrastructure as code. What you do with Terraform is you describe your infrastructure as code. Execution plans are then created. These plans outline what will happen when your code runs. It actually builds a graph of your resources and automates changes. This results in far less manual intervention and fewer errors.</p>
<p>Terraform uses its own domain-specific language (DSL) called Hashicorp Configuration Language (HCL). HCL is JSON-compatible and is used to create these configuration files that describe the infrastructure resources to be deployed.</p>
<p>Azure Resource Manager is another IaC tool. Using Azure Resource Manager, you can define your infrastructure and necessary app dependencies in templates that can be re-used. You can also group interdependent resources so they can be deployed or deleted in a single action. Azure resource manager also allows you to manage access to resources through user permissioning.</p>
<p>Chef is one of the more popular tools in the IaC space. It’s used in the continuous integration and delivery processes by many organizations. </p>
<p>With Chef, what you do is create “recipes” and “cookbooks” using its Ruby-based language. Recipes and cookbooks are the actual terms that are used. They are used to specify the steps that are needed to achieve a desired configuration of applications on existing servers. Chef allows you to take a “procedural” approach to configuration management, because what you do, essentially, is describe a specific procedure that is necessary to produce a desired state.</p>
<p>Like Terraform, Chef is cloud-agnostic. This means you can use it with many different cloud service providers, including AWS, Azure, GCP, and others.</p>
<p>Puppet is another popular configuration management tool that is similar to Chef. It’s typically used, to continuously deliver software. Like Chef, puppet uses a Ruby-based domain specific language. With puppet, you define your infrastructures desired end state and what you want it to do. Puppet automatically enforces this desired end state and automatically fixes incorrect changes that have been identified.</p>
<p>Unlike Chef, which takes a procedural approach to configuration management, puppet takes a declarative approach. This means that you declare what your configuration should look like and puppet figures out how to make that happen. Another difference between chef and puppet is the target audience. While Chef is intended mainly for developers, puppet was designed with system administrators in mind.</p>
<p>Like Chef, Puppet integrates with many different cloud providers, including AWS, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>, Google Cloud, and even VMware.</p>
<p>Saltstack is an IaC tool that takes a different approach than tools like Chef and Puppet. Instead of taking the “infrastructure as code” approach, Saltstack takes an “infrastructure as data” approach.</p>
<p>This means that the declarative configuration patterns in Saltstack, which are written in Python, are language-agnostic. This makes them easier to read and understand.</p>
<p>One other thing I want to point out about Saltstack is the fact that it supports remote execution of commands. This differs from solutions like Chef and Puppet, because the configuration code for those solutions needs to be pulled from their servers.</p>
<p>Ansible is another player in the infrastructure as code space. It’s actually an infrastructure automation tool from Red Hat. This IaC tool is used to model your infrastructure. You do this by describing the relationships between your components and systems, rather than by managing each system independently.</p>
<p>Ansible code is written in YAML. When you use Ansible, you define Ansible Playbooks. These playbooks produce easily understood configurations that are easy to deploy.</p>
<p>I should also mention that Ansible doesn’t require any agents.</p>
<p>Another entrant in the IaC space is Docker. Using Docker, you can create containers that are used to package your code and dependencies together. By packaging things together in containers, you can run your applications in virtually any environment.</p>
<p>The configuration files used in Docker are called Dockerfiles, and they are created using YAML. Dockerfiles are essentially blueprints that are used to build your container images. Your container images include all the components that you need to run a piece of software. These components include things like code, runtime, system tools and libraries, and settings.</p>
<p>Organizations that leverage hybrid clouds and even multi-cloud environments will often turn to Docker because of the portability it offers for applications. This portability allows these organizations to run their apps in whatever environments they are leveraging.</p>
<p>Other IaC tools and technologies include offerings like JuJu, Vagrant, Pallet, (R)?ex, CFEngine, and a few others.</p>
<p>JuJu, for example, is from Canonical, which is the same company that brought us Ubuntu. This tool uses “charms” and “bundles”. Charms are really just sets of scripts that are used to deploy and operate software. Bundles are collections of charms that are linked together. You use these bundles to deploy entire app infrastructures in one fell swoop.</p>
<p>Vagrant is an another IaC tool. It comes from the same company that makes Terraform. While most other IaC tools focus on building large infrastructures, Vagrant focuses more on the creation of smaller development environments that run on a small number of virtual machines. You can run Vagrant on top of VMs from VirtualBox, VMware, Azure, and many other cloud providers.</p>
<p>Pallet is used to automate infrastructure in various environments. You can use it to automate infrastructure in the cloud, on server racks, and even on virtual machines. Pallet can stop and start nodes, it can configure nodes, and it can be used to deploy projects. It can even be used to run administrative tasks.</p>
<p>Written in Clojure, Pallet runs in a Java Virtual Machine. Although it works with things like AWS, OpenStack, and VirtualBox, it does NOT work with Azure or GCP.</p>
<p>(R)?ex is an open-source infrastructure automation tool. It has its own domain specific language, or DSL, that you use to describe your infrastructure configuration. (R)?ex is agentless and uses SSH to execute commands and manage remote hosts. </p>
<p>Released in 1993, CFEngine is possibly the oldest IaC tool available today. You use CFEngine and its DSL to define the desired state of your infrastructure. The CFEngine agents will then monitor your environment to ensure it remains in its desired state.</p>
<p>So, as you can see, there are quite a few tools available to you. Your requirements will dictate which tool or tools that you will need to leverage.</p>
<h3 id="Lectures-3"><a href="#Lectures-3" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/hosting-infrastructure/">Hosting Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/an-intro-to-infrastructure-as-code/">An Intro to Infrastructure as Code</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/infrastructure-as-code-iac-technologies/">Infrastructure as Code (IaC) Technologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/technical-debt-templates/">Technical Debt &amp; Templates</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/transient-infrastructure/">Transient Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/mitigating-infrastructure-state-drift/">Mitigating Infrastructure State Drift</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Technical-Debt-amp-Templates"><a href="#Technical-Debt-amp-Templates" class="headerlink" title="Technical Debt &amp; Templates"></a>Technical Debt &amp; Templates</h1><p>Hello and welcome to Technical Debt and Templates. In this lesson, we are going to look at what technical debt is and how to deal with it in templates.</p>
<p>Technical debt is a headache that is almost never-ending. It’s a concept in software development circles that refers to the cost of additional rework that often results from choosing to implement a quick and easy solution now, instead of implementing a more thought-out solution that would take longer to deploy. Technical debt is often incurred when developers are tasked with rushing a job to completion, rather than ensuring it is done right.</p>
<p>While technical debt doesn’t necessarily impact business functionality, it does make updating and troubleshooting more difficult. It undermines productivity because it makes code hard to understand, time-consuming to change, and difficult to validate. </p>
<p>Technical debt typically starts out small and grows over time as rushed changes and features are implemented in existing code in an undisciplined manner.</p>
<p>The term “technical debt” refers to any tasks or work that a team must complete in order to deploy production code and to keep it running. Some examples of technical debt include performance issues that are encountered, bugs in code that are discovered, operational issues in production that must be addressed. Other examples of technical debt include changes that are made on the fly and situations where you need to switch to technologies that were not accounted for in the development process.</p>
<p>While technical debt is often a result of manual actions that are taken during the development process, it can also turn up in Azure Resource Manager templates, which are used to deploy and configure <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> resources.</p>
<p>When you think about it, during the process of deploying resources via templates, you’ll often find yourself retrieving information about the different resource providers and types. This is all well and good, but whenever a resource provider enables new features, a new version of the associated REST API is also usually released. If you have components defined within your resource manager templates that rely on specific API versions, those components will usually need to be updated periodically. </p>
<p>To address this issue with templates, what you should do is validate the API versions in those templates to ensure they are current. This should be completed as part of your general development work.</p>
<p>Although you are not likely to ever completely eliminate technical debt, you can manage it effectively if you take the necessary steps to do so.</p>
<p>To properly manage technical debt, you need to understand it and be able to locate it within your code. You should also weigh the costs of remediation versus non-remediation because, while fixing technical debt incurs costs, not fixing it also incurs costs – and the costs of not addressing that technical debt could very well be larger.</p>
<p>Taking proactive steps to enforce policies that specifically focus on preventing debt from becoming worse makes it easier to manage that debt.</p>
<p>Steps that you can take, for example, include planning for and completing periodic updates to your Microsoft Azure Resource Manager templates to account for new API versions. This should be done as part of your general development work.</p>
<p>You should also restrict manual configurations. Ensuring that no manual configurations are allowed goes a long way toward limiting ad-hoc changes that often result in more technical debt.</p>
<p>And since technical debt is virtually unavoidable, you should be sure that you track it over time to ensure that it’s shrinking, rather than growing. When you do identify technical debt, it should be remediated as quickly as possible – and as AUTOMATICALLY as possible.</p>
<p>You might even consider setting the priority of technical debt so that it’s just as important as the development of new application features. This ensures that it is addressed in an ongoing fashion.</p>
<p>Third-party tools like SonarQube can be used to quantify technical debt so that it can be addressed.</p>
<p>To learn more about SonarQube, visit the URL that you see on your screen: <a target="_blank" rel="noopener" href="https://www.sonarqube.org/">https://www.sonarqube.org/</a></p>
<h3 id="Lectures-4"><a href="#Lectures-4" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/hosting-infrastructure/">Hosting Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/an-intro-to-infrastructure-as-code/">An Intro to Infrastructure as Code</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/infrastructure-as-code-iac-technologies/">Infrastructure as Code (IaC) Technologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/transient-infrastructure/">Transient Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/mitigating-infrastructure-state-drift/">Mitigating Infrastructure State Drift</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Transient-Infrastructure"><a href="#Transient-Infrastructure" class="headerlink" title="Transient Infrastructure"></a>Transient Infrastructure</h1><p>Hello. Welcome to Transient Infrastructure. In this lesson, we are going to talk a little bit about transient infrastructure and its role in DevOps.</p>
<p>In a typical hardware-based IT environment, the deployment of an infrastructure is highly manual and takes time to complete. Such an infrastructure typically remains in place for as long as the project that requires it remains active. Taking it a step further, in many cases, when a production infrastructure is deployed, a matching dev environment and maybe even a matching test environment are also deployed. This is obviously highly manual, time-consuming, and sometimes error prone. In such a scenario, every environment uses essentially the same resources.</p>
<p>By leveraging cloud computing resources, organizations can quickly deploy identically configured infrastructures and they can quickly tear them down when they are no longer needed. Such infrastructures are known as transient infrastructures. They are transient because they are built up and torn down in bursts, meaning they often have short lifecycles.</p>
<p>Now, while a production infrastructure isn’t really considered transient in most cases, the deployment of containers and the use of autoscaling, which spins up resources when they are needed and spins down resources when they are no longer needed, kind of makes the production infrastructure almost semi-transient.</p>
<p>So where does this fit in?</p>
<p>Well, when an organization is following the agile methodology, it should be leveraging automation that allows it to deploy the infrastructure that is needed at the time, to run any tests that are necessary, and then to automatically decommission the infrastructure when it’s no longer needed. This automation is where transient infrastructures really shine.</p>
<p>To achieve a truly transient environment, the environment build should be completely scripted. This can be achieved in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> through ARM templates. By incorporating versioning and self-service, transient environments can be spun up by virtually any team member. These types of transient environments should have automatic termination procedures built into the provisioning sequence. This ensures that such environments do not linger after they are no longer needed.</p>
<p>Leveraging transient environments or transient infrastructures ensures that such environments or infrastructures adhere to enforced standards. This reduces the chances of creating a bunch of snowflake infrastructures due to configuration drift. Transient infrastructures also result in more efficient use of resources and capacity, since these infrastructures only exist when they are needed and only include the resources that are needed. This obviously also results in cost savings.</p>
<p>There are two parts to a deployment of a transient environment. The first part is the policy that defines the configuration and lifecycle of the environment to be spun up. Project requirements will often dictate these aspects, which are included in the script that will deploy the environment. </p>
<p>The second part of the deployment of a transient environment is the technical implementation, which includes the creation of the script and the deployment of the environment from that script.</p>
<p>So, at the end of the day, the deployment of transient infrastructures is not only useful in speeding projects up, but it is also useful when trying to save on costs. In addition to speeding up projects and saving on costs, transient infrastructures also help ensure adherence to established standards.</p>
<h3 id="Lectures-5"><a href="#Lectures-5" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/hosting-infrastructure/">Hosting Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/an-intro-to-infrastructure-as-code/">An Intro to Infrastructure as Code</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/infrastructure-as-code-iac-technologies/">Infrastructure as Code (IaC) Technologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/technical-debt-templates/">Technical Debt &amp; Templates</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/mitigating-infrastructure-state-drift/">Mitigating Infrastructure State Drift</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Mitigating-Infrastructure-State-Drift"><a href="#Mitigating-Infrastructure-State-Drift" class="headerlink" title="Mitigating Infrastructure State Drift"></a>Mitigating Infrastructure State Drift</h1><p>Hello. Welcome to Mitigating Infrastructure state Drift. In this lesson, we’re going to discuss what infrastructure drift is and ways to mitigate it. </p>
<p>Self-service is a common offering in the DevOps world – and while it allows for easier deployments for those who need them, it also raises the risk of infrastructure state drift – especially when servers are used to fulfill multiple roles, rather than individual roles. An application server that also functions as a file server would be an example of this. In addition, because cloud computing has made deployment and configuration of infrastructure so easy and fast, it has also accelerated the number of servers that get deployed, while making it possible to completely automate the provisioning process.</p>
<p>Now, as a result of all this ease of use, DevOps teams will sometimes comingle infrastructure deployments with their roles as developers. This often results in environments being spun up in an ad-hoc fashion. When you are spinning infrastructure up in an ad-hoc fashion, it becomes very easy to introduce infrastructure state drift.</p>
<p>Now, before we go any further, let me explain what infrastructure state drift is and what, specifically, causes it.</p>
<p>Drift refers to changes that occur to an environment over time. These changes can, and often do, result in an infrastructure that exists in a non-standard state. This kind of configuration drift can cause all kinds of problems, including security issues. For example, left unchecked, state drift can result in certain users having more access or privileges than they should have. Other problems associated with drift include scenarios where software and hardware changes that have been introduced get forced into production, despite their unique support requirements.</p>
<p>Although the changes that produce infrastructure state drift are not intentionally disruptive, they often become disruptive nonetheless – and worse, such changes are often due to things that can be controlled. For example, poor communication between teams is often a common cause of infrastructure state drift. This is because the lack of communication forces developers or teams of developers to work in a vacuum. When this happens, each developer has his own vision of what configuration is needed – without knowing what the needs of other developers and team members are.</p>
<p>Since everyone is working on their own island, there is often a lack of available (and coherent) documentation. This lack of documentation results in some (or all) team members not understanding why certain changes were made. It also results in uncertainty surrounding ongoing support and testing.</p>
<p>Furthermore, tight (or unrealistic) deadlines will often force teams into a tough spot, where they need to make changes without running those changes though any sort of meaningful testing or approval process. </p>
<p>To mitigate these issues, and the resulting infrastructure state drift that comes along with them, mitigations processes and tools need to be implemented.</p>
<p>Mitigating infrastructure state drift should start with a configuration management database. Using a configuration management database ensures that all changes are logged, along with a justification for them. </p>
<p>In addition to the implementation of a configuration management database, team members should be coached to maintain communication with all other team members. This ensures everyone is on the same page.</p>
<p>Limiting manual configuration management is also helpful because it hinders the creation of snowflake systems and infrastructures that require special support and non-standard maintenance.</p>
<p>Auditing and policy enforcement are also critical to mitigating drift because they force compliance with standard configurations. </p>
<p>Another key to mitigating drift is to automate – because it eliminates the human component of the deployment process. This, of course, reduces the number of configuration mistakes that are made. By automating the deployment process and building it into pipelines, using standard templates, drift can be further mitigated. Whenever the template gets updated, deployments that are created from them are also updated at the same time. This, of course, results in standards adherence.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a> offers several tools and features that can help minimize infrastructure state drift.</p>
<p>For example, Azure Resource Manager helps ensure consistent deployments through the use of JSON templates. </p>
<p>Desired state configuration, or DSC, is another tool at your disposal. DSC provides an automated way to create baselines that deployments should adhere to.</p>
<p>Another Azure tool that can help mitigate drift is the Azure activity log. By leveraging its logs and query engine, you can search the logs for changes that roll out configurations that are outside the scope of the standard templates. </p>
<p>Practically speaking, drift can never be totally eliminated. However, it can be mitigated with the right combination of processes, tools, and communication.</p>
<h3 id="Lectures-6"><a href="#Lectures-6" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/hosting-infrastructure/">Hosting Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/an-intro-to-infrastructure-as-code/">An Intro to Infrastructure as Code</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/infrastructure-as-code-iac-technologies/">Infrastructure as Code (IaC) Technologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/technical-debt-templates/">Technical Debt &amp; Templates</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/transient-infrastructure/">Transient Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/conclusion/">Conclusion</a></li>
</ul>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Congratulations! You’ve come to the end of Designing an Infrastructure and Configuration Management Strategy. Let’s review what you’ve learned.</p>
<p>We started things off by looking at hosting infrastructure. We looked at Infrastructure as a service options, platform as a service options, functions as a service options, and at some modern native app options.</p>
<p>We then took an introductory look at Infrastructure as Code. You learned what infrastructure as code means and you learned about several tools and technologies that are used to deploy and manage infrastructure as code.</p>
<p>Later on, we touched on some of the more common infrastructure as code tools and technologies. You learned about things like Terraform, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Resource Manager, Chef, Puppet, and more.</p>
<p>After covering the different infrastructure as code tools and technologies, we got into technical debt and templates, where we looked at what technical debt is and how to deal with it in templates.</p>
<p>Coming down the home stretch, you learned about transient infrastructure and its role in the delivery lifecycle.</p>
<p>We wrapped things up by covering the mitigation of infrastructure state drift.</p>
<p>At this point, you should have the foundational knowledge that is required to effectively design an Infrastructure and Configuration Management Strategy.</p>
<p>To learn more about designing an Infrastructure and Configuration Management Strategy, you can, and should, read Microsoft’s published documentation. You should also keep an eye out for new courses on Cloud Academy, because we’re always publishing new ones. Be sure to give this course a rating, and if you have any questions or comments, please let us know. Thanks for watching and happy learning.</p>
<h3 id="Lectures-7"><a href="#Lectures-7" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/hosting-infrastructure/">Hosting Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/an-intro-to-infrastructure-as-code/">An Intro to Infrastructure as Code</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/infrastructure-as-code-iac-technologies/">Infrastructure as Code (IaC) Technologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/technical-debt-templates/">Technical Debt &amp; Templates</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/transient-infrastructure/">Transient Infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-an-infrastructure-and-configuration-management-strategy-1048/mitigating-infrastructure-state-drift/">Mitigating Infrastructure State Drift</a></li>
</ul>
<h1 id="5Technical-Debt-amp-Templates"><a href="#5Technical-Debt-amp-Templates" class="headerlink" title="5Technical Debt &amp; Templates"></a>5<strong>Technical Debt &amp; Templates</strong></h1><p><a target="_blank" rel="noopener" href="https://www.sonarqube.org/">SonarQube</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Deploying-Custom-App-Image-to-Container-Apps-using-Azure-Container-Registry-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Deploying-Custom-App-Image-to-Container-Apps-using-Azure-Container-Registry-10/" class="post-title-link" itemprop="url">AZ-400-Deploying-Custom-App-Image-to-Container-Apps-using-Azure-Container-Registry-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:42" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:42-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-23 21:28:10" itemprop="dateModified" datetime="2022-11-23T21:28:10-04:00">2022-11-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Deploying-Custom-App-Image-to-Container-Apps-using-Azure-Container-Registry-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Deploying-Custom-App-Image-to-Container-Apps-using-Azure-Container-Registry-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Building-Containers-with-Azure-DevOps-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Building-Containers-with-Azure-DevOps-9/" class="post-title-link" itemprop="url">AZ-400-Building-Containers-with-Azure-DevOps-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:40" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:40-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 10:56:02" itemprop="dateModified" datetime="2022-11-27T10:56:02-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Building-Containers-with-Azure-DevOps-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Building-Containers-with-Azure-DevOps-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to “Building Containers with Azure DevOps”. My name is Thomas Mitchell and I’ll be taking you through this course. </p>
<p>I’m an Azure Instructor at Cloud Academy and I have over 25 years of IT experience, several of those with cloud technologies. If you have any questions, feel free to connect with me on LinkedIn, or send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>This course is intended for DevOps professionals who wish to learn how to design and implement, through the use of containers, strategies for developing application code and infrastructure that allow for continuous integration, testing, delivery, monitoring, and feedback.</p>
<p>To get the most from this course, you should have a basic understanding of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> platform and of container concepts.</p>
<p>We’ll kick off by talking about ways to create deployable images. You’ll learn about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/the-power-of-docker-containers/">docker containers</a> and their role in development. You’ll also learn about microservices and where they fit in.</p>
<p>After discussing microservices, we’ll dive into the different Azure container-related services. We’ll talk about Azure Container Instances, the Azure Kubernetes Service, the Azure Container Registry, and Azure Service Fabric. We’ll also tough on Azure App Service.</p>
<p>Once we finish up with the different container-related services, we’ll look at a typical Dockerfile.</p>
<p>Later on, in this course, we’ll look at Docker multi-stage builds. You’ll learn what multi-stage builds are, and things to consider when working with multi-stage builds.</p>
<p>We’ll round things out with a hands-on demonstration that shows you how to create an Azure Container Registry,</p>
<p>By the time you complete this course, you’ll have a better understanding of containers and how they are used in Azure DevOps.</p>
<p>We’d love to get your feedback on this course, so please give it a rating when you’re finished. If you’re ready to learn how to build containers with Azure DevOps, let’s get started.</p>
<h1 id="The-Power-of-Docker-Containers"><a href="#The-Power-of-Docker-Containers" class="headerlink" title="The Power of Docker Containers"></a>The Power of Docker Containers</h1><p>Hello and welcome to “The Power of Docker Containers”. Let’s talk a little bit about what Docker containers bring to the table as far as the development process goes.</p>
<p>As a software containerization platform, Docker offers developers a common toolset and packaging model. It also provides a deployment mechanism for containerized apps. This results in simplified management, regardless of the host, as well as a seamless <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/introduction/">DevOps</a> experience.</p>
<p>Docker images can be created and deployed identically across virtually any environment in seconds. The Docker ecosystem is huge. This ecosystem includes hundreds of thousands of apps that are packaged in Docker containers. DockerHub, which is the Docker-maintained public containerized application registry, publishes almost 200,000 applications in the public community repository. </p>
<p>The notion that you can deploy a SQL Server Linux instance in seconds, using a Docker image, is a testament to the power of containers. It is this power that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/microservices-and-containers/">containerization</a> brings to the DevOps sphere.</p>
<h1 id="Microservices-and-Containers"><a href="#Microservices-and-Containers" class="headerlink" title="Microservices and Containers"></a>Microservices and Containers</h1><p>Hi there. Welcome to Microservices and Containers. Although containers have been most commonly used to simplify <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/introduction/">DevOps</a> through simplified developer-to-test-to-production flows, there are other uses for them. Microservices, for example, is a quickly-growing use-case for containers. </p>
<p>The term “Microservices” refers to an application development strategy where each part of an application is actually deployed as a completely self-contained component (or microservice). The microservices that comprise an application can then be individually scaled and updated. </p>
<p>The easiest way to explain the concept of microservices is to use an example scenario.</p>
<p>Let’s imagine for a second that your organization is the author of a large, monolithic tax application. As part of the development of the next revision of the software, your organization wants to migrate it to a collection of microservices. </p>
<p>Now, the current app might include a piece of code that does some specific tax calculation in certain circumstances – and this code may exist in several spots within the app. Whenever new tax laws are approved or changed, changes are needed in the calculation of taxes. This means that the same changes need to be made anywhere this tax calculation is performed within the app.</p>
<p>By moving to a collection of microservices, you could allow the application to create a notification that a tax calculation needs to be made in response to some scenario. Microservices involved in that calculation can be subscribed to those notifications – and then those individual microservices can do what they need to do to perform the tax calculations. You’d likely have one specific microservice that does the actual calculation.</p>
<p>Whenever a new tax law is enacted that affects tax calculations, you would only need to update the microservice(s) responsible for the calculations. You wouldn’t need to make changes throughout the app code base.</p>
<p>While you might operate a development or testing environment on a single server or by using a single instance of each microservice, production is typically a different story. In such an environment, you are likely going to want to be able to scale things out. You’ll probably want to scale out to multiple instances across a cluster of servers, rather than running things on one server. You’ll want the ability to scale in as well. You may also want different teams within your development department to be able to independently work on, and update, different microservices that each team is responsible for.</p>
<p>This is where microservices can really shine. By leveraging the benefits of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/the-power-of-docker-containers/">Docker containers</a> when working with more complex microservice-based applications, organizations can become more agile, because those microservices can be quickly scaled out and in to meet the loads on the application. While doing so, however, the isolation of resources and namespaces offered by containers ensures that one microservice instance does not interfere with any other instances. This makes it easier to design a solid microservice architecture, which, in turn, allows organizations to deal with the management, deployment, orchestration, and patching needs of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/container-related-services-in-azure/">container-based services</a> – while limiting the risks to availability.</p>
<h1 id="Container-Related-Services-in-Azure"><a href="#Container-Related-Services-in-Azure" class="headerlink" title="Container-Related Services in Azure"></a>Container-Related Services in Azure</h1><p>Hi there. Welcome to “Container-Related Services in Azure”. In this lecture, we’ll take a quick 30,000 foot view of each of the container-related services that are available in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>. We’ll look at Azure Container Instances, the Azure Kubernetes Service, the Azure Container Registry, Azure Service Fabric, and Azure App Service.</p>
<p>Let’s start with Azure Container Instances.</p>
<p>When you run workloads in Azure Container Instances, you can focus on app development and deployment instead of the deployment and management of the underlying infrastructure that’s necessary to run those apps.</p>
<p>While Azure Container Instances are easy to deploy, the main advantage of using them is the security that the hypervisor isolation that they provide for each container group. With this type of isolation available, you can be sure that your organization’s containers aren’t sharing their OS kernel with other containers.</p>
<p>You can read more about Azure Container Instances by visiting the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/container-instances/">https://azure.microsoft.com/en-us/services/container-instances/</a></p>
<p>The Azure Kubernetes Service started out as Azure Container Services, or ACS. It originally supported Docker Swarm and Mesos&#x2F;Mesosphere DC&#x2F;OS for orchestration management. However, when Kubernetes support was added, it became so popular that Microsoft eventually renamed the Azure Container Service to the Azure Kubernetes Service, or AKS.</p>
<p>At this point, Kubernetes is really the standard for container orchestration. Using the Azure Kubernetes Service, you can not only deploy and manage Kubernetes, but you can also scale and run your applications in a secure environment.</p>
<p>To learn more about the Azure Kubernetes Service, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/kubernetes-service/">https://azure.microsoft.com/en-us/services/kubernetes-service/</a></p>
<p>The Azure Container Registry is another container-centric service offering available in Azure. It allows you to store and manage your container images in a central registry, which is integrated with several other Azure services, including the App Service, Batch, and Service Fabric, among others. </p>
<p>Azure Container Registry supports many types of container deployments, including DC&#x2F;OS, Docker Swarm, and Kubernetes. Because of the broad support that Azure Container Registry offers, you can manage the configuration of your applications without being locked into the configuration of the target hosting environment. </p>
<p>To read more about the Azure Container Registry, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/container-registry/">https://azure.microsoft.com/en-us/services/container-registry/</a></p>
<p>Azure Service Fabric is a distributed systems platform that allows you to build and operate always-on, scalable, distributed apps. The service makes it easier to package, deploy, and manage scalable and reliable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/microservices-and-containers/">microservices and containers</a>. It can also host and orchestrate containers.</p>
<p>By leveraging Azure Service Fabric, you can avoid infrastructure problems and focus solely on the deployment of mission-critical workloads that are not only reliable, but also scalable. </p>
<p>For more details on the Azure Service Fabric, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/service-fabric/">https://azure.microsoft.com/en-us/services/service-fabric/</a></p>
<p>Azure Web Apps is an Azure offering that provides you with a managed service for both Windows-based and Linux-based web applications. This service allows you to deploy and run containerized apps for both platforms and it also offers auto-scaling and load balancing options. You can even integrate with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/introduction/">Azure DevOps</a>.</p>
<p>To read more about the Azure App Service, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/app-service/">https://azure.microsoft.com/en-us/services/app-service/</a></p>
<h1 id="Anatomy-of-a-Dockerfile"><a href="#Anatomy-of-a-Dockerfile" class="headerlink" title="Anatomy of a Dockerfile"></a>Anatomy of a Dockerfile</h1><p>Hi there. Welcome to “Anatomy of a Dockerfile”. In this lecture, we’re going to look at a basic Dockerfile, line by line.</p>
<p>Dockerfiles are used by docker build to assemble images. They are essentially text files that contain the commands that are necessary to build an image. On your screen is an example of a very basic Dockerfile. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu </span><br><span class="line">LABEL maintainer=&quot;tmitchell@cloudacademy.com&quot; </span><br><span class="line">ADD appsetup / </span><br><span class="line">RUN /bin/bash -c &#x27;source $HOME/.bashrc; echo $HOME&#x27; </span><br><span class="line">CMD [&quot;echo&quot;, &quot;Hi everybody!&quot;] </span><br></pre></td></tr></table></figure>

<p>Generally speaking, every image is based off another existing image. In this sample Dockerfile here, line one refers to the parent image that this new image will be based on. The Ubuntu image referred to in line one would be retrieved from either a local cache or from DockerHub.</p>
<p>I should point out here that an image that doesn’t have a parent is called a base image. If that were the case here, what we could do with this Dockerfile is completely omit the FROM line altogether. We could also replace it with FROM scratch, instead. However, since we’re using the ubuntu base image, we are referencing it in the file.</p>
<p>The second line in this sample Dockerfile uses the LABEL command to set the email address of the person who maintains the file.</p>
<p>The third line in our example uses the ADD command to add a file, called “appsetup” into the root folder of the image being created.</p>
<p>The fourth line is a RUN command that runs when the image is being created by docker build. This part of the Dockerfile is typically used to configure things within the image. </p>
<p>The last line in our sample file calls a command that we want to execute once the new container is created from the image.</p>
<p>Of course, every Dockerfile will be different – and they can be as simple or as complex as they need to be. Simpler, however, is always better than complex, when possible.</p>
<p>For more information, visit the Dockerfile reference URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a></p>
<h1 id="Multi-Stage-Builds"><a href="#Multi-Stage-Builds" class="headerlink" title="Multi-Stage Builds"></a>Multi-Stage Builds</h1><p>Hi there. Welcome to “Multi-Stage Builds”. In this lecture, we’re going to take a look at what multi-stage builds are and what they bring to the table.</p>
<p>So, Multi-stage builds are a new feature that makes life easier when working with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/anatomy-of-a-dockerfile/">Dockerfiles</a>. They are extremely helpful when trying to optimize Dockerfiles while ensuring that they are still easy to read.</p>
<p>Prior to the introduction of multi-stage builds, what you would typically have is one Dockerfile to use for development. This Dockerfile would contain everything that you need to build the application that you wished to deploy. You’d also have a second, slimmed-down Dockerfile that you would use for production. This second file would contain just the application and only the resources needed to run it. This “builder pattern” of maintaining two Dockerfiles, obviously, isn’t ideal.</p>
<p>The code that you see on your screen is a good example of what the typical “builder pattern” consists of. Notice we have 3 different files. We have DockerFile.build, the Dockerfile, and build.sh.</p>
<p>Dockerfile.build:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3</span><br><span class="line">WORKDIR /go/src/github.com/alexellis/href-counter/</span><br><span class="line">RUN go get -d -v golang.org/x/net/html </span><br><span class="line">COPY app.go .</span><br><span class="line">RUN go get -d -v golang.org/x/net/html \</span><br><span class="line"> &amp;&amp; CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br><span class="line">Dockerfile:</span><br><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY app .</span><br><span class="line">CMD [&quot;./app&quot;] </span><br><span class="line">build.sh:</span><br><span class="line">#!/bin/sh</span><br><span class="line">echo Building alexellis2/href-counter:build</span><br><span class="line">docker build --build-arg https_proxy=$https_proxy --build-arg http_proxy=$http_proxy \ </span><br><span class="line">  -t alexellis2/href-counter:build . -f Dockerfile.build</span><br><span class="line"> </span><br><span class="line">docker create --name extract alexellis2/href-counter:build </span><br><span class="line">docker cp extract:/go/src/github.com/alexellis/href-counter/app ./app </span><br><span class="line">docker rm -f extract</span><br><span class="line">echo Building alexellis2/href-counter:latest</span><br><span class="line">docker build --no-cache -t alexellis2/href-counter:latest .</span><br><span class="line">rm ./app</span><br></pre></td></tr></table></figure>

<p>If you look at this example, you’ll see that it artificially compresses two different RUN commands together. This is done to avoid creating an additional layer in the image. As is the case with any sort of coding, the more code you have, the more error-prone it becomes – and the more difficult it becomes to maintain.</p>
<p>When the build.sh script is run, it first has to build the first image. Then, it needs to create a container from it, so that it can copy the artifact out, before building the second image. In this scenario, you are left with 2 images – both of which take up room on your system. You are also left with the app artifact on your local disk as well.</p>
<p>Enter Multi-stage builds, which greatly simplify things.</p>
<p>Prior to multi-stage builds, keeping image sizes down was a challenge. This is because every instruction included within a Dockerfile adds a layer to the image. Not only that, but you also need to remember to clean up unneeded artifacts before moving on to the next layer. </p>
<p>Until multi-stage builds became available, authoring an efficient Dockerfile meant using shell tricks and other logic to keep the layers as small as possible. You also had to use the same tricks to ensure that each layer has only the artifacts that it needs from the previous layer and nothing else.</p>
<p>On the screen is an example of a multi-stage file.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3</span><br><span class="line">WORKDIR /go/src/github.com/alexellis/href-counter/</span><br><span class="line">RUN go get -d -v golang.org/x/net/html </span><br><span class="line">COPY app.go .</span><br><span class="line">RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=0 /go/src/github.com/alexellis/href-counter/app .</span><br><span class="line">CMD [&quot;./app&quot;] </span><br></pre></td></tr></table></figure>

<p>When you leverage multi-stage builds, you can use multiple FROM statements within the Dockerfile. Each of the FROM statements begins a new stage. The stages, themselves, are numbered in order, starting with stage 0. What you would typically do, though, to make the Dockerfile easier to maintain, is use the AS clause to name, or alias, each stage.</p>
<p>Notice the aliasing that’s been added to the file on your screen:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3 AS builder</span><br><span class="line">WORKDIR /go/src/github.com/alexellis/href-counter/</span><br><span class="line">RUN go get -d -v golang.org/x/net/html </span><br><span class="line">COPY app.go  .</span><br><span class="line">RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=builder /go/src/github.com/alexellis/href-counter/app .</span><br><span class="line">CMD [&quot;./app&quot;] </span><br></pre></td></tr></table></figure>

<p>This example on your screen names the stage and uses the name in the COPY instruction. By referencing the name in the copy instruction, even if the instructions in this Dockerfile are re-ordered later on for some reason, the COPY won’t break.</p>
<p>I should also mention that when you build an image, you don’t have to build the entire Dockerfile, including every stage. Instead, you can specify a single target build stage. The command that you see on your screen, when using our example Dockerfile, stops at the stage named builder:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker build --target builder -t alexellis2/href-counter:latest</span><br></pre></td></tr></table></figure>

<p>The –target option, in this command, tells docker build to create an image up to the target of builder, which is a named stage in our example file. </p>
<p>In the next lesson, we’ll take a look at some best practices that you can follow when working with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/best-practices-for-multi-stage-builds/">multi-stage builds</a>.</p>
<h1 id="Best-Practices-for-Multi-Stage-Builds"><a href="#Best-Practices-for-Multi-Stage-Builds" class="headerlink" title="Best Practices for Multi-Stage Builds"></a>Best Practices for Multi-Stage Builds</h1><p>Hi there. Welcome to Best Practices for <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/multi-stage-builds/">Multi-Stage Builds</a>. In this lecture, we’re going to review some of the best practices that you should be following when working with multi-stage builds. We’re going to talk about adopting container modularity, avoiding unnecessary packages, choosing an appropriate base, and avoiding the inclusion of application data.</p>
<p>When working with builds, you really want to avoid overly complex container images that couple together several applications. Instead, what you should be doing is using multiple containers, with each one intended for a single purpose. For example, you might want to put a website in one container but relegate the database for the website to another container.</p>
<p>While there are always going to be exceptions to this rule, splitting up the components of an application into separate containers makes it more likely that you will be able to minimize work effort by being able to reuse containers. Adopting container modularity will also often make it easier to scale an application. Using our website example from earlier, container modularity would allow you to add replicas of the website container while leaving the database container alone. </p>
<p>An easy way to minimize image sizes is to avoid including unnecessary packages in your images. For example, instead of including packages that you think you MIGHT need, leave them out until you are sure you need them. Once you are sure you need them, you can include them.</p>
<p>Choosing an appropriate base image, or parent image, allows you to optimize the contents of your Dockerfile. By starting with an image that only contains the packages that you need, you can keep your Dockerfiles in check. </p>
<p>Although it’s possible to store your application data right inside the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/demo-create-an-azure-container-registry/">container</a>, doing so increases the size of your image. This runs counter to the idea of optimizing things. Instead of storing app data in your containers, you should consider using docker volume support. By doing so, you can maintain isolation of the application itself, and its data. </p>
<p>For more Dockerfile best practices, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/">https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/</a></p>
<h1 id="Demo-Create-an-Azure-Container-Registry"><a href="#Demo-Create-an-Azure-Container-Registry" class="headerlink" title="Demo: Create an Azure Container Registry"></a>Demo: Create an Azure Container Registry</h1><p>Hi there and welcome back. In this demonstration, we’re going to do a couple of different things. First, we’re going to deploy a Container Registry in Microsoft Azure using the Azure portal. Once we’ve got that deployed, we’re going to log in to that registry, using the Azure CLI from our local workstation. Once we’ve logged into our registry, what we’re going to do is use the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/demo-add-docker-support-to-an-existing-application/">Docker</a> terminal which is also installed on our local workstation. And we’re going to first, pull down a basic hello world image from the Docker Hub.</p>
<p>Once we have the Hello World image pulled down from Docker Hub, we’ll take that image and we’ll push it to our Container Registry in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>. Once we have it pushed to our Container Registry in Azure, we’ll go back to the Azure portal, and we’ll take a look and confirm that our hello world image has in fact been pushed into our Container Registry. So let’s get started here.</p>
<p>On the screen, you can see I’m logged into my Azure Portal here, I’m at my homepage. I am logged in as the admin here. To deploy a Container Registry, we’re simply going to create a resource. And we’ll search the marketplace for Container Registry. And we can see Container Registry here and we’ll create it. We need to give our registry a unique name. And we’ve called it my9878 and it appends, what Azure will do here is append this azurecr.io domain name to your registry name.</p>
<p>So the registry name needs to be unique across Azure. We’ll deploy into our lab subscription and into my resource group. We can leave the rest of this stuff here at its default. We’ll go ahead and click Create. The deployment here shouldn’t take long, usually a couple minutes at most. We can see it’s been deployed. So we’ll go to our resource here. And if we go into repositories within our registry, we can see we have no repositories here.</p>
<h1 id="Demo-Add-Docker-Support-to-an-Existing-Application"><a href="#Demo-Add-Docker-Support-to-an-Existing-Application" class="headerlink" title="Demo: Add Docker Support to an Existing Application"></a>Demo: Add Docker Support to an Existing Application</h1><p>So now that our my9878 <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/demo-create-an-azure-container-registry/">container registry</a> has been deployed. Let’s log into it using the Azure CLI from my workstation. So let me bounce down here. I actually have my PowerShell running and my Docker terminal here. So let’s open up PowerShell here. And from PowerShell what I’m going to do is run the az acr login command. And when I do this, I need to specify the name of my repository.</p>
<p>Now oddly enough, I don’t need to nor should I include the azurecr.io domain name when I specify the name for my repository here. So we bounced back out to overview. See the full login server here, my9878.azurecr.io Instead, I just use the name that I gave my repository. So we’ll go ahead hit enter here and we can see that our login has succeeded. Now what we’re going to do into this exercise is push the basic hello world image up to my container registry. That being said, I have to obtain that hello world image first and I’m going to obtain that from Docker Hub.</p>
<p>Now to do that, I’m going to switch over to my Docker terminal here. Now from Docker terminal, what I’m going to do is run a Docker pull hello world command and what this will do is pull the latest hello world image from Docker.io So it’s coming from Docker Hub. Now before we push this image into our container registry. We need to first tag it with the fully qualified domain name of our ACR login server.</p>
<p>If we bounce out to our Azure portal, we can see the FQDN for our login server is my9878.azurecr.io So that’s what we’re going to use here. So let’s bounce back into our terminal here. And we’re going to use the Docker tag command to perform this tagging. Along with the tag command, we need to specify the image. After specifying the name of the image we want to tag, we need to specify the ACR login server. And with that we need to specify the name of our image and the versioning for it.</p>
<p>So we’ll go ahead and tag it. And then what we’ll do now is perform the push using the Docker push command. And essentially we’re going to specify the name we just called in the Docker tag command. And we can see it prepares and then pushes and tells us the image has been pushed. To confirm that our image has been pushed, we can go into our portal and then take a look at our repositories. And we can see hello world is now listed as a repository.</p>
<p>At this point, we can now try to run the image from our container registry. And to do that we’ll bounce back down into our Docker terminal. And from here we’ll use the Docker run command. So we’ll go Docker run and again, we’ll specify the image from our registry. And if we look closely here we can see we get a message from Docker telling me that our installation appears to be working correctly.</p>
<p>So with that, we’ve deployed a container registry in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a> using the portal. We used Azure CLI within Azure PowerShell from our local workstation to log in to our registry. And then we used the Docker terminal to pull down an image from Docker Hub. We tagged it and then we pushed that image up into our own registry. Once we confirmed that the push was successful, we were also able to successfully run the image from our registry. So with that let’s call it a wrap.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Congratulations! You’ve come to the end of “<a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/introduction/">Building Containers with Azure DevOps</a>“. Let’s review what you’ve learned!</p>
<p>We kicked things off by talking about ways to create deployable images. You learned about docker containers and their role in development. You also learned about microservices and where they fit in.</p>
<p>After discussing microservices, we looked at the different Azure container-related services. We talked about Azure Container Instances, the Azure Kubernetes Service, the Azure Container Registry, and Azure Service Fabric. We also touched on Azure App Service.</p>
<p>After finishing up with the different container-related services, you learned what a typical Dockerfile looks like.</p>
<p>Later on, you learned about Docker multi-stage builds. You learned what multi-stage builds are, and things to consider when working with multi-stage builds.</p>
<p>We wrapped up with a hands-on demonstration that showed you how to create an Azure Container Registry.</p>
<p>At this point, you should have a better understanding of containers and how they are used in Azure DevOps.</p>
<p>I should point out, before you go, that, in addition to completing courses like this one, you should always keep up with the latest features and services by reading Microsoft’s published documentation as well.</p>
<p>As always, thanks for watching, and happy learning!</p>
<h1 id="4Container-Related-Services-in-Azure"><a href="#4Container-Related-Services-in-Azure" class="headerlink" title="4Container-Related Services in Azure"></a>4<strong>Container-Related Services in Azure</strong></h1><p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/container-instances/">Azure Container Instances</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/kubernetes-service/">Azure Kubernetes Service</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/container-registry/">Azure Container Registry</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/service-fabric/">Azure Service Fabric</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/app-service/">Azure App Service</a></p>
<h1 id="5Anatomy-of-a-Dockerfile"><a href="#5Anatomy-of-a-Dockerfile" class="headerlink" title="5Anatomy of a Dockerfile"></a>5<strong>Anatomy of a Dockerfile</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">Dockerfile reference</a></p>
<h1 id="7Best-Practices-for-Multi-Stage-Builds"><a href="#7Best-Practices-for-Multi-Stage-Builds" class="headerlink" title="7Best Practices for Multi-Stage Builds"></a>7<strong>Best Practices for Multi-Stage Builds</strong></h1><p><a target="_blank" rel="noopener" href="https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/">Dockerfile best practices</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Managing-Application-Configuration-and-Secrets-in-Azure-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Managing-Application-Configuration-and-Secrets-in-Azure-8/" class="post-title-link" itemprop="url">AZ-400-Managing-Application-Configuration-and-Secrets-in-Azure-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:39" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:39-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 10:52:18" itemprop="dateModified" datetime="2022-11-27T10:52:18-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Managing-Application-Configuration-and-Secrets-in-Azure-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Managing-Application-Configuration-and-Secrets-in-Azure-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Hi, and welcome to this Managing Application Configuration and Secrets in Azure course. This is very much a hands-on course with lots of demonstrations that require some preexisting knowledge of software development and use of the Azure platform, so directed mainly at intermediate level developers and DevOps engineers. Having said that, project managers may find it insightful as to what is involved in implementing secure configuration and a compliant development pipeline.</p>
<p>Firstly, we’ll take a practical look at app configuration and the Azure App Configuration service in terms of running and deploying apps, followed by using Azure Key Vault in different contexts. Secondly, I’ll delve into the elements needed to build what’s termed a compliant pipeline. Basically, this is automating security and vulnerability testing within your development pipeline. While this is an Azure DevOps course and the demos will be using Visual Studio, I’ll also be talking about Azure Kubernetes Service and OWASP, the Open Web Application Security Project, so it will be helpful if you have some basic understanding of what these are.</p>
<p>My name is Hallam Webber and I’ll be your instructor for this course. We welcome all comments and feedback, so please feel free to reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> with any questions or comments. Let’s get started.</p>
<h1 id="Azure-App-Configuration-Overview"><a href="#Azure-App-Configuration-Overview" class="headerlink" title="Azure App Configuration Overview"></a>Azure App Configuration Overview</h1><p>As developers or anyone involved with deploying applications, we’re all aware of configuration settings. Settings allow apps to change their behavior without changing the application code. The old standby example is database connection strings. You can easily change which server and database the application connects to just by editing the config file. Once upon a time, in the Windows world, you’d put the connection string in an ini file. That’s ini for initialization.</p>
<p>Then, we started using XML config files, and more recently, JSON app setting files. The same config file principle applies no matter what operating system or programming language you work with. And for the most part, this has worked well in the single app on-premise environment. However, looking just at the connection string example, there are a couple of aspects that aren’t ideal.</p>
<p>Firstly, the password is there in plain text. This will give anyone with access to the config file access to the database with the application login, So that’s access with all the privilege of the application and consequently, no way of tracking that any changes made were not made by the app or that sensitive information has been unlawfully taken. Secondly, and not immediately obvious, is that cloud-based applications often run as multiple instances, widely distributed on virtual machines or containers, possibly spread over multiple regions.</p>
<p>Supporting this scenario with a traditional configuration setup would require multiple config files. This would fly in the face of the one version of the truth best practice that is make maintenance a headache and prone to errors. These issues, along with a myriad of other challenges posed by cloud deployment, led Azure to introduce the App Configuration Service. The App Configuration Service provides centralized and secure configuration management that offers many more features than just storing connection strings. At the moment, Azure App Configuration has good SDK support for .Net Core and Java’s Spring framework.</p>
<h1 id="DEMO-Basic-App-Configuration"><a href="#DEMO-Basic-App-Configuration" class="headerlink" title="DEMO: Basic App Configuration"></a>DEMO: Basic App Configuration</h1><p>Having said that let’s start by looking at how you would implement basic app settings with the Azure App Configuration Service. I have here a basic ASP.Net Core 3.1 web app that connects to an Azure SQL database and retrieves products. There are 2 versions of the database, a development one and a production one. You can see the connection string here in appsettings.json. I’ll run the app to see the two datasets. Here we have the development data with dummy products.</p>
<p>Now I’ll go back to appSettings and change the connection to the production database. Okay, now we’ve seen the 2 datasets, let’s look at moving the connection string to App Configuration. Log in to the Azure portal and create a new App Configuration resource. Give it a name, select a resource group, location, and choose a pricing tier. After the app configuration has been deployed click on configuration explorer under operations and create a new key-value. I’ll call the key HowAppDb and paste the database connection from appSettings into the value field. I’m gonna leave the Label field blank for the moment, but enter the connection string into the content-type field. Content-type has no intrinsic meaning but is a user-defined field that can indicate what type of data the value is.</p>
<p>Before we head back to the app, we’ll have a quick look at Access keys under settings. The connection string value is what we’ll use to connect to the App Configuration service. Okay, back at the app we need to add a NuGet package to use the App Configuration functionality. Search for Microsoft.Azure.AppConfiguration.AspNetCore and add it to the project. Next, open the project file by right-clicking on it and choosing “Edit project file” to see the .csproj XML file. Right-click again on the project in solution explorer and select manage user secrets. An empty secrets.json file is opened and a UserSecretsId XML tag is added to the property group node of the project file. The idea with the secrets file is that it is not part of the project and is not automatically stored in the project’s repo when you commit your changes. It would kind of defeat the whole purpose of secrets if they were easily accessible. The secrets id GUID points to a file location within the user profile folder. This can be seen by opening the file’s location.</p>
<p>Now we have to add a link to the Azure App Configuration service. In secrets.json add a connectionStrings:AppConfig key and go back to the Azure portal and get the connection string from Access keys under Settings and paste it in as the value. I’ll remove the database connection string from appSettings as this is no longer necessary. Next, I’ll need to modify the application code to use app settings from Azure. First, I need to modify CreateHostBuilder in program.cs by referencing Azure App Configuration using the connection string stored in secrets.json under the key ConnectionStrings:AppConfig.</p>
<p>Now that we are getting the app settings from Azure I need to change the database connection initialization in startup.cs. This just involves deleting the GetConnectionString method off configuration and replacing it with an index reference to the database connection string key from Azure App Configuration. Right, now we’ve made the changes, let’s run the app again. There, we’ve got the development data showing, and I’ll go back to Azure and change the connection to ProdDb and there we go now connecting to production data.</p>
<h1 id="Basic-Configuration-Recap"><a href="#Basic-Configuration-Recap" class="headerlink" title="Basic Configuration Recap"></a>Basic Configuration Recap</h1><p>I just wanna recap what’s gone on here. We simply moved the database connection string from appsSettings to Azure App Configuration by way of user secrets. User secrets aren’t part of App Configuration. They are merely a way to secure sensitive information, like passwords and API keys, in the development environment by preventing them from being checked in with the source code. I could just have easily used the App Configuration server end point URL to reference the configuration settings.</p>
<p>One other thing to bear in mind is order in which .Net Core processes configurations. AppSettings.json is read first, followed by any environment-specific app settings, then a local user secrets file if it exists, then environment variables, and finally, command line arguments. This is how the secrets.json file was automatic-magically read. Just remember, whatever is read last overwrites earlier settings.</p>
<h1 id="DEMO-Using-Labels-for-Environments"><a href="#DEMO-Using-Labels-for-Environments" class="headerlink" title="DEMO: Using Labels for Environments"></a>DEMO: Using Labels for Environments</h1><p>When I initially set up the database connection string, I left the label blank. Now, I’m going to show you how you can use labels to differentiate between environments. Let’s go back to the portal and delete the database connection string and remake it with a label of Release. Having done that, I can now make what is essentially a copy of the setting by right-clicking on the ellipsis button on the far right, and selecting Add value. I’ll paste in the DevDb connection string, and set the label to Development.</p>
<p>Going back to Visual Studio, I’ll create another profile called HowAppConfigProd and change the ASPNETCORE ENVIRONMENT variable to Release. Next, I have to adjust the CreateHostBuilder and program.cs by adding a select option, specifying any key and using the environment name as the label filter. We’ll need to add Microsoft.Extensions.Configuration .AzureAppConfiguration to the using statements. You’ll also notice that I’m now using the connect method to retrieve the configuration settings. Because user secrets don’t really work outside the development environment, I’m going to have to create a release environment-specific appSettings JSON file, and put the Azure App Configuration connection string in it. If I run the Prod profile, the ProdDb connection string is retrieved by its label and we connect to the correct database.</p>
<h1 id="DEMO-Configuring-Feature-Flags"><a href="#DEMO-Configuring-Feature-Flags" class="headerlink" title="DEMO: Configuring Feature Flags"></a>DEMO: Configuring Feature Flags</h1><p>Feature flags are an important tool in the DevOps kit bag. They allow code to be released where not all functionality has been fully finished. App Configuration allows you to set up and manage feature flags. I’m gonna set up a feature flag to turn on and off access to a new order feature.</p>
<p>First, lets set up the flag in the portal by going into Feature manager under Operations in App Configuration and clicking Add. I’ll give it a name of orders feature, no label and a description. By default the new feature flag is turned off. If I go back into Configuration explorer I can see the flag has been added as a setting. Clicking the ellipses and selecting view key-value data displays the raw flag data including any dynamic conditions.</p>
<p>Back in Visual Studio I need to add the FeatureManagement. AspNetCore NuGet package and make some adjustments to program.cs. First off I need to add UseFeatureFlags to AddAzureAppConfiguration options. Because the feature flag has no label I need to add another filter condition to also include settings where the label is null. In startup I need to add feature management to services.</p>
<p>Next, I’ll create an AppFeature enumerated type with an OrdersFeature flag and add the FeatureGate decorator with the OrdersFeature flag to the index method of the orders controller. This will also involve adding FeatureManagement to the using statements. In view imports, I’ll add a FeatureManagement tag helper. Having done this I can now add the orders menu item to the navbar wrapped in a feature tag.</p>
<p>Okay, let’s take a breath and run it. As expected, no order menu item is visible and if I type the orders path directly into the address bar, still no joy. I’ll go back to App Configuration and enable the orders feature flag and re-run the app. This is necessary to reload the settings on app start up. There we have the order menu item and if I click on it we go through to the orders page. Also I can now go directly to the orders page with the URL path.</p>
<h1 id="DEMO-Realtime-Settings"><a href="#DEMO-Realtime-Settings" class="headerlink" title="DEMO: Realtime Settings"></a>DEMO: Realtime Settings</h1><p>Having to restart the app to read updated configuration settings isn’t ideal. There is a solution to this problem and that comes in the form of a special settings key called a sentinel key. The sentinel key is monitored by the app for changes in its configuration. Let’s add the key to our settings with the name Sentinel an the value one with no label. In program.cs, I need to register the sentinel key to be monitored, and for the purpose of demonstration, I’ll set the cache expiration to 10 seconds. This is saying when the sentinel key changes, refresh all the app settings.</p>
<p>By the way, there is nothing special about the word sentinel. You could call it anything you like, like settings have changed, but from what I’ve seen, calling it sentinel has become a bit of a convention. I’ll add UseAppConfiguration to the configure method of startup.cs.</p>
<p>Next, I’ll just add the current time to the home index page, so we can monitor what is going on. Right, I’ll run the app and go back to the Azure portal and turn the orders features on and change the sentinel value. With that enabled, I’ll just keep hitting refresh, and in a few seconds, we’ll see the change. And there we go. The orders feature is now showing without having to restart the application.</p>
<h1 id="DEMO-CLI-Importing-amp-Namespaces"><a href="#DEMO-CLI-Importing-amp-Namespaces" class="headerlink" title="DEMO: CLI, Importing, &amp; Namespaces"></a>DEMO: CLI, Importing, &amp; Namespaces</h1><p>Up until now, I’ve been treating app configuration as you would an appsettings.json file for a single application, but this isn’t how App Configuration is designed to be used. You can have multiple applications using the one App Configuration by implementing something like namespaces. What I’m going to do is first is show you creating an App Configuration using the command line interface and then creating a key with namespaces. Let’s open a CLI command prompt. The command for creating an app configuration is <code>az appconfig create -n</code> for name and then the name of your app configuration followed by <code>-g</code> for the resource group, then <code>-l</code> for the location and then we have to specify the SKU which is either standard or free. Now that’s done I’m going to create a key.</p>
<p>So once again <code>az appconfig</code>, now <code>kv</code> for key value, set -n app configuration. I’m guessing that’s for set a new kv in this configuration, not entirely intuitive. Now the name of the key with the namespaces separated by semicolons. So app name then settings then key, with the value of 0. If we go back to the portal and into the newly created App Configuration we can see the key. Instead of manually entering the rest of the settings I’m going to import them with the import&#x2F;export feature.</p>
<p>Before I do this let’s have a look at the settings file. This is obviously not the same as the appSetting.json that gets produced when you create an app with the Visual Studio Wizard, but it’s in a format that will conform to the namespaces I want in the app configuration. Importing is straightforward, It’s a JSON configuration file for a .Net app. Before the data is saved to the configuration you have the opportunity to change the namespace separator and prefix a namespace if you want.</p>
<p>Because I’m using managed identity to connect to the App Configuration I’ve given the app service that will be reading these app settings the App Configuration Data Reader role on this resource. Right, back to the app to see what changes we need to make. First off we need managed identity credentials and now I’m connecting with just using the endpoint url. Using managed identity credentials means you won’t be able to run the app locally, it needs to be run from the app service to work. The refresh key is the new fully qualified namespace and I’m using the demoApp scope just to retrieve settings for this app.</p>
<h1 id="DEMO-Using-App-Configuration-with-Azure-DevOps-Pipelines"><a href="#DEMO-Using-App-Configuration-with-Azure-DevOps-Pipelines" class="headerlink" title="DEMO: Using App Configuration with Azure DevOps Pipelines"></a>DEMO: Using App Configuration with Azure DevOps Pipelines</h1><p>So far we’ve talked about App Configuration in terms of released products, but you can use it elsewhere, like within your DevOps pipeline. So if you’re using your App Configuration as a central settings repository, but you need to incorporate it into some kind of static settings, because you have an application that may not be connected, or maybe it needs settings before connections are fully established, you can export your settings from App Configuration. Here, I’m just going to show you how you can use a pipeline within Azure DevOps to access your app settings from App Configuration and export it into a file.</p>
<p>So we’ll just make up a pipeline here and just for the sake of demonstration, I’m going to connect to my app settings with an inline script, and we’re going to use the AZ app config key-value export command in our PowerShell script. And then I’m going to publish that artifact so we can have a look at it. I’ll just add the publish build artifacts task. Having done that, let’s just save and run the script. I don’t want to commit this to the master, so I’ll just create a new branch and save and run.</p>
<p>So now we go and have a look at the job. Here we can see it running. Obviously I didn’t need to check out my code for this demonstration. Logging on using the Azure CLI does take a little while. Okay, the job’s now finished. So let’s go back to the pipeline summary and we’ll have a look at what’s being produced here in terms of exporting the settings.</p>
<p>If I download this file, have a look at it inside Visual Studio, we can see that is exactly what we uploaded to the App Configuration. Not only can you use Azure CLI, but there is also an App Configuration extension that you can incorporate into your Azure DevOps. I’m just going to go to the marketplace and we’ll search for App Configuration. Okay, there it is, and it’s free. So I’ll just install that.</p>
<p>Now that’s done, I’ll go back to Azure DevOps. Now, what I’m going to do here is I’m just going to change the pipeline and show you how to use the App Configuration extension to access my settings. So I’ll search for the App Configuration task and I’m going to set up my connection to my subscription. I’ll select my instance from the dropdown list, and I’m also going to specify the key filter for just getting the settings of my demo app.</p>
<p>Now that we’ve got the connection, I’m going to grab the settings. I’m just going to use a PowerShell script task to print out one of the settings and display it in the command line. Just use a simple echo command here. So it’s an inline script and I’m just going to echo out the background color. As you can see, the App Configuration extension makes settings available as pipeline variables. Now add the PowerShell task and let’s save and run. Right, now that’s finished let’s go to our PowerShell task and here we can see the light green has been printed out from our configuration settings.</p>
<h1 id="Secrets-in-Production"><a href="#Secrets-in-Production" class="headerlink" title="Secrets in Production"></a>Secrets in Production</h1><p>I’ve already mentioned secrets in the context of Visual Studio development, but obviously, we use secrets, certificates, and keys in the production environment where the security of these assets is crucial. Azure Key Vault is not only a place to store keys, secrets, and certificates, but also provides functions that allow your users and applications to access them securely and correctly.</p>
<p>Apart from the usual role-based access control that is available with most Azure resources, the key vault has access policies, which allow you to set permissions on keys, secrets, or certificates per user or application or service principle. In this section, we will access a key vault, retrieving secrets in various applications and environments, and see how you can maintain secure access in both production and your development pipeline.</p>
<h1 id="DEMO-Accessing-Key-Vault-from-App-Configuration"><a href="#DEMO-Accessing-Key-Vault-from-App-Configuration" class="headerlink" title="DEMO: Accessing Key Vault from App Configuration"></a>DEMO: Accessing Key Vault from App Configuration</h1><p>Let’s look at how we can add secrets to our app settings and configuration. I’ll go into my howconfigkeyvault, and we can see I already have a secret set up there called supersecret. Because I’m using managed identity, I need to give Key Vault access to my app. I’ll do that by going into access policies and clicking add access policy. In add access policy, I’ll select a secret management template, which defaults to all non-privileged operations. Selecting a secrets template doesn’t exclude you from selecting operations on keys or certificates.</p>
<p>Next I’ll select the howatconfig principle. Then in terms of seek permissions, I’ll get rid of everything that’s not just a read or list permission. And add that access policy and save. I’ve got to say, this saving after adding is a bit clunky and I’ve forgotten to do it more than once. I mean, I’ve selected it, I’ve added it, it appears in the policy list, but wait, there’s more. Anyway, now going back to my configuration instance. I will need to add a key to my settings. That will be a Key Vault reference key. I’ll give that a name and make it part of mydemo@namespace, and I’ll select my Key Vault and my secret and create the key.</p>
<p>Now, back in the app, we need to configure access to the Key Vault. And that is just done by adding a configure key vault with set credentials and using the currently available credentials. Now over at the controller, I just need to access that setting which is a string saying demo app and config secret. No, not, that won’t do. It should be demo app. Okay. I just need another div to display the secret. Let’s publish that and see what it looks like. And there we have my secret revealed.</p>
<h1 id="DEMO-Replacing-App-Settings-with-Secrets-in-a-Release-Pipeline"><a href="#DEMO-Replacing-App-Settings-with-Secrets-in-a-Release-Pipeline" class="headerlink" title="DEMO: Replacing App Settings with Secrets in a Release Pipeline"></a>DEMO: Replacing App Settings with Secrets in a Release Pipeline</h1><p>Now I want to look at substituting secrets for settings within your app settings JSON file. What I’m going to do is replace the settings DB password and super secret as well as background color from within the settings group. I’ll just commit those changes I’ve made to the code and push them up to my repo. Now I’ll go over to the portal and I will create the secrets that are going to take the place of my settings. So the background is going to be red and the DB password is going to be my secret production database password, and finally, just a super secret.</p>
<p>Okay, having created my Key Vault secrets, now I’m going to create a release pipeline. So back in Azure DevOps under pipelines and releases, I’ll create a new pipeline. A run of the mill app service deployment task will do the trick. I’ll use the how pipeline service connection that I’ve created for this project. The app is being deployed to Linux App Service, and because of the resource group associated with the service connection, my app appears in the dropdown. Let’s save that.</p>
<p>Next I need to access my secrets. Just go back to the service connection and click on service principle. It’s gonna take me back to Azure portal and I’ll grab the name of the service principle. Then go to the Key Vault and into access policies and add a new access policy. This will allow my release pipeline to be able to access the secrets so I’ll just select a list and get permissions on the secrets dropdown, paste in the service principle name and select not forgetting to save the access policy.</p>
<p>Okay, back at Azure DevOps, I’m gonna go and create a variable group under library for grabbing the secrets from the Key Vault. I’ll give it an interesting name, like Key Vault secrets. Leave allow access to all pipelines and turn on link secrets from an Azure Key Vault as variables. Next, click, the add button and select our service connection followed by the Key Vault name and hit add variables. I’m just going to select all the variables and click OK. Now save secrets in the library.</p>
<p>Next thing I have to do is add the variables, the Key Vault secret variables to my pipeline variables. Just go to variable groups and link variable group and select Key Vault secrets and there we have them. Back at the deploy task, I will set up my runtime stack now going to application and configurations settings and this is where you map your secret variables to your app settings. I’m just going to paste ones I prepared earlier, but if we hit the edit button, we can go in and see them all laid out.</p>
<p>I’m going to get rid of the background color because I’ve already got it as settings background color. Now save that. The next thing I need to do is add a file transform task. This will allow me to access settings within my appsettings.json file. The file transform functionality used to reside inside the app deploy task. I just select the file format to JSON and add app sittings as my target file. The most important thing is I need code to deploy. So I need to add my build artifact and I can just select all of that from the dropdown list and click add.</p>
<p>There’s one more thing I need to do, which is a little bit redundant in my opinion, but completely necessary is add another variable. Now this variable is going to met to my nested settings background color in app settings JSON. So I need to specify the variable name as the full path, and I’ll just set the value as the automatically created variable that is getting populated from my Key Vault. So once again, I need to go into variables, into variable groups and link to that new group I’ve just created. And we can see the background color of the Key Vault variable matches the value of the app settings variable.</p>
<p>Now I can create the release. I’ll just fast forward through the running pipeline. Right just to refresh our memories, here we’ve got the local host version with the blue and the Dev password. Now I’ll going to the deployed app service, we can see that our background has become red and we’re showing the production password and the very secret Key Vault secret. There is another way to achieve the same result by using the Azure Key Vault task. So here I’ve added the Azure Key Vault task and connected to my subscription in Key Vault.</p>
<p>Over in the library, you can see that I’ve removed all the variable groups. And if I go back to the pipeline and have a look at my variables, you can see they’re all unlinked. And I’ve just got one variable, which is the one I need for mapping to my app settings background. So here is what the Key Vault task looks like in YAML script. Now back over to the Azure vault and I’ll set up a new version of my background color, which I have set to dark orchid. So now I’ll just go and refresh the page and we can see that that also works using the Azure Key Vault task.</p>
<h1 id="DEMO-CLI-Secrets"><a href="#DEMO-CLI-Secrets" class="headerlink" title="DEMO: CLI Secrets"></a>DEMO: CLI Secrets</h1><p>Before we move on, let’s have a look at a few simple command line interface key vault commands. First of all, let’s list our current key vaults with <code>az keyvault list</code>. With the key vault name, we can list it secrets with <code>az keyvault secret list</code> using the vault name parameter. The <code>az keyvault secret show</code> command will display the secret’s value. We can see darkorchid from the earlier demo.</p>
<p>Now, I’ll create a secret with the secret set command. The secret name is MI5 and it is only valid for the month of June using the not-before and expires parameters. If I go and look in the portal, I can see the newly created secret as you’d expect. Let’s create another version of that secret, which is only valid for the month of July, but is currently disabled.</p>
<p>Going back to the portal, we can see this new version with the new ID and sure enough, the status is disabled. I’ll copy the unique ID and use it with the set attributes command to enable the secret. Finally, I’ll delete the MI5 secret.</p>
<h1 id="Kubernetes-Configuration"><a href="#Kubernetes-Configuration" class="headerlink" title="Kubernetes Configuration"></a>Kubernetes Configuration</h1><p>In the context of configuration, Kubernetes essentially hides our containers from view inside pods. We could treat our containers as black boxes and just bake the configuration and secrets into them. But we don’t need to do that as Kubernetes does provide a mechanism for updating configuration and secrets. You can specify configuration settings inside a yaml file called a config map and you can do the same with the secrets also inside a yaml file, where the secret values are encoded in base 64.</p>
<p>Here we have the app settings from the previous web app represented inside a yaml file. While you could separate them all out as individual key value pairs, like databasetype, azurersql. I’m preserving the app settings as a complex data type so I keep code changes minimal within my application. The next question is how do we make our configuration settings available to our container image? Well, we do this by mounting the ConfigMap in a directory within the container that is relative to our app location, and we specify the location in our deployment yaml file.</p>
<p>Okay, so not entirely straightforward but better than baking the configuration settings into our container image. But what about secrets? In Kubernetes, at this time, secrets are essentially the same as configuration. Although they are base 64 encoded. While secrets are logically separate from configuration, you could hardly say that base 64 encoding is military grade encryption. What we can do with the Azure Kubernetes service, AKS, is pull our secrets from an Azure key vault and that’s what we’ll look at now.</p>
<h1 id="DEMO-Secrets-from-Azure-Key-Vault"><a href="#DEMO-Secrets-from-Azure-Key-Vault" class="headerlink" title="DEMO: Secrets from Azure Key Vault"></a>DEMO: Secrets from Azure Key Vault</h1><p>If we go to the Azure portal, I’ve already got a container registry set up with a misspelled repository in it. What I’ll do now is create an AKS Kubernetes two node cluster using Azure’s CLI with the az aks create command. The last argument is attach acr, so AKS knows where to pull the container images from. While we’re here, I wanna point out a bit of a glitch or problem that I had, and after some research found that I wasn’t the only one.</p>
<p>I initially tried to attach an existing cluster to my container registry with the az aks update command and kept getting an error of could not create a role assignment for ACR. Are you the Owner on this subscription? I’m not the owner, and I’m guessing most Azure users aren’t a subscription owner either. But I could attach to the container registry if I did it while creating the cluster. Strange, but there you go. </p>
<p>Okay, having created the cluster, let’s get the service principal’s client ID. Going back to the portal and looking at active directory app registrations, we can see it there. Next, I need to get credentials so I can start using the kube control commands. Because I’ve been through this process before, when I couldn’t get AKS to attach to an existing cluster to my repository, I’m being asked if I wanna overwrite previous configurations which I will say yes to.</p>
<p>Now I’m going to apply the deployment RBAC yaml file of Azure active directory pod identity project to make my Kubernetes cluster aware of Azure managed identities. Having done that, I’m now going to create a managed identity which I will use to access the key vault. With the new managed identity created, I’m gonna give it access to read secrets from my key vault. So go into access policies through add access policy and from the template, I’ll select secret management. Then in terms of permissions, I’ll just set get and list.</p>
<p>Now search for my newly created identity and add it, and crucially, don’t forget to save. So back at the CLI, I’m going to apply an Azure identity yaml file to my AKS set up. This file has the client ID of my managed identity as well as the resource ID. The meta data name doesn’t necessarily have to be the same as the name of the managed identity. We’re going to use that reference later in a binding file.</p>
<p>Okay I’ll just apply the active directory pod identity yaml file. The next step is to bind that identity to our Kubernetes cluster with an Azure identity binding file. So I told the binding file about the managed identity yaml file and now I apply that binding file to my cluster. Here’s the service principal ID that I created. I’m now going to do a role assignment of managed identity operator to that ID. Then apply my config map yaml file. And here I’m telling my deployment file all about the identity binding.</p>
<p>Finally I will apply my deployment file. Next I’ll get my external IP address and will bring up the app in the browser. So if I go back into my configuration file, my config map file, and change the background color to red. And I’ll go back to the key vault and replace the secret with another value. Just have a look at that changed value.</p>
<p>There we go, this is a changed secret. Back to the Azure CLI and I will apply the new config map file. I’ll restart my container and will go back and refresh, and there we go. We got the red background and the updated secret value.</p>
<h1 id="Securing-a-Kubernetes-Build-Pipeline"><a href="#Securing-a-Kubernetes-Build-Pipeline" class="headerlink" title="Securing a Kubernetes Build Pipeline"></a>Securing a Kubernetes Build Pipeline</h1><p>Now, I wanna talk about how you would secure a build pipeline delivering containers to a Kubernetes cluster. So, going back to first principles, there are three main elements we would start with. We have a code repository with application code in it. We have a container registry, such as Azure Container Registry, and we have our Kubernetes cluster. </p>
<p>So, the question is how do we securely get our code into our container, and our container into our Kubernetes cluster while maintaining the security of the build pipeline? You need to control access and the ability to push images into your registry, and at the same time, you need to control or ensure that only images from your registry are deployed to a cluster.</p>
<p>Kubernetes has an element or piece of code called an admissions controller. The admissions controller validates all incoming requests to the cluster. We can configure the admissions controller to say that when it pulls an image, we wanna make sure that the image comes only from our container registry. Azure policy integrates with Kubernetes admission controller to provide seamless and role based administration of Kubernetes clusters. So, we secure the deployment of our images from our container registry. But what about pushing the images into our container registry? How do we control that?</p>
<p>We have a build pipeline that pulls source code from the repository and creates container images. Who has the permission to push images to the container registry? Because whoever has that permission also has permission to push any image to the Kubernetes cluster. We want to restrict the ability to push images to the registry through our build pipeline, and we do that with the key. We then know that all images that are pushed to the registry come from a verified source, that is our build pipeline. By restricting access to the container registry to the build pipeline, we are ensuring the integrity of the entire DevOps process.</p>
<p>So, everything that gets released has had to go through code reviews and unit testing as part of the pull request merge process. In addition, we can incorporate vulnerability and credential scanning as part of the build process, checking that any secretive or sensitive information has not been included in the source code.</p>
<h1 id="Creating-a-Compliant-Development-Process"><a href="#Creating-a-Compliant-Development-Process" class="headerlink" title="Creating a Compliant Development Process"></a>Creating a Compliant Development Process</h1><p>When it comes to delivering software, security is usually the first element that is sacrificed when trying to meet a deadline. This is particularly unfortunate in light of recent data breaches that we’ve seen with some organizations. While the security sacrifice may, in the short-term, benefit the bottom line, in the long run, it’s turned out to be a false economy where the financial and reputational impact in some cases has been almost fatal, to the shareholders anyway.</p>
<p>In the traditional or old school software development lifecycle, security and testing typically came at the end of a project, just prior to release. If security issues are found at this stage, they, as with bugs, are expensive to fix due to the amount of rework needed. The CI&#x2F;CD DevOps process has revolutionized software development, and like bugs, security issues are far cheaper to fix if found early on in the development process.</p>
<p>DevSecOps is a newly coined term for incorporating security and security testing into the entire DevOps pipeline, so that it is part of your development process rather than something added on at the end as an afterthought at pre-deployment. Integration of security or moving security from the delivery end of the process is sometimes referred to as shifting left. If you picture the development process as linear, moving from left to right, you get the idea. From a DevOps point of view, the easiest way to shift left is to incorporate automated tools to run alongside existing unit testing, thereby having minimal impact on the development process. So far, we have looked at configuration, Azure Key Vault, and secrets in some depth.</p>
<p>Let’s see how we can use those elements as well as other tools, such as automated testing, to build a secure and compliant pipeline. In the spirit of left shifting, let’s start with the code.</p>
<h1 id="DEMO-Visual-Studio-Static-Code-Analyzers"><a href="#DEMO-Visual-Studio-Static-Code-Analyzers" class="headerlink" title="DEMO: Visual Studio Static Code Analyzers"></a>DEMO: Visual Studio Static Code Analyzers</h1><p>While you’ve been working away in Visual Studio, you no doubt, in fact, it’s almost certain that you have found the suggestions, the hints, the tooltips, whatever you wanna call them, sometimes very helpful and sometimes quite a nuisance. They’re included as part of the Roslyn code analyzer packages. If we go over to Solution Explorer and click on dependencies and then analyzers, you can expand the different analyzer packages. At the moment, we’ve got some warnings about unused variables, which in and of themselves aren’t really a security risk.</p>
<p>Now it’s possible to add other code analyzing packages. I’ll just open up NuGet package manager and search for FxCop. Here we are at the top of the list code analysis FxCop analyzer. I’ll just install that and accept the license agreement. Now we can see some extra packages have been added under analyzers. You can expand the different analyzers and view the rules that they support and if you right click on a rule, you can see the rule severity and have a look at its properties.</p>
<p>Now that I’ve added FxCop analyzer, when I do a rebuild, I see a bunch of warnings about SQL command strings accepting any user input. This is another way of saying that it is a possible entry point for injection attacks, specifically SQL injection attacks. If I click on that warning, I can drill down into the piece of code and see exactly where the issue is. What I can do is customize the rules, warnings and alerts for a particular analyzer. If I go to the NuGet folder under my profile and drill down into the FxCop’s folder, I can find the file that is related to the security rules under rule sets. The one I’m looking for is SecurityRulesEnabled.ruleset. I copy that file and paste it into the root of my project.</p>
<p>Okay, let’s have a look inside that file by double clicking, and it will bring up the rule window, but we can also open it with the XML Editor. So here we have a bunch of warnings and at the very top, there is review SQL queries for security vulnerabilities. What I’m going to do is change the action from a warning to an error. This is the same as right clicking on the rule in Solution Explorer and selecting severity from the context menu and changing the severity there. As it turns out, injection attacks specifically SQL injection attacks are the number one type of attack of the top 10 that is compiled by OWASP. To integrate these rules into my project, I need to add a code analysis rule set tag to my project file and then specify the file with the rules.</p>
<p>So I’ve edit the rule set. Now, I’ll just put in some actual code to call my deviant database procedures. Not that it’s really necessary as the analyzer is already really picking up the problems. Now, when I compile a code, instead of getting a warning, I get errors and the compilation fails. These customizable rules and severity levels are a great way to enforce coding standards within your team and project. These analyzer packages can be edit either as a Visual Studio extension or as we have done here just into the project itself.</p>
<p>Adding the packages and rules to the project means that everyone that is developing on the project gets the same set of rules. Obviously it doesn’t make much sense to have the stored procedure name, giving an error for SQL injection, so I’m going to change the action of the CA2100 rule back to warning from error.</p>
<p>Next, I’ll commit the code to my repo, adding little comment about adding the FxCop rules. I’ll just push that to my repo. If I go over to the build pipeline, we can see a build has been triggered and we can see the same warnings about SQL commands accepting user input have been displayed in our build task. The FxCop NuGet packages were polled as part of the build process and the customized rules edit to the source code repository.</p>
<h1 id="DEMO-Sonar-Cloud"><a href="#DEMO-Sonar-Cloud" class="headerlink" title="DEMO: Sonar Cloud"></a>DEMO: Sonar Cloud</h1><p>So far, I’ve been talking exclusively about Visual Studio and demonstrating in C#. Which I guess is fair enough as it is a Microsoft technology like Azure DevOps. Obviously, there’re a lot of other programming languages out there and in fact, C# is not the most popular by a long way. I want to look at integrating Sonar Cloud into my pipeline.</p>
<p>For all intents and purposes, Sonar Cloud is language agnostic. It supports COBOL through to Go and all the usual suspects in-between, such as Java, PHP, Python, Ruby, Scala, Swift, quite an extensive list. Sonar Cloud is a service that can be integrated into Azure DevOps via an extension. I’ll just go to the marketplace and search for Sonar Cloud. So here we can see SonarQube and Sonar Cloud. Now, Sonar Cloud is the one we’re after. SonarQube is for use with on-premise code repositories. Right, I’ll just hit Get it Free.</p>
<p>Okay, it looks like I’m not logged in as the account administrator. Yes, for reasons too complicated and pedestrian to go into I’m going to have to open up another browser and approve that request and install the extension. Back at the project, I need to set up a service connection to Sonar Cloud. It’s easy enough. I just go into New service connections and select it from the dropdown.</p>
<p>Next, I need to go over to my Sonar Cloud account, and under Security, I’m going to generate a new token for howlinuxconfig project that I’ve already set up as public. With Sonar Cloud you can have a free account as long as it’s public, so I need to make sure that my Azure DevOps project is also public. Just paste in the Sonar Cloud token and verify it, and I’ll give the service connection name and verify and save. </p>
<p>So now I can go back to my pipeline and add the Sonar tasks. This is a three-step process, which involves adding a preparation task followed by the code analysis task and then publishing the results of the analysis. Firstly, I will add the prepare analysis configuration task, which is going to go before my build task. This task basically sets up the connection with Sonar Cloud using the already created service connection, using our project key which just coincidentally happens to be my project name, and then the project name. Add that.</p>
<p>Next, I need to add the tasks that will do the actual code analysis, so it’s just a simple add. And finally, the task that’s going to publish the results which I’ll just add straight after the Sonar Cloud analysis task. I’ll add a comment basically to the effect that we’ve added Sonar Cloud. And I’ll save that, which will trigger my pipeline to run. We’ll just quickly run through the build. Sonar Cloud tasks are executing without incident.</p>
<p>To see the results of Sonar Cloud analysis we go to the build summary and under the Extensions tab, click on the link to Sonar Cloud. So under Security, we can dig into what looks like an E rating which doesn’t look too good. We can see that the usual unused variables have been highlighted. We can actually dig through the project structure to look at the different files where the issues are and here again, we can see the same SQL injection type issues being highlighted plus a bug.</p>
<p>So I’m not checking that the parameters may be null. Then if we go and look at the security hotspots, we can see all the highlighted issues are categorized and rated. In terms of categorization, we can see vulnerability and code smell, which translates to bad smelling code. Sonar uses a skunk icon on their website if you want to go and look at code smell. This means that this section of code may be indicative of some deep underlying issue or problem that you might want to look at. Our code smells are rated as major or minor. Clicking on code takes us to the folder structure and I can see in the bugs column there are quite a few. Just drilling down to these bugs and they are related to bootstrap JavaScript.</p>
<h1 id="DEMO-Pull-Request-Branch-Policies"><a href="#DEMO-Pull-Request-Branch-Policies" class="headerlink" title="DEMO: Pull Request Branch Policies"></a>DEMO: Pull Request Branch Policies</h1><p>So, we’ve set up these code analyzers, Roslyn, FXcop, and SonarCloud, and they are providing us with a wealth of information. But what we really need is this data to be used in preventing poor quality builds from being deployed. One way we can do this is set up branch policies so that pull requests to the main branch have to pass through certain quality gates for the code to be merged. I’ve created another branch called DBRework and I’ve made a bunch of changes to my code which I’m going to commit to the rework branch and then create a pull request to the master branch, which will have a branch policy set. The branch policy will use information from SonarCloud analysis to decide if the merge should be allowed.</p>
<p>If I go to branches and then to the context menu of the master branch, I can select branch policies. In a real-world situation, we would check require a minimum number of reviewers and we wouldn’t let me review my own code, but obviously for the sake of demonstration, I will just leave those checkboxes unchecked. Under build validation, I’m going to add a build policy. I’ll select the pipeline and give it a name and hit the save button. I’ve left the trigger on automatic so that the source branch will be built on changes committed, thereby triggering the SonarCloud quality gate.</p>
<p>To integrate SonarCloud into my branch policy, I’ll need to get a token. So, I go into my user account settings, and under security, select personal access tokens. I’ll give it the name SonarGateCheck. And I need to give it read and write access to my source code. So, this has been created, but I need to grab the access token that I’ll use in SonarCloud to give access to my pipeline. Next, I’ll go into my SonarCloud project and under administration menu, choose general settings and then pull requests. Under the provider, choose Azure DevOps, services and the personal access token. Paste that token that we’ve grabbed from the Azure DevOps account.</p>
<p>Right, so back in the DevOps project, I’ll create a pull request to pull the commits from my DB rework branch to master, and it sets off a build. I know what you’re thinking, so what? Nothing’s happened yet. Well, we haven’t finished. Let’s go back to branch policies, and this time I will go down to require approval from additional services and click the add status policy button. Having set up the connection with the personal access token in SonarCloud, the quality gate is now available to choose from the status to check drop-down list, which was not the case before. I’ll select my SonarCloud quality gate and save.</p>
<p>Back in Visual Studio, I’ll make a few more code changes and commit them, and then sync those changes back to my repo. Again, that triggers a build, but this time data from SonarCloud is going to come back, and we can see Sonar comments are visible inside the pull request overview. More importantly, we can see here on the upper right, the quality gate check failed. I’ve been back into the code and fixed the bugs and re-run the build pipeline, and this time we can see it has passed the quality gate.</p>
<p>So, a little workaround to suppress those warning messages is to insert pragma warning disable and then the code of the rule you want to have suppressed. There is a full listing of all the analyzer rules on the docs.microsoft.com site with detailed explanations and examples of how the rule is violated and how to fix those violations.</p>
<h1 id="Using-Open-Source-Libraries"><a href="#Using-Open-Source-Libraries" class="headerlink" title="Using Open Source Libraries"></a>Using Open Source Libraries</h1><p>Who doesn’t like free stuff? Open-source libraries are great. They don’t cost anything, most of the hard work has been done, you can even customize them so they work just how you want. I wouldn’t go as far as saying you get what you pay for, but this is another example of no free lunches.</p>
<p>There are a couple of considerations to take into account when using open source libraries. The most important one is the fact that you may be introducing vulnerabilities into your code base. When your own product has security vulnerability, it typically applies just to your product. So a hacker with nefarious intentions has to find a vulnerability in your software and then they can only exploit your data.</p>
<p>In the case of open source libraries, once a vulnerability has been found, the potential impact can be widespread. Potentially every piece of code out there that relies on that open-source package is now subject to malevolent forces. Another consideration when using open source software is that not all open-source software licenses are the same. When most people think of open source they think of being able to download it, use it as much as they like, incorporate it into their code, modify, really just no limit to what you can do with it. While this may be the case for many open source products, it is definitely not the case for all. </p>
<p>There are some licenses that stipulate, if you make any changes or modification to the original, you must also make those changes open source as well. Or make software that uses the libraries also open source. The GNU GPL requires that when you use GPL-licensed software to make other software and release it to the public, the resulting software must be open-sourced with the same license. Bottom line is be aware of your license obligations. The whole open-source thing definitely seems like a potential minefield, especially in the context of automated DevOps pipelines.</p>
<p>Due to the widespread use of open-source libraries and the rapid adoption of DevOps processors, it was only a matter of time before someone saw a gap in the market to provide tools that will help you navigate the open-source library obstacles. Whitesource is a company that makes tools for scanning open source libraries for potential security vulnerabilities and versions to make sure you’re up to date and informing you of your license obligations. Next, I wanna look at how to integrate Whitesource Bolt, the Azure DevOps variant of the Whitesource product.</p>
<h1 id="DEMO-Scanning-Open-Source-Libraries"><a href="#DEMO-Scanning-Open-Source-Libraries" class="headerlink" title="DEMO: Scanning Open Source Libraries"></a>DEMO: Scanning Open Source Libraries</h1><p>To use WhiteSource Bolt in our build pipeline, first thing we need to do is add the extension into our Azure DevOps from the marketplace. So let’s go and browse the marketplace and search for WhiteSource Bolt. Okay, got two options here. I’m gonna choose WhiteSource Bolt, which is free.</p>
<p>Obviously there are limitations to a free tool, and one of those limitations is the number of scans you can do per day, which is currently limited to five. My organization is already there in the Azure DevOps list. I’m just going to hit install. You can see there is an option for installing WhiteSource for on-premise Azure DevOps servers.</p>
<p>Okay, back to Azure DevOps and we can see now there is a WhiteSource Bolt menu item underneath pipelines. We’ll go into there and just fill in some details to get the extension set up completely. Having done that, I will click the get started button, but all that’s done is take us to the WhiteSource website where there is some useful information on how to get started, which is exactly what we’re doing now. To use it, we need to include it in our pipeline.</p>
<p>I’ll go back to the pipeline and just for simplicity, I’m going to remove the SonarCloud tasks and then search for the WhiteSource Bolt task and add it. We can leave the working directory blank, and under advanced settings, there the options to exclude folders or files or add extra folders to scan. So, I’ll just click add to add the task. We don’t need any inputs, and I will just give it a display name of scanning open source. Okay, let’s save that with a comment. Yes, I should not have done that on the master branch. Nevermind, I’ll just create a new branch for the commit. So, here’s the build running, there’s nothing really to see here in terms of what WhiteSource Bolt does, it all happens remotely, and it’s just the end report that gets pushed back to Azure DevOps.</p>
<p>Once the pipeline has finished running, we go back to the WhiteSource Bolt menu item and the report is loaded. So, at the top of the report, we have a security summary where there’s a vulnerability score. The score is that of the most vulnerable element in the report. It gives us a breakdown in terms of severity of all the vulnerabilities and also has an indication of when vulnerabilities were first logged. So, we can see some are more than 90 days old.</p>
<p>As this is pretty much just a boilerplate project, there’s nothing much to see here. All the issues are with jQuery libraries being out of date. The next section, license risks and compliance, tells us what kind of open source licenses we have in our project. Apparently, WhiteSource Bolt does not know what risk we’re at with the Azure SDK, that’s interesting. Below licenses, we have a list of all the libraries that are outdated or that could be updated. 290 does seem a lot. And then at the very bottom, we have an inventory of all the libraries involved in the project. Wow, that just goes to show how much code you don’t write when you’re building an application.</p>
<h1 id="DEMO-Penetration-Testing"><a href="#DEMO-Penetration-Testing" class="headerlink" title="DEMO: Penetration Testing"></a>DEMO: Penetration Testing</h1><p>Penetration testing is really the last piece in the DevOps puzzle in terms of application testing and brings us full circle, or rather back to the right-hand end of the DevOps pipeline, so to speak. The question becomes, how can we automate penetration testing as part of our CI, CD process? There are many tools out there that will do this, but I’m going to look at one in particular and how to integrate it with the build pipeline.</p>
<p>If you recall earlier on in the course, when I talked about injection attacks and I said it was the number one on the top 10 list of OWASP, the Open Web Application Security Project Security Risks. Well, it turns out, OWASP has a product called OWASP ZAP, which can perform both passive and active penetration testing on your web app.</p>
<p>Currently, OWASP ZAP is not provided as a service. But to give you an idea of how it works, you can download an executable, which I’ve done here and I’m running it against my app service. The good people at Microsoft have created an extension you can download for free into your Azure DevOps organization.</p>
<p>Let’s go over to the marketplace and find that extension. I’ll just search for OWASP ZAP. Here it is, the OWASP ZAP scanner. We’ll just have a quick look down the page because there are some interesting scripts here that we’re going to use later on for integrating their reports into our DevOps tests. I’ll just install it by clicking the Get It For Free button and go back to my project build pipeline.</p>
<p>So the first thing I need to do is add the OWASP scanner task. You have two choices of scan type, targeted or scan on an agent. We are going to use targeted scan, as this means targeting a URL. Scan on an agent is when you want to scan on a container app. Aggressive scan mode means active penetration testing, which simulates what a hacker might do to try and break into or break your website.</p>
<p>Failure threshold is the number of failures the scanner will tolerate until it fails the build. And of course, I’ll be scanning on port 80, but obviously you can change that if the need arises. After the scanning has been run, we need to get hold of the reports. And we are going to do that with a publish build artifacts task. I’m just going to give it a meaningful name, but I will also need to change the path to publish to, Build dot Sourcedirectory forward-slash owaspzap. I don’t need the published location. And while I remember, we shouldn’t run aggressive mode on OWASP ZAP scanning during continuous integration and deployment. Aggressive or active mode is resource and time-intensive and it’s something that should be done on a scheduled job, perhaps overnight when people aren’t working on the build pipeline. And if the application is in production, at times when it is likely not to be highly-utilized.</p>
<p>Having published the reports, this is where I want to go back to the marketplace and grab those scripts to integrate into Azure DevOps testing. So I’ll just copy the first one and paste it in, then grab the next script, which is another bash script, paste that in. And then the last task is to publish the test results. Now that we’ve finished setting up the build script, we can save and run it. Let’s just go and have a look at the pipeline in action.</p>
<p>As I mentioned before, OWASP ZAP is not a service. So to get it to work as an extension within Azure DevOps, there is a build of the executable in a container that gets downloaded and run. OWASP has a regularly updated container image on their website that is free to use. So we go to our artifacts under build summary and we can see the raw, OWASP ZAP reports.</p>
<p>Let’s have a look at the HTML report. Next, if we go into tests under summary, we can see that the tests are being integrated from OWASP ZAP. Let’s have a look at the details. But you can also create a bug issue directly from the tests, with all the details from the report filled in and suggested solutions. So I’ll just do that and assign it to the guy with the five empty coffee cups on his desk. And there we go.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>We started off this course by looking at how we could use user secrets to prevent sensitive password information from being saved into the repository with the source code. Then we had a look at the App Configuration Service as a central and secure repository for your application settings. App Configuration has filtering mechanisms such as labels and also uses namespace format to allow you to have settings for multiple applications and multiple sections within an application. App Configuration also supports feature flags, which are very important if you’re using a release flow deployment strategy.</p>
<p>It has a mechanism called a Sentinel key which your application can monitor for changes, and when it sees a change in that particular key, which doesn’t necessarily have to be called Sentinel, it knows to update all the settings and re-pull them without you having to restart your application. App Configuration also seamlessly integrates with Azure DevOps pipelines so you can pull settings directly into your application from App Configuration Service and you can set up a key within App Configuration that references a secret within your Azure Key Vault by giving your app configuration access to the Azure Key Vault.</p>
<p>You can also replace settings in your DevOps pipeline with secrets from the Azure Key Vault. We had quite a detailed look at how you can use managed identities and access secrets from within your Key Vault to use in container apps that will be running on an Azure Kubernetes cluster. We also saw how a secure development build pipeline might look when you are wanting to deploy code and an image to a Kubernetes cluster using secure keys to enforce security between your pipeline, your container registry, and your production cluster.</p>
<p>Next, we looked at other elements that are needed to make up a compliant pipeline, and the concept of DevSecOps and shifting your security left from the deployment end of the pipeline. We started with the integrated development environment, specifically Visual Studio, and saw how we can download extra code analyzer packages to complement the existing Roslyn code analyzer. We can inspect and customize the rules within the code analyzer, and by making the customized rule sets part of the source code, those rules will remain with the project and be incorporated into the build pipeline.</p>
<p>Third-party code analyzers, such as SonarCloud, support a wide variety of languages. SonarCloud can be integrated into the Azure DevOps pipeline via an extension from the marketplace. We saw how we can implement branch policies to prevent the merging of poor quality code. SonarCloud quality gates integrate with the branch policy and we can specify whether a build is valid or not. Open-source libraries, while useful and great time savers, have their own set of issues. This ranges from accepting any vulnerability within the library into your own project and possible fishhooks with the open source license.</p>
<p>WhiteSource Bolt is an extension from the Azure DevOps marketplace that allows you to integrate open source library scanning into your build pipeline. It reports on known security issues and which type of license pertains to each library. Finally, we saw how to incorporate passive penetration testing into your build pipeline, and how you could schedule active or aggressive penetration testing during down times.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Adding-Mobile-Devices-to-Your-Azure-DevOps-Strategy-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Adding-Mobile-Devices-to-Your-Azure-DevOps-Strategy-7/" class="post-title-link" itemprop="url">AZ-400-Adding-Mobile-Devices-to-Your-Azure-DevOps-Strategy-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:37" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:37-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 10:06:24" itemprop="dateModified" datetime="2022-11-27T10:06:24-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Adding-Mobile-Devices-to-Your-Azure-DevOps-Strategy-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Adding-Mobile-Devices-to-Your-Azure-DevOps-Strategy-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Hi, my name is Kelso Sharp, I have been writing web applications for the past 20 years. I’m also a Microsoft Certified Professional and an Azure Cloud Architect. I’ve been working with the Microsoft web development stack since 1996. Welcome to my course on implementing mobile DevOps strategy.</p>
<p>In this course, we will be discussing and demonstrating how to implement a mobile DevOps strategy. If you run into any issues during the course, please feel free to email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> and the team here at Cloud Academy we’ll do our best to help you get your issues sorted out as quickly as possible. As always, please remember to rate my course and provide feedback as I’m always looking to improve my courses.</p>
<p>Visual Studio App Center is a collection of common services and integrated tools that are part of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>. It allows you to create a mobile DevOps strategy for building, testing, and releasing your mobile applications. With App Center, you can perform build and automated UI testing for iOS, Android, Universal Windows Platform, and tvOS, using different native and web-based testing frameworks.</p>
<p>Just like Azure DevOps pipelines, App Center can trigger builds and distribute your releases. It also provides you with a way to test your mobile applications on real devices. You can group devices together into a set and create a test series that has one or more device sets. In a future module, we’ll talk about device sets, device series, and device tiers.</p>
<p>Although compared to <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/az-400-exam-prep-microsoft-azure-devops-solutions-1-1368/">Azure DevOps</a>, App Center is a little bit more streamlined and build and release functionality is not quite as robust. However, it makes up for this with the ability to do testing on real devices and with its integration with Azure DevOps pipelines. We will go through a demo on how to integrate apps in our UI testing and distribution into an Azure DevOps pipeline in a future module. If you’d like to know more about Azure DevOps pipelines, you can check out one of my other courses here on Cloud Academy called <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/course-introduction/">Implementing and Managing Azure Build Infrastructure</a>.</p>
<p>Implementing a mobile DevOps strategy can involve a number of tasks such as continuous integration, continuous delivery, and user interface testing. One of the greatest challenges to mobile development is the ability to run the same application across hundreds of different devices and still achieve the same user experience.</p>
<p>This necessitates the need to do testing that spans multiple device tiers, multiple operating systems, and multiple device manufacturers. This could be a truly expensive and daunting task. Lucky for us, this is where App Center comes to the rescue.</p>
<p>As I mentioned, App Center has a number of services and tools for creating a strategy for building and releasing our mobile applications. These tools are <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/mobile-build-services/">build services</a> which uses built-in agents or custom scripts to perform built functions, Test Runs, which is an automated UI testing tool that runs our unit tests on a real device or set of devices.</p>
<p>App center supports iOS, Android, Universal Windows Platform, and tvOS. Your mobile applications can also utilize App Center’s analytic and diagnostic services to collect data about your application and how it’s being used. This can give valuable insights and how to improve your application, what new features to add, and to find undiagnosed issues that will surely show up from time to time.</p>
<p>App Center allows you to automate your release process and distribute your application to testers prior to releasing it to the various app stores. This can reduce the possibility of missed tests and rejected app store approvals, which can be time-consuming and costly.</p>
<p>This course is intended for IT and DevOps professionals that need to implement a DevOps strategy for mobile applications, IT professionals that wanna learn more about mobile DevOps options in Azure, and anyone looking to increase their knowledge on how to deploy mobile applications using Visual Studio App Center.</p>
<p>For this course, you should have an understanding of DevOps processes, you should be able to set up your IDE or integrated development environment for mobile application development. You should be able to install Node Package Manager and other command-line tools if you intend on following along in the demos. You should have a good understanding of Azure Pipelines, and how to work with YAML files and service connections in Azure DevOps.</p>
<p>In our first module, we’ll look at the Visual Studio App Center, what it is, and how it plays into our mobile DevOps strategy. In our second module, we will look at build services and iterate on the build service options available there. Then we’ll go through a demonstration of creating an App Center account, creating an organization, connecting to our code repository, and building our application.</p>
<p>From there, we’ll have a look at UI testing for multiple device sets and go through a demo of setting up a test series and creating a device set that will allow us to run our UI tests on actual devices. We will do this using both App Center CLI, and Azure DevOps pipelines. Then we’ll have a look at distribution groups and how to release our application to these groups. Finally, we will discuss implementing analytics and diagnostics, and demonstrate how to build this functionality into our application.</p>
<p>By the end of this course, you should be comfortable with creating an App Center account, creating an organization, and adding a new application. You should be able to create public and private distribution groups, build your applications, run UI testing across multiple device sets using both App Center Command Line Interface, and Azure DevOps pipelines. Finally, you should be able to understand and use analytics and diagnostics provided by App Center.</p>
<p>Now that we have a plan, let’s get started.</p>
<h1 id="Mobile-Build-Services"><a href="#Mobile-Build-Services" class="headerlink" title="Mobile Build Services"></a>Mobile Build Services</h1><p>In this module, we will be discussing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/demo-app-center-build-services/">App Center Build services</a>. Build services allow you to connect to your code repository and trigger a build when code is committed. This is an optional service, and it’s up to you to decide if App Center build services will work for your application or not. You also have the option to use <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps Pipelines or you can also simply upload pre-built application files for testing and release. To help you determine if build services will be appropriate for your application or not, let’s talk about the capabilities of the build service.</p>
<p>Build services can build <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">mobile applications</a> for iOS, Android, Windows Platforms, and tvOS respectively. For iOS, App Center supports the Swift and Objective C programming languages, and for platforms, it supports React Native, Xamarin, and Cordova is currently in preview. For Android, the supported languages are Java and Kotlin, and supported platforms are React Native, Xamarin, and again Cordova in preview. Windows Operating system platforms are Universal Windows Platform, Windows Presentation Foundation, WinForms, and Unity. tvOS currently only supports Objective C and swift.</p>
<p>The currently supported code repositories are GitHub, GitLab, BitBucket, and Azure Repos, but this service offering will likely continue to grow as App Center expands and improves. You are able to connect by giving app center third party access to your code repositories. When you build your application, you can perform a one-time configuration that allows you to sign any of your builds so that you can distribute your application to groups of users such as testers or to the respective app stores. Unsigned builds can only be run on an emulator, so if you plan to distribute your mobile application, signing your build is mandatory. Build signing is a way to ensure that no one other than you can update your application.</p>
<p>Your mobile applications are built using single-use VMs that are expressly spun up to build your application, after which they are immediately torn down to ensure that you have a clean environment and to maintain the security of your application.</p>
<p>Builds for iOS and Android by default use a macOS VM with common development and runtime software installed. You can find the list of software installed here at this <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/appcenter/build/software">URL</a>.</p>
<p>Windows mobile applications are built using a Microsoft hosted agent running windows and visual studio. You also have the option to build your Android applications using this configuration although it’s not the default setting. You can find the relevant information for the VM and software installed here at this <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops#software">URL</a>.</p>
<p>Now that we know what’s included in the build services, let’s go take a look at our build services demo.</p>
<h1 id="DEMO-App-Center-Build-Services"><a href="#DEMO-App-Center-Build-Services" class="headerlink" title="DEMO: App Center Build Services"></a>DEMO: App Center Build Services</h1><p>In this demonstration, we’ll be diving into App Center <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/mobile-build-services/">build services</a>. App Center can handle building <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">mobile applications</a>, testing UI on physical devices, managing distribution groups, crash reporting, and usage monitoring.</p>
<p>The first thing that we need to do is point our browsers to appcenter.ms. This will take us to the main sign up page for App Center. We will just click the “GET STARTED” button in the top right. This link will take us to the create account page. Here we have the option to use third-party authentication from GitHub, Microsoft, Facebook and Google. We can also create an App Center account directly. I’m just gonna use my GitHub account because that will make it simple to connect to our code repository on GitHub.</p>
<p>Once we have signed into our account, the first thing we might wanna do is to set up an organization. Organizations in App Center are just a convenient way to organize our applications, especially if we’re working with multiple teams and multiple applications. Attaching an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> subscription to this organization will allow you to integrate it with tools like Azure Key Vault and Active Directory from that subscription.</p>
<p>Now we can click the “New App” button in the top right. That will bring up our new application configuration page. Here we can enter our application name, and note this did not need to be the same as our project name. We then pick an operating system and a platform and finally click the “Add new app” button to create our application. I’m not gonna add this app since I’ve already created one for us to use in this demo.</p>
<p>If you’d like to clone the mobile application to follow along with this demo, you can find it at the following <a target="_blank" rel="noopener" href="https://github.com/kelsosharp/MobileDevOps">URL</a>. This application is just a template Xamarin drawer style Android application, with a Xamarin UI Test project for the UI testing. We will need this later to integrate our tests with the App Center Testing API.</p>
<p>The main overview page should now be displayed. This page explains how to configure App Center to work with your particular development environment and IDE setup. We’re using straight Xamarin on a Windows 10 client machine, which means we will need to have the Xamarin SDK installed at a minimum.</p>
<p>Now let’s have a look at how to connect our code repository to our new App Center application. We will just click on the build option from the menu on the left-hand side, that will give us a list of code repository providers and currently App Center only supports <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/az-400-exam-prep-microsoft-azure-devops-solutions-1-1368/">Azure DevOps</a>, GitHub, Bitbucket and GitLab. Our code is currently in GitHub so that’s what I will select.</p>
<p>Now we see a list of repositories on our GitHub account. I will select the MobileDevOps project and that’s all we need to do to link our App Center account to our code repository. If you did not use GitHub as your credentials provider, you will most likely need to authenticate with whichever provider you use and provide access for App Center to your repository.</p>
<p>Since I’ve already committed the master branch to my code repo and it is ready to build, we can now configure our build settings for this branch. When we click it we can see that there’s now a “Configure build” button on our last commit. I will go ahead and click it so we can configure the App Center build options.</p>
<p>This is our build configuration page. Here we can set the type of build we want, debug or release. We can set the version of the Xamarin SDK used to build our application, we can specify any build scripts we might wanna use, and set our build frequency. This is what triggers our builds. Some of the other options are Build Android App Bundle, this creates a bundle as well as your APK file if checked. We can turn on auto-increment for updating our app version automatically, we can set environment variables. If you’re going to use App Center to deploy your application, this is where you would provide your KeyStore or provisioning profile for Android and Apple. This will allow App Center to sign our builds for deployment to the app stores and enable physical device testing.</p>
<p>At this point, we can set our build to do an initial test on an actual device, but in order to do that, we need to change a project property in our Android project. “Use a shared Runtime” needs to be unchecked. Let me show you how to do that in Visual Studio.</p>
<p>The first thing we need to do is to right-click on our main Android project and select the project properties page. On the properties page, we need to click on the “Android Options” section and then uncheck the “Use Shared Runtime” option. This is usually checked by default and I previously unchecked it when I created this project for use in our course.</p>
<p>Now that we have our build configuration set up, let’s run our build. This is gonna take a few minutes, so while that’s building, let’s talk about the costs for App Center Services. The free tier of App Center gives you 240 build minutes per month, with a single build time limit of 30 minutes. You get a 30-day free trial of UI testing on real devices to start with, unlimited distributions and unlimited users, and full access to analytics and crash reporting.</p>
<p>If the free tier is no longer meeting your needs, for example, if you have multiple teams building multiple applications in App Center at the same time, without concurrent builds, they will simply be queued and execute in sequence. You can add additional concurrent builds for an extra $40 per month for each additional build. This is referred to as build concurrency. This is defined by Microsoft as “the number of builds that can run in parallel “at any given time.”</p>
<p>For <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/ui-testing-in-app-center/">UI testing</a> this is a similar pricing model. After your 30-day trial is over, testing costs $99 per month for 30 hours of device time per device concurrency. For example, if you have five devices, the devices will be tested in sequence up to 30 hours in a month. If you were to purchase five concurrent devices, then all five devices would be tested at the same time and reduce your overall testing time to one fifth, and you have 150 total device hours to work with.</p>
<p>Now that our build is done, we can see that it’s been successful. So as you can see there’s plenty of options in the build services for App Center. In our next module, we’ll be talking about how we go about testing our mobile applications on real devices. I hope I see you there.</p>
<h1 id="UI-Testing-in-App-Center"><a href="#UI-Testing-in-App-Center" class="headerlink" title="UI Testing in App Center"></a>UI Testing in App Center</h1><p>One of the major benefits of testing in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/demo-app-center-build-services/">App Center</a> is the ability to test your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">application</a> on multiple physical devices, without the need to buy the actual devices themselves. App Center provides the ability to create something called a device set. A device set is a set of device configurations grouped together by different options, such as OS version, manufacturer, and device tier. A device here is a value of 1, 2 or 3, with the lower numbers representing the most current hardware released. You can also filter the device list by other options, such as CPU, memory, and form factor.</p>
<p>You can add these device sets to a test series. A test series can be a way to organize different types of tests like smoke test or <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/demo-ui-testing/">UI test</a>. You can also use it as a way to group multiple device sets to run them together. There’s no limit to the number of device sets or test series you can have.</p>
<p>You can use one of a number of testing frameworks in App Center. The current testing frameworks include Appium, XCUITest, Espresso and Xamarin.UITest. To utilize a testing framework, you simply add one of the chosen frameworks to your solution as you normally would, then create a test run in App Center that is configured to use your testing framework.</p>
<p>App Center takes the unit test framework and utilizes it to run your test on each of the devices in the device set. It takes a screenshot of the test results and creates a test report and adds them to the App Center. </p>
<p>You can run your tests in App Center by using one of three options. You can use the App Center CLI, the App Center REST API, or you can use an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps pipeline task. In a later demo, we will walk through running our tests from the App Center CLI and from an <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/az-400-exam-prep-microsoft-azure-devops-solutions-1-1368/">Azure DevOps</a> pipeline. If you’d like to know more information on the App Center API, you can find it at the following <a target="_blank" rel="noopener" href="https://openapi.appcenter.ms/#/test">URL</a>.</p>
<p>Now that we have a basic understanding of what testing entails in App Center, let’s check out the demo.</p>
<h1 id="DEMO-App-Center-CLI-UI-Testing"><a href="#DEMO-App-Center-CLI-UI-Testing" class="headerlink" title="DEMO: App Center CLI UI Testing"></a>DEMO: App Center CLI UI Testing</h1><p>In this demo, we’re going to use the App Center command-line interface to upload and run our unit tests from our test project. This requires that you have Node js 6.3 or later, npm and the App Center CLI installed. You also need to make sure that the testing framework you picked when you set up your test run is also installed and configured for the project. In our case, we’re using the Xamarin.UITest.</p>
<p>So now that we have a successful build, let’s have a look at testing. We will select the test menu option on the left side and that will bring up our test runs page. If you’ve already had a few test runs, this will show a list of the runs here. If you’ve set your build configuration to do an initial test on a real device, you will see that test run listed here.</p>
<p>If you do not have any test runs yet, there should be a button that says start testing your app. Clicking this will bring up a dialog, that asks you if you want to start your free trial of device testing. We can click on the Start Trial button and this will bring up a list of devices to choose from.</p>
<p>We can select whichever devices that you’d like to use to test your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">mobile application</a> on. You can filter the device list by Foreign factor, CPU, memory, operating system and manufacturer. Once you have a list of devices you want to use, just check the checkbox beside each one or check the checkbox at the top of the list to select them all. There’s a lot of options here. If you want to see the list of all the devices available, you can check them out at this <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/appcenter/test-cloud/devices/android">URL</a>.</p>
<p>When you run the command to upload and execute your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/ui-testing-in-app-center/">UI tests</a> using the App Center CLI, there’s a few pieces of information that would be helpful to have handy, or to know when typing out the command. The first is your App Center username or organization name. However, you need to replace the @ symbol in any spaces with a hyphen as these cannot be used in the command-line. The next piece of information that’s handy to have, is the path to the APK file that’s built by your project. You can specify this path when you build your Android application for the first time in Visual Studio.</p>
<p>The next piece of information that you need to have is the path to the cloud-test.exe file in the Xamarin.UITest NuGet package for our test project. Xamarin no longer uses a NuGet.config file or creates a packages folder as part of the project. So you need to specify the path using the argument, <code>--uitest-tools-dir</code>, to specify the path. This will be different depending on the version of the NuGet package you’re using. And if you upgrade your NuGet package, you will need to update this path as well.</p>
<p>This is a fragile and brittle implementation and can cause you hours of frustration if you’re not aware of it. The App Center Dev team at <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft</a> is aware of this issue and is currently working on providing a better way to handle it.</p>
<p>Finally, you will need the name of the device set in the App Center, that you want to use to test your project. If you create a new series without first creating a device set, there will be a button named Save set, in the top right of the configuration page. If you do not click this and give the set a name, a random ID will be created for the set and the set itself will not be saved.</p>
<p>Once you’ve chosen your testing series and your testing framework, you will then be given the opportunity to copy the <code>appcenter-cli</code> command to the clipboard. This command has the device set random identifier, and this is what we used for this demo. But if you do not copy to the clipboard now, or you do not click the Save set button and give it a name, you will not be able to find it again once you leave this page. So I recommend that you either create a device set prior to creating a test series or save the device set for later use.</p>
<p>I’m just going to take these paths and paste them along with the command in the notepad here to make it easier for me to edit. You will need to use this every time you want to run the UI tests on this device set in App Center. The identifier given as the name for the device set defines a list of devices being used for the tests. So if you copy these identifiers, you can reuse them in PowerShell or bash so that you can run multiple UI tests against multiple device sets.</p>
<p>Now, I will copy the command and run it in our command window. This, will upload the project files and run the unit tests that are in the Xamarin.UITest project on all the devices in the test series we named Master.</p>
<p>Now as we watch, we can see that these tests are being run in sequence and not in parallel. If we had paid for additional concurrent devices for our apps in our account, we could have run these in parallel and reduce the length of time needed to run our tests. For just a few devices, this is not too much of a problem. But if our device set had hundreds of devices, it could take many hours to get through them all and we would like to run out of device minutes quite quickly.</p>
<p>Once our tests have completed, we can look at the test run page in App Center and see that they’ve passed successfully. And we can have a look at the results for each of the test as an image on the device it was run on.</p>
<p>That’s all there is to having App Center run our UI unit tests on actual physical devices. This gives us the comfort of knowing exactly how our UI will run on each of the devices we used to test with. A small price to pay to know our user experience will be consistent across many devices.</p>
<p>In our next module, we’ll be demonstrating how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/demo-app-center-azure-devops-integration/">integrate Apps Center UI testing into an Azure DevOps Pipeline</a>. It’s some pretty cool stuff, so let’s go check it out.</p>
<h1 id="DEMO-App-Center-Azure-DevOps-Integration"><a href="#DEMO-App-Center-Azure-DevOps-Integration" class="headerlink" title="DEMO: App Center Azure DevOps Integration"></a>DEMO: App Center Azure DevOps Integration</h1><p>In our previous demo, we used the App Center CLI to upload and run our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/demo-ui-testing/">unit tests</a> from our Xamarin.UITest project. Now, I will show you how to add the App Center Test task to a pipeline in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/https://cloudacademy.com/library/azure/">Azure</a> DevOps. This task is the equivalent to the App Center CLI command, and it uses the same REST API that the App Center CLI used. We will use this task to upload our project to the App Center and test it on the device set we called master. But before we can do this, there are a few things that we need to do to enable us to run this task successfully.</p>
<p>Before we head to <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/az-400-exam-prep-microsoft-azure-devops-solutions-1-1368/">Azure DevOps</a>, we need to create an API access token for the App Center. This token will grant Azure DevOps access to our App Center organizations and applications. We can do this by clicking on our profile icon in the top-right corner of the page, selecting account settings, and then scrolling down and clicking the API tokens. This will open a page that lists our existing API tokens, if we have any.</p>
<p>Now, we need to click the New API token button in the top-right. This opens a configuration page and here we need to give our token a name and set the access level to full. Once we have done that and clicked the add, this will bring up a dialog box that has our access token displayed, and gives you the opportunity to copy your token to the clipboard. This is the only chance you’ll get to copy this token, so make sure to copy it and save it to a safe place.</p>
<p>We can now head to Azure DevOps and open our project settings page. From the list of options now displayed, we click on the service connections menu option. This will display a list of current service connections and a new service connection button will be in the top-right. Clicking this button will give us a list of options to choose from. We wanna scroll down and select Visual Studio App Center. After selecting this option and clicking the next button, we need to paste our new API key here in the API token box and add any details that you’d like to add. The checkbox under the security heading, if checked, gives all pipelines immediate access to this service connection upon creation. If unchecked, you need to authorize each pipeline that uses this service connection individually the next time you build those pipelines. Click save, and you now have a new service connection to the App Center.</p>
<p>Azure DevOps pipelines are the CI&#x2F;CD or Continuous Integration Continuous Delivery automation process for building our projects. The first thing we’re gonna wanna do is to create a variable group and variables to hold our security information for signing our build. We will use encrypted variables to prevent them from being saved as plain text in our code repository.</p>
<p>So we will click on the Pipelines option from our project menu on the left side, and then click Library. The Library is where we can create variable groups, environment variables, and upload secure files, like our Android Keystore. We can also set up access roles for these files and variables and assign users to those roles.</p>
<p>Now we click on the Add Variable Group button. This will bring up a configuration page for creating a new variable group. I’m gonna call this group Android key group, and we’re going to add each of the variables we need for signing our build. The variables that we need are the Keystore password, our key alias, and our key password. If your App Center or Azure DevOps organization is linked to an Azure subscription, we would be able to use the Azure Key Vault here to store these values and access them.</p>
<p>Unsigned Android builds are only allowed to run in an emulator. So if you wanna be able to test on a real device, you’re required to sign your builds. In order to sign a build for an Android application, you need to create a Keystore file using Android studio. If you’d like to know more about how to sign your Android builds for use outside of an emulator, you can get more information at the following <a target="_blank" rel="noopener" href="https://developer.android.com/studio/publish/app-signing.html">URL</a>.</p>
<p>The first variable we will create is going to hold our Keystore password. We will name the variable keystore.password. And by clicking a lock on the right here, it will encrypt this variable and change the input to a password text box type. Next, we’ll create the variable for the key alias and we will name it key.alias. Finally, we will add the variable for the key password itself and name it key.password. Oh, I forgot to save my variable, so when I clicked on the Pipeline’s menu option, I got a popup that asked me if I really wanna leave the page. So, I will click no, and I will save my variables now by clicking on the save icon at the top of the page.</p>
<p>The last thing that we would need to do is upload our Keystore file to the secure files area of the library section. We will click on the secure files link here, and this is where you would upload your Keystore file. I’ve already done that, so I do not need to do it again. And if we click on the file, we see that we can add additional properties and we can authorize its use for all pipelines.</p>
<p>Now that we have our sensitive data configured, we will edit our Azure Pipeline by updating our YAML page. YAML is the configuration markup language Azure DevOps uses to configure tasks for its pipelines. You can find out more about YAML and the schema that Azure DevOps uses at the following <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema,parameter-schema">URL</a>.</p>
<p>We will need to modify our YAML page so that we can add our App Center Test Task. It requires a bit of setup to be able to run successfully. We’ll do that next.</p>
<p>We will click on the pipelines link on the left. That will take us to a list of builds that we’ve run previously. We can click on any one of these, since we will be creating a new branch for our edited pipeline. Then we will click the edit button in the top-right, which will bring up the YAML configuration page for our default master branch.</p>
<p>Now that we have our YAML page up, we can begin editing it. Because we are adding a variable group to the variable section, we need to be a bit more specific in how we define our other variables. So we need to add the name and value properties to the variable declarations.</p>
<p>These variables are buildConfiguration, which is set to Debug, and outputDirectory, which is set to our bin directory, using the built-in build variable, build.binariesDirectory. Wow, that was a mouthful. Then a slash and our buildConfiguration variable that we just said. This is the standard path Visual Studio uses when compiling projects. By using this built-in build variable, it points our outputDirectory variable to the correct path for the project we’re building. That way, we only need to set it once for both projects. We can now add our variable group. YAML is very particular in how the commands are laid out in regards to spacing and how things are defined.</p>
<p>After we’ve added our variable group, we need to modify the NuGet Command to specify our directory to put our NuGet packages into. One of the challenges of implementing an App Center Test Task is that Xamarin projects no longer use a NuGet.config file, and it no longer copies the NuGet packages into a packages directory in the project by default. So we need to force the command to do that, so we know where to find the cloud-test.exe file that App Center needs in order to upload and run our Xamarin.UITest project.</p>
<p>The hosted build agent, however, does use the Nuget.config file. And we can fix this by adding the restoreDirectory property to the NuGet Command task and pointing it to our main project folder slash packages. We can do this using the built-in build.sourcesDirectory. This variable always points to the current project’s main directory.</p>
<p>The next thing we need to do is to create separate build tasks for each of our projects. By default, the main Xamarin.Android task tries to build all projects within our solution, assuming that they are all Android applications. This will not work for our test project. So we need to enter the name of our specific project file for our Android main project. We will keep our output directory and configuration the same, since they are already pointing to the variables we set up. Although not completely necessary, I like to specify the Java Developers Kit version here, since I have multiple versions of this STK on my computer. JDK Version sets this to the most current version.</p>
<p>The next task we want to prepare is signing our build so that we can test our application on real devices. So if we go over here to the right, and at the top, we see that there’s an Android signing task. When I click this, it shows the configuration page for this task.</p>
<p>The first thing you need to do is enter the path to the APK file that’s created from our previous build task. That value is stored in our outputDirectory variable. So we can just use that here rather than hard-coding a path. Hard-coding a path can be very brittle and can cause the build to break if it changes.</p>
<p>Next, we need to make sure the Sign the APK checkbox is checked, and for the Keystore file, we just need to enter the name of the file we uploaded to the secure files area of the pipeline library. We will likely need to authorize this later when we go to run the build. For the Keystore password, alias, and key password, we just need to enter the variables that we previously added to our variable group in the library. Doing this prevents us from needing to add them here as plain text and storing it in our repository.</p>
<p>There’s a few additional properties here, apksigner is used if you wanna use a different signing library for our applications. The Zipalign checkbox is used when we wanna conserve RAM. And the last property is our location for a custom application for Zipalign. We can just use the defaults that are part of the Android SDK and click the Add button to add our task.</p>
<p>Our next task is to build our test project. We need to do this because the binaries need to be uploaded to the App Center for our final App Center Test Task. I’ll just scroll down to the Xamarin.Android task and click it to bring up the configuration page. The last time we modified the YAML directly, this time we’ll configure a new task and add it to our YAML page. I could just copy the Xamarin.Android task, but I wanted to show you both ways to add a task.</p>
<p>First, we enter our project file name. Next, we have the target property. This is why we need to use a separate task for building this project. When you build an Android application, it expects a target. And since this is not an Android application, we will not have a target, causing our build to fail. So we need to set the prepare package property here to false so that it doesn’t look for a value in the target property and cause an error.</p>
<p>Next, we put our outputDirectory variable into our output directory property, and our buildConfiguration variable into our configuration property. Again, we could change our JDK version here and as you can see, there’s multiple options we can choose, but we’ll just use the default. And now that we’ve configured our task, we just click add.</p>
<p>Now to the main topic of this demo. The rest was all done to prepare our build process to be able to run our App Center Test Task. We can just click on the task, since it’s at the top of the list on the right here. And as usual, we get our task configuration page. First thing we wanna do is specify the path to our APK file that was created when we built our main Android project. We just need to add our outputDirectory variable, and a slash, and our APK file name. What we could have done previously is create a variable to hold the name of the APK file or the full path. That would have saved us some keystrokes.</p>
<p>We wanna make sure that the checkbox for Prepare Tests is checked, because if we miss this, when the App Center tries to run the UI test, there won’t be any binary files in the output directory. The Artifacts directory has already been set up for us and points to the App Center staging directory. We have no access or control over this directory on App Center. However, we could set up our own Artifact’s directory in either Azure DevOps, or if we were using a self-hosted build agent, we could specify one on the agent machine. I explained build agents in mode depth in my course on implementing and managing Azure build infrastructure here on cloudacademy.com. Be sure to check that out, if you wanna know more about managing your build infrastructure in Azure DevOps.</p>
<p>The next property we need to set is the testing framework we’re using. This is Xamarin.UITest. Now, we set the path to our build directory. In our case, we can use our outputDirectory variable. Now, we could have added our Keystore file and signing credentials here, but I like to keep this separate. It makes the task smaller and easier for us to change later on without potentially impacting our testing task. It’s just a matter of personal preference, you’re free to configure it here without any additional Android signing tasks.</p>
<p>At this point, we come to the part that can make this task in the App Center CLI command challenging to get working properly. This property here is the reason we added the restoreDirectory property to the new command previously. Since Xamarin.Android project’s default NuGet package style has migrated to using NuGet as a PackageReference, it no longer copies the packages into a package folder in your solution. This can make it very difficult to find the cloud-test EXE file consistently, as it will be in a different location for each developer on your team and for each version of the package.</p>
<p>We need to enter the path to our tools directory. That will be the built-in build.sourcesDirectory environment variable, &#x2F;packages&#x2F;Xamarin.UITest&#x2F;3.0.3, which is the version of our Xamarin.UITest NuGet package. And then a &#x2F;tools.</p>
<p>For Xamarin.Android projects, I highly recommend that you add the file to the project as a resource and check it into your code repository so that it’s always in the same location for everyone. This just makes it easier to find when testing your project either in App Center CLI or Azure DevOps. Just remember to update your file, if you change the version of the Xamarin.UITest NuGet package.</p>
<p>Next, we select our Visual Studio App Center service connection, and then the App Center Slug property is the App Center’s way of identifying which application we’re trying to test. It consists of the project owner, in this case our organization, and then a forward slash and our app name in App Center. This might not be the same name as our project. Our devices property value, depending on if we are using a saved device set or a random identifier created by App Center is the identifier. Or if we have an organization, it’s our organization name, a forward slash, and then the name of our device set, which is master. Our test series is also named master. The dSYM directory is for iOS, and we’re not gonna be using it. Our language is set to English United States, and this will set our locale to en_US.</p>
<p>Now we can click add and have a look at our task in YAML. As we can see, all of our settings have been configured. Although the Prepare tests property is set to true by default, I like to add it here to make sure that it’s clear. If it’s set to false, then the Test Task will fail because there’s not any binary files for the App Center to use to run our tests.</p>
<p>Now we can click save, and Azure DevOps will ask us if we wanna update our existing branch or create a new one. I’m gonna create a new one here and add a commit message. I’m not gonna create a pull request for this change, since I’d be the one that has to approve the change request anyway. Now, we will click run and see what happens. The build process takes about 17 or 18 minutes, so I’m gonna speed this up until we get to the end of the App Center Test Task. We can see from the build log, it ran successfully. And we can now go into the App Center and have a look at the test results. We can see that all the tests ran successfully and we have our report in App Center.</p>
<p>In this demo, we integrated Azure DevOps with our App Center testing by adding an App Center Test Task to an Azure DevOps pipeline. This runs our project unit tests against the device set we created in the App Center. Having the ability to use Azure DevOps to run our App Center UI tests certainly makes it much easier to implement a mobile DevOps strategy. It is just a matter of adding a few additional tasks to our CI&#x2F;CD pipeline, and we have a completely automated build process for our mobile applications.</p>
<p>In our next module, we will talk about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/distribution-groups-introduction/">distribution groups</a> and how to get your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">mobile application</a> to your testers and your users. I hope to see you there.</p>
<h1 id="Distribution-Groups"><a href="#Distribution-Groups" class="headerlink" title="Distribution Groups"></a>Distribution Groups</h1><p>Once an application has been built, regardless of how you build your application, App Center can distribute your release to a target audience.</p>
<p>A distribution group is a group of users that are managed together. This gives you the ability to manage access and releases on a group level. This can reduce the management headache of trying to get your application out to each of your users individually. Distribution groups can be public or private, and the App Center can also distribute your application to the various app stores for sale. Distribution groups are private by default.</p>
<p>Private distribution groups contain users that are invited via email, and are given access to releases that are set for that distribution group. Users in private distribution groups will be notified via email whenever a new release is made available to the group. They are required to have an App Center account, and log in to their account to access these releases.</p>
<p>Public distribution groups allow unauthenticated users to access releases from publicly available links. Once a group has been named, it can be set public or private, and members of a public group will receive a notification of a new release just like members of a private distribution group. They are not, however, required to log in to the App Center to download the release, and anyone that has the link to the application can download and install your application. Tread carefully here.</p>
<p>You can create distribution groups for many types of users. Testers, beta testers, pre-release users, pre-order users, the list goes on and on. And distribution groups gives you an easy way to manage these users.</p>
<p>Organizations in App Center are used to group applications and users into a container. This allows the ability to create a shared distribution group, and, if connected to an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> subscription, allows you to use Active Directory to add and manage users. This can greatly simplify access permissions and release management if your users all need the same access permissions.</p>
<p>For example, it’s highly likely that an organization will write multiple <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">mobile applications</a> that will be tested by the same group of testers. A shared distribution group is a quick and easy solution to this problem. Shared distribution groups are created at the organization level, rather than the application level, and shared groups can be either public or private. Shared distribution groups can access all applications that are part of the organization.</p>
<p>You can add users to a shared distribution group via email, or, if your organization is linked to an Azure subscription, by adding an Active Directory user or group. To add users from Active Directory, you must first link your Azure subscription to your organization. And then, add the Azure Active Directory tenant to the organization in App Center. Once the tenant is connected, you can add Active Directory groups and users to your App Center distribution groups.</p>
<p>Distribution groups are a simple but powerful tool for getting your mobile application out to the people that need it, and provides an easy way to notify your users of new releases and updates to your mobile applications. So let’s go check out the next module that demonstrates how to add <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/demo-distribution-groups/">distribution groups</a> to our mobile DevOps strategy.</p>
<h1 id="DEMO-Distribution-Groups"><a href="#DEMO-Distribution-Groups" class="headerlink" title="DEMO: Distribution Groups"></a>DEMO: Distribution Groups</h1><p>For this Demonstration, we are going to look at how to release our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">mobile application</a> to our users. First, we will create a public distribution group, then we will create a private distribution group. Then we will configure our master branch to release a build to one of our groups every time new code is committed to our master branch. Users come in many types and Distribution groups allow us to manage and deploy our application to our users.</p>
<p>To help me to explain the concept of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/distribution-groups-introduction/">distribution groups</a>, I am going to break them down into 3 broad types. There are obviously many more types of users but for the purposes of this explanation 3 types will be enough to get the point across. The definition of the 3 types are:</p>
<ul>
<li>Internal Developers: These are users that actively develop mobile applications within the company and are part of the dev teams that contribute directly to the application. Generally, these users would be added to the default collaborators distribution group.</li>
<li>UAT Testers: these testers perform User Acceptance Testing as part of an agile process and provide feedback and bug reporting. They are most likely company employees and have accounts in App Center for the purpose of manually testing the application before releasing an application out into the wild.</li>
<li>Finally, End Users: These are the users that will possibly be purchasing your application from the various App Stores, or downloading it from an Intune company portal.</li>
</ul>
<p>Mobile development generally speaking must go through a more rigorous testing process. This is because App Stores, like Google Play and Apple iTunes have strong testing requirements and there is a cost to releasing a mobile application to an App Store, if the mobile app fails the App Store quality check, the cost of the application is forfeit. Repeated failures can get really costly.</p>
<p>Also with mobile applications, it’s not an accepted practice to report errors directly to a user, being able to recover from errors without user input or knowledge is the accepted standard for mobile applications. So rigorous multi-level testing is required to meet the standards for being released successfully to an App Store.</p>
<p>This is why there is a need to have testing and feedback from multiple groups of users. Mobile application development is a costly endeavor so making every effort to release the highest quality application possible is even more important when creating mobile applications. Having multiple user groups that fill different testing roles is a great way to achieve this goal. Now that we have an idea of some of the reasons why having multiple distribution groups is beneficial, let’s have a look at how we can create and use them.</p>
<p>We’ll start off on the distribution groups page, and since we don’t have any distribution groups yet, there is a button there for us to add a group. App Center creates a new collaborators group whenever a new app is created. This is a good place to add our developers.</p>
<p>I will just click the add group button and this will bring up the configuration page for creating our new group, the first one will be a public group named “Beta” with just one person in it. Next, I will add another group this time we will make it private, and add 2 people to the group, That is all there is to creating distribution groups. If we had an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Active Directory Tenant, we would be able to add any users from that tenant in the add users’ textbox.</p>
<p>Now that we have added our distribution groups, we can specify that whenever a specific branch in our code repository is built it will automatically create a release to distribute to one of our groups. So I will just go over to where the builds are and select our master branch and click the edit settings in the top right. I will scroll down to the Distribute option and select one of the groups we created, we can also do this with the different app stores as well. Distributing to Google Play, Apple iTunes, or Microsoft Intune App Stores requires previous setup.</p>
<p>App Center is very easy to use, considering how challenging it can be sometimes to write mobile applications. Distribution groups give us a very simple way to release our application to our testers and users with very little setup needed.</p>
<p>Now that we have seen how to create a distribution group and trigger a release in App Center, let’s go check out how we can create an App Center Distribute Task in our Azure DevOps pipeline. In our previous demo, we created a task that runs our unit tests for our mobile application. Now we want to release our app to our testers.</p>
<p>In App Center, we created a very simple build and release CI&#x2F;CD automation process. We did this by creating a group and configuring our build so that every time new code was committed, If the build was successful for that specific branch it would create a release for the specific distribution group we set up.</p>
<p>In <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/az-400-exam-prep-microsoft-azure-devops-solutions-1-1368/">Azure DevOps</a> and YAML there are many more things we can do. We can make releases conditional on build step success or add check gates for approvals that need to happen before we decide to release, just to name a few. Let’s check out how to release a build to one of our distribution groups from Azure DevOps.</p>
<p>We have here our previous YAML page from our demo on testing using Azure DevOps pipeline integration. All I have done is comment out our App Center test task, since that takes about 18 minutes to complete, and for this example distribution, we don’t require a successful UI test task, but if we wanted to provide a success condition for that task it is an option.</p>
<p>So the first thing we do is click on the App Center Distribute Task on the right. This will bring up the usual task configuration page for us to fill out. First, we need to select the service connection we created. Paste in our App Slug, it’s the same as our test task, our binary path to the .apk file is also the same so I will cut and paste that as well.</p>
<p>Build version is for windows applications so we don’t need to do anything with that. We will use the default “Enter Release Notes” for the release notes type option. This allows us to enter a release note manually, but we could also have used a previously created release notes document if wanted to.</p>
<p>Now we come to the decision about what group we want this task to release to, so we will come back to App Center and go to the distribute tab and then groups. I previously deleted our user groups so we would have a clean environment to work with, which is why you do not see any groups from the previous demo.</p>
<p>Since we have no groups we will click the “Add group”, this brings up the configuration page, we simply name our group and add the email of any users we would like to add to our new Tester group. I will add my email address here. This will add me to the group and I will be notified when there is a new release. Now we just click create group, and we now have a new Testers group.</p>
<p>Now to be able to identify our group in Azure DevOps we need the group id. So we need to go back into our group settings by clicking on the wrench up in the top right corner and that will open our group configuration page again, and you will see in light grey an ID number. It’s a bit hard to see. We will copy it to the clipboard and head back to Azure DevOps.</p>
<p>When we are asked for a Distribution Id we just paste the id from the App Center there. Lastly, we change our symbols to Android, and that is all we need to do for this task. Click add and we see it on our YAML Page. We then click save, and then click run, and click run again. The pipeline will then go through all of our YAML tasks except for the commented out UI test task, when it’s done we should have an email letting us know there is a new release available.</p>
<p>We can also do this for the various app stores and the intune company portal. We won’t be able to go too far with deploying to the Google Play store or the Microsoft Intune company portal. However, you can find them under the Stores menu items here in App Center. For The Google Play store, you need to set up your build signing and the Google Play API to enable 3rd party deployments, and for the Intune company portal, your organization has to have the Enterprise Mobility and Security Suite from Microsoft and you also will need an account for Intune. For Apple, you need to set up your developer profile and production certificate. However, that is beyond the scope of this course.</p>
<p>If you need information on how you can do this you can find the information at the following URLs:</p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/appcenter/distribution/stores/apple">https://docs.microsoft.com/en-us/appcenter/distribution/stores/apple</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/appcenter/distribution/stores/googleplay">https://docs.microsoft.com/en-us/appcenter/distribution/stores/googleplay</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/mem/intune/fundamentals/what-is-intune">https://docs.microsoft.com/en-us/mem/intune/fundamentals/what-is-intune</a></p>
<p>So in this demonstration, we’ve learned how to create public and private distribution groups and where we can go to set up our connection to the various app stores. In the next module, we will take a look at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/analytics-and-diagnostics/">Analytics and Diagnostics</a>.</p>
<h1 id="Analytics-and-Diagnostics"><a href="#Analytics-and-Diagnostics" class="headerlink" title="Analytics and Diagnostics"></a>Analytics and Diagnostics</h1><p>With all software application development, there will inevitably be errors and bugs, and mobile development is no different. However, bug reporting and understanding the application state on a device you don’t own can be much more difficult. Luckily, App Center has a few APIs that you can add to your application that allows you to gather analytical data and perform event and error logging to App Center itself.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">Mobile applications</a> differ in scale and architecture from your typical client-server or web application. These types of applications have central processing servers that you will have complete control over, and can write to event tracking systems and create your own log files. These files can be as large or as small as you’d like, and this gives you a lot of freedom to do whatever you need to do.</p>
<p>The same cannot be said for mobile applications. Mobile apps are more like off-the-shelf software packages installed on computers you don’t own. You can still create log files and do event reporting, but you don’t have ready access to these files or systems.</p>
<p>The limitations and restrictions for mobile applications is far stronger for a few reasons. The respective app stores have rules that must be followed, and so the data that you’re allowed to collect is restricted. Further, the storage space on mobile devices is at a premium so you can’t simply create large log files on these devices.</p>
<p>App Center solves both of these issues for mobile application development. When you distribute your mobile applications through the App Center, they are tracked and monitored. App Center Analytics will give you data on how many current installs are out in the wild. It can also tell the difference between unique installs and re-installs. It gives you statistics for daily usage, length of sessions, device specs, app version information, location, and language data. That’s a lotta great information without any extra work for the developers.</p>
<p>As a developer myself, I’ve seen many challenges in regards to error handling and logging. Everyone seems to have their own way of doing things, and every developer has his own favorite tools for performing these tasks. In my experience, error handling in a consistent manner across multiple development teams is challenging and anything but consistent. The event logging and error handling in App Center gives you a simple and concise set of APIs to accomplish these tasks, while at the same time allowing for a fair amount of flexibility in the data you can track.</p>
<p>In our next module, I’ll show you how to add the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/demo-analytics-and-diagnostics/">App Center analytics and diagnostics</a> to your mobile application, and how to use some of the simple methods to get you started in event logging and error tracking to the log flow in App Center.</p>
<h1 id="DEMO-Analytics-and-Diagnostics"><a href="#DEMO-Analytics-and-Diagnostics" class="headerlink" title="DEMO: Analytics and Diagnostics"></a>DEMO: Analytics and Diagnostics</h1><p>In this demo, on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/analytics-and-diagnostics/">App Center Analytics and Diagnostics</a>, I will show you the steps you need to take to enable App Center, to track your application and implement basic event logging and error handling using the App Centers APIs.</p>
<p>I’m gonna show you how to set up an Android Xamarin application. The process will differ depending on the operating system and platform you’re using to write your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">mobile application</a>. You can find the steps to accomplish this task for each of the platforms on the main overview page for the app you wish to create.</p>
<p>I already have the application open and ready in visual studio. So let’s go and take a look. If you’d like to follow along, you can clone the mobile application from a GitHub repository at the following <a target="_blank" rel="noopener" href="https://github.com/kelsosharp/MobileDevOps">URL</a>.</p>
<p>We will be modifying the main activity CS file. This is the file I have open now. The first step we need to take is to add the App Center Analytics and Apps Center Crashes NuGet packages. Both of these are dependent on the App Center NuGet package, and it will be installed as a dependency of either of the two other packages.</p>
<p>So to install the NuGet packages, I’ll right-click on our main DevOps project and select manage NuGet packages. You can also select the tools menu and the manage NuGet packages and then manage NuGet packages for solutions option if you prefer. After the package manager window has loaded, I’ll click the browse and then type in microsoft.appcenter and filter the list of packages down. First I will install the App Center analytics package, and then I’ll install the App Center crashes package. During the install, it will ask you to accept the package license agreement, and we can go ahead and click okay.</p>
<p>Once the packages are installed, we can go back to the main activity class and add the three using statements for App Center, App Center analytics, and Apps Center crashes to the using statement area.</p>
<p>Now that we have a reference to the App Center packages, we will need to start the analytics process thread. We can do this by using the App Center start method. We will need to use the App Center secret that you have on the main overview page of your app, in the App Center, you add this as a string to the first parameter of the start method. The second and third parameters are passing in the system type declaration for the analytics and crash classes.</p>
<p>We can now run this application on a device emulator, and it will show up on the main analytics page in App Center. Obviously there’s not going to be a lot of useful data at this point, but I’m sure that you can imagine what this will look like with hundreds or even thousands of devices using your application.</p>
<p>The next thing we’re gonna do is log in event to the log flow area in App Center. To do that, we just call it track event method on the analytic static object and pass in a string that has the message we want to add to the event log. So I’ll just add the appcenter.track event call with the message letting us know that the main activity class has been executed.</p>
<p>I’m adding this code to the onCreate method of the main activity class. This will execute just before the main screen is loaded on the device. This is a good place to add the start method we added earlier. And as for the rest of the code snippets that we will add today, these can be added virtually anywhere you would add event logging or error handling normally in an application.</p>
<p>We will just quickly check out the Log Flow area in the Analytics section and make sure our event was passed in the App Center. And we can see there that it shows up in our logs.</p>
<p>Now we will add a bit of error handling. App Center diagnostics will report, both handled and unhandled exceptions and provide you with a stack trace of the crash. If you need more information, you can create a dictionary with a key value pair of type string and add additional data to send the App Center.</p>
<p>In order to generate an exception for this demonstration, the crash aesthetic object of the App Center NuGet package has a method called generate test crash. This takes no arguments, but we’ll throw an exception for you.</p>
<p>We’re gonna use this method within a try-catch block to add some additional data that we can send back to App Center using the crashes track error method. The benefit here is that we can use this to test error handling without actually crashing the application on the device. It becomes a handled error and the application carries on like nothing’s wrong. And the user is not aware that there’s even an issue.</p>
<p>It will take a few minutes to show up in the apps in our log flow area, but handled and unhandled exceptions will show up this log under the analytics tab.</p>
<p>Now on the diagnostics page, we can see that we had four crashes and they happen in the onCreate method of the main activity class. When we click on the error in the list, we can see more detailed information, things like the number of reports, percentage of users this issue is affecting, along with devices and operating systems affected. This is awesome information to have when we’re trying to narrow down the exact cause of a bug.</p>
<p>To get even more detailed information on our crashes. We can have a look at the report tab. We can find demographics on the device, and this is also where we can find the custom data that we passed to App Center when we caught our error.</p>
<p>Additionally, if we wanted to, if the property’s dictionary is not sufficient for our needs, when handling errors. For example, we’re using a log file to gather additional information from the device itself and need to send it to the App Center. We can do this as an attachment, which will then be made available for us here.</p>
<p>App Center has provided us with an easily accessible way to get errors and crash data in a very simple and straightforward manner. I’m sure as time goes on, more and more features will be added to App Center to improve our mobile DevOps strategies, and App Center will become an invaluable tool in our DevOps utility belt.</p>
<h1 id="Course-Conclusion"><a href="#Course-Conclusion" class="headerlink" title="Course Conclusion"></a>Course Conclusion</h1><p>In this <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/course-introduction/">course</a>, we’ve taken a deeper look at the Visual Studio App Center as the central means to implementing a mobile DevOps strategy. We were able to create many of the tasks that are needed to create a fully automated CI&#x2F;CD pipeline for our mobile applications. And doing so has helped us to create a complete mobile DevOps strategy.</p>
<p>In the modules on build services, we looked at the various build configuration options, such as build signing and versioning. And in the demo on build services, we configured and signed our build for deployment to our distribution groups and the Google Play Store.</p>
<p>In the modules on UI testing, we talked about and demonstrated how to add a test series and test runs. We were able to use the App Center CLI and <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps to kick off our test runs, allowing us to test our application on our actual physical devices.</p>
<p>In the module on distribution groups, we were able to set up both public and private groups and invite users to those groups. We created and performed releases to these groups and we were able to automate builds from specific group branches to release our application to these groups.</p>
<p>We were able to use the App Center Distribute Task in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/az-400-exam-prep-microsoft-azure-devops-solutions-1-1368/">Azure DevOps</a> pipeline to distribute our application to one of our distribution groups. This, combined with the App Center test task, resulted in a complete CI&#x2F;CD implementation for our mobile application.</p>
<p>In the module on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/adding-mobile-devices-to-your-azure-devops-strategy-1047/demo-analytics-and-diagnostics/">analytics and diagnostics</a>, we talked about and implemented some basic logging and error handling, and we were able to send this data to the App Center. We were also able to track crash reports and learn that we can send additional error data to the App Center in the form of a list of key value pairs or file attachments. All of these options are available to give us a better understanding of the usage and issues in our application.</p>
<p>Mobile application development can be challenging and costly, but it doesn’t have to be. The App Center provides us with a simple way to implement a mobile DevOps strategy by giving us simple, effective tools to create a completely automated CI&#x2F;CD process. Not only that, it allows us to integrate with Azure DevOps. By combining these two service offerings, we get a much more powerful and flexible toolbox with which to create and customize our mobile DevOps strategies and automation processes.</p>
<h1 id="1Course-Introduction"><a href="#1Course-Introduction" class="headerlink" title="1Course Introduction"></a>1<strong>Course Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/course-introduction/">Course: Implementing and Managing Azure Build Infrastructure</a></p>
<h1 id="2Mobile-Build-Services"><a href="#2Mobile-Build-Services" class="headerlink" title="2Mobile Build Services"></a>2<strong>Mobile Build Services</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/appcenter/build/software">List of software</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops#software">VM and installed software info</a></p>
<h1 id="3DEMO-App-Center-Build-Services"><a href="#3DEMO-App-Center-Build-Services" class="headerlink" title="3DEMO: App Center Build Services"></a>3<strong>DEMO: App Center Build Services</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/kelsosharp/MobileDevOps">Clone mobile application</a></p>
<h1 id="4UI-Testing-in-App-Center"><a href="#4UI-Testing-in-App-Center" class="headerlink" title="4UI Testing in App Center"></a>4<strong>UI Testing in App Center</strong></h1><p><a target="_blank" rel="noopener" href="https://openapi.appcenter.ms/#/test">App Center API</a></p>
<h1 id="5DEMO-App-Center-CLI-UI-Testing"><a href="#5DEMO-App-Center-CLI-UI-Testing" class="headerlink" title="5DEMO: App Center CLI UI Testing"></a>5<strong>DEMO: App Center CLI UI Testing</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/appcenter/test-cloud/devices">List of devices available</a></p>
<h1 id="6DEMO-App-Center-Azure-DevOps-Integration"><a href="#6DEMO-App-Center-Azure-DevOps-Integration" class="headerlink" title="6DEMO: App Center Azure DevOps Integration"></a>6<strong>DEMO: App Center Azure DevOps Integration</strong></h1><p><a target="_blank" rel="noopener" href="https://developer.android.com/studio/publish/app-signing.html">How to sign your Android builds</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops&tabs=schema,parameter-schema">YAML and Azure DevOps schema</a></p>
<h1 id="8DEMO-Distribution-Groups"><a href="#8DEMO-Distribution-Groups" class="headerlink" title="8DEMO: Distribution Groups"></a>8<strong>DEMO: Distribution Groups</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/appcenter/distribution/stores/apple">Apple setup</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/appcenter/distribution/stores/googleplay">Google Play setup</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/mem/intune/fundamentals/what-is-intune">Microsoft Intune setup</a></p>
<h1 id="10DEMO-Analytics-and-Diagnostics"><a href="#10DEMO-Analytics-and-Diagnostics" class="headerlink" title="10DEMO: Analytics and Diagnostics"></a>10<strong>DEMO: Analytics and Diagnostics</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/kelsosharp/MobileDevOps">Clone mobile app GitHub</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/kelsosharp/MobileDevOps">Clone mobile app GitHub</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Azure-Key-Vault-and-Disk-Encryption-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Azure-Key-Vault-and-Disk-Encryption-6/" class="post-title-link" itemprop="url">AZ-400-Azure-Key-Vault-and-Disk-Encryption-6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:36" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:36-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-24 00:53:30" itemprop="dateModified" datetime="2022-11-24T00:53:30-04:00">2022-11-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Azure-Key-Vault-and-Disk-Encryption-6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Azure-Key-Vault-and-Disk-Encryption-6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AZ-400-Implementing-and-Managing-Azure-Build-Infrastructure-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AZ-400-Implementing-and-Managing-Azure-Build-Infrastructure-5/" class="post-title-link" itemprop="url">AZ-400-Implementing-and-Managing-Azure-Build-Infrastructure-5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 20:37:35" itemprop="dateCreated datePublished" datetime="2022-11-18T20:37:35-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 01:25:08" itemprop="dateModified" datetime="2022-11-27T01:25:08-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-400/" itemprop="url" rel="index"><span itemprop="name">AZ-400</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AZ-400-Implementing-and-Managing-Azure-Build-Infrastructure-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AZ-400-Implementing-and-Managing-Azure-Build-Infrastructure-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Hi, my name is Kelso Sharp, thanks for checking out my course. I’ve been writing web applications for the past 20 years. I’m also a Microsoft Certified Professional and an Azure Cloud Architect. I have been working with the Microsoft web development stack since 1996.</p>
<p>In this course, we’re gonna be diving into implementing and managing build infrastructure using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-pipelines/">Azure pipelines</a>. If you run into any issues during the course please feel free to email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> and the team here at Cloud Academy will do our best to get your issue sorted out as quickly as possible.</p>
<p>This course is intended for IT professionals looking to become certified DevOps professionals, IT professionals from other cloud providers looking to get a better understanding of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>‘s way of doing things, and pretty much anyone that wants to get a better understanding of the Azure build infrastructure processes.</p>
<p>It’s expected that you have some understanding of the software development process, maybe you’ve played with DevOps a little bit and would like to learn more. I have the expectation that you understand what DevOps is and understand the relevant terminology. While you don’t need to have knowledge of any particular programming language I recommend that you have at least a high-level understanding of the software development process. It would also be prudent if you had knowledge of Git and some knowledge of the agile software development methodology.</p>
<p>We will be covering the course objectives with more of an eye towards the technical side so more on the practical side as opposed to theory. I will be showing you the various build options. While there will be some theory in this course, most of it will be explaining and demonstrating how to implement our build infrastructure.</p>
<p>First, we will look at what an Azure build pipeline is, and why you might wanna use it and then we’ll learn how to create a pipeline, and add tasks to it. This is gonna give us a first look at the Azure DevOps user interface. Then we will move on to build agents, what they are, what the differences between the different agents are and agent pools, that they are, what they do and why we use them. And we’ll look at third party build server integration with Jenkins, third party code repos like GitHub.</p>
<p>By the end of this course, you should be comfortable with building an Azure DevOps pipeline, adding automation tasks, using the visual designer for adding build tasks. You should be able to create a pipeline that closely matches an existing build process you work with and you should be able to determine a pipeline strategy based on the information given to you during a discovery process.</p>
<p>Alright, that’s a lot to cover, so let’s get rolling!</p>
<h1 id="Azure-Pipelines"><a href="#Azure-Pipelines" class="headerlink" title="Azure Pipelines"></a>Azure Pipelines</h1><p>In this lecture, we will be talking about Azure Pipelines. Azure Pipelines is Microsoft’s cloud service offering for creating Continuous Integration and Continuous Delivery.</p>
<p>In this section, we will be talking about what Azure Pipelines are and what they do, and then we will go through a demo of how to create an Azure Pipeline.</p>
<p>What are Azure Pipelines? Well, we cannot talk about Azure Pipelines without first having a clear understanding of what Continuous Integration and Continuous Delivery CI&#x2F;CD for short are.</p>
<p>The concept of Continuous Integration is constantly and frequently integrating code updates to a shared repository for testing and use by other developers and processes. Code committed to this shared repository is verified by an automated build process and additional tasks allowing individual pieces of code to be checked for problems that could introduce breaking changes or potential bugs into the application. Typically code that breaks the build is prevented from being committed to a shared repository.</p>
<p>The concept of Continuous Delivery is the process of handing off code to the quality assurance team for testing as part of an agile process. Testing at this stage is to detect bugs that are logical in nature and while the syntax of the code committed may not break the build, it may not execute as expected or may not meet the acceptance criteria for resolving the problem. Testing earlier in the software development process is done in hopes of detecting and fixing bugs before they make it to production. This process typically involves deploying to a staging area and generally does not add direct value to the user, but it is a necessary step in the agile software methodology.</p>
<p>As an aside, the concept of continuous deployment often gets confused with continuous delivery. In this automated process of deploying code, this code must again pass a series of automated tests and manual requirements to be accepted as production-ready and deployable. The continuous delivery and continuous deployment terms are often confused with each other. However, they are not the same thing and comprise two parts of a larger DevOps whole.</p>
<p>I find the best way to visualize a build pipeline is like an assembly line in an auto factory. The raw materials in our case code starts at the beginning of the process and at the end, you have a drivable vehicle. For us, a testable piece of code. At each station, along the way, a new piece of functionality or modification is added. For us, we call these tasks and once all the stations or tasks have done their work, we should have a testable piece of code that can be sent to the QA department.</p>
<p>Azure pipelines work with many different languages like C#, C++, Python, Go, and many more. One requirement of using Azure Pipelines or any CI&#x2F;CD automation tool is having a code base stored in a version control repository accessible by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>. You must set up both an organization and a project in Azure DevOps. Projects can either be publicly accessible or private. This has a significant impact on the pricing model used for creating and running Azure Pipelines.</p>
<p>Azure Pipelines also works with many various version control systems like Azure Repos, Team Foundation version control and GitHub, which we will be talking about later in this course. You can build and deploy almost any type of application. Some examples are .NET, Node JS, Java, even Xcode.</p>
<p>It’s also possible to deploy your code to multiple deployment targets such as virtual machines, on-premise servers or Azure-hosted environments. You can even deploy to other cloud service providers like AWS or GCP.</p>
<p>You can choose from many types of deployment packages like NuGet, npm, zip files and others. There’s an amazing amount of versatility within the Azure DevOps service offering from Microsoft and Azure Pipelines are the foundation upon which the build process automation are built.</p>
<p>As I mentioned during the intro section, your project accessibility has a large impact on the pricing model used by Azure DevOps pipelines. If you have a publicly accessible project in a public repository like GitHub, then Azure Pipelines are free. Free is a nice option, but having your code visible to the world might not work for you. If you have proprietary systems that should be protected or if youR code security is an issue for you, then this is not likely an option for you. Luckily, Microsoft provides 1800 minutes or 30 hours of build time for pipelines in private projects. This is a lot of time, but if you have exceptionally large projects that take many hours to build, then managing your build hours can be very important. Luckily, you can disable your pipeline to prevent unintentionally using build minutes. Later in this course, we’ll be talking about private hosted build agents. This is another option for reducing build hours used in Azure Pipelines.</p>
<p>So now we know what Azure Pipelines are, why do we wanna use continuous integration and continuous delivery and Azure Pipelines? Some of the major reasons for using continuous integration are increasing and maintaining code quality, catching potential errors earlier in the development process, ensuring code testing and code test coverage is maintained and maintaining consistent code style across entire applications. </p>
<p>Major reasons for using continuous delivery, reducing and removing time-consuming and error-prone manual processes, keeping your development staging and production environments up to date, ensuring that your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/course-introduction/">infrastructure</a> is secure and properly maintained, automating integration and regression testing. These are just some of the reasons that you might wanna use CI and CD.</p>
<p>All right, now we know what Azure Pipelines are and what we use them for. Next, we’ll go through a demo and we’ll create our own <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-pipelines-demo/">Azure Pipeline</a>.</p>
<h1 id="Azure-Pipelines-Demo"><a href="#Azure-Pipelines-Demo" class="headerlink" title="Azure Pipelines Demo"></a>Azure Pipelines Demo</h1><p>In this demo, we create our first <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-pipelines/">Azure DevOps pipeline</a>. Pipelines exist inside of a project and projects are part of an organization. So we’re gonna create all three and connect our project with a code repository in GitHub. I have created a sample project you can use in GitHub, and we will discuss how you can use that in your own project in a few minutes.</p>
<p>The first thing we need to do is open our browser to dev.azure.com. Now, if you do not have an Azure DevOps account, you can click on the Start free button and create one or if you already have a GitHub account, you can click on the Start free with GitHub button and use your GitHub credentials to sign up for a DevOps account.</p>
<p>Once we have an Azure DevOps account, and we’ve signed in, we can click on the sign into Azure DevOps link, and it will take us to our organization page. You can have multiple organizations and here’s where we’re gonna create our new organization for our project.</p>
<p>We will click the new organization link here on the left-hand side, this will bring up the Getting Started with Azure DevOps dialog. At this point, Microsoft is indicating that if we continue past this point, we are accepting the terms of service for Azure DevOps. It also has a checkbox here if you want Microsoft to send you more information on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>. So let’s just go ahead and click the Continue button.</p>
<p>You will be asked to provide a name for your organization. Keep in mind here that your name must be unique across all of Azure DevOps. I’m just gonna use the one that they’ve provided for me here, this will be appended to dev.azure.com to form the access URL for your organization. We will also need to select the region where we’d like our organization to be hosted. So we will select a region and click Continue.</p>
<p>This brings us to the main page for our organization. This page has a form for creating your first project on the right-hand side. We’re just going to enter a project name for our new project. I just wanna mention here again how private and public projects differ. If you choose a private project here, your code will be kept private, and no one will be able to see or contribute to your project, unless you expressly give them permission by adding them to your organization and to the project user group. Make sure you exercise caution here, if you’re not sure, choose private, you can always make it public later. So in my case, I have a sample code repository set as public so that I can share it with you. So I’m gonna choose public.</p>
<p>At this point, let’s talk briefly about the sample project in GitHub, I’ve created a GitHub repo at the following <a target="_blank" rel="noopener" href="https://github.com/kelsosharp/Managing-and-Implementing-Build-Infrastructure">URL</a>. This is a basic .net core solution with a main project and a test project. It is a basic MVC project template and the test project just has a couple of working unit tests. The goal with this project is not to show you how to write code, but it’s just so that we have a code project for our pipeline to build. All you need to do at this point is go to the sample project at the URL I gave you and fork this project to your own GitHub account by clicking the fork button in the top right of the repository page.</p>
<p>Okay, now we have our organization, our DevOps project, and our .net core solution, we can now go ahead and create our new pipeline. On our project page, let’s click our pipeline menu option. And since we do not already have a pipeline yet in our project, a page is gonna show up with a button on it for us to click to start configuring our first pipeline.</p>
<p>It’s going to ask us where our code is, this is where we select our repository type provider. There are a few options here like the Bitbucket or Subversion. You can even select a generic git repository provider if you have your own on-premise git server. In our case, however, we’re using GitHub, so let’s just go ahead and click on the GitHub selection.</p>
<p>Now it will ask us to provide our GitHub credentials and sign in to GitHub. If you forked the sample repository to your own account, you should see your repo in this list, go ahead and click this repository.</p>
<p>Although not shown here in the demo, this action will likely bring you to a repository authorization page for your code repo. And it’s gonna ask you to give Azure DevOps access to the selected repos, you can go ahead and grant access for the Select repos and this will bring you back to the Azure DevOps page. And it will now ask us what type of pipeline we’d like to configure.</p>
<p>As you can see, there are a ton of choices here, from .net core, Node.js, you can configure for Android, you can even use XCode workspaces where you can build and test MacOS applications. There’s something here for everyone and this list will continue to grow as Microsoft adds to its service offerings for Azure DevOps.</p>
<p>Since our solution is a .net core project, we will choose the .net core for our pipeline. This will create an initial pipeline with a few initial tasks. These are tasks that are common to most .net core solutions.</p>
<p>Now that our initial pipeline is created, what you’re looking at here is YAML. It’s the configuration language that Azure DevOps uses to configure the task that you will run whenever you commit changes in your software project to your GitHub repo.</p>
<p>So we have our initial pipeline configured, let’s save and run it and see what happens. Clicking Save and run it will ask us if we wanna commit our new YAML file to the master branch, or if we’d like to create a new branch. This means that our pipeline <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-pipelines/">build</a> configuration is also stored in our code repository, and we’re able to track changes to this file, just like any other code file in our project.</p>
<p>It will just take a minute or two to create the pipelines. So I’ll speed up the time here and then we’ll click into the jobs and see it as it runs.</p>
<p>While our job is running, I’ll explain what is happening in our basic pipeline. First, it initializes the job. If we have a look at the job initialization task, we see that it’s using a hosted agent. This is a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-build-agents/">build agent</a> that is supplied by Microsoft to run our build tasks. In the module on build agents, we will look at what a build agent is in a bit more depth, and have a look at the differences between private and hosted agents like this one. Then the job is gonna download all of our tasks to the agent machine, it’s going to check out our code from our code repository. After that, it will install our NuGet package manager, then it restores all of our NuGet packages, builds our project and runs our unit tests. It then cleans up after itself by removing the repository credentials and tearing down the virtual machine that the build agent is using. This is an important step. We don’t want our code out on a shared agent machine. Finally, if we have any reporting tools configured for our tests, it will generate the tests or code coverage reports and drop them into our artifacts section.</p>
<p>Now at the time of this recording, the initial unit testing tasks that’s created on the default pipeline doesn’t actually run your unit tests. Microsoft is in the process of making updates to Azure DevOps and there’s a known issue with this particular task. This task is trying to use an older version of MSBuild, and unfortunately, it cannot actually find the test in your project. When we look at YAML in a later module, I’m gonna show you how we’re gonna change this task to use the updated .net core command line build tools to run our unit tests.</p>
<p>Okay, there you have it, we’ve built our first Azure DevOps pipeline. Now we have it, what can we do with it? Well, you’re in luck, I’m gonna show you some additional things we can do with it in the next few modules.</p>
<h1 id="Azure-Build-Agents"><a href="#Azure-Build-Agents" class="headerlink" title="Azure Build Agents"></a>Azure Build Agents</h1><p>In this lecture, we are going to talk about build agents. A build agent is a piece of software that runs a series of build tasks called a job, on a machine. In <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-pipelines-demo/">Azure pipelines</a>, there are two types of build agents, Microsoft-hosted agent, and self-hosted agents. We are gonna go through a deeper explanation of build agents in this module.</p>
<p>A Microsoft-hosted agent are agents that Microsoft provides for running your build tasks, but there is more to a Microsoft Hosted agent than just a simple piece of software. You are also getting a new virtual machine that is spun up with the latest operating system, whether it’s Windows or Linux, or macOS, it will have the latest security updates, patches and maintenance updates without you needing to do anything at all. Microsoft-hosted agents can run a job directly on a virtual machine, or in a container, but for now the easiest way to understand a container is it’s just a toolbox with a very specific set of tools that you hand pick to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/course-introduction/">build your application</a>. In many cases for simple to moderately complex applications a simple Microsoft-hosted agent is more than enough to satisfy the needs of the project. Best of all, this is a free service up to 1,800 minutes a month, with a maximum build job length of six hours, for private projects and completely free for up to 10 parallel jobs for public projects.</p>
<p>Next, we have Self-Hosted Agents. A self-hosted agent is an agent that you install on your own machine whether that is a virtual machine in the cloud managed by you or an on-premise server that you also manage yourself, you are responsible for making sure that the operating system is kept up to date with the latest security patches and maintenance updates. Not only can you use self-hosted agents in Azure pipelines, but also in Team Foundation Server if you happen to have an on-premise TFS server. This agent can be installed on any machine you have access to, however that machine will also need to be able to access <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>, either through the internet or over a private express route connection.</p>
<p>As a first step, Microsoft recommends trying the Microsoft-hosted agent first to see if it does all that you need it to do for you, if not you can check the logs that are created in the build process to see where you might need more than what the Microsoft-hosted agent can offer. There are however a few guidelines that can tell you quickly if you need to host your own agents on your own machine.</p>
<p>The simplest factor is project size, the maximum project size is 10 gigs including build output. Microsoft Hosted agents use a Standard_DS2_v2 virtual machine image. This image has two virtual CPU’s seven Gigs of Ram, 14 Gigs of temporary SSD storage. This is just an average machine in most respects. If you need more information about this image, you can find it here at this <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-general#dsv2-series">URL</a>. If you need more powerful hardware, then self-hosted is likely the way to go.</p>
<p>Some of the other limitations of the Microsoft-hosted agents are that you cannot sign into the agent machine, you cannot drop artifacts to a UNC file share, and you cannot run XAML builds.</p>
<p>Microsoft-hosted agents also use a standardized list of operating systems and software configurations. There are eight options, four versions of Windows, two versions of Ubuntu, and two versions of macOS. You can find the list at this <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops">URL</a>.</p>
<p>So, if you need a version of an operating system or Visual Studio that’s not part of the standard list you will need to use a self-hosted build agent instead.</p>
<p>If only the version of the software installed on the agent machine is a concern and not the operating system itself, Microsoft-hosted agents also have the option of using either a Windows or Linux container and using your pipeline to install the version-specific software and tools you need to build your project.</p>
<p>There are however some unique requirements and limitations for each operating system. You can find the specific requirements and limitations of the container Jobs here at this <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/process/container-phases?view=azure-devops&tabs=yaml">URL</a>.</p>
<p>In this topic on build agents, I would be remiss if I did not discuss pricing and costs, since one of the main determining factors of the cost of Azure pipelines, is the length of time and concurrency of running our build jobs. As I mentioned before, pricing is affected by the visibility of your project. A free tier is limited to 1,800 minutes or 30 hours of build time per month, with a maximum job length of six hours and can only run a single job at a time. For public projects, the build minutes restriction is removed, the job length, however, is still six hours. In the free tier with a public project gives you 10 parallel jobs. Once you go above the free tier though you will need to pay for each parallel job, and buying the first parallel job for a private project does not actually allow you to run more than one job at a time, it simply removes the build minutes restriction and you will need to buy a second parallel job if you wanna run two parallel jobs at the same time.</p>
<p>This brings us to Agent pools. Instead of dealing with each agent individually, sometimes it makes more sense to manage a group of agents. Agent pools are just that, a way to manage groups of agents rather than each individual agent on its own. In Azure Pipelines the agent pools are configured in the organization and project settings depending on the user’s access permissions and these pools are accessible based on the level at which they are created. Therefore build agent machines are registered in the organization agent pool, and the organization level pool can be referenced by any project-level pool in a one to many mapping. Project pools, however, can only be referenced by one project at a time.</p>
<p>For example, let’s say we have an organization called Org1, and we create an agent pool called Org1 Agent Pool, and we add 2 build agent machines to this pool called Build Agent A and Build Agent B. Let’s assume that we have project X and Y. In project X, we create a project agent pool called Pool X, and we reference the Org Agent Pool. Project X would then be able to use both Build Agent A and Build Agent B in its build pipeline to build project X. Now let’s create an agent pool in project Y. called Pool Y at the project level. If we tried to reference another agent pool we would not be able to see the Pool X. We would only be able to see Org1 Agent Pool. If we choose not to reference an existing organization level agent pool when creating this new pool, a new pool would be created at the organization and project level named Pool Y and the new organization-level pool would not contain any build agents. With Azure DevOps Server, the new name for what was formally Team Foundation Server, agent pools are shared across the entire server so you can utilize agent pools across projects and collections.</p>
<p>The default agent pool type is for registering self-hosted agents. The Azure pipelines hosted pool can be used with any of the standard machine configurations from the Microsoft-hosted agent virtual machine images list. As with most things in Azure pipelines, agent pools can be managed in several ways, from the web UI, YAML, and from the Azure DevOps Command Line Interface.</p>
<p>In our next video, I will show you how to create a self-hosted agent, and how to create and configure an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-build-agents-and-pools-demo/">agent pool</a> for our project.</p>
<h1 id="Azure-Build-Agents-and-Pools-Demo"><a href="#Azure-Build-Agents-and-Pools-Demo" class="headerlink" title="Azure Build Agents and Pools Demo"></a>Azure Build Agents and Pools Demo</h1><p>In this demo, I’m gonna show you how to create organization and project level agent pools, and how to install and register <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-build-agents/">build agents</a> into our organization pool and how to reference organization pools from our projects.</p>
<p>Some things to know when using self-hosted windows agents is that it must be Windows 7 or higher, if you are using a client operating system, and if you are using Windows Server, then it must be Windows Server 2008 R2 SP1 or later, and you must have PowerShell 3.0 installed. It’s also recommended that you have installed Visual Studio 2015 or later, and the .net framework 4.6.2 or later.</p>
<p>One of the main reasons for using a self-hosted agent is machine hardware. For example, the code for Azure DevOps itself uses a pipeline with a 24-core server and four self-hosted agents on a single machine.</p>
<p>To prepare for this demo, I have created an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> virtual machine with Windows Server 2019 and Visual Studio Enterprise 2019. This is a Standard_D4_v3 image, it has four CPUs, and 16 gigs of RAM.</p>
<p>When creating agent pools, there are two levels of scope for an agent pool: One is at the organization level, and one is at that project level. When you create an agent pool at the project level, a matching agent pool is created at the organization level, so you’ll have two identical pools, one at the project level and one at the organization level. This is a one-to-one mapping.</p>
<p>The first thing we’re gonna do is sign into our organization page. Then, we’ll click the organization settings in the bottom left-hand corner. This will open our settings menu. Then, we will click on the agent pool menu option. This will open our agent pools page. This already has a pool named Azure Pipelines and one named Default. The Azure Pipelines is for Microsoft Hosted agents, and the Default is for self-hosted agents. If you do not mind the name of Default or Azure Pipelines, then you don’t need to change anything here. You can simply add Microsoft hosted agents or self-hosted agents to the various pools. We are going to add a new pool here and show how to reference the organizational level pool from the project level pools. There are two configuration options when creating an organizational level pool. The first, grant access permissions to all pipelines, if checked, will give all existing pipelines access to the newly created agent pool. The second is Auto-provision this agent pool in all projects. This option, if checked, will automatically add a matching project level pool to any new projects we create. I’m gonna leave these unchecked for now.</p>
<p>Next, we will create our project-level agent pool. We bring up our project, click on the project settings. Then we’ll click on the agent pool’s option, and that will bring up our agent pool’s page. They’re matching Azure Pipelines and Default pools on this page. This is at the project level. We’ll go ahead and click the Add button, and now we have the option for selecting a new or existing agent pool. Selecting the existing agent pool will give us a list of the organizational agent pools to choose from. We will select the existing option and select our Org1 Agent Pool. I’m gonna uncheck the Grant access to all pipelines, and click Create. This will create a project-level agent pool, that has a direct reference to our Organizational agent pool of the same name.</p>
<p>The next thing we need to do is to create a Personal Access Token, or PAT. This is how we are gonna authenticate the agent machines with our DevOps organization, and project. We will click on the person icon here in the top right, and bring up the profile menu. Then, we will select Personal Access Tokens. This will bring us to the page where we can add an additional token. Go ahead and click the New Token Button. And we’ll give our token a descriptive name. I’m gonna use Build Agent. Then, we need to set the permissions for the token here. We have the option of giving full access to the token or we can give very specific permissions. We can also set the expiration length here. It has the option to never expire. But I’m just gonna leave it for the default expiration of 30 days, and we only need to give Build, Read and Execute, and Agent Pools, Read and Manage, permissions to this token. To find Agent Pools, we’re gonna need to click the Show All Scopes. Once we have set everything set, click create, this will create a new Personal Access Token, and give us the option to copy the token. It’s very important to remember that this is the only time Azure will show you this token value. There is no way to retrieve the token value once you leave this page. You can edit the permissions later and you can regenerate the token, but you can never get this particular value back. So it’s a good idea to copy it and store it with your other secure passwords.</p>
<p>Now, we need to login to our build agent machine and open our browser to our organization DevOps page. After we sign into our organization, we click the organization settings. Then, we’ll click on the agent pools options and then we’ll click the new Org1 Agent Pool that we created and click on the Agents tab at the top. This will bring us to our Agents page with a button for adding a new agent. We will go ahead and click this button, and a dialog will pop-up with instructions on how to install an agent. It has the option to download the zip file for the agent software, or you can copy and paste a script, we are gonna download this file and run the installer. If you are comfortable with PowerShell and fully understand the script that is being run, then feel free to use that option.</p>
<p>Once we’ve downloaded the file, we will unzip it to its own directory. Once this is done, I am going to create a second build agent directory and copy all of the files to the new directory, so that I can create a second build agent on this machine. In the address bar of the Windows Explorer, we can just type CMD and that will open a Windows Command Prompt in our current directory. We will run the config.cmd file and this will ask us a series of questions about our organization and agent pools and authenticate our build machine using the Personal Access Token we created earlier. This will configure our agent, and send a list of capabilities to our organization. The first thing it needs to know is the access URL to our organization. I’m just gonna copy mine from the browser address bar. Then, it’s gonna ask us how we want to authenticate with our organization. We can choose PAT, Personal Access Token, by default by typing PAT or hitting enter. You can also use a windows username in the format of domain slash username, or <a href="mailto:&#x75;&#x73;&#x65;&#x72;&#110;&#97;&#109;&#101;&#x40;&#x64;&#111;&#x6d;&#x61;&#x69;&#110;&#x2e;&#x63;&#111;&#109;">&#x75;&#x73;&#x65;&#x72;&#110;&#97;&#109;&#101;&#x40;&#x64;&#111;&#x6d;&#x61;&#x69;&#110;&#x2e;&#x63;&#111;&#109;</a>. If you’re using this format, it will then ask you for a password for your account. We, however, will be using a Personal Access Token. Then, I will just paste my Personal Access Token here. It then authorizes the build agent machine with our organization and connects. Now, it’ll ask us which agent pool we’d like to use. You can just hit enter if you want to use the Default, a self-hosted agents pool, but in our case, we want to use the one we created, so I’ll just copy the name from our organization agent pool page.</p>
<p>Now, we enter our build agent name. Hitting enter will use the machine name of the computer. I’m gonna use Build Agent A. It then scans for all the relevant software installed on the build agent machine and adds the machine to our organizational agent pool. It’s then going to ask us for a name for our working directory. This is the directory that all our tasks and working files are saved to when building our project and using our build agent. I’m gonna hit enter and use the default entry. It’s now asking us if we want to run the agent as a service or interactively. Running interactively requires additional configuration and can involve domain policies, which are not within the scope of this course. We are gonna type Y for yes, and then it’s gonna ask us if we want to use the NT AUTHORITY\NETWORK SERVICE as the user account for the service. We’re gonna hit enter and accept the default. It will then finish setting up our build agent and let us know if it was successful or not.</p>
<p>Some of the reasons that it may fail are failure to authorize our PAT, no network connectivity to the server, or not having write permissions on the directory the build agent is installed in. We have now installed our first build agent, and I’m going to quickly install a second build agent. It’s an identical process, it lends itself well to scripted and unattended agent installations from multiple agents.</p>
<p>Now that we have our agent pools configured and our build agents installed, we can now update our pipeline to use our pool of agents to build our project. I will navigate to our project pipeline and click edit. This will bring up the YAML file. Now, I’ve already configured a pipeline to use the new project agent pool, which delegates to our matching organization pool. All that I’ve changed here was to replace the vmImage attribute with the Name attribute under the pool heading in the YAML file. I’ve also set the name attribute to our new project agent pool Org1 Agent Pool. And I’ll just click run. But wait, what happened? Remember when I unchecked the option to grant access to all pipelines? Unchecking this box means that we must give permissions to the pipeline to access our agent pool. So we can just click the Authorize Resources button to the right of this message.</p>
<p>Now if we happen to forget to create a project-level agent pool that maps to our organizational agent pool, then when we run this pipeline, we would get an error that says it cannot find the pipeline named Org1 Agent Pool. So, there must be an agent pool at the project level that references an organizational level agent pool that contains build agents in order to be able to use the agents to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/course-introduction/">build our project</a>.</p>
<p>Now, let’s try and run our pipeline a second time and see if it uses both of our build agents. This first one here, it’s using Build Agent B. Let’s check the other running job and see which build agent it’s using. And as you can see here, Build Agent A is being used. So, both the agents in our agent pool are being used.</p>
<p>So, in this demo, we created an agent pool at the organization and project level and mapped the project agent pool to the organization agent pool. We installed and configured two separate build agents on a virtual machine that has Windows Server and Visual Studio installed, and we’ve configured our pipeline to grab an available agent from the build agent pool to build our project. Before Azure pipelines came along, this process involved knowing and running complex PowerShell scripts that could be hard to understand and fix when things did not go quite as planned or unique environments came into play. But as you can see in this demo, Azure DevOps has really made an effort to simplify this process as much as they can.</p>
<p>I hope you enjoyed this demo, and I look forward to seeing you in our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/parallel-jobs/">next module</a>!</p>
<h1 id="Parallel-Jobs"><a href="#Parallel-Jobs" class="headerlink" title="Parallel Jobs"></a>Parallel Jobs</h1><p>In this lecture, we are going to talk about concurrent or parallel jobs and some of the strategies for determining how many we need. Recommending a strategy for running <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-build-agents-and-pools-demo/">build jobs</a> in parallel or concurrently requires several considerations. Concurrent jobs or as they are now called in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps, parallel jobs. Azure DevOps provides the ability to run one parallel job for free. Now the term job has multiple meanings in Azure DevOps and the context is important.</p>
<p>A pipeline can have several jobs, which in this case is a task or series of tasks to be run in a pipeline. A parallel job is what runs a pipeline job and one parallel job is required to execute one pipeline job.</p>
<p>Azure DevOps gives one parallel job to every organization for free. Private projects have a build time limit of 1800 minutes or 30 hours, it also can only run one pipeline job at a time. Public projects, which is projects that are part of the Azure Pipelines public project, or pipelines that build the public repository from GitHub, the build time limit time is removed and you get 10 free parallel jobs.</p>
<p>There’s also a slight difference in the free tier offering when it comes to types of agents being used. With Microsoft-hosted agents, there is a maximum time limit for the job of 360 minutes or six hours. If you choose to use a self-hosted agent then there’s no time limit on the self-hosted jobs. You can also get one additional free self-hosted parallel job for every user in your organization that has an active Visual Studio Enterprise subscriber account.</p>
<p>So how do we determine if we need additional parallel jobs? According to Microsoft, the simple estimate would be to run one parallel job for every four to five users in your organization.</p>
<p>Some scenarios where you might consider additional parallel jobs are: you have multiple teams of developers working on a single application, if your continuous integration triggers multiple builds from multiple branches, it’s recommended that you have one parallel job for each branch, or if you develop multiple applications in a single DevOps Organization, the simple recommendation is one parallel job per application.</p>
<p>This can be extended as needed by your organization. Let’s consider this scenario.</p>
<p>You have a single organization, your organization has four development teams of five people on each team, working on three different applications. The first application is a website for your company, second application is an internal desktop application and the third application is a XAML application for your warehouse to provide updated inventory levels on demand. It requires two XAML build controllers. Your web application has a master, develop, test, stage and release branch, and your CI pipeline will build and deploy to a deployment pipeline in Azure if after deploying to the test server, all of the automated tests pass. Your internal application is a very small utility that takes a couple of minutes to build and deploy, it just has a master branch and when commits are made to the master branch it triggers a build and drops the output into an artifact’s directory if successful. Your XAML application has a master, develop, test and release branch. All code committed to each of the branches triggers a build job and an artifact drop, but deployments are done manually.</p>
<p>Using the guidelines provided previously, we would need four parallel jobs for the development teams, each team having four to five members each. Three for our applications, one per application. In our website pipeline, we have five branches but only two of them actively use CI triggers. So that’s only an additional two rather than five. Also, server jobs, either Azure Pipelines or TFS, and deploying to a deployment group using a release pipeline do not take up a parallel job.</p>
<p>Though there could be an argument made here for five depending on the size and build time of the application in question.</p>
<p>We’ve already counted a parallel job for our internal application, and since this is a small application, it doesn’t take very long to build and deploy an additional parallel job for the master branch is likely overkill. Our last application already has a parallel job, but this is a XAML application and it requires two build controllers. XAML build controllers require one parallel job per controller and we can count the one for the application and one additional for our second XAML build controller.</p>
<p>So, another simple way to determine how many parallel jobs you need without initially spending any money is if the number of builds queued starts to exceed the number of parallel jobs you have and the queue delays starts to take more time than is acceptable, simply add additional parallel jobs until the queue delay time is back down to an acceptable level. The exception to this rule is XMAL, XAML always requires one parallel job per build controller and must use a self-hosted agent and self-hosted parallel job.</p>
<p>An important note here, when you first decide to buy your first parallel job, that will only remove the time restriction from the free tier, if you need to actually run two jobs in parallel, you will have to purchase an additional parallel job.</p>
<p>How do I know if I have more builds queued than parallel jobs? Well, I am glad you asked. If you wanna check out your parallel job usage information, it’s good to know where to find it. Parallel job information can be found in the organizational settings by selecting Parallel Jobs from the Menu.</p>
<p>This page will show you a lot of useful information, like how many minutes of your free tier you have used, how many parallel jobs you have, and it gives a breakdown of where your parallel jobs are coming from; free tier, visual studio enterprise subscriber or paid. This is also a nice way to visualize the free tier limitations.</p>
<p>So now that we have a better understanding of parallel jobs and we have some strategies on how to determine how many parallel jobs we might need. I look forward to seeing you in our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/service-connections-hooks-and-webhooks/">next module</a>.</p>
<h1 id="Service-Connections-Hooks-and-Webhooks"><a href="#Service-Connections-Hooks-and-Webhooks" class="headerlink" title="Service Connections, Hooks, and Webhooks"></a>Service Connections, Hooks, and Webhooks</h1><p>In this lecture, we are gonna talk about service connections, service endpoints, and webhooks. A service connection, in simple terms, is a defined connection that allows <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps to communicate with an external service. Some well-known services, like GitHub or Jenkins, already have service connections that you can configure in Azure DevOps, but what if you wanted to create your own unique service that responded to events from Azure DevOps to perform some unique action? That’s where service endpoints, extensions, and webhooks come into the picture. Service providers, or service hook publishers, create a definition for a set of events that can be subscribed to.</p>
<p>Subscriptions are a way to subscribe and listen for specific events. Subscriptions also define a set of actions that can be taken directly, or they can target external services, called consumers, that run their own action or set of actions based on an event that was raised.</p>
<p>To integrate Azure DevOps with external services, you use service connections or service hooks, and the service endpoint is the set of properties provided to a service connection or service hook. To integrate Azure DevOps with external services, you use service connection or service hooks, and the service endpoint is the set of properties provided to the service connection or service hook to give you the information needed for the connection to function.</p>
<p>Some of the properties that you’ll find in a service endpoint are Service Name, Description, Server URL, Certificates or Tokens, User names, and Passwords.</p>
<p>Publishers, consumers, and service endpoints all come together to form a service connection that can be used to create extensions or custom tasks that need to connect to external services. Extensions utilize service endpoints to gather information to be able to integrate Azure DevOps and perform their own tasks based on events they subscribe to.</p>
<p>There are many extensions available in the Visual Studio Marketplace that utilize service hooks and endpoints. They fall into several categories: Build and Release, Collaborate, Customer Support, Plan and Track, and Integrate.</p>
<p>Under the Integrate category, we find webhooks. Webhooks are a way for a server to call a client-provided endpoint to let the client know that an event has occurred on the server without the client being required to constantly poll the server for new events. You can create a service connection that uses a webhook to post to a REST endpoint in your own application, or you can use a webhook to connect to an external server that has a REST endpoint.</p>
<p>You can set up a webhook by providing a trigger definition. Some examples of this are Build Completed, Code Pushed, and Pull Request Created. You also can add a filter, so that your webhook can be limited to a single project, and a build status, like succeeded or failed. You can also send HTTP Headers, messages, and event data. Event data is sent as JSON. You can also optionally send basic authentication credentials. Once you created your webhook, you can test it by sending a message during the configuration steps to make sure that you’ve configured the webhook correctly and you’re getting a response from your endpoint.</p>
<p>So, as you can see there’s a ton of extensibility options available to you if Azure DevOps doesn’t have everything you need. We will be talking more about third-party integration in the module on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-jenkins-integration/">integrating Azure with Jenkins</a>. I’ll see you there.</p>
<h1 id="Azure-Jenkins-Integration"><a href="#Azure-Jenkins-Integration" class="headerlink" title="Azure Jenkins Integration"></a>Azure Jenkins Integration</h1><p>In this lecture, we’re gonna talk about Jenkins and it’s integration with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-pipelines-demo/">Azure Pipeline</a>. Jenkins is a build automation server that helps you automate, build, and scale your continuous integration and delivery process. Jenkins can be hosted in Azure or on-premise. You can also use <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> to extend the capabilities of your on-premise Jenkins server by utilizing Azure in several ways.</p>
<p>There are three ways to install and host a Jenkins server in Azure. You can create a Linux virtual machine and install Jenkins manually, you can use the Jenkins solution template, and finally, you can host it on a Kubernetes cluster running in the Azure container service. Jenkins server deployments can be monitored through the Azure monitored logs and Azure Command Line Interface.</p>
<p>Azure’s integration with Jenkins supports build automation scaling for your build capacity by allowing you to add additional <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-build-agents/">build agents</a> to your Jenkins server as complexity and needs increase. You can use the Azure VM Agents plug-in to run these additional build agents.</p>
<p>Azure-integrated servers use Azure Service Principle for credentials and allow you to secure your integration with Azure storage for archiving and storing build artifacts, connecting to pipelines, and Azure Repos, all of which can be monitored using the Azure monitoring service.</p>
<p>Finally, you can use Jenkins to deploy your applications to Azure services, such as Azure App Service and Azure Kubernetes.</p>
<p>In the demo for integrating Jenkins with Azure Pipelines, we’ll <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-jenkins-integration-demo/">create a Jenkins server</a> using the Jenkins Solution template to create a VM that has Ubuntu on it and integrate with our build pipeline we created previously in the demo on creating an Azure Pipelines.</p>
<p>Let’s check it out.</p>
<h1 id="Azure-Jenkins-Integration-Demo"><a href="#Azure-Jenkins-Integration-Demo" class="headerlink" title="Azure Jenkins Integration Demo"></a>Azure Jenkins Integration Demo</h1><p>In this demo, I’m going to show you how to create a Linux VM using cloud-init to spin up the virtual machine and install Jenkins. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/azure-jenkins-integration/">Jenkins</a> is a build automation server that you can integrate with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> to scale out your build capacity.</p>
<p>So, the first thing we need to do is create a working directory and a new text file. We’ll call the text file cloud init Jenkins, cloud-init-jenkins.txt. And from there, we will copy the cloud-init contents from the Microsoft documentation located at this <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/developer/jenkins/configure-on-linux-vm">URL</a>.</p>
<p>This is the file that will be used to download and install Jenkins to the Linux virtual machine using the cloud-init process. Let me just quickly create the cloud-init file here and copy the file contents from the URL I mentioned earlier.</p>
<p>Now that the file is ready, the next thing we need to do is make sure that we have the Azure CLI, or command-line interface installed. If you do not have Azure CLI installed, you can find here at this <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest">URL</a>. I already have the Azure CLI installed, so I am not going to install it. But if you are working on Windows, then you can simply download and install the MSI package and that works just fine.</p>
<p>You can check to see if you already have the Azure CLI installed by opening a command prompt and typing <code>az --version</code>. This will also tell you if Azure CLI is installed, which version of the Azure CLI it is, and if there is an update available or not.</p>
<p>The first thing that you might need to do if you do not already have a resource group is to create one, I will just quickly show you how to create a resource group for your Jenkins VM. The command is az, for the Azure CLI, group, for the resources group context, now we specify the create command and then we specify a name with the name attribute, and a location with the location attribute. That’s all there is to create a resource group using the Azure CLI.</p>
<p>The next thing we need to do is to create a virtual machine that our Jenkins server will be installed on. Since we’re using the Azure command line interface, all commands start with az, then we are working with a virtual machine context, so we type vm, now we specify the create command, and add the parameters. The first parameter is resource-group, and this tells Azure the name of the resource group we want to add the virtual machine to. The next attribute is name, this is the name you want to give your virtual machine. Then we need to specify the machine image using the image attribute, in our case, we want to use the latest version of Ubuntu, so we’re going to enter UnbuntuLTS. Now we specify the initial admin username and use the generate-ssh-keys attribute to create the public and private encryption keys for our new virtual machine.</p>
<p>Finally, we use the custom-data attribute to specify our cloud-init file that tells the template which software to install on our new machine. If the cloud-init file is not in your current working directory, you need to specify the entire path to the file.</p>
<p>It will take a few minutes to install, once it’s installed, make sure to take note of your public IP address, we’re going to need it later.</p>
<p>Now that our virtual machine is created, we need to allow incoming communications with our Jenkins server on a specific port. So we will use the open port command to update our network security group and add an entry to our inbound port rules.</p>
<p>The command is <code>az vm</code>, our resource group, our virtual machine name, what port we want to open, and what priority we want to use for the inbound port rule. We are going to use 8080 for the port and 1001 for the priority. If you are not sure what to use for the port and priority, check with your network administrator, they should be able to tell you what these values should be.</p>
<p>After we have opened the port to allow communication to our Jenkins server, we now need to remotely connect to our new VM over SSH. Once we have successfully connected, we can check to see if our Jenkins service is running by typing <code>service jenkins status</code> and we should get a message telling us that the service is loaded.</p>
<p>Now that we know the service is running, we need to get the initial password that was created when the service was installed. We can do that by printing it out to the screen using <code>sudo cat</code> command, along with the file path to the initial password file. The file is located at the following path: &#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;secrets&#x2F;initialAdminPassword</p>
<p>Once we have our password, it’s time to open our browser and navigate to our Jenkins server. We can find it at the public IP address we got when we created our virtual machine, followed by a colon with the port number we opened in our firewall. If you need to find the public IP address again, you can go to the VM in Azure and it will show you the public IP address on the overview screen for the virtual machine.</p>
<p>This will bring us to a page where we can unlock our Jenkins server by using the password that we printed to the screen previously. I will just paste that here and click Continue. The next page is where we can decide if we want to just install the most common Jenkins plugins or if we want to select additional plugins to add. We need to add the GitHub plugin, so I will click on the select plugins option on the right.</p>
<p>This page shows all the plugins that will be installed, and this is where we can search for additional plugins and select them. I will search for the GitHub plugin, and check the checkbox and click Install.</p>
<p>Now Jenkins will install all the plugins that were selected. And I’ll just speed up the time. So now that that’s done, we need to create our first admin user.</p>
<p>I will just add the relevant information and create the user. Finally, the last thing we need to configure is a friendly URL for our Jenkins server. If you are going to use a DNS entry for your Jenkins server, then this is when you would add the friendly domain address. The root address is needed for almost everything that Jenkins does, so it is good to put the user-friendly address in now so that you can avoid confusion later.</p>
<p>Now our Jenkins server is installed and ready to use. I hope you enjoyed this demo, and I’ll see you in our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/course-summary/">next module</a>.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>In this <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/course-introduction/">course</a>, we’ve talked about managing and implementing your build infrastructure with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> DevOps. In the section on Azure Pipelines, we talked about pipelines, what they are, what they’re used for and how to use them. We talked about the difference between public and private projects. We talked about jobs and tasks, what they are. And in our pipelines demo, we went through the process of creating a build pipeline for our software development project. We had a look at the initial tasks that were created by default in a .net core project.</p>
<p>In the section on build agents, we talked about the types of build agents available to use, what each of the types of build agents are for. We talked about the restrictions and limitations of the types of build agents. We talked about organizational and project-level build pools, what they are used for, and how they relate to each other and to our pipeline. In our demo, we created both an organizational and a project-level agent pool. We installed two self-hosted agents on a virtual machine in the cloud, and we added them to our organizational pool and had our pipeline use them to build our code project.</p>
<p>In the lecture on concurrent and parallel jobs, we talked about parallel jobs being the new name for concurrent jobs. And that parallel jobs are what are utilized to run a pipeline build job. We talked about what Azure DevOps offers as part of the free tier and the limitations on time and number of parallel jobs run in the free tier.</p>
<p>In the section on service and webhooks, we talked about what a service connection is and what a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-and-managing-azure-build-infrastructure/service-connections-hooks-and-webhooks/">webhook</a> is, and what they’re used for. We talked about the service endpoints, and what they are and they can be used for.</p>
<p>In the section on Jenkins integration, we talked about integrating Jenkins with Azure DevOps and in the demo, we saw how to spin up a virtual machine and install and configure Jenkins server.</p>
<p>There are tons of options for building, configuring, customizing, extending, managing, and implementing our build infrastructure in Azure DevOps. And it will only get better with time. I truly hope that you enjoyed my course, and as always, please feel free to leave feedback and rate this course. I’m always looking for ways to improve my course, and your feedback helps me do just that.</p>
<p>Until next time, happy building.</p>
<h1 id="2Azure-Pipelines"><a href="#2Azure-Pipelines" class="headerlink" title="2Azure Pipelines"></a>2<strong>Azure Pipelines</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/azure/devops/pipelines/get-started/key-pipelines-concepts">Key concepts for new Azure Pipelines users</a></p>
<h1 id="4Azure-Build-Agents"><a href="#4Azure-Build-Agents" class="headerlink" title="4Azure Build Agents"></a>4<strong>Azure Build Agents</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/jenkins/tutorial-jenkins-github-docker-cicd">Microsoft documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-windows?view=azure-cli-latest">Azure CLI</a></p>
<h1 id="9Azure-Jenkins-Integration-Demo"><a href="#9Azure-Jenkins-Integration-Demo" class="headerlink" title="9Azure Jenkins Integration Demo"></a>9<strong>Azure Jenkins Integration Demo</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/virtual-machines/sizes-general#dsv2-series">Standard_DS2_v2 virtual machine image</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops">List of Microsoft-hosted Windows agents</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/process/container-phases?view=azure-devops&tabs=yaml">Container jobs requirements &amp; limitations</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/developer/jenkins/configure-on-linux-vm">Get Started: Install Jenkins on an Azure Linux VM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/developer/jenkins/pipeline-with-github-and-docker">Create a Jenkins pipeline using GitHub and Docker</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/64/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/64/">64</a><span class="page-number current">65</span><a class="page-number" href="/page/66/">66</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/66/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
