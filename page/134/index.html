<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/134/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/134/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Using-Amazon-Key-Management-Service-to-Encrypt-S3-and-EBS-Data-33/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Using-Amazon-Key-Management-Service-to-Encrypt-S3-and-EBS-Data-33/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Using-Amazon-Key-Management-Service-to-Encrypt-S3-and-EBS-Data-33</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:37" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:37-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:03:50" itemprop="dateModified" datetime="2022-11-19T23:03:50-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Using-Amazon-Key-Management-Service-to-Encrypt-S3-and-EBS-Data-33/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Using-Amazon-Key-Management-Service-to-Encrypt-S3-and-EBS-Data-33/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Understanding-S3-Encryption-Mechanisms-to-Secure-your-Data-32/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Understanding-S3-Encryption-Mechanisms-to-Secure-your-Data-32/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Understanding-S3-Encryption-Mechanisms-to-Secure-your-Data-32</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:35" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:35-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:58:32" itemprop="dateModified" datetime="2022-11-19T22:58:32-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Understanding-S3-Encryption-Mechanisms-to-Secure-your-Data-32/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Understanding-S3-Encryption-Mechanisms-to-Secure-your-Data-32/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to this course focused on the five different S3 encryption mechanisms covering SSE-S3, SSE-KMS, SSE-C, CSE-KMS, and CSE-C, and I’ll explain how both the encryption and decryption process works for each option.</p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the details shown on the screen. Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#x75;&#x70;&#112;&#x6f;&#x72;&#x74;&#64;&#x63;&#x6c;&#111;&#117;&#100;&#97;&#99;&#97;&#100;&#x65;&#109;&#x79;&#x2e;&#99;&#111;&#x6d;">&#115;&#x75;&#x70;&#112;&#x6f;&#x72;&#x74;&#64;&#x63;&#x6c;&#111;&#117;&#100;&#97;&#99;&#97;&#100;&#x65;&#109;&#x79;&#x2e;&#99;&#111;&#x6d;</a> where one of our cloud experts will reply to your question. </p>
<p>This course is specifically designed for those who are responsible for storing, managing, and protecting data that is stored on Amazon S3. </p>
<p>The content of this course will focus on the following lectures:</p>
<ul>
<li>Overview of encryption mechanisms</li>
<li>Server-Side Encryption with S3 Managed Keys</li>
<li>Server-Side Encryption with KMS Managed Keys</li>
<li>Server-Side Encryption with Customer Provided Keys</li>
<li>Client-Side Encryption with KMS Managed Keys</li>
<li>Client-Side Encryption with Customer Provided Key</li>
</ul>
<p>Each lecture within this course has been specifically designed to be short, focused, and to the point delivering a single objective from each lecture. </p>
<p>This course does not focus on recommendations of when to encrypt data or discuss the pros and cons for each encryption method. Instead, the single objective of this course is to enable you to understand the process for encryption and decryption for all S3 encryption options, providing you a full understanding of how S3 is protecting and securing your data when an encryption option is needed based on your business requirements. </p>
<p>For this course, it is essential that you have an understanding of S3 and have the knowledge to enable you to upload and retrieve data along with how to select different encryption options. In addition to this, you must also be familiar with the KMS service and understand both Customer Master Keys, CMKs, and data encryption keys. For additional information on these points, please search our content library for these existing courses as shown on the screen. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you can contact <a href="mailto:&#x73;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#64;&#99;&#108;&#111;&#x75;&#x64;&#x61;&#99;&#x61;&#100;&#101;&#109;&#x79;&#x2e;&#99;&#x6f;&#x6d;">&#x73;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#64;&#99;&#108;&#111;&#x75;&#x64;&#x61;&#99;&#x61;&#100;&#101;&#109;&#x79;&#x2e;&#99;&#x6f;&#x6d;</a>. </p>
<p>That brings me to the end of this lecture. Coming up next, I’ll provide an overview of the five different encryption mechanisms.</p>
<h1 id="Overview-of-Encryption-Mechanisms"><a href="#Overview-of-Encryption-Mechanisms" class="headerlink" title="Overview of Encryption Mechanisms"></a>Overview of Encryption Mechanisms</h1><p>Depending on your requirements, one method of encryption may be more appropriate than another. To help you decide, here is a quick overview of each. </p>
<p>Server-side encryption with S3 managed keys, SSE-S3. This option requires minimal configuration and all management of encryption keys used are managed by AWS. All you need to do is to upload your data and S3 will handle all other aspects. </p>
<p>Server-side encryption with KMS managed keys, SSE-KMS. This method allows S3 to use the key management service to generate your data encryption keys. KMS gives you a far greater flexibility of how your keys are managed. For example, you are able to disable, rotate, and apply access controls to the CMK, and audit it against their usage using AWS CloudTrail. </p>
<p>Server-side encryption with customer provided keys, SSE-C. This option gives you the opportunity to provide your own master key that you may already be using outside of AWS. Your customer-provided key would then be sent with your data to S3, where S3 would then perform the encryption for you. </p>
<p>Client-side encryption with KMS, CSE-KMS. Similarly to SSE-KMS, this also uses the key management service to generate your data encryption keys. However, this time KMS is called upon via the client not S3. The encryption then takes place client-side and the encrypted data is then sent to S3 to be stored. </p>
<p>Client-side encryption with customer provided keys, CSE-C. Using this mechanism, you are able to utilize your own provided keys and use an AWS-SDK client to encrypt your data before sending it to S3 for storage. </p>
<p>Okay, that has given us a very high-level overview of the five different methods. Via a series of diagrams, I will now explain how the encryption and decryption process works for each.</p>
<h1 id="Server-Side-Encryption-with-S3-Managed-Keys-SSE-S3"><a href="#Server-Side-Encryption-with-S3-Managed-Keys-SSE-S3" class="headerlink" title="Server-Side Encryption with S3 Managed Keys (SSE-S3)"></a>Server-Side Encryption with S3 Managed Keys (SSE-S3)</h1><p>Server-Side Encryption with S3 Managed Keys, SSE-S3. The encryption process is as follows. Firstly, a client uploads Object Data to S3. S3 then takes this Object Data and encrypts it with an S3 Plaintext Data Key. This creates an encrypted version of the Object Data, which is then saved and stored on S3. Next, the S3 Plaintext Data Key is encrypted with an S3 Master Key. Which creates an encrypted S3 Data Key. This encrypted Data Key is then also stored on S3 and the Plaintext Data Key is removed from memory. The decryption process is as follows. A request is made by the client to S3 to retrieve the Object Data. S3 takes the associated encrypted S3 Data Key off the Object Data and decrypts it with the S3 Master Key. The S3 Plaintext Data Key is then used to decrypt the object data. This object data is then sent back to the client.</p>
<h1 id="Server-Side-Encryption-with-KMS-Managed-Keys-SSE-KMS"><a href="#Server-Side-Encryption-with-KMS-Managed-Keys-SSE-KMS" class="headerlink" title="Server-Side Encryption with KMS Managed Keys (SSE-KMS)"></a>Server-Side Encryption with KMS Managed Keys (SSE-KMS)</h1><p>Server-Side Encryption with KMS managed keys, SSE-KMS. The encryption process is as follows. Firstly, a client uploads object data to S3. S3 then requests data keys from a KMS-CMK. Using the specified CMK, KMS generates two data keys, a plain text data key and an encrypted version of the same data key. These two keys are then sent back to S3. S3 then combines the object data and the plain text data key to perform the encryption. This creates an encrypted version of the object data which is then stored on S3 along with the encrypted data key. The plain text data key is then removed from memory. The decryption process is as follows. A request is made by the client to S3 to retrieve the object data. S3 sends the associated encrypted data key of the object data to KMS. KMS then uses the correct CMK with the encrypted data key to decrypt it and create a plain text data key. This plain text data key is then sent back to S3. The plain text data key is then combined with the encrypted object data to decrypt it. This decrypted object data is then sent back to the client.</p>
<h1 id="Server-Side-Encryption-with-Customer-Provided-keys-SSE-C"><a href="#Server-Side-Encryption-with-Customer-Provided-keys-SSE-C" class="headerlink" title="Server-Side Encryption with Customer Provided keys (SSE-C)"></a>Server-Side Encryption with Customer Provided keys (SSE-C)</h1><p>Server-Side Encryption with Customer Provided Keys, SSE-C. The encryption process is as follows. Firstly, a client uploads Object Data and the Customer-provided Key to S3 for a HTTPS. It will only work with the HTTPS connection. Otherwise, S3 will reject it. S3 will then use the Customer-provided Key to encrypt the Object Data. S3 will also create a sorted HMAC value of the Customer-provided Key for future validation requests. The encrypted Object Data, along with the HMAC value of the Customer Key is then saved and stored on S3. The Customer-provided Key is then removed from memory. The decryption process is as follows. A request is made by the client via HTTPS connection to S3 to retrieve the Object Data. At the same time, the Customer-provided Key is also sent with the request. S3 uses the HMAC value of the same key to confirm it’s validity of the requested object. The Customer-provided Key is then used to decrypt the encrypted Object Data. The Object Data is then sent back to the client.</p>
<h1 id="Client-Side-Encryption-with-KMS-Managed-Keys-CSE-KMS"><a href="#Client-Side-Encryption-with-KMS-Managed-Keys-CSE-KMS" class="headerlink" title="Client-Side Encryption with KMS Managed Keys (CSE-KMS)"></a>Client-Side Encryption with KMS Managed Keys (CSE-KMS)</h1><p>Client-Side Encryption with KMS Managed Keys, CSE-KMS. The encryption process is as follows. Using an AWS SDK, such as the Java client, a request is made to KMS for Data Keys that are generated from a specific CMK. This CMK is defined by providing the CMK-ID in the request. KMS will then generate two Data Keys from the specified CMK. One key will be a Plaintext Data Key. The second will be a Cipher blob of the same Data Key. Both keys are then sent back to the client. The client will then combine the Object Data with the Plaintext Data Key to create an encrypted version of the Object Data. The client then uploads both the encrypted Object Data and the Cipher blob version of the Data Key to S3. S3 will then store the encrypted Object Data and associate the Cipher blob Data Key as Metadata of the encrypted Object Data. The decryption process is as follows. A request is made by the client to S3 to retrieve the Object Data. S3 sends both the encrypted Object Data and the Cipher blob back to the client. Using an AWS SDK, such as the Java client, the Cipher blob Data Key is sent to KMS. KMS combines the Cipher blob Data Key with the corresponding CMK to produce the Plaintext Data Key. This Plaintext Data Key is then sent back to the client and the Plaintext Data Key is then used to decrypt the encrypted Object Data.</p>
<h1 id="Client-Side-Encryption-with-Customer-Provided-Keys-CSE-C"><a href="#Client-Side-Encryption-with-Customer-Provided-Keys-CSE-C" class="headerlink" title="Client-Side Encryption with Customer Provided Keys (CSE-C)"></a>Client-Side Encryption with Customer Provided Keys (CSE-C)</h1><p>Client-Side Encryption with Customer Provided Keys, CSEC. The encryption process is as follows. Using an AWS SDK, such as the Java client, it will randomly generate a plain text data key which is used to encrypt the object data. The customer provided CMK is then used to encrypt this client-generated data key. The encrypted object data and encrypted data key are then sent to S3. S3 will then store the encrypted object data and associate the encrypted data key as metadata of the encrypted object data. The decryption process is as follows. A request is made by the client to S3 to retrieve the object data. S3 sends both the encrypted object data and the encrypted data key back to the client. The customer-provided CMK is then used to decrypt the encrypted data key. The plain text data key is then used to decrypt the object data. You should now have a deeper understanding of the process of encryption and decryption for each of the encryption methods that S3 offers. </p>
<p>It is a simple process to apply encryption, but understanding what’s happening behind the scenes, is essential from a security standpoint. Especially when you are responsible for maintaining the integrity of the data stored in S3. Many of us have seen and heard the news whereby large, international organizations have failed to apply either correct level of permissions, or, indeed, an encryption mechanism to customer data which has been accidentally exposed. Causing a detrimental effect to all organizations involved. </p>
<p>To accompany this course, I’ve also created an infographic which shows all five encryption options and this can be found using the <a target="_blank" rel="noopener" href="https://awsinfographics.s3.amazonaws.com/S3_Encryption_Infographic.png">link</a> on screen. I have also added this URL within the transcript of this lecture. </p>
<p>If you have any feedback on this course, positive or negative, please do get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#117;&#112;&#x70;&#111;&#114;&#116;&#x40;&#99;&#x6c;&#x6f;&#x75;&#100;&#x61;&#x63;&#97;&#x64;&#x65;&#x6d;&#121;&#46;&#x63;&#111;&#x6d;">&#115;&#117;&#112;&#x70;&#111;&#114;&#116;&#x40;&#99;&#x6c;&#x6f;&#x75;&#100;&#x61;&#x63;&#97;&#x64;&#x65;&#x6d;&#121;&#46;&#x63;&#111;&#x6d;</a>.</p>
<p>Thank you for your time, and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="1Introduction"><a href="#1Introduction" class="headerlink" title="1Introduction"></a>1<strong>Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-big-data-security-encryption/introduction-43/">Course: AWS Encryption for Data Analytics</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to use KMS Key encryption to protect your data</a></p>
<h1 id="7Client-Side-Encryption-with-Customer-Provided-Keys-CSE-C"><a href="#7Client-Side-Encryption-with-Customer-Provided-Keys-CSE-C" class="headerlink" title="7Client-Side Encryption with Customer Provided Keys (CSE-C)"></a>7<strong>Client-Side Encryption with Customer Provided Keys (CSE-C)</strong></h1><p><a target="_blank" rel="noopener" href="https://awsinfographics.s3.amazonaws.com/S3_Encryption_Infographic.png">S3 Encryption Infographic</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Share-CMKs-Across-Multiple-Accounts-Using-AWS-KMS-31/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-How-to-Share-CMKs-Across-Multiple-Accounts-Using-AWS-KMS-31/" class="post-title-link" itemprop="url">AWS-Security-Specialty-How-to-Share-CMKs-Across-Multiple-Accounts-Using-AWS-KMS-31</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:33" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:33-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:57:30" itemprop="dateModified" datetime="2022-11-19T22:57:30-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-How-to-Share-CMKs-Across-Multiple-Accounts-Using-AWS-KMS-31/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-How-to-Share-CMKs-Across-Multiple-Accounts-Using-AWS-KMS-31/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome. In this course, I will be looking at how we can use the same customer master key, or CMK, for encryption across multiple AWS accounts using the Key Management Service. My name is Stuart Scott and if you have any questions, please reach out to me using the details shown on screen. Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, where one of our cloud experts will reply to your question.</p>
<p>This course has been created for security engineers and architects who are responsible for managing and implementing data encryption methods across AWS accounts. And this course will define the key principles and components within KMS that are needed to share CMKs across your accounts. And I will then perform a demonstration on how to implement this sharing.</p>
<p>By the end of this course, you will have the knowledge and understanding of how to use KMS and your CMKs across each of your AWS accounts. And to get the most from this course, you should be familiar with the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Key Management Service</a> and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">IAM permissions</a> and JSON policies. Okay, let’s get started.</p>
<h1 id="Sharing-CMKs-Across-Multiple-AWS-Accounts"><a href="#Sharing-CMKs-Across-Multiple-AWS-Accounts" class="headerlink" title="Sharing CMKs Across Multiple AWS Accounts"></a>Sharing CMKs Across Multiple AWS Accounts</h1><p>Hello and welcome to this lecture, focusing on KMS permissions and key policies. Before I begin, I just want to highlight a couple of key components within KMS. Firstly, the Customer Master Key is known as the CMK. The CMK is the main key type within KMS and can generate, encrypt and decrypt data encryption keys known as the DEKs, which are used outside of the KMS service by other AWS services to perform encryption against your data. </p>
<p>CMKs can either be managed by AWS or by you and me as customers of AWS. CMKs managed by AWS are used by other AWS services that have the ability to interact with KMS directly to perform encryption against data. For example, Amazon S3, in particular, SSE-KMS, which is server-side encryption using the Key Management Service. CMKs that are created and generated by you and I rather than AWS provide the ability to implement greater flexibility, such as being able to manage the key including rotation, governing access and key policy configuration, along with being able to both enable and disable the key when it is no longer required.</p>
<p>With many services that you use within AWS, you can control access using identity-based access IAM policies. However, KMS also uses resource-based policies when it comes to CMK access. If you want to allow other IAM users or roles in a different AWS account in which a CMK was created, then you must understand how KMS permissions work. In all cases, to manage access to your CMKs, you must use a key policy. Without a key policy associated to your CMK, users will not be able to use it. Permissions to allow you to access and use a CMK from a different AWS account can’t be given and generated using IAM alone. As a result, you have to use and edit a resource-based key policy in the AWS account where the CMK resides, in addition to an IAM identity-based policy in the AWS account that wants to access the CMK. One point to remember is that you can only edit key policies for keys that you have created. So you can’t share AWS managed CMKs between accounts.</p>
<p>Key policies are resource-based policies that are tied to your CMK. And the key policy document itself is JSON-based, much like IAM policies. As a result, they typically look like the following. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">  &quot;Statement&quot;: [&#123;</span><br><span class="line">    &quot;Sid&quot;: &quot;statement identifier&quot;,</span><br><span class="line">    &quot;Effect&quot;: &quot;effect&quot;,</span><br><span class="line">    &quot;Principal&quot;: &quot;principal&quot;,</span><br><span class="line">    &quot;Action&quot;: &quot;action&quot;,</span><br><span class="line">    &quot;Resource&quot;: &quot;resource&quot;,</span><br><span class="line">    &quot;Condition&quot;: &#123;&quot;condition operator&quot;: &#123;&quot;condition context key&quot;: &quot;context key value&quot;&#125;&#125;</span><br><span class="line">  &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>During the creation of a CMK, whether you create it programmatically or if you’ve created it through the AWS Management Console, KMS will create a default key policy for you to allow principles to use the CMK in question. By default, KMS will configure the root user of the AWS account in which the key was created full access to the CMK within the key policy. </p>
<p>When you create a CMK through the Management Console, then you can configure different permissions sets. These include key administrators and users. Key administrators can only administer the CMK, but not use it to perform any encryption functions. Whereas users have the ability to access the CMK to perform encryption of data. The permissions that can be given to use the key for any user selected, are as follows. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kms:Encrypt</span><br><span class="line">kms:Decrypt</span><br><span class="line">kms:ReEncrypt*</span><br><span class="line">kms:GenerateDataKey* </span><br><span class="line">kms:DescribeKey</span><br></pre></td></tr></table></figure>

<p>So the first step in allowing the sharing of your CMK across AWS accounts is to add the principles from the external account into the key policy of the CMK. To do so, you will need to add a statement as shown where the text embed should be replaced with the external AWS account number.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;Sid&quot;: &quot;Allow another AWS Account to use this CMK&quot;,</span><br><span class="line">    &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">    &quot;Principal&quot;: &#123;</span><br><span class="line">        &quot;AWS&quot;: [</span><br><span class="line">            &quot;arn:aws:iam::123456789012:root&quot;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;Action&quot;: [</span><br><span class="line">        &quot;kms:Encrypt&quot;,</span><br><span class="line">        &quot;kms:Decrypt&quot;,</span><br><span class="line">        &quot;kms:ReEncrypt*&quot;,</span><br><span class="line">        &quot;kms:GenerateDataKey*&quot;,</span><br><span class="line">        &quot;kms:DescribeKey&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;Resource&quot;: &quot;*&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>You could create this key policy statement with even more granularity by limiting the specific actions required, or by specifying individual users or roles. For example, instead of using the root of the external account and the principal parameter, you could add an IAM user named Bob. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&quot;Principal&quot;: &#123;</span><br><span class="line">   &quot;AWS&quot;: [</span><br><span class="line">     &quot;arn:aws:iam::123456789012:user/Bob&quot;</span><br><span class="line">   ]</span><br><span class="line"> &#125;,</span><br></pre></td></tr></table></figure>

<p>Once this statement is added to the key policy, the users within the account would still not be able to use the CMK until IAM permissions in the external account have been added to specific users or roles. And once the key policy has been edited to allow either the external account or specific users or roles from the external account access to a CMK, identity-based policies need to be associated with those users or roles who intend to use the CMK.</p>
<p>Let’s assume that in the key policy, we set the principal component of the root of the external account. This would mean I can set the IAM permissions on any user or role in the external account to allow access to the CMK. The permissions required would need to include a statement as shown for the user or role.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line"> &quot;Statement&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">   &quot;Sid&quot;: &quot;Allow the use of a CMK In Account 1234567889012&quot;,</span><br><span class="line">   &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">   &quot;Action&quot;: [</span><br><span class="line">    &quot;kms:Encrypt&quot;,</span><br><span class="line">    &quot;kms:Decrypt&quot;,</span><br><span class="line">    &quot;kms:ReEncrypt*&quot;,</span><br><span class="line">    &quot;kms:GenerateDataKey*&quot;,</span><br><span class="line">    &quot;kms:DescribeKey&quot;</span><br><span class="line">   ],</span><br><span class="line">   &quot;Resource&quot;: &quot;arn:aws:kms:us-east-2:123456789012:key/5e698272-88c6-4f6b-970e-1d1dbd0c6412</span><br><span class="line">&quot;</span><br><span class="line">  &#125;</span><br><span class="line"> ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>In this policy, you’ll notice that we have an additional parameter of resource, which details which CMK the action should take effect on. And in this instance, is the Amazon resource name of the CMK in which we updated the key policy for. One other point that’s important is that if for example, your key policy only allowed the <code>kms:Decrypt</code> action, but your IAM policy allowed both the <code>kms:Decrypt</code> and <code>kms:encrypt</code> actions, then only the <code>kms:Decrypt</code> action would be allowed, as that is all that is allowed in the key policy, which takes precedence.</p>
<p>Once the policy is associated with the user or role, you will now be able to use the CMK to perform encryption functions. However, please do be aware that even though you have access to the CMK from a different account, the CMK will not be shown in the AWS Management Console for integrated services. Instead, you must use the ARN of the CMK.</p>
<p>I will now perform a demonstration that shows the following steps. I’ll show you how to update the key policy of an existing customer-managed CMK allowing access to an external account to use the CMK. I’ll show you how to apply the required IAM permissions to a user in the external account to allow them to use the CMK. And then I’ll test the access of the CMK across accounts by uploading a file within S3 using the shared CMK.</p>
<p>So I’ve logged-in to my AWS Management Console of the AWS account, where I have the CMK. So let’s go to the KMS service because the first thing we need to do is to edit the key policy of my CMK. Over here we have our AWS managed keys, and also our customer-managed keys. And remember, we can only share customer-managed keys between AWS accounts, we can’t share AWS managed keys. So I’ve got two keys here, one of them is pending deletion at the moment, and we have an active key here. So the demo key is the key that I’m going to use. So to edit the key policy, what we need to do is first select the key, and this brings up all the configuration and details about the CMK. And if we scroll down here, we can see the key policy. And over on the right-hand side, we can switch to the policy view and this will show us the JSON view of the policy itself. So as we scroll down here, we can see different statement IDs, for example, this one will allow access for key administrators. So this will show who the key administrator is, and the actions that they can perform. And further down here, we have another statement, which is, “Allow the use of the key”. And this is the statement that we’re interested in because what we want to do is, add another principle in here, that points to the root account of the secondary AWS account that we want to allow access.</p>
<p>So let’s go ahead and click on this Edit button. And I’m going to scroll down to that statement ID again, and I’m going to add in another principle. So if I just copy and paste this for ease, but I want to change the AWS account that I’m interested in. So let me put in the details of the secondary AWS account and it’s going to be the root account. So what this section of the policy is saying, is allowing all the principles listed here, the ability to use this key and perform these KMS actions such as encrypt, decrypt, etc. So let’s go now and save those changes. </p>
<p>Okay, so now what we want to do is to swap over to our secondary account where I now need to apply identity-based policies to allow a user or multiple users within that account to have access to this CMK. But before I do that, I need to make a note of the ARN of this key, which is found here at the top because you’re gonna need to add this ARN in as the resource within the identity-based policies. So make sure you take a copy of the key’s ARN. </p>
<p>Also, as I mentioned earlier in the course, that KMS is regional. So whatever encryption you’re going to be performing from the other account, it needs to be in the same region. And as we can see up here, currently, I’m in the Ireland region. So that’s just something to remember as well.</p>
<p>Okay, so let me now cross over to the other account. Okay, so now I’m logged-in to my secondary account. So the first thing I need to do, is to go to IAM. And from here, I’m going to select the user that I want to have access to the CMK that I just edited the key policy for. So I’m going to select Stuart, and I’m going to attach an inline policy. So this policy will be directly attached to this identity. So if I go across to Add inline policy, and go to JSON, and to make things a lot easier for this demonstration, I created a policy before, so I’m just going to paste that in. So let me just run through that quickly, it’s a very simple policy. So we have the effect of Allow and these are the actions which are the KMS actions, which is effectively the same as what we had in the key policy. But in this policy, we also have the resource parameter. And this is the ARN of the key that we edited the key policy for. So once you’ve added the policy in, click on Review policy, and then we need to give it a name. I’ll just call this Key Demo, and then down to Create policy. Now if we look at the inline policies, we can now see that we have the Key Demo policy attached to this identity.</p>
<p>So let’s now go ahead and test this encryption to see if the user Stuart within this account can actually encrypt using the CMK in the primary account. Now as we can see at the top here, we’re currently logged in as Stuart, so let’s go ahead and try some encryption. Let me just flip over to S3 to try and encrypt using the CMK from the primary account. Now I have a number of buckets here, we can see that I have two buckets here, in the Ireland region. Because remember, the key was created in the Ireland region of the primary account, so I need to make sure that I’m encrypting within the same region. So if I go into this bucket here, and if I upload an object, just add in an image file, just a JPEG. Click on Next, now we have a number of different options here for S3, just gonna leave all those as default for this demonstration. Now the part that I’m interested in is on this property page. So if I scroll down, we have the encryption option here. So I want to use the AWS KMS master-key to encrypt this file. If I select on this box here, I can select a custom KMS ARN so that the key won’t actually be listed in this drop down list because it doesn’t exist within this account. So we have to use the full ARN of the CMK from the primary account. So if I select the custom KMS ARN, and then paste in the ARN of the key from the primary account. We can see here that this is a different account ID to the current account ID that I’m using showing that this is the key from the primary account. And then I can simply click on Upload. And there we have it, the file is uploaded.</p>
<p>Now if I select this file, we can see here down under encryption, we have server-side encryption enabled, and here we have the KMS ID. And this is the ARN of the key from the primary account. And we know that, because we have the primary account ID here, so that’s worked. So just to quickly run through what we’ve done again, from the primary account where the Customer Master Key existed, we edited the key policy and added the secondary account root user as a user of that key. We then went into the secondary account, and we added an inline policy for a user that we wanted to be able to use that key from the primary account, and we referenced the resource as the ARN of that CMK from the primary account. And then we went into S3 and uploaded an object using a custom KMS ARN, and again, put in the ARN of the primary key. And it’s as simple as that.</p>
<p>That brings me to the end of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-to-share-cmks-across-multiple-accounts-using-kms/sharing-cmks-across-multiple-aws-accounts/">this course</a> which explained how to share a CMK across multiple different accounts through the use of resource-based key policies and identity-based policies. If you’d like additional information on both IAM and the Key Management Service, then please take a look at the resources below.</p>
<p>Overview of AWS Identity &amp; Access Management (IAM): <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/</a></p>
<p>How to use KMS Key Encryption to Protect your Data: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/">https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/</a></p>
<p>Feedback on our courses here at Cloud Academy is valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you can contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="2Sharing-CMKs-Across-Multiple-AWS-Accounts"><a href="#2Sharing-CMKs-Across-Multiple-AWS-Accounts" class="headerlink" title="2Sharing CMKs Across Multiple AWS Accounts"></a>2<strong>Sharing CMKs Across Multiple AWS Accounts</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Course: Overview of AWS Identity &amp; Access Management (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/">Course: How to use KMS Key Encryption to Protect your Data</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30/" class="post-title-link" itemprop="url">AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:32" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:32-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:50" itemprop="dateModified" datetime="2022-11-19T22:53:50-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to this course focused entirely on how AWS KMS, the Key Management Service, can be used to encrypt your data within AWS. You will learn the basic concepts of the service through to how to manage more complex components such as configuring Key policies. </p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the details shown on the screen. Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#x70;&#x70;&#111;&#x72;&#116;&#64;&#x63;&#x6c;&#111;&#x75;&#x64;&#x61;&#99;&#97;&#100;&#101;&#109;&#121;&#46;&#99;&#x6f;&#109;">&#x73;&#x75;&#x70;&#x70;&#111;&#x72;&#116;&#64;&#x63;&#x6c;&#111;&#x75;&#x64;&#x61;&#99;&#97;&#100;&#101;&#109;&#121;&#46;&#99;&#x6f;&#109;</a>, where one of our Cloud Experts will reply to your question. </p>
<p>As this course focuses on data encryption, it’s ideally suited to those in the following roles: Cloud Security Engineers, Cloud Security Architects, Cloud Administrators, and Cloud Support and Operations. The KMS service is also heavily featured within the AWS Specialty Certification, so this could be advantageous to those who are studying for this certification. </p>
<p>The course consists of a number of theory lectures and practical demonstrations. As a result, the course is compiled as follows: </p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/what-kms/">What is KMS</a>? This lecture provides a high-level overview of encryption itself before explaining what the KMS service is, and what it is used for. </li>
<li>Key <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">Components of KMS</a>. To understand how KMS works, you need to be aware of the different components that make up the service, and this lecture explains each element. </li>
<li>Understanding <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">Permissions and Key Policies</a>. KMS is a powerful service, and so understanding how to control access is critical. This lecture focuses on how to grant access to specific keys. </li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/key-management/">Key Management</a>. This lecture looks at some of the security best practices in understanding how to maintain your Key infrastructure. </li>
<li>And the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/course-summary/">Course Summary</a>. This final lecture provides a high-level summary of the key points taken from each of the previous lectures.</li>
</ul>
<p>By the end of this course, you will able to define how the Key encryption process works, explain the differences between the different key types, create and modify Key policies, understand how to rotate, delete, and reinstate keys, and define how to import your own Key material. </p>
<p>To gain the most from this course, you should have a basic understanding and awareness of the following: AWS CloudTrail and AWS IAM, specifically relating to the understanding of policies. </p>
<p>Throughout this course, I will reference a number of URL links which will help and direct you to related information on specific topics. To make these links easily accessible to you, I have included them at the top of the Transcripts section within the lecture that they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#115;&#x75;&#x70;&#x70;&#111;&#114;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#x75;&#100;&#x61;&#x63;&#x61;&#100;&#x65;&#x6d;&#x79;&#46;&#x63;&#111;&#x6d;">&#115;&#x75;&#x70;&#x70;&#111;&#114;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#x75;&#100;&#x61;&#x63;&#x61;&#100;&#x65;&#x6d;&#x79;&#46;&#x63;&#111;&#x6d;</a>. </p>
<p>That brings me to the end of this lecture. Coming up next, I will introduce the KMS service by looking at it from a fundamental level and answering the question of: What exactly is KMS?</p>
<h1 id="What-is-KMS"><a href="#What-is-KMS" class="headerlink" title="What is KMS?"></a>What is KMS?</h1><p>Hello and welcome to this lecture where I shall explain what the KMS service is and the function that it provides to enhance your data security. </p>
<p>Before we dive into KMS itself, I want to first provide a high-level overview of encryption to help you understand which cryptography method AWS KMS uses. Unencrypted data can be read and seen by anyone who has access to it and data stored at rest or sent between two locations in transit is known as plain text or clear text data. The data is plain to see and could be seen and understood by any recipient. There is no problem with this as long as the data is not sensitive in any way and doesn’t need to be restricted. However, on the other hand, if you have data that is sensitive and you need to ensure that the contents of that data is only viewable by a particular recipient or recipients, then you need to add a level of encryption to that data. But what is data encryption? </p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Data encryption</a> is the mechanism in which information is altered, rendering the plain text data unreadable through the use of mathematical algorithms and encryption keys. When encrypted, the original plain text is now known as cipher text which is unreadable. To decrypt the data, an encryption key is required to revert the cipher text back into a readable format of plain text. A key is simply a string of characters used in conjunction with the encryption algorithm and the longer the key the more robust the encryption. This encryption involving keys can be categorized by either being symmetric cryptography or asymmetric cryptography. </p>
<p>AWS KMS only uses symmetric cryptography so let’s take a look at what this is and what this means. With symmetric encryption, a single key is used to both encrypt and also decrypt the data. So for example if someone was using a symmetric encryption method, they would encrypt the data with a key and then when that same person needed to access that data, they would use the same key that they used to encrypt the data to decrypt the data. However, if the encrypted data was being read by a different person, that person would need to be issued the same key. Remember, the same key is needed to decrypt the data that was used to encrypt it. As a result, this key must be sent securely between the two parties and here it exposes a weakness in this method. If the key is intercepted by anyone during that transmission, then that third party could easily decrypt any data associated with that key. </p>
<p>AWS KMS resolves this issue by acting as a central repository, governing and storing the keys required and only issues the decryption keys to those who have <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">sufficient permissions</a> to do so. Some common symmetric cryptography algorithms that are used are AES which is Advanced Encryption Standard, DES, Digital Encryption Standard, Triple DES and Blowfish. </p>
<p>Now let’s compare this to asymmetric encryption which involves two separate keys. One is used to encrypt the data and a separate key is used to decrypt the data. These keys are created both at the same time and are linked through a mathematical algorithm. One key is considered the private key and should be kept by a single party and should never be shared with anyone else. The other key is considered the public key and this key can be given and shared with anyone. Unlike with the symmetric encryption, this public key does not have to be sent over secure transmission. It doesn’t matter who has access to this public key as without the private key, any data encrypted with it cannot be accessed. Both the private and public key is required to decrypt the data when asymmetric encryption is being used. So how does it work? </p>
<p>If another party wanted to send you an encrypted message or data, they would encrypt the message using your own public key which can be made freely available to them or anyone. It’s public for a reason. The message is then sent to you where you will use your own private key which has that mathematical relationship with your public key to decrypt the data. This allows you to send encrypted data to anyone without the risk of exposing your private key, resolving the issue highlighted with symmetric encryption. </p>
<p>The advantage that symmetric has over asymmetric is the speed of encryption and decryption. Symmetric is a lot faster from a performance perspective. However, it does carry an additional risk as highlighted. Some common examples of asymmetric cryptography algorithms are RSA, Diffie-Hellman, and Digital Signature Algorithm. So now we know that AWS KSM uses symmetric cryptography for its encryption. Let me explain a little more about the service itself. </p>
<p>The Key Management Service is a managed service used to store and generate encryption keys that can be used by other AWS services and applications to encrypt your data. For example, S3 may use the KMS service to enable S3 to offer and perform server-side encryption using KMS generated keys known as SSE-KMS. There are different types of keys used within KMS which perform different roles and functions. I will go into detail on these key types in the next lecture.</p>
<p>Due to the nature of this service, the contents contain highly sensitive data, the key is to decrypt your private data. As a result, administrators at AWS do not have access to your keys within KMS and they cannot recover your keys for you should you delete them. AWS simply administers the underlying operating system and application. All administrative actions performed by Amazon on the underlying system require dual authentication by two Amazon administrators to make sure the action is correct and will not cause any issues with the service. As AWS has no access to your keys, it’s our responsibility as the customer and users of the KMS service to administer our own encryption keys and administer and restrict how those keys are deployed and used within our own environment against the data that we want to protect. It is important to understand that the KMS service is for encryption at rest only which can include for example S3 Object Storage, RDS, EMR and EBS Encryption to name a few. KMS does not perform encryption for data in transit or in motion. If you want to encrypt data while in transit, then you would need to use a different method such as SSL. However, if your data was encrypted at rest using KMS, then when it was sent from one source to another, that data would be a cipher text which could only be converted to plain text with the corresponding key. </p>
<p>Another important aspect of encryption at rest is whether it is done server-side by the server or client-side by the end user. Examples of server-side encryption are back end servers that encrypt the data as it arrives transparent to the end user such as the example I gave earlier with SSE-KMS. The overhead of performing the encryption and managing the keys is handled by the server, in this case S3, not by the client-side application or the end user. Client-side encryption is quite different. Client-side encryption requires the user to interact with the data to make the data encrypted and the overhead of encryption process is on the client rather than the server. </p>
<p>When working with encrypted data, compliance and regulations are often tightly integrated. As a result, KMS works seamlessly with AWS CloudTrail to audit and track how your encryption keys are being used and by whom in addition to other metadata captured by the APIs used such as the source IP address, et cetera. The CloudTrail logs that are stored in S3 record KMS API calls such as Decrypt, Encrypt, GenerateDataKey and GetKeyPolicy among others as shown on the screen. </p>
<p>When architecting your environment with regards to data encryption, you need to be aware that AWS KMS is not a multi-region service like IAM is for example. It is region specific. Therefore, if you are working in a multi-region system with multi-region failover, you need to establish a Key Management Service in each region that you want to encrypt data. </p>
<p>Now we have a base understanding that KMS generates and provides a secure central repository of encryption keys to allow you to encrypt your data at rest or integrating with numerous AWS services. In the next lecture, I want to expand the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">components that makeup the service</a>.</p>
<h1 id="Components-of-KMS"><a href="#Components-of-KMS" class="headerlink" title="Components of KMS"></a>Components of KMS</h1><p>Hello, and welcome to this lecture where I’m going to drill down into the different elements and components that make up the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">KMS service</a> to allow you to understand how it operates and functions. This section will consist of customer master keys, data keys, or data encryption keys, key policies and grants. </p>
<p>Let me start off by explaining the CMK, the customer master key. This is the main key type within KMS. This key can encrypt data up to 4 kilobytes in size, however it is typically used in relation to your data encryption keys, DEKs. The CMK can generate, encrypt and decrypt these DEKs, which are then used outside of the KMS service by other AWS services to perform encryption against your data.</p>
<p>It’s important to understand that there are two types of customer master keys. Firstly, those which are managed and created by you and I, as customers of AWS, which can either be created by using KMS or by importing key material from existing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/key-management/">key management applications</a> into a new CMK. And secondly, those that are managed and created by AWS themselves. The CMKs which are managed by AWS, are used by other AWS services that have the ability to interact with KMS directly to perform encryption against data. For example, Amazon S3, in particular, SSE KMS, these AWS managed keys can only be used by the corresponding AWS service that created them within a particular region. As remember, AWS KMS is a regional service. </p>
<p>These CMKs that are used by the services, are generally created the first time you implement encryption using that particular service. CMKs that are generated and created by us, rather than AWS, provide the ability to implement greater flexibility, such as being able to manage the key, including rotation, governing access and key policy configuration, along with being able to both enable and disable the key when it is no longer required. Now there is a big difference between disabling a CMK and deleting a CMK, which can have significant effect on being able to access your data, governed by the CMK that encrypts it. I shall discuss more on these differences in an upcoming lecture, where I’ll talk about key management. </p>
<p>Like the AWS managed CMKs, AWS services can be configured to use your own customer CMKs, too. Depending on the service and your own internal security policies, the choice is yours as to which CMK the supported services uses to encrypt your data. Any CMKs created within KMS are protected by FIPS, validated cryptography modules. As I mentioned in the previous few slides, data encryption keys, or data keys, are created by the CMK and are used to encrypt your data of any size. When a request to generate a key is issued, the CMK specified in the request will create a plain text data encryption key and an encrypted version of the same data encryption key. Both of these keys are then used to complete the encryption process. As a part of this process, your plain text data is encrypted with the plain text data key using an encryption algorithm. Once encrypted, the plain text data is deleted from memory and the encrypted data key is stored alongside the encrypted data. If anyone gains access to the encrypted data, they will not be able to decrypt it, even if they have access to the encrypted key, as this key was encrypted by the CMK, which remains within the KMS service. This process of having one key encrypted by another key is known as envelope encryption. The only way you would be able to decrypt the object is if you have the relevant decrypt permission for that CMK that the data keys are associated to. </p>
<p>Let me explain the process of how these data keys are used with the CMK outside of KMS, such as S3. And I shall stick with the example of SSE KMS, server side encryption, using KMS keys.</p>
<h3 id="Start-of-SSE-KMS-Sketch-Demonstration"><a href="#Start-of-SSE-KMS-Sketch-Demonstration" class="headerlink" title="Start of SSE-KMS Sketch Demonstration"></a><em><strong>Start of SSE-KMS Sketch Demonstration</strong></em></h3><p>Hello, and welcome to this Cloud Academy sketch video. I’m going to be talking about S3, and in particular, server side encryption with KMS, which is the key management service. So I’m going to show you how the process works from both an encryption point of view and also decryption. Let’s start by creating our user over here. And let’s call our user Bob, keep it nice and simple. </p>
<p>Now, Bob has a document that contains sensitive information. So when he stores it on his S3 bucket, he wants to make sure it’s encrypted, and he’s going to select server side encryption using KMS. So S3 will handle the encryption process for him using keys that are generated by the KMS service. So when he uploads it to his S3 bucket, he specifies that he wants to use SSE KMS. At this point S3 realizes that it needs to invoke services from the KMS service. So it calls upon KMS to generate some data keys for us. So it then sends a request over to KMS. At this point, KMS uses the customer master key, the CMK, to generate two keys. Now the first key is just a plain text data key, and the second key that’s generated is the same key but an encrypted version of that key. </p>
<p>Now both of these keys are then sent back to S3. So S3 receives both those keys, both the plain text key and the encrypted data key. Now S3 has all the keys it needs to perform the encryption process. So S3 will take the object uploaded by Bob, which is his document. It will then combine this with the plain text data key, and it will perform an encryption algorithm, and then that will generate an encrypted version of Bob’s document. So now his document is encrypted. And S3 will store and associate the encrypted data key alongside this object. So these two objects are then stored on S3. Meanwhile, the plain text data key is then deleted from memory. So that’s how encryption works using SSE KMS. </p>
<p>So just to recap. The end user or client will upload the object to S3, specifying that SSE KMS should be used. S3 will then contact KMS, and using the specified CMK, it will generate two data keys, a plain text data key and an encrypted version of that data key. Both of these keys are then sent back to S3, at which point S3 can then encrypt the object that was uploaded, using the plain text data key, to generate an encrypted version of your object. And then S3 will associate and store the encrypted data key alongside your encrypted object. </p>
<p>Okay, so let’s now look at how the decryption process works. So if we just get rid of some of this detail. So Bob will request the object from S3. At this point, S3 knows that the object is encrypted and it has the associated encrypted data key. So what it does, it sends that associated encrypted data key over to KMS, and it asks KMS to generate a plain text data key. So what it’ll do, it’ll use the same CMK plus the encrypted data key. And this will generate a plain text version of that data key. Now just this single plain text data key is then returned to S3. At this point, S3 can then access the encrypted object and use the plain text data key that it just got back from KMS, perform an encryption algorithm again to decrypt the object. And this will generate a plain text version of the object, at which point this can then be returned to Bob. </p>
<p>So just to recap. The user will request access to the encrypted object. S3 will then send the associated encrypted data key to KMS, to generate a plain text version of that encrypted data key, using the associated CMK. This plain text data key is then sent back to S3. The plain text data key is then used to decrypt the encrypted object, generating a plain text version of the object, which can then be returned to the user. So that’s how the encryption process works for S3, SSE KMS. Thank you.</p>
<h3 id="End-of-SSE-KMS-Sketch-Demonstration"><a href="#End-of-SSE-KMS-Sketch-Demonstration" class="headerlink" title="End of SSE-KMS Sketch Demonstration"></a><em><strong>End of SSE-KMS Sketch Demonstration</strong></em></h3><p>The key policy is a security feature within KMS that allows you to define who can use and access a particular key within KMS. These policies are tied to the CMKs, making these resource-based policies, and different key policies can be created for different CMKs. These permissions are defined within this key policy document, which is JSON-based, much like IAM policies are. I will cover key policies in far greater depth in the next lecture, when <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">I discuss permissions </a>and access control. </p>
<p>Grants are another method of controlling access and use of the CMKs held within KMS. Again, they are a resource-based policy, but they allow you to delegate a subset of your own access to a CMK for principals, such as another AWS service within your AWS account. The benefit of this is that there is less risk of someone altering the access control permissions for that CMK. If anyone has the KMS put key policy permission, then they could simply replace the key policy with a different one. Using grants eliminates this possibility, as a grant is created and applied to the CMK for each principle requiring access. Again, coming up in the following lecture, I shall dive deeper into grants and how they work. </p>
<p>That now brings me to the end of this lecture, which leads me nicely onto permissions and access control within KMS. And I shall look at key policies and grants in greater detail.</p>
<h1 id="Understanding-Permissions-amp-Key-Policies"><a href="#Understanding-Permissions-amp-Key-Policies" class="headerlink" title="Understanding Permissions &amp; Key Policies"></a>Understanding Permissions &amp; Key Policies</h1><p>Hello and welcome to this lecture where I will be diving deeper on how to secure access to your KMS keys and associated levels of permission. </p>
<p>With many services that you use within AWS, you can control access using IAM policies whether that is against a user, group, role or even a federated user. The point is access control for most services can be completely controlled and governed by using IAM alone. For KMS however, this is not the case. In all cases, to manage access to your CMKs, you must use a key policy. Without a key policy associated to your CMK, users will not be able to use it. </p>
<p>Permissions to allow you to access and use your CMK can’t be given and generated using IAM alone. As a result, there are a number of different ways in which you can control access to your keys in KMS, just by using key policies, using key policies with IAM, and using key policies with grants. Let me run through each of these options starting with key policies. </p>
<p>As I briefly mentioned <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">in the previous lecture</a>, key policies are resource based policies which are tied to your CMK. And if you want a principal to be able to access your CMK, then a key policy must be in place to do so. The key policy document itself is JSON based much like IAM policies are and the document syntax is much like other IAM policies. They contain elements such as resource, action, effect, principal and optionally conditions. As a result, they typically look like the following. </p>
<p>During the creation of a CMK, whether you create it programmatically or if you create it through the AWS Management Console, either way KMS will create a default key policy for you to allow principals to use the CMK in question as remember a key policy is required for all CMKs. Within both of these default key policies, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">KMS</a> will configure the root of the AWS account full access to the CMK, by doing so ensures that the CMK will never become unusable as it’s not possible to delete the root account. If full access of the CMK was given to another user and then that user was deleted from IAM, it would not be possible to manage the CMK unless you contacted AWS support to regain the control required. Also, by allowing the root account full access to the CMK performs another important and useful function. It allows access to be given to the CMK by normal IAM policies for users and roles, etc. That’s why it’s not possible to allow users access to a CMK without a key policy. Without the root account having full access in the key policy, IAM can’t be used to manage access for other users. </p>
<p>Within the key policy, this section would look as follows. The resource section shows an asterisk which essentially means this CMK that the key policy is being applied to. When you create your CMK through the Management Console, then you have the chance to configure different permission sets. These include key administrators of the CMK and users which are allowed access to the CMK. </p>
<p>You’ll be asked to firstly define the key administrators. These can either be users or roles that you have setup and configured within IAM. These principals can only administer the CMK and not use it to perform any encryption function that may be needed within the CMK. The administrative actions that are allowed by the key admins are shown on screen. During the selection of key admins, you can also specify if you would like them to be able to delete the key via a checkpoint option. An important point to bear in mind is that although these key administrators do not have access to use the CMK, they do have access to update the associated key policy. As a result, if they wanted to, they could give themselves access to use the CMK. </p>
<p>Next you’ll be asked which users you want to allow to use the CMK. And by use, it’s essentially asking which users should be allowed to perform any encryption using the CMK. In addition to these permissions, the users associated will also be able to use grants to delegate a subset of their own permissions to another service which integrates with KMS. For example, if you are attaching an encrypted EBS volume to an EC2 instance, then through the use of grants, you as the key user of the CMK which encrypted the EBS volume implicitly give permission to EC2 to be able to attach that encrypted EBS volume. Within the key policy, the grant permissions look as follows. For every user or role selected as a user of the key will be displayed within the principal section. As you can see from this example, the only user currently selected for this key is Cloud Academy. The permissions given to use the key for any user selected are as follows. </p>
<p>As the key policy uses the same syntax as IAM policies, you can also restrict access to the CMK in the same way that you can restrict access to resources via the Effect element. Instead of using the Effect Allow, you can instead restrict and deny a particular user access to a CMK by specifying the Effect as deny. As mentioned, we can also use key policies in conjunction with IAM policies, but only if you have the following entry within the key policy allowing the root full KMS access to the CMK, by doing so enables you to centralize your permissions administration from within IAM as you would likely be doing for many other AWS services. This would mean you can configure your IAM policies to allow users, groups and roles to perform the encryption and decryption process, for example using the KMS Encrypt and KMS Decrypt permissions. </p>
<p>Using the resource component within the policy, you can also specify which CMKs the user, group or role can use to perform the encryption and decryption process. In this example, we can see that the policy will allow the identity associated with the policy to use two different CMKs to encrypt and decrypt data. The first CMK is within the US-East-1 region and the second CMK is within the EU-West-2 region. </p>
<p>Finally, you can assign permissions using grants alongside key policies. I’ve mentioned grants a couple of times already, but in essence, they allow you to delegate your permissions to another AWS principal within your AWS account. Much like the key policy, grants are another resource based method of access control to the CMKs. The grants themselves need to be created using the AWS KMS APIs. It’s not possible to create them using the AWS Management Console and these grants are then attached to the CMK, much like key policies are. Within the key policy of the CMK, you will see the Sid which allows the users of the CMK to also perform three grant actions, kms:CreateGrant, kms:ListGrants and kms:RevokeGrant. </p>
<p>When issuing the CreateGrant API, a number of different parameters are also issued such as the CMK identifier. The grantee principal to gain the permissions and the required level of operations that the grantee can perform include the following on screen. Once the grant is active, the principal or grantee can then adopt the permissions programmatically based on the level of access provided within the grant. Also, after the grant has been created, a GrantToken and a GrantID is issued. When a grant is created, there may be a delay in being able to use the permissions and this is due to the fact that eventual consistency has to take place. To get around this, you can use the GrantToken with some APIs which would allow the grantee to perform the operation specified within the grant with immediate effect without having to wait for eventual consistency to complete.</p>
<p>I gave an example earlier relating to EC2 and EBS whereby another AWS service can use grants to gain access to and use a CMK. I will now provide a demonstration of how the grantee of a grant can be an IAM user to show how grant permissions work. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration 1"></a>Start of demonstration 1</h3><p>Okay so for this demonstration, we’re going to use two different users, one user that has access to user CMK and create grants and another user that has no IAM policies at all and isn’t explicitly allowed access within the CMK policy either. So let’s go ahead and take a look at the CMK we’re going to be using first. Go down to encryption keys and we’re going to be using this demo key. If we have a look at the policy of this CMK, scroll down and take a look, then we can see that the users within this policy are listed here and we have a user called Alice and Alice can also create grants as well as we can see down here and here’s the permission CreateGrant. So we’re going to be using the user Alice. </p>
<p>So let’s now create an entirely new user with no permissions at all. We’ll call this user Bob. We’ll give them programmatic access because we’ll be using the AWS CLI for this demonstration. We won’t create any permissions or attach them to any groups. And as we could see here, the user has no permissions. Create user and we have their access key here and their secret access key. </p>
<p>So what I’m going to do now I’m going to swap across to my terminal and I’m going to set them up a profile on my AWS CLI. Okay so what I need to do is type in aws configure –profile of Bob and this will ask us for the access key ID so we can copy and paste that in and the same for the secret access key ID. It will then ask us for the default region name. Our CMK is in us-east-1 so it’ll accept that as the default and also the default output format. So now we have two users, Alice and Bob, and Alice is already setup as a profile on the CLI and we’ve just setup Bob as well. As we know, Bob doesn’t have any permissions within the key policy of the CMK and he doesn’t have any permissions within IAM either. </p>
<p>So let’s just do a quick test with Bob. We’ll try and encrypt something using the CMK ID and it will fail so let’s just take a look at that. So if I type in aws kms encrypt using plaintext and we’ll encrypt CloudAcademy using the key-id alias of DemoKey which is the name of my CMK using the profile of Bob and there we can see an error has occurred access denied. That’s because Bob doesn’t have any encrypt permissions. So if we do the same but this time with Alice, so if we change the profile to Alice, then we can see that it has encrypted the file using this CiphertextBlob here. So that file is now encrypted and Alice can do it because she is listed as a user within the key policy for that CMK. </p>
<p>Okay, let me just clear the screen. So now what I want to do is use grants to allow Bob access to encrypt and decrypt data and this would be issued by the user Alice. So the command to do this is as follows so it’s aws kms create-grant using the key-id and we need the ARN of our key here so if we just go across, select our CMK, we have our ARN here so paste that in. We’ll then need to add the grantee-principal and this will be the person who is actually going to gain this access and again we need the ARN of the user. I’ll go ahead and find Bob, copy his ARN, paste that in. Then we need to specify the operations which is the actual access that’s going to be given so we’re want to grant encrypt and decrypt access and this is going to be issued by the profile of Alice. So Alice is using grants to allow Bob to give access of encrypt and decrypt which is access that Alice has. And we could see that that now works. </p>
<p>We now have a GrantToken and also a GrantId as well. And we can use this information to then perform encryption and decryption straight away using the user Bob. So let me now show you how that works. So now we have the GrantToken and the GrantId. What we can do is encrypt or decrypt data. So let’s go ahead and try and encrypt a new file with Bob’s profile. So to do this we type in aws kms encrypt –plaintext and “Friday”. We add the key-id. We have the key-id up here so I’ll just copy and paste that. And then we need to issue the grant-tokens and here I need to add in the GrantToken that was generated which is this block of text so I’ll paste that in. And then finally because we want to encrypt with Bob, we’ll need to add Bob’s profile at the end. As we can see, we have successfully encrypted that file of Friday. So whereas before Bob was unable to encrypt, we have now used the GrantToken which was generated by Alice to allow Bob access to encrypt data. </p>
<p>So just to run through what we’ve done in this demonstration, we had Alice who is a user within the key policy of a CMK. We then created a new user Bob who had no IAM policies attached and he wasn’t listed within the key policy. We then tested that Bob wasn’t able to encrypt anything. We then confirmed that Alice could. We then created a grant using Alice’s profile allowing access to Bob for encrypt and decrypt and there are other permissions there as well such as generate data keys, etc. We just allowed encrypt and decrypt. We then used the GrantToken that was generated from the create-grant to allow Bob to encrypt a file using the new permissions that he gained through the grant. So once you understand the methodology of how you can grant these additional permissions, it’s a fairly simple process to go ahead and allow other principals to use the CMK based on the permissions that you have. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration 1"></a>End of demonstration 1</h3><p>I now want to show you how to create a CMK using the Management Console from scratch and what the final key policy looks like and how key administrators, users and grants all fit in. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration 2"></a>Start of demonstration 2</h3><p>Okay so I’ve just logged in to my AWS account and first of all I’ll need to go to Identity and Access Management which is where KMS can be found. And if we go down the left-hand side on the menus down to encryption keys and this is essentially KMS. This is the KMS function within IAM. Now to create a new CMK, we go up to the blue button here that says create key and we need to add an alias and a description so I’m just going to call this DemoKey and description This is a demo. If we click on advanced options, here we can see different key material origins and I’ll talk more about the difference between KMS and external origins in the following lecture. So for this demonstration, I’m just going to select the key material origin of KMS and this will allow KMS to generate the key material. If we click on next step, here we can add tags. For this demonstration, I’m just going to leave them as blank. </p>
<p>And now it’s asking us to define the key admins, so essentially who should be the administrators of these keys. And remember as I explained earlier, the key administrators aren’t actually able to use the CMK to encrypt and decrypt data. So let me just pick a couple of users here, go down to the next step, and now it’ll ask me to define the users of the CMK. So these are the identities that can actually perform encryption and decryption using this CMK. So again I’ll just pick a couple of users, click on next step, and now we can look at the key policy that’s being created based on the parameters that I have selected. </p>
<p>So if we look at the different sections here, firstly this Sid Enable IAM User Permissions so what we have here is an Allow for the root account or KMS access to this particular CMK. So as I discussed earlier, this entry within the policy ensures that you can use IAM policies to govern access to this CMK. If we scroll down, the next section shows the key administrators for this CMK and here are the two identities that I selected. And if we look at the actions, we can see a number of different KMS actions, but there is no kms:Encrypt and no kms:Decrypt because the key administrators aren’t actually able to use the key for encryption. We scroll down to the next section of the policy and this section defines the actual users of the policy so whoever is listed here under the principal section can actually use this CMK to encrypt data and decrypt data. And if we look at the actions, we can see here kms:Encrypt and kms:Decrypt and GenerateDataKeys, etc. Whereas you’ll find that these permissions aren’t actually within the administrator list. As you can see, there’s no kms:Encrypt. So that’s the users of the CMK. And finally, this section relates to grants. So this also allows whoever has been added to the user section above, which are these two accounts here, it allows them to create grants and generate grants for the CMK. And then if we click on finish, that’s it. </p>
<p>The CMK is created and we can see here that the status is enabled. And if we click on the DemoKey, it would just bring up some different information. So we have the ARN of the key itself. We can see that the status is enabled. And if we scroll down, we can see the key administrators. And there’s a checkbox here that allows the key administrators to delete this key. If you don’t want a key deleted, then you can simply uncheck that box. Scroll down further, we can see the users of the key as well. Any tags that you added would also be there. And that’s it, it’s as simple as that. So it’s a very simple method to create your CMK, give it a name, add any tags that you need, add the key administrators, add the users, and then select finish, and then you will have your CMK. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration 2"></a>End of demonstration 2</h3><p>The overall access of understanding who has access to a CMK in KMS can be a little confusing as there are three potential ways of gaining access and using a CMK, through the key policy, with IAM policies and also Grants. And determining the correct level of access means you need to understand how they all work together. So let’s look at a simple example to ensure we understand some key points. In this scenario, we have three CMKs and four users. </p>
<ul>
<li>CMK-A key policy provides access to the root account which as we know also enables IAM policies to be used in conjunction with the key policy. </li>
<li>CMK-B key policy allows access to Bob and Charlie. </li>
<li>CMK-C key policy provides access to the root account. Access is explicitly denied for Bob, Charlie and David, but full access is given to Alice. </li>
<li>Alice’s IAM policy allows all KMS actions to CMK-A and CMK-B. </li>
<li>Bob has no IAM policy</li>
<li>Charlie’s IAM policy allows KMS encrypt access to CMK-A </li>
<li>and David’s IAM policy allows all KMS actions to CMK-B and CMK-C.</li>
</ul>
<p>So let’s now look at each of these user’s access and what they have access to, starting with Alice. </p>
<p>Alice’s access to CMK-A is successful as her IAM policy allows all KMS actions against CMK-A and CMK-A allows for IAM policies to be used. Her access to CMK-B provides no access as the key policy for this CMK does not allow for IAM policies to be used. And her access to CMK-C is successful as the key policy allows her access despite her having no IAM policy relating to permissions. </p>
<p>Now let’s take a look at Bob. His access to CMK is denied as there are no explicit entries in the key policy for Bob’s access and he has no IAM policy. His access to CMK-B is successful as the key policy allows him access despite him having no IAM policy relating to permissions and access is denied to CMK-C due to explicit deny actions within the key policy and an explicit deny will always overrule any other allow. </p>
<p>Now let’s look at Charlie’s access. For CMK-A, he has encrypt access only which is given through his IAM policy and IAM policy permissions are allowed. For CMK-B, access is also successful as the key policy allows him access. His IAM policy permissions are irrelevant as the CMK does not allow for IAM policies to be used. And his access to CMK-C is denied due to the explicit deny actions within the key policy and an explicit deny will overrule any other allow. </p>
<p>And finally David’s access. He has no access to CMK-A as neither the key policy or his IAM policy provides permissions. He has no access to CMK-B as the key policy for this CMK does not allow for IAM policies to be used and access is also denied to CMK-C due to explicit deny actions within the key policy. </p>
<p>That now brings me to the end of this lecture covering access control permissions for KMS. Coming up next, I shall be looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/key-management/">key management within KMS</a>.</p>
<h1 id="Key-Management"><a href="#Key-Management" class="headerlink" title="Key Management"></a>Key Management</h1><p>Hello and welcome to this lecture where I shall be explaining how to manage your keys within KMS. I will look at rotation of CMKs, How to import key material from an existing key management system outside of AWS, and the deletion of CMKs. So let me start off by looking at how rotation works within your CMKs. </p>
<p>Firstly, why do we need to rotate keys? The simple answer is for security best practice reasons. Largely, the longer the same key is left in place, the more data is encrypted with that key, and if that key is breached, then the wider the blast area of data is at risk. In addition to this, the longer the key is active, the probability of it being breached increases. </p>
<p>The simplest method of rotating keys is to use automatic key rotation, whereby KMS will automatically rotate your keys every 365 days. But what does that mean, exactly? When automatic key rotation is enabled, many of the details of the CMK remain the same, such as the CMK-ID and the ARN, along with any <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">associated permissions and policies</a>. The only thing that changes is the backing key of the CMK. This backing key is the fundamental cryptographic element that is used when the encryption process is taking place. During the rotation, older backing keys are retained to decrypt data that was encrypted prior to this rotation. </p>
<p>One important point to bear in mind is that should a breach of the CMK occur, rotating the key would not remove the threat of that breach. Consider the following scenario. You have encrypted 100 objects which are stored on S3, using SSE-KMS with a customer-managed CMK. It has been made apparent that a breach of your cryptographic material, of your CMK, has occurred, and you decide to rotate the key. As we now know, rotating the key simply creates a new backing key, and retains the older key to allow you to continue to decrypt existing data, encrypted by the original backing key. So by rotating the key, doesn’t prevent the malicious user from decrypting the 100 objects. However, by rotating the key, it does stop them from decrypting any future encryptions of objects, as it now has a new cryptographic material of the backing key. </p>
<p>There are a couple of points to bear in mind when it comes to automatic key rotation. Firstly, Automatic key rotation is not possible with imported key material, and secondly, the key rotation happens every 365 days and there is no way to alter that time frame. If these two points are an issue, then the only solution is to perform a manual key rotation, which would give you greater flexibility and control of how and when you perform rotations of your keys. If your CMK is the state of disabled or pending deletion, then KMS will not perform a key rotation. If the key is re-enabled, or if the deletion process is canceled, then KMS will assess the age of the backing key, and if that key is older than 365 days, the automatic key rotation process will rotate the key immediately. </p>
<p>One final point on automatic key rotation is that it’s not possible to manage the key rotation for any AWS managed CMKs. These are by default rotated every 1095 days, which is essentially three years. Let me now look at manual key rotation, and how this differs from automatic key rotation.</p>
<p>The process of manual key rotation requires a new CMK to be created, which is different from the automatic process. By doing so, a new CMK-ID is created along with a backing key. As a result, if you have any applications that reference the CMK-ID of the old key, you will need to update them and point them to the new CMK-ID. To make things easier in this process, you could use alias names for your keys, and then simply update your alias target to point to the new CMK-ID. To do this, you can use the update alias API, as shown here. </p>
<p>Once you have performed a manual key rotation, you should keep any CMKs that were used to encrypt data before it, as KMS will use the same CMK that it did to encrypt it. So, for example, let’s say you had a document A encrypted with CMK-A, using the backing key of that CMK to perform the cryptographic mechanism. You then performed a manual key rotation to comply with your own internal security policies. In doing so, a new CMK was created, CMK-B, and this CMK then encrypted document B. You then deleted or disabled CMK-A. When Alice tries to retrieve document A in plain text format, the operation fails. This is because <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">KMS</a> knows which CMK was used to encrypt the object, and it must use the same CMK to decrypt it. However, if it is disabled or deleted, then this is not possible. To resolve this issue, the older CMKs must remain active. </p>
<p>When I was just talking about key rotation, I mentioned that it’s not possible to perform automatic key rotation of imported key material, but what is this key material? Key material is essentially the backing key, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">component</a> that completes and implements the encryption and decryption process on behalf of the CMK itself. When customer-managed CMKs are generated and created within KMS, the key material is automatically created for the CMK. However, it is possible to create CMKs without any key material at all. This is done by selecting the External option for the Key Material Origin, under the Advanced Options of the Create Alias and Description page of the CMK creation within the management console. By doing so, you can still create a CMK which will have its own ARN and CMK-ID. However the actual key material for the CMK can then be imported from your own key infrastructure that you may be using with your own on-premise material that you have already been using. When using your own key material, it becomes tied to that CMK, and no other key material can be used for that CMK. This is why it’s not possible to enable automatic key rotation, as only this single backing key or key material can ever be used for that CMK. </p>
<p>The process for importing your own material follows four key points. The first is, as I have already mentioned, creating your CMK with no key material generated by KMS by selecting External for your key origin. Following this, you will then be asked to download a wrapping key, which is also referred to as a public key, and an import token. This wrapping key is to allow you to upload your key material in an encrypted form. When you import your key material, you do not want this to be in plain text format, So AWS KMS provides a means of encrypting it with this public&#x2F;wrapping key. During this step, you will be asked to select an encryption algorithm. The options for this are as shown. AWS recommends that you select option A that’s shown if possible. If not, then to use option B, and lastly, C. The wrapping key then uses this method of encryption to encrypt your key material before it’s uploaded. The import token that is downloaded is used as a part of the import process when uploading your encrypted key material, to ensure that the process completes correctly. Both the wrapping key and the import token is only active for 24 hours once you have downloaded it, so you need to ensure that you complete the process within this timeframe, otherwise you will need to complete this step again, and download a new key and import token. </p>
<p>Once you have downloaded the wrapping key and import token, you are then ready to encrypt your key material that you’ve extracted from your own key management system or hardware security module. One point regarding this step is that the key material must be in a binary format to allow you to use the wrapping key. For detailed information on the commands to carry out this step, you can visit the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-encrypt-key-material.html">link</a>. The final step is to actually import your key material that is now encrypted into KMS and to associate it with your currently empty CMK. This step can be performed programmatically, or from within the AWS Management Console, whereby you need to select your CMK, and select to import the key material. At this point, you would then be prompted for the location of the key material, along with the location of the import token. Optionally, you also have the option of setting an expiration of the key material being imported. If you do set an expiry, CMK would cease to operate when this expires. To activate the CMK again, you would need to re-upload the key material, following steps two to four. </p>
<p>There are some considerations of using your own key material that differ from key material generated by KMS, which is largely down to durability and availability. The key material created by KMS for customer-managed CMKs receives a far higher durability and availability than that of your own key material that has been imported. For example, it’s not possible to set an expiration time with key material generated by KMS, but you can for your own imported material. When this expires, the key material is deleted, but the CMK and metadata of that CMK is retained. Also, if a region-wide failure occurred where your keys were being used and stored, you would need to ensure that you had the key material to import back into the CMK, as KMS would not be able to retain this information. </p>
<p>I now want to talk about the process involved with deleting CMKs. When you no longer have a need or requirement for a particular CMK, you may want to delete it for security best practices and general housekeeping of your key infrastructure. Deleting a key, of course, can have significant impact against your operations and data if there are services that are still using it without your knowledge. To help in this scenario, KMS enforces a scheduled deletion process, which can range from seven to 30 days. When a key is scheduled for deletion, the CMK is taken out of action and put in a state of Pending deletion. When a key is in this state, it can’t be used to perform any encryption or decryption actions, neither can the backing keys for the CMK be rotated. You could also analyze AWS CloudTrail event logs to look for events relating to the use of your CMK, such as an encrypt action against the CMK-ID. </p>
<p>AWS also recommends that you set up a Cloudwatch alarm to identify if anyone tries to use this key to perform an encryption or decryption request. This will help you identify if it is, in fact, still in use, allowing you to cancel the deletion. Details on how to configure this can be found using the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-creating-cloudwatch-alarm.html">link</a> on screen. If you are not confident that your CMK is no longer in use, or that it should be deleted, then you can simply disable the CMK. Again, this will change the key to a state of disabled and prevent it from being used, and, again, prevent the backing key from being rotated. If you are using a CMK which has your own key material imported, then you can delete just the key material from the CMK, as well as having the option of deleting the entire CMK. </p>
<p>I’m now going to perform a quick demonstration to show you how to schedule a deletion of a CMK, and how you can disable and re-enable CMKs via the AWS Management Console. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so for this demonstration, I’m going to show you how to schedule a deletion of a CMK, and then also how to disable and re-enable a CMK. So, again, if we go to IAM. Go down to Encryption keys, which is essentially where KMS lives, and as you remember from the demonstration earlier, where we created the DemoKey, so now I’m going to schedule a deletion for this key. </p>
<p>So if I select the key, and go across to Key actions, and then down to Schedule key deletion, now here you can enter a waiting period between seven and 30 days, so the minimum is seven and the maximum is 30 days. So this will give you a buffer to ensure that you do actually want to delete this key, and give you enough time to ascertain and see if it’s still in use. So, let’s just put in seven days there, go down to Schedule deletion. And we can see here, that the Status is now changed to Pending Deletion. So it won’t be possible to use this CMK anymore, and it won’t be rotated either. </p>
<p>If you change your mind, and you don’t want this CMK deleted, you can just select it again, go to Key actions, and Cancel key deletion. You’ll see at this stage that it puts the key in this state of Disabled, so what you need to do, again, is to select the key, go to Key actions, and simply Enable the key and then it will be fully operational again, and you can use that CMK to encrypt and decrypt data. </p>
<p>You may have noticed in the Key actions there, let me just select the key once more, to Disable the key, there’s an option there, so we can just go ahead and Disable that, and you can see that the Status has changed, and, again, if you need to re-enable it, just simply click on Enable. So to disable and enable the key is very simple, simply selecting the CMK and then just selecting the appropriate option from the Key actions box. And that’s it, so that’s how you can schedule a deletion for your CMK and also disable and enable the key as required.</p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Hello and welcome to this final lecture in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">this course</a> where I shall provide a summary of the key points taken from each of the previous lectures. </p>
<p>I started this course by explaining <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/what-kms/">what exactly KMS was</a> and a high-level overview of Symmetric and Asymmetric cryptography. In this lecture, we learned that unencrypted data is known as plaintext or cleartext, and data encryption is the mechanism in which information is altered, rendering the plaintext data unreadable, which is then known as ciphertext. To decrypt encrypted data, an encryption key is required to revert the ciphertext back into a readable format of plaintext. A key is simply a string of characters used in conjunction with the encryption algorithm. Symmetric cryptography uses a single key, which is used to both encrypt and also decrypt the data. Asymmetric cryptography uses two separate keys. One is used to encrypt the data, and another separate key is used to decrypt the data. One key is considered the private key and the other is considered the public key. KMS is used to store and generate encryption keys that can be used by other AWS services and applications to encrypt your data. AWS administrators do not have access to your keys within KMS and they cannot recover your keys for you should you delete them. KMS is used for encryption at rest only, and KMS works seamlessly with AWS CloudTrail to audit and track how your encryption keys are being used and by whom.</p>
<p>Following this lecture, I then moved on to talking about the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">different components of KMS</a>, and the main points from this lecture were as follows. Customer Master Keys, CMKs, are the main key type within KMS. CMKs can generate, encrypt, and decrypt data encryption keys known as DEKs. Data encryption keys are used outside of the KMS service by other AWS services to perform encryption. And there are two types of Customer Master Keys, customer managed and AWS managed. Customer managed CMKs provide greater flexibility, such as being able to manage the key, including rotation, Key policy configuration, and enable and disable the key. Any CMKs created within KMS are protected by FIPS, validated cryptographic modules. Data encryption keys are created by the CMK and are used to encrypt your data of any size. The CMK can create a plaintext data encryption key and an encrypted version of the data encryption key, and both of these keys are then used to complete the encryption process. Plaintext data is encrypted with the plaintext data key and plaintext data keys are deleted from memory after use. Envelope encryption is the process of having one key encrypted by another key, and Key policies are a security feature that allows you to define who can use and access a particular key within KMS. Key policies are tied to CMKs as a resource-based policy, and Grants allow you to delegate a subset of your own access to a CMK for principals. </p>
<p>Next I covered and explained the different methods of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">implementing permissions against your CMKs</a>, and throughout this lecture, I explained that access to use CMKs is not possible through the use of IAM policies alone. To manage access to your CMKs, you must use a Key policy. And there are three different ways of granting access to the use of a CMK, Key policies, Key policies with IAM policies, and Key policies with Grants. Looking at Key policies, there are resource-based policies which are tied to your CMK, they are JSON-based, much like IAM policies are, the syntax follows the same as IAM policies. A Key policy is required for all CMKs, and KMS will configure the root user of the AWS account full access to the CMK which allows access to be given to the CMK by normal IAM policies for users and roles. Without the root account having full access in the Key policy, IAM can’t be used to manage access for other users. Key Administrators and Users can also be configured within the Key policy. And Key Administrators can only administer the CMK and not use it to perform any encryption functions. Users of the CMK define which users should be allowed to perform any encryption using this CMK, and users are also able to use Grants. Using Key policies with IAM policies. Key policies can only be used with IAM policies if you have the following entry within the Key policy allowing the root full KMS access to the CMK. Using Key policies with Grants, you can also assign permissions using Grants alongside Key policies. Grants allow you to delegate your permissions to another AWS principal within your account, Grants need to be created using the AWS KMS APIs, Grants are attached to a CMK, much like Key policies are, and to create a Grant, you use the Create-Grant API. When creating the Grant, you need to specify the CMK identifier, the grantee principal to gain the permissions, and the required level of operations that the grantee can perform. Permissions can then be adopted programmatically by the grantee, and Grants generate a GrantToken and a GrantID. And GrantTokens allow the GrantID to perform the operations with immediate effect without having to wait for eventual consistency to complete. </p>
<p>The final lecture focused on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/key-management/">how to manage your keys and their lifecycle</a>. This lecture highlighted the following points. Key rotation follows security best practices, and the simplest method of rotating CMKs in KMS is to use this automatic key rotation. And the automatic key rotation rotates keys every 365 days. During the automatic key rotation, the only part that changes of the CMK is the backing key, and the backing key is the cryptographic element that is used when the encryption process is taking place. All existing backing keys are retained during and after rotation, and rotating keys does not remove the threat of a security breach. Automatic key rotation is not possible with imported key material. CMKs in the state of disabled or pending deletion will not be rotated, and it’s not possible to manage the key rotation for any AWS-managed CMKs. These are rotated every 1,095 days, or three years. Manual key rotation is the process of replacing the current CMK with a new CMK. For any applications that reference the CMK-ID of the oldie key, you will then need to update and point them to the new CMK-ID following a manual rotation or update the Alias name of the key with the new CMK-ID Target. Key material is the backing key, the component that completes and implements the encryption and decryption process on behalf of the CMK itself. It is possible to create CMKs without any key material at all, allowing you to import your own, and the process for importing your own key material follows four main points. Creating your CMK with no key material, download a wrapping key, which is also referred to as a public key, and an import token, and these will remain active for 24 hours, encrypt your key material using the wrapping key, and then import your key material that is now encrypted into KMS and associate it with your currently empty CMK. Deleting a key can have significant impact against your data if there are services that are still using it without your knowledge, and KMS enforces a scheduled deletion process, which can range from seven to 30 days to help manage and identify if the key is still in use. Keys scheduled for deletion can’t be used to perform encryption or decryption actions, neither can the backing keys for the CMK be rotated. By analyzing AWS CloudTrail event logs, you can look for events relating to the use of the CMK, such as an encrypt action against the CMK-ID. AWS also recommends that you set up a CloudWatch alarm to identify anyone trying to use this key to perform any encryption or decryption requests. And you should consider disabling a key instead of deleting a key, as this will still take the key out of operation, however, you can reinstate it again at any point should you need it. </p>
<p>That now brings me to the end of this lecture and to the end of this course. You should now have a greater understanding of the AWS Key Management Service, and through the use of different keys, how it can provide a level of encryption of your data across a range of AWS services. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:&#x73;&#x75;&#112;&#x70;&#x6f;&#114;&#116;&#64;&#x63;&#x6c;&#111;&#117;&#100;&#97;&#99;&#97;&#100;&#x65;&#x6d;&#121;&#46;&#99;&#111;&#109;">&#x73;&#x75;&#112;&#x70;&#x6f;&#114;&#116;&#64;&#x63;&#x6c;&#111;&#117;&#100;&#97;&#99;&#97;&#100;&#x65;&#x6d;&#121;&#46;&#99;&#111;&#109;</a>. Your feedback is greatly appreciated. </p>
<p>Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="3Components-of-KMS"><a href="#3Components-of-KMS" class="headerlink" title="3Components of KMS"></a>3<strong>Components of KMS</strong></h1><p><a target="_blank" rel="noopener" href="https://csrc.nist.gov/projects/cryptographic-module-validation-program/Certificate/3139">FIPS 140-2 Validated Cryptographic Modules</a></p>
<h1 id="5Key-Management"><a href="#5Key-Management" class="headerlink" title="5Key Management"></a>5<strong>Key Management</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-encrypt-key-material.html">Encrypting Imported Key Material</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-creating-cloudwatch-alarm.html">Creating an Amazon CloudWatch Alarm to Detect Usage of a Customer Master Key that is Pending Deletion</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring-Metrics-and-Logging-29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring-Metrics-and-Logging-29/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring,-Metrics-and-Logging-29</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:30" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:30-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:54:26" itemprop="dateModified" datetime="2022-11-19T22:54:26-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring-Metrics-and-Logging-29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring-Metrics-and-Logging-29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to CloudAcademy.com’s Advanced AWS Monitoring Metrics and Logging course. This course is intended for DevOps engineers that want to get the most out of monitoring metrics and logging and, in general, keeping track of the state of their cloud.</p>
<p>So in this lecture, we’ll be going through an introduction of the course, which will include a definition of the three terms that make up the title of the course. We’ll also go over the intent of the course and what you’re trying to learn while watching this video series. We’ll go over the scope of the content, so everything that’ll be covered beyond just the intent of what you should learn, the benefits of advanced systems running for monitoring metrics and logging on AWS, the different lectures in the course that you should expect to see, and then a brief summary of the intent of the course in a final mission statement.</p>
<p>So without further ado, let’s get into it. So first, let’s define the term “monitoring”. When we think of monitoring, we should be thinking of how we can observe and check the progress or quality of something over a period of time, and keep it under a systematic review. Now in Amazon Web Services, this means verification that your cloud works, and I’ve included icons for three of the main services that people should be familiar with for doing this. You may not have used all of them before, but you’ll become more familiar with them over this course as we talk about how we might do these things.</p>
<p>That’s the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-route53-dns/dns-private-hosted-zones-virtual-private-cloud-1/">Route 53</a> icon in the top-left there. We can use Route 53 health checks, where we ping a certain DNS endpoint and do a holistic check to make sure that we get a 200 error code, or just a general heartbeat or response back.</p>
<p>That’s the CloudWatch icon up there in the top-right, which can include CloudWatch Events, Metrics, and Logs, and as well as Alarms.</p>
<p>Then, we have the ELB icon there. There are ELB health checks for when we’re thinking about doing auto-scaling, and making sure that individual instances behind a load balancer are healthy.</p>
<p>So we should also look at metrics. When we think about defining metrics, we have a standard or a system of measurement. So in AWS, this means quantifying cloud behavior and state. So standard or system of measurement here means that we have a specific thing that we’re measuring whenever we think about a metric. It’s a little bit different than monitoring, where we are more looking for a binary “is it online or not” over a course of time. Whereas, a metric we’re looking to quantify something using CloudWatch, which is that icon there. So quantifying our cloud behavior and state could include things like reading the amount of traffic that comes across EC2 instances or an Elastic Load Balancer, reading the amount of provisioned throughput or consumed throughput for DynamoDB tables. Any number of things that we can put numbers to and are nicely graphed over time, that’s what you should be thinking about for metrics.</p>
<p>So we also use metrics as input for determining CloudWatch Alarms, which is that icon in the bottom-right there, where that red presumably would be a threshold that I cross. That’s one of the reasons that I would be using metrics, is to see whenever I cross certain thresholds and create certain behaviors.</p>
<p>So moving on to logging, this one might surprise people, recording of performance events or day-to-day activities. So in Amazon Web Services, this means a time sequence of system occurrences, which is very generic. It’s not like you might think, where most people coming from a non-cloud background or from a cloud that doesn’t have sophisticated value-added services tools to help us manage our logging, might think of logging as dumping out files that represent things that happened during your application code. Which, that’s a useful abstraction for maybe a single desktop computer, but it’s extremely difficult to handle something like that when thinking of log files as just individual files. Not just logging in general, log events, performance events, or day-to-day activities. It’s very difficult to manage that level of complexity in the cloud, so that’s why you’re watching this course.</p>
<p>So intent of this course is enable actionable understanding of your cloud, which is a very generic statement. But what that means is that we go from thinking about logs as afterthoughts, or things that we might use for debugging whenever things go wrong, and move forward towards this after, where monitoring metrics and logging is a first-class design task that delivers massive business value. So what does that mean? Well, it means that when we implement correctly monitoring metrics and logging, as a DevOps engineer, one of the primary things that you should be praised for is the level of sophistication for your monitoring metrics and logging system. By making it a very easy system to extract value from, answer questions about the operational metrics of a system, and ensure high availability, ensure good software delivery practices. So monitoring metrics and logging is very important, and as we move towards this value-added thinking around monitoring metrics and logging, rather than afterthoughts, we’ll be better DevOps engineers.</p>
<p>So the scope of our content is that we fundamentally rethink the log. So as we eluded to earlier when we defined the log, we need to in this course fundamentally rethink the log away from a set of files, maybe, that you might run on your virtual machine or bare metal if you’re coming from a data center that you actually have.</p>
<p>We have to think about logs and metrics, and how they offer value. So one of the statements that I made on an earlier slide was that we are going to turn these things that are typically afterthoughts into value-added systems. So we have to cover how we can do something like that and extract that value. So we’ll learn the skills to extract the insight from the logs. When I talk about value, typically the type of value that we’re thinking of when we talk about using logs to get values insight, so we have three levels of completeness of information or data. So we have data, which is just that might be individual lines in your log. We have information, which is a slightly higher level of abstraction where I can speak an English sentence and explain what the data means. Then, insight, which is where I take away some critical thing that I didn’t know before, or I’ve learned something new from that kind of information or data. So we want to be able to use logs to extract new insights that we’ve never thought of or seen before, and deliver value that way.</p>
<p>So we want to learn practical methods for handling logs. If we’re going to be messing with all these logs and these metrics, which are really kind of just a subset of logs, then we need practical methods for handling these things and handling complexity. Because as you know in a highly dynamic cloud environment where we’ve got lots of distribution and lots of moving parts, sometimes the challenge can just be managing complexity.</p>
<p>We’ll design some automation around log event streams. So you should know ways, in addition to having human eyes derive insight from logs, the insight that you can receive from logs if they’re structured can also derive automation. So we’ll get into that a little bit later. But there are a number of places where you can read or sift through logs, and depending on what you see, do automation actions. So we’re going to get logging superpowers, effectively.</p>
<p>Monitoring and metrics are also in the title of the course. But I like to think about those as subsets of logging in general, which is the more generic thing where monitoring is a little bit more binary in the context of the cloud, where we’re monitoring for uptime. Metrics are a little bit more oriented towards quantitative goals.</p>
<p>So what are the benefits of some of the advanced systems that we’ll be building? Advanced logging helps manage systems in a number of different ways. Logging techniques should scale the business. We should yield immediately the convenient insights that we were talking about earlier, and we will reduce the ongoing DevOps effort of managing our cloud.</p>
<p>Finally, when we look at the lectures in our course, we have events everywhere handling distribution, try the ELK stack and ChatOps with Slack. So what that gets us is events everywhere. We’ll be talking about how to rethink the log in general, that it’s changing our brain around the paradigm shift that is going from thinking about logs as a file-oriented system into this events-driven system. Handling distribution we’ll get into a little bit around the nature of the systems that we’ll use to manage the complexity around delivering these insights and this additional value from logging systems. Try the ELK stack will be a show-and-tell, where I walk through a very, very common, the most common actually, logging extraction and insight extraction tool on Amazon. The ELK stack is now offered as a service from Amazon, and ChatOps with Slack. So ChatOps eludes to when you present automation with your system. Where if system events occur, rather than emailing you or phone-calling you, since most people spend a lot of their time in chat and chats are effectively an event stream as well, we can insert log notices into our chat system. I’ve picked Slack, which is an enterprise chat system that got really popular in 2014 and 2015, where we can do easy API-driven development to insert insight into our group chats.</p>
<p>So when we think about doing these different lectures, we should be thinking about all of these different graphs and insights that we might be deriving. We will mostly see some of these in the ELK stack, but start thinking about your logs as a tool to derive insight and do analytics, which is what we have all these graphs on here on the side.</p>
<p>So in summary, this course will teach you how to extract value from observing your AWS systems. It’s very generic-speaking, but effectively it means we first rethink the way that we are handling logs in general, and start packaging them and utilizing them in a different way. Then, design appropriate technical systems to handle the new format of the logs that we’re going to deal with in the best way possible, and make them usable and consumable by other systems as best possible, and usable and consumable by both technical and sometimes even nontechnical users. Then, we’ll go over some practical examples for how we can actually utilize these things so you can envision implementing these in your own systems or your own company.</p>
<p>So next up, we’ll be doing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/events-everywhere/">Events Everywhere</a>, which is the video course that’ll teach us how to rethink how we do log systems.</p>
<h1 id="Events-Everywhere"><a href="#Events-Everywhere" class="headerlink" title="Events Everywhere"></a>Events Everywhere</h1><p>Welcome back to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/introduction-27/">Advanced AWS Monitoring Metrics and Logging</a> on CloudAcademy.com. In this lecture, we’ll be talking about how events are everywhere and how this relates to logging.</p>
<p>First of all, monitoring metrics and logging, they’re all events. We’ll talk about how we can use metrics and how they’re slightly different than normal logs. We can talk about how logging is everything and the value that it holds. We’ll go over event sources and consumers, so different things that are creating events and different things that are reading or seeing these events come across and acting on them, and how streams solve a lot of the problems associated with managing these logs. Finally, after going through all of the logic in the previous slides, we’ll talk about why we’re saying “death to the log file” as we eluded to in the introduction lecture.</p>
<p>So again, events are everywhere. Monitoring metrics and logging, they’re all events. So let that sink in for a moment. Events are, “Hello. Boom. Something happened. Event.” Monitoring is a typically binary, “Oh, holistic check. Is my system online?” This can be a DNS ping or HTTP check or something. Metrics are typically a quantifiable, time-sequenced thing, so metrics of usage, metrics of traffic, metrics of throughput. Logging is a little bit more generic and is the parent category to both of these, in that it can be unstructured or structured. So every health check, metrics collection ping, line of log content is an event, or a thing that happened.</p>
<p>You can imagine that there can be a timestamp associated with all three categories of these things, so we just think about it as recording an event or a thing that happened. Like normal analytics, what happens has value, so there’s a logical jump here. If we think about what most businesses, or at least small businesses or companies that haven’t undergone technology modernization efforts, most businesses think only about analytics from a marketing or even a physical operations perspective perhaps on a factory floor.</p>
<p>These business analysis, business intelligence, business analytics tools, these are a sophisticated ecosystem, but we’re just seeing log analytics catch on. So if we think about events as different things that occur and we have analytics over events in the marketing space, we should also be able to do analytics over the logging space when we have log events.</p>
<p>When we think about metrics, we should think about this kind of graph where they’re entirely quantitative and our metrics are best used for time-series algorithms and reaction. Because metrics are inherently quantified already, they have some sort of number associated with them, the metric that we like to think about, we can make decisions based on what we see.</p>
<p>So here we actually look at a DynamoDB consumed write capacity units throughput table, and this is a metric in the CloudWatch Console. This metric tells me that I just consumed about a quarter of a million requests in five minutes after a period of essentially no requests overtime. So metrics are useful because they let you do things like detect when something like that happens. Where I go from almost no requests coming across my system or making writes to my DynamoDB table, and suddenly one minute there was 0.25 million requests, and the previous 5-minute period there was over 200,000 requests. We want to be able to figure out when these kind of spikes happen, and we do that using metrics in the CloudWatch Metrics Console.</p>
<p>So logging is everything. Logging, the more generic field of things that we’re talking about, where metrics are technically just log events with a well-quantified field to them. They still have timestamps. So business logic is loggable and maybe quantifiable, the quantifiable part being the metrics, and we can think about logs as append-only series of events in a flexible semi-or-unstructured format that can be quantified. That’s a little bit of a doozy of a sentence to think about. But append-only, meaning, if you think about a log file, when you concatenate to the end of the log file you append to each new line that you come out with. Log files are just a file representation of a stream, like we’re looking at here in the bottom-right. So we have an ordering where the top or left, it depends on the orientation of your log, in a file it’d be the top of the file, is the oldest event. The next record that you write to the log will always be guaranteed to be the newest record at the time. It’s also append-only is very important here, when we think about it. We’re not modifying 0 through 11 in that log event right there in the bottom-right. We are creating number 12, and that append-only property is very useful to us, as we’ll see a little bit later here.</p>
<p>They’re also best served with structure. That’s my little joke there. I have a small JSON object that I’ve typed into a text editor. But what this tells us is that a metric is actually one of the more structured log types. It has an associated quantified piece to it. But all logs should be served with structure and consistent fields. So rather than just logging or printing text out to the console whenever you’re trying to do your debugging, it’s more helpful if you return the kind of error or the nature of the error, or any kind of details or parameters that were provided to a method when you log out an error.</p>
<p>When we think about logging, we should be thinking about how to move from free text, or unstructured format, to more structured formats since that’s what computers and people will be able to do analysis over.</p>
<p>Note that I don’t mention the storage medium here, that stream on the bottom-right there. It doesn’t say that it’s stored to disk, that log sequence. It doesn’t say that it’s stored to disk. It doesn’t say that it’s in a database. It doesn’t say that it’s in memory. It doesn’t really matter, because logs can be transported or represented in a number of different ways. So don’t think about files when you think about logs. Think about these sequences of events that may or may not have quantitative properties on them and should be structured if you’re doing things correctly.</p>
<p>Talking about event sources and consumer, first of all, data availability is good. This is an age-old mantra of anybody that’s ever done business intelligence or analytics from a data warehousing or data-like perspective that generally making more data available to more different parties is a good thing. Because we get better insights, better integration, and more actionable insights as we diversify the way that we can consume these things.</p>
<p>Metrics and log events are data. Right? So particularly if we think about from the previous slide, where I had a “Best”: “Served” “With”: “Structure” JSON object, if my logs are in JSON format there are very sophisticated and well-built out tools to help us do analysis over things like JSON objects. Because they’re effectively a serialized representation of objects in memory, once we have our logs into JSON or some representation like that, then we can use them as first-class citizens or primary data sources.</p>
<p>So events are good, because we’re saying if data availability and having more data about our business that’s relevant is clean, if that kind of data availability is good and metrics and log events are data, then we should be looking towards our metrics and log event data as good, or sources of value for the business.</p>
<p>One example where somebody created an actionable system that actually delivers value just using logs, this is a metric. Which in my mind, it’s a subset of the log and that’s how you should think about it as well. Auto Scaling actually uses this method. So if we look at, we have an Auto Scaling group here on the left, we have a group of instances. If all of these instances are publishing metrics, which they are, to Amazon CloudWatch and we have a thresholding algorithm on them, which are our CloudWatch alarms. If you’ve ever done an Auto Scaling threshold alarm, that’s one in the same. Then, we have logic that once we emit to the alarm and say, “Oh, I flagged a specific pattern in the log data, and I’ve come up with some analysis and some actionable thing that I need to do based on what I’ve seen,” then we trigger the Auto Scaling and command something like an auto scaling system to scale up and down. So this is actually effectively how you would accomplish auto-scaling.</p>
<p>Beyond AWS, you know Amazon is very sophisticated and they have a very straightforward infrastructure¬-driven requirement to use a form of log to deliver value, we can also think about delivering value via a normal SaaS company by using log data to make self-managing systems. So let’s trace through the user journey here. In the bottom-left we have a user or an end user, customer. The user can be a human being in the case of a web application, or it could be another software system in the case of more of a systems-oriented or a service-oriented architecture. They interact with your service, which I’ve represented as a collection of EC2 instances here, but it could be any number of complex things or stacks.</p>
<p>We produce business logs from the different components of our service, and we emit them over to CloudWatch Logs. We can pick them up from CloudWatch Logs using Amazon Elasticsearch Service, or ELK, which we’ll get into a little bit later in the course. Based on thresholds coming out of that system, we can create custom alarms, notify an SNS topic, and have different actions be taken based on that SNS topic. We can write the SNS topic into a support database using a Lambda perhaps, or another system watching for this, and have customer support read out of a support database. We can page engineers based on different things that we see come out of the logs. We can also, if we have sophisticated enough logs that provide us with enough information, sometimes perform actions that solve the problem completely autonomously, which would be a healing Lambda automaton. It could also be EC2 instances as well. But we can implement self-healing logic if we have business logic logs that can flag things like increased error rates, or something like that.</p>
<p>So beyond just metrics that you’re thinking about for throughput, we can also do more sophisticated logic like detecting patterns in the actual text or enumerated inputs inside of business logs, and create fairly simply, even though there’s a lot of boxes and arrows here. There’s lots of different value-added services that Amazon provides to us, such that we can string together one of these self-managing SaaS systems without creating our own instances or software simply by stringing together a couple different Amazon Web Services managed services.</p>
<p>So we have a complexity management problem already, just looking at that other slide over there. We think about strings as time-sequenced event buffers. We already talked about streams, and we already talked about events. Events are these different things that pop up in our logs, or they’re the individual things that happen. Streams are entire sequences of these events. Streams also work as buffers, because if we have a stream that is the data intermediary to carry between two systems that might want to share log data, the streams will allow us to do a number of different things. They’ll allow us to natively support log event style, so that one’s clear. If we think about logs as a sequence of events and streams are simply time sequences of events, then this is a natural thing to want to start streaming our logs.</p>
<p>We also get a unified transport to other services from a stream. So if we think about what Kinesis is primarily used for, if you’re familiar with the rest of the Amazon platform, it’s primarily used for moving data in the correct order from one place to another. If we think about firehoses, if you’ve ever heard of that before, data firehoses, those are just streams as well. The firehose typically just means the velocity with which they come through. Then, three and four, they allow different consumption and production rates. If we think about if we have a system that is creating things, events very quickly or in a burst-y matter, and then another system that slowly DQs things in a constant rate, we can use a stream to buffer between those two systems as we have the spiky production and consistent consumption. Or even the other way around, where we might have constant addition to the stream, and then a spiky read off of the stream to do analysis and perhaps a spark, or write to a database.</p>
<p>It also allows us to decouple producer and consumer systems. So rather than having to keep a registry of all of the DNS addresses of all other micro-services in a complex system, we could have our log systems simply write to a stream that never changes location or DNS address. Write to that stream, and not concern itself with the consumers that are picking these things up.</p>
<p>We could also potentially have multiple producers writing to the same stream, and multiple consumers reading off of the same stream. So we can have a many-to-many relationship in which the producer only has to be aware of one thing, where the stream is, and the consumer only has to be aware of one thing. Even if we had 10 producers and 10 consumers all producing to and reading from the same stream, the only thing that any of those services need to be aware of is the stream location, so we have a nice decoupling there.</p>
<p>Streams are great, and we talked about why they’re great. We need to realize that logs aren’t just files that you open and scan when something breaks. They’re a primary data transport mechanism, that is we can put the data that come out of our log streams and do a number of different things on them, even replicate databases. They’re a primary first-class citizen for data, and log events are fundamental design building blocks.</p>
<p>So if we looked at my self-healing system, the primary thing that it was operating over were logs. All of my business logic was centered around the log, and even in Auto Scaling the primary thing that Auto Scaling does is use the design of the log system as the fundamental building block. It just reads out of this log stream, these time-sequenced events of load, and then make decisions for when to scale out. Hopefully, you’ve learned a thing or two about why streams and events are everywhere in a logging problem. Next up, we’ll be talking about how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/handling-distribution/">handle distribution</a>, that is how to handle the distributed nature of log producers and log consumers, and in general how to write a distributed system that handles logs in the cloud.</p>
<h1 id="Handling-Distribution"><a href="#Handling-Distribution" class="headerlink" title="Handling Distribution"></a>Handling Distribution</h1><p>Welcome back to Advanced <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/introduction-27/">Amazon Web Services Monitoring Metrics and Logging</a> on CloudAcademy.com. In this lecture, we’ll be talking about how we handle distribution and the distributed nature of stacks that we need to log and monitor.</p>
<p>First, we’ll talk about how we need to stream everything to make this goal of handling distributed event sources work correctly. We need to understand the log group structure. We need to talk about how we perform unification and distribution of our streams and log events into multiple consumers. We need to talk about different kinds of stream syncs that we can use when we’re talking about creating our log streams and how they can be useful to syncing data or transactions into individual syncs. We’ll talk a little bit about how you might perform an archival and backup solution, so how we might achieve logically the long-term auditability requirements that you may have rather than using the logs as an in-flight or real-time log streaming. And then we’ll talk briefly about aggregation with the ELK stack and how that is logically achieved.</p>
<p>So, we’re talking about handling distribution. We are talking about how we want to stream it all. So when we’re talking about streaming in an Amazon Web Services cloud, now we say we need to stream it all and that’s because streams are helpful transport mechanism as we learned in the other lesson. But when we’re thinking about streaming logs specifically in Amazon Web Services, we should be thinking of Kinesis CloudWatch Log Streams and how they are your friend and we’re going to use them copiously as we set up these logging systems.</p>
<p>So first of all, we need to log everything, truly everything to CloudWatch Logs first and use this as a source of truth. So it’s a common temptation to log to disk because it’s easy and convenient and it’s nice and easy or log to standard out even. If we’re logging to standard out, we should be using a CloudWatch Logs daemon to take each of the lines that we’re logging to standard out and pick them up and emit them to CloudWatch Logs which Amazon provides a tool that lets us reroute anything that’s sent to standard out or to disk to CloudWatch Logs which runs as a daemon on Amazon Linux, Ubuntu, or their CentOS distributions. So, rather than just using a plain file, we should be thinking about using a daemon like that and submitting to CloudWatch Logs first. And if we’re using anything like a Lambda or something, we will natively support that CloudWatch Logs.</p>
<p>So all processing should be done stream with CloudWatch. This is an important notion that the top of our log stream funnel should be starting with CloudWatch just because in Amazon we have the ability to do long-term retention if we’re streaming into CloudWatch. But we also have the ability to read off of it like an inflight stream that might be more transient or temporary. So you get the best of both worlds when we’re thinking about using CloudWatch Logs because we can get long-term persistence as well as event ordering and event buffering through the streaming behavior.</p>
<p>So, when we’re thinking about how logs work and we’re CloudWatch and how log model works, this actually works the same model, this diagram that we’re looking at. This is actually how Apache Kafka works and CloudWatch is in some ways similar to Kafka in that it has similar partitioning and such. So let’s take a look.</p>
<p>So we have… CloudWatch as a service encompasses this entire diagram. But these two inner rectangular shapes that we have, we see one log group and another log group on the left and right here. Log groups are the logical grouping of our logs. So, Amazon Lambda already creates log groups based on the Lambda name and names spaces them under the AWS Lambda. You can also create log groups by putting a log group when you’re running an easy two instance if you name the group along the same vein. So for instance, I might decide to use one singular log group for each process that I might be running inside of an auto-scaling group. So if I’m running two processes at a time, two primary processes at a time on each instance in an auto-scaling group, I might use two log groups, two log groups that are used from each system.</p>
<p>So, log groups are great but they’re not the unit that you read from. They’re just the logical unit of abstraction. When we’re thinking about actually pulling data off in those time sequenced recordings like we were talking about in the other lecture, only the log streams which are belonging to these log groups are guaranteed to be in order. So there’s no absolute ordering of events inside of a log group at all. There is absolute ordering inside of a log stream. So, that’s a key distinction because we might have multiple parallel streams being produced at the same time but not have any way to globally order them if we don’t create a system on top of it. So, if you think about CloudWatch Logs and the model that we have, we have there are many CloudWatch has many log groups. Log group has many streams. Stream has many events. And inside of each stream, the events are guaranteed to be in order as you can see here.</p>
<p>So, when we’re thinking about having these multiple logical streams, it’s important to remember that if we’re trying to configure a useful streaming application for logs where there may be systems that need to consume multiple streams or there may be systems that need to consume streams in a different format, than they’re used otherwise, then we have these two different operations that we can perform which are our primary method of working with log streaming system design. So, on the left here, we have unification where we have the capability to unify two streams into a single stream and publish them to a consumer who may be considered, who may be concerned with the correctly global ordered streams, that is stream C, that is the unification or interleaving of A and B. We may be not concerned with the global ordering. We might just be concerned with the mixing two topics together. So those could be API and database logs. This is actually how you might merge two streams within a log group into stream C. So presumably, we could imagine if we stream into Kinesis from two different streams inside of the same log group, then that stream C there might be the Kinesis or the unified log which represents the merged events from all streams inside of a group. And if we have a consumer that is interested in reading all events from that entire group, then consumer A will be happy with stream C.</p>
<p>We also have this ability to do a transformation with a map or a filter or both at the same time, filter map or a map filter. So, if we see we have this stream before and we run through some sort of logical process and emit into a stream after. In Amazon, this is typically achieved by reading out of a Kinesis Stream running a funk door on a Lambda on shards of the events coming out, shards being a frame of multiple events coming out at once as we optimize for network throughput. So for instance we wouldn’t want to, if we’re streaming a million records every hour, we wouldn’t necessarily want to read out of the stream one record at a time. We might want to chunk 10 at a time, that would be a shard.</p>
<p>So these map or filter operations here, we can submit or we can have a Lambda read out of the upstream stream, execute some sort of mapping or filtering logic and put it into a second stream. When we combine these two methods, this unification and this distribution through a map or filter, this transformation, we combine these two and realize that we can chain them together. We can design systems of arbitrary complexity because we can also have multiple consumers per stream, right? So key concepts, merge and alter.</p>
<p>But there’s this third concept where we have stream syncs and we can sync the output of a stream into multiple different places here rather than showing a fanout of any streaming, typically fanout happens where have an end consumer rather than fanning out into two individual streams. So if you see here, potentially, this is an example that we could use where we have an application database that’s receiving application-style reads and writes in a transaction log. Now that transaction log, we can actually use a stream as our transport mechanism. So the transaction log only needs to be aware of the stream location. So, app database only knows where to submit its stream. And then our ElasticSearch, Free-Text search, our Hadoop, Ad-hoc analytics, our Redshift for analysis and business intelligence SQL, our S3 and then subsequently Glacier backup, our replica application database even since the transaction log represents a changed stream that we can use to recreate a replica or anything else, we only need to know where the transaction log and that where is that data that we’re curious about exists. We actually don’t need to know where the application database lives. That way if we have any of the databases go down either upstream or downstream, the only piece that needs to have a consistent address that needs to be addressed is that transaction log.</p>
<p>So log streams are great for replication if we see the app database versus the replica app database. The log is actually the ideal method for replicating any kind of state across two different databases simply because that time ordered sequence of events, if you replay it on the other database, you get an exact copy and you can also use the transaction to recreate the secondary database at any point in time since you can only partially play forward if you want if you restore the database.</p>
<p>So, common question is if we’re not looking at logs as files anymore, that we go and peruse through when we need to do some sort of lookup in the past to see what went wrong, how do we do archival backup and auditability when we’re talking about this brave new world of log events and streaming? Well, rather than just looking up files and doing direct S3, simply look at the S3 objects as another sync. So, again, we can have our database creating a transaction log stream, maybe publishing to Kinesis, and we can have a Lambda reading off of Kinesis and writing an S3 object for each shard. So we have logs that are divided by time and we have many objects over time. We can also then still have the other consumers read from the same transaction log stream. So the archival and auditability actually is no longer a special case at all. It’s a primary case where it’s just another sync. It’s just another reader off of the same stream.</p>
<p>Once we write into the S3 objects, we could set life cycle rules on the S3 bucket object and you can go and look up the documentation in Amazon Web Services if you so choose. But a life cycle rule effectively tells S3 to change the storage class of data after a certain amount of time. Storage class changes can include altering to this AWS Glacier which is storage on magnetic tape which is at the time of this recording of this video it’s seven-tenths of a cent in US dollars in the primary regions, the US East and US West versus three cents for the S3 storage. So it’s four times cheaper to store in Glacier, so we might imagine that after a certain period of time, it would be advantageous for us to simply life cycle rule into Glacier.</p>
<p>But the important piece here is that the S3 object writing is the part that finishes dealing with the streams and then we batch after that. That portion there is not a special case anymore in this archival and backup in auditability when you’re using streams. It’s just another consumer. So, it’s a very clean design and consistent. If you really want files, you should do it this way.</p>
<p>So, if you think about how we do event sources and consumers, we can also realize that we can make the ELK stack another consumer. So the ELK stack stands for Elasticsearch, Logstash, and Kibana. Now Elasticsearch is a free text search database and arbitrary query analysis, noSQL engine. It’s typically used for free text search which lends itself really well to doing kind of lookups or forensic work on your cloud system because you can do a free text search for the error message or a reference code or something and see every time that a reference code or error messages appear in the entire history of your log stream, so E is great in ELK.</p>
<p>L stands for Logstash and it’s our indexing mechanism. It is the way that data is flumed into Elasticsearch typically. Since that part is pretty much abstracted away by CloudWatch Logs and Lambda, we don’t really think about that too much when we’re rolling our own ELK stack solution on AWS but that’s what the L stands for.</p>
<p>K is Kibana. So the E in ELK for that search engine in Elasticsearch service, that’s just an API-driven database that speaks JSON as its wire protocol and it uses HTTP. The K there stands for Kibana. Kibana is a system that uses, it’s just a graphical interface and an indexing system and some prebuilt logic and prebuilt indexes logics on the cluster. So it runs on the Elasticsearch cluster itself and it’s a GUI that lets you do things like create graphs, do log analysis, and general business logic on the index logs inside of the E portion, that Elasticsearch portion. So K, Kibana is what adds the GUI and the credibility value.</p>
<p>So, if we look at this entire flow chart here, it’s the same thing that we’ve been seeing with those white boxes early on in the slides, only this has some exact services named to it. So, the Elasticsearch, yet again, is just another sync. So we have our database or presumably, this would be something like a DynamoDB or even a SQL database if you wanted to implement your own transaction log scraper. In this case, I’ve used the Dynamo logo. DynamoDB can submit its changes to this stream, change system, so there’s DynamoDB Streams is something that is supported now. In effect you are turning on what looks almost exactly like a Kinesis Stream that is populated with all of the events from a change stream off the database. So it’s almost exactly like a transaction log.</p>
<p>You can have a Lambda poll on that transaction log or that change log so we can have a Lambda reading off the change stream. Then you could emit… Then whenever you have these changes occurring, you can have the CloudWatch log streams that are created by Lambda by default, in this case, we can have those logs automatically indexed by CloudWatch. That’s a default thing if we set the role policy correct to allow Lambdas to create the logs. CloudWatch, we can also configure to stream into another Lambda. So we can stream into this Lambda off of our CloudWatch Logs and have the Lambda insert into the Elasticsearch service in the correct way for Kibana to operate efficiently. That is actually a console action that is handled for you.</p>
<p>And then we have a fast, searchable, united graphical logs UI which is excellent for Ops management. So this one of the more common syncs that people think of because it uses logs for the same utility that people are used to thinking about using them for, for debugging. This is just the extremely sophisticated way to debug an entire cloud because you don’t need to submit only the API Lambda logs or the database transaction logs. You don’t need to submit only those things to the Elasticsearch service. We can also submit logs from anywhere else in the cloud and still have them aggregated in a centralized place. So not only do we get unification of the multiple streams inside of log groups, but we can also aggregate across log groups and then create pretty charts and different analytics and metrics based not only on hard numeric metrics that CloudWatch natively support. But also, we can create arbitrary metrics off of a query that might be derived from working over the JSON representation of different logs.</p>
<p>So next we’re going to do a little hands-on demonstration and try the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/elk-stack/">ELK stack</a> out by creating one inside the console.</p>
<h1 id="ELK-Stack"><a href="#ELK-Stack" class="headerlink" title="ELK Stack"></a>ELK Stack</h1><p>Welcome back to CloudAcademy’s course on Advanced <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/introduction-27/">Amazon Web Services Monitoring Metrics and Logging</a>. In this lecture, we’re going to be trying out the ELK stack which was one of our logs stream sinks that we had talked about. It’s a log aggregation service that let’s us aggregate across multiple CloudWatch log streams, any kind of events that we want to index into the system. It integrates nicely with the Amazon Web Services stack with the introduction recently of the AWS Elasticsearch service which runs the E in that service and also the K. And the Lambda ends up being our L for the indexing of the log.</p>
<p>Without further delay, let’s get started. So just a reminder of what we’re trying to build. If we look really quickly at this flow chart that I’m working with here, I’m using a flow chart software that we can take peeks at so we can make these diagrams. But effectively, I’ve already got some of this built out here. We’re going to be simulating a little API, but the main important piece here is that we’ll be emitting events into CloudWatch logs, pulling logs out, feeding them into a Lambda which then subsequently inserts them to this Elasticsearch service that’s actually running the E and the K in our ELK stack. Then we’ll go through this log inspector cycle here where we try out that fast, searchable unified graphical logs UI.</p>
<p>So I need to navigate over to my CloudWatch console first and we need to first see what these log events might look like. I’m actually going to use AWS Lambda to create some logs just because Lambda integrates with CloudWatch logs really nicely. It’s important to note that anything that can submit logs into CloudWatch logs will work for this part. This just happens to be lightweight, easy to test thing during a screen share.</p>
<p>Here we’re looking at some function code inside of a Lambda. It doesn’t really matter. The idea here though is that I’m running a test event where I’ve got some logs coming out of this output. Those might be a little small on your screen, but these logs should be visible now if we go look into CloudWatch logs. So I’m actually going to open these logs up. If we look at the raw log stream here, we can see that we have these function-loaded portions here and we can see that we have some events coming through. That’s all well and good.</p>
<p>Let’s go back to the Lambda console and rather than echoing, I’m now going to configure my test event and run a ping. We should see it come back with Pong here and if I go back to my log streams… …we can see that last execution that I did for the echo and eventually that pong will show up. There’s a little bit of latency there. So I won’t bore us to death waiting for it.</p>
<p>Here we’re navigating inside of the CloudWatch log group. These are interface here. We can navigate into individual streams which we were talking about in our previous lecture. Now in general this is helpful, but it’s not optimal because I might want also to be able to search for events at the log group level instead of just the log stream which as you can see all I can do is search by stream prefix here and I can only search by text once I navigate into this user interface for the individual streams.</p>
<p>Without further ado, let’s set up an ELK stacks. So you’d want to navigate over to the Elasticsearch service somehow if you’re looking in the all AWS Services, it’s alphabetical. So we could just click up here. And once we click on our Elasticsearch services, we’ll navigate to this tab. I’ve already set this up so we can accelerate the pace of this video, but I’ll show you what it takes to set one of these domains up since it takes about 10 minutes usually.</p>
<p>So this is exactly how I set up that other domain. I just hit yes. All of these default values work. I’m just selecting size and the count of the instances that will join that Elasticsearch cluster. I’ll hit next. I would use open access to the domain for this demonstration just so we can easily see what’s going on. I hit next and then I would hit confirm and create. I actually did this step already to accelerate the demonstration here. So we actually have a complete cluster already here.</p>
<p>So if I click on that Kibana 4 user interface, we should see a loading screen like this. So for the first time that you do that load, it will take a while and you’ll see that little loading screen. We need to configure an index pattern. So we have index contains…  we want index contains time event. We’re actually not going to do this until I go and create some more log events that will be submitted into my system. Again, I’m going to run another test with my ping and my pong. May be able to see more events going up here. Not yet. Okay. So even though I didn’t get any events showing up from that one yet, what we can do is move back to the log groups. I actually want CloudAcademyDynamoLambda which I’ll copy from from up here. I can use that identifier to search for it. Then click on the radio button here and stream into Elasticsearch service.</p>
<p>So I only have one cluster left, I can create a new role to allow the Lambda that I’m pushing into to publish into Elasticsearch. So I’m granting it that ability to post into my cluster. So I hit allow. I want to move forward. Realize that my log format will be coming through in the AWS Lambda format. We can see that we have some sample events coming up here. Then after I’ve done all of my configurations, this filter pattern should be familiar if you were watching very carefully here for what my log formats look like on this system, and then I can start streaming.</p>
<p>So what this is doing is it’s setting up this portion here where we have this arrow going into a Lambda and this arrow going into the Elasticsearch service. So we just set that up. We’ll wait just a second to allow that to activate. We’re provided with the link to Kibana 4 but we already have it open. Another best thing that I can do now is to configure test events to actually run these operations and we’re actually going to demonstrate what happens if we have a broken Dynamo configuration here. So I can do something like setting the received event here. I’m actually going to use a slightly different formatting scheme here, and allow my event to be logged in JSON formatting like that. So if I just log my event, then I can save and test. I should see operation ping and message “Hello World!” since I just added the console.log of my JSON and now I can see any invocation that comes into this Lambda in my log output. Now this log output will also show up if look at my log stream over here. So I’ve navigated back to my CloudWatch logs group and I can see that I’ve already got the JSON file or JSON line showing up inside here.</p>
<p>Now when we configured our ELK stack streaming service here when we did that. I’m going to reload so that we can see if those indices start showing up here and what we can do here is go directly to our cluster. We can run a search and see that we started having some events showing up here. So we’ve got some Kibana 4 indices. We’ve got some data showing up in the cluster now at the search engine end point. And if we scroll around here. We can see that my index that it set up is CloudWatchLogs- with the date that it occurs, with the type of the log stream names. Given that, we now know how we need to configure our index setting.</p>
<p>So I’m actually going to set this to cwl for CloudWatch Logs and set it to…use a time stamp field name of @timestamp. So this is the correct configuration for when we’re doing CloudWatch log segregation in Kibana. So once I hit create, I should see all of my metadata start showing up since we already acquired some events from running before, and that’s actually sufficient for us to begin using the discovery module here where I can see I start seeing all of my events that came through. Now, this is not particularly useful until we actually start creating some more logs. So let’s create an error and see what happens.</p>
<p>So I’m going to create an error by…if we can see my default when I switch cases if I select an operation that is unknown then I can get it to throw an error. So let’s do that. Going to set it up for an operation of many question marks, hit save and test, run that a couple times. We can see some very angry execution results with JSON representations of an error as well as the input value itself.</p>
<p>So if we think about this, this could be if somebody fuzzes your API and sends you some input that you don’t understand. Perhaps your API doesn’t support unicode. Rather than an intentional error like I’ve created here, this could easily be a 500 error or a 404 or some other problem that might organically occur where you want your API if this was a stream that’s was being logged to standard out even on an EC2 instance. Then we would also want that to show up on this end.</p>
<p>What happens if I just search? We can see a whole bunch more events because I ran the test a couple more times. We can see my older event that I ran that ping up here and then I sent some more events where I have question marks. We can see the incoming event itself and we can see some error message events. So, say for instance, I want to know how many times there were unrecognized operations. Then I can see that there were three events during which there were unrecognized operations. We can also see the frequency with which they occurred. They occurred three times very frequently there. So this is excellent if you’re trying to find specific values. Now you could also use this if you’re trying to debug specific customers.</p>
<p>So again, this is contrived, but say we also set a customer ID field and set it one through zero, and search for customer ID. So we can see that we have this customer ID string and once I do the search for it after I allowed some time for those events to propagate, we can see that whenever I want my customer ID to show up, we can search for just the customer ID and see these events show up. I can also go back and expand by different properties, see the messages that are coming through etc, etc. So this is what a log aggregation system look like.</p>
<p>Now, we can also do our log analytics. If I want to visualize, for instance, an area chart from a new search, if I want my X-axis to be a histogram or a date histogram on time stamp over an automatic interval and add a sub-aggregation where I could split the area and I want to go by terms inside of the field for say on event or it can do an application. We could see the different frequencies of terms as we look inside of a system. So I could see where different numbers or terms showed up, and I can actually further refine this and say “Okay, I only want to see a histogram of these events.”</p>
<p>So we can do all kinds of things like graphing on different fields and just generally perform magic. You can also add visualizations to different dashboards. You can set up any number of different things that you want based on these sub-aggregations. And of course, we have this excellent capability of doing searches.</p>
<p>There are commercial products that you can use that do the similar behaviors here, but we’re a big fan of open source and being able to do value-added automation on top of our system. So we can see this big, nice log stream here. We have the original streams and sources that it came from, the AWS account associated with it, and we can do free text search. So again, we can search for specific error types if we so please.</p>
<p>So this kind of thing is very helpful for if you’re trying to do a debugging session if a customer’s complaining. Then, we of course want to be able to see what the customer’s talking about by, for instance, searching for the error code that was dumped onto the page if they’re on the phone and you’re trying to do some kind of support with them. You can use this for any number of things. It’s very helpful and it was very easy because all I had to do was create one of these domains, run through, create some sort of events so that they show up in CloudWatch logs, go to the log group, and check that streaming box and send it to Elasticsearch granting the role to the Lambda to allow the post. So that’s it for ELK stack demonstration.</p>
<p>I hope to see you soon on the next lecture in which we’ll be doing a little bit of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/slack-chatops/">chatOps</a> in which we do another trivial solution where I make something pop up in my slack channel whenever a certain event occurs in my Amazon account.</p>
<h1 id="Slack-ChatOps"><a href="#Slack-ChatOps" class="headerlink" title="Slack ChatOps"></a>Slack ChatOps</h1><p>Welcome back to Cloud Academy’s course on Advanced <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/introduction-27/">Amazon Web Services Monitoring Metrics and Logging</a>. Today, we’re going to look at a demo of a ChatOps system, which is a system whereby we post operational messages or alerts into a chat system. I’ll be using Slack today, since I like their API. It’s nice and easy, and I already have a domain that I could use. So without further ado, let’s get started.</p>
<p>So the first thing that we need to realize is that we can reuse from the previous demo the same kind of logic that we were using to degenerate these unhandled errors here. So I’m going to continue using that same Lambda, simply because it’s already pre-integrated with CloudWatch, and I don’t have to do any additional work to get this thing to generate logs and submit them to CloudWatch. I wouldn’t have to install the daemon like I do on an EC2 instance.</p>
<p>So looking at the function, all we need to do is give it an operation value that does not meet any of these cases, and we’ll get this uncaught exception, which is what I did. I gave it this name “Fatal Operation”. So how do we get the logs from this thing to show up in a Slack chat whenever there’s an error or something? Well, it’s relatively simple. I have to create a Lambda function that will post into Slack. So one way that I can do that is by creating one of these Slack integrations. So if I go to my custom integrations, I could configure an incoming Webhook. All you have to do is click Yes, and click Add Configuration. I have this set up to post into AWS, and then I copy my Slack Webhook URL, which I’m not going to show you because then you could post into my account. Then, once we realize that I have an endpoint that I can post into Slack with, I need to create a Lambda function that will actually let me post into Slack. So let’s do “SlackChatOpsDemo2”, then enter some code in that will handle the stream <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/events-everywhere/">events</a> that come out of CloudWatch.</p>
<p>So a couple things going on here, really only three. I receive the event, I un-Base64 the event, and I G-unZip the event. So CloudWatch Logs is going to try to save bandwidth by transporting all the events pre-GZipped and Base64-ed so they compress well. So all I’m doing here is decompressing and then parsing and re-stringifying in a nicer format for me to post into Slack. When I post into Slack, literally all it is is adding some marked down-style block code formatting, joining it with new lines, and then running an HTTPS request against my Slack, Hook, host and path.</p>
<p>So I need to then assign a role to the Lambda, which will allow it to both create new logs and read from log streams. So I’m going to create a new role policy and manually edit this one. There’s no out-of-the-box role for us to use for this kind of ChatOps system. But I can just give Lambda full access to logs for now, allow it to have access to those full sets for the logs, then up my timeout to about five seconds so we don’t have Slack issues, and create the function.</p>
<p>So the next thing I need to do is start streaming that data from a log group which corresponds to my Cloud Academy Dynamo Lambda, which is where I’m generating my error, so my Cloud Academy Dynamo Lambda. Stream that to AWS Lambda. If I can actually find my demonstration Lambda, that Lambda that we just created that does the G-unZipping and Base64 decoding, we want to strip it over there. Since our event generator again is this Cloud Academy Dynamo Lambda, we want to use the AWS Lambda format.</p>
<p>So we should now be streaming from our Dynamo Lambda. We’ll automatically create log lines that go into CloudWatch Logs at the top of the stream. So this is an inserting or publishing function into a stream that is our log stream. Then, use this other Lambda, this Slack ChatOps Demo 2 as my consumer. So I set that subscription up when I went over and checked this box, and started subscribing to that log stream with this other Lambda. So now we should expect to be able to run a test, have the “Fatal Operation” fail, check the actual logs, see that we have some fatal operations, which we’ll then subsequently post into Slack. So you can see that I have Slack showing up here. This is the un-Base64, un-GZipped message.</p>
<p>Again, we went from Lambda, which generates log event lines, sends them into CloudWatch Logs here. We then went to the CloudWatch Logs Group user interface, and went to our subscriptions and added a subscription filter to Lambda. This Lambda was the recipient Lambda, which has these events coming in. These events correspond to our CloudWatch Logs event data. The way that those are formatted and sent to us to save bandwidth are Base64 and GZip, so we had to undo those steps. We had to decode the Base64 and then G-unZip, and then simply publish to Slack via the API endpoint and, voila, we have our ChatOps system. We can see something terrible happened, and we have fatal operations, and now our entire team that’s on our chat system should be able to see a message like this.</p>
<p>So hopefully, you enjoyed seeing the practical way to implement a very simple ChatOps system using totally serverless technologies, as well as CloudWatch log streams, and treating logs as a first-class citizen for insight and automation. ChatOps, which is one of these things, is a very simple way for us to alert people on our team when certain log events that are scary or frequent, or whatever other metric we want to us, publish into Slack and notify the entire team.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:28" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:28-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:03:56" itemprop="dateModified" datetime="2022-11-19T23:03:56-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/" class="post-title-link" itemprop="url">AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:27" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:27-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:54:50" itemprop="dateModified" datetime="2022-11-19T22:54:50-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Resources-Referenced"><a href="#Resources-Referenced" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/">How to implement &amp; Enable Logging Across AWS Services (Part 1 of 2)</a></p>
<h2 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to the second part of this two-part series of courses which have been designed to help you understand how <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> performs logging for a number of key services and how to use this data captured by the logs to resolve instance and identify security threats. If you haven’t already taken part one of the series, then you can use the link on the screen.</p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy, specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the detail shown on the screen. Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#x70;&#112;&#111;&#114;&#x74;&#64;&#99;&#x6c;&#111;&#x75;&#100;&#97;&#x63;&#x61;&#100;&#x65;&#x6d;&#x79;&#46;&#x63;&#111;&#109;">&#x73;&#x75;&#x70;&#112;&#111;&#114;&#x74;&#64;&#99;&#x6c;&#111;&#x75;&#100;&#97;&#x63;&#x61;&#100;&#x65;&#x6d;&#x79;&#46;&#x63;&#111;&#109;</a> where one of our cloud experts will reply to your question. </p>
<p>The focus of this two-part series is to understand the logging process and how to monitor this data to your organization’s benefit from both an operational and security perspective. As a result, those who have the following or similar roles would benefit from this content: cloud security engineers, cloud security architects, cloud administrators, cloud support and operations, and compliance managers. </p>
<p>As this is part two in the series, the content will continue the theme of logging across AWS services by explaining the following: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/cloudfront-access-logs/">CloudFront Logs</a>. Here I’ll look at how to log the requests from each user requesting access to your website and distribution. Next, I look at VPC Flow Logs. And this lecture focuses on how to log the network data, traversing your network interface cards within your VPC. Next, I focus on AWS Config Logging, and here I look at how AWS Config provides a timeline of changes against your AWS resources. And then lastly, I look at filtering and searching of log data. And within this lecture, I look at how to use Amazon Athena to query logs being stored on S3. </p>
<p>For information, part one of this series dived into the following: the benefits of logging, and in this lecture I focused on the core principle of why logging is important. I also looked at CloudWatch Logs, and within that lecture I explained how to implement logging using CloudWatch Logs and the associated agent. I also touched on CloudTrail logging, and CloudTrail records all API calls so here I explained how you can use these logs and how they are constructed. I then looked at the monitoring of those CloudTrail Logs, and here I looked at how you can use CloudWatch to monitor CloudTrail events. And finally in part one, I looked at S3 Access Logs, where this lecture focuses on the logging capabilities of S3 buckets. </p>
<p>The objectives of this series is to enable you to understand when and why you should enable logging of key services, how to configure logging to enhance incident resolution and security analysis, and you’ll understand how to extract specific data from logging data sets. This is an advanced level course series, and so you should be familiar with the following services and understand the individual use cases and feature sets. Throughout this series, I will reference a number of URL links which will help and direct you to related information on specific topics. To make these links easily available to you, I have included them at the top of the transcript within the lecture the they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#115;&#117;&#x70;&#x70;&#x6f;&#114;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#117;&#x64;&#x61;&#x63;&#x61;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#111;&#109;">&#115;&#117;&#x70;&#x70;&#x6f;&#114;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#117;&#x64;&#x61;&#x63;&#x61;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#111;&#109;</a>.</p>
<h1 id="CloudFront-Access-Logs"><a href="#CloudFront-Access-Logs" class="headerlink" title="CloudFront Access Logs"></a>CloudFront Access Logs</h1><h2 id="Resources-Referenced-1"><a href="#Resources-Referenced-1" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">Web distribution log file format</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">RTMP distribution log file format</a></p>
<h2 id="Transcript-1"><a href="#Transcript-1" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello, and welcome to this lecture focusing on the access logs generated by Amazon CloudFront. Amazon CloudFront is AWS’s content delivery network that speeds up distribution of your static and dynamic content through its worldwide network of edge locations. When you use a request content that you’re hosting through Amazon CloudFront, the request is routed to the closest edge location which provides it the lowest latency to deliver the best performance. When CloudFront access logs are enabled you can record the request from each user requesting access to your website and distribution. As with S3 access logs, these logs are also stored on Amazon S3 for durable and persistent storage. There are no charges for enabling logging itself, however, as the logs are stored in S3 you will be stored for the storage used by S3. </p>
<p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/introduction/">logging</a> process takes place at the edge location and on a per-distribution basis, meaning that there will not be data written to a log that belongs to more than one distribution. For example, distribution a, b, c, will be saved in a different log to that of distribution d, e, f. When multiple edge locations are used for the same distribution, a single log file is generated for that distribution and all edge locations write to the single file. </p>
<p>The log files capture data over a period of time and depending on the amount of requests that are received by Amazon CloudFront for that distribution will depend on the amount of log fils that are generated. It’s important to know that these log files are not created or written to on S3. S3 is simply where they are delivered to once the log file is full. Amazon CloudFront retains these logs until they are ready to be delivered to S3. Again, depending on the size of these log files this delivery can take between one and 24 hours. </p>
<p>When these log files are delivered they use a standard naming convention as follows. So let’s say for example you had the following settings. The bucket name was access-logs, the prefix was web-app-a, and you had the following distribution ID. Then your name and convention for the log would look something like this. Let me now show you a very simple demonstration on how to enable log in for your CloudFront distribution. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>So setting up access logs for your CloudFront distributions is very simple and easy to do. So let’s go into CloudFront. I’ll just select an existing distribution here, and then if you click on distribution settings and under the general tab you select edit, and then if we scroll down these settings here you’ll see a section where it starts referring to logging. And at the moment I have logging off. So to enable logging I simply click on on and then I select the bucket in S3 where I want the access logs to reside, so I’m going to select CloudFront Access Logs, which is an existing bucket I have set up for this. Now here I can add a log prefix if I want to, if I’ve got different distributions, etc. I’m just going to leave that as blank for this demonstration. And here we can have cooking logging on or off, which will log all cookie data within the request, and it’s as simple as that. And then once you’re happy with that you just click on yes to confirm your changes. And now any access requests that go via your CloudFront distribution will be logged via S3. And that’s it. </p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>To perform the demonstration that I just completed and to access the logs when they are stored, you will need specific permissions to the S3 bucket designated for logging. To enable the log in for your distribution, the user account activating that feature must have full control on the ACL for the S3 bucket, along with the S3 GetBucketAcl and S3 PutBucketAcl. The reason for this is that during the configuration process, CloudFront will use your credentials to add the AWS data-feeds account to the ACL with full control access. This is an account used by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> which will write the data to the log file and deliver it to your designated S3 login bucket. Therefore, if you’re trying to enable the login feature for your distribution and it’s failing, then you should check your access to ensure you have the required permissions. </p>
<p>Depending on the delivery type of your CloudFront distribution, either WEB or RTMP, the log output will vary. The number of fields within the log files differ between the two types. Web distributions have a total of 26 different fill types for each entry within the log, whereas the RTMP distributions only have 13. I won’t go through every single field explaining their purpose and use, however, I want to highlight a few points of interest starting with the web delivery type. These logs contain information which allow you to identify the following. The date and timestamp of the request of the user and which edge location received this request, source metadata of the requester including IP address details, HTTP access method of the request, such as PUT, DELETE, or GET, etc., the HTTP status codes of the request such as 200, the distribution domain name relating to the request, and the encryption and protocol data used in request such as SSL, V3, or AES256-SHA. For full information on each field and options please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">link</a>. </p>
<p>Now looking at the RTMP delivery type, the points of interest are as follows. Again, a timestamp of the request of the user and which edge location received this request, the source IP address of the requester, the event being carried out by the requester such as play, pause, or stop, and the URL of the page where your SWF file is linked to. Again, for full information on field data captured within RTMP logs you can view the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">here</a>. </p>
<p>One final feature of logging with CloudFront is cooking logging. If you enable this within your distribution, then CloudFront will include all cookie information with your CloudFront access log data. This is only recommended if your origin of your distribution points to anything other than S3 such as an EC2 instance as S3 does not process cookie data. </p>
<p>That now brings me to the end of this lecture covering AWS CloudFront logs. Coming up next I shall be looking at the logs generated at the network level within your VPC with the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC flow logs</a>.</p>
<h1 id="VPC-Flow-Logs"><a href="#VPC-Flow-Logs" class="headerlink" title="VPC Flow Logs"></a>VPC Flow Logs</h1><h2 id="Transcript-2"><a href="#Transcript-2" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture covering VPC Flow Logs. Within your VPC, you could potentially have hundreds or even thousands of resources all communicating between different subnets both public and private and also between different VPCs through VPC peering connections. VPC Flow Logs allows you to capture IP traffic information that flows between your network interfaces of your resources within your VPC. This data is useful for a number of reasons, largely to help you resolve incidents with network communication and traffic flow in addition to being used for security purposes to help spot traffic reaching a destination that should be prohibited. </p>
<p>Unlike S3 access logs and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/cloudfront-access-logs/">CloudFront access logs</a>, the log data generated by VPC Flow Logs is not stored in S3. Instead, the log data captured is sent to CloudWatch logs. Before creating your VPC Flow Logs, you should be aware of some of the limitations which might prevent you from implementing or configuring them. If you are running a VPC peered connection, then you’ll only be able to see flow logs of peered VPCs that are within the same account. Or if you are still running resources within the EC2-Classic environment, then unfortunately you are not able to retrieve information from their interfaces. And once a VPC Flow Log has been created, it cannot be changed. To alter the VPC Flow Log configuration, you need to delete it and then recreate a new one. </p>
<p>In addition to this, the following traffic is not monitored and captured by the logs. DHCP traffic within the VPC, traffic from instances destined for the Amazon DNS Server. However, if you decide to use and implement your own DNS Server within your environment, then the traffic to this will be logged and recorded within the VPC Flow Log. Any traffic destined to the IP address for the VPC default router and traffic to and from the following addresses, 169.254.169.254 which is used for gathering instance metadata, and 169.254.169.123 which is used for the Amazon Time Sync Service. Traffic relating to an Amazon Windows activation license from a Windows instance and finally the traffic between a network load balancer interface and an endpoint network interface. All other traffic both ingress and egress can be captured at a network IP level. </p>
<p>You can set up and create a flow log against three separate resources. These being a network interface on one of your instances, a subnet within your VPC, and your VPC itself. Obviously for option two and three, this will contain a number of different resources. As a result, data is captured for all network interfaces either within the subnet or the VPC respectively. I mentioned earlier that this data is then sent to CloudWatch logs via a CloudWatch log group. For every network interface that publishes data to the CloudWatch log group, it will use a different log stream. And within each of these streams, there will be the flow log event data that shows the content of the log entries. Each of these logs captures data during a window of approximately 10 to 15 minutes. </p>
<p>To enable your flow log data to be pushed to a CloudWatch log group, an IAM role is required for permissions to do so. This role is selected during the setup configuration of the VPC Flow Log. If your role does not have the required permissions, then your log data will not be delivered to the CloudWatch group. At a minimum, the following permissions must be associated to the role. In addition to this, you will also need to ensure that the VPC Flow Log service can assume that IAM role to perform the delivery of logs to CloudWatch. This can be achieved with the following permissions. </p>
<p>While on the topic of permissions, I want to also show you the required permissions for someone to review and access the VPC Flow Logs or indeed be able to create one in the first place. The following three EC2 permissions allows you to create, delete, and describe flow logs. These being ec2:CreateFlowLogs, ec2:DeleteFlowLogs, and ec2:DescribeFlowLogs. The logs:GetLogData permissions is used to enable you to list log events from a data stream. If you wanted to create flow logs, then you need to also grant the use of the IAM permission of iam:passrole which allows the service to assume the role mentioned previously to create these flow logs on your behalf. </p>
<p>Let me now show you how to create a flow log for an interface on an instance, a subnet, and lastly the VPC itself. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay so firstly I’m going to set up a VPC Flow Log for the running instance that we’ve used in a previous demonstration which was for the logging server. So what I need to do is go down to our network interfaces under network and security and select the ENI of the logging server. As you can see, it’s this bottom instance here. So if I select that interface, if I just drag this up a little bit, and we have three tabs here, details, flow logs, and tags. If we select the flow logs tab of this interface, we can see that there’s no flow log created as yet. </p>
<p>What we need to do is click on create flow log. Now we can select the filter for this flow log to only log either accepted requests or rejected requests so I’m going to select all so it gets accepted and rejected. We now need to select our role and I created a role earlier and I called that Flow-Logs-Role so that has the required permissions to push data to CloudWatch logs. And here we have the ARN of the role. The destination log group for CloudWatch, I set up a log group prior to this demonstration and I’ve just called this Flow-Logs. And then click on create flow log. And that’s it, it’s as simple as that. So now you can see for this eni interface here, we now have a flow log created. It gives it a flow log ID. Shows the filter which we have ALL here. Their destination log group. The ARN of the role and it’s currently active. So now any traffic going in and out of that interface on that EC2 instance will be captured and the data will be sent to the flow logs log group in CloudWatch. And let’s take a look at how you set up flow logs for a subnet. </p>
<p>So let’s go across to our VPC service. I have a couple of VPCs here and we’ll use our logging VPC. So if we go down to our subnets, and let’s select the public subnet for our logging VPC, now again we have the tabs for this subnet. We have the summary, route table, network ACL, etc, and we also again have the flow logs tab. Very simple process again. Click on create flow log. The same filters. Select the same role and the same log group. And then simply create flow log. And that’s now having the flow logs enabled on this particular subnet so all traffic going in and out of this subnet will be captured and sent to the flow logs log group. </p>
<p>And for the VPC, it’s very similar. So you simply select your VPC so we have our logging VPC here, again we have our flow logs tab. Create flow log. Select the role and the destination log group of flow logs and then create flow log and that’s it. So it’s very easy to set up your flow logs for your EC2 network interface clouds or your subnet or your entire VPC. And that’s it. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>Let’s now take a look at a record within one of these flow logs. When you access the logs, you will find each entry has the following syntax. These entries are defined as follows. Version, which is the version of the flow log itself. Account-id, this is your AWS account ID. Interface-id, this is the interface ID of which the log stream data applies to. Source address, this is the IP source address. Destination address, this is the IP destination address. Source port, this is the source port being used for the traffic. And the destination port is the destination port being used for the traffic. The protocol, this defines the protocol number being used for the traffic. Packets, this shows the total number of packets sent during the capture. Bytes, again this shows the total number of bytes sent during the capture. Start and end shows the timestamp of when the capture window started and finished. Action, this shows if the traffic was accepted or rejected by security groups and network access control lists. And the log-status shows the status of the logging through three different codes. OK, where data is being received by CloudWatch logs. NoData, this means there was no traffic to capture during the capture window. And SkipData, where some data within the log was captured due to an error. </p>
<p>One of the key fields from an incident response and troubleshooting perspective is the action field. For example, if you are troubleshooting an issue of traffic not being received by a particular resource, then you could check the VPC Flow Logs to see if the traffic is getting blocked at the subnet level by a network ACL. This will then allow you to review your entries within the NACL to make the changes that’s necessary from a security perspective. </p>
<h1 id="AWS-Config-Logging"><a href="#AWS-Config-Logging" class="headerlink" title="AWS Config Logging"></a>AWS Config Logging</h1><h2 id="Transcript-3"><a href="#Transcript-3" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture regarding AWS Config. AWS Config is a great security and compliance tool that integrates well with many other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS services</a>. As you may or may not know, AWS Config can perform the following functions. It can capture resource changes, so any change to a resource supported by Config can be recorded, which will record what changed along with other useful metadata or held within a file known as the Configuration Item, the CI. It can act as a resource inventory. AWS Config can discover supportive resources running within your environment, allowing you to see data about that resource type. It can store configuration history for individual resources. The service will record and hold all existing changes that have happened against the resource, providing a useful history record of changes. It can provide a snapshot in time of current resource configurations where an entire snapshot of all supported resources within a region can be captured that will detail their current configurations with all related metadata. It can enable notifications of when a change has occurred on a resource. So SNS is used with AWS Config to capture a configuration’s stream of changes enabling you to process and analyze the changes to resources. It can provide information on who made the change and when through AWS CloudTrail Integration. AWS CloudTrail is used AWS Config to help you identify who made the change and when and with which API. It can enforce rules that checks the compliancy of your resource against specific controls. Predefined and custom rules can be configured within AWS Config, allowing you to check resources compliance against these rules. It can perform security analysis within your AWS environment. A number of security resources can be recorded and when this is coupled with rules relating to security such as encryption checks, this can become a powerful analysis tool. And finally it can provide relationship connectivity information between resources. The AWS Management Console provides a great relationship allowing you to quickly see and identify which resources are related to any other resource. For example, when looking at an EBS volume, you’ll be able to see which EC2 instance it is connected to. </p>
<p>From a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/introduction/">logging</a> perspective, the ability to store configuration history is highly valuable. So let me look into this a little further. The configuration history uses Configuration Items, CIs, to collate and produce a history of changes to a particular resource. This allows you to see the complete set of changes made to a resource over a set period of time. The information can be accessed either programmatically though the AWS CLI using the following command, or you can also specify the resource type. So for example, if you wanted to look at the configuration history for a subnet you could enter the following into the CLI. Or you could also access this history via the AWS Management Console. Additionally, AWS Config also sends a configuration history file for each resource type to an S3 bucket that is selected during the setup of AWS Config. This configuration file is typically delivered every six hours and it contains all CI changes for all resources of a particular type. For example, there’ll be one configuration history file covering six hours for all RDS DB instant changes in one region. A Configuration Item, or CI as it’s known, is a key component of AWS Config. It is comprised of a JSON file that holds the configuration information, relationship information, and other metadata as point in time snapshot view of a supported resource. All the information that AWS Config can record for resource is captured within the CI.</p>
<p> A CI is created every time a supported resource has a change made to its configuration in any way. In addition to recording the details of the affected resource, AWS Config will also record CIs for any directly related resources to ensure the change did not affect those resources, too. For example, if there was a rule change to a security group, perhaps additional rules were added with new ports. AWS Config will record all CI information for that resource. But it also gathers CI information for any instances that were part of that security group. These will then be sent to the configuration stream. As so much data is gathered within these CIs, it’s important we look at these in further detail. </p>
<p>So for every CI generated there will be five different sections. Firstly, metadata. This essentially contains details about the configuration item itself. So within this metadata we have both a version ID and a configuration ID, which uniquely identifies the CI. In addition to this, other information includes an MD5Hash that allows you to compare other CIs already recorded against the same resource as well as ensuring there are no duplications. And then we have the time of the capture and a state ID, which puts the CIs for a particular resource into an order of sequence. </p>
<p>Attributes, this holds common attribute information against the actual resource. Within this section we also have a resource ID and any key-value tags that are associated to the resource. The resource type is also listed. For example, if this was a CI for an EC2 instance, the resource types listed could be the network interface or the EIP for that EC2 instance. The Amazon Resource Name, the ARN, for the resource would also see shown along with the availability zone that the resource belonged to. Bear in mind that for services and resources that are not fixed to a particular availability zone, such as IAM, then this section would not be applicable. Lastly, the time that the resource was created would also be given. </p>
<p>Relationships, this holds information for any connected relationships that the resource may have. So within this section it would show a clear description of any relationship to other resources that this resource had. For example, if the CI was from EC2 instance the relationship instance may show the connection to a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC</a> along with a subnet that the EC2 instance resides in. </p>
<p>Current configuration. This will display the same information that would be generated if you were to perform or describe or list API call made by the AWS CLI. AWS Config uses the same API call to get the same information. So depending on the API call and resource, different information will be returned, which is resource specific. </p>
<p>Related events, and this relates to AWS CloudTrail. This will display the AWS CloudTrail event ID that is related to the change that triggered the creation of this CI. There is a new CI made for every change made against a resource. As a result, a different CloudTrail event ID will be created. This allows you to deep dive into who or what and when made the change that triggered the CI. A great feature allowing for some great analysis to be taken, specifically when this affects security resources. </p>
<p>As you can see, the configuration item is a fundamental aspect of AWS Config when recording changes and data made to a supported resource. The configuration history file is stored on the S3 bucket that was selected at the time of configuration and is used to store all the configuration history files that are generated for each resource type, which happens every six hours. If you have multiple AWS accounts you may want to aggregate your configuration history files into the same S3 bucket for your primary account. However, you’ll need to grant write access for this service principle, config.amazonaws.com, and your secondary accounts with write access to the S3 bucket in your primary account. Let me now demonstrate how to use the AWS Management Console to view configuration details and history within my account. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so I’ve just logged into my AWS account, and I’m going to go up to services and you can see the AWS Config is under management tools. So if we select config, and this is the first splash screen you’ll be presented with if you don’t have AWS Config started as yet. So, let’s get started by clicking on the get started button. And then there’s three steps to setting AWS Config up. </p>
<p>So the first step is configuring the settings. And this is where we essentially start the configuration recorder, by identifying what resources we’d like included in our AWS Config configuration. So starting from the top, we can either select all resources within this region, remember, AWS Config is region specific. So we can choose to record all the resources supported within this region, and currently I’m in the London region. And we can choose to include global services such as IAM, etc. But for this demonstration we’ll just choose to record all resources in this region. </p>
<p>Here you can select specific types if you’re required. So if I unticked this we can then go down to this dropdown box and select specific resource types that we’d like set up for this configuration. So you can be quite specific. Selecting the load balancer or different elements of IAM and again different elements of RDS, etc. So there’s a list that it can go through if you just want to record a specific resource type. But like I said for this demonstration I’m just going to select all resources. </p>
<p>Next we need to define our S3 bucket. And this is where our configuration history and snapshot files are stored. So we can ask AWS Config to just create a new bucket for us and it will prefix it with this bucket name here or we can choose an existing bucket from my account, by selecting the dropdown list or we can select a bucket from another account. So if you had multiple accounts you can send the configuration history files and snapshot files for all those accounts to a single primary AWS account in a single bucket. So for this example, though, for this demonstration I’m just going to ask AWS Config to create a new bucket for us. Config bucket Cloud Academy. </p>
<p>Now moving down to the SNS topic. Now this is the configuration stream. So for any events that AWS Config picks up it will send it to this SNS topic for the configuration stream. Now we can create a new topic, and again, AWS Config will just give us topic name here or we can select to choose another topic from your account or again from another account. For this demonstration we’ll just ask AWS Config to create the topic name for us, and I’ll just add in Cloud Academy. </p>
<p>Now finally we have the AWS Config role, and this is where we spoke about the different permissions. And this is required to allow AWS Config to send the configuration history file and snapshots to S3 as well as having access to send events to the SNS topic. I’m not forgetting enabling AWS Config to be able to kind of go out and poll your resources as a describe or list API call to get the details of those resources. And again, for this demonstration, I’m just going to ask AWS Config to create a role for us. I’ll just add Cloud Academy on the end. So that’s settings. So this screen is essentially your configuration recorder. By setting this up you’re asking AWS Config to start recording. We have set up the S3 bucket to allow our configuration history files and our snapshots files to be captured. And we have created our configuration stream by configuring an SNS topic. And we have also allowed AWS Config to have the necessary permissions to carry out those functions. So once that’s all set up we’ll click on next. </p>
<p>And here we have a set of predefined AWS managed config roles. This is a number of templates that you can select to check the compliance of your resources against these rules. So for now I’m just going to click on skip. And here we’re on the final stage which is review. So we can see here the resource types we’ve selected, which are all resources, and we’ve not chosen to include global resources. We have our S3 bucket set up for our history files and snapshots and we have our SNS topic for our configuration stream configured, and we have the permissions given by the AWS Config role. And that’s essentially the elements to setting up AWS Config. So once you’re happy with that and once you’re done with that you can click on confirm, and you can see that it’s now setting up AWS Config for this region. And that’s it. It’s set up and it’s running. </p>
<p>So whatever create, deletes, or changes I make to support the resource types within this region AWS Config will not begin to monitor that and record it and log it within the configuration stream and also within the configuration history files. Now what I’ve done, I’ve already set this up in another region, and I’ve made a few changes. So what I’ll do now, I’ll swap to that region, and we can take a look at some of the other relevants such as the configuration history file, the timeline of events, etc., and look at how the relationships are set up within this dashboard. So let me swap over to another region which is island. </p>
<p>So as you can see, I’ve just swapped to the Ireland region, and we don’t have the option of setting up AWS Config because I’ve already set it up and only have it running once within the region. So let’s take a look at what we have here. Straight away here we can see that I’ve had existing rules. I had a rule name of S3 bucket versioning enabled. So I wanted to check if my S3 buckets had versioning enabled. So this is one of the config rules. So if I just open that up I can see that a number of these buckets do not have versioning enabled. And they have been marked as noncompliant. I can still use these buckets. I can still write to the buckets. I can still delete objects from those buckets. It’s just notified me that these buckets are noncompliant with this specific config rule, which is to check if, it says it checks whether versioning is enabled for your S3 buckets. </p>
<p>Now let’s have a look at some of the other resources that we have. So I can filter on specific resource type that I want to take a look at. So let me take a look at my VPC. Let me look up everything to do with my VPC. So I can see here that I have two VPCs, and let’s take a look at one of them. And this has now taken us to the timeline of events of these VPCs. So on the 15th of March at 11:25 there was a resource discovery. So that’s probably when I activated AWS Config within this region and it went out and done a discovery of all resources. And then at 11:32 there was a change on this resource item. So lets take a look at the rest of the page. So under the configuration details we have a number of details here. We have the Amazon Resource Name, the resource type, the ID, the CIDR block of the VPC. So there’s different configuration details that we can see for different items. </p>
<p>Now we can also see the tag name here, which is CA demo and further down we can look at the relationships. So this will show other resources that have a direct relationship with this VPC. So we can see we have a couple of network ACLs there. We have an instant gateway, we’ve got some root tables, security groups, and some subnets. So each of these resource types has a direct relationship with this VPC, and if I wanted to I can select straight from here to click on that network ACL and it will take me to the details for that ACL. And again, we have some resource type information here, the ARN, and the ID, etc. Again, if we click on the relationships we should see the VPC that we just came from. So let’s go back to our VPC. So as you can see it’s very easy to look at the relationships between different resources and navigate between them, and it’s kind of grouped in a logical manner to allow you to quickly get between the different resource types that you’d like to. </p>
<p>Now we can see up here that on the 15th of March 11:23 there was a change. Now if we go down to our changes here, we can have a look at what that change was. We can see it’s defined by two different categories, a configuration change or a relationship change. We can see that by the number that this change was to do with relationships. And it looks as though a new subnet was added to this VPC. And then finally, we have our CloudTrail events. And this will capture any API calls that made changes to resources. However, it will only capture these for the previous seven days. So because this change was made on the 15th of March and it is now the 30th of March, so if we look at the CloudTrail events there’s nothing there, they have expired. So what I’ll do, I’ll go and make a couple of changes. I’ll create a new subnet within this VPC, so I’ll pause the video, wait a few minutes, and then I’ll start it again and we can analyze the CloudTrail event of that new change. </p>
<p>Okay, so I’ve gone off and I’ve created a new subnet within this VPC so we can see that the 30th of March, which is today, that there was a change that occurred and two new events. So let’s go ahead and take a look at the change. So we can see that there is a new subnet. This is the subnet that I just created. And if we look at the CloudTrail events, we can see that we have the creation of the new subnet there. Now if we click on the actual CloudTrail event itself it will take us to CloudTrail and we can look at additional information. So now we’re looking at the details of the API call itself, and we can see a number of different information here. We can see the user that created it, the source IP address, the time, the events source, and also the region as well, along with any affected resources as well. </p>
<p>So here we can drill down into exactly what time it occurred. As you can see up here the date and time and by who and what event actually was created. So as you can see, clicking on that CloudTrail event you can get additional information to kind of help you with resolving instance and looking at potential security breaches to try and gather more information as to identifying who is doing what and when. So now let’s take a look at the configuration history, which is stored in S3. So let’s go across to S3. So if I go to the bucket that I use for the ireland region, and then navigate through the folders to the correct date and time of today, you can see the configuration history folder. And then if I download one of these files and then open that, we can see here that that configuration history file contained the subnet creation that we just created for our VPC, and it shows you all the different information about it, the availability zone, the CIDR block used, etc., etc. And there’s the subnet ID. And it also highlights the relationships to other resources for that new subnet such as any network ACLs that may be associated. So that’s the configuration history file that’s stored in S3 in a JSON format. That brings us to the end of this demo. So we look to how to set up AWS Config for a region. We then looked at the details of a specific resource type. We looked at the VPC and how the timeline of events work. We looked at a couple of changes and also the CloudTrail event logs as well. And then finally we looked at one of the configuration history files as a JSON document. That now brings me to the end of this lecture covering the AWS Config service and the config history login data.</p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="Filter-and-Query-data-with-Amazon-Athena"><a href="#Filter-and-Query-data-with-Amazon-Athena" class="headerlink" title="Filter and Query data with Amazon Athena"></a>Filter and Query data with Amazon Athena</h1><h2 id="Transcript-4"><a href="#Transcript-4" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture. We will learn how to analyze and search for specific data across log files being stored in S3. If your log data is being stored in S3, such as your CloudTrail logs, then you can use Amazon Athena to query that data within S3 to search for specific entries. The following demonstration has been created by Jeremy Cook, one of our AWS expert trainers here at Cloud Academy. In this demonstration, Jeremy will walk through the steps required to set up Athena to allow you to query CloudTrail log data.</p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>In this demo we’ll walk through the steps required to set up Athena to allow us to query CloudTrail log data. This demo will involve configuring CloudTrail, S3, EC2, and the Athena services. The end result will allow us to perform SQL queries against CloudTrail data stored in an S3 bucket. This type of setup will aid your DevOps and SecOps experience when building on top of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS platform</a>. </p>
<p>Let’s get started. Within the AWS console, select the Athena service. The first step involves us creating a new Athena database, which will host our custom data table created later in this demo. This task can be accomplished either through an Athena provided wizard or manually through the query editor using an appropriate SQL create statement. We’ll perform this by using SQL statements. But for learning purposes, I will show you where the wizard resides within the Athena console. Let’s see where this is located. Clicking the catalog manager, menu item in the Athena menu bar, takes us into the catalog manager. Initially, we have only the default Athena database to work with. We can click the Add Table action. This opens the Add Table Wizard. The first step requires us to either choose an existing database or create a new database. In this case, I’m going with the create new database option. I need to provide the name for the new database. In this case, I’m going with New DB. And then I need to provide the name for the new table. I’ll go with New Table. If I were to complete this wizard, I would additionally need to specify the S3 bucket location of our input set. However, I’m now going to cancel out of this wizard and perform the same process manually using SQL statements within the query editor. </p>
<p>Clicking back on the query editor menu item, we’re taken back into the query editor. Clicking within the editor itself clears the area. I’m now going to switch over to my visual code editor that has some pre baked SQL queries. I’m taking a copy here of a create database statement. Flipping back to the query editor, I paste the create database statement into the editor pane. As you can see, this particular create database statement will create as a new database named CloudTrail DB1. The create database statement, in this example, specifies an S3 bucket location. This bucket will be used to store the new database’s catalog. Let’s go and create this S3 bucket now, opening the S3 console in a separate browser tab. Clicking the Create Bucket button. In the resulting create bucket window, we paste in our bucket name that our create database statement references. We then hit the Create button at the bottom of the window. This creates our S3 bucket that will store the database catalog. </p>
<p>Okay, great. Our bucket has been created successfully as shown here. Next, we’ll go back to the Athena query editor. The create database statement can be configured to execute with metadata that may be relevant to your cause. In this example, I’m simply going to set the creator as myself, the company to be Cloud Academy, and created to the current year. With all this in place, we can now go ahead and execute the create database statement. We do so by clicking the Run Query button. Let’s do this now. We now wait for the query to complete. As we can now see, the create database statement has executed successfully, as per the query successful response in the result section. Additionally, we can see that we have our new database now displaying in the left hand side menu. We’ll now create our first table within our new database. </p>
<p>Let’s flip over to Visual Code and take a copy of a pre built table creation statement. Back within the query editor, we paste in the table creation statement. Here you can see that our new table will be named CloudTrail logs. The create table statement specifies all the relevant column attributes that CloudTrail tracks per order record. Next, we highlight the serializer-deserializer, or SERDE in short form, that is used. In this case, we’re using the Amazon Provided CloudTrail SERDE. Finally, I draw your attention to the S3 bucket location that needs to be provided. This particular S3 URL represents the location where our raw CloudTrail logs will reside once configured. Let’s go ahead and create this bucket. Switching back over into the S3 console, click the Create Bucket button. In the resulting Create Bucket window, we paste in our bucket name that our create table statement will reference. In our case, we’ll create our new bucket with the name CA CloudTrail Logs Demo. We then hit the Create button at the bottom of the window. This creates our S3 bucket that CloudTrail will be next to configure to push logs into, and for which Athena will scan from when executing our SQL queries. </p>
<p>Okay, great. Our bucket has been successfully created, as can be seen here. Switching back into the query editor window, we now paste in the S3 bucket name we just created. Okay, everything looks ready. Let’s now click on the Run Query button. And again, our query has executed successfully. And in this case, our new CloudTrail logs table has been created. On the left hand side, we see our newly created table listed. Clicking on the preview icon to the right of the table name executes the sample query now shown in the editor pane. This query will perform a Select All across the table, but limited to the first 10 rows. Since our CloudTrail bucket has yet to be populated, it’s expected that the query will return an empty result set as it does. Next, if we expand the table name itself, we see the column names and types that define it. Finally, clicking on the table properties icon, we are presented with a view of all the respected table properties associated with our new table. Important properties include, table name, database name, S3 bucket location, and serialization library. Let’s now go and establish a new CloudTrail trail and configure it to push its logs into our S3 CloudTrail bucket. </p>
<p>Under services, select the CloudTrail service. Once in the CloudTrail console, click the Create Trail button. Give the new trail a name. Here we’re gong to call ours CA CloudTrail Logs Demo. Leave all defaults as is until we get to the storage location section. Disable the create new S3 bucket option, and instead, select the name of the S3 CloudTrail bucket that we built earlier. Next, under Advanced, disable the enable log file validation option. This is unnecessary for this demo. Finally, click the Create button at the bottom of the screen. If all goes well, we should see fairly quickly our new trail has been provisioned successfully, as we do now. </p>
<p>Let’s switch over into the S3 console, and check to see if our newly created trail is publishing events into our bucket. Clicking on our CloudTrail configured bucket and drilling down into the lowest folder, we can see that we are indeed receiving logs from CloudTrail. This is great. Let’s go back into the Athena console and perform a couple of queries against this data. Clicking on the preview icon in the right of our CloudTrail table, kicks off the sample query for us again. And we’re now successfully seeing some early results coming through. Next, we’ll flip across to the Visual Code and copy a pre configured SQL select statement. Back within the query editor, we paste in the select statement. Before we execute, let’s click the Format Query button and have the editor reformat the query for us. This is a great feature that aids the readability of any SQL statements that we craft by hand. </p>
<p>Okay, running the formatted query still returns just four rows of data. This implies we’re still waiting for more CloudTrail logs to be delivered into our S3 bucket. Okay, now that we have all of the individual parts wired up successfully, let’s try out the following scenario. We’ll create a new example only security group within the EC2 service. The security group itself won’t be attached to anything. We’re creating it only to generate and capture the associated API calls within CloudTrail, for which, we will eventually query for within Athena. We’ll add in some inbound rules on this new security group. Performing these actions will generate CloudTrail data that will be published into our CloudTrail S3 bucket. The end result being that we should be able to query and discover these actions within Athena. Right, let’s start by heading over to the EC2 console. Click into the Security Group section, and then click the Create Security Group button. Give the security group a name. Here we name ours DemoSG. Don’t worry about setting the VPC. In the inbound rule section we’ll add a couple of rules. Clicking the Add Rule button, we add the first rule, allowing incoming traffic from source IP address 1.1.1.1&#x2F;32 and to port 1000. Add a second rule. This time to allow incoming traffic from source IP address 2.2.2.2&#x2F;32 and to port 2000, and then click the Create button. Next, we need to take a copy of the security group ID for the security group we just created. We’ll use this within our Athena query. </p>
<p>Jump back into the Athena query editor and update our like clause referencing the security group ID we just copied. This now tells Athena to search for all records who your request parameters attribute contains the security group ID we pasted into the like clause. Okay, let’s now execute this query and see if we get any results. As you can see, no results have come back, likely due to relatency involved in CloudTrail receiving, processing, and saving out to the S3 bucket. Let’s try again at approximately five minutes time. Okay, running the query again now provides us with results. As you can see, there are six rows in our output. Scrolling across the fourth row until we see the request parameters column. Here we can see two of the inbound rules we attached to the security group earlier. The first inbound rule allows incoming traffic to port 1000 from source IP address of 1.1.1.1&#x2F;32. And the second inbound rule allowing incoming traffic to port 2000 from source IP address of 2.2.2.2&#x2F;32. </p>
<p>Okay, let’s now expand our query by adding an additional clause. This time we’ll filter out all events except for the authorize security group ingress event. Running this query now gives us back just the one row, as expected. Okay, let’s now take a quick look at some of the other useful features within the Athena console. Each query that you author in the editing will be saved and replayed at a later stage. So I’ll save our current query. Click the Save As button. Give the saved query a name and description. In this example, we’ll call ours Important SG Query for both name and description. Click the Save button, and our query is saved and accessible in the saved queries area. Clicking on the Saved Queries top menu item shows us all of the previously saved queries, including our just saved Important SG Query at the bottom of the list. If we click on the query, it will be recalled back into the editor pane as can now be seen. </p>
<p>Next, let’s look at the history feature. This allows us to examine all past executed queries. Here we can see the most recent query at the top of the list. This was our last query that we ran, where we added the extra and the clause to filter on the event name column. If we click this query, once again, it’s recalled to the editor pane. But additionally, it also shows us the results that were returned at the time the query was actually executed. Bonus points. Going back into the history feature, I’ll now highlight a couple of the other important attributes for each captured query. Firstly, each query has a state associated with it. Here our query succeeded. If it hadn’t, it would track as an error. Next, there is a time the query took to run captured in seconds. This is useful for performance tuning and troubleshooting. Then there is the amount of data scanned recorded in kilobytes. This is useful to understand how much each query is going to cost you. Finally, there is a download results link that allows you to get a local copy of the results. Clicking the link for this row downloads the results locally. We’ll now use our local terminal to output the contents of the file to the screen. </p>
<p>Again, we can see that the details of the two inbound rules we attached to the security group in question. The first inbound allowed incoming traffic to port 1000 from source IP address of 1.1.1.1&#x2F;32. And the second inbound allowed incoming traffic to port 2000 from source IP address of 2.2.2.2&#x2F;32. This concludes the demo. But before we finish, let’s quickly go through the process of doing some clean up within Athena. Firstly, we’ll drop our CloudTrail table. Back within the query editor we type the statement, drop table CloudTrail Logs. Running this query will drop our table, allowing us to then drop the database. Next, clear the editor and type the statement, drop database CloudTrail DB1. And execute it. This will drop our custom Athena database. Don’t forget to delete the CloudTrail trail and remove the S3 buckets as used in this demo.</p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>That now brings me to the end of this lecture. Coming up <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/course-summary/">next</a>, I will summarize the key points taken from the previous lectures of this course.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><h2 id="Transcript-5"><a href="#Transcript-5" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this final lecture within <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/introduction/">this course</a>. In this lecture, I want to summarize and highlight the key points from the previous lectures. </p>
<p>I started by talking about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/cloudfront-access-logs/">Amazon CloudFront Access Logs</a>. During this lecture, we learned the following points. Amazon CloudFront is AWS’s content delivery network that speeds up distribution of your static and dynamic content through its worldwide network of edge locations. When a user requests content via CloudFront, the request is routed to the closest edge location providing the lowest latency. CloudFront Access Logs can record the request from each user requesting access to your website and distribution. And the logs are stored on S3 for durable and persistent storage. The logging process for CloudFront takes place at the edge location and on a per distribution basis. And the log files capture data over a period of time and are dependent on the amount of requests received. CloudFront retains the logs until they are ready to be delivered on S3. And to enable the logging for your distribution, you must have FULL_CONTROL on the ACL for the S3 Bucket, along with the s3:GetBucketAcl permission and the s3:PutBucketAcl permission. And log output varies on the distribution type, whether this be Web or RTMP. And by enabling cookie logging, it will include all cookie information with your CloudFront Access Log data. </p>
<p>Following CloudFront, I then looked at network logging at the VPC level through the use of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC Flow Logs</a>. In this lecture, I explained that VPC Flow Logs allow you to capture IP traffic information that flows between your network interfaces and your resources within your VPC. The log data generated by VPC Flow Logs is then sent to CloudWatch Logs. And if you are running VPC peered connections, then you’ll only be able to see Flow Logs of peered VPCs in the same account. Flow Logs are not available for EC2-Classic environments. And once a VPC Flow Log has been created, it can’t be changed. The following traffic is not monitored and captured by the logs: DHCP traffic within the VPC, traffic from instances destined for the Amazon DNS Server, any traffic destined to the IP address of the VPC default router, traffic to and from 169.254.169.254 and 169.254.169.123, traffic relating to an Amazon Windows activation license, and traffic between a Network Load Balancer Network Interface and an Endpoint Network Interface. Flow Logs can be set up for a network interface on an instance, your subnet, or your entire VPC. And each interface that sends data to CloudWatch will do so in its own stream. Specific permissions are needed to allow VPC Flow Logs to push data to CloudWatch, as well as permissions to assume the role with the required permissions, these being CreateLogGroup, CreateLogStream, PutLogEvents, DescribeLogGroups, and DescribeLogStreams. And the log files have the following syntax. </p>
<p>Next, I reviewed how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/aws-config-logging/">AWS Config</a> uses configuration history to display all changes within your environment. During this lecture, we learned that AWS Config is a great security and compliance tool that integrates well with many other AWS services. The Configuration History uses Configuration Items, CIs, to collate and produce a history of changes to a particular resource. AWS Config sends a Configuration History file for each resource type to an S3 Bucket that is selected during the setup of AWS Config. And this configuration file is typically delivered every six hours. A Configuration Item is comprised of a JSON file that holds the configuration information, relationship information, and other metadata as a point in time snapshot view of a supported resource. A CI is created every time a supported resource has a change made to its configuration. And a CI consists of the following sections: metadata, attributes, relationships, current configuration, and related events. And it’s important to note that you can aggregate Configuration History files from multiple accounts into one S3 Bucket. </p>
<p>The final lecture focused on how to retrieve data from your logs that are being stored on Amazon S3, and this was shown via a demonstration. </p>
<p>That now brings me to the end of this lecture and to the end of part two of this two-part course series. If you have now viewed both parts, you should now have a deeper understanding of some of the logging capabilities that AWS provides across a number of key services. You will also have an insight into how these logs are constructed and how to search for specific information that you might need to use in your day-to-day operations. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. </p>
<p>Thank you for your time, and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="1Introduction"><a href="#1Introduction" class="headerlink" title="1Introduction"></a>1<strong>Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/">How to implement &amp; Enable Logging Across AWS Services (Part 1 of 2)</a></p>
<h1 id="2CloudFront-Access-Logs"><a href="#2CloudFront-Access-Logs" class="headerlink" title="2CloudFront Access Logs"></a>2<strong>CloudFront Access Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">Web distribution log file format</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">RTMP distribution log file format</a></p>
<h1 id="3VPC-Flow-Logs"><a href="#3VPC-Flow-Logs" class="headerlink" title="3VPC Flow Logs"></a>3<strong>VPC Flow Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/whats-new/2018/08/amazon-vpc-flow-logs-can-now-be-delivered-to-s3/">Flow Logs delivery to S3</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/" class="post-title-link" itemprop="url">AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:25" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:25-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:54:14" itemprop="dateModified" datetime="2022-11-19T22:54:14-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to part one of this two part series of courses which has been designed to help you understand how <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> performs logging for a number of key services and how to use this data captured by the logs to resolve incidents and identify security threats. </p>
<p>Before we start I would like to introduce myself. My name is Stuart Scott. I am one of the trainers here at Cloud Academy specializing in AWS Amazon Web Services. Feel free to connect with me with any questions using the details shown on screen. Alternatively you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> where one of our cloud experts will reply to your question.</p>
<p>The focus of this two part series is to understand the login process and how to monitor this data to your organization’s benefit from both an operational and security perspective. As a result those who have the following or similar roles would benefit from this content: Cloud Security Engineer, Cloud Security Architect, Cloud Administrators, Cloud Support and Operations and Compliance Managers. </p>
<p>Part one of this series is constructed of the following lectures. The Benefits of Logging. This lecture focuses on the core principle of why logging is important. CloudWatch Logs. Here we’ll look at how to implement logging using CloudWatch Logs and the associated agent. Next I’ll look at CloudTrail Logging and CloudTrail records all API calls. So here I explain how you can use these logs and how they are constructed. Next we’ll be monitoring CloudTrail Logs, and here I look at how you can use CloudWatch to monitor CloudTrail events, and then finally in this course we look at S3 Access Logs, and this lecture focuses on the logging capabilities within S3 buckets. </p>
<p>Part two of this series will continue the theme of logging across AWS services by explaining the following. CloudFront Logs, and here we’ll look at how to log the request from each user requesting access to your website and distribution. Next I look at VPC Flow Logs, and this lecture focuses on how to look at the network data traversing your network interface cards within your VPC. Next is AWS Config Logging, and here I’ll look at how AWS Config provides a timeline of changes against your AWS resources. And finally in Part two, I look at Filtering and Searching of Log Data, and this lecture looks at how to use Amazon Athena to query logs being stored on S3. </p>
<p>The objectives of this series is to enable you to understand why and when you should enable logging of key services, how to configure logging to enhance incident resolution and security analysis, and you’ll understand how to extract specific data from logging data sets. </p>
<p>This is an advanced level course series, and so you should be familiar with the following services and understand the individual use cases and feature sets. Throughout this series I will reference a number of URL links which will help and direct you to related information on specific topics. To makes these links easily available to you I’ve included them at the top of the transcript within the lecture they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#x73;&#x75;&#112;&#x70;&#111;&#114;&#x74;&#x40;&#99;&#108;&#x6f;&#117;&#x64;&#x61;&#x63;&#97;&#100;&#101;&#x6d;&#x79;&#x2e;&#99;&#111;&#109;">&#x73;&#x75;&#112;&#x70;&#111;&#114;&#x74;&#x40;&#99;&#108;&#x6f;&#117;&#x64;&#x61;&#x63;&#97;&#100;&#101;&#x6d;&#x79;&#x2e;&#99;&#111;&#109;</a>. </p>
<p>That brings me to the end of this lecture. Coming up next I want to start off by looking at the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/benefits-logging/">benefits that logging brings</a> to your operational environment.</p>
<h1 id="The-Benefits-of-Logging"><a href="#The-Benefits-of-Logging" class="headerlink" title="The Benefits of Logging"></a>The Benefits of Logging</h1><p>Hello, and welcome to this short lecture, where I want to discuss a few of the different benefits that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> can bring you and your infrastructure. For some, many people consider logging an afterthought, something that is implemented after it’s too late. This is often the case where an incident or breach of security has occurred that resulted in a delay of resolution and safeguarding of your environment. In hindsight of these situations, logging would have been a great idea to have had running and implemented in the first place to rectify the event quickly and efficiently, or even prevent it from happening in the first place.</p>
<p>So how can logging help?</p>
<p>Generally, logs are created by services and applications which contain a huge amount of information, which is recorded and retained on persistent storage, to be reviewed and analyzed at any time that it might be needed. Some logs can be monitored in real time, allowing automatic responses to be carried out, depending on the data contents of the log. From an auditing perspective, these logs are invaluable. They often contain vast amounts of metadata, including date stamps, source information such as IP address or usernames, and this is especially true when you’re looking at CloudTail logs. These logs can be used to help you achieve specific compliance certifications that require evidence of traceable and auditable actions that have been carried out. </p>
<p>Being able to resolve an incident as quickly as possible is paramount within your organization. Whether it’s a priority one, two, or three, being able to gain as much insight into what happened just before and just after the incident can significantly reduce your time to resolution. Using logs to ascertain the state of your environment before and after and even during the incident provides clarity and enables you to detect where the incident occurred, allowing you to pinpoint your efforts in a specific area. Quicker resolution results in a better customer experience for your organization. </p>
<p>By monitoring the data within your logs, you’re able to quickly identify potential issues that you want to be made aware of as soon as they occur. By combining this monitoring of logs with thresholds and alerts, you are able to receive automatic notifications of potential issues, threats, and incidents, prior to them becoming a production issue. By logging what’s happening within your applications, network, and other cloud infrastructure, you are able to build a baseline of performance and establish what’s routine and what isn’t. By having this baseline, you are able to identify threats and anomalies easier through the use of third party tools and management services. </p>
<p>To have a thorough understanding of what’s happening within your infrastructure provides a huge benefit to your operational teams. Having an inside look of how your infrastructure is performing and communicating helps achieve the previous benefits that I’ve already discussed, and having more data about how your environment is running far outweighs the disadvantage of not having enough information, especially when it really matters to your business in the case of incidents and security breaches. </p>
<p>That now brings me to the end of this lecture. There are many more reasons as to why you should be capturing data that can be logged. But I just wanted to provide a few key points to you. </p>
<h1 id="CloudWatch-Logging-Agent"><a href="#CloudWatch-Logging-Agent" class="headerlink" title="CloudWatch Logging Agent"></a>CloudWatch Logging Agent</h1><p>Hello and welcome to this lecture where I shall explain what CloudWatch Logs are, how they work and how they are configured. As we know CloudWatch is a monitoring service that is used to collate and collect metrics on resources running on your AWS account allowing you to monitor their performance and respond to alerts that meet to find thresholds. In addition to this, Amazon CloudWatch is a powerful tool that allows you to collect logs of your applications and a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. </p>
<p>When data is fed into Cloudwatch Logs you are able to monitor the logstream in real time and set up metric filters to search for specific events that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. Let me explain the different components of this feature to give you a better understanding of how it all fits together. Starting with the Unified CloudWatch Agent. </p>
<p>With the installation of the Unified CloudWatch Agent you are able to collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. Interestingly this metric data is in addition to the D4EZ2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. The agent can be installed on a number of different operating systems and at the time of writing this course the operating systems versions supported are as follows. </p>
<p>To install the agent and configure it requires a number of different steps. To install the agent on your EC2 instances you need to perform the following, firstly you need to create a role and attach it to the instance with permissions allowing CloudWatch to collect data from the instances in addition to interacting with AWS systems manager SSM. You then need to download and install the agent onto the EC2 instance. And lastly configure and start the CloudWatch Agent. The most efficient way of completing this installation and configuration is with the use of the EC2 systems manager service known as SSM. You will need to create two roles. One role will be used to install the agent and also to send the additional metrics gathered to CloudWatch. The other role is used to communicate with the parameter store within SSM, to store a configuration file of the agent which then can be shared with other EC2 instances. </p>
<p>From a security perspective it’s best that only one of your EC2 instances has this permission to write to the parameter store. Once the agent configuration file is stored on SSM there is no need for other EC2 instances to do the same. In fact once the write has been completed, this role should be detached from the EC2 instance and the other role applied which simply allows the agent to send data to CloudWatch. </p>
<p>The role with the additional permissions for SSM needs to be configured as follows when creating a role. The options, ‘select the type of trusted identity’ needs to be ‘AWS service’. The option to ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. The ‘Attach Permissions Policies’ needs to be ‘CloudWatch Agent Admin Policy’ and the ‘Amazon EC2 role for SSM’. </p>
<p>The role that is simply used to install the agent and send data back to CloudWatch needs the following configuration, the ‘select type of trusted identity’ needs to be ‘AWS service’. The option ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. And finally under the ‘Attach Permissions Policies’ it needs to be ‘CloudWatch Agent Server Polic’y and ‘Amazon EC2 Role for SSM’. </p>
<p>Once your roles are created you can then associate the role shown in orange here, which I shall call ‘CloudWatch Agent Admin Role’ to your EC2 instance that will store the configuration file in the parameter store. </p>
<p>From the EC2 instance with additional permissions that will be saved in the configuration file with in the parameter store of SSM, you must then install the agent which can be done using systems manager or it can be downloaded from an S3 public link either for Linux or Windows. However as mentioned earlier I will explain how to do this via SSM in particular the run command function. As a prerequisite to the CloudWatch Agent installation you’ll need to verify that your EC2 instance has access to the internet to communicate with SSM and CloudWatch endpoints. In addition to this you must also have the SSM agent installed. For some AMI’s as stated on the screen the agent may already be installed. For more information on how to install or update your SSM agent on your EC2 instance please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">link</a>. Let me now perform a very quick demonstration to show you how to install the actual CloudWatch agent on an Amazon Linux EC2 instance using the SSM role command. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration 1"></a>Start of demonstration 1</h3><p>Okay so I’m within my AWS management console in the EC2 dashboard and as you can see I have an EC2 instance here named Logging Server and I have it in a VPC that has internet access. I have the SSM agent installed because it’s based on one of the latest Linux AMI’s comes by default with this, and I’ve also attached the CloudWatch Agent Admin Role which I discussed in the previous section so I’ve met all the prerequisites to install the CloudWatch Agent. </p>
<p>Now what I want to do is use the EC2 systems manager to install the CloudWatch Agent itself. So if I load up SSM which now has it’s own console and on the left hand side if I go down to the run command and click on run command to create a new command. What I want to select for the command document is the AWS configure AWS package. So if you just select the entry and then scroll down. And then next I need to select which EC2 instance that I want as the target. So which instance I’m going to install this package on and here we can see the Logging Server so I would just highlight that. And as you can see it’s added our EC2 instance already up there. </p>
<p>Now we come down to the command parameters and the action I want to perform is an install. The name of the package is Amazon CloudWatch Agent and I want to install the latest version. You can add some other comments here and some timeouts, for this demonstration I’m just going to leave those default. If you wanted to perform this same installation via the AWS CLI then you can click on the AWS command line interface command and then based on the above parameters you can simply cut and paste this command and run that command. But for this demonstration I’m going to use the run command within SSM. </p>
<p>So if I click on run we can see here that the command ID was successfully sent. It’s currently in progress and that will just take a moment to process and install the agent, then we should get a return of successful. So if I go back to the main dashboard of the run command we can see here that it was a success. This is the package that we just run using using this command ID, so now the agent is successfully installed. </p>
<p>As you can see here I have a previous command that failed so I just want to show you that worst on here. So if we select that command and go to view details we can drill down to understand why this failed. And if we scroll down to the bottom you can see here that it failed. Select the instance ID and view the output. Now step one, we can dive deeper to see why this failed. Now if we scroll down to the error itself and here it said it failed to retrieve the manifest and it could not find the latest version of this particular package. Now what happened here was I didn’t enter the correct package name. What I entered was CloudWatch Agent instead as we performed in the demonstration just now. The correct name is Amazon CloudWatch Agent. So it couldn’t find the package that was available and that was the reason why it failed. </p>
<p>However, going back to one that was successful, we can see here, we can drill down into this just to make sure everything was okay. Again here, success and we can view the output of this as well like we done with the failed one. And we can see here that it was all installed. It found all the files and it went through and processed everything okay. And that’s it, that’s how you install the CloudWatch Agent using the SSM run command. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration 1"></a>End of demonstration 1</h3><p>On your first instance you’ll need to create the CloudWatch Agent configuration file. Without doing so, you will not be able to start the agent. This file stores configuration parameters that specify which metrics and logs to capture on the instance which are then sent to CloudWatch. It can be created manually or by using a wizard. If you create it manually then you have a much wider scope for capturing elements that are not included within the wizard. Let me show you via another demonstration how to configure the agent using the wizard. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration 2"></a>Start of demonstration 2</h3><p>So I’m now on my Logging Server instance where we installed the agent and now need to configure it. So to configure it we’ll run this command here and this will launch the Amazon CloudWatch Agent configuration wizard. And it just goes through a number of questions before it completes so let’s take a look. So the first question it asks which operating system your using Linux or Windows, take the default choice of Linux. And if we’re going to try and collect logs on EC2 instances or On-Premises So it’s EC2 for us. </p>
<p>We then have a question if we want to monitor any host metrics such as CPU, memory, etc, I’m going to set a default of yes. And if we want to monitor CPU metrics per core and here additional CloudWatch charges may apply. For this demonstration I’m going to say no. We then have a question if we want to add any EC2 dimensions such as the instance ID or the instance type into our metrics if the information is available. Say yes. Then we have a question about how often CloudWatch will collect these metrics. Whether that’s one second, 10 seconds 30 seconds or 60. I’ll accept the default of every minute, 60 seconds and then asks which default metric config we want whether that’s basic, standard, advanced or none. I’ll accept the default of basic for this demonstration. It then just shows you an example of that configuration that you just selected. If you’re happy with that you can say one for yes or two for no. And then select a different default metrics config so I will just select yes for this demonstration, I’m happy with that. </p>
<p>Now it asks the question if we have an existing CloudWatch log agent configuration file that we want to import. At the moment we don’t but what we’re trying to do is get in the position of creating one to then import into the parameter store of SSM. So at this moment our default choice is no which is correct. It then asks if we want to monitor any log files on this EC2 instance. I’m going to say no just for this demonstration because all we are trying to do is configure the CloudWatch agent at the moment. Our next question asks if we want to store the config in the SSM parameter store, and here yes we do cause we want to upload this file to the parameter store to allow all other EC2 instances to connect to the SSM parameter store to download the configuration file to prevent us from doing this on each and every instance. So I’m going to say yes which is number one that we do want to store the config in the SSM parameter store. </p>
<p>It then asks what default name you want to give this configuration file. And it just gives you a little message there saying you should use the prefix of Amazon CloudWatch hyphen if you’re using any of the AWS managed policies. So I would go with their suggested name of Amazon CloudWatch Linux and you shouldn’t run into any issues there. It then asks you which region you want to store the config in the parameter store in and it gives a default choice. I’m just going accept that default. It then asks a question about which credentials should be used to send the config to the parameter store. I’m going accept the default credentials there which should have access and we now have a message that it successfully put the config to the parameter store Amazon CloudWatch-Linux. And that’s it, that’s the end of the wizard so it’s a very simple and quick and easy wizard. And now your CloudWatch agent configuration file is configured and it’s been uploaded to the parameter store so now any other EC2 instances can click to that parameter store and simply download it and have the agent running very quickly and easily. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration 2"></a>End of demonstration 2</h3><p>Now the configuration file is configured and successfully copied to the SSM parameter store I simply now need to start the agent and again I will use the systems manager service to complete this file with the run command. Let’s take a look. </p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration 3"></a>Start of demonstration 3</h3><p>Okay so the final stage is to start the agent and again I’m going do this from the AWS systems manager so I’m at the systems manager dashboard. Again I’m going use the run command so so it’s over on the left hand side here. Click on run command. Then click on the orange run command button and this will allow us to start a new command for the command document. What we need to look for is the following, which is Amazon CloudWatch Manage Agent. And here we have the command document here. So I’m going to select that. </p>
<p>Scroll down, we then need to select our target instance and we have our Logging Server here. Now the command parameters, for the action we want to select configure rather than stop because we’re going to select the configuration file that we uploaded to the parameter store first using the EC2 mode rather than On-Premises. The optional configuration source which is SSM which is where we stored the configuration file so we’ll leave that as SSM. Now in the optional configuration location we need to enter the name of the file that we stored it as. And if you can remember that was Amazon CloudWatch-Linux. So we’ll paste that in. </p>
<p>Now under optional restart we want that as yes because it will then start the agent once it’s pulled the information from SSM. If we scroll down to the bottom simply click on run. And now we have it, the command ID was successfully sent. If we go back to our dashboard we can see that it was a success. And here the document name was the CloudWatch Manage Agent. So the CloudWatch Agent will now be running on that EC2 instance, the Logging Server. And it’s as simple as that. Now we have our logs configured and log data of our EC2 instance is being sent to CloudWatch along with the additional metric information. It’s now possible to search for specific entries within the logs for points of interest. </p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration 3"></a>End of demonstration 3</h3><p>That now brings me to the end of this lecture on CloudWatch logs where I explained how CloudWatch can be used to centralize login for more EC2 instances or applications running on your instances by determining logs past configured by the CloudWatch Agent and log groups within CloudWatch. Coming up next I should be talking about CloudTrail logs.</p>
<h1 id="CloudTrail-Logging"><a href="#CloudTrail-Logging" class="headerlink" title="CloudTrail Logging"></a>CloudTrail Logging</h1><p>Hello, and welcome to this lecture focusing on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> capabilities and configuration of AWS CloudTrail. You should already be familiar with what CloudTrail is and what it does. However, to quickly summarize: It’s a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface: CLI, from within the AWS Management Console, Or even from a request made by another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service. For example, when auto-scaling automatically sends an API request to launch or terminate an instance. These API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the new log file. </p>
<p>CloudTrail also records and associates other identifying metadata with all events. For example, the identity of the caller, which can be the user or the account that made the API call, the timestamp of when the request was initiated, and the source IP address. The logs generated are the output of the CloudTrail service and they hold all of the information relating to the API calls that have been captured. So as a result, it’s important to know what you can do with these logs in order to maximize the benefit of the data they contain. </p>
<p>So, what is a log file and what does it look like? Log files are written in a JSON format. Much like access policies within IAM and S3. Every time an API is captured, it’s associated with an event and written to a log. And new logs are created approximately every five minutes or so, but they are not delivered to a nominated S3 bucket for persistent storage for approximately 15 minutes after the API was called. So if you expect to see the log file for an API called seven minutes ago, then you may not see the log as expected for potentially another eight minutes. The log files are held by the CloudTrail service until final processing has been completed. Only then will it be delivered to S3, and optionally, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a>, depending on your configuration of the trail. </p>
<p>When an event reflecting an API call is written to a log, a number of attributes are also written to the same event capturing key data about that call. As you can see from this example. Without going through every attribute here, I just want to point out some of the more interesting ones. These being eventName. This refers to the name of the actual API that was called. EventSource. This refers to the service as to which the API called was made against. EventTime. This was the time that the call was made. SourceIPAddress. This displays the source IP address of the requester who made the API call. This is a great piece of information when trying isolate an attacker from a security perspective. UserAgent. This is the agent method that the request was made through. Example values of these are: Signin.amazonaws.com and this is what we have in our example and it simply means that user made this request from within the AWS management console. Console.amazonaws.com, this is the same as the previous, however, if this was displayed, it would mean that the request was made by the root user of the account, and lambda.amazonaws.com, this is fairly obvious, this would reflect that the request was made with AWS lambda. UserIdentity. This contains a larger set of attributes that provides information on the identity that made the API request. Once events have been written to the logs and then delivered and saved to S3, they are given a standard name and format, as shown. </p>
<p>The first three elements of this naming structure are self-explanatory. The AccountID, Name of the Service delivering the log, CloudTrail, and the region that it came from. The next part relates to the date and time. The year, months, and days. The T indicates the next part is the time reflecting hour and minutes. The Z simply means that the time is in UTC. The UniqueString value is a random 16 alphanumeric character string that is simply used by CloudTrail as a unique file identifier to ensure that it doesn’t get overwritten with the same name of another file. Currently, the FileNameFormat is defaulted to json.gz which is a compressed GZ version of a JSON text file. While we are looking at structures, let me also talk about the bucket structure way your logs are stored. </p>
<p>You may feel that the logs are all stored in one folder within your S3 bucket. However, there is a lengthy but very useful folder structure as follows: Firstly, you have your dedicated S3 BucketName that you selected during the creation of your Trail. Next, is the prefix that is also configured during Trail creation and is used to help you organize a folder structure for your logs corresponding to different Trails. Following this, is a fixed folder name of AWSLogs. Followed by the originating AWS account ID. Then another fixed folder name of CloudTrail indicating which service has delivered the logs. And after that, the RegionName of where the log file originated from. This is useful for when you have Trails that apply to multiple regions. The last three folders show the year, month and day that the log file was delivered. As you can see, although there are multiple folders underneath your nominated S3 bucket, it does provide an easy navigation method when looking for a specific log file.</p>
<p>This folder structure comes into even greater use if you have multiple AWS accounts delivering logs to the same S3 bucket. Some organizations may be using more than one AWS account, and having CloudTrail logs stored in different S3 buckets across multiple accounts can be inconvenient in certain circumstances and require additional administration to manage. Thankfully, AWS offers the ability to aggregate CloudTrail logs for multiple accounts into a single S3 bucket belonging to one of these accounts. This is why there is an accountID folder within your S3 bucket. Please note that you are unable to aggregate CloudTrail logs for multiple AWS accounts into CloudWatch logs that belongs to a single AWS account. </p>
<p>So to have all your logs from your accounts delivered to just one S3 bucket is a fairly simple process with the end result allowing you to essentially manage all your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/monitoring-cloudtrail-cloudwatch/">CloudTrail logs</a>. Let’s take a look at how this solution is configured. Firstly, you need to enable CloudTrail by creating a Trial in the AWS account that you want all log files to be delivered to. Permissions need to be applied to the destination S3 bucket allowing cross account access for CloudTrail. And once permissions have been applied to your policy, you need to edit the bucket policy and add an additional line for each AWS account requiring access. Then you need to create a new Trial in your other AWS accounts and select to use an existing S3 bucket for the log files. When prompted, add the bucket name used in step one and when alerted, accept the warning that you want to use a bucket from a different AWS account. An important point to make here when configuring the bucket selection is to ensure that you use the same prefix as the one you used when you configured the bucket in the first step. That is unless you intend to edit the bucket policy to allow CloudTrail to write to the location of a new prefix you wish to use. When you have configured your Trail, click create and your new Trail will now deliver it’s log files to the S3 bucket in your AWS account used in the first step. Again, this is a great solution that allows you to essentially manage all of your CloudTrail logs in one single account and S3 bucket. However, there may be uses such as system administrators who manage the other AWS accounts where the logs have come from that might need access to data within these logs. So how would they gain access to the S3 bucket to allow them only to access their CloudTrail logs that originated from their AWS account? </p>
<p>It could be done quite easily by configuring a few elements within IAM. Firstly, in the master account, IAM Roles would need to be created for each of the other AWS accounts requiring read access. Secondly, a policy would need to be assigned to those Roles allowing access to the relevant AWS account logs only. Lastly, users within the requesting AWS accounts would need to be able to assume this Role to gain read access for their CloudTrail logs. The easiest way to show you how to configure the permissions required is by a demonstration while I shall perform the following steps: I shall create a new Role. Apply a policy to this Role to only allow access for AWS account B’s folder in S3. Show the Trust Relationship between AWS account A and B. I will then create a new IAM user in account B. And create a Policy and apply the sts:AssumeRole permissions to this user allowing them to assume the new Role we created in account A. So let’s take a look at how and where we apply these permissions. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so, as I just said the first thing we need to do is create a new Role in our primary account. So, if we go across to IAM, which is under Security, Identity and Compliance and then once that’s loaded, we need to go across to Roles and then Create New Role. So let’s give this Role a name. I’ll call it ‘Cross Account CloudTrail’ Click on Next Step. We then need to select a Role type, and what we want to do is select Role for Coss-Account access because we will allow users in another AWS account to access the log files in this primary AWS account. And this will set up the trust relationship between this account and then my secondary account. So, for that we will select this top option of providing access between AWS accounts you own. Then next, I’ll need to enter the secondary account ID that I want to create the trust relationship with. So I’ll just enter that number. </p>
<p>Okay, then after you have entered your account ID click on next step. And now we need to attach a Policy to this Role. Prior to this demo, I set up my own Policy and this allows cross-account access to read only from my secondary account to the bucket on this primary account. But I’ll explain this Policy in a few moments and I’ll show exactly what it contains. And from here click on next step and this is just a review of the Role. So we have the Role name, the ARN, the Amazon Resource name, the trusted entities. So this is the secondary account ID that I entered, and then the actual Policy and then the link that we can give to users in the secondary account to allow them to switch Roles. So, create Role. And there we go, the cross-account CloudTrail Role that we just created. So, let’s take a look at this.</p>
<p>Firstly, I’ll show the trust relationships. So, because we added a cross-account Role access and then we entered the secondary AWS account ID, we can see that this account is trusted by our primary account and that allows entities in this account to assume this Role. Now, I mentioned earlier that I previously set up a Policy with permissions in. So let’s take a look at that Policy. I named it Cross-Account read only for CloudTrail. So if I show the Policy, as you can see it. I made a very small Policy. Very simple. Now we have an effect of allow which will allow any S3:Get and any S3:List command, so essentially, read only access on this resource here specified by this line. Now this resource links to the bucket and folder where CloudTrail logs are delivered for our secondary account as you can see here. So, essentially, what this Policy does is allow read only access to any folders within the secondary account’s CloudTrail log folders. So this account won’t be able to access any other accounts CloudTrail logs, which is important. So, if we come out of this. So let’s just have a quick recap of what we’ve achieved so far.</p>
<p>So, so far, what we’ve done: We’ve created a Role in our primary account for our secondary account access. And we’ve also assigned an access Policy to this Role in order for the secondary AWS account to access the relevant folder in S3. So now what we need to do is assign a user in the secondary account and then apply the permissions to that user to enable them to assume the new Role in the primary account. So let’s go ahead and do that. </p>
<p>Okay, so I’ve now logged into the secondary account where I’ll need to create a new user and assign the correct permissions. So, to start with, I’m going to set up a permission Policy to assign to the user. So if I go down to Security, Identity and Compliance and select IAM. And then go across to Policies, and from here I want to create a new Policy. And I am going to create my own Policy, so I’m going to select the bottom option. I’m going to call this AssumeRoleforCloudTrail And description will be Assume role in primary AWS account. And for the Policy document, I’m going to paste in a Policy that I’ve already created. As you can see, it’s only a very small Policy again. And we have an allow effect that allows the Assume Role action from the security token service against the following resource, and this resource links back to a Role on our primary account where we created the Role cross-Account CloudTrail. </p>
<p>So this Policy will allow the user to assume this Role in the primary account. So let’s go ahead and create that Policy. Let’s validate it first. And then create. Now what we need to do is to assign a user to use that Policy. Now, I created a new user earlier prior to this demo. So, let’s just find our new Policy that we just created. And here it is at the bottom, AssumeRoleforCloudTrail. And I’m going to attach a user. And I’ve called our user CloudTrailUser1. And then attach Policy. And there we go. </p>
<p>So we now have one user attached to this Policy. So that’s all the actions and steps necessary to allow a user in a secondary account to access CloudTrail log files that have been delivered to an S3 bucket in a primary account. And it would do this by using the permission Policy that we just applied to that user to access the Role in the primary account. And that Role has a Policy attached that allows S3 read access to it’s own CloudTrail logs. </p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>CloudTrail allows you to enable a feature called Log File Integrity Validation. Which simply allows you to verify that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. This is typically used for security and forensic investigations where by the integrity of the log files are critical to confirm that they have not been tampered with in any way. </p>
<p>When a log file is delivered to an S3 bucket a hash is created for it by CloudTrail. A hash file is a set of characters that are unique that are created from a data source. In this case, the log file. The hashing algorithms used by CloudTrail are SHA-256. In addition for a hash for every log file created, </p>
<p>CloudTrail creates a new file every hour, called a digest file, which is used to help verify your log files have not changed. The digest file contains details of all the logs delivered within the last hour along with a hash for each of them. These files are stored in the same bucket as the key pair. When it comes to verifying the integrity of your log files, the public key of the same key pair is used to programmatically check that the logs have not been tampered with in any way. Verification of the log files can be achieved via a programmatic access and not via the console. Using the AWS CLI, this can be checked by issuing the following command. The folder structure for the digest is very similar to the CloudTrail logs, as you can see. But the digest files are clearly distinguishable by the CloudTrail digest folder. </p>
<p>That has now taken me to the end of this lecture. Coming up next, I’ll explain how you can use CloudTrail and CloudWatch together as a monitoring solution.</p>
<h1 id="Monitoring-CloudTrail-with-CloudWatch"><a href="#Monitoring-CloudTrail-with-CloudWatch" class="headerlink" title="Monitoring CloudTrail with CloudWatch"></a>Monitoring CloudTrail with CloudWatch</h1><p>Hello and welcome to this lecture where we will look at how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudtrail-logging/">AWS CloudTrail</a> interacts with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a> and SNS to create a monitoring solution. In addition to S3, the logs from CloudTrail can be sent to CloudWatch Logs, which allows metrics and thresholds to be configured, which in turn, can utilize SNS notifications for specific events relating to API activity. CloudWatch allows for any event created by CloudTrail to be monitored. This enables a whole host of security monitoring checks to be utilized. A great example of this is to be notified when certain API calls requesting significant changes to your security groups or network access control lists within your VPC. Other examples of these checks that are common within organizations are API calls relating to it starting, stopping, rebooting, and terminating EC2 instances. If instances are being created that shouldn’t be, then your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> cost could rise dramatically and quickly. Also, if instances are being rebooted or stopped, this could have a severe impact to your services if they are not configured in a high availability and resilient solution. Changes to security policies within IAM and S3. If changes are being made to your policies that shouldn’t be, access can be inadvertently removed for authorized users and access granted to unauthorized users, having a massive impact on operational services. Even a minor change to a policy can pave the way for an untrusted user to exploit the error. Failed login attempts to the Management Console. Monitoring failed attempts here can help to prevent unauthorized access at your environment’s front door. API calls that result in failed authorization. Not only does CloudTrail track successful API calls whereby the correct authorization was met by the authenticated identify, but it also tracks unsuccessful API requests, too, which would likely be due to the permissions applied. Special attention should be given to these unsuccessful attempts, as this could be a malicious user trying to gain access. However, it could also be a legitimate user trying to access a resource they should have access to for their role, but the incorrect permissions had been applied with their associated IAM policy. </p>
<p>To configure CloudTrail to use CloudWatch, you must first create a trail. Once your trail has been created, you can then configure it to use an existing CloudWatch Log group or have CloudTrail create a new one. Having CloudTrail create a new one for you is recommended if it is your first time doing this, as CloudTrail will take care of all of the necessary roles, permissions, and polices required. You may be wondering why roles and policy are required, so let me give you a high-level overview of the simple process that takes place when sending CloudTrail logs to CloudWatch. When a log file is created by CloudTrail, it is sent to your selected S3 bucket and your chosen CloudWatch Log group, assuming your trail has been configured for this feature. To allow CloudTrail to deliver these logs to CloudWatch, CloudTrail must have the correct permissions and these are gained by assuming a role with the relevant permissions needed to run two CloudWatch APIs. The first being CreateLogStream, and this enables CloudTrail to create a CloudWatch Logs log stream in the log group, and PutLogEvents, and this allows CloudTrail to deliver CloudTrail events to the CloudWatch Logs log stream. CloudWatch then delivers logs to the CloudWatch Logs. </p>
<p>When using the AWS Management Console, you can have the CloudTrail create this role for you, along with the correct policy. By default, the role is called CloudTrail_CloudWatchLogs_Role. For those that are curious, the policy for this role looks as shown. It’s important to point out that CloudWatch Log events have a size limitation of 256 kilobytes on the events that they can process. Therefore, any events that are larger than 256 kilobytes will not be sent to CloudWatch by CloudTrail. </p>
<p>Now that you have your logs with the associated events being sent to CloudWatch, you must then configure CloudWatch to perform analysis of your CloudTrail events within the log files. This is done by configuring and adding metric filters to the log within CloudWatch. These metric filters allow you to search and count a specific value or term within your events in your log file, which then allows for customizable thresholds to be applied against them. When creating these metric filters, you must create a filter pattern which determines what exactly you want CloudWatch to monitor and extract from your files. These filter patterns are usually fully customizable strings but as a result, a very specific pattern syntax is required. So, if you’re creating these for the first time, you must understand the correct syntax. </p>
<p>Just to reiterate what we have spoken about so far, I want to provide a demonstration on how to edit an existing trail to configure it to send logs to CloudWatch Logs. I will then configure a metric filter with the associated metric pattern, and finally, I will set up an SNS alert to notify me when a particular threshold is met. So, let’s take a look. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so what I need to start with is going into CloudTrail to edit an existing trail to enable CloudWatch Logs. So, if I go down to Management Tools and click on CloudTrail and then across to Trails, as you can currently see, under CloudWatch Logs log group, there’s no log group selected. So, if we go into the trail and then scroll down to CloudWatch Logs, click on Configure, and there we can get CloudTrail to automatically set up this group and it’ll create the necessary roles and permissions, etc. So, let’s call this CloudTrail&#x2F;Demo and then click on Continue. So, we’ve given it a name and here it just gives a message to say that for CloudTrail to deliver events and logs to CloudWatch Logs, it needs to assume a role with permissions to run two API calls, which are these two here. And if we go down into the details, you can see that the IAM role that it’s going to use is the CloudTrail_CloudWatchLogs_Role and we’ll ask it to create a new policy. And here’s the policy document. </p>
<p>So, go down to Allow, and then if we scroll down to our CloudWatch Logs section, you can now see that we have a log group created in CloudWatch called CloudTrail&#x2F;Demo. So, if we now go across to CloudWatch and if we click on Logs on the left-hand side here, we can see that we have our log group that was just created by CloudTrail, and it’s CloudTrail&#x2F;Demo. Now, if we go into our log group and select it, you’ll see this log stream, which is the incoming stream of events being sent from CloudTrail. Now, as we’ve only just started, there’s only a few events coming in here, so you might want to wait a few minutes before setting up your metric filters to give you more of a test pattern to search on. So, what I might do is just leave it a couple of minutes for some more events to start streaming in before we set up our metric filters here, just so we have something to search on. </p>
<p>Okay, so I’ve left it a few minutes, so let’s go back into the log group and you can see we’ve now got a couple of streams, and if we go into these, we can see there’s a lot more events. So, if we go back a couple of pages, back to our log group, now we need to create our metric filters to allow us to define what we want to search on within our logs. So, if we select the tick next to our log group and then go up to Create Metric Filter, and here within the metric filter, we need to define a filter pattern. Now, as explained earlier, filter pattern will define what we’re actually searching for within our logs. So, for this example, I’ll keep it fairly simple. I’m going to search for any API call that’s been made from my machine, so from my IP address. So, for that, I need to enter the following command, ( $.sourceIPAddress equals 2.218.11.188, which is my IP address. And now we can test to make sure that that filter pattern’s okay using this Test Pattern box here, and what that does, that’ll run this test filter on some log data we see from this log here and the output of that log is in this box here. So, all we need to do is click on Test Pattern and we can see at the bottom here that it found 47 matches out of 50 events in the sample log. So, we know that the syntax is okay for this filter pattern, so I’m going to go ahead and assign this metric. </p>
<p>And we can see up here that we’ve got our filter name and our filter pattern, and I’m going to create a new name space for this metric and I’ll call it Demo, and the metric name will be IPAddress. And then what we need to do is click on Create Filter. Now, as you can see, our filter has been created and we have the details in this screen here. Now, what we can do at this point is create an SNS alarm so it could be notified if a certain threshold was met. So, let’s go ahead and do that. </p>
<p>So, the first thing that we need to do is add a name, so I’m going to call this SourceIPAddress and description will be Too many calls from my IP. Now, I’m going to set this to be 30. So, whenever my IP address is used as a source IP address that is greater or equal to 30 times for one consecutive period over five minutes, then I want it to set to a state of an alarm. And I want to be notified, so I’m going to enter a new list, give this a new topic, SourceIPAddressAlarm, and I want that to be sent to myself. So, as we can already see, with the current data it’s got that it has already breached the alarm, but it has gone back down below, so we’ll see how this goes and we’ll create the alarm. And this is a message just to say that I need to subscribe to that AWS notification, and I can do that in just a few moments. So, if we go across to our Alarms, we can see that we have our source IP address alarm in the state of OK. </p>
<p>So, at the minute, it’s currently below the 30 threshold. As soon as it goes above that, it will alarm and I will get a notification. Now, over the past few minutes, I’ve just been having some activity within the Management Console, and as we can now see, we do have an alarm on our alert. We can see that it just crossed the threshold, and so, I’ve received an email notification to say that it is now in a state of alarm. And if we take a quick look at that email, we can see here that it was crossed with a data point of 33 and the threshold was 30. So, that is how you set up CloudTrail to use CloudWatch with the inclusion of SNS to create alarms against API activity.</p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="S3-Access-Logs"><a href="#S3-Access-Logs" class="headerlink" title="S3 Access Logs"></a>S3 Access Logs</h1><p>Hello, and welcome to this lecture, where I shall be looking at Amazon S3 access logs, what they are, and what they contain. As you may have guessed from the name, Amazon S3 access logs collate data based on who has been accessing a particular S3 bucket, and these logs record information, such as the source bucket that was accessed, a timestamp of the event, the identity requesting access to the object in the bucket, and the action that they were performing with the object. </p>
<p>By default, when you create a new bucket, access logging is not enabled. However, should you have a requirement to understand who is accessing your S3 buckets, then this logging can quickly and easily be enabled. The configuration of this process is based upon a source bucket and a target bucket. The source bucket is the bucket in which you want to log access requests for. The target bucket is the bucket in which the access logs will be delivered to. It’s best practice to use different buckets for both the source and the target for ease of management. When configuring your buckets for logging, you need to be aware that the source and target buckets need to be in the same region. </p>
<p>To allow S3 to write logs to this target bucket, it will of course require specific permissions. These permissions allow write access for the Log Delivery group, which is a pre-defined Amazon S3 group, which is used to deliver log files to your target buckets. If the configuration of your access logging is configured using the management console, then the setup process automatically adds the Log Delivery group to the ACL of the target bucket, allowing the relevant access. However, if you were to configure the access logging using the command line, then you would need to manually configure these permissions. </p>
<p>To set up the access logs using the console is a very simple process. Firstly, you select the S3 bucket that you would like to capture access logs for, select the properties tab, select server access logging, choose Enable Logging. From the dropdown, select your target bucket, and this is the bucket in which the logs will be delivered and saved to. Enter a prefix for your log files if required, and click save. If you then look at the ACL permissions for that bucket, you’ll notice that the log delivery group has automatically been given write access for that bucket. If you wanted to enable logging on the bucket programmatically, then you can do so using the S3 API or the AWS SDKs. When doing so, you need to configure the write access for the Log Delivery Group on the target bucket as an additional action. More information on how to perform these steps can be found using the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/enable-logging-programming.html">link</a>. </p>
<p>When viewing your logs within the target bucket, you’ll notice that each entry is made up of a number of different parameters. Let me show an example of a log access file to show you the data that makes up the entry. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so I’ve just opened up one of the S3 access logs that I have, just a very small snippet. And we can see at the top here, we’ve got a couple of entries, so let’s just run through this top entry, just some of the key bits of information that make up the access log. This first entry here, and that’s the canonical ID of the owner of the source bucket. Next we have the AWS bucket itself that was accessed along with the date and time as well. Next we have some information from the requester, which is their source IP address. This hyphen means that the user was unauthenticated. If it was a user within IAM then we’d see their user ID there. This set of characters is generated by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> just as a unique ID for this request. Next we have the action that was carried out, which is a get object request against the object within the bucket which is this image file here. And again, we can see that here and also we have this response code of 200. Also we have some information here with regards to the amount of bytes sent and the object size, and also some timings in milliseconds as well for that request. We also have the referrer here of cloudacademy.com where the request initially came from, and then finally just some information regarding to the requester’s application software. So that’s a very quick summary and example of what an entry looks like within your AWS S3 access logs.</p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Hello, and welcome to this final lecture within Part One of this two-part series relating to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging in AWS</a>. In this lecture, I want to summarize and highlight the key points from the previous lectures.</p>
<p>I started off by talking about the benefits of logging to your organization. Within this lecture, we learnt that logging allows you to rectify incidents quicker and more efficiently, or even prevent the incident from happening in the first place. Also logs created by services and applications contain a huge amount of information which is then recorded and retained for later use. Some logs can be monitored in real-time allowing automatic responses to be carried out depending on the data contents of the log, and logs are invaluable from an auditing perspective as they contain vast amounts of metadata. Logs can also be used to help achieve compliance. Using logs to ascertain the state of your environment before and after and even during an incident enables you to detect where the incident occurred. And by combining the monitoring of logs with thresholds and alerts, you can configure automatic notifications of potential issues, threats and incidents prior to them becoming a production issue. Using logs, you can establish a baseline of performance allowing you to determine anomalies easier through the use of various third-party tools and management services. and finally, having more data about your environment and how its running far outweighs the disadvantages of not having enough information. </p>
<p>Following this lecture, I explained how CloudWatch Logs were configured and used. During this lecture, the following points were made. Amazon CloudWatch is a powerful tool that allows you to collect logs of your applications and a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. You are able to monitor the log stream in real-time and set up metric filters to search for specific events that you need to be alerted on or respond to. And CloudWatch Logs acts as a central repository for real-time monitoring of log data. The unified CloudWatfch agent allows you to collect logs and additional metric data from your EC2 instances and well as from on-premise servers. And to install the agent on your EC2 instances you need to create two roles. One role will be used to install the agent and also to send the additional metrics gathered to CloudWatch, and the other role is used to store a configuration information file in the parameter store within SSM. You then need to download and install the agent onto the EC2 instances using SSM and the Run Command and finally, configure and start the CloudWatch agent using a Wizard or the manual configuration. </p>
<p>I then looked at a different service, which was AWS CloudTrail, which records and tracks all API requests made within your AWS account. Within this lecture, I explain the following. When an API request is initiated, AWS CloudTrail captures a request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the log file and the logs generated are the output of the CloudTrail service. Log files are written in JSON, Javascript Object Notation format, much like access policies within IAM and S3. New logs are created approximately every five minutes but they are not delivered to the nominated S3 bucket for persistent storage for approximately 15 minutes after the API was called. The log files are held by the CloudTrail service until final processing has been completed. Log files are delivered to S3 and optionally CloudWatch Logs as well. And any logs that are delivered to S3 are given a standard naming convention of the following. AWS offers the ability to aggregate CloudTrail logs from multiple accounts into a single S3 bucket belonging to one of these accounts. But do be aware you are unable to aggregate CloudTrail Logs from multiple AWS accounts into CloudWatch Logs that belongs to a single AWS Account, and CloudTrails allows you to enable a feature called “Log file integrity validation,” which allows you to verity that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. And finally, when a log file is delivered to your S3 bucket, a hash is created for it by CloudTrail. </p>
<p>At this point, we had looked at both CloudWatch and CloudTrail. So the following lecture looked at how you could use CloudWatch to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/monitoring-cloudtrail-cloudwatch/">monitor CloudTrail Logs</a>. And here we learnt that the logs from CloudTrail can be sent to CloudWatch Logs allowing metrics and thresholds to be configured which, in turn, can utilize SNS notifications for specific events relating to API utility. CloudWatch allows for any event created by CloudTrail to be monitored and you can then configure your new and existing trails to use an existing CloudWatch Log Group, or have CloudTrail create a new one. To allow CloudTrail to deliver logs to CloudWatch, CloudTrail must have the following permissions given via role, and that’s the CreateLogStream permission and PutLogEvents. CloudWatch Log Events have a size limitation of 256KB on the events that they process and adding CloudWatch metric filters allows you to perform analysis of your CloudTrail events within the log files. And finally, metric filters allow you to search and count a specific value or term within your events in your log file. </p>
<p>The final lecture in Part 1 of this course series looked at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/s3-access-logs/">S3 Access Logs</a>. Within this lecture, the key points were as follows. Amazon S3 Access Logs collate data based on who has been accessing a particular S3 Bucket. And by default, when you create a new bucket, access logging is not enabled. S3 Access Logs are based upon a source bucket and a target bucket. The source bucket is the bucket in which you want to log access request for, and the target bucket is the bucket in which the access logs will be delivered to. It’s best practice to use different buckets for both the source and the target bucket for ease of management. And remember, the source and target buckets needs to be in the same region. During the configuration of access logs using the Management Console, permissions for right access for the log delivery group which is a predefined Amazon S3 group which is used to deliver log files to your target buckets. And if you wanted to enable logging on the bucket programmatically, then you can do so using the S3 API or the AWS SDKs. When doing so, you need to configure the write access for the Log Delivery Groups on the Target bucket as an additional action. </p>
<p>That now brings me to the end of this lecture and to the end of Part 1 of this course series. If you are ready to dive deeper into further AWS services relating to logging, including CloudFront Access Logs, VPC Flow Logs, AWS Config Logging, and how to filter data using Amazon Athena, then head over to Part Two, which can be found using the link on screen. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. </p>
<p>Thank you for your time, and good luck with your continued learning of cloud computing. Thank you.</p>
<p>URLs referenced during this course can be found below:</p>
<h1 id="3CloudWatch-Logging-Agent"><a href="#3CloudWatch-Logging-Agent" class="headerlink" title="3CloudWatch Logging Agent"></a>3<strong>CloudWatch Logging Agent</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">Metrics collected by the CloudWatch Agent</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent Linux</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/windows/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent for Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">How to install&#x2F;update your SSM Agent</a></p>
<h1 id="6S3-Access-Logs"><a href="#6S3-Access-Logs" class="headerlink" title="6S3 Access Logs"></a>6<strong>S3 Access Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/enable-logging-programming.html">Enabling Logging Programmatically</a></p>
<h1 id="7Course-Summary"><a href="#7Course-Summary" class="headerlink" title="7Course Summary"></a>7<strong>Course Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/">How to implement and enable logging across AWS services - Part 2 of 2</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Building-CloudWatch-Dashboards-25</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:23" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:23-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:57:40" itemprop="dateModified" datetime="2022-11-19T22:57:40-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, my name is Will Meadows! Welcome to this course on Building CloudWatch dashboards. This course is geared towards helping you understand the value in building your own dashboards within CloudWatch, to give you unparalleled visibility into your architecture and dedicated systems.</p>
<p>If you have any questions about anything I cover in this lecture please let me know at <a href="mailto:will.meadows@cloudacademy.com">will.meadows@cloudacademy.com</a></p>
<p>Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> and one of our cloud experts will reply to your question, concern, or comment. </p>
<p>This lecture is perfect for someone who is looking to gain insight on how their infrastructure is performing through the use of easy to understand graphics, metrics, and tables.</p>
<p>After watching this course you will be able to create your own CloudWatch dashboard to monitor the items that are important to you, understand how CloudWatch dashboards can be shared across accounts, and understand the cost structure of CloudWatch dashboards and the limitations of the service.</p>
<p>This lecture will cover high-level topics related to CloudWatch and many other AWS services. Having a basic understanding of the most common AWS services (such as EC2, S3, and RDS) as well as a moderate level of experience with CloudWatch is recommended.</p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>Please note that, at the time of writing this content, all course information was accurate. AWS implements hundreds of updates every month as part of its ongoing drive to innovate and enhance its services.</p>
<p>As a result, minor discrepancies may appear in the course content over time. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. So, if you notice any information that is outdated, please contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. This will allow us to update the course during its next release cycle.</p>
<h1 id="CloudWatch-Dashboards"><a href="#CloudWatch-Dashboards" class="headerlink" title="CloudWatch Dashboards"></a>CloudWatch Dashboards</h1><p>CloudWatch dashboards are a fantastic way to visualize your data within AWS without having to dig into the nitty-gritty of each individual service.it allows you to quickly display key information at a glance giving you the ability to make decisions about your workload And your processes. These dashboards are created from individual widgets that You can combine together to create graphs and provide detailed information quickly about the topics you desire. even allows you to run queries within these widgets to display even more detailed and specific information. </p>
<p>CloudWatch also has automatic dashboards which are created for you by the service itself. These automatic dashboards work on a service by service basis and pick out some of the key components that you might be interested in. </p>
<p>For example, if you have any ec2 instance already running, there is probably an automatic dashboard that has been created to monitor your ec2 workloads.</p>
<p>I recommend you take a look at some of these automatically created dashboards as they give you a really good understanding of what is available with this service and the types of metrics you can harness to build the perfect dashboard for yourself.</p>
<p>There are two ways that you can create a dashboard. You can either do so visually through the editor or you can create dashboards programmatically and even use them inside cloud formation templates.</p>
<p>Both methods allow you to pick from many different media types called widgets. There are currently 8 flavors of these widgets and they are as follows:</p>
<ol>
<li>Line charts - A line chart is a type of chart which displays information as a series of data points connected by straight line segments. It is a basic type of chart common in many fields.</li>
<li>Stacked area chart -This type of chart compares the totals of many different subjects within the same graph</li>
<li>Number Widget - Allow you to instantly see the value for a certain metric that you’re particularly interested in - this could be as simple as displaying the current number of online instances.</li>
<li>Bar Charts - compares values of multiple types of data within the same graph.</li>
<li>Pie charts - Proportional data in direct relationship to other information fitted within a circle.</li>
<li>Text widget - which is free text with markdown formatting allowing you to add useful information to your dashboards as you see fit</li>
<li>Log tables - which explore results from log insights. Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch</li>
<li>And finally, we have alarm statuses: in case you have an alarm set up that you’d like to know immediately if something is going wrong right on this dashboard</li>
</ol>
<p> One extremely cool feature of CloudWatch dashboards is they allow you to perform math on the metrics you want to display. So if you wanted to see how a graphed metric looked when applying normalization techniques or filters to your data you have the power to do so. </p>
<p>Additionally when working with dashboards are also allowed to aggregate data across multiple sources, like an auto scaling group for example, so if you were interested in seeing how the CPU load was handling overtime across your entire fleet you could create a dashboard that would display that. </p>
<p>Well by now you might be curious how to actually create your own dashboards. It is actually fairly painless to create dashboards with the visual dashboard creation tools provided by AWS within the CloudWatch console.</p>
<p>Creating dashboards in the editor is as simple as drag and dropping and adding new widgets onto a blank canvas. the editor allows you to pick any of the previously mentioned different types of media widgets and place them where you please. Pieces are rearrangeable and can be placed with as much finite controls as you desire. all widgets have a stretchable window view that you can position into specific sizes.</p>
<p>Dashboards can also be written as code giving you programmatic access to all the same information and tools. This means you can also put these code snippets inside cloud formation templates for easy dashboard creation on new accounts or projects. Creating these codified dashboards however is not as easy as it may sound at first. There is a lot of work that goes into testing and making sure your creation functions well.</p>
<p>Your dashboard code is written as a string in JSON formatting and can include anywhere between 0 to 100 separate widget objects. You have to specifically note down the x,y location of your widgets as well as the width and height of each element. That can be a little tedious to set up for the first time, but if you already have a functional blueprint, you can modify that fairly easily.</p>
<p>Here is an example of what your dashboards will look like when written out in code. This structure contains one metric widget ( this is the number widget displaying a metric ) and one text widget. The metric widget follows the CPU utilization of one instance with the id: i-012345.</p>
<p>And the text widget is just a simple title showing off the capabilities of the widget.</p>
<p>When you’re building your charts and after you have them completed you have the ability to add annotations to your graphs. This is Helpful for displaying when a certain event has taken place in the past which could help give other members of your team insight and exposure to certain peaks and valleys in your information. Just like writing good code requires comments it’s especially important to make sure your graphs and charts also have that advantage.</p>
<p>You can have both horizontal and vertical annotations in your graphs - each having their own purpose. For example, horizontal annotations can denote reasonable top and bottom bounds for a service’s CPU load while vertical annotations are great for noting when a specific event happened in the past.</p>
<p>You also have the ability to link to other dashboards within your own systems or even across accounts. These dashboards don’t have to be in the same region either. This is a very powerful tool that helps to centralize operations teams, DevOps, and other service owners who all need to have visibility into the status of your applications.</p>
<p>In order to allow cross-account and cross-region access, you need to enable it within the CloudWatch settings for your account as well as each of the accounts you wish to connect to. You can then link your accounts together, to share CloudWatch data between. These settings can also be activated within the AWS SDK and CLI.</p>
<p>Now, this sharing capability is not all or nothing, you have a few options:</p>
<ul>
<li>Share a single dashboard and designate specific email addresses and passwords of the people who can view the dashboard.</li>
<li>Share a single dashboard publicly, so that anyone who has the link can view the dashboard.</li>
<li>Share all the CloudWatch dashboards in your account and specify a third-party single sign-on (SSO) provider for dashboard access. All users who are members of this SSO provider’s list can access the dashboards in the account. To enable this, you integrate the SSO provider with Amazon Cognito.</li>
</ul>
<p>CloudWatch Dashboards allow you to have up to three dashboards - each containing up to 50 metrics at no charge. This is more than enough for anyone just practicing or having a few applications they want to monitor. For any more than that however, you will be charged $3 per month per new dashboard you wish to create. </p>
<p>For an enterprise company, that is not too much to spend. However If you are a solo developer or a small shop just starting off - those little 3 dollar charges can add up if you create dashboards willy nilly. So make sure you use your resources appropriately when building dashboards for your services.</p>
<p>Some Dashboard best practices:</p>
<ul>
<li>Use larger graphs for the most important display metrics - this may seem like a fairly obvious thing to do, but it’s important to keep in mind that humans are visual creatures. If you want them to pay attention to something, make it big and obvious.</li>
<li>Layout your dashboards and graph for the average minimum display resolution of your users. - this can help make sure that all relevant data is on screen at one time. This prevents users from missing key information that might be off-screen, which in the case of time-sensitive issues or events could be catastrophic. Most screens these days are able to handle 1920 by 1080 fairly well, however if you know your support staff all look at things on their phones, maybe you can design your dashboards around that instead.</li>
<li>Display time zones within your graphs for time-based data and if multiple operators are expected to be using the same dashboard simultaneously keep the time zone in UTC. This allows people to know at a glance, when an event took place. Its also important during an emergency that all users are working on the same premise in regards to the time the event happened, having to calculate differences in time zones can be frustrating when your customers’ satisfaction and your business is on the line.</li>
<li>Default your time interval and datapoint period to whatever is the most common use case.</li>
<li>Avoid plotting too many datapoints within your graphs. Having too much data can slow the dashboard loading time and might reduce the visibility of anomalies.</li>
<li>Annotate your graphs with the relevant alarm thresholds for your services. - this allows your users to understand at a glance if one of your services is about to go over its SLA times or when something terrible is about to happen. Having alarms is great, but never triggering them because you knew something was wrong ahead of time is way better.</li>
<li>Don’t assume your users will know what each metric means - be aggressive with tagging and having descriptions right in the dashboard using text widgets.</li>
</ul>
<h1 id="Wrap-Up"><a href="#Wrap-Up" class="headerlink" title="Wrap Up"></a>Wrap Up</h1><p>One of the biggest troubles when working with cloud architectures is understanding what’s going on behind the scenes. The technology is no longer built with physical devices that you can get your hands on so pulling information out of the system is paramount to success. </p>
<p>CloudWatch dashboards gives you visibility into what your resources are doing. being able to effectively use CloudWatch dashboards will give you such incredible insight into your architectures that not doing so might even be considered detrimental. </p>
<p>And it’s almost inexcusable not to at least give it a try, the creation system with the visual Wizard allows you to drag and drop these widget elements into a live dashboard. Allowing you to mix and match and change the sizes of anything that you desire. And once you understand what’s important to you and your architectures you can make repeatable code that you can inject right into your cloud formation templates to automatically create these highly visual and extremely potent dashboards for every system you set up.</p>
<p>And heck if you don’t even want to get your hands dirty, CloudWatch automatically sets up dashboards for you that can help you track some of the high-level systems within your services. You can go in and modify these automatically created dashboards to make them your own if you’re not comfortable starting from scratch.</p>
<p>I guess the big thing I really want you to take away from this is that CloudWatch dashboards are so easy to use. That I want you to just go give it a try, and you might be surprised what kind of insights you’ll gain by having all your information at the tips of your fingers. </p>
<p>Well, that brings us to the end of this course. My name is Will Meadows and I’d like to thank you for spending your time here learning about CloudWatch Dashboards. If you have any feedback, positive or negative, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated, thank you!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24/" class="post-title-link" itemprop="url">AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:21" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:21-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:56:58" itemprop="dateModified" datetime="2022-11-19T22:56:58-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course where I shall be taking a high-level look at Amazon CloudWatch and some of its features and components which enable you to monitor the health and performance of your environment to ensure it’s operating within its expected thresholds.</p>
<p>Before we start I’d like to introduce myself, my name is Stuart Scott, and I am the AWS content and security lead here at Cloud Academy. Feel free to connect with me to ask any questions using the details shown on the screen, alternatively you can always get in touch with us here at Cloud Academy by sending an e-mail to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> where one of our Cloud experts will reply to your question.</p>
<p>This course is ideally suited to those who are unfamiliar with Amazon CloudWatch and are looking to find out more about the service at an introductory level. </p>
<p>The objective of this course is to introduce you to Amazon CloudWatch, explaining what the service is used for and some of the features that it provides.</p>
<p>As a prerequisite to this course, you should have a very basic understanding of AWS. This is an introductory course and so it will cover the basics of Amazon CloudWatch</p>
<p>Feedback on our courses here at Cloud Academy is valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>Please note that, at the time of writing this content, all course information was accurate. AWS implements hundreds of updates every month as part of its ongoing drive to innovate and enhance its services.</p>
<p>As a result, minor discrepancies may appear in the course content over time. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. </p>
<p>So, if you notice any information that is outdated, please contact <a href="mailto:&#x73;&#x75;&#112;&#x70;&#x6f;&#x72;&#116;&#x40;&#99;&#108;&#111;&#117;&#100;&#x61;&#99;&#97;&#100;&#x65;&#x6d;&#121;&#46;&#99;&#111;&#109;">&#x73;&#x75;&#112;&#x70;&#x6f;&#x72;&#116;&#x40;&#99;&#108;&#111;&#117;&#100;&#x61;&#99;&#97;&#100;&#x65;&#x6d;&#121;&#46;&#99;&#111;&#109;</a>. This will allow us to update the course during its next release cycle.</p>
<p>Thank you! </p>
<h1 id="What-is-Amazon-CloudWatch"><a href="#What-is-Amazon-CloudWatch" class="headerlink" title="What is Amazon CloudWatch?"></a>What is Amazon CloudWatch?</h1><p>Hello and welcome to this lecture which will provide you with a high-level overview of what Amazon CloudWatch is and does.</p>
<p>Amazon CloudWatch is a global service that has been designed to be your window into the health and operational performance of your applications and infrastructure. It’s able to collate and present meaningful operational data from your resources allowing you to monitor and review their performance. This gives you the opportunity to take advantage of the insights that CloudWatch presents, which in turn can trigger automated responses or provide you with the opportunity and time to make manual operational changes and decisions to optimize your infrastructure if required. </p>
<p>Understanding the health and performance of your environment is one of the fundamental operations you can do to help you minimize incidents, outages and errors. As a result Amazon CloudWatch is heavily used by those in an operational role and site reliability engineers. </p>
<p>There are a wide range of components to Amazon CloudWatch, making this an extremely powerful service. Let me now run through at a high level some of these features and what they allow you to do, including CloudWatch Dashboards, CloudWatch Metrics and Anomaly Detection, CloudWatch Alarms, CloudWatch EventBridge, CloudWatch Logs, CloudWatch Insights.</p>
<p>Using the AWS Management console, the AWS CLI, or the PutDashboard API, you can build and customize a page using different visual widgets displaying metrics and alarms relating to your resources to form a unified view. These dashboards can then be viewed from within the AWS Management Console.</p>
<p>Here is an example of the different types of widgets you can select to build your dashboard.</p>
<p>The resources within your customized dashboard can be from multiple different regions making this a very useful feature. Being able to build your own views, you can quickly and easily design and configure different dashboards to represent the data that you need to see from a business and operational perspective. For example, you might need to view all performance metrics and alarms from resources relating to a particular project, or a specific customer. Or you might want to create a different dashboard for a specific region or application deployment. The key point is that they are fully customizable to be designed how YOU want to represent your data.  </p>
<p>For more information of selecting the right chart type to visualize data, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/">https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/</a></p>
<p>Once you have built your Dashboards, you can easily share them with other users, even those who may not have access to your AWS account. This allows you to share the findings gathered by CloudWatch with those who may find the results interesting and beneficial to their day-to-day operational role, but don’t necessarily require the need to access your AWS account.</p>
<p>Metrics are a key component and fundamental to the success of Amazon CloudWatch, they enable you to monitor a specific element of an application or resource over a period of time while tracking these data points. For example, the number of DiskReads or DiskWrites on an EC2 instance, these are just 2 metrics relating to EC2 that you can monitor. Different services will offer different metrics, for example, there is no DiskReads for Amazon S3 as it’s not a compute service, and so instead metrics relevant to the service are available, such as NumberOfObjects, which tracks the number of objects in a specified bucket.</p>
<p>By default when working with Amazon CloudWatch, everyone has access to a free set of Metrics, and for EC2, these are collated over a time period of 5 minutes. However, for a small fee, you can enable detailed monitoring which will allow you to gain a deeper insight by collating data across the metrics every minute. In addition to detailed monitoring, you can also create your own custom metrics for your applications, using any time-series data points that you need, but be aware that when you create a metric they are regional, meaning that any metrics created in 1 region will not be available in another.</p>
<p>CloudWatch metrics also allow you to enable a feature known as anomaly detection. This allows CloudWatch to implement machine learning algorithms against your metric data to help detect any activity that sits outside of the normal baseline parameters that are generally expected. Advance warning of this can help you detect an issue long before it becomes a production problem.</p>
<p>Amazon CloudWatch Alarms tightly integrate with Metrics that I just discussed and they allow you to implement automatic actions based on specific thresholds that you can configure relating to each metric.</p>
<p>For example, you could set an alarm to activate an auto scaling operation, such as provisioning another instance if your CPUUtilization of an EC2 instance peaked at 75% for more than 5 minutes. You could also configure an alarm to send a message to an SNS Topic when the same instance drops back below the 75% threshold, causing it to come out of an ‘alarm’ state notifying engineers of the change. </p>
<p>For more information on SNS, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/">https://cloudacademy.com/course/using-sqs-sns-ses/</a></p>
<p>Speaking of Alarm states, there are 3 different states for any alarm associated with a metric, these being OK – The metric is within the defined configured threshold, ALARM – The metric has exceeded the thresholds set, and INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.</p>
<p>CloudWatch alarms are also easily integrated with your dashboards as well, allowing you to quickly and easily visualize the status of each alarm. When an alarm is triggered into a state of ALARM, it will turn red on your dashboard, giving a very obvious indication.</p>
<p>CloudWatch EventBridge is a feature that has evolved from an existing feature called Amazon Events. So if you have any prior experience working with CloudWatch Events then this will be fairly familiar to you.  </p>
<p>CloudWatch EventBridge provides a means of connecting your own applications to a variety of different targets, typically AWS services, to allow you to implement a level of real-time monitoring, allowing you to respond to events that occur in your application as they happen.  </p>
<p>But what is an event? Basically, an event is anything that causes a change to your environment or application.</p>
<p>The big benefit of using CloudWatch EventBridge is that it offers you the opportunity to implement a level of event-driven architecture in a real-time decoupled environment. </p>
<p>EventBridge establishes a connection between your applications and specified targets to allow a data stream of events to be sent. Currently, there is a wide range of targets that can be used as a destination for events as you can see here.</p>
<p>For the latest list of targets, please see the relevant documentation here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html</a></p>
<p>Let me provide a quick level overview of some of the elements of this feature, and these include Rules, Targets, and Event Buses.</p>
<p>So starting with Rules. A rule acts as a filter for incoming streams of event traffic and then routes these events to the appropriate target defined within the rule. The rule itself can route traffic to multiple targets, however the target must be in the same region. </p>
<p>Next, we have Targets. We saw a list of these just a few moments ago, so targets and where the events are sent by the Rules, such as AWS Lambda, SQS, Kinesis or SNS. All events received by the target are done os in a JSON format</p>
<p>Now finally, Event Buses. An Event Bus is the component that actually receives the Event from your applications and your rules are associated with a specific event bus. CloudWatch EventBridge uses a default Event bus that is used to receive events from AWS services, however, you are able to create your own Event Bus to capture events from your own applications. </p>
<p>CloudWatch Logs gives you a centralized location to house all of your logs from different AWS services that provide logs as an output, such as CloudTrail, EC2, VPC Flow logs, etc, in addition to your own applications.</p>
<p>When log data is fed into Cloudwatch Logs you can utilize CloudWatch Log Insights to monitor the logstream in real time and configure filters to search for specific entries and actions that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. </p>
<p>An added advantage of CloudWatch logs comes with the installation of the Unified CloudWatch Agent, which can collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. This metric data is in addition to the default EC2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. </p>
<p>There are now 3 different types of insights within CloudWatch, there are Log Insights, Container Insights, and Lambda Insights.</p>
<p>But what exactly are insights? Well as the name suggests, they provide the ability to get more information from the data that CloudWatch is collecting. So let’s look at each of these at a high level to understand the role that they perform, starting with Log Insights.</p>
<p>This is a feature that can analyze your logs that are captured by CloudWatch Logs at scale in seconds using interactive queries delivering visualizations that can be represented as bar, line, pie, or stacked area charts. The versatility of this feature allows you to work with any log file formats that AWS services or your applications might be using.</p>
<p>Using a flexible approach, you can use Log insights to filter your log data to retrieve specific data allowing you to gather insights that you are interested in. Also using the visual capabilities of the feature, it can display them in a visual way.</p>
<p>Much like Log insights, Container Insights allow you to collate and group different metric data from different container services and applications within AWS, for example, the Amazon Elastic Kubernetes Service, (EKS) and the Elastic Container Service (ECS). </p>
<p>In addition to the standard metrics collected for these services by CloudWatch, Container Insights also allows you to capture and monitor diagnostic data giving you additional insights into how to resolve issues that arise within your container architecture. This monitoring and insight data can be analyzed at the cluster, node, pod, and task level making it a valuable tool to help you understand your container applications and services.</p>
<p>As you may have guessed by now, this feature provides you the opportunity to gain a deeper understanding of your applications using AWS Lambda. Working on the principles as we have seen with the previous 2 insight features, it gathers and aggregates system and diagnostic metrics related to AWS Lambda to help you monitor and troubleshoot your serverless applications.</p>
<p>To enable Lambda Insights, you need to enable the feature per Lambda function that you create within Monitoring Tools section of your function:</p>
<p>This ensures that a CloudWatch extension is enabled for your function allowing it to collate system-level metrics which are recorded every time the function is invoked.</p>
<h1 id="CloudWatch-infrastructure-monitoring"><a href="#CloudWatch-infrastructure-monitoring" class="headerlink" title="CloudWatch - infrastructure monitoring"></a>CloudWatch - infrastructure monitoring</h1><p>Hi and welcome to our seventh lecture. In this lecture, we will take a look at Cloud Watch and explore its main features. We will also work with one of the best features of Cloud Watch, the custom metrics, where we can upload any data we want and monitor it from AWS.</p>
<p>First, we need to go to the Cloud Watch dashboard. Here, we can see the two alarms that we created in the last lecture. The low CPU utilization alarm is sending emails to the Cloud Motors admins topic.</p>
<p>We don’t want that, as this information is not critical for us. So let’s remove this notification and adjust the alarm, because after a while, we noticed that it was removing instances too soon. In the same way, let’s modify the high CPU utilization alarm, because it was also adding instances to auto scaling too soon. If you explore enough, you will notice that we could use the same alarm to trigger the increase and decrease policies that we specified in auto scaling. Let’s check the metrics. Here, we can see a list of all available metrics that we can query. We can see graphics, combine graphics with multiple metrics, or set alarms for the metrics that we see here. Depending on what you have in your environment, you could have more or less metrics to view.</p>
<p>If we select one metric, we can see a graphic with the metrics value over the time. We can also combine the current graphic with more than one metric. That way, we could, for instance, see what happens with the instance IO when the CPU utilization is high, or how the RDS database reacts when there is too much network traffic at the instances on the auto scaling group, or how many requests are being processed by ELB during the working hours. There are many possible choices. It’s up to you to combine the data and explore the possibilities. You could select the metrics that you want, build a graphic and share it. You just need to click on Copy URL and share the link.</p>
<p>You can select the exact time frame that you want to see on the graphic. But besides AWS services, Cloud Watch can also monitor how much you’re going to pay for the current month. You can also set alarms for that. You just need to go to the North Virginia region and view the billing metrics. Here you could, for instance, create a billing alarm in case you have a defined budget for your project. Since we want to save money, let’s create an alarm that will send an email to the one who’s responsible for the bills. Whenever the estimated cost is more than zero, it will trigger the alarm. In other words, we don’t want to pay anything. To really have billing alarms, you would need to first go on the billing and cost management console, click on preferences, and then select the receive billing alerts. I can’t do this since I’m not using the route account, and my IAM user doesn’t have permissions for that. Now, we are done.</p>
<p>For me, the best feature of Cloud Watch is the custom metrics that we can send and monitor as we do with any other metric.</p>
<p>I’ve written a simple script that’s available in the GitHub repository in the <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/cloudmotors/tree/master/scripts">scripts folder</a>. Before running, you need to have the AWS command line interface configured on your machine. Since I have it already, I will skip that part and show you the script.</p>
<p>You just need to change the end point variable and allow ping on the ELS’ security group to allow it to work properly. The ping will get the latency and curl will count the time that the welcome page is taking to load. I’m running this script as we speak, and it’s sending data already.</p>
<p>Let’s check the data. And here we have our custom metrics. And we can compare them on the same graphic over a time frame as we already saw. We can also export the graphic URL as any other graphic. I strongly recommend you take some time and explore everything that Cloud Watch can provide.</p>
<p>Lectures:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/what-to-study-and-course-introduction/">AWS SysOps Administrator Roles and Responsibilities</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/initial-application-setup-cloudformation/">Initial Application Setup</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/troubleshooting-and-diagnosing-failpoints/">Troubleshooting</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/managing-instances-through-scripts-with-aws/">Managing instances</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/deploying-elb-as/">Deploying an Elastic Load Balancer</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/complete-the-elastic-infrastructure-with-aws/">Complete the Elastic infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/cloudwatch-infrastructure-monitoring-with-aws/">Infrastructure monitoring with CloudWatch</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/standards-deployment-aws/">Industry standards</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/application-security-in-aws/">Application Security</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/aws-shared-responsibility-model-1/">The AWS Shared Responsibility Model</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/elastic-beanstalk/">Elastic Beanstalk</a></li>
</ul>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello and welcome to the final lecture in this course where I just want to quickly summarize what we have covered. </p>
<p>So we now know what Amazon CloudWatch is, it’s a global service designed to be your window into the health and operational performance of your applications and infrastructure. It comes complete with many different features that allow us to capture and log essential data relating to our resources and applications. </p>
<p>By creating CloudWatch Dashboards you can build and customize different visual widgets to display metrics and alarms relating to your resources to form a unified view. This provides a very quick and easy method of viewing the status of your infrastructure based on metrics you have defined.</p>
<p>CloudWatch Metrics are key to the service, they enable you to monitor a specific element of an application or resource over a period of time while tracking these data points. This can help to ensure that you are not undersizing your resources, or in fact, oversizing your resource with too much spare capacity. Different services offer different metrics, allowing you to monitor the most important performance factors for each service.</p>
<p>CloudWatch Alarms enable you to implement automatic responses and actions based on custom thresholds defined for your metrics. Alarms can be in any 1 of 3 different states, either </p>
<ul>
<li>OK – The metric is within the defined configured threshold</li>
<li>ALARM – The metric has exceeded the thresholds set </li>
<li>INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state</li>
</ul>
<p>By using CloudWatch EventBridge provides a means of connecting your own applications to different targets to allow you to implement a level of real-time monitoring, allowing you to respond to events that occur in your application as they happen. This offers you the opportunity to implement a level of event-driven architecture in a real-time decoupled environment. </p>
<p>CloudWatch Logs centralize your logs from different AWS services and applications allowing you to monitor them in real time and filter for specific entries and actions. They can also capture log data from your on-premise infrastructure through the use of a logging agent.</p>
<p>There are 3 different variants of CloudWatch Insights, these being:</p>
<ul>
<li>Log Insights</li>
<li>Container Insights</li>
<li>Lambda Insights</li>
</ul>
<p>And they all provide the ability to collate and display more information from the data that CloudWatch is collecting. Log insights can analyze your logs at scale using interactive queries. Container insights allow you to collate and group metric data from container services such as EKS and ECS, and Lambda insights provides you the opportunity to gain a deeper understanding of your applications using AWS Lambda.</p>
<p>That now brings me to the end of this lecture and to the end of this course, and so you should now have a greater understanding of Amazon CloudWatch and the different features and options it can provide.</p>
<p>Feedback on our courses here at Cloud Academy is valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="2What-is-Amazon-CloudWatch"><a href="#2What-is-Amazon-CloudWatch" class="headerlink" title="2What is Amazon CloudWatch?"></a>2<strong>What is Amazon CloudWatch?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/course-introduction/">Data Visualization: How to Convey your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-sqs-sns-ses/">Using SQS, SNS and SES in a Decoupled and Distributed Environment</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">List of metrics</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">List of targets</a></p>
<h1 id="3CloudWatch-infrastructure-monitoring"><a href="#3CloudWatch-infrastructure-monitoring" class="headerlink" title="3CloudWatch - infrastructure monitoring"></a>3<strong>CloudWatch - infrastructure monitoring</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/cloudmotors/tree/master/scripts">Scripts Folder</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/133/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/133/">133</a><span class="page-number current">134</span><a class="page-number" href="/page/135/">135</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/135/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
