<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/54/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/54/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:45" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:45-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:00:22" itemprop="dateModified" datetime="2022-11-20T19:00:22-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Management Fundamentals in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce some management services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#x75;&#112;&#x70;&#111;&#114;&#116;&#64;&#x63;&#x6c;&#111;&#117;&#x64;&#x61;&#99;&#x61;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#109;">&#115;&#x75;&#112;&#x70;&#111;&#114;&#116;&#64;&#x63;&#x6c;&#111;&#117;&#x64;&#x61;&#99;&#x61;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various management services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to various management services in AWS that can assist with auditing, reporting, and monitoring within your AWS environments, including:</p>
<ul>
<li>AWS CloudTrail,</li>
<li>AWS Config, and</li>
<li>Amazon CloudWatch.</li>
</ul>
<p>These services are covered by Domain 2 in the official AWS Certified Cloud Practitioner exam blueprint: Security and Compliance, which accounts for 25% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="AWS-CloudTrail"><a href="#AWS-CloudTrail" class="headerlink" title="AWS CloudTrail"></a>AWS CloudTrail</h1><p>Hello, and welcome to this lecture. In this lecture I will explain the basic fundamentals of AWS CloudTrail to give you an overview of the service before we look deeper at the inner workings, revealing how the different elements work together. So what is CloudTrail and what does it do? CloudTrail is a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface, from within the AWS management console, or even from a request made by another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service.</p>
<p>For example, when auto scaling automatically sends and API request to launch or terminate an instance. These API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file, which is then stored on S3. Each API call represents a new event within the log file. CloudTrail also records and associates other identifying metadata with all the events.</p>
<p>For example, the identity of the caller, the timestamp of when the request was initiated, and the source IP address. In a later lecture entitled Insight Into CloudTrail Logs, I will look at these log files deeper, while I’ll provide an example of a log file showing the different attributes recorded. For greater management, new log files are typically created every five minutes. Which are then delivered and stored within an S3 bucket that is defined by you during your CloudTrail configuration.</p>
<p>This allows you to easily go back and review the history of all API requests made. There’s also an option to have these logs delivered to a CloudWatch Logs log file as well. Having this association with CloudWatch enables custom metrics to be converted to monitor specific API requests. Thresholds can be set against these metrics and when crossed, the simple notification service, SNS, can be triggered to notify your security teams to investigate. That, at a very high level, is the overall function of the AWS CloudTrail service.</p>
<p>Now let’s take a look at the CloudTrail architecture to understand where it can be implemented from an AWS region standpoint, and which services can be supported. AWS CloudTrail is a global service with support for all regions. Support for the latest region, EU London was added in mid December, 2016. In addition to this worldwide coverage, CloudTrail also provides support for over 60 AWS services and features across a wide-range of service categories.</p>
<p>As you can imagine, with this extensive coverage, CloudTrail can capture a vast amount of data if you have a multi-region, multi-service infrastructure environment deployed. So armed with this information, what can you do with it? How can you use this data to help you manage and support your AWS infrastructure? Well there are a number of ways you can use the data captured by CloudTrail to help you enhance your AWS environment. Firstly, and as mentioned earlier, it can be used very effectively as a security analysis tool. CloudTrail events provide very specific information about where an API call originated from and who, or what initiated the request.</p>
<p>As a result, if malicious activity was detected via irregular trends or restricted API call thresholds, with the use of CloudWatch, then a number of security controls can be quickly implemented to prevent the user from causing additional damage. Another common use for CloudTrail is to help resolve and manage day to day operational issues and problems. Using built-in filtering mechanisms, it’s possible to quickly find who, what and when a particular API was used, which could have potentially caused an outage or service interruption. This enables quicker root cause identification, resulting in a speedy resolution.</p>
<p>Appropriate actions can then be taken to ensure the incident does not reoccur in your environment. As API calls to add, modify or delete results are captured CloudTrail can be an effective method of tracking changes to resources within your environment. There is another AWS service that is specifically designed to order and track changes to resources, which is called AWS Config, which CloudTrail interacts with, however, CloudTrail can be used to capture the actual API request, and all associated data which made the change. And if you’re not using AWS Config then this at least provides some base level of monitoring and tracking.</p>
<p>From a governance and security legislation perspective, many certifications require the ability to recall and provide evidence of log files relating to specific changes to resources. CloudTrail provides all of this by default, through the use of capturing events and writing them to a log file, which is then stored on S3. AWS has a great whitepaper on achieving compliance, using CloudTrail entitled Logging in AWS, How AWS CloudTrail Can Help You Achieve Compliance by logging API calls and Changes to Resources.</p>
<p>The following URL will take you to that whitepaper. If you need to be able to capture and track API requests within your AWS account, for any of these reasons mentioned, or perhaps for other reasons you may have of your own, then CloudTrail can do this for you and deliver the output as a log file into an S3 bucket of your choice.</p>
<p>Lectures:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/introduction-15/">Management fundamentals for AWS</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-config-2/">AWS Config</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-cloudtrail-1/">AWS CloudTrail</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/amazon-trusted-advisor-2/">Amazon Trusted Advisor</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/management-fundamentals-of-aws-for-sol-arch/amazon-clouwatch-1/">Amazon CloudWatch</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-personal-health-dashboard-2/">AWS Personal Health Dashboard</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/summary-36/">Summary</a></li>
</ul>
<h1 id="AWS-Config"><a href="#AWS-Config" class="headerlink" title="AWS Config"></a>AWS Config</h1><p>Hello and welcome to this lecture where we will talk about the AWS Config service, itself, what it is, and what it does. So let’s get started. As many of you will be aware, one of the biggest headaches in any organization when it comes to resource management of IT infrastructure is understanding the following: What resources do we have? What devices are out there within our infrastructure performing functions? Do we have resources that are no longer needed and therefore can we be saving money by switching them off? What is the status of their current configuration?</p>
<p>Are there any security vulnerabilities we need to worry about? How are our resources linked within the environment? What relationships are there and are there any dependencies? If we make a change to one resource, will this effect another? What changes have occurred on the resources and by whom? Do we have a history of changes for this resource that shows us how the resource has changed over time? Is the infrastructure compliant with specific governance controls and how can we check to ensure that this configuration is meeting specific internal and external requirements?</p>
<p>And do we have accurate auditing information that can be passed to external auditors for compliance checks? Depending on the size of your deployment with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, trying to answer some of these questions can be very time-consuming and laborious. Some of this information can be captured by the AWS CLI by performing a describe, or list, against the specific resource. But implementing a system to capture those results and output them into a readable format could be very resource-intensive.</p>
<p>And of course, this will only help you with a small piece of the puzzle. AWS is aware that due to the very nature of the cloud and its benefits, the resources within an AWS environment are likely to fluctuate frequently, along with the configurations of the resources. The cloud by its very nature is designed to do so, and so trying to keep up with the resource management can be a struggle. Because of this, AWS released AWS Config to help with this very task.</p>
<p>The service has been designed to record and capture resource changes within your environment, allowing you to perform a number of actions against the data that helps to find answers to the questions that we highlighted previously. So what did AWS design AWS Config to do? Well, in a nutshell, AWS Config can capture resource changes. So any change to a resource supported by Config can be recorded, which will record what change along with other useful metadata all held within a file known as a configuration item, a CI.</p>
<p>It can act as a resource inventory. AWS Config can discover supported resources running within your environment, allowing you to see data about that resource type. You can store configuration history for individual resources. The service will record and hold all existing changes that have happened against the resource, providing a useful history record of changes. It can provide a snapshot in time of current resource configurations. An entire snapshot of all supported resources within a region can be captured that will detail their current configurations with all related metadata.</p>
<p>Enable notifications of when a change has occurred on a resource. The Simple Notification Service, SNS, is used with AWS Config to capture a configuration stream of changes, enabling you to process and analyze the changes to resources. It can provide the information on who made the change and when, through AWS CloudTrail integration. AWS CloudTrail is used with AWS Config to help you identify who made the change and when, and with which API. You can enforce rules that check the compliancy of your resource against specific controls. Predefined and custom rules and be configured with AWS Config, allowing you to check resources’ compliance against these rules. You can perform security analysis within your AWS environment.</p>
<p>A number of security resources can be recorded and when this is coupled with rules relating to security, such as encryption checks, this can become a powerful analysis tool. And it can provide relationship connectivity information between resources. The AWS management console provides a great relationship query, allowing you to quickly see and identify which resources are related to any other resource. For example, when looking at an EBS volume, you’ll be able to see which EC2 instance it is connected to.</p>
<p>And it does all of this and presents the data in a friendly format. This is a lot of incredibly useful data that can be used across a range of different scenarios, some of which we will cover later in this course. Now unfortunately at the time we’re writing this course, the AWS Config service does not capture this information for all services. But it certainly captures data for the most common services and resources, which you would want to hold information for.</p>
<p>Services such as: EC2, RDS, IAM, and VPC. And it’s great to see that within each of these, there are specific security resources that are covered, such as security groups and custom IAM policies. This makes AWS Config very useful when it comes to carrying out a security analysis, which we will cover in a later lecture. For more information on the latest resources that AWS Config supports, please see the link on-screen.</p>
<p>AWS Config is region-specific, meaning that if you have resources in multiple regions, then you will have to configure AWS Config for each region you want to record resource changes for. When doing so, you are able to specify different options for each region. For example, you could configure Config in one region to record all supported resources across all services within that region and add a pre-defined AWS manage config rule that would check if EBS volumes are encrypted.</p>
<p>In another region, you could select to only record a specific type of resource, such as security groups, with no pre-defined rules allocated. Some of you may be wondering, what if the service you want to monitor is not region-specific, such as IAM? Well in this case, there is a separate option to include global services, which IAM falls under.</p>
<p>Lectures:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/introduction-15/">Management fundamentals for AWS</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-config-2/">AWS Config</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-cloudtrail-1/">AWS CloudTrail</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/amazon-trusted-advisor-2/">Amazon Trusted Advisor</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/management-fundamentals-of-aws-for-sol-arch/amazon-clouwatch-1/">Amazon CloudWatch</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-personal-health-dashboard-2/">AWS Personal Health Dashboard</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/summary-36/">Summary</a></li>
</ul>
<h1 id="What-is-Amazon-CloudWatch"><a href="#What-is-Amazon-CloudWatch" class="headerlink" title="What is Amazon CloudWatch?"></a>What is Amazon CloudWatch?</h1><p>Hello and welcome to this lecture which will provide you with a high-level overview of what Amazon CloudWatch is and does.</p>
<p>Amazon CloudWatch is a global service that has been designed to be your window into the health and operational performance of your applications and infrastructure. It’s able to collate and present meaningful operational data from your resources allowing you to monitor and review their performance. This gives you the opportunity to take advantage of the insights that CloudWatch presents, which in turn can trigger automated responses or provide you with the opportunity and time to make manual operational changes and decisions to optimize your infrastructure if required. </p>
<p>Understanding the health and performance of your environment is one of the fundamental operations you can do to help you minimize incidents, outages and errors. As a result Amazon CloudWatch is heavily used by those in an operational role and site reliability engineers. </p>
<p>There are a wide range of components to Amazon CloudWatch, making this an extremely powerful service. Let me now run through at a high level some of these features and what they allow you to do, including CloudWatch Dashboards, CloudWatch Metrics and Anomaly Detection, CloudWatch Alarms, CloudWatch EventBridge, CloudWatch Logs, CloudWatch Insights.</p>
<p>Using the AWS Management console, the AWS CLI, or the PutDashboard API, you can build and customize a page using different visual widgets displaying metrics and alarms relating to your resources to form a unified view. These dashboards can then be viewed from within the AWS Management Console.</p>
<p>Here is an example of the different types of widgets you can select to build your dashboard.</p>
<p>The resources within your customized dashboard can be from multiple different regions making this a very useful feature. Being able to build your own views, you can quickly and easily design and configure different dashboards to represent the data that you need to see from a business and operational perspective. For example, you might need to view all performance metrics and alarms from resources relating to a particular project, or a specific customer. Or you might want to create a different dashboard for a specific region or application deployment. The key point is that they are fully customizable to be designed how YOU want to represent your data.  </p>
<p>For more information of selecting the right chart type to visualize data, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/">https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/</a></p>
<p>Once you have built your Dashboards, you can easily share them with other users, even those who may not have access to your AWS account. This allows you to share the findings gathered by CloudWatch with those who may find the results interesting and beneficial to their day-to-day operational role, but don’t necessarily require the need to access your AWS account.</p>
<p>Metrics are a key component and fundamental to the success of Amazon CloudWatch, they enable you to monitor a specific element of an application or resource over a period of time while tracking these data points. For example, the number of DiskReads or DiskWrites on an EC2 instance, these are just 2 metrics relating to EC2 that you can monitor. Different services will offer different metrics, for example, there is no DiskReads for Amazon S3 as it’s not a compute service, and so instead metrics relevant to the service are available, such as NumberOfObjects, which tracks the number of objects in a specified bucket.</p>
<p>By default when working with Amazon CloudWatch, everyone has access to a free set of Metrics, and for EC2, these are collated over a time period of 5 minutes. However, for a small fee, you can enable detailed monitoring which will allow you to gain a deeper insight by collating data across the metrics every minute. In addition to detailed monitoring, you can also create your own custom metrics for your applications, using any time-series data points that you need, but be aware that when you create a metric they are regional, meaning that any metrics created in 1 region will not be available in another.</p>
<p>CloudWatch metrics also allow you to enable a feature known as anomaly detection. This allows CloudWatch to implement machine learning algorithms against your metric data to help detect any activity that sits outside of the normal baseline parameters that are generally expected. Advance warning of this can help you detect an issue long before it becomes a production problem.</p>
<p>Amazon CloudWatch Alarms tightly integrate with Metrics that I just discussed and they allow you to implement automatic actions based on specific thresholds that you can configure relating to each metric.</p>
<p>For example, you could set an alarm to activate an auto scaling operation, such as provisioning another instance if your CPUUtilization of an EC2 instance peaked at 75% for more than 5 minutes. You could also configure an alarm to send a message to an SNS Topic when the same instance drops back below the 75% threshold, causing it to come out of an ‘alarm’ state notifying engineers of the change. </p>
<p>For more information on SNS, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/">https://cloudacademy.com/course/using-sqs-sns-ses/</a></p>
<p>Speaking of Alarm states, there are 3 different states for any alarm associated with a metric, these being OK – The metric is within the defined configured threshold, ALARM – The metric has exceeded the thresholds set, and INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.</p>
<p>CloudWatch alarms are also easily integrated with your dashboards as well, allowing you to quickly and easily visualize the status of each alarm. When an alarm is triggered into a state of ALARM, it will turn red on your dashboard, giving a very obvious indication.</p>
<p>CloudWatch EventBridge is a feature that has evolved from an existing feature called Amazon Events. So if you have any prior experience working with CloudWatch Events then this will be fairly familiar to you.  </p>
<p>CloudWatch EventBridge provides a means of connecting your own applications to a variety of different targets, typically AWS services, to allow you to implement a level of real-time monitoring, allowing you to respond to events that occur in your application as they happen.  </p>
<p>But what is an event? Basically, an event is anything that causes a change to your environment or application.</p>
<p>The big benefit of using CloudWatch EventBridge is that it offers you the opportunity to implement a level of event-driven architecture in a real-time decoupled environment. </p>
<p>EventBridge establishes a connection between your applications and specified targets to allow a data stream of events to be sent. Currently, there is a wide range of targets that can be used as a destination for events as you can see here.</p>
<p>For the latest list of targets, please see the relevant documentation here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html</a></p>
<p>Let me provide a quick level overview of some of the elements of this feature, and these include Rules, Targets, and Event Buses.</p>
<p>So starting with Rules. A rule acts as a filter for incoming streams of event traffic and then routes these events to the appropriate target defined within the rule. The rule itself can route traffic to multiple targets, however the target must be in the same region. </p>
<p>Next, we have Targets. We saw a list of these just a few moments ago, so targets and where the events are sent by the Rules, such as AWS Lambda, SQS, Kinesis or SNS. All events received by the target are done os in a JSON format</p>
<p>Now finally, Event Buses. An Event Bus is the component that actually receives the Event from your applications and your rules are associated with a specific event bus. CloudWatch EventBridge uses a default Event bus that is used to receive events from AWS services, however, you are able to create your own Event Bus to capture events from your own applications. </p>
<p>CloudWatch Logs gives you a centralized location to house all of your logs from different AWS services that provide logs as an output, such as CloudTrail, EC2, VPC Flow logs, etc, in addition to your own applications.</p>
<p>When log data is fed into Cloudwatch Logs you can utilize CloudWatch Log Insights to monitor the logstream in real time and configure filters to search for specific entries and actions that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. </p>
<p>An added advantage of CloudWatch logs comes with the installation of the Unified CloudWatch Agent, which can collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. This metric data is in addition to the default EC2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. </p>
<p>There are now 3 different types of insights within CloudWatch, there are Log Insights, Container Insights, and Lambda Insights.</p>
<p>But what exactly are insights? Well as the name suggests, they provide the ability to get more information from the data that CloudWatch is collecting. So let’s look at each of these at a high level to understand the role that they perform, starting with Log Insights.</p>
<p>This is a feature that can analyze your logs that are captured by CloudWatch Logs at scale in seconds using interactive queries delivering visualizations that can be represented as bar, line, pie, or stacked area charts. The versatility of this feature allows you to work with any log file formats that AWS services or your applications might be using.</p>
<p>Using a flexible approach, you can use Log insights to filter your log data to retrieve specific data allowing you to gather insights that you are interested in. Also using the visual capabilities of the feature, it can display them in a visual way.</p>
<p>Much like Log insights, Container Insights allow you to collate and group different metric data from different container services and applications within AWS, for example, the Amazon Elastic Kubernetes Service, (EKS) and the Elastic Container Service (ECS). </p>
<p>In addition to the standard metrics collected for these services by CloudWatch, Container Insights also allows you to capture and monitor diagnostic data giving you additional insights into how to resolve issues that arise within your container architecture. This monitoring and insight data can be analyzed at the cluster, node, pod, and task level making it a valuable tool to help you understand your container applications and services.</p>
<p>As you may have guessed by now, this feature provides you the opportunity to gain a deeper understanding of your applications using AWS Lambda. Working on the principles as we have seen with the previous 2 insight features, it gathers and aggregates system and diagnostic metrics related to AWS Lambda to help you monitor and troubleshoot your serverless applications.</p>
<p>To enable Lambda Insights, you need to enable the feature per Lambda function that you create within Monitoring Tools section of your function:</p>
<p>This ensures that a CloudWatch extension is enabled for your function allowing it to collate system-level metrics which are recorded every time the function is invoked.</p>
<h1 id="4What-is-Amazon-CloudWatch"><a href="#4What-is-Amazon-CloudWatch" class="headerlink" title="4What is Amazon CloudWatch?"></a>4<strong>What is Amazon CloudWatch?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/course-introduction/">Data Visualization: How to Convey your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-sqs-sns-ses/">Using SQS, SNS and SES in a Decoupled and Distributed Environment</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">List of metrics</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">List of targets</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:44" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:44-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:00:10" itemprop="dateModified" datetime="2022-11-20T19:00:10-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Decoupled and Serverless Architectures in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the concepts of decoupled and event-driven architectures, as well as some serverless services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about decoupled and serverless architectures in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to decoupled and event-driven architectures in AWS. We’ll see how services such as the Amazon Simple Queue Service, or SQS; the Amazon Simple Notification Service, or SNS; and Amazon Kinesis can be used within these architectures. And finally, we’ll provide a survey-level overview of serverless services in AWS.</p>
<p>These topics and services are covered by Domains 1 and 3 in the official AWS Certified Cloud Practitioner exam blueprint: Cloud Concepts and Technology. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="What-is-a-Decoupled-and-Event-Driven-Architecture"><a href="#What-is-a-Decoupled-and-Event-Driven-Architecture" class="headerlink" title="What is a Decoupled and Event-Driven Architecture?"></a>What is a Decoupled and Event-Driven Architecture?</h1><p>Hello and welcome to this lecture where I want to explain what we mean by decoupled and event-driven architectures.</p>
<p>Firstly, let me focus on decoupled architecture, and to understand decoupling, we first need to address monolithic architectures which is how applications have been done in the past. Monolithic applications were built with a close and tight-knit relationship to each other, for example, between the front end and back end of an application. If a change was made the back end, it could easily disrupt services and operation in the front end, and that’s because they were very tightly coupled together and had a lot of built-in dependencies against each other. Although this had some advantages, it wasn’t able to offer what a decoupled architecture could.</p>
<p>When you implement and design a solution using a decoupled architecture you are building a solution put together using different components and services that can operate and execute independently of one another, instead of being closely coupled to each of its connecting components to operate and function. Each component in a decoupled solution is effectively unaware of any other changes to other components due to the segregation of boundaries applied.</p>
<p>Each service within a decoupled environment communicates with others using specific interfaces which remain constant throughout its development regardless of what configurations are made. By having this layered and independent approach, you are able to design, develop and configure each component without worrying about any dependencies within your solution. This allows your development teams to work faster and more efficiently as their scope of operation is refined on a particular service or component. They can make changes to a specific area of the application without having to worry about affecting other components, this helps to drive innovation and progress at a far greater rate.</p>
<p>As you go through this course, I will introduce you to an AWS service that is commonly used in a decoupled architecture, this being Amazon SQS, the Simple Queue Service.</p>
<p>Let me now look at Event-Driven Architectures.</p>
<p>Event-driven architectures closely relate and interact with decoupled architectures, however, services in an event-driven architecture are triggered by events that occur within the infrastructure. So what is an event? Well, an event can be a number of things, for example, a change of state, so a resource such as an EC2 instance changing from ‘running’ to ‘stopped’—that is a change of state, or perhaps an order has been placed on your website and an item has been moved from for sale to sold, that could be a change of state within your application.</p>
<p>When utilizing and implementing event-driven architectures in AWS, they will typically have three components: a producer, an event router, and consumers.</p>
<p>A producer is the element within the infrastructure that will push an event to the event router. The event router then processes the event and takes the necessary action in pushing the outcome to the consumers. By having the event router sat between both the producer and consumers, each of these two components are decoupled from each other and carry the benefits of a decoupled architecture that I discussed previously.</p>
<p>As we go through this course, I will introduce you to a number of different event-driven services, which act as the event routers, and these include Amazon SNS (the Simple Notification Service), Amazon Kinesis, and AWS Lambda.</p>
<h1 id="Introduction-to-the-Simple-Queue-Service"><a href="#Introduction-to-the-Simple-Queue-Service" class="headerlink" title="Introduction to the Simple Queue Service"></a>Introduction to the Simple Queue Service</h1><p>Hello and welcome to this lecture, which will cover the SQS Service, Simple Queue Service. With the continuing growth of microservices and the cloud best practice of designing decoupled systems, it’s imperative that developers have the ability to utilize a service, or system, that handles the delivery of messages between components. And this is where SQS comes in. SQS is a fully managed service offered by AWS that works seamlessly with serverless systems, mock services, and any distributed architecture. Although it’s simply a queueing service for messages between components, it does much more than that. It has the capability of sending, storing, and receiving these messages at scale without dropping message data, as well as utilizing different queue types depending on requirements. And it includes additional features, such as dead-letter queues. It is also possible to configure the service using the AWS Management Console, the AWS Sarli, or using the AWS SDKs. Let me focus on some of the components to allow to understand how the service is put together. The service itself uses three different elements. Two of which are a part of your distributed system, these being the producers and the consumers, and the third part is the actual queue, which is managed by SQS and is managed across a number of SQS servers for resiliency. Let me explain how these components work together.</p>
<p> The producer component of your architecture is responsible for sending messages to your queue. At this point, the SQS service stores the message across a number of SQS servers for resiliency within the specified queue. This ensures that the message remains in the queue should a failure occur with one of the SQS servers. Consumers are responsible for processing the messages within your queue. As a result, when the consumer element of your architecture is ready to process the massage from the queue, the message is retrieved and is then marked as being processed by activating the visibility timeout on the message. This timeout ensures that the same message will not be read and processed by another consumer. When the message has been processed, the consumer then deletes the message from the queue. Before moving on, I just want to point out a little more relating to the visibility timeout. As I said, when a message is retrieved by a consumer, the visibility timeout is started. The default time is 30 seconds, but it can be set up to as long as 12 hours. During this period, the consumer processes the message. If it fails to process a message, perhaps due to a communication error, the consumer will not send a delete message request back to SQS. As a result, if the visibility timeout expires and it doesn’t receive the request to delete the message, the message will become available again in the queue for other consumers to process. This message will then appear as a new message to the queue. The value of your visibility timeout should be longer than it takes for your consumers to process your messages. I mentioned earlier that there were different types of queues. </p>
<p>These being standard queues, first-in, first-out queues, and dead-letter queues. Standard queues, which are the default queue type upon configuration, support at-least-once delivery of messages. This means that the message might actually be delivered to the queue more than once, which is largely down to the highly distributive volume of SQS servers, which would make the message appear out of its original order or delivery. As a result, the standard queue will only offer a best-effort on trying to preserve the message ordering from when the message are sent by the producers. Standard queues also offer an almost unlimited number of transactions per second, TPS, making this queue highly scalable. If message ordering is critical to your solution, then standard queues might not be the right choice for you. Instead, you would need to use first-in, first-out queues. This queue is able to ensure the order of messages is maintained, and that there are no duplication of messages within the queue. Unlike standard queues, FIFO queues do have a limited number of transactions per second. These are defaulted to 300 per second for all send and receive and delete operations. If you use batching with SQS, then this changes to 3,000. Batching essentially allows you to perform actions against 10 messages at once within a single action. So, the key takeaways between the two queues are for standard queues, you have unlimited throughput, at-least-once delivery, and best-effort ordering. And for first-in, first-out queues, you have high throughput, first-in, first-out delivery, and exactly-once processing. For both queues, it is also possible to enable encryption using server-side encryption via KMS. A dead-letter queue differs to the standard and FIFA queues as this dead-letter queue is not used as a source queue to hold messages submitted by producers. Instead, the dead-letter queue is used by the source queue to send messages that fail processing for one reason or another. This could be the result of cloud enabling your application, corruption within the message, or simply missing information within a database that no message data relates to.</p>
<p>By the way, if the message can’t be processed by a consumer after a maximum number of tries specified, the queue will send the message to a dead-letter queue. This allows engineers to assess why the message failed, to identify where the issue is, to help prevent further messages from falling into the dead-letter queue. By viewing and analyzing the content of these messages, it might be possible to identify the problem and ascertain if the issue exists from the producer or consumer perspective. A couple of points to make with a dead-letter queue is that is must be configured as the same queue type as the source is used against. For example, if the source queue is a standard queue, the dead-letter queue must also be a standard queue type. And similarly, for FIFA queues, the dead-letter queue must also be configured as a FIFA queue. Before I end this lecture, I just want to show a quick demonstration on how to set up a queue and some of the configuration options available during this process. That now brings me to the end of this lecture which covered an introduction to the Simple Queue Service.</p>
<h1 id="Introduction-to-the-Simple-Notification-Service"><a href="#Introduction-to-the-Simple-Notification-Service" class="headerlink" title="Introduction to the Simple Notification Service"></a>Introduction to the Simple Notification Service</h1><p>Hello, and welcome to this lecture, covering the SNS service. It’s likely that out of the free services covered within this course, this is a service you may have come across the most, simply due to its integration with a number of other AWS services. The Simple Notification Service is used as a publish and subscribe messaging service. But what does this mean? SNS is centered around topics, and you can think of a topic as a group for collecting messages. Users or endpoints can then subscribe to this topic, and messages or events are then published to that particular topic. When a message is published, all subscribers to that topic receive a notification of that message. This helps to implement event-driven architectures within a decoupled environment.</p>
<p>Again, much like SQS, SNS is a managed service and highly scalable, allowing you to distribute messages automatically to all subscribers across your environment, including mobile devices. It can be configured with the AWS management console, the CLI, or the AWS SDK. As mentioned, SNS uses a concept of publishers and subscribers, which can also be classed as consumers and producers, and works in the same principle as SQS, from this perspective. The producers or publishers send messages to a topic, which is used as the central communication control point. Consumers or subscribers of the topic are then notified of this message by one of the following methods, HTTP, HTTPS, email, email JSON, Amazon SQS, application, AWS Lambda, or SMS.</p>
<p>Subscribers don’t just have to be users. For example, it could be a web server, and they may be notified of the message via the HTTP protocol. Or if it was a user, you could use the email notification method and enter their email address. SNS offers methods of controlling specific access to your topics through a topic policy. For example, you might want to restrict which protocols subscribers can use, such as SMS, or HTTPS, or only allow access to this topic for a specific user. The policy themselves follow the same format as IAM policies. For more information on IAM policies, please see our existing IAM course which is available within our library of content. I will now perform a demonstration, showing you how topic policies are configured and the different options within them, which allows you to apply access security to your topics. </p>
<p>So in this demonstration, I’m just gonna show you where to find your SNS policies and how to edit them, et cetera. So if we go to our SNS service, and if we create a new topic, go down to create topic here. And if we give it a topic name of Cloud Academy, and then go to create topic. From here, we can now edit the topic policy. So if we go to other topic actions, we can see here, the second option down is edit topic policy. Now there’s two views here, a basic view and an advanced view. The basic view shows the policy is very much a point and click overview, so it’s very simple to understand.</p>
<p>So this top section here allow these users to publish messages to this topic. We can say only myself, everyone or only these users, and we can enter users in this box here. I’m just gonna leave it as only me for this demonstration. And at the bottom here, you can see, allow these users to subscribe to the topic. So again, you can restrict it to only yourself, everyone or only specific AWS users. And down here, you can also specify different delivery protocols as well that I spoke about earlier. So again, for this demonstration, I’m just gonna leave it as only me. If we go to the advanced view, we can see the policy as a JSON view.</p>
<p>Now I explain that if you are familiar with IAM policies, this is laid out in exactly the same fashion. So we have the version at the top. We have a statement. We have a statement ID. And then also we have the usual parameters of effect principle, an action and resource. So if you look at this, for example, we can see that this will allow any principle with the following actions. For example, we can publish the SNS. We can delete the topic, et cetera. Now if we scroll down, we can see resource. And this resource line shows that it’s the Cloud Academy topic that I just created. Now you can edit this policy directly in here and simply click on update policy if you’re confident to do so. But like I say, it follows the same principles as IAM policies. And it’s through these policies that you can control access to your SNS topics. And that’s it.</p>
<p>Both SNS and SQS integrate with each other, which makes sense as both of these services are designed to run in a highly distributed and decoupled environment. By working together, a solution can be designed to send messages to subscribers through a push method, or SQS handles incoming messages and waits for consumers to pull data. Therefore, being able to use SNS as a producer for an SQS queue makes perfect sense from a development perspective. To do this, you’ll need to have your SQS queue subscribed to the SNS topic. And this can be achieved by performing the following steps within this demonstration.</p>
<p>So in this demonstration, I’m gonna show you how you can use SNS as a producer for an SQS queue. So from within the management console, if we go to our SQS service again, and we can find the queue that we had earlier. So if I just select that, and then go to queue actions, then you can see down here, you have subscribe queue to an SNS topic. If I select that, now I can select the topic region and the topic that I want. Now the topic that I created earlier was in the EU Ireland region, so by selecting this drop down list, I should be able to find the Cloud Academy topic. And there it is. And that gives our topic ARN. If we say subscribe, we get a confirmation message to say that this queue has successfully subscribed to the following SNS topic, which is the Cloud Academy topic. And it also set up the relevant permissions as well to allow that to happen. So if we click on okay, now we can test this out. If we go through to SNS, and if we go to our topics and Cloud Academy, and the first thing that we can see is the SQS queue is actually subscribed to this topic. So that was the action we just carried out.</p>
<p>Now I wanna test this by publishing a message to this topic to make sure that SQS receives a copy of it. So if I click on publish to topic, and I’ll just create a message here just called, Cloud Academy Test. This is a test, and click on publish message. Now that has published a message to that topic, so any subscribers should receive a copy, which means our queue should also receive a copy. So if we go back to SQS, we can see here that messages available is one. So if we go across and select our queue, and go to view&#x2F;delete messages, click on start polling for messages, we can see here straight away that we have our message that we received from the topic. And if we go to more details, we can see here the subject of Cloud Academy Test, and also the message, “This is a test.” So this proves that we have successfully used SNS as a producer for this SQS queue. Much like SQS, SNS also integrates well with AWS Lambda, a key serverless computer service. To learn more about serverless technologies, you can view our existing learning path, entitled, “<a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/serverless-computing-aws-developers-45/">Serverless Computing on AWS for Developers</a>,” which can be found <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/serverless-computing-aws-developers-45/">here</a>.</p>
<p>This integration allows SNS notifications to invoke existing Lambda functions. Like SQS, the Lambda function has to be subscribed to the topic. Then when a message is sent to the topic, the message is pushed out to the Lambda function to invoke it. The function itself uses the payload of the message as an input parameter, where it can then alter the message if required, or forward the message onto another AWS service, or indeed to another SNS topic.</p>
<p>To configure AWS Lambda to work with the topic, you can perform the following steps. From within the SNS dashboard of the AWS management console, select topics. Select the topic that you want to subscribe to with the Lambda function. Select actions and subscribe to topic. Using the protocol menu, select the AWS Lambda option. Then you must select the Lambda function to be used from the endpoint dropdown box.</p>
<p>Finally, you can select the version or alias of the function, and select the latest version of the function. Choose the latest option. Select create subscription. To gain more insight into this process and to see an example of how this can be used to create a sample message history store using SNS Lambda and Amazon DynamoDB, you can view this blog post made by AWS found here. We also have a lab which would teach you how to process SNS notifications with a Lambda function. As a simple example, the lab uses Python to log custom metrics to CloudWatch based on the message payload. That now brings me to the end of this lecture, which covered an introduction to the Simple Notification Service.</p>
<h1 id="Introduction-to-Amazon-Kinesis"><a href="#Introduction-to-Amazon-Kinesis" class="headerlink" title="Introduction to Amazon Kinesis"></a>Introduction to Amazon Kinesis</h1><p>Hello and welcome to this lecture introducing Amazon Kinesis.</p>
<p>Amazon Kinesis makes it easy to collect, process, and analyze real-time streaming data so you can get timely insights and react quickly to new information. With Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT to imagery data, and more, into your databases, your data lakes and data warehouses.</p>
<p>It enables you to process and analyze data as it arrives and responds to it in real-time, instead of having to wait until all your data is collected before the processing can begin.</p>
<p>Amazon Kinesis can continuously capture terabytes of data per hour from hundreds or thousands of sources, such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>
<p>From a storage perspective, Amazon Kinesis does not store persistent data itself, unlike many of the other Amazon big data services. As a result, Amazon Kinesis needs to be deployed as part of a larger event-driven solution.</p>
<p>Amazon Kinesis provides three different solution capabilities. Amazon Kinesis Streams: This enables you to build custom applications that process or analyze streaming data for specialized needs. This comes in 2 different variations, Kinesis Data Streams, and Kinesis Video streams. Data Streams offer a real-time data streaming service capable of elastically scaling to support hundreds of thousands of data feeds to help you build real-time solutions, such as live dashboards or identifying any security anomalies. Video Streams are designed to securely elastically scale and ingest video streams on a massive scale, connecting to millions of video streaming devices, where it can then store, and encrypt the data ready for processing by your data analytics solutions. Amazon Kinesis Data Firehose. This enables you to load streaming data into Amazon Kinesis Analytics, Amazon S3, Amazon RedShift, Amazon Elastic Search, and Splunk. Amazon Kinesis Analytics. This enables you to write standard SQL queries on streaming data.</p>
<p>Amazon Kinesis Streams is based on a platform as a service style architecture where you determine the throughput of the capacity you require and the architecture and components are automatically provisioned and stored and configured for you. You have no need or ability to change the way these architectural components are deployed.</p>
<p>An Amazon Kinesis stream is an ordered sequence of data records. A record is the unit of data in an Amazon Kinesis stream. Each record in the stream is composed of a sequence number, a partition key, and a data blob. The data blob is the data of interest that your data producer adds to a stream.</p>
<p>So what is a Producer? A producer is an entity that is continuously pushing data to Kinesis Streams, for example, a web service sending log data to a stream is a producer.</p>
<p>And then we have Consumers, now a consumer receives records from Amazon Kinesis Streams and processes them in real-time. Consumers can store their results using an AWS service, such as Amazon DynamoDB, Amazon Redshift, or Amazon S3. These consumers are known as Amazon Kinesis Streams applications and typically run on a fleet of EC2 instances. You need to build your applications using either the Amazon Kinesis API or the Amazon Kinesis Client Library.</p>
<p>Okay, let’s have a look at the architecture that underpins the Amazon Kinesis Firehose. While still under the Kinesis moniker, the Amazon Kinesis Firehouse architecture is different to that of Amazon Kinesis Streams.</p>
<p>Amazon Kinesis Firehose is a fully-managed service for delivering real-time streaming data to destinations such as Amazon S3, Amazon Redshift, Amazon Elasticsearch Service and Splunk.</p>
<p>With Kinesis Firehose, you do not need to write applications as your consumers. Instead, you configure your data producers to send data to Kinesis Firehose, where the service then automatically delivers the data to the destination that you specify. You can also configure Amazon Kinesis Firehose to transform your data before data delivery.</p>
<p>A delivery stream is the underlying entity of Kinesis Firehose. You use Kinesis Firehose by creating a Firehose delivery stream and then sending data to it, which means each delivery stream is effectively defined by the target system that receives the streamed data. Firehose can also invoke an AWS Lambda function to transform incoming data before delivering it to the selected destination. You can configure a new Lambda function using one of the Lambda blueprints AWS provides or you can choose on of your existing Lambda functions.</p>
<p>Let’s have a quick look at the difference between Amazon Kinesis Streams and Firehose. Amazon Kinesis Streams is a service for workloads that require custom processing, per incoming record, with sub-one-second processing latency, and a choice of stream processing frameworks.</p>
<p>Amazon Kinesis Firehose is a service for workloads that require zero administration, with data latency of 60 seconds or higher. You use Firehose by creating a delivery stream to a specified destination and send data to it, you do not have to create a stream or create a custom application as the destination. But Firehose is limited to S3, Redshift, and Elasticsearch and Splunk as the data destinations.</p>
<p>Amazon Kinesis Analytics is a fully managed service that enables you to quickly author SQL code that continuously reads, processes and stores data. With Amazon Kinesis Analytics, you can ingest in real-time billions of small data points. Each and every individual data point can then be aggregated to provide intelligent business insights, which in turn can be used to continually optimize and improve business processes.</p>
<p>Working with Kinesis Analytics requires you to perform three steps. You must create an input stream. Input streams typically come from streaming data sources such as Kinesis streams. Create SQL processing logic, a series of SQL statements that process input and produce output. The SQL code will typically perform aggregations and generate insights. And finally, create an output stream. Output streams can be configured to hold intermediate results that are used to feed into other queries or be used to stream out the final results. Output streams can be configured to write out to destinations such as S3, Redshift, Elasticsearch and&#x2F;or other Kinesis streams.</p>
<p>What is the benefit of using Kinesis Analytics, well, the ability to maintain peak performance of a business is often related to the ability to make timely decisions. The earlier we can make informed and actionable decisions, the quicker we can adjust and maintain optimal performance, and hence highlights the importance of being able to process data in near to real-time.</p>
<p>The type of decision making we can make is based on the age of the data itself. Considering this, we can see that data processed within real-time allows us to take preventative and&#x2F;or predictive decisions.</p>
<p>Your SQL querying statements that you author represent the most important part of your Kinesis Analytics application as they generate the actual analytics that you wish to derive. Your analytics are implemented using one or several SQL statements, used to process and manipulate input and produce output.</p>
<p>This process can involve intermediary steps, whereby the outputs of one query feed into a second in-application stream. This process can be repeated multiple times until a final desired result is achieved persisted to an output stream.</p>
<h1 id="A-Survey-of-Serverless"><a href="#A-Survey-of-Serverless" class="headerlink" title="A Survey of Serverless"></a>A Survey of Serverless</h1><p>Hello, my name is Will Meadows and I would like to welcome you to this survey of the serverless services. </p>
<p>Today we’re going to go over all the different serverless categories and services that are available within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Initially serverless was a very small domain that only offered Lambda as the sole option for someone getting into the serverless category. However, At this point in time, there are over 12 different serverless services for you to choose, each providing a unique benefit and solving a specific problem. </p>
<p>That’s why I think it’s important for someone just getting in the domain to take a survey of all of these options to get a real understanding of what it is that they want to learn about. This lecture will spend just a few minutes on each of the 12 different services to help you drive forward your learning and education. And for each service where we have a course, I will link to that lecture so that way you can continue your education on anything that you deem fit.</p>
<p>So grab a coffee, sit back and relax, and let’s take a look at all the different serverless options that AWS has available.</p>
<p>To get us started there are currently three different categories of serverless services. These categories include: serverless compute services, serverless application integration services, and serverless database services.</p>
<p>I think we should start off by looking at something you’re probably the most familiar with, which is the serverless compute services.</p>
<p>Compute is probably one of the most important things that any application will have to deal with. It is the way that the actual work gets performed. </p>
<p>Traditionally this job was completed by fleets of servers all running specific applications or code to complete their jobs. These jobs might range from running websites, big data analytics, or even dealing with long-running applications like video game servers.</p>
<p>In this category of serverless compute there are currently two different services that can complete these jobs in varying ways. Each of the options have some positives and negatives that lend themselves to some use cases over others. Let’s start off with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">AWS Lambda</a> and take a look at where it might shine.</p>
<p>Lambda was released in 2014 as the first serverless service. It offered the ability to run code, without needing to manage a server, for up to 5 minutes at a time. </p>
<p>This new paradigm of compute, the idea of functions as a service Is what really pushed forward the category of serverless as a whole.</p>
<p>Lambda is an event-driven service. This means that it needs some kind of action, or event, to trigger the code you wish to run. In many situations, the event or action is just a change of state. This change of state could be something as simple as when an image is placed into an S3 bucket.</p>
<p>These days Lambda has increased its execution time to up to 15 minutes, and has greatly increased its range of functionality from its 2014 introduction.</p>
<p>If you are just starting your journey into this space, getting a deeper understanding of lambda will really help your learning and understanding of every other serverless offering.</p>
<p>We have a course that covers AWS Lambda in-depth over here if you want to learn more: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/</a></p>
<p>This service allows you to run serverless containers on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/ecs-ec2-container-service/">Amazon ECS</a> (the elastic container service). If you are currently running a container-based application, and would like to convert to a serverless format, this is the perfect solution for you! </p>
<p>One of the coolest aspects about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/course-automatically-created-2018-10-02-113135226141/c3l4-terraform1/">Fargate</a> is that it allows you to get over the 15 minutes execution hurdle that plagues AWS lambda. You are allowed to run your Fargate tasks (containers) for an unlimited amount of time. The serverless component of AWS Fargate is in regard to where those containers actually live.</p>
<p>If you would like to learn more about ECS, Fargate, and Microservices - we have a course that covers all of that right here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/">https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/ </a></p>
<p>Application integration services are a part of the interstitial glue that helps weave everything together. They allow you to connect to and send data to other applications and AWS services. There are a number of these types of offering available nowadays, and they all fill a slightly different niche. </p>
<p>Amazon has a serverless event-based service called Amazon Eventbridge that functions as a serverless event bus. Think of an event bus as a kind of event coordinator. It allows you to intake information from both external SAAS providers, AWS services (over 90 different ones are supported), and even your own custom applications. With these input events, Amazon Eventbridge can filter, manage, and direct them to other systems that listen for them, and are able to take action based on their content. It does this filtering process with routing rules, which give you full control over your event bus.</p>
<p>If you would like to learn more about this service, please check out this course over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/">https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/</a></p>
<p>AWS Step Functions can best be described as a serverless state machine service. For those who don’t know what a state machine is, think of your standard vending machine. </p>
<p>A vending machine sits there waiting for a customer to come up to it and input money (that’s its idle state). Once money has been added into the machine, it movies onto the next state, which would be item selection. The user inputs their choice, and the machine moves into the final state of vending the product. After the workflow has been completed it returns back to the idle state, waiting for another customer.</p>
<p>AWS Step Functions allow you to create serverless workflows just like the vending machine, where you can have your system wait for inputs, make decisions, and process information based on the input variables.</p>
<p>If you would like to know more about AWS Step Functions please check out this course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-step-functions-1117/introduction/">https://cloudacademy.com/course/aws-step-functions-1117/introduction/</a></p>
<p>Amazon SQS (the simple queue service) is a messaging queue system. It can help you decouple your applications by providing a system for sending, storing, and receiving messages between multiple software components. SQS is a managed service that offers two types of queues: FIFO and Standard (which is a best-effort ordering queue, with at least once delivery).</p>
<p>Amazon SQS is a fantastically useful service that works splendidly in most serverless applications but can also find a home in many standard architectures.</p>
<p>For more on SQS please check out this course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/">https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/</a></p>
<p>This service is a Pub-Sub notification service that provides both application-to-application or application-to-person communication. This communication works well for high-throughput applications as well as many-to-many messaging between distributed systems. SNS can also act as an event-driven hub similar to Amazon Eventbridge - It’s just more bare bones.</p>
<p>For more info on Amazon SNS take a look over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/">https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/</a></p>
<p>AWS has created a fully managed service called Amazon API Gateway which helps deal with: building, publishing, monitoring, securing, and maintaining API within AWS. It works quite well at any scale, and is able to support serverless, generic web applications, and even containerized workloads on the back end. </p>
<p>You can build your APIs for public use, for private use, or even for third-party developers. The best part about it is that it is entirely serverless, and does not require you to manage any infrastructure and you pay just for what you use.</p>
<p>The service is also able to handle accepting and processing hundreds of thousands of concurrent requests. If things start to get out of hand, API Gateway is able to monitor all traffic and can throttle requests as desired.</p>
<p>Please check out this introductory course on API gateway if you are interested in learning more: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/">https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/</a></p>
<p>AWS app sync allows you to manage and synchronize data across multiple mobile devices and users. The service also allows users to modify data while offline, and having those changes be automatically synced when the device reconnects to the internet. </p>
<p>This functionality allows you to build real-time, multi-user collaborative tools and applications that work between browsers, mobile applications, and even Amazon Alexa skills. </p>
<p>AWS AppSync uses GraphQL to enable clients to fetch, change, and subscribe to data from databases, microservices, and APIs all from a single GraphQL endpoint.</p>
<p>Setting up and managing databases is a huge challenge for many organizations. Having to deal with scalability and right-sizing can take a lot of knowledge, time, and money to get set up just right. Having data storage be serverless can greatly increase your productivity as well as reduce a lot of headaches. There are a number of fantastic data storage services available serverlessly these days, so let’s take a moment to look at each of them.</p>
<p>Amazon s3 is an object-based serverless storage system that is able to handle a nearly unlimited amount of data. S3 provides great scalability, availability, and speedy performance for many different use cases. Amazon S3 is able to support files as small as zero bytes, and tops out at five terabytes. </p>
<p>Objects stored in S3 have a durability of 11 nines (99.999999999%) and so, the likelihood of losing data is extremely rare.</p>
<p>S3 has native integrations with AWS lambda, allowing you to create event-based workflows with ease. Additionally, s3 provides some of the cheapest data storage available through the use of the S3 Glacier storage class. </p>
<p>Please take a look over here at this course if you wish to learn more about Amazon S3: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/">https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/</a></p>
<p>DynamoDB is a fully managed serverless, NoSQL database that has been built to run high-performance applications at any scale. The service can operate at single-digit millisecond latency which is very valuable for time-sensitive applications that require the fastest response times. DynamoDB is a key-value store database that has no strict design schema that it needs to conform to.</p>
<p>DynamoDB is designed to be highly available. Your data is automatically replicated across three different availability zones within a geographic region. In the case of an outage or an incident affecting an entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>For more information about DynamoDB please take a look at this course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/">https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/</a></p>
<p>Amazon RDS Proxy is a fully managed, serverless, highly available database proxy for Amazon RDS. The proxy allows you to build serverless applications that are more scalable than your standard direct to RDS implementations.</p>
<p>If you are opening many new connections to your RDS databases through lambda functions or other serverless methods - you might have issues when large surges of connections are required.</p>
<p>RDS Proxy allows you to pool and share already established database connections, reducing the latency of your applications when establishing a new connection. Additionally RDS Proxy helps the availability of your serverless application by denying access to unserviceable connections that may degrade your database’s performance. </p>
<p>Aurora serverless is a fully on-demand SQL database configuration for Amazon Aurora. It automatically starts up, shuts down, and scales its capacity based on the application’s needs. It operates on a pay-per-second basis while the database is active, and can be used through a simple database endpoint. If you ever need to switch over to standard workload and leave the realm of serverless, you can do so with the click of a button.</p>
<p>Aurora is built to be highly available, fault-tolerant, and self-healing as it replicates your data 6 ways across multiple availability zones.</p>
<p>If you would like to learn more about Aurora serverless please take a look at this course over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/">https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/</a></p>
<p>AWS has developed and positioned a well-thought-out series of serverless services that can help almost any application. These days there’s pretty much an in-place stand-in for every piece of normally servered component, that can be run serverlessly.</p>
<p>I would highly recommend looking at the coursework for any of the services covered today that interested you. Each course dives directly into the service and can really help explain where it fits in an architecture and what problems it can help you solve.</p>
<p>Hopefully, you’ve enjoyed this survey of all the service services and categories within AWS. My goal was to give you just a little taste of each facet of serverless so that way you could explore and find what you want to learn about.</p>
<p>My name is Will Meadows and I’d like to thank you for spending your time here learning about Serverless. If you have any feedback, positive or negative, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated, thank you!</p>
<h1 id="4Introduction-to-the-Simple-Notification-Service"><a href="#4Introduction-to-the-Simple-Notification-Service" class="headerlink" title="4Introduction to the Simple Notification Service"></a>4<strong>Introduction to the Simple Notification Service</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/serverless-computing-aws-developers-45/">Serverless Computing on AWS for Developers</a></p>
<h1 id="6A-Survey-of-Serverless"><a href="#6A-Survey-of-Serverless" class="headerlink" title="6A Survey of Serverless"></a>6<strong>A Survey of Serverless</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">Understanding AWS Lambda to Run &amp; Scale Your Code</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/">Introduction to Microservices, Containers, and ECS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/">Connecting Application Data using Amazon EventBridge</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-step-functions-1117/introduction/">AWS Step Functions</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/">Introduction to the Simple Queue Service</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/">Introduction to the Simple Notification Service</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/">Introduction to API Gateway</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/">Storage Fundamentals for AWS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/">Working with DynamoDB</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/">Amazon Aurora High Availability</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:42" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:42-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:07:48" itemprop="dateModified" datetime="2022-11-20T19:07:48-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Networking-CLF-C01-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Networking-CLF-C01-16/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Networking-CLF-C01-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:41" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:41-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:46" itemprop="dateModified" datetime="2022-11-20T18:58:46-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Networking-CLF-C01-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Networking-CLF-C01-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Networking in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various Networking services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#x75;&#x70;&#112;&#x6f;&#x72;&#x74;&#x40;&#x63;&#108;&#111;&#117;&#100;&#x61;&#99;&#x61;&#x64;&#101;&#109;&#121;&#46;&#99;&#x6f;&#109;">&#115;&#x75;&#x70;&#112;&#x6f;&#x72;&#x74;&#x40;&#x63;&#108;&#111;&#117;&#100;&#x61;&#99;&#x61;&#x64;&#101;&#109;&#121;&#46;&#99;&#x6f;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Networking services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Networking services in AWS, including:</p>
<ul>
<li>Amazon Virtual Private Cloud, or VPC;</li>
<li>Configuring security groups and Network Access Control Lists, or NACLs;</li>
<li>AWS Virtual Private Network, or VPN solutions; and</li>
<li>AWS Direct Connect.</li>
</ul>
<p>You’ll also learn about global networking services such as Route 53: Amazon’s highly available and scalable Domain Name System, or DNS service; Amazon CloudFront: a high-performance Content Delivery Network, or CDN; and the AWS Global Accelerator, which leverages the AWS global infrastructure to reduce latency and improve the overall performance of your applications.</p>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#x75;&#112;&#112;&#111;&#114;&#116;&#x40;&#x63;&#x6c;&#x6f;&#117;&#x64;&#97;&#99;&#97;&#100;&#x65;&#x6d;&#121;&#46;&#99;&#111;&#x6d;">&#115;&#x75;&#112;&#112;&#111;&#114;&#116;&#x40;&#x63;&#x6c;&#x6f;&#117;&#x64;&#97;&#99;&#97;&#100;&#x65;&#x6d;&#121;&#46;&#99;&#111;&#x6d;</a>. Thank you!</p>
<h1 id="What-is-a-VPC"><a href="#What-is-a-VPC" class="headerlink" title="What is a VPC?"></a>What is a VPC?</h1><p>Hello and welcome and I’m going to be talking to you about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>s. Virtual Private Clouds. Now to understand what a VPC is, let’s just take a look at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> infrastructure. </p>
<p>So, here is the AWS Cloud. Very simple. And a VPC resides inside of the AWS Cloud and it’s essentially your own isolated segment of the AWS Cloud itself, so here is your VPC sitting inside the AWS Cloud. </p>
<p>Now by default when you create your VPC, the only person that has access to this is your own AWS account, just you. It is totally isolated and no one else can gain access to your VPC other than your own AWS account. Now obviously there are millions upon millions of other VPCs within the AWS network created by other customers all across the world. So, there are millions of customer VPCs. However, they do not have access to your VPC and likewise, you do not have access to their VPC. </p>
<p>Now what do you use a VPC for? Well, essentially it allows you to start deploying resources within your VPC, for example, different compute resources or storage or database and other network infrastructure among others and this allows you to start building and deploying your solutions within the Cloud. </p>
<p>Now by default from a limitation perspective, you are allowed up to five VPCs per region per AWS account and it’s very simple to create a VPC. All you need to do is to give it a name, when you create your VPC and also define an IP address range that the VPC can use and this is done in the form of a CIDR block which stands for Classless Inter-Domain Routing. And I’ll talk more about that when I talk more about subnets in a few minutes. </p>
<p>So, just to recap at a high level, simply put, a VPC is an isolated segment of the AWS public cloud that allows you to provision and deploy resources in a safe and secure manner. I now want to dive deeper into the VPC architecture and start talking about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnets</a> and how you can segment your VPC out into different areas across multiple availability zones for resiliency and high availability, so let’s take a look.</p>
<h1 id="Subnets"><a href="#Subnets" class="headerlink" title="Subnets"></a>Subnets</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/vpc-cidr-blocks/">VPC TCP&#x2F;IP Addressing</a></p>
<p><strong>Transcript</strong></p>
<p>So <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-what-vpc/">now we know what a VPC is</a>, let’s take a look at subnets. Now, subnets reside inside your VPC, and they allow you to segment your VPC infrastructure into multiple different networks. Now, you might want to do this to create better management for your resources, or to isolate certain resources from others, or even to create high-availability and resiliency within your infrastructure. So let’s take a look at the subnets. </p>
<p>Firstly, then we’ll just draw our VPC quickly. So this is our VPC. And I mentioned when talking about VPCs that when you create your VPC, there’s two pieces of information that you need. You need to give it a name and also a CIDR block address. Now, the CIDR block address is a range of IP addresses and this CIDR block range can have a subnet mask between a range of IP addresses from a &#x2F;16 all the way through to a &#x2F;28. Now, if you’re not familiar with TCP&#x2F;IP addressing, now please take a look at the link on screen and check out the following course and this will dive into the CIDR block and TCP&#x2F;IP addressing in greater detail. </p>
<p>Now, for our example, let’s say we created our VPC with the following CIDR block. 10.0.0.0&#x2F;16. Now, this is important because any subnets that we create within our VPC need to reside within this CIDR block range. So let’s take a look at a couple of subnets. </p>
<p>Now, in this section, I want to talk to you about public subnets and also private subnets. So let’s just create a public subnet there and also a private subnet here. This yellow one can be our public subnet and the green one can be our private subnet. Now, similarly, when we create a VPC, we need to give it a CIDR block range. We need to do the same with our subnets as well. So let’s say for example this is 10.0.1.0&#x2F;24. Now, this range of addresses sits within this bigger CIDR block here, and then this private subnet can be 10.0.2.0&#x2F;24. And again, this CIDR block sits within the bigger VPC CIDR block. </p>
<p>Now, what makes a subnet public and what makes a subnet private? Well, essentially a public subnet is accessible from outside of your VPC. So essentially from the Internet. For any resources created within your public subnet, for example web servers, would be accessible from the Internet. Now, because we want these web service accessible from the Internet, I have two IP addresses. So they have their own internal IP address which will be within the range of the subnet, which, for this subnet, it’s 10.0.1.0&#x2F;24. And then also we’re going to assign them a public IP address as well, because to be accessible from the Internet, the instance itself has to have a public IP address. </p>
<p>Any resources created within your private subnet, for example your backend databases, would be considered private and inaccessible by default from the Internet. So how do you make a subnet public and how do you make one private? When you create a subnet, you create them both exactly the same. It’s what you configure afterwards that will dictate if a subnet is public or private. </p>
<p>There’s two changes you need to make to your infrastructure to make a subnet public. The first is to add an Internet gateway. Now, an Internet gateway is a managed component by AWS that is attached to your VPC and acts as a gateway between your VPC and the outside world. So essentially the Internet. So let’s just add in an Internet gateway here, IGW for Internet gateway. So now we have our Internet gateway attached to our VPC. And this Internet gateway then also connects out to the Internet. So we now have a bridge between our isolated VPC to the Internet by the Internet gateway which is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. </p>
<p>Now, you might think that a public subnet now has access to the Internet because there’s an Internet gateway. However, before the public subnet can access the Internet, we need to add a route to the public subnet’s route table. Now, associated with every subnet when it’s created will also be an associated route table. Now, you can have the same route table associated to multiple subnets. That’s not a problem. However, you can’t associate more than one route table to a single subnet. </p>
<p>Now, by default, when your subnet’s created, it will have a default route in it, and this is a local route. Let’s take a look. Now, your route table will contain a destination field and also a target field. Now, the destination field is the destination address that you’re trying to get to. The target essentially specifies the route to that destination. Now, within every route table that’s created, there will be this local route here. Now, what this enables your subnets to do is simply talk to each other. So any subnet within your VPC is able to communicate with each other without you having to configure any routes. It’s there by default. Every route table has this local route. It can’t be deleted, and it simply allows all subnets within your VPC to communicate with each other. </p>
<p>So what we need to do is we need to add a route to this route table that’s associated to the public subnet. Now, this new route here that’s been added to the route table has a destination of 0.0.0.0&#x2F;0. Now, that essentially means that for any IP address that’s not known within the route table already, send it to this target. Now, this target is the Internet gateway as shown by the IGW. This part here is simply the ID of the Internet gateway. So by adding this route to the route table, this public subnet now knows exactly how to get to the Internet by going by the Internet gateway as shown in the route table. Now, those two components are essentially what makes a subnet public, the fact that we have an Internet gateway attached to the VPC and this subnet has a route pointing to the Internet gateway for any traffic that it doesn’t know how to get to. </p>
<p>Now, if we compare the route table of the private subnet, we can see that it only has this local route, so it has no route to the Internet gateway. It’s not aware that an Internet gateway even exists, so it has no route out to the public Internet. So this is considered a private subnet. Now, if we go back to the public subnet, before we added this route here, so let’s just take that out. This subnet is effectively a private subnet, because it doesn’t have a route to the Internet gateway. So with that in mind, every time you create a subnet, it is a private subnet to begin with and that is until you attach an Internet gateway to your VPC and then add this additional route. </p>
<p>So now we’ve looked at public subnets and also private subnets. Let’s now look at architecting multiple subnets across your VPC for high availability and resilience. So let’s just clear the screen, give us a blank VPC to work with. So let’s consider we have three subnets this time. We’ll have a public subnet, and we’ll have two private subnets. This can be our public subnet, and these two will be our private subnets. This will be our web layer. This will be our application layer, and this will be our database layer. </p>
<p>So in our public subnet, we will have some web servers. In our application layer, we’ll just have some EC2 instances. And in our database layer, we’ll just have some databases. So there we have our three tiers of our deployment. So as we know with any subnet, we have a local route, so each of these will all have a local, as you can’t remove that local route. And this enables all of these subnets to communicate with each other. The public subnet, as we know, will also have a route to the Internet gateway. Now, when you create a subnet, you have to create it in one of the availability zones that are available within that region. Now, if you’re not too familiar with the AWS global infrastructure, then please take a quick read of the blog post below. </p>
<p>Let’s say for example when we created this subnet, we created it in availability zone one, and we’ve done the same for the remainder of our subnets as well, and placed them all in the same availability zone. And that’s all okay, we can deploy infrastructure all within the same availability zone and our solution would be operating fine. However, should AWS have an issue with availability zone within this region, for example they might experience a flood or a fire or a natural disaster, and it took out the services to availability zone one, what would happen to our resources? </p>
<p>Well, effectively, these would also be taken down because they’re all running in availability zone one. So that’s not ideal. It’s not best practice to deploy all of your resources within the same availability zone, within a single region, simply because it doesn’t offer high-availability and resilience. So what should you do in that situation? Well, the best thing to do to ensure high-availability is to add additional subnets to allow for resiliency. So we’ll add an additional web tier and also additional application, and also a database. </p>
<p>So now we have six subnets, and again, we’d replicate our resources, so it’d have our web infrastructure here, we’ll have our application service here and our databases here. Again, then we’ll have the routes and then Internet gateway route as well. This will have a local route, and as we know, this allows communication between all subnets. So now, every subnet can talk to every other subnet with a local route, and also this also has a route to the Internet gateway as well. So now let’s look at the availability zones that we’ll deploy our infrastructure in this time. </p>
<p>Let’s say for example we’ll deploy this in AZ-1, this application subnet in AZ-2, and this database subnet in AZ-3, and similarly, down here, we have this public subnet in AZ-3. This one in one, and this final one in two. So now let’s run through the scenario again. Let’s imagine AZ-1 experienced a failure. So what would happen here? This public subnet would be out of action. This application subnet and that is it. So in this situation, we still have one subnet available in each layer of our infrastructure. So should we experience a failure with availability zone one, our services will remain up and running. </p>
<p>So let’s do the same with availability zone two. What would happen in this situation? Well, both of our public subnets would be okay, because everyone’s in AZ-1 and three. This application subnet would be down, and this database subnet would also be down. So again, at least one subnet in each of our layers is operational and available. So again, our services would still be up and running. </p>
<p>Now, finally, just for clarity, if we take down availability zone three, this web layer would go and also this database layer. So again, we still have at least one subnet in each tier or each layer of our infrastructure operational. So this is a much better design. This allows you to ensure your resources stay up and running should a failure occur in one of the availability zones. Before we move onto some security features, let me just clear the screen because I want to talk to you just quickly about IP addressing, just a couple of points that I want to mention with regards to the subnets. So let’s just clear this quickly. </p>
<p>So I mentioned that when you create your subnet, you have to assign it a CIDR block range that fits within the VPC CIDR block. So say for example we created a subnet here, and we give the subnet the address of 10.0.1.0&#x2F;24. Now, with a slash 24 mask, this gives this subnet a total of 256 IP addresses. You can only actually use 251 IP addresses. And I’ll explain why. So the very first IP address in this subnet is 10.0.1.0 and this is known as the network address. Now, you’re not able to use this as an IP address to assign to your host addresses. This is reserved for networking. Now, the next available IP address after the network address is 10.0.1.1. And this is reserved for AWS routing. So again, not a network address. You can’t use this address as a host network in your subnet. Now, the next available IP address is 10.0.1.2. And again, this one is reserved by AWS, but this time for DNS. So you cannot use this IP address. Now, the fourth IP address that you won’t be able to use in this subnet is 10.0.1.3. And this is actually reserved by AWS for future use. Now, the fifth and final address that you can’t use in an AWS subnet is the last available address in the subnet. So in this case, it’d be 10.0.1.255, and the last address in any subnet is known as the broadcast address, and again, you cannot use this for host resources. </p>
<p>So when working with TCP&#x2F;IP addresses within your subnet, first four addresses in any subnet are reserved and you cannot use for host addresses and also the very last address is reserved. So that’s why use only 251 IP addresses available to you that you can use to assign to your host resources. So now we’ve covered what a VPC is. We’ve looked at subnets, both public and private, and also how it’s best to architect your subnets across multiple availability zones for high-availability. So now let’s look at some security features. I want to start with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-network-access-control-lists-nacls/">network access control lis</a>t. Let’s take a look.</p>
<h1 id="Network-Access-Control-Lists-NACLs"><a href="#Network-Access-Control-Lists-NACLs" class="headerlink" title="Network Access Control Lists (NACLs)"></a>Network Access Control Lists (NACLs)</h1><p>Security is a key part of any deployment within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, and managing security around your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">virtual private cloud</a> is no different. So I want to talk to you about a couple of different components here. </p>
<p>Firstly, I want to talk to you about NACLs which are network access control lists. Now these are essentially virtual network-level firewalls that are associated to each and every <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnet</a>, and they help to control both ingress and egress traffic moving in and out of your VPC and between your subnets. So let’s just quickly draw out our VPC. Very simple, and let’s just draw in a public subnet for example. So this is going to be public. Up here, we’ll have our internet gateway attached to our VPC, and obviously, we have a route out to the gateway, which then communicates with the internet. </p>
<p>So what we can do here to help maintain security is to configure the network access control list associated to this subnet. Now much like route tables, whenever you create a new subnet, it will also associate a network access control list. Now by default, this NACL will allow all traffic, both inbound and outbound, so it’s not very secure, so it’s a really good practice to configure your NACLs to only allow the traffic that you want to come in and out of your subnet. </p>
<p>Now with this being a public subnet, we’ll probably have some web servers in here talking over HTTP and HTTPS, so let’s look at the inbound network access control list that could be associated to this subnet. Now as you can see, there’s a number of different fields. We have the rule number, the type, the protocol, port range, source, and allow or deny. Now the rule numbers allow you to specify what order the rules will appear inside the NACL, and as soon as traffic hits one of these rule where it matches all of the type, protocol, port range, and source, et cetera, it will carry out the action at the end, whether that is allow or deny. </p>
<p>So let’s look at the requirements required to match this rule here. The type of traffic will need to be HTTP under port 80 using the TCP protocol, and again, the port range is 80 as that’s what used for HTTP. Now the source can be any IP address, so any IP address running HTTP coming into our subnet will be allowed. So as long as they’re running this protocol, then the traffic will be allowed inbound into our public subnet. </p>
<p>Now let’s look at the second rule. Now the second rule uses HTTPS using the TCP protocol using port 443, and again, any source, and the action will be allowed. Now the last rule here, now this is a default rule that’s applied at the end of every network access control list, and that’s why it doesn’t have a rule number, and it states that all traffic using any protocol in any port range from any IP address, then deny that access. So this rule is kind of a cover rule. So basically, what that allows you to do is ensure that any traffic that doesn’t meet the rules that you’ve entered is deleted and denied access to your subnet. </p>
<p>So with this in mind, the only traffic allowed in our public subnet is essentially HTTP and HTTPS, which is exactly what we want for our web servers here, and all other traffic will be denied. So that’s the inbound NACL. Let’s now take a look at the outbound. Now the field types are all exactly the same other than this one here. This has a destination whereas on the inbound, it has the source. So on the outbound, we restrict traffic against its destination. </p>
<p>So the first rule we have here says any traffic using any protocol in any port range going to any destination, then allow that traffic. Anything else should be denied, but in this case, there won’t be anything else because this outbound rule is essentially saying send any traffic you want to using any protocol out from this subnet to any destination. </p>
<p>Now an important point to make about NACLs is that they are stateless, and this means that any response traffic generated from a request will have to be explicitly allowed and configured in either the inbound or the outbound ruleset, depending on where the response is coming from. Now again, much like route tables, you can have the same NACL applied to a number of subnets, but only a single NACL can be associated to one subnet. So network access control lists are a great way to control traffic that comes into and out of a particular subnet. </p>
<p>Let me now talk about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-security-groups/">security groups</a>, and these are another method of controlling traffic, but this time, they work at the instance level rather than the network level like NACLs do.</p>
<h1 id="Security-Groups"><a href="#Security-Groups" class="headerlink" title="Security Groups"></a>Security Groups</h1><p>So, staying with security, I now want to talk to you about security groups. Now these are similar to network access controllers where they filter traffic both inbound and outbound but whereas <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-network-access-control-lists-nacls/">NACLs</a> worked at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnet</a> level, security groups work at the instance level and I’ll explain more about this as we go. </p>
<p>So let’s say we have three subnets, okay, so we just draw these out quickly and these would be three private subnets, for example. Each of them will have their IP addresses listed, first one being 10.0.1.0&#x2F;24. .2.0. And the last one, 3.0. Now this first subnet will have EC2 instances in them. The second subnet will have RDS instances running MySQL or Aurora and the last subnet here will also have EC2 instances in them. </p>
<p>Now each of these three subnets are associated to the same network access control list. So, this one is linked with this one and also this one. And this network access control list looks like this. And this is simply saying that any traffic that is running a TCP Protocol across any port range from any source, then allow it and deny all other traffic. So, between these subnets, any TCP Protocol on any port can be used and for simplicity, the same NACL rules are being used for both inbound and outbound. Now that’s not very secure, it’s not very restrictive but from a subnet network level, that is what it’s controlling. Now what we want to do is restrict access to which instances can actually talk to our RDS and Aurora databases here. </p>
<p>Now we only want to allow access from this subnet over here and deny access from this subnet here and we can use security groups to do just that. So, let’s take a look at the security group for this subnet here, from where our databases are. Now security groups have similar fields to NACLs but there’s just a couple less. So, there’s no rule number with the security group which means all the rules within the security group will be assessed before a decision is made on the action and you’ll also notice, there’s no allow or deny either. With security groups, if there’s a rule in there, then it’s considered allowed, if there’s no rule, then all traffic is dropped by default, so with this security group is stating that any MySQL or Aurora traffic using a TCP Protocol on the port 3306 from the source 10.0.1.0 which is this subnet here, then it’s considered allowed as we don’t have another rule in this security group for the source of 10.0.3.0&#x2F;24 which is this subnet here. Then it’s considered denied. It doesn’t exist, so it’s not allowed access, so how do both these NACLs and security groups work together? </p>
<p>Well, the NACL works at the subnet level, so let’s say the NACL is this purple line and as this NACL is associated to this subnet as an example, let’s just put that NACL around the edge of the subnet like so and let’s say this orange is our security group and that security group is associated to our databases inside this subnet. So, let’s assume that our EC2 instances here are looking to communicate with the RDS and Aurora databases over here, so let’s have a look how that traffic would flow through the NACL and also the security group. </p>
<p>So, the request would be sent, it would get to the NACL and the NACL say okay, is this traffic TCP traffic within this port range from any source? And it is. So, the traffic is allowed. So, that traffic is now allowed inside the subnet. It then hits the security group and the security group says is this a MySQL or Aurora traffic running the TCP Protocol using port range 3306 coming from 10.0.1.0? And it is as we’re trying to communicate with the databases, then access is allowed. Now if we look from this subnet here, the 10.0.3.0 and do the same thing where these two EC2 instances are trying to communicate with the RDS and Aurora instances using port 3306, let’s follow the same process. </p>
<p>So, the request is sent, it hits the NACL, the NACL says are you running TCP within this port range from any source? The answer is yes, so access is allowed. It then hits a security group and it says is this traffic MySQL or Aurora using TCP Protocol on port range 3306? At this point, everything is correct, yes. However, the source is different. We don’t have a source address of 10.0.3.0. It doesn’t exist in the security group. So, at this point, the traffic is dropped at the security group and access is not allowed. </p>
<p>So, you can see how NACLs and security groups can be used to filter traffic at different layers. The NACLs are used for the subnet and network layer and the security groups are used at the instance layer. Now one final thing I wanna say about security groups is that unlike NACLs which are stateless by design, security groups are stateful which means you don’t have to configure specific rules to allow return traffic from requests like you have to do with NACLs.</p>
<h1 id="NAT-Gateway"><a href="#NAT-Gateway" class="headerlink" title="NAT Gateway"></a>NAT Gateway</h1><p><strong>Resources referneced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">AWS Shared Responsibility Model</a></p>
<p><strong>Transcript</strong></p>
<p>I now want to talk to you about another VPC component, and that is the NAT gateway. To help explain what this does, let me just draw out our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a> quickly. So we have a very simple VPC, and we’re gonna have two subnets in this VPC, we’ll have a public subnet and also we’ll have a private subnet as well, and it’s the private subnet that we’re going to be focusing on. </p>
<p>So this will be our public, and the green one will be our private subnet. Now obviously we’ll have an Internet gateway attached to our VPC, which will then connect out to the Internet. Okay, so we have a public subnet, and a private subnet. Now in our private subnet we’ll have a number of EC2 instances running our applications, and in our public subnet we’re likely to have a number of web servers as well. As we know, each of these subnets also have a route table attached. Public route table will have access to the Internet gateway, and also to the other private subnet. </p>
<p>Now we need to start thinking about security again. Now, looking at our EC2 instances in the private subnet, we are responsible, as a part of the AWS Shared Responsibility Model, to update and patch the operating systems running on each of our EC2 instances. Now if you’re not familiar with the AWS Shared Responsibility Model, I suggest you take a look at it. It’s critical to all of your AWS deployments, and it essentially defines the boundaries of security as to what your roles and responsibilities are of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-security-groups/">implementing security within the cloud</a>, and what AWS’s responsibility is of maintaining security of the cloud. For more information, you can take a look at this blog post here. </p>
<p>Okay, so with that in mind, if we have the responsibility of maintaining the operating systems of our EC2 instances, then we need to be able to download updates as and when we need to. However, this subnet is private. Meaning it has no access to the Internet gateway, and therefore the Internet, so how can we download those updates? Well, what we can do, we can add a NAT gateway. </p>
<p>Now, a NAT gateway sits within the public subnet. Because it sits within the public subnet, it has to have a public IP address in the form of an EIP which is an Elastic IP address, and this is assigned to the instance itself. Now because it sits within the public subnet, it has a route out to the Internet gateway, and to the Internet. Now once we have our NAT gateway set up and configured, we need to update the route table of our private subnet. Now, by default our route table in our private subnet will just have the local route that all route tables have. But if we update that to provide a route to the NAT gateway, and we can see that I’ve added this additional route in here. Now this looks very familiar to the route we added to the public subnet to get access to the Internet via the Internet gateway, and it is essentially the same. So we’ll add the 0.0.0.0&#x2F;0 which is essentially a destination to any IP address unknown in the route table already. Then, send it to the target of the NAT gateway. And they can tell it’s a NAT gateway as this first part here, is prefixed with nat. And then this section along here, is essentially the ID of the NAT gateway within your VPC. </p>
<p>So what this route table is telling us, is that if any resource within this subnet needs to gain access to the Internet to perform an update, then it can do so via our NAT over here. This NAT gateway will then take the request, go via the Internet gateway, and download the appropriate software that’s required, and send it back to the EC2 instance requesting it. Now the important thing with a NAT gateway, is that it will not accept any inbound communication initiated from the Internet. It will only accept outbound communications originating from within your VPC. So it will deny all inbound traffic that’s been initiated from the Internet. </p>
<p>Now the NAT gateway itself is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, so you don’t have to provision the instance itself. It’s very easy to do, you simply create the NAT gateway, specify what subnet it should reside in, and associate an Elastic IP address, and AWS will manage all other configuration. Because it’s managed by default, AWS will set up multiple NAT gateways for resiliency, but you’ll only see the one NAT gateway within your account with the associated ID. </p>
<p>Now, earlier I mentioned about configuring your resources across Multi-Availability Zones. So if you have multiple public subnets in different Availability Zones, you will need to set up another NAT gateway within that subnet as well. AWS will not automatically deploy a NAT gateway within each of your public subnets. </p>
<p>So just as a quick summary, a NAT gateway allows instances within a private subnet access to the Internet, but the NAT gateway itself will block all incoming initiations from the Internet. So it protects the private subnet in that way. And this allows you to ensure that you maintain the security of your EC2 instances ensuring that their OS is kept up to date, and any patch management is taken care of as well. Now the next component I want to talk to you about is the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-bastion-hosts/">bastion host</a>. So let’s take a look.</p>
<h1 id="Bastion-Hosts"><a href="#Bastion-Hosts" class="headerlink" title="Bastion Hosts"></a>Bastion Hosts</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/security/securely-connect-to-linux-instances-running-in-a-private-amazon-vpc/">SSH Agent Forwarding</a></p>
<p><strong>Transcript</strong></p>
<p>In this section, I want to talk to you about bastion hosts. Now, consider a scenario where you might have EC2 instances sitting in a private subnet, but you want to be able to gain access to those instances from maybe your home office or from somewhere else on the internet. But because they’re sitting in the private subnet, how can you do that? Well, one of the ways you can do this is via a bastion host. </p>
<p>So let’s draw out our VPC configuration to allow me to explain how this works. So here, we have our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>. We’re going to have a public subnet and we’ll also have a private subnet as well. So this will be our public and this green one here will be our private. Now, obviously, we have an internet gateway attached as we have a public subnet, our IGW, and this connects out to the internet. We also have routes added to allow the subnets to talk to each other and also the public subnet to route out to the internet gateway as well. </p>
<p>Now, also, in the outside world, we have an engineer. Now, this engineer might be sitting at home in their home office in front of their laptop and what they need to do is to connect to resources sitting within the private subnet over here. Now, in this private subnet, we’re going to have a couple of EC2 instances. Now, we know it’s not possible to initiate an outside request to connect through to the internet down through to the internet gateway of our VPC and then across to our private subnet. It’s not possible. There aren’t routes to enable us to do that. Access isn’t allowed and it is private by design. </p>
<p>However, this engineer here needs to gain access to the EC2 instances to perform some maintenance or updates to those resources. Now, to enable you to do this, you need to use a bastion host. Now, this bastion host sits within the public subnet and this is just another EC2 instance. Now, this instance, to follow best practices, needs to be very secure. It needs to be hardened and very robust, but effectively, it needs to be tightened down to remove any kind of vulnerabilities and loose access controls. </p>
<p>Now, this EC2 instance is a part of a security group and this security group needs to be configured as shown. Now, what this security group shows is the inbound connectivity, and it allows SSH on port 22 from this IP address, which is from the engineer’s IP. So it’s being configured for this engineer over here. So this bastion host will essentially allow an SSH connection coming from our engineer over here. Now, that’s great because this engineer can then gain access to the bastion host here. And then, what that engineer can do is then use this as like a jump server and connect from the bastion host through to our EC2 instances here. </p>
<p>But before any of that can happen, we need to set up another security group for our EC2 instances here. So we’ll have another security group around our EC2 instances and this will be configured as shown. Again, this is the inbound rule set, and we can see that SSH is allowed on port 22 from this source here. Now, this source is actually a security group. It is prefixed with sg, which is security group, and this security group is actually this one here. This is associated with the bastion host. So what this is saying is any instances associated to this security group allow inbound SSH from any resource sitting within this security group, which as we know, is associated to our bastion host. So that will just allow the bastion host SSH access to these instances. </p>
<p>So now, we have our security groups set up and configured. However, let me just talk you through the connection process. So our engineer here will connect to our bastion host. Now, the engineer will be able to access the bastion host using the private key. So let’s just follow this process through. So the engineer will SSH to our bastion host, so it’ll connect via the internet. The connection will then come through the internet gateway. Let’s assume that any net calls that we have allow the access and we come to the security group here. Now, this security group says, allow connection if it’s an SSH connection from this IP address and this is the IP address of our engineer over here. So it allows access through. So now, this engineer has access to our bastion host. But now, our engineer needs to jump across to our private instances. Now, again, we’re going to need a private key to do that. </p>
<p>Now, one method would be to store the private keys on this bastion host and then run the command to SSH and access would be allowed, but that’s not best practice at all. We really don’t want to be installing private keys within the public subnet or on the bastion host because if this bastion host ever got compromised, then the malicious user will be able to use any private keys that are stored on the bastion host and connect to our private instances, which would be very bad. So how does this engineer SSH into our EC2 instances if he doesn’t have the private key? </p>
<p>Now, the best way to do this is to set up something called SSH agent forwarding. Now, what this allows us you to do is to store the private keys for the instances within the private subnet on your local client, so that when you connect through to the bastion host, you can then SSH, but using the private key to the EC2 instances that is stored on your client rather than storing it on the bastion host. Now, with that in mind, once you have connected to your bastion host, using the example that I just showed you, you can then SSH into your private instances, at which point, it will hit the private security group that allows any SSH access on port 22 from the security group associated with the bastion host and then there, you can gain access. </p>
<p>So just to summarize exactly what we’ve done here. We started off by creating an EC2 instance within the public subnet marked as our bastion host. We then hardened that instance to try and protect it against as many security threats as possible and to lock down access to that instance. We then associated a security group that only allowed SSH inbound access from a particular IP address or a particular range of IP addresses. We then added a rule to the security group associated to our private instances that allowed SSH inbound access from the bastion host security group. You then need to ensure that SSH agent forwarding is configured on your client and then this allows you to firstly connect to your bastion host using the private key of the bastion host and then using that as the jump server to jump into your private subnet from your bastion host using the private instances, private key, which is also stored on your client PC. </p>
<p>In the next section, I’m going to be coming away from the security aspects of VPC’s list and I’m going to be focusing more on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpn-direct-connect/">VPC connectivity</a>.</p>
<h1 id="VPN-amp-Direct-Connect"><a href="#VPN-amp-Direct-Connect" class="headerlink" title="VPN &amp; Direct Connect"></a>VPN &amp; Direct Connect</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-ipsec-vpns-understanding-building-and-configuring/">Amazon VPC IPSec VPNs- Understanding, Building and Configuring</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/">AWS Virtual Private Cloud: Subnets and Routing</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this section and I’m going to be talking about VPN connections, Virtual Private Networks. Now a Virtual Private Network is essentially a secure way of connecting two remote networks across the internet. So, let’s have a look how we can use VPNs within our <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>. </p>
<p>So, if we just create a VPC over here and we also have our remote data center over here. Just with a little door and this is our data center and this is our VPC. Now within our VPC, we’re going to have a single subnet. Now this is going to be a private subnet, so there’s no internet gateway and there’s no route to the internet gateway as well. It’s totally isolated. So, a bit more information relating to IP addressing, so our VPC will have a CIDR block of 10.0.0.0&#x2F;16 and for our data center, let’s say the IP address sits on a 192.168.0.0 address space. </p>
<p>Okay, so we have our VPC over here sitting in AWS and we have our remote data center over here, maybe sitting in London somewhere. Okay, now we have resources within our data center in London and we also have some resources over in our private subnet, for example, some EC2 instances. Now what we want to do is enable communications between our resources in our private subnet in our VPC in AWS and to resources that are held on premise within our data center. Now we want to do this via a secure connection. </p>
<p>So, one option is to create a VPN connection, a Virtual Private Network. Now let’s look at some of the components involved with that. Firstly, on your VPC side, you need to create something called a virtual gateway and this attaches directly to your VPC. Much in the same way an internet gateway does to enable public subnets access out to the internet and again, this is managed by AWS. So, here we have our virtual gateway. Now over in our data center, we also need another endpoint and this will be our customer gateway. And then this could be a piece of hardware or it can be a software virtual appliance, other way it need to be host within your data center. So, now we have an endpoint at our data center, the customer gateway and we also have an endpoint attached to our VPC, the virtual private gateway. </p>
<p>Now during the creation of our virtual private gateway, we’ll need to supply some additional information that’s going to be used in our customer gateway such as the customer gateway’s IP address and the type of routing to be used whether it’s dynamic or static. Now if you’re not familiar with dynamic or static routing, then please see our existing course shown on the screen and this dives into different types of routing across subnets and across site to site as well. So, that will give you a little bit in-depth information on how the routing would work. </p>
<p>Now once your virtual private gateway is attached to your VPC and configured and also your customer gateway’s installed, then what we can do is initiate a tunnel between the two endpoints. Now, this VPN tunnel can only be initiated from your customer gateway. It can’t be initiated from your virtual gateway. Now if there was some idle activity across this link for a period of 10 seconds or more, then this VPN tunnel connection would drop. So, to prevent that from dropping, you can set up network monitoring to set up continuous network pings from the customer gateway side to the virtual gateway to ensure that connection remains up and running. </p>
<p>So, now we have our VPN tunnel up and running created between our virtual private gateway and our customer gateway, we need to change the route table associated to this private subnet, so our EC2 instances know how to connect to the 192.168 network. So, let’s take a look at that. Now we can see here that we have the local route which we have with every route table as we know but we also have this additional route here. Now the destination is 192.168.0.0&#x2F;16 which points to our data center network and the target is this virtual gateway. Now we know that’s a virtual gateway ‘cause it’s prefixed with vgw and then this is the ID of the virtual gateway itself and this relates to our virtual gateway up here. So the instances within this subnet now have an additional route that points to this virtual gateway to get to the network of the data center. </p>
<p>What you can do is also enable route propagation within your route table as well. Now what this will do is once your VPN tunnel is up and running, then any routes that are represented across your VPN connection will be automatically added to your route table, so you might have other networks within your data center other than the 192.168 that are configured to use that VPN tunnel, so any traffic from another network received by your virtual gateway will allow these routes to be automatically propagated to the route tables that you’ve enabled route propagation on. Now depending on what sort of customer gateway you installed, will depend if it supports the BGP Protocol, which is the Border Gateway Protocol and if it does, then this supports dynamic routing, so this will populate all the routes for the VPN connection for you which means you won’t have to implement any static routing. Now it is recommended that if you can install a customer gateway that does support BGP, then it’s probably best to do so. </p>
<p>Now once our routes were in place, we also need to ensure we have our security groups configured for our instances as well to allow traffic to come from my resources over here and via the customer gateway across the VPN link to our virtual private gateway and then onto our instances but as we know, they are protected by a security group, so we need to ensure that the right protocols et cetera are allowed on the inbound rule set of our security group for our resources that are based over here. So, if we wanted to allow SSH access, for example, or RDP access, then the security group would look as shown. Now we can see that this security group allows both SSH and also RDP and it’s from the source 192.168.0.0 which is of course our network that we’re using on our data center. </p>
<p>So, to quickly recap. We have our virtual private gateway attached to our VPC and we have our customer gateway installed at our remote location. We then configure it with either dynamic or static routing and here we have a static route added for our subnet that points to the virtual private gateway within our VPC to get to our destination network which is of course our destination network of the remote data center. And then we also have our security group protecting our resources within our VPC allowing only specific ports and protocols which are inbound for my remote data center network. So, that’s just a simple example of a site-to-site connection using a VPN which is a secure connection across the internet. </p>
<p>I now want to talk to you about using another site-to-site connection called Direct Connect but this does not use the internet. This is totally isolated infrastructure. So, let me explain how this works. </p>
<p>Okay, so in this section, I’m going to be talking to you about Direct Connect. Now this is another method of connecting your remote location such as your data center or remote office to your AWS environment. Now whereas your VPN connection used the internet to get to your VPC, a Direct Connect connection doesn’t traverse the internet. Instead it uses private infrastructure and connects directly to your VPC. So, there’s no public network that the traffic traverses, so let’s look at the architecture of this to see how it works and how it’s different to a VPN. </p>
<p>Now I’m not going to go into fine configuration details on this, I just want to provide you a high-level overview of how the Direct Connect infrastructure is presented, so let’s take a look. So, let’s start with our on-premise data center that we’ll just over here. This would be our data center. And within our data center, we’ll have a router. Now with a Direct Connection, there’s a middle entity before you get to AWS infrastructure, now this is usually an AWS partner or an AWS customer that holds Direct Connect infrastructure and there’s two parts to this. </p>
<p>The first part is the partner’s infrastructure or the customer’s infrastructure and the other part will be managed by AWS. So, effectively we will have a customer side and also an AWS side as well. Now this is all held within a facility owned and managed by a partner of AWS. This is a separate building entirely to your remote data center. Now again in the customer side, there will also be a router and another router in the AWS side as well. Okay, let’s move on to the final section. So, here we have our AWS region because with AWS Direct Connect, it enables you to create a connection between your data center and an AWS region, not just a VPC, it’s actually connected to a defined region. So, this will be our region here. And within that region we also have our VPC as well. So, this is our VPC and again, within our VPC we’ll have a subnet with perhaps an EC2 instance in it, for example. </p>
<p>Now the reason it’s connected to a region and not a VPC is that a Direct Connect connection allows you to access public as as private resources, so an example of a public source could be Amazon S3 and that’s because Amazon S3 resources can be accessed over the internet via a public connection. Now attached to our VPC, we’ll also have a virtual gateway much like we did when we was talking about our VPN connection. </p>
<p>Okay, let me just recap the three elements that we have here before we go any further. So, we have our customer data center over here with a router. Now here in the middle we have our Direct Connect location, so this is our Direct Connect location. And this sits between our on-premise data center and our AWS infrastructure and this is separated into two cages effectively. We have our customer partner router and we also have the AWS router as well. And then finally we have our AWS infrastructure over here with our region and inside the region, a VPC and we have our public components over here. Let’s just nest that in the AWS cloud. So, that all sits within AWS. </p>
<p>Now I mentioned previously that you can have a private connection and also a public connection and as a part of that configuration, you can configure private virtual interfaces and also public virtual interfaces on your router. So, let’s take a look. So, there’ll be two virtual interfaces. So, one of them will be a private virtual interface and we’ll define this by using this gray line here. So, that connects from your on-premise router to the customer side of the Direct Connect location. Now from here there’ll be a cross connect from the customer router to the AWS router within the same Direct Connect location. And then from here, this virtual private interface will then connect to your virtual gateway. Then this will allow connectivity through to your resources within your VPC. </p>
<p>Now the second interface is a public virtual interface, so let’s use this reddish color for that. So again the connection comes from your on-premise router into the customer side of the Direct Connect location, then there a cross connect across to the AWS router and from here it connects to inside of your AWS region and from here you can access your public AWS resources such as Amazon S3 et cetera. So, now we’ve established a connection from our on-premise data center into a region within AWS where we can access both private resources and also public resources and it’s all done without having to traverse the public internet. Instead there’s dedicated and isolated infrastructure using the Direct Connect locations. Now to be able to use Direct Connect, the only path that you need to establish is from your on-premise data center to a Direct Connect location to enable you to establish a connection to this customer router here. So, as long as you have a dedicated network route to a co-location that provides a Direct Connect connection, then you can establish this dedicated network that we can see here. </p>
<p>Now the great thing when working with Direct Connect is that it’s private connection and also you get speeds from 1 through to 10 gigabits per second. Okay, the final section I want to talk to you about in this course is relating to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpc-peering/">VPC peering</a> and also the Transit Gateway, so let’s take a look at this final section.</p>
<h1 id="VPC-Peering"><a href="#VPC-Peering" class="headerlink" title="VPC Peering"></a>VPC Peering</h1><p>In this section, I want to talk to you about VPC peering. Now we’ve looked at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpn-direct-connect/">VPN connectivity</a> which looked at connecting your on-premise data center or remote office to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a> and also Direct Connect which done the same thing but over an isolated network. Now with VPC peering, it’s relating to connectivity again but what it allows you to do is connect two VPCs together. </p>
<p>So, we have one here and another VPC here. Now each of these VPCs will have resources in them. EC2 instances or databases, et cetera and what we want to allow to happen is for these two VPCs to be able to communicate with each other. Now these VPCs might be in the same region to they might be in different regions. Either way we can allow VPC peering to allow them to communicate with each other. Now the peering connection itself here that links the two VPCs is actually run and hosted on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> infrastructure. So, this is highly resilient, there’s no single point of failure and also there’s no bottlenecked bandwidth either. So, it’s a very good way of linking two VPCs together to allow you to exchange information and for each VPC to communicate with another. </p>
<p>Now you might have multiple VPCs for organization or management and there will be times when you want resources in one VPC to communicate with another. And a quick and simple solution is to implement VPC peering. Now it’s important to mention that this peering connection is a one-to-one connection only. So, if we had a third VPC down here, call this one VPC-3, again this had additional resources in as well and you had a VPC peering connection between two and three, resources in VPC-1 could not go via VPC-1 and then through VPC-2 to get through to VPC-3. That simply is not allowed as it’s a one-to-one connection only. If you wanted VPC-1 to connect to VPC-3, then you’d have to set up a separate VPC peering connection between one and three. So, that’s a very important point when mapping out your peering connections between your VPCs. </p>
<p>Now another important point relates to IP addressing, so for example, if VPC-1 had an address of 10.0.0.0&#x2F;16, VPC-2 was 172.31.0.0&#x2F;16 and then VPC-3 was also 10.0.0.0&#x2F;16, then this connection here would not be possible because when you create VPC peering connections, each VPC cannot have an IP address overlap between them and these two VPCs have the same IP addressing scheme, so this VPC connection would not be possible. So, that’s also something else to bear in mind when creating your VPC peers. So, let’s take that connection away. </p>
<p>Now I also mentioned that you can have VPC peering configured between the same region or between different regions. So, let’s say VPC-1 and VPC-2 was in one region and VPC-3 was in another region. Then this link here would be an inter-region VPC connection. Let me now run through the process of how this peering connection is initiated. </p>
<p>So, let me just get rid of what we have on the screen here and start again. So, we have two VPCs. Our first one and also our second one. VPC-1 and VPC-2. Now VPC-1 is going to be known as the requester and VPC-2 is going to be known as the accepter. Now the owner of VPC-1 needs to send a VPC peering request to the owner of VPC-2. And again, remember, we need to make sure that the CIDR blocks of these VPCs do not overlap, so that request comes across to the VPC accepter and that’s the first stage. If the VPC accepter is happy with that peering connection, then an acknowledgement and acceptance of that request is sent back to the requester and that’s the second stage and this creates the peering connection between the two. At this stage, each VPC needs to update their routing tables to allow the traffic from VPC-1 to get to the destination of VPC-2. </p>
<p>Now to do this, we need to know the CIDR blocks of these VPCs. So, let’s assume VPC-1 is 10.0.0.0&#x2F;16 and VPC-2 is 172.31.0.0&#x2F;16. So that are two CIDR blocks that we have for our VPCs and as we know, they’re not overlapping, so from an IP perspective, there’s no issues there. So, now let’s look at the route table for each of these. So, firstly, VPC-1. As we can see, we always have our local route and then we also have this additional route here. So, the destination 172.31.0.0&#x2F;16 which is VPC-2 to go via the target of this peering connection. And the pcx simply means that this target is a peering connection. And these digits here are the ID of that peering connection. Now this VPC knows how to get to the 172.31 network by going via the peering connection here. </p>
<p>So, let’s now look at the route table for VPC-2. Again, we have our local route which every route table has and then also this additional route that points to VPC-1 again across the same peering connection. So, this VPC can now access the network of VPC-1 again via the same peering connection. </p>
<p>Now the final part of the configuration would be to modify the security groups that are hosting any resources within your VPC. So, you might have a security group here and a security group here each with EC2 instances or databases and we’ll simply need to update the rules to allow the correct resources, ports and protocols to communicate with each other. </p>
<p>So that’s a high-level overview, that is VPC peering. Now what I want to talk to you about is the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-transit-gateway/">AWS Transit Gateway</a> and again, this looks at how to connect more than one VPC together but through a one-to-many connectivity method, so let’s take a look at that.</p>
<h1 id="Transit-Gateway"><a href="#Transit-Gateway" class="headerlink" title="Transit Gateway"></a>Transit Gateway</h1><p>So, the final element I want to talk to you about is the AWS Transit Gateway. And this is essentially a development on from the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpc-peering/">VPC peering</a>. In today’s world we’re using more and more <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPCs</a> to segment and manage different workloads and as our organization gets bigger and bigger, we’re creating more and more VPCs, we have more and more connections from our remote locations such as our data centers and offices, et cetera and creating VPC peering connections to each one of these bearing in mind it’s a one-on-one connection can be very cumbersome and time consuming and just not very well to manage. </p>
<p>So, let’s say we had four VPCs represented by these circles here. And we also had a couple of remote offices as well. So, one there and one there. Now if we wanted to connect these VPCs into our office locations, now based on what we’ve already spoken about so far, we can use VPC peering to link our VPCs together. But as we know, this is just a one-on-one connection, so we also need a connection across there and also a connection across there. So, we have one, two, three, four, five, six VPC peering connections there. Now one of these remote locations might be using a VPN connection to get to that VPC, and also a VPN connection there and maybe even a third VPN connection to this VPC as well and this remote location might be used in Direct Connect to get to a couple of different VPCs in different regions. Now, that is a lot of connections and a lot of gateways to manage. We have customer gateways at the remote ends and also private gateways within our VPCs as well. </p>
<p>What AWS Transit Gateway allows you to do is to connect all of this infrastructure, so all of your VPCs, all of your remote locations, whether it’s over Direct Connect or VPN via a central hub. So, let’s take a look at how that looks. So, again we have our four VPCs and also we have our two data centers here at the bottom, our two remote locations. However, this time, we have the AWS Transit Gateway in the middle. Now, for each VPC or remote location that we want to allow to talk to each other, then all we need to do is to create a single connection to the Transit Gateway, so one from each of the VPCs and also one each from the remote locations as well. Again, these will be a VPN connection and maybe a Direct Connect connection. So, either way, VPN, Direct Connect or VPC, they all connect to this central hub, this AWS Transit Gateway. </p>
<p>As you can see between the two designs, this one over here has a lot more connections than this one over here. So, the AWS Transit Gateway simplifies your whole network connectivity. It allows all of your VPCs to easily communicate with one another and also communicate with your remote locations as well. All the routing is managed centrally within that hub and when any new remote locations or VPCs are created, for example, you might have another two VPCs created, all you’d need to do is to connect it to the AWS Transit Gateway and each of these new VPCs can then communicate with the entire rest of your infrastructure. </p>
<p>Now because the Transit Gateway goes through this central hub, it allows you to centralize all your monitoring as well for your network traffic and connectivity all through the one dashboard which is great. So, that was just a very quick high-level overview of AWS Transit Gateway and how it differs from the VPC peering. And that’s the last component I want to discuss in this course relating to VPCs and network connectivity. </p>
<p>So, in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-summary/">final lecture</a>, I’m just going to quickly review what we’ve covered throughout this course.</p>
<h1 id="Amazon-Route-53"><a href="#Amazon-Route-53" class="headerlink" title="Amazon Route 53"></a>Amazon Route 53</h1><p>Welcome to this lecture covering an overview of Amazon Route 53, which is used as the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/introduction/">DNS service</a> offered by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, but firstly, what is DNS?</p>
<p>DNS, or Domain Name System, is a hierarchical distributed naming system for computers, services or any resource connected to the internet or a private network. It associates various information with domain names, such as CloudAcademy.com or Amazon.com, and is responsible for the translation of domain names to IP addresses. A common analogy used to explain DNS, is that it is the phone book of the internet, as you can look up a human-friendly name. For example, <a target="_blank" rel="noopener" href="http://www.cloudacademy.com/">www.CloudAcademy.com</a>, and it will provide the respective IP address.</p>
<p>Now going back to Route 53, Route 53 is Amazon’s highly available and scalable domain name system that provides secure and reliable routing of requests, both for services within AWS and infrastructure that is outside of AWS. Route 53 is able to provide this service through its global network of authoritative DNS servers that reduce latency and can be managed via the management console or API.</p>
<p>To understand Route 53, let me explain some of the components and elements used within the service, starting with Hosted Zones.</p>
<p>A hosted zone is a container that holds information about how you want to route traffic for a domain such as CloudAcademy.com. Route 53 supports the following type of zones: A public hosted zone. This zone determines how traffic is routed on the internet and can be created when you register your domain with Route 53. A private hosted zone. This zone determines how traffic is routed within the Amazon VPC. If your resources are not accessible outside of the VPC you can use any domain name you wish.</p>
<p>Next, there are different domains that are supported by Route 53, and these include: generic top-level domains, known as TLDs. For example .watch which may be used for websites relating to streaming, video, or watches. Or perhaps .clothing, used by those in the fashion industry, such as retailers and department stores, as well as designers. So essentially TLDs are used to help determine what information you might expect to find on the website Then we have: Geographic domains For example .com.au (Australia), or .uk for the United Kingdom. So these are used to represent the geographical location of the site itself.</p>
<p>Next, we have resource record types, and Route 53 supports the most common types, which will meet the needs for the majority of customer DNS requirements as shown in this table. In addition to these record types, Route 53 also uses Alias records, which are a Route 53-specific extension to DNS. These Alias records which act like a CNAME record allow you to route your traffic to other AWS resources, such as Elastic load balancers, Elastic Beanstalk environments, CloudFront distributions, VPC Interface Endpoints, or S3 buckets configured as static websites.</p>
<p>Routing Policies. When you create a resource record set, you must choose a routing policy that will be applied to it, and this then determines how Route 53 will respond to these queries. The routing policies available within Route 53 include:</p>
<p>Simple routing policy: This is the default policy, and it is for single resources that perform a given function. For example, a single web server, in this case, all responses to the DNS query are based solely on the values you entered into the resource record when you created it.</p>
<p>Failover routing policy: This allows you to route traffic to different resources based upon the health of those resources. If the primary resource is healthy then traffic will be directed to that resource, if it becomes unhealthy then the routing policy will route the traffic to an alternate healthy resource. This is considered as an active-passive failover mechanism.</p>
<p>Geo-location routing policy: This lets you route traffic based on the geographic location of your users. You can define geographic routing policies based on continent, country or state in the U.S. If you have overlapping geographic regions, for example continent and country, it will direct to the smallest denominator, and in this case, that would be country. Geo-location can also be used to restrict access to your site based on location the geographic origin of the traffic. Or perhaps you want to direct all DNS queries from Europe to an elastic load balancer in the London region.</p>
<p>Geoproximity routing policy: This policy is based upon the location of both the users and your resources, whereas geo-location is based purely on the location of the users. Geoproximity allows you to set a bias against resources that can either route more or less traffic to your resource. This bias can either expand or reduce the geographic regional scope of traffic that can be routed to your resource.</p>
<p>Latency routing policy: This is suitable when you have resources in multiple regions, and you want Route 53 to respond to DNS queries with resources that provide the lowest latency for the request.</p>
<p>Multivalue answer routing policy: This allows you to get a response from a DNS request from up to 8 records at once that are picked at random, all of which will be healthy resources. This helps to factor in a level of load balancing and enhance availability.</p>
<p>Weighted routing policy: This is suitable when you have multiple resource records that perform the same function, such as a website, and you want to route traffic between them based on proportions that you specify. To determine the probability, the formula is the weight of the individual resource record divided by the sum of the total value in resource record set. For example, if you have three servers, weights are assigned two, two and six, a sum of 10. The first two are selected 20% of the time, and the last one 60% of the time.</p>
<h1 id="Amazon-CloudFront"><a href="#Amazon-CloudFront" class="headerlink" title="Amazon CloudFront"></a>Amazon CloudFront</h1><p>Hello and welcome to this lecture which will introduce you to the Amazon CloudFront service.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/amazon-cloudfront/">Amazon CloudFront</a> is AWS’s fault-tolerant and globally scalable content <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/introduction/">delivery network service</a>. It provides seamless integration with other Amazon Web Services services to provide an easy way to distribute content.</p>
<p>Amazon CloudFront speeds up distribution of your static and dynamic content through its worldwide network of edge locations. Normally, when a user requests content that you’re hosting without a CDN, the request is routed back to the source web server which could reside in a different continent to the user initiating the request. However, if you’re using CloudFront, the request is instead routed to the closest edge to the user’s location which provides the lowest latency to deliver the best performance through cached data.</p>
<p>So essentially Amazon CloudFront acts as a content delivery network service, which provides a means of distributing the source data of your web traffic closer to the end-user requesting the content via AWS edge locations as cached data. As this data is cached, after a set period, this cached data will expire and so AWS CloudFront doesn’t provide durability of your data. Instead, it distributes the source data which resides on durable storage, such as Amazon S3.</p>
<p>AWS edge locations are sites deployed in major cities and highly populated areas across the globe. While edge locations are not used to deploy your main infrastructure, such as EC2 instances or EBS storage, they are used by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services such as AWS CloudFront to cache data and reduce latency for end user access. For example, you may have your website hosted on EC2 instances or S3 within the Ohio region, with an associated CloudFront distribution. When a user accesses your website from Europe, they would then be redirected to their closest edge location in Europe, where cached data could be read off your website. This significantly reduces latency.</p>
<p>CloudFront uses distributions to control which source data it needs to redistribute and to where. These distributions can be configured as one of two different delivery methods. Firstly, a web distribution and this type of distribution is used if you want to speed up distribution of static and dynamic content, for example, .html, .css, .php, and graphics files, distribute media files using HTTP or HTTPS, add, update, or delete objects, and submit data from web forms, and use live streaming to stream an event in real-time. Alternatively, you can create an RTMP distribution, which is used if you want to distribute streaming media with the Adobe Flash media service RTMP protocol. The benefit of using RTMP distribution is that your end user can start viewing the media before the complete file has been downloaded from the edge location. The source data for an RTMP distribution can only exist within an S3 bucket and not an EC2 web server.</p>
<p>When configuring your distributions, you will be required to enter your origin information, this is essentially where the distribution is going to get the data to distribute across edge locations and it will be the DNS name of the S3 bucket or the HTTP server. If the origin is an S3 bucket, then it can be selected from a drop-down list. If you are using S3 as a static website you must enter the static hosting website endpoint.</p>
<p>If using an S3 bucket as your origin, then for additional security you can create a CloudFront user called an origin access identity, known as OAI, which can be associated with your newly created distribution. This simply ensures that only this OAI can access and serve content from your bucket and therefore preventing anyone circumventing your CloudFront distribution by accessing the files directly in the bucket using the object URL.</p>
<p>You will also be required to select a host of different caching behavior options, defining how you want the data at the edge location to be cached via various methods and policies. Lastly, you will define the distribution settings themselves, and this will look at which edge locations you want your data to be distributed to, which can either be US, Canada, and Europe, US, Canada, Europe, and Asia, or all edge locations for the best performance. You can also define if you want your distribution to be associated to a web application firewall access control list for additional security and web application protection. For more information on AWS WAF, please see the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/protecting-web-apps-aws-waf-shield-firewall-manager/introduction/">course</a>. In addition to using a web application firewall access control list, you can also implement additional encryption security by specifying an SSL certificate that must be used with a distribution.</p>
<p>Once your distribution is configured, you simply enable the distribution for it to be created. When content from your website is accessed, the end-user will be directed to their closest edge location in terms of latency, to see if the content is cached by CloudFront at that edge location. If the content is there, the user will access the content from the edge location instead of the origin, therefore reducing latency. If the content is not there, or the cache has expired for that content at the edge location, then CloudFront will request the content from the source origin again. This content will then be used to maintain a fresh cache for any future request until it again expires.</p>
<p>That now brings me to the end of this lecture and to the end of this course. If you have any feedback, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="AWS-Global-Accelerator"><a href="#AWS-Global-Accelerator" class="headerlink" title="AWS Global Accelerator"></a>AWS Global Accelerator</h1><p>Hello and welcome to this lecture covering the AWS Global Accelerator, which is a Global <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service and therefore not tied to a specific region.</p>
<p>The ultimate aim of the AWS Global Accelerator is to get UDP and TCP traffic from your end user clients to your applications faster and quicker and more reliably, through the use of the AWS global infrastructure and specified endpoints, instead of having to traverse the public internet, which is not as reliable and carries a higher security risk.</p>
<p>Global Accelerator uses two static IP addresses associated with a DNS name which is used as a fixed source to gain access to your application which could be sitting behind a load balancer, such as a network or application load balancer, or directly connected to your EC2 instance or the Elastic IP address. These IP addresses can be mapped to multiple different endpoints, each operating in a different region if a multi-region application is deployed to enhance performance of routing choices.</p>
<p>Because the routing of your request is based across the AWS Global Infrastructure, Global Accelerator intelligently routes customers requests across the most optimized path using its global reach of edge locations, for the lowest latency and avoids any resources that are unhealthy. This helps to improve regional failover and high availability across your deployment.</p>
<p>To set up and configure AWS Global Accelerator there are effectively four steps to follow.</p>
<p>Firstly, you must create your accelerator and give it a name. You must also select if you want to use two IP addresses from AWS’ pool of IP addresses or use your own. For each accelerator created, you must select two IP addresses.</p>
<p>Next, you need to create a listener. The listener is used to receive and process incoming connections based upon both the protocol and ports specified, which can either be UDP or TCP based.</p>
<p>Once your listener is created you must associate it with an endpoint group. Each endpoint group is associated with a different region, and within each group there are multiple endpoints. You can also set a traffic dial for the endpoint group, and this is essentially a percentage of how much traffic you would like to go to that endpoint group. And this helps you with blue and green deployments of your application to control the amount of traffic to specific regions. At the stage of adding your endpoint groups you can also configure health checks to allow the global accelerator to understand what should be deemed as healthy and unhealthy. </p>
<p>Finally, you must associate and register your endpoints for your application. And this can either be an application load balancer, a network load balancer, an EC2 instance or an EIP. For each endpoint, you can also assign a weight to route the percentage of traffic to that endpoint in each of your endpoint groups.</p>
<p>Let me now provide a very quick demonstration to show you how this creation looks within the AWS Console.</p>
<p>Okay so I’m logged in to my AWS Management Console and I need to go to the Global Accelerator which is under the Network and Content Delivery category. So if I select the Global Accelerator, now at the moment I don’t have any Global Accelerators configured. So from here I’ll simply click Create Accelerator.</p>
<p>Now, to start with, I need to select a name for my accelerator. So let me just call this MyAccelerator. Now here we have the IP address type, which is IPv4, and then we have the IP address pool selection. And the default is to use Amazon’s pool of IP addresses but if you want to use your own pool of addresses, then this is where you could change it. And also you can add any tags to this service if you need to.</p>
<p>So onto the next stage, this is where we add our listeners. So we can add in a port, for example, port 80. Either TCP or UDP as the protocol, and then you also have Client affinity here. And we can see that if you have state full applications, Global Accelerator can direct all requests from a user at a specific client IP address to the same endpoint resource to maintain client affinity. The default for this option is None. We don’t need that for this demonstration, so I’m just gonna leave that as None. And if you want to add any more listeners, simply click on Add Listener, and fill in the relevant details.</p>
<p>For this demonstration, I’m just gonna leave it as the one listener. Once your listeners are configured as you need to, click on Next. And here we have our endpoint groups. Here you select your regions that you want your application to reside in. So, for example, I’ll select the London region. And also we have our traffic dial, which as I explained previously, is essentially the percentage of traffic to this region. We can add additional endpoints, so we can have multiple regions if we want to. And you can keep going, add in more more regions. So let’s go and remove those two, just leave it as the one region. If you select on the configure Health checks, then you can set your health check configuration as need be just so the AWS Global Accelerator knows what it deems as healthy.</p>
<p>Once you have set your health checks, then you can select Next. On the final stage we need to add our endpoints. So here we have our endpoint group, and we select add endpoint. Now we can either add an Application Load Balancer, Network Load Balancer, an EC2 instance or an Elastic IP address. For this example, I’m gonna select an EC2 instance. And I won’t need to select the specific instance. I have one here called MyApplication. And again we have weight information, which directs the amount of traffic to each of your endpoint in your groups. I only have the single endpoint in this group, so I’m just gonna change that to 255. It can be from zero all the way to 255.</p>
<p>To add additional endpoints, simply click on Add endpoint. Select the endpoint that you’d like and then the related resource. I’m just gonna have the one endpoint in this endpoint group. And then once you’ve done that, simply click Create accelerator. And this would take a few minutes to configure itself and become active. And as you can see the status is in progress.</p>
<p>Also, as I explained earlier, we can see that this Global Accelerator has been given a DNS name, which results to this two static IP addresses. And these are the two IP addresses from the AWS pool of addresses. So that means you can add or change your endpoint groups and related endpoints in any of the regions without having to change the DNS name or the static IP addresses that it results to. So it’s very easy to change and increase region availability and high availability for your Global Accelerator. And it’s as simple as that.</p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="3Subnets"><a href="#3Subnets" class="headerlink" title="3Subnets"></a>3<strong>Subnets</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/vpc-cidr-blocks/">VPC TCP&#x2F;IP Addressing</a></p>
<h1 id="6NAT-Gateway"><a href="#6NAT-Gateway" class="headerlink" title="6NAT Gateway"></a>6<strong>NAT Gateway</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">AWS Shared Responsibility Model</a></p>
<h1 id="7Bastion-Hosts"><a href="#7Bastion-Hosts" class="headerlink" title="7Bastion Hosts"></a>7<strong>Bastion Hosts</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/security/securely-connect-to-linux-instances-running-in-a-private-amazon-vpc/">SSH Agent Forwarding</a></p>
<h1 id="8VPN-amp-Direct-Connect"><a href="#8VPN-amp-Direct-Connect" class="headerlink" title="8VPN &amp; Direct Connect"></a>8<strong>VPN &amp; Direct Connect</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-ipsec-vpns-understanding-building-and-configuring/">Amazon VPC IPSec VPNs- Understanding, Building and Configuring</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/">AWS Virtual Private Cloud: Subnets and Routing</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:39" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:39-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:10" itemprop="dateModified" datetime="2022-11-20T19:01:10-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Knowledge-Check-Databases-CLF-C01.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:38" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:38-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:08" itemprop="dateModified" datetime="2022-11-20T19:08:08-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Databases-CLF-C01-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:36" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:36-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:38" itemprop="dateModified" datetime="2022-11-20T18:58:38-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various Database services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#x75;&#x70;&#x70;&#111;&#114;&#116;&#64;&#x63;&#108;&#111;&#x75;&#x64;&#x61;&#x63;&#97;&#100;&#101;&#x6d;&#x79;&#x2e;&#99;&#111;&#109;">&#115;&#x75;&#x70;&#x70;&#111;&#114;&#116;&#64;&#x63;&#108;&#111;&#x75;&#x64;&#x61;&#x63;&#97;&#100;&#101;&#x6d;&#x79;&#x2e;&#99;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Database services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Database services in AWS, including:</p>
<ul>
<li>Managed relational databases using the Amazon Relational Database Service, or RDS;</li>
<li>Managed NoSQL databases including Amazon DynamoDB; and</li>
<li>Data warehousing using Amazon Redshift.</li>
</ul>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="The-AWS-Database-Landscape"><a href="#The-AWS-Database-Landscape" class="headerlink" title="The AWS Database Landscape"></a>The AWS Database Landscape</h1><p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> cloud has thousands of services and features. It’s easy to be amazed and overwhelmed by the number of choices available. </p>
<p>However, inside AWS, the three primary types of services used by customers are compute, storage, and databases.</p>
<p>My focus, in this course, is on the managed database offerings available from AWS and the types of workloads they support.</p>
<p>Databases are the foundation of modern application development. A database’s implementation and how data is structured will determine how well an application will perform as it scales.</p>
<p>There are two primary types of databases, relational and non-relational. </p>
<p>Relational databases, sometimes called SQL databases because they use the Structured Query Language to manage information storage and retrieval, have been commercially available since the 1970s and are optimized around data storage.</p>
<p>Sometimes the Structured Query Language is called Ess-Queue-Ell and other times it’s pronounced Sequel. Both are correct.</p>
<p>Non-relational databases, also called NoSQL databases because data is stored and retrieved primarily using methods other than SQL, became popular in the 21st Century. Over the past 20+ years, developers have created web-based applications that needed to process large amounts of unstructured and semi-structured data quickly and reliably. </p>
<p>NoSQL databases are often distributed databases where data is stored on multiple computers or nodes.</p>
<p>A database is any type of mechanism used for storing, managing, and retrieving information. It is a repository–or collection–of data. </p>
<p>Though, an application developer probably thinks of a database as a place to put the stuff their application needs to run.</p>
<p>There are nine primary categories of databases available on AWS.</p>
<ol>
<li>Relational Databases</li>
<li>Key-Value Databases</li>
<li>Document Databases</li>
<li>In-Memory Stores</li>
<li>Graph Databases</li>
<li>Columnar Databases</li>
<li>Time Series Databases</li>
<li>Quantum Ledger Databases</li>
<li>Search</li>
</ol>
<p>Each one of these database types is optimized to support a specific type of workload. </p>
<p>Matching an application with the appropriate database type is essential for highly performant and cost-efficient operation.</p>
<p>I can remember a time when choosing a database was a platform choice. The underlying technology was almost secondary. Three or four vendors would be considered, one would be chosen as a primary platform, and every application would be built using it. It was less of a database management system and more of a vendor relationship.</p>
<p>Many people had a favorite database and felt that it could be used to manage any type of workload. On a small scale, this might be true. </p>
<p>If I have a small car, I can take 2-3 kids to school every day without issue.</p>
<p>However, what happens when I need to get 20 kids to school? The small car would still work but would be terribly inefficient. It would take several trips, take a considerable amount of time, be very hard on the car, and waste resources. A better option is the school bus.</p>
<p>Thankfully, the days of choosing a single database solution for all applications are long past. </p>
<p>While it is possible to make a general-purpose database manage just about any type of workload, it will not scale as demand increases. </p>
<p>One of the reasons this is important is because the cloud has the promise of agility. That is, being able to respond to changes in a business environment as they happen. </p>
<p>This includes the ability to grow as needed and it’s called scalability.</p>
<p>Another promise of the cloud, one that is talked about but not always understood, is elasticity. That is, it’s great to be able to scale up to meet demand but, elasticity allows for the opposite to happen. When demand decreases, so does the scale and its related expenses.</p>
<p>Who wants to pay for idle resources?</p>
<p>Today, it’s important for application developers to examine the data and consider its size, shape, and computational requirements for processing and analysis. These three things determine what type of database is needed for a particular application to allow for scalability and elasticity. </p>
<p>This also means there could be multiple databases used by an application; each one serving a unique purpose. </p>
<p>The two primary workload types are operational and analytical.</p>
<p>Operational applications are the ones often referred to as Online Transactional Processing applications, OLTP. </p>
<p>OLTP applications are among the most common built today. A transaction is a record of an exchange. OLTP is centered around a set of common business processes that is regular, repeatable, and durable. </p>
<p>It could be something like e-commerce, a content management system, or information management. </p>
<p>Data goes in and reports come out. This is regular, expected work. Often, the database on the back end is relational. The data is very structured, and the results are predictable.</p>
<p>The other type of workload is the analytical application. Online Analytics Processing applications, OLAP, are run–as needed–for things like business intelligence workloads and data analysis. </p>
<p>The goal is to gain insight. Workloads are often Retrospective, Streaming, and Predictive.</p>
<p>Retrospective analytics examine an organization’s history. What happened last quarter, last year, or–maybe–the last five years. </p>
<p>Streaming workloads are gathering data, in real-time, to discover trends or raise an alarm.</p>
<p>Predictive analytics uses data to try to look into the future. There are applications that are beginning to use Machine Learning and Artificial Intelligence to improve. </p>
<p>Applications that process analytics workloads are run on-demand. It is unknown what questions they’re going to answer. The data used has little structure and the workloads, themselves, are unpredictable.</p>
<p>Databases power 21st Century applications.</p>
<p>The efficient use of resources in the cloud requires organizations to be agile. Agility implies being responsive to change. However, because costs in the cloud are primarily based on consumption, it is important to consider the requirements of scalability and elasticity.</p>
<p>Resources should grow as demand increases but they also need to shrink as that demand subsides.</p>
<p>Online Transactional Processing Applications, OLTP, are usually powered by relational databases. The data is highly structured, controlled, and predictable.</p>
<p>Online Analytics Processing Applications, OLAP, are often powered on the backend using non-relational databases. The data for these applications is either semi-structured or unstructured, the workloads are unpredictable, and the output is used to answer questions about the unknown. </p>
<p>That’s a little about what databases do, but why is structured data important? The answer to that question involves something called a schema, and you’ll learn about it in the next lecture on relational databases.</p>
<h1 id="Relational-Databases"><a href="#Relational-Databases" class="headerlink" title="Relational Databases"></a>Relational Databases</h1><p>Relational databases have been commercially available since the 1970s. They provide an efficient, intuitive, and flexible way to store and report on highly-structured data. </p>
<p>These structures, called schemas, are defined before any data can be entered into the database.</p>
<p>Schemas are designed and built based on reporting requirements. </p>
<p>This means that a database’s expected output drives the creation of the database and how data is stored inside it.</p>
<p>Once a schema has been defined, database administrators and programmers work backward from these requirements to define how data will be stored inside the database.</p>
<p>No data can be stored in a database until this work has been completed. </p>
<p>Schema changes to existing databases are expensive in terms of time and compute power.  It also has a risk of corrupting data and breaking existing reports.</p>
<p>Data in a relational database is stored in tables. Each table–sometimes called a relation–contains one or more rows of data. </p>
<p>Each row–sometimes called a record–contains a collection of logically related data that is identified by a key. </p>
<p>The pieces of data stored in a row are called attributes or fields.</p>
<p>Visually, a table looks like a spreadsheet that has rows and columns. </p>
<p>Stored in one of the columns, each table has a primary key that uniquely identifies the information stored in each row. </p>
<p>Relationships between tables are created using these keys and there are rules that govern their behavior. The primary key in one table is a foreign key in another.</p>
<p>Data integrity is of particular concern in a relational database, there are a number of constraints that ensure the data contained in tables is reliable and accurate.</p>
<p>These reliability features–commonly referred to as ACID transactions–are atomicity, consistency, isolation, and durability. </p>
<p>Atomicity refers to the elements that make up a single database transaction. A transaction could have multiple parts. It is treated as a single unit that either succeeds completely or fails completely.</p>
<p>Consistency refers to the database’s state. Transactions must take the database from one valid state to another valid state.</p>
<p>Isolation prevents one transaction from interfering with another.</p>
<p>Durability ensures that data changes become permanent once the transaction is committed to the database.</p>
<p>Data in a relational database must be kept in a known and stable state. </p>
<p>As part of the requirements to maintain database stability, Primary and Foreign Keys are constrained–they have rules that govern them–to ensure the integrity of database tables.</p>
<p>Entity Integrity ensures that, in a table, the primary key is unique to the table and it has a value. Primary keys cannot be blank or null.</p>
<p>Referential Integrity requires that every value in a Foreign Key column exists as the Primary Key of its originating table. If four tables are related and a record is deleted in one of them, then the corresponding records in related tables must be deleted as well.  </p>
<p>The standard user and application programming interface–or API–of relational databases is the Structured Query Language, SQL. </p>
<p>Pronounced as either Ess-Queue-Ell or Sequel, it can be used either interactively or programmatically to create, update, and maintain the data inside a relational database.</p>
<p>SQL is the dominant query language for relational databases. </p>
<p>SQL is an industry standard, it is interoperable between database engines and application programming languages, well-documented, and stable.</p>
<p>Security is one of the most important responsibilities of a database administrator. </p>
<p>Relational database engines have built-in features for securing and protecting data but planning and effort are required to properly implement them. </p>
<p>These features include user authentication, authorization, and audit logging.</p>
<p>As part of the structure, data stored in relational databases is highly normalized. Normalization is a process where information is organized efficiently and consistently before storing it.</p>
<p>Duplicate data is discarded.</p>
<p>Closely related fields are grouped together.</p>
<p>Data should only be stored one time in a relational database. Fields that are logically related, like a first and last name, should be stored in the same table. </p>
<p>Removing redundancy and keeping similar data close reduces storage costs and improves the efficiency of data retrieval.</p>
<p>Relational databases are not partition tolerant. A data partition, in this case, refers to the disk. </p>
<p>Adding another disk would be like creating a second copy of the database. This copy, or partition, is called a shard. </p>
<p>When a shard is created, it uses the original database’s schema. This is a horizontal partition of a database.</p>
<p>To use it, logic outside of the database must be created to direct queries to the correct database.</p>
<p>This is because relational databases are designed to validate how data is stored. They do not check to see if information belongs inside it.</p>
<p>To illustrate, here’s a weather database split into a pair of shards, Rain and Snow. They are identical except for the information stored inside them.</p>
<p>An application determines if data should be stored in Rain or if it should be stored in Snow.</p>
<p>If a record belonging in Rain ends up in Snow and it matches the database schema, it will be stored. </p>
<p>However, since that record belongs in Rain, the reports will be wrong and applications will break when trying to query data.</p>
<p>Because of this complexity, most of the time relational databases are scaled vertically. </p>
<p>Horizontal scaling adds a copy of the database server. Vertical scaling is growing the server; usually by adding memory, CPU, or expanding a disk volume.</p>
<p>Vertical scaling has limits. There are only so many resources that will fit inside a server. Once these limits have been reached, a database will either need to be redesigned or broken into shards.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> has six fully-managed database engines available inside the Relational Database Service, RDS. </p>
<p>They are Amazon Aurora, MySQL, Postgres, MariaDB, Oracle, and Microsoft SQL Server.</p>
<p>Amazon Aurora is AWS’s cloud-native version of MySQL and Postgres. </p>
<p>As a review…</p>
<p>Relational databases are highly-structured data stores.</p>
<p>The structure is called a schema.</p>
<p>The schema defines how data is stored in tables.</p>
<p>Inside tables there are rows and columns.</p>
<p>A row is a record and each column is an attribute or field of the record.</p>
<p>Tables have keys that identify data in a table.</p>
<p>A Primary Key uniquely identifies a row in a table.</p>
<p>Foreign Keys are used to connect data in a row to rows in other tables.</p>
<p>Scaling is usually done vertically by adding compute resources to an existing database.</p>
<p>Horizontal scaling is called sharding and requires logic outside of the database.</p>
<p>Relational databases are ideal for applications that do online transactional processing. </p>
<p>These OLTP applications include online banking, e-commerce sites, inventory management, human resource management, and financial services.</p>
<p>OLTP transactions usually perform specific tasks and involve a single record or a small selection of records.</p>
<p>An online banking customer might send money from a checking account to a saving account.</p>
<p>A transaction like this involves two accounts and no other customers of the bank.</p>
<p>But, what about analytical applications where hundreds, thousands, or millions of transactions need to be processed quickly, efficiently, and at a low cost? </p>
<p>That’s where non-relational databases are helpful. Though, unlike the various relational database engines that have similar needs around structured data, the size &amp; shape of the unstructured or semi-structured data determine the type of non-relational database to choose.</p>
<p>These non-relational databases are often called NoSQL databases because, when they were first developed, they used something other than SQL to store and retrieve data. </p>
<p>However, over time, SQL has been adapted to be used with some of these non-relational databases. Because of this, NoSQL can also mean “Not Only SQL.”</p>
<p>If any type of data can be stored in a relational database, why bother with a non-relational database? </p>
<p>In the next lecture, let’s learn about NoSQL Databases, what they are, and what differentiates them from relational databases.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/the-aws-database-landscape/">The AWS Database Landscape</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/nosql-databases/">NoSQL Databases</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/types-of-managed-nosql-on-aws-part-1/">Types of Managed NoSQL on AWS - Part 1</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/types-of-managed-nosql-on-aws-part-2/">Types of Managed NoSQL on AWS - Part 2</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/summary-and-conclusion/">Summary and Conclusion</a></p>
<h1 id="NoSQL-Databases"><a href="#NoSQL-Databases" class="headerlink" title="NoSQL Databases"></a>NoSQL Databases</h1><p>Relational databases are highly structured repositories of data. They use schemas to define how information is organized and that schema must exist before the database can even be created.</p>
<p>This fixed nature of data structures makes relational databases sub-optimal for analytical processes where data is semi-structured or unstructured.</p>
<p>While relational databases are highly-structured repositories of information, non-relational databases do not use a fixed table structure. They are schema-less.</p>
<p>Since it doesn’t use a predefined schema that is enforced by a database engine, a non-relational database can use structured, semi-structured, and unstructured data without difficulty.</p>
<p>NoSQL is a general term that refers to a particular type of database model. It encompasses a wide variety of different models that don’t fit into the relational model.</p>
<p>Non-relational NoSQL-type databases have been around since the 1960s, but it wasn’t until the early 2000s that the NoSQL approach started to have broad appeal and a new generation of NoSQL systems began to hit the market.</p>
<p>Today, the term NoSQL describes a family of schema-less, non-relational, distributed data stores.</p>
<p>NoSQL databases are popular with developers because they do not require an upfront schema design; they are able to build code without waiting for a database to be designed and built.</p>
<p>It’s this flexibility–a dynamic approach to organizing data–that has been popular with companies needing to store unstructured or rapidly changing data.</p>
<p>The term NoSQL has two meanings. In the beginning, it described databases that used mechanisms other than SQL to manage data. </p>
<p>There was “No SQL” used when accessing and manipulating data.</p>
<p>The definition has been expanded to mean, “Not Only SQL.” Some systems use SQL along with other technologies and query languages.</p>
<p>There are people that argue that the one thing all NoSQL databases have in common is that they’re non-relational and that a better name would be, “NoREL.” </p>
<p>Personally, I don’t think I have enough free time to care that much about it.</p>
<p>NoSQL databases, in general, share a few basic characteristics. </p>
<p>They are non-relational, open-source, schema-less, horizontally scalable, and do not adhere to ACID constraints.</p>
<p>Most NoSQL databases access data using their own Application Programming Interface, API. However, some NoSQL databases use a subset of SQL for data management.</p>
<p>In many cases, the non-relational model is a good fit for an application’s requirements. </p>
<p>The data might be unstructured or semi-structured. The amount of data might be impractical for a relational database. Or, the data might be of one single type and doesn’t need the controls that come with a relational database.</p>
<p>Being open source is not a requirement of NoSQL databases. It’s more of a NoSQL observation. There are many relational and non-relational databases that open source projects. However, the developers of NoSQL databases lean towards providing open-source solutions.</p>
<p>Most NoSQL databases have no fixed schema. </p>
<p>Relational databases require a schema to be designed before the database is created. NoSQL databases don’t. Instead, schemas can be created dynamically as data is accessed or embedded into the data itself.</p>
<p>NoSQL databases have a reputation for being more flexible with the data they can accept and support agile and DevOps philosophies.</p>
<p>NoSQL databases are often run in clusters of computing nodes.</p>
<p>Data is partitioned across multiple computers so that each computer can perform a specific task independently of the others.</p>
<p>Each node performs its task without having to share CPU, memory, or storage with other nodes.</p>
<p>This is known as a shared-nothing architecture.</p>
<p>Most NoSQL databases relax ACID constraints found in relational databases.</p>
<p>NoSQL solutions were developed around the purpose of providing high availability and scalability in a distributed environment.</p>
<p>To do this, either consistency or durability has to be sacrificed. By relaxing consistency, distributed systems can be highly available and durable. </p>
<p>Using a NoSQL approach, inconsistent data is expected. There’s no problem as long as it’s recognized and managed appropriately.</p>
<p>Currently, there is no standard query language that is supported by all NoSQL databases. </p>
<p>Some NoSQL databases have their own query language. Others use languages such as JavaScript, Java, Python, XQuery, and SPARQL.</p>
<p>NoSQL databases are a family of non-relational databases that include Key-Value Databases, Column Family Stores, Document Stores, and Graph Stores.</p>
<p>Key-Value databases are the simplest NoSQL data stores to use from an API perspective. Using a RESTful API, a client can get the value for the key, put a value for a key, or delete a key from the data store. </p>
<p>A Document Store Database is a database that uses a document-oriented model to store information. Each document contains semi-structured data that can be queried. Essentially, the schema for the data is built into the document, itself, and can change as needed. </p>
<p>Here is an example of a simple document store. It’s written in JSON, JavaScript Object Notation. What makes this different than a key-value store is that, for some of the values, there are nested key-value pairs that can be indexed and retrieved.</p>
<p>A Graph Store is a database that uses a graphical model to represent and store information. It has two primary components, Vertices and Edges.</p>
<p>Those are some of the types of NoSQL databases that are available, and I’ll eventually cover them in more detail, but why use them? What advantages do NoSQL databases have over relational databases?Scaling a NoSQL database is easier and less expensive than scaling a relational database because the scaling is horizontal instead of vertical. In general, for relational databases to scale, they must add memory, CPU, or storage. This is vertical scaling. However, NoSQL scaling is done by adding a compute or disk node. This is horizontal scaling. NoSQL databases generally trade consistency for performance and scalability.</p>
<p>Relational databases have four properties that support reliability. These properties, commonly referred to as ACID, are atomicity, consistency, isolation, and durability.</p>
<p>Consistency refers to the database’s state. In a relational database, a transaction takes a database from one valid state to another valid state. With most NoSQL databases, it’s possible for data to be inconsistent; a query might return old or stale data.</p>
<p>You might hear this phenomenon described as being eventually consistent. Over time, data that is spread across storage nodes will replicate and become consistent. What makes this behavior acceptable is that developers can anticipate this eventual consistency and allow for it. That said, some NoSQL databases do support strong consistency. </p>
<p>To review, NoSQL is a general term that refers loosely to a particular type of database model, or database management system.</p>
<p>NoSQL databases generally share a number of characteristics. They are Non-relational, databases, Open-source, Schema-less, and Horizontally Scalable.</p>
<p>Additionally, NoSQL databases do not generally adhere to the ACID principles found in relational databases and most do not use SQL to access data.</p>
<p>This is a good time to discuss the types of fully-managed NoSQL databases available from <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Or, it would be, but this is the end of this lecture.</p>
<p>In the next lecture, I’m going to describe, in some detail, the types of managed NoSQL database available on AWS. It won’t be overly technical. It’s a discussion, really, about what’s possible and how to start thinking about your data.</p>
<h1 id="Types-of-Managed-NoSQL-on-AWS-Part-1"><a href="#Types-of-Managed-NoSQL-on-AWS-Part-1" class="headerlink" title="Types of Managed NoSQL on AWS - Part 1"></a>Types of Managed NoSQL on AWS - Part 1</h1><p>Hello and welcome to this lecture about the types of managed NoSQL databases available on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Managed services are those where the provisioning and maintenance is done by AWS according to your specifications on your behalf. </p>
<p>This lecture will serve as an overview of four types of managed NoSQL database technologies available on AWS. I will cover Key-Value stores, Document stores, Column Family stores, and In-Memory stores. In the next lecture, I will continue my overview and discuss Graph databases, Time Series databases, Ledger databases, and Search databases. </p>
<p>This lecture–as well as the one that follows it– will not cover implementation details. It is mostly a discussion of what’s possible.  </p>
<p>I’m going to provide an overview of each NoSQL technology, name the service from AWS that uses it, and provide a use case.</p>
<p>Let’s get started.</p>
<p>In a relational database, data is stored in tables composed of rows and columns. These tables and the types of data they’re going to store are defined prior to application development. This allows for storage and access patterns to be optimized.</p>
<p>It also means that relational databases are relatively inflexible. </p>
<p>Key-Value Databases, also called Key-Value Stores, are often considered to be the simplest type of NoSQL database. They are typically more flexible than relational databases and offer fast performance for reads and writes. </p>
<p>The AWS managed NoSQL database that is a Key-Value store is DynamoDB.</p>
<p>Key-Value stores are designed for storing, retrieving, and managing associative arrays and are well suited for working with large amounts of data.</p>
<p>An associative array, also known as a dictionary or a hash table, stores data with a unique identifier called a key. The data stored, which could be one or more items, is the value. </p>
<p>These are simple examples of key-value pairs. </p>
<p>It is also possible to store lists as the value.</p>
<p>Key-Value Stores have no schema that defines the structure of the data. There is only the key and its associated value.  </p>
<p>The key in a key-value pair must be unique. This is a unique identifier that allows access to the value associated with that key.</p>
<p>Before using a key-value store, it helps to have a naming convention for key names. It will help keep key-value stores consistent and minimize confusion.</p>
<p>The value in a key-value store can be text, numbers, a list of items, documents, or another key-value pair.</p>
<p>In key-value stores, data is stored and retrieved using operations such as get, put, and delete. </p>
<p>Queries to Key-Value Stores are simple. Lookups are based on the key and retrieval is often measured in milliseconds regardless of the size of the data returned.</p>
<p>Key-Value Stores are not optimized for search. It’s very expensive to scan in terms of time and cost. </p>
<p>They are not suitable for applications requiring frequent updates or complex queries involving specific data values. </p>
<p>There are several types of data and access patterns that are well suited for Key-Value Stores.</p>
<p>Web applications can store user profiles, shopping cart data, and preferences in a Key-Value Store. </p>
<p>Real-time recommendation engines and advertising systems are often powered by Key-Value Stores. </p>
<p>Key-Value Stores are commonly used for in-memory data caching. They can speed up applications by minimizing reads and writes to slower disk-based systems. </p>
<p>Binary objects, such as pictures and other multimedia items–can be stored in key-value databases. However, a better solution–in terms of time and cost–is to save binary files in object storage and use a key-value database for lookups.</p>
<p>DynamoDB is a key-value database. However, since it can store key-value pairs as a value, it is also a type of NoSQL Document database.</p>
<p>Document Databases were invented to store semi-structured data. Instead of having the structure defined as part of the database in advance–like a relational database–each document in the database has its own unique schema that defines its structure.</p>
<p>The AWS managed NoSQL document database service is Amazon DocumentDB. As a document database, Amazon DocumentDB is designed to store, query, and index JSON data.</p>
<p>Document Databases are similar to Key-Value Databases in that they also have a key and a value. The difference is that, in a Document Database, the value contains structured or semi-structured data. This structured&#x2F;semi-structured value is referred to as a Document.</p>
<p>Here is an example of a document that could be stored in a document database. It is written in JSON, JavaScript Object Notation.</p>
<p>In semi-structured data, there is no separation between the schema and the data. Each document stored has its own unique schema that defines what it contains.</p>
<p>The database engine uses this structure of the stored data to create metadata that is used for database optimization and queries.</p>
<p>Consider an application to track patient records in a doctor’s office. A patient–a person–does not fit in a relational database row.  There is no schema that can be used to describe every person on earth.</p>
<p>When visiting the doctor, data is generated and entered by multiple people. There’s insurance information, billing, height, weight, blood pressure, medications, and related information.</p>
<p>Defining a person’s medical history in rows is impractical and inefficient.</p>
<p>A more efficient way is to think of patient information as a collection of documents. At every appointment, a new document is added with updated information.</p>
<p>Document Stores scale horizontally. Data can be stored over multiple nodes that can number in the 1,000s.</p>
<p>One benefit that document store databases have over key-value databases is that, in a document store, the data inside the document can be queried.</p>
<p>This is different from a Key-Value store where a query returns the value in its entirety. </p>
<p>In a document store, queries can be run against the structure of a document as well as the elements inside it to return only the information required.</p>
<p>Document Databases have a variety of use cases. They are used in web applications, for managing user-generated content, shopping catalogs, gaming, and for storing sensor data from IoT devices.</p>
<p>Where a relational database uses rows to store similar types of data, a Column Store is a type of NoSQL database that stores data using a column-oriented model. </p>
<p>On AWS, the NoSQL column store available as a managed service is Amazon Keyspaces.  </p>
<p>Using columns allows the database to precisely access data needed to answer a query without having to scan each row in a table and discard unwanted items.</p>
<p>Column Store databases are also referred to as:</p>
<ul>
<li>Column databases</li>
<li>Column-Family databases</li>
<li>Column-Oriented databases</li>
<li>Wide-Column Stores</li>
<li>Columnar databases</li>
<li>Columnar stores</li>
</ul>
<p>A column store database uses a concept called a keyspace to define the data it contains.</p>
<p>A keyspace is similar to a relational database’s schema. The keyspace contains a collection of column families that look like tables from a relational database.</p>
<p>The column families contain rows and these rows contain columns.</p>
<p>A closer look at a column family shows:</p>
<p>A Column Family consists of multiple rows.</p>
<p>Each row can contain a different number of columns.</p>
<p>Each column is limited to its row.</p>
<p>Columns are kept in their own row. They do not span all rows like a relational database does. Each column contains a name-value pair along with a timestamp.</p>
<p>Here’s how each row is constructed. From left to right there is a row key and one or more columns. </p>
<p>The row key is a unique identifier for that row.</p>
<p>Each column contains a name-value pair and a timestamp.</p>
<p>The timestamp is the date and time the data was inserted. This is often used to determine the most recent version of the data.</p>
<p>Some Column-Family databases have composite columns that allow for objects to be nested inside a column.</p>
<p>Column stores are efficient doing data compression and partitioning.</p>
<p>Due to their structure, columnar databases excel at doing aggregation-type queries. That is, they can SUM, COUNT, and calculate AVG values easily.</p>
<p>Columnar databases scale well. They are suitable for workloads that do Massively Parallel Processing where data is spread across a large cluster of compute nodes that could number in the 1,000s.</p>
<p>Columnar stores can be loaded fast and efficiently. A one-billion row table can be loaded into a columnar store in seconds with queries and analysis starting almost immediately.</p>
<p>From an end-user perspective, the metadata in a columnar database looks and feels like a relational database. Some columnar database engines are SQL compliant and support the same controls that maintain the data’s state.</p>
<p>NoSQL databases tend to be either Key-Value type stores or Document stores. Columnar Store databases are neither.</p>
<p>Columnar databases are typically used with analytical applications, data warehousing, and Big Data processing.</p>
<p>In-Memory data stores are used by applications that require real-time access to data. Since the data is stored in memory, In-Memory stores provide microsecond latency to applications.</p>
<p>These stores are used as caches and the managed NoSQL service available from AWS is Amazon ElastiCache. </p>
<p>Amazon ElastiCache has two NoSQL In-Memory database engines; Redis and Memcached.</p>
<p>Before I go too much farther, I think it’s important to explain that a caching system is not a database. It is something that sits in front of a database to improve throughput. It also removes the need for putting a caching layer inside an application.</p>
<p>The primary purpose of an in-memory key-value store is to provide inexpensive access to data with sub-millisecond latency.</p>
<p>Most data stores have areas of data that are frequently accessed but rarely updated. Querying a database and getting the results from disk is always slower and more expensive than locating a key in a key-value pair cache. </p>
<p>Some relational database queries are expensive to perform. This might be a query that requires data from multiple tables or one that does a number of calculations before returning a result. </p>
<p>By caching query results, the cost of the query is only incurred once. The data can be returned multiple times without needing to run the query again.</p>
<p>An In-Memory data store keeps its entire dataset in RAM and is not stored on disk. The reward is speed. However, there is a downside. The risk when using an In-Memory store is that when a machine goes down the data is lost. </p>
<p>Some In-Memory stores, like Redis, are able to add persistence for recovery by saving a transaction log to disk and taking snapshots of datasets stored in memory.  </p>
<p>Cached data is stale data. It is important to know, before implementing an in-memory cache, if an application can tolerate stale data and, if it can, in what context.</p>
<p>As an example, if an application displays stock prices, customers might be willing to accept staleness with a disclaimer saying prices are delayed by 5 minutes. However, a stockbroker will want real-time data.</p>
<p>Caching should provide a speed or cost advantage. It doesn’t make sense to cache data that is dynamic or that is seldom accessed. </p>
<p>For caching to provide a benefit, data should be relatively static and frequently accessed like a personal profile on a social media site. </p>
<p>An in-memory store is well-suited to be a frond-end for relational databases and key-value stores. </p>
<p>It can provide a high-performance middle-tier for applications having high request rates or low-latency requirements.</p>
<p>In-memory stores can be used to cache session data, web pages, and leaderboards.</p>
<p>This has been a high-level overview of Key-Value stores, Document stores, Column Family stores, and In-Memory stores available on AWS. In my next lecture, I’m going to continue the discussion and cover Graph databases, Time Series databases, Ledger databases, and Search databases.</p>
<h1 id="Types-of-Managed-NoSQL-on-AWS-Part-2"><a href="#Types-of-Managed-NoSQL-on-AWS-Part-2" class="headerlink" title="Types of Managed NoSQL on AWS - Part 2"></a>Types of Managed NoSQL on AWS - Part 2</h1><p>Hello and welcome to this lecture about the types of managed NoSQL databases available on AWS. Managed services are those where the provisioning and maintenance are done by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> according to your specifications on your behalf.</p>
<p>This lecture is a continuation of the previous one and will serve as an overview and discussion of Graph databases, Time Series databases, Ledger databases, and Search databases.</p>
<p>Like the previous lecture, it will not cover implementation details. It is mostly a discussion of what’s possible.  </p>
<p>I will provide an overview of each NoSQL technology, name the service from AWS that uses it, and provide a use case.</p>
<p>Let’s get started.</p>
<p>A Graph database is a database that uses a graphical model to represent and store data about relationships. Relationship data is important for things such as building social networking applications, recommendation engines, doing fraud detection, creating knowledge graphs, and modeling the life sciences.</p>
<p>The AWS managed NoSQL graph database is Amazon Neptune. </p>
<p>Graph databases are composed of three elements, vertices, edges, and properties.</p>
<p>Vertices, also called nodes, are objects such as people or artifacts. Each node in a graph database has a unique identifier expressed in key-value pairs.</p>
<p>The singular of vertices is vertex. A vertex can represent data such as integers, string, people, locations, and buildings.</p>
<p>Edges represent the connection–or relationship–between two objects. Each edge is defined by a unique identifier that provides details about a starting or ending node along with a set of properties.</p>
<p>The vertices and edges can each have properties associated with them. This allows a graph database to depict complex relationships between otherwise unrelated data.</p>
<p>Here is a simple graph database.</p>
<p>The circles are the vertices and the arrows represent edges. Each edge has a property that defines the relationship.</p>
<p>The composer node, John Williams, has relationships with a number of movies. However, he’s got two relationships with the movie Schlinder’s List because he wrote the movie’s score and won an Academy Award for his work.</p>
<p>As more data is added to the database, the schema changes to match the relationships.</p>
<p>Many graph database management systems use their own proprietary query language. Some graph database systems support access using methods such as Gremlin from Apache TinkerPop, JavaScript, JSON, XQuery, and SPARQL. </p>
<p>Depending on the graph database, they can process either transactional or analytical workloads.</p>
<p>Graph databases can process large sets of user profiles and interactions to build social networking applications.</p>
<p>Graph databases can store relationships between customer interests, friends, and purchase history to create recommendations.</p>
<p>Use graph databases to process financial and purchase transactions in near real-time to detect fraud patterns.</p>
<p>A knowledge graph stores information in a graph model and uses graph queries to enable the navigation of highly connected datasets.</p>
<p>Use a knowledge graph to add topical information to product catalogs, build and query complex models of regulatory rules, or model general information.</p>
<p>Graph databases can be used to create applications that store and navigate the life sciences.</p>
<p>Use a graph database to map a computer network and answer questions about hosts and application usage. If a malicious file is on a host, a graph database could be used to find the connection between the hosts that spread the malicious file and trace it back to the host that downloaded it.</p>
<p>Time-series Databases efficiently collect, synthesize, and derive insights from data that changes over time.</p>
<p>The AWS managed NoSQL database for time-stream data is Amazon Timestream.</p>
<p>In a Time-Series Database data is collected at regular intervals as the value and is stored with the time as the key. </p>
<p>While it’s possible to retrieve a single item from time-series data–like the price of an item–computation is usually applied over a range of time data to return a result.</p>
<p>The primary purpose of a Time-Series Database is to provide answers. A query will process a range of data, do the appropriate computations, and return the results.  </p>
<p>For example, determining the MIN, MAX, and AVG of CPU utilization on a database server over the past seven days.</p>
<p>Time-series databases are ideal for DevOps applications that collect data millions of times per second and analyze that data in real-time to improve application performance and availability. </p>
<p>Use Time-Series databases to quickly analyze time-series data generated by IoT applications using analytic functions such as smoothing, approximation, and interpolation. </p>
<p>Time stream databases can be used to store and analyze clickstream data to understand user activity across applications over a period of time. </p>
<p>Use a Time Stream database to store and analyze time-series data for industrial equipment maintenance, trade monitoring, fleet management, and route planning.</p>
<p>Ledger Databases provide a centralized and trusted authority to maintain a scalable, immutable, and cryptographically verifiable record of transactions for an application.</p>
<p>The AWS managed NoSQL ledger database is the Amazon Quantum Ledger Database.</p>
<p>These databases maintain their trust, in part, by being fully auditable and transparent. All transactions are recorded in a log to track activity.</p>
<p>QLDBs are immutable. This means that the data in the database remains unchanged once saved. Instead, the action of updating data creates a new version of the record. Changes to the database do not overwrite existing database records.</p>
<p>Cryptographic verification is used to ensure data is immutable. When a record is committed, a hash is created by the database.</p>
<p>Hashing is an algorithm performed on data to produce a number called a checksum or hash. This hash is used to verify that data has not been modified, tampered with, or corrupted. </p>
<p>No matter how many times the hashing algorithm is run against the data, the hash will always be the same when the data is the same.</p>
<p>Quantum Ledger Databases use blockchain technology when creating hashes. This means they use two pieces of information to create a hash value; the record data and the hash of the previous record. This ensures that the entire chain of records is valid.</p>
<p>Anyone can create an audit log to show how data is used, but how can they legally prove that the data has not been altered? </p>
<p>Even with the best user interfaces and audit tracking, a skilled programmer can change electronic records without leaving a trace.</p>
<p>Blockchains can be used to build trust and ensure policy, governance, and regulation of data processes. </p>
<p>Banks often need a centralized ledger-like application to keep track of critical data such as credit and debit card transactions. </p>
<p>Instead of building a custom ledger with complicated auditing functionality, a ledger database can easily store an accurate and complete record of financial transactions.</p>
<p>Manufacturing companies have a need to reconcile data between supply chain systems to track the manufacturing history of a product. </p>
<p>A ledger database can be used to record the history of each transaction and provide the details of each individual batch of a product manufactured at a facility.</p>
<p>Insurance applications need a way to track the history of claims.</p>
<p>Instead of building complex auditing functionality using relational databases, insurance companies can use a ledger database to maintain the history of claims.</p>
<p>When conflicts arise, a ledger database can cryptographically verify the integrity of the claims data.</p>
<p>HR systems have to track and maintain a record of employee details such as payroll, bonus, benefits, performance history, and insurance. </p>
<p>By implementing a system-of-record application using a ledger database, companies can easily maintain a trusted and complete record of the digital history of employees in a single place.</p>
<p>Retailers need to access information on each stage of a product’s supply chain.</p>
<p>With a ledger database, retail companies can track the full history of inventory and supply chain transactions.</p>
<p>Search engines help people find the information they need. Search databases are optimized to store and retrieve search-related data and typically offer specialized methods such as full-text search, complex search expressions, and the ranking of search results.</p>
<p>The managed NoSQL offering from AWS is the Amazon Elasticsearch Service.</p>
<p>Search databases securely ingest unstructured data from multiple locations, store and index it, and make it searchable.</p>
<p>Data ingestion is the process of taking raw data from a variety of sources, then parsing, normalizing, and enriching it.</p>
<p>The raw data sources include logs, system metrics, and web applications.</p>
<p>Once ingested, the data is indexed inside the search database. An index is a collection of documents that are related to each other. The search database, Elasticsearch, stores indexes as JSON documents. Each document has a set of keys that have corresponding values.</p>
<p>Elasticsearch uses a data structure called an inverted index that provides fast full-text searches.</p>
<p>An inverted index lists every unique word that appears in any document and identifies all of the documents where each word occurs.</p>
<p>Search databases can be used to provide a fast, personalized search experience for applications, websites, and data lake catalogs.</p>
<p>A real estate business could use a search database to help people find homes in a desired location, at a chosen price range, from among thousands of properties. </p>
<p>Search databases can be used to store, analyze, and correlate application and infrastructure log data to find and fix issues.</p>
<p>Use search databases to analyze network and systems logs for real-time threat detection and incident management. </p>
<p>Well, that covers the types of fully-managed NoSQL databases available on AWS. It was a fair amount of information. However, it should give you an idea of what’s possible in the AWS cloud.</p>
<p>In my next lecture, I’m going to do a quick summary of relational and non-relational databases, the AWS fully-managed options, and their use cases. </p>
<p>I’ll also give you some options for your next steps; depending on your needs and interests.</p>
<p>This has been a high-level overview of Graph databases, Time Series databases, Ledger databases, and Search databases. </p>
<p>NoSQL databases let developers divide complex applications into manageable pieces and create rich experiences for customers.</p>
<p>In the final lecture in this series, I’ll summarize what I’ve covered about the managed database types on AWS and give you an idea of what next steps you should take.</p>
<p>Thanks for watching.</p>
<h1 id="Amazon-Relational-Database-Service"><a href="#Amazon-Relational-Database-Service" class="headerlink" title="Amazon Relational Database Service"></a>Amazon Relational Database Service</h1><p>Hello and welcome to this lecture, where I shall be discussing the first of the AWS database services that I will be covering in this <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/course-introduction/">course</a>, the Amazon Relational Database Service, commonly known as RDS. I will look at a number of different common features of the service to give you a general idea of how it’s configured. So as the name suggests, this is a relational database service that provides a simple way to provision, create, and scale a relational database within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. It’s a managed service, which takes many of the mundane administrative operations out of your hands, and it’s instead managed by AWS, such as backups and the patching of both the underlying operating system in addition to the database engine software that you select.</p>
<p>Amazon RDS allows you to select from a range of different database engines. These currently include MySQL, and this is considered the number one open source relational database management system. MariaDB. This is the community-developed fork of MySQL. PostgreSQL. This database engine comes in a close second behind MySQL as the preferred open source database. Amazon Aurora. Amazon Aurora is AWS’s own fork of MySQL, which provides ultrafast processing and availability, as it has its own cloud-native database engine. Oracle. The Oracle database is a common platform in corporate environments. And SQL Server. This is a Microsoft database with a number of different licensing options.</p>
<p>In addition to so many different database engines, you also have a wide choice when it comes to selecting which compute instance you’d like to run your database on. The varying different options offer different performance and allowed to architect your environment based on your expected load. When you create your RDS database, you must select an instance to support your database from a processing and memory perspective, as shown here in this screenshot, using the MySQL database engine. Currently, these are the different instance types available to you for each of the database engines, which are categorized between general purpose and memory-optimized.</p>
<p>For a breakdown of the performance of each of these instance types, please refer to the following <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">AWS documentation</a>. For each of these instance types, you also have various instance sizes, each equating to a different performance level from a vCPU and memory perspective. For example, when looking at the T3 instance, we have the following sizes available.</p>
<p>You can deploy your RDS instance in a single availability zone. However, if high availability and resiliency is of importance when it comes to your database, then you might want to consider a feature known as Multi AZ, which stands for multi availability zones. When Multi AZ is configured, a secondary RDS instance is deployed within a different availability zone within the same region as the primary instance. The primary purpose of the second instance is to provide a failover option for your primary RDS instance. The replication of data between the primary RDS database and the secondary replica instance happens synchronously.</p>
<p>Let’s look at how Multi AZ would work in a production environment. If you have configured Multi AZ and an incident occurs which causes an outage to the primary RDS instance, then the RDS failover process takes over automatically. This process is managed by AWS, and it’s not something that you need to manually perform or trigger. RDS will update the DNS record to point to the secondary instance. This process can typically take between 60 and 120 seconds. The length of time is very dependent on the size of the database, its transactions, and the activity of the database at the time of failover. This automatic changeover enables you to continue using the database without the need of an engineer making any changes to your environment. The failover process will happen in the following scenarios. If patching maintenance has been performed in the primary instance, if the instance of the primary database has a host failure, if the availability zone of the primary database fails, if the primary instance was rebooted with failover, and if the primary database instance class on the primary database is modified.</p>
<p>As you can see, activating Multi AZ is an effective measure and precaution to implement to ensure you have resiliency built in should an outage occur. For detailed information on RDS Multi AZ, please refer to our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">here</a>.</p>
<p>Over time, your workloads on your database will fluctuate. And so how can you optimize your RDS database to ensure it is capable of meeting the demands of your load, both from a storage and compute perspective? When it comes to scaling your storage, you can use a feature called storage autoscaling. MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server all use Elastic Block Store, EBS volumes, for both data and log storage. However, Amazon Aurora uses a shared cluster storage architecture and does not use EBS. The database engines that use EBS support general purpose SSD storage, provisioned IOPS SSD storage, and magnetic storage.</p>
<p>The general purpose SSD storage is a good option for a broad range of use cases which provides single-digit millisecond latencies and offers a cost-effective storage solution. The minimum SSD storage volume for your primary dataset is 20 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle. However, the maximum for SQL Server is 16 tebibytes. Provisioned IOPS SSD. Now, this option is great for when you have workloads that operate at a very high I&#x2F;O. You can provision a minimum of 8,000 IOPS and a maximum of 80,000 for MySQL, PostgreSQL, MariaDB, and Oracle, but the maximum for SQL Server is 40,000.</p>
<p>In addition to being able to provision the IOPS needed for your workload, the minimum storage for your primary dataset is a 100 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle, and 16 tebibytes for SQL Server. Magnetic storage is simply supported to provide backwards compatibility. And so instead, AWS recommends that you select general purpose. The following screenshot shows the configuration screen when setting your storage requirements during the creation of a MySQL RDS database. </p>
<p>n this example, general purpose has been selected as the storage with a minimum of 100 gibibytes of primary storage. Under the storage autoscaling section, I’ve enabled the feature with the checkbox and set a maximum storage threshold of 1000 gibibytes. This means that one storage will start at 100 gibibytes when the database is created and the storage will automatically scale with demand up to a maximum of 1000 gibibytes without you having to intervene in any way. The maximum storage threshold can be set at 65,536 gibibytes. Aurora doesn’t use EBS and instead uses a shared cluster storage architecture which is managed by the service itself.</p>
<p>When configuring your Aurora database in the console, the option to configure and select storage options like we saw previously does not exist. Your storage will scale automatically as your database grows. To scale your compute size, which is effectively your instance, is easy to do in RDS both vertically and horizontally. Vertical scaling will enhance the performance of your database instance. For example, scaling up from an m4.large to an m4.2xlarge. Horizontal scaling will see an increase in the quantity of your current instance. For example, moving from a single m4.large to three m4.large instances in your environment through the means of read replicas.</p>
<p>At any point you can scale your RDS database vertically, changing the size of your instance. When doing so, you can select to perform the change immediately or wait for a scheduled maintenance window. For horizontal scaling, read replicas can be used by application and other services to save read only access to your database data via a separate instance. So, for example, let’s assume we have a primary RDS instance which serves both read and write traffic. Due to the size of the instance and the amount of read-intensive traffic being directed to the database for queries, the performance of the instance has taken a hit. So to help resolve this, you can create a read replica. </p>
<p>A snapshot will be taken of your database, and if you’re using Multi AZ, then this snapshot will be taken of your secondary database instance to ensure that there are no performance impacts during this process. Once the snapshot is complete, a read replica instance is created from this data. The read replica then maintains a secure asynchronous link between itself and the primary database. At this point, read only traffic can be directed to the read replica to serve queries. Before implementing read replicas, please check with the latest AWS documentation to identify database engine read replica compatibility.</p>
<p>As I mentioned previously, many of the administrative tasks for RDS are taken care of by AWS. For example, patching and automated backups. As Amazon RDS is a managed service, and from a shared responsibility model is considered a container service where you have no access to the underlying operating system on which your database runs on. As you can see, both platform and application management and operating systems falls under the realm of AWS responsibilities. As a result, AWS is responsible for both the patching of the operating system and any patching for the database engine themselves. More information on the AWS shared responsibility model can be found in my existing blog post found <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">here</a>.</p>
<p>From a backup perspective, by default, Amazon RDS provides an automatic feature seen here. This is enabled on all new RDS databases, which backs up your RDS instance to Amazon S3. You are able to configure the level of retention in days from zero to 35 and implement a level of encryption using the key management service, or KMS. More information on KMS can be found in our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>You can also perform manual backups anytime you need to, which are known as snapshots. However, these snapshots are not bound by the retention period set in the automatic backup configuration and are only deleted through a manual process. When using a MySQL-compatible Aurora database, you can also use a feature called backtrack, and this allows you to go back in time on the database to recover from an error or incident without having to perform a restore or create another database cluster.</p>
<p>As you can see from the configuration page during the Aurora database creation process, it is enabled via a checkbox and allows you to enter a number of hours of how far you would like to backtrack to with a maximum of 72 hours. In this example, I’ve entered 12 hours, and so Aurora will retain log data of all changes for 12 hours as specified. We’ve now covered the fundamentals of Amazon RDS and some of the key features to be aware of. If you’d like to get some hands-on experience of using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/demo-creating-amazon-rds-database-2/">Amazon RDS</a>, feel free to check out the following labs.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="DEMO-Creating-an-Amazon-RDS-Database"><a href="#DEMO-Creating-an-Amazon-RDS-Database" class="headerlink" title="DEMO: Creating an Amazon RDS Database"></a>DEMO: Creating an Amazon RDS Database</h1><p>Hello, and welcome to this lecture, which will be a demonstration on how to create an RDS database. So let’s get straight into it. So as you can see, I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console. And the first thing I need to do is to go to RDS. Now you can find RDS under the Database category, and you can see that it’s the first database option. So if you select the RDS service, and this is the dashboard for <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/amazon-relational-database-service-2/">Amazon RDS</a>. As you can see, I don’t have any database instances running. So let’s go ahead and create our first database.</p>
<p>So under the Create database section here, we can either restore an existing database from Amazon S3, from a backup, or we can create a new database. For this demonstration, I’m going to create a new database. So let’s click that option. Now, firstly, we need to choose a database creation method. We can either do a standard create or an easy create. Now the standard create allows us to set more-configurable options. So for this demonstration, that’s the option I’m going to select.</p>
<p>Then we have our database engine types. As I explained previously, we have Amazon Aurora, MySQL, MariaDB, Postgres, Oracle, and Microsoft SQL Server. I’m just going to select MySQL for this demonstration. So scrolling down again, we can then select our version of MySQL, whichever version we’d like. I’ll just leave it as the default option. And then we have something called Templates.</p>
<p>Now, depending on what template we select here will predefine a list of other configurable components. So here that’s highlighted is Production and this uses defaults for high availability and fast and consistent performance. The Dev&#x2F;Test template is intended for development use outside of a production environment. And the Free Tier is simply to allow you to get hands-on experience with RDS and doesn’t really use many of the features. But I want to show you the full feature set. So I’m going to select Production.</p>
<p>Now, if we go down to Settings. Here, we can enter a database instance identifier. So this is the name of the database instance, not the actual name of a database table. So I’m happy to call it database one, just leave that as default. Now we have our credentials. Now we have a master user name to connect to the database instance, so we can have admin. We can either auto-generate a password or we can select our own. So I’ll just go ahead and enter my own one.</p>
<p>Next, we have the option to select the database instance size. So we have our Standard classes or Memory Optimized or even Burstable classes. So I’m going to select the Standard class and using this dropdown, we can select the size of the instance that we want. And as you can see, there’s a different number of VCPUs and RAM. And so I’m just going to select the smallest instance size.</p>
<p>Now, if we go down to Storage, we can select our storage type. So we have General Purpose or Provisioned IOPS. If we look at the Provisioned IOPS, we can define the allocated storage and then also the number of IOPS as well, which is the input and output operations per second. For this demonstration, I’m just gonna leave it as General Purpose. I’ll just accept the storage defaults there of 20. Here we have Storage autoscaling. If we want to enable that or not, it’s just a simple tick box. RDS will automatically scale up to whatever value we put in here. So for example, 100 gig. That will give us the flexibility of starting at 20 and scaling all the way up to 100 automatically. </p>
<p>Now, if we go down to Availability and durability. Here, we have a Multi-AZ deployment. So it’s enabling this, will create another standby instance in a different availability zone to create high availability and data redundancy. For this demonstration, I’m just gonna leave it as a single AZ deployment. So I’m gonna select Do not create a standby instance. If we go down to Connectivity, we can select the VPC that we’d like this RDS instance to reside in. And you can see here, after a database is created, you can’t change the VPC selection. If you expand this option here for additional connectivity configuration, we can see a few additional options.</p>
<p>So we can select a database subnet group. And this simply defines which subnets and IP ranges the database instance can use in the VPC. Have an option here, if the database should be publicly accessible or not. If you select yes, then it will be issued a public IP address and devices and instances outside of your VPC will be able to try and connect to your database, if the VPC security groups allow it. For this demonstration, I’m just going to keep it a private RDS database, so it won’t assign any kind of public IP address. And only instances inside the VPC will be able to connect. Here, we can choose our security group, which will essentially define which resources can talk to our RDS instance.</p>
<p>Now, if we select an existing, then we can use this dropdown box here to select which security group that we’d like to use. I’m just going to select the default. I haven’t set any kind of security groups up for this as this is just a demonstration, but that’s where you would apply your security groups for your RDS instance. And as you can see, it’s added it in there. If you’d like to deploy your RDS instance in a particular availability zone, then you can select one. If you have no preference, simply select no preference. And also what port the database will be using.</p>
<p>If you go down to Database authentication. We have two options here for MySQL. Password authentication. Now this will allow anyone to connect to the instance just using the database passwords. If you want it more secure, then you can use the database password in addition to verifying that the user has permissions to access the RDS database, through permissions that were assigned directly to the user or group or role. So that just offers an additional level of security. If we go down further to Additional configuration, we can configure additional options.</p>
<p>So here we have our database options. You can enter an initial database name that will run on your database instance. Let’s just say my database. You can select a parameter group. Parameter groups is essentially a grouping of configurable parameters that operate at the database engine level. You’re able to create different parameter groups that contain different settings for the same database engine, depending on your use case and how you’d like these parameters to be configured. Now the parameter group itself sits outside of the database, and this means that the same parameter group can be applied to multiple databases. So if you update the values within the parameter group, then this will update all the databases that use that same parameter group. Depending on which database engine you select, you are able to select an option group. And these option groups allow you to configure additional features to help you manage and secure your databases. Again, like parameter groups, they sit outside of the database itself.</p>
<p>Here we have our Backup section, so we can enable our automatic backups. If you don’t want this, you can simply un-tick it, but it’s pretty useful, so I tend to leave that enabled. And here we have our backup retention period. And you can select the number of days, up to 35 days. Just leave that at seven. We have a backup window. We can select a window, select in the time and the duration. So if you have a particular time that you’d like to run your backups, you can simply add in the hours and minutes, and also how long it should run for. If you don’t have a specific window, you can simply select no preference. If you have any tags for your database, you can copy that to your backup snapshots as well.</p>
<p>When it comes to encryption, you can either encrypt your database. The default is to have your database encrypted, and then you can select your key here. Now, this is the default AWS managed key for RDS, which is used by KMS, the key management service. or you can select your own CMK, your own customer master key, if you have a different one yourself. I’m just gonna leave it as the default AWS RDS managed key.</p>
<p>Down here, we have performance insights. Performance insights allows you to implement a level of performance tuning and monitoring, which enables you to see and review the load on your database, and if any actions should be taken. Here we can make some additional monitoring changes. We can enable enhanced monitoring, and we can set the granularity of this monitoring from anything from 60 seconds to one second. I’ll leave that as a default of 60 seconds for enhanced monitoring. And I’m just going to leave RDS to create the default role for that enhanced monitoring.</p>
<p>We can export our logs to Amazon CloudWatch Logs. Either the error, the general or the slow query log or all of them, or any combination. So if you want to export any of your logs to CloudWatch Logs for additional monitoring and queries then you can do so.</p>
<p>Then we have maintenance. We can enable or disable auto minor version upgrade. And here we can see that by enabling auto minor version upgrade, it will automatically upgrade to new minor versions as they are released. And the automatic upgrades occur during the maintenance window that we’ve scheduled for the database.</p>
<p>Now, speaking of maintenance window, we can select one here. So we can select a window. We can say on a particular day, that’s good for us, Saturday at four o’clock in the morning for two hours, for example, that could be our maintenance window. So if there’s any auto minor version upgrades to take place, then these will be scheduled during our maintenance window. And then finally you have deletion protection. And this simply prevents a database from being deleted accidentally.</p>
<p>Now at the very bottom, it has your estimated monthly costs. So we can see the cost of the database instance and also for the storage. Once you’re happy with all your options, simply click Create database. And now we can see that here’s our database instance, and we can see the status is Creating. And we have a message up here saying that the database might take a few minutes to launch. Okay, we now have a message that says the database has successfully been created. And it’s as simple as that.</p>
<h1 id="Amazon-DynamoDB"><a href="#Amazon-DynamoDB" class="headerlink" title="Amazon DynamoDB"></a>Amazon DynamoDB</h1><p>Hello and welcome to this lecture covering Amazon DynamoDB. Amazon DynamoDB is a NoSQL database, which means that it doesn’t use the common Structured Query Language, SQL. It falls into a category of databases known as key-value stores. A key value store is simply a collection of items or records, and you can look up data by using a primary key for each item or through the use of indexes.</p>
<p>Amazon DynamoDB is designed to be used for ultra high performance, which could be maintained at any scale with single-digit latency, making this a very powerful database choice used commonly for gaming, web, mobile and IoT applications to name but a few. Much like Amazon RDS, DynamoDB is also a fully managed service, taking many of the day-to-day administration operations out of your hands, giving you more time to focus on the business logic of your database. That’s one of the great things about Amazon DynamoDB, there’s no database administration required by us as a customer, no service to manage and nothing to back up. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> handles all of this for you. This makes the creation of a DynamoDB database very easy. All you have to do is set up your tables and configure the level of provision throughput that each table should have. Provision throughput refers to the level of read and write capacity that you want AWS to reserve for your table. You are charged for the total amount of throughput that you configure for your tables plus the total amount of storage space used by your data.</p>
<p>If we actually look at the configuration screen when creating a new DynamoDB database, as seen here, you can see that there are very few options required to create a new database. And in fact, in its simplest form, you can just provide a table name and a primary key, which is used to partition data across hosts for scalability and availability. You can then accept any remaining defaults and create your database, it’s as simple as that. DynamoDB tables are considered schemaless because there’s no strict design and schema that every record must conform to. As long as each item has an appropriate primary key, the item can contain varying sets of attributes. The records in a table do not need to have the same attributes or even the same number of attributes. This can be very convenient for rapid application development and if you want to add a new column to a table, you don’t need to alter the table, you can just start including the new field as an attribute when you insert new records. Likewise, you never need to adjust the data type for a column as DynamoDB generally isn’t interested in data types for individual attributes.</p>
<p>If when creating your DynamoDB database, you choose not to reset all the defaults, what other options exist? Let’s take a look. Unchecking the use default settings from the Table settings section provides you with the following. Firstly, you’ll be asked about secondary indexes, which allow you to perform queries on attributes that are not part of the table’s primary key. The default option provides no secondary index. However, you can add them here if required. DynamoDB lets you create additional indexes so that you can run queries to search your data by other attributes. If you’ve worked with relational databases, you’ve probably used indexes with those, but there are a couple of big differences in how indexes operate in DynamoDB.</p>
<p>First, each query can only use one index. If you want to query and match on two different columns, you need to create an index that can do that properly. Second, when you write your queries, you need to specify exactly which index should be used for each query. It’s not like a relational database that has a query analyzer, which can decide which indexes to use for our query. Here you need to be explicit and tell DynamoDB what index to use. DynamoDB has two different kinds of secondary indexes, global indexes let you query across the entire table to find any record that matches a particular value and by contrast, local secondary indexes can only help find data within a single partition key.</p>
<p>Following secondary indexes, you can modify the default settings applied to your table’s read&#x2F;write capacity mode. When you create a table in DynamoDB, you need to tell AWS how much capacity you want to reserve for the table. You don’t need to do this for disk space as DynamoDB will automatically allocate more space for your table as it grows. However, you do need to reserve capacity for input and output for reads and writes. Amazon charges you based on the number of read capacity units and write capacity units that you allocate. It’s important to allocate enough for your workload, but don’t allocate too much or DynamoDB could become prohibitively expensive.</p>
<p>By default, when you create a table in the AWS Console, Amazon will configure your table with five read capacity units and five write capacity units. There are two modes that you can choose from, provisioned and on-demand. Provisioned mode allows you to provision set read and writes allowed against your database per second by your application and is measured in capacity units, RCUs for reads and WCUs for writes. Depending on the transaction, each action will use one or more RCUs or WCUs. Provisioned mode is used generally when you have a predicted and forecasted workload of traffic. On-demand mode does not provision any RCUs or WCUs, instead they are scaled on demand. The downside is that it is not as cost effective as provisioned. This mode is generally used if you do not know how much workload you are expected to experience. Over time, you are likely to get more of an understanding of load and you can change your mode across to provisioned.</p>
<p>Once you have selected the provisioned mode, you will then have the opportunity to add configuration information relating to how your RCU and WCU are scaled as demand increases and decreases. As you can see, by entering your minimum and maximum provisioned capacity along with your target threshold utilization as a percentage, you can confidently rely on Amazon DynamoDB to manage the scaling operations of your throughput.</p>
<p>The last main point of the configuration allows you to set encryption of your tables, which is enabled by default for data at rest. Through the use of the key management service, KMS, you are able to select either a customer managed or AWS managed CMK to use for the encryption of your table instead of the default keys used by DynamoDB. For more information on CMKs and the key management service in general, please refer to our existing course found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Before I finish this lecture covering DynamoDB, I just want to cover some of its advantages and also what can be considered disadvantages. Some of the advantages of DynamoDB is that it’s fully managed by AWS, you don’t have to worry about backups or redundancy, although you’re welcome to set up these kinds of safeguards using some more advanced DynamoDB features.</p>
<p>As mentioned previously, DynamoDB tables are schemaless so you don’t have to define the exact data model in advance, the data model can change automatically to fit your application’s needs.</p>
<p>DynamoDB is designed to be highly available and your data is automatically replicated across three different availability zones within a geographic region. In the case of an outage or an incident affecting the entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>DynamoDB is designed to be fast, read and writes take just a few milliseconds to complete and DynamoDB will be fast no matter how large your table grows, unlike relational database, which can slow down as the table gets large. DynamoDB performance is constant and stays consistent even with tables that are many terabytes in size. You don’t have to do anything to handle this, except adjusting the provisioned throughput levels to make sure you’ve preserved enough read and write capacity for your transaction volume.</p>
<p>There are also some downsides to using DynamoDB too. As I just mentioned, your data is automatically replicated. Three copies are stored in three different availability zones and that replication usually happens quickly in milliseconds, but sometimes it can take longer and this is known as eventual consistency. This happens transparently and many operations will make sure that they’re always working on the latest copy of your data, but there are certain kinds of queries and table scans that may return older versions of data before the most recent copy. You need to be aware of how this works and you may need to adjust certain queries to require strong consistency.</p>
<p>DynamoDB’s queries aren’t as flexible as what you can do with SQL. If you are used to writing advanced queries with joins and groupings and summaries, you won’t be able to do that with DynamoDB. You’ll have to do more of the computation in your application code. This is done for performance reasons to ensure that every query finishes quickly and that complicated queries can’t hog the resources on a database server. </p>
<p>DynamoDB also has some strict limitations in the way you’re allowed to work with it. Two important limitations are the maximum record size of 400 kilobytes and the limit of 20 global indexes and five secondary indexes per table. There are other limitations that can be adjusted by contacting AWS customer support like the maximum number of tables in an AWS account.</p>
<p>Finally, although <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/demo-creating-dynamodb-database-2/">DynamoDB</a> performance can scale up as your needs grow, your performance is limited to the amount of read and write throughput that you’ve provisioned for each table. If you expect a spike of the database use, you’ll need to provision more throughput in advance or database requests will fail with a ProvisionedThroughputExceededException message. Fortunately, you can adjust throughput at any time and it only takes a couple of minutes to adjust. Still, this means that you’ll need to monitor the throughput being used in each table or you’ll risk running out of throughput if your usage grows.</p>
<h1 id="DEMO-Creating-a-DynamoDB-Database"><a href="#DEMO-Creating-a-DynamoDB-Database" class="headerlink" title="DEMO: Creating a DynamoDB Database"></a>DEMO: Creating a DynamoDB Database</h1><p>Hello, and welcome to this lecture. This is going to be a demonstration on how to quickly, and easily create a DynamoDB database. Now, first I’ll need to go to the database category, and here we can see <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/amazon-dynamodb-2/">DynamoDB</a>. Now I don’t have any DynamoDB databases sets up yet. So if you select create table, and you’ll be presented with this screen.</p>
<p>So first we’ll need to give it a table name, just call this my database and also a primary key. Now the primary key is essentially used to uniquely identify each item in the table. And the primary key is essentially comprised of a partition key. So let me just add one in. I’ll just call this product ID, and we can slate either a string, binary, or a number. I’ll leave that as a string. If we need to, we can also add in a sort key as well. And as we can see here the sort key simply allows you to search within a partition. Just remove that sort key.</p>
<p>Now essentially you can now create your table simply from providing that information, because this tick box here allows you to use lots of default settings that essentially fills in the rest of the configuration for you. So if you’re happy with your table name and primary key, with these default settings for your table, you can simply click create and it’s done. However, I want to uncheck the default settings so you can see the different configurable components used. So let’s take a look.</p>
<p>Now, firstly we have our secondary indexes, so you can add a secondary index, and these allow you to perform queries on attributes that are not part of the table’s primary key. Next, we have our read and write capacity mode, either provisioned or on demand. If we select the provisioned capacity mode, then we can select our read capacity units, and also our write capacity units.</p>
<p>Now scrolling down a bit further to auto scaling. We can set up auto scaling for our read and write capacity units. So when the read capacity gets to 70% utilization, we can scale up to a maximum of 40,000 units, and the same with the write capacity. So you can alter these figures if you need to, and change them to whatever values you need. As a part of that auto scaling process, DynamoDB needs an auto-scaling service link role to give it permission to do so.</p>
<p>Once you’re happy with your read and write capacity units, we can then scroll down to encryption at rest. Now by default encryption is enabled. The default option uses a key that’s owned by DynamoDB, and you are not charged for the use of any encryption keys in this default setting. However, you can use a KMS custom managed CMK, which is the CMK that you may have created, and you can select it from this box here, if you have any and enter the ARN, or you can use the KMS <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> managed CMK, which is this key here.</p>
<p>So it depends on the level of control that you want for the encryption key. First demonstration I’m just gonna leave it as the default. At the bottom here, you can add any text to a database if you’d like. So once you’re happy with the configuration, you simply click on create. As we can see the table is being created. And once it’s created, you can then use these tabs along the top to set up any alarms and review your capacity units set up your indexes, backups, etc, etc, etc. But for this demonstration, I simply wanted to show you how quickly and easy it is to set up and configure a DynamoDB table.</p>
<h1 id="Amazon-Redshift"><a href="#Amazon-Redshift" class="headerlink" title="Amazon Redshift"></a>Amazon Redshift</h1><p>Hello, and welcome to this lecture where I will look at Amazon Redshift. Amazon Redshift is a fast, fully-managed, petabyte-scale data warehouse. And it’s designed for high performance and analysis of information capable of storing and processing petabytes of data and provide access to this data, using your existing business intelligence tools, using standard SQL. It operates as a relational database management system, and therefore is compatible with other RDBMS applications. Redshift itself is based upon PostgreSQL 8.0.2, but it contains a number of differences from PostgreSQL. These differences are out of scope for this course, but for more information, please refer to the documentation <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">here</a>.</p>
<p>A data warehouse is used to consolidate data from multiple sources to allow you to run business intelligent tools, across your data, to help you identify actionable business information, which can then be used to direct and drive your organization to make effective data-driven decisions to the benefit of your company.</p>
<p>As a result, using a data warehouse is a very effective way to manage your reporting and data analysis at scale. A data warehouse, by its very nature, needs to be able to store huge amounts of data and its data may be subjected to different data operations such as data cleansing, which as an example, may identify, correct, replace or remove incomplete records from a table or recordset.</p>
<p>This can be expanded upon for the need to perform an extract, transform and load or an ETL job. This is the common paradigm by which data from multiple systems is combined to a single database data store or warehouse for legacy storage or analytics.</p>
<p>Extraction is the process of retrieving data from one or more sources. Either online, brick &amp; mortar, legacy data, Salesforce data and many others. After retrieving the data, ETL is to compute work that loads it into a staging area and prepares it for the next phase.</p>
<p>Transformation is the process of mapping, reformatting, conforming, adding meaning and more to prepare the data in a way that is more easily consumed. One example of this is the transformation and computation where currency amounts are converted from US dollars to euros.</p>
<p>Loading involves successfully inserting the transform data into the target database data store, or in this case, a data warehouse. All of this work is processed in what the business intelligent developers call an ETL job.</p>
<p>Now we have an understanding of what Amazon Redshift is. Let’s move on to looking at the architecture of the service and the components that is built upon.</p>
<p>Let me start with clusters and nodes. A cluster can be considered the main or core component of the Amazon Redshift service. And in every cluster, it will run its own Redshift engine, which will contain at least one database. As the name implies, a cluster is effectively a grouping of another component, and these being compute nodes.</p>
<p>Each will contain at least one compute node. However, if the cluster is provisioned with more than one compute node, then Amazon Redshift will add another component called a leader node.</p>
<p>Compute nodes all contain their own quantity of CPU attached storage and memory. And there are different nodes that offer different performances. For example, the following RA3 node types. Also, as you can see here, the dense compute node types.</p>
<p>The leader node of the cluster has the role of coordinating communication between your compute nodes in your cluster and your external applications accessing your Redshift data warehouse. So the leader node is essentially gateway into your cluster from your applications. When external applications are querying the data in your warehouse, the leader node will create execution plans, containing code to return the required results from the database.</p>
<p>If the query from the external application references tables associated with the compute nodes, then this code is then distributed to the compute nodes in the cluster to obtain the required data, which is then sent back to the leader node. If the query does not reference tables stored on the compute nodes, then the query will run on the leader node only.</p>
<p>Each compute node itself is also split into slices, known as node slices. A node slice is simply a partition of a compute node where the nodes memory and disk spaces split. Each node slice then processes operations given by the leader node where parallel operations can then be performed across all slices and all nodes at once for the same query. As I mentioned previously, compute nodes can have different capacities and these capacities determine how many slices each compute node can be split into.</p>
<p>When creating a table, it is possible to distribute rows of that table across different nodes slices based upon how the distribution case is defined for the table. For a deeper understanding on how to select the best distribution style, please see the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">here</a>.</p>
<p>When your Amazon Redshift database is created, you will of course connect to it using your applications. Typically these applications will be your analytic and business intelligence tools, that you’re running with your organization. Communication between your BI applications and Redshift, will use industry standard open database connectivity, ODBC. And Java database conductivity, JDBC drivers for PostgreSQL.</p>
<p>The performance that Amazon Redshift can generate is a huge benefit to many organizations. In fact, at the time of writing this course, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> currently boasts that it’s three times faster than other cloud data warehouses.</p>
<p>From a query perspective, Amazon Redshift has a number of features to return results quickly and effectively. Let’s take a look at a few of them.</p>
<p>Firstly, massively parallel processing. As highlighted in the previous section by associating rows from tables across different nodes slices and nodes. It allows the node leader to generate execution plans, to distribute crews from external applications across multiple compute nodes at once, allowing them to work together to generate the end result, which is an aggregated by the leader node.</p>
<p>Columnar data storage. This is used as a way of reducing the number of times the database has to perform disk I&#x2F;O, which helps to enhance query performance. Reducing the data retrievals from the disk means there is more memory capacity to carry out in memory processing of the query results. Result caching. Caching in general is a great way to implement a level of optimization.</p>
<p>Result caching helps to reduce the time it takes to carry out queries by caching some results of the queries in the memory of the leader node in a cluster. As a result, when a query is submitted, the leader node will check its own cache copy of the results and if a successful match is found, the cached results are used instead of executing another query on your Redshift cluster.</p>
<p>Amazon Redshift also integrates with Amazon CloudWatch, allowing you to monitor the performance of your physical resources, such as CPU utilization and throughput. In addition to this, Redshift also generates query and load performance data that enables you to track overall database performance. Any data relating to query and load performance is only accessible from within the Redshift console itself and not Amazon CloudWatch.</p>
<p>During the creation of your Redshift cluster, you can as an optional element, select up to 10 different IAM roles to associate with your cluster. This allows you to grant the Amazon Redshift principle, redshift.amazonaws.com access to other services on your behalf, for example, Amazon S3 where you might have a data lake. Accessing data within S3 will require a set of credentials to authorize Redshift access to S3. And the best way to do that is by using an IAM role. Therefore, if you intend to perform actions such as this when using your Amazon Redshift cluster, you might need to consider which access you need and what roles you will need to create.</p>
<p>To learn more about IAM and roles, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">here</a>. In the next lecture, I want to show you how to create a new Redshift cluster.</p>
<h1 id="DEMO-Creating-an-Amazon-Redshift-Cluster"><a href="#DEMO-Creating-an-Amazon-Redshift-Cluster" class="headerlink" title="DEMO: Creating an Amazon Redshift Cluster"></a>DEMO: Creating an Amazon Redshift Cluster</h1><p>Hello and welcome to this lecture. This is going to be a quick demonstration on how to set up an Amazon Redshift cluster. So as you can see, I’m in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console at the moment, and to find Redshift, we can scroll down to the database category and we can see Amazon Redshift here. So if we select on that, we’re then taken to this splash screen here. So I don’t have any Redshift clusters at the moment.</p>
<p>So to start with, all I need to do is to click on the orange create cluster button, and this takes us to the configuration page. And the first thing we need to do is give it a name. So I’m just going to call this my cluster. Then we can choose our node type. So we have the RA3 nodes and the dense compute nodes here. Now, AWS recommends the RA3 nodes due to the high performance and the managed storage aspect, or you have the dense compute nodes here.</p>
<p>Now, for this demonstration, I’m just going to select a dense compute node. Now, if you scroll down, we can select the number of nodes that we would like. As you can see, it ranges from one to 32, so we can scroll up or down. And also if you see in this configuration summary section the cost per month, and the total compressed storage will also change with the amount of nodes that I select. So there you can see it is scrolling up and down. I’m just going to leave it as two nodes.</p>
<p>Now I can scroll down to the database configurations. We can give the database a name, and also the port that it’s going to be using, and also a master username and password. So let me just enter a password. And then we have cluster permissions. Now, this is an optional step. So if you want your AWS Redshift cluster to interact with other AWS services on your behalf, for example, maybe Amazon S3, you might want to import data, then you can associate an IAM role that has access to S3 to allow that process to happen. But as I said, this is an optional component.</p>
<p>Now, at the very bottom here, we have additional configuration. Now, these are the default settings. So we have a default network, default backup options, maintenance, default security groups, and also a parameter group, as well. But if you turn off those default settings, then you can go through and modify any of those components. For example, network and security. You can select the VPC for it to run in. You can select the security groups that are associated with your clusters to define what resources can access it. You can also define a subnet group which defines what subnets that the clusters will be launched in and also any availability zones. You can also specify if you want any cluster traffic to purely route through your VPC and if you want your cluster to be publicly accessible or not. So there’s a few network and security features that you can change there.</p>
<p>Looking at database configurations, here you can select a parameter group if you have any configured, and you can also configure any encryption using AWS KMS, and if you want to use the default Redshift key, or if you want to use one of your own CMKs, for example, I have a CMK here in my account. For this demonstration, I’m just gonna disable encryption.</p>
<p>Under maintenance, you can set a maintenance window so that the day and time of the week that any maintenance will be carried out to your cluster. And also you can specify which cluster version you’d like, and you have three options. Either use the most current approved cluster version, use the cluster version before the current version, or use the cluster version with beta releases of new versions. I’ll just leave that as current.</p>
<p>Under monitoring, you can have CloudWatch alarms. So for example, you can create a new alarm for disk usage threshold when that reaches 80%, and then you can notify people via an SNS topic that you might already have configured. I’ll say no alarms. And finally, backup. And also you can specify your snapshot retention, which is how long you’ll keep the backups for. And finally, if you want to configure cross-region snapshot, you can either enable that or disable it. And this will back up your cluster to a different region. So if you enable it, you can then select an alternate region to where your cluster currently resides. I’m just going to disable that.</p>
<p>So there are the different options that are available, but I’m just going to select the defaults that it already suggested. And then once you’re happy with your settings, simply click create cluster. As we can see here now, it’s now creating our cluster. This might take a few minutes, so I’ll come back when that’s done.</p>
<p>Okay, as you can see, the cluster is now available. If we select the dashboard, then we can see that we have one new cluster in the Ireland region with two nodes, and we can see that it’s already taken an automated snapshot, as well.</p>
<p>So cluster overview here, so we can see a number of queries, any database connections, disk space used, CPU utilization. As you can see, there’s not much going on at the moment. We’ve simply just created it. If we have any alarms, and down here, any events, and also a query overview here. So I won’t go into any more detail than that.</p>
<p>This is just a very high-level, quick introduction on how to create an Amazon Redshift cluster. And that’s it.</p>
<h1 id="7Amazon-Relational-Database-Service"><a href="#7Amazon-Relational-Database-Service" class="headerlink" title="7Amazon Relational Database Service"></a>7<strong>Amazon Relational Database Service</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">Instance Types Performance</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">Course: When to use RDS Multi-AZ &amp; Read Replicas</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Shared Responsibility Model Security</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Lab: Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Lab: Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="9Amazon-DynamoDB"><a href="#9Amazon-DynamoDB" class="headerlink" title="9Amazon DynamoDB"></a>9<strong>Amazon DynamoDB</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Dat</a></p>
<h1 id="11Amazon-Redshift"><a href="#11Amazon-Redshift" class="headerlink" title="11Amazon Redshift"></a>11<strong>Amazon Redshift</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">Differences between Redshift and PostgreSQL</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">Selecting a Distribution Style</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Course: Overview of AWS IAM</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:35" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:35-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:18" itemprop="dateModified" datetime="2022-11-20T19:01:18-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Knowledge-Check-Storage-CLF-C01.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:33" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:33-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:00" itemprop="dateModified" datetime="2022-11-20T19:08:00-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:32" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:32-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:16" itemprop="dateModified" datetime="2022-11-20T19:08:16-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/53/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/53/">53</a><span class="page-number current">54</span><a class="page-number" href="/page/55/">55</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/55/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
