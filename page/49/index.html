<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/49/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/49/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Management-SAA-C03-43</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:14:01" itemprop="dateCreated datePublished" datetime="2022-11-18T22:14:01-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-28 10:52:06" itemprop="dateModified" datetime="2022-11-28T10:52:06-04:00">2022-11-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Management-SAA-C03-43/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Management-SAA-C03-Introduction"><a href="#Management-SAA-C03-Introduction" class="headerlink" title="Management (SAA-C03) Introduction"></a>Management (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on AWS management fundamentals, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the various management services currently available in AWS that may be covered on the exam.</p>
<p>Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#x70;&#x70;&#x6f;&#x72;&#x74;&#64;&#x63;&#108;&#x6f;&#117;&#100;&#97;&#99;&#97;&#100;&#101;&#x6d;&#121;&#46;&#99;&#111;&#109;">&#x73;&#117;&#x70;&#x70;&#x6f;&#x72;&#x74;&#64;&#x63;&#108;&#x6f;&#117;&#100;&#97;&#99;&#97;&#100;&#101;&#x6d;&#121;&#46;&#99;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various management services in AWS in preparation for the exam.</p>
<p>The objective of this course is to provide a high-level introduction to the various management services in AWS that can assist with auditing, reporting, and monitoring within your AWS environments, including:</p>
<ul>
<li>Amazon CloudWatch,</li>
<li>AWS CloudTrail, and</li>
<li>AWS Config.</li>
</ul>
<p>You’ll learn about different approaches for managing multi-account environments in AWS using AWS Organizations and AWS Control Tower. We’ll take a look at AWS Systems Manager and see how it can be used to manage your applications and resources. We’ll then spend some time discussing logging in AWS, along with an in-depth discussion of logging options for AWS services including CloudWatch, CloudTrail, CloudFront, and VPC Flow Logs. And given the exam’s emphasis on designing cost-optimized architectures, we’ll delve into AWS cost management services including Cost Explorer, Cost and Usage Reports, and Budgets.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#117;&#112;&#x70;&#x6f;&#x72;&#x74;&#64;&#x63;&#108;&#x6f;&#x75;&#x64;&#97;&#99;&#x61;&#x64;&#101;&#109;&#121;&#x2e;&#x63;&#111;&#109;">&#115;&#117;&#112;&#x70;&#x6f;&#x72;&#x74;&#64;&#x63;&#108;&#x6f;&#x75;&#x64;&#97;&#99;&#x61;&#x64;&#101;&#109;&#121;&#x2e;&#x63;&#111;&#109;</a>. Thank you!</p>
<h1 id="What-is-Amazon-CloudWatch"><a href="#What-is-Amazon-CloudWatch" class="headerlink" title="What is Amazon CloudWatch?"></a>What is Amazon CloudWatch?</h1><p>Hello and welcome to this lecture which will provide you with a high-level overview of what Amazon CloudWatch is and does.</p>
<p>Amazon CloudWatch is a global service that has been designed to be your window into the health and operational performance of your applications and infrastructure. It’s able to collate and present meaningful operational data from your resources allowing you to monitor and review their performance. This gives you the opportunity to take advantage of the insights that CloudWatch presents, which in turn can trigger automated responses or provide you with the opportunity and time to make manual operational changes and decisions to optimize your infrastructure if required. </p>
<p>Understanding the health and performance of your environment is one of the fundamental operations you can do to help you minimize incidents, outages and errors. As a result Amazon CloudWatch is heavily used by those in an operational role and site reliability engineers. </p>
<p>There are a wide range of components to Amazon CloudWatch, making this an extremely powerful service. Let me now run through at a high level some of these features and what they allow you to do, including CloudWatch Dashboards, CloudWatch Metrics and Anomaly Detection, CloudWatch Alarms, CloudWatch EventBridge, CloudWatch Logs, CloudWatch Insights.</p>
<p>Using the AWS Management console, the AWS CLI, or the PutDashboard API, you can build and customize a page using different visual widgets displaying metrics and alarms relating to your resources to form a unified view. These dashboards can then be viewed from within the AWS Management Console.</p>
<p>Here is an example of the different types of widgets you can select to build your dashboard.</p>
<p>The resources within your customized dashboard can be from multiple different regions making this a very useful feature. Being able to build your own views, you can quickly and easily design and configure different dashboards to represent the data that you need to see from a business and operational perspective. For example, you might need to view all performance metrics and alarms from resources relating to a particular project, or a specific customer. Or you might want to create a different dashboard for a specific region or application deployment. The key point is that they are fully customizable to be designed how YOU want to represent your data.  </p>
<p>For more information of selecting the right chart type to visualize data, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/">https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/</a></p>
<p>Once you have built your Dashboards, you can easily share them with other users, even those who may not have access to your AWS account. This allows you to share the findings gathered by CloudWatch with those who may find the results interesting and beneficial to their day-to-day operational role, but don’t necessarily require the need to access your AWS account.</p>
<p>Metrics are a key component and fundamental to the success of Amazon CloudWatch, they enable you to monitor a specific element of an application or resource over a period of time while tracking these data points. For example, the number of DiskReads or DiskWrites on an EC2 instance, these are just 2 metrics relating to EC2 that you can monitor. Different services will offer different metrics, for example, there is no DiskReads for Amazon S3 as it’s not a compute service, and so instead metrics relevant to the service are available, such as NumberOfObjects, which tracks the number of objects in a specified bucket.</p>
<p>By default when working with Amazon CloudWatch, everyone has access to a free set of Metrics, and for EC2, these are collated over a time period of 5 minutes. However, for a small fee, you can enable detailed monitoring which will allow you to gain a deeper insight by collating data across the metrics every minute. In addition to detailed monitoring, you can also create your own custom metrics for your applications, using any time-series data points that you need, but be aware that when you create a metric they are regional, meaning that any metrics created in 1 region will not be available in another.</p>
<p>CloudWatch metrics also allow you to enable a feature known as anomaly detection. This allows CloudWatch to implement machine learning algorithms against your metric data to help detect any activity that sits outside of the normal baseline parameters that are generally expected. Advance warning of this can help you detect an issue long before it becomes a production problem.</p>
<p>Amazon CloudWatch Alarms tightly integrate with Metrics that I just discussed and they allow you to implement automatic actions based on specific thresholds that you can configure relating to each metric.</p>
<p>For example, you could set an alarm to activate an auto scaling operation, such as provisioning another instance if your CPUUtilization of an EC2 instance peaked at 75% for more than 5 minutes. You could also configure an alarm to send a message to an SNS Topic when the same instance drops back below the 75% threshold, causing it to come out of an ‘alarm’ state notifying engineers of the change. </p>
<p>For more information on SNS, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/">https://cloudacademy.com/course/using-sqs-sns-ses/</a></p>
<p>Speaking of Alarm states, there are 3 different states for any alarm associated with a metric, these being OK – The metric is within the defined configured threshold, ALARM – The metric has exceeded the thresholds set, and INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.</p>
<p>CloudWatch alarms are also easily integrated with your dashboards as well, allowing you to quickly and easily visualize the status of each alarm. When an alarm is triggered into a state of ALARM, it will turn red on your dashboard, giving a very obvious indication.</p>
<p>CloudWatch EventBridge is a feature that has evolved from an existing feature called Amazon Events. So if you have any prior experience working with CloudWatch Events then this will be fairly familiar to you.  </p>
<p>CloudWatch EventBridge provides a means of connecting your own applications to a variety of different targets, typically AWS services, to allow you to implement a level of real-time monitoring, allowing you to respond to events that occur in your application as they happen.  </p>
<p>But what is an event? Basically, an event is anything that causes a change to your environment or application.</p>
<p>The big benefit of using CloudWatch EventBridge is that it offers you the opportunity to implement a level of event-driven architecture in a real-time decoupled environment. </p>
<p>EventBridge establishes a connection between your applications and specified targets to allow a data stream of events to be sent. Currently, there is a wide range of targets that can be used as a destination for events as you can see here.</p>
<p>For the latest list of targets, please see the relevant documentation here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html</a></p>
<p>Let me provide a quick level overview of some of the elements of this feature, and these include Rules, Targets, and Event Buses.</p>
<p>So starting with Rules. A rule acts as a filter for incoming streams of event traffic and then routes these events to the appropriate target defined within the rule. The rule itself can route traffic to multiple targets, however the target must be in the same region. </p>
<p>Next, we have Targets. We saw a list of these just a few moments ago, so targets and where the events are sent by the Rules, such as AWS Lambda, SQS, Kinesis or SNS. All events received by the target are done os in a JSON format</p>
<p>Now finally, Event Buses. An Event Bus is the component that actually receives the Event from your applications and your rules are associated with a specific event bus. CloudWatch EventBridge uses a default Event bus that is used to receive events from AWS services, however, you are able to create your own Event Bus to capture events from your own applications. </p>
<p>CloudWatch Logs gives you a centralized location to house all of your logs from different AWS services that provide logs as an output, such as CloudTrail, EC2, VPC Flow logs, etc, in addition to your own applications.</p>
<p>When log data is fed into Cloudwatch Logs you can utilize CloudWatch Log Insights to monitor the logstream in real time and configure filters to search for specific entries and actions that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. </p>
<p>An added advantage of CloudWatch logs comes with the installation of the Unified CloudWatch Agent, which can collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. This metric data is in addition to the default EC2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. </p>
<p>There are now 3 different types of insights within CloudWatch, there are Log Insights, Container Insights, and Lambda Insights.</p>
<p>But what exactly are insights? Well as the name suggests, they provide the ability to get more information from the data that CloudWatch is collecting. So let’s look at each of these at a high level to understand the role that they perform, starting with Log Insights.</p>
<p>This is a feature that can analyze your logs that are captured by CloudWatch Logs at scale in seconds using interactive queries delivering visualizations that can be represented as bar, line, pie, or stacked area charts. The versatility of this feature allows you to work with any log file formats that AWS services or your applications might be using.</p>
<p>Using a flexible approach, you can use Log insights to filter your log data to retrieve specific data allowing you to gather insights that you are interested in. Also using the visual capabilities of the feature, it can display them in a visual way.</p>
<p>Much like Log insights, Container Insights allow you to collate and group different metric data from different container services and applications within AWS, for example, the Amazon Elastic Kubernetes Service, (EKS) and the Elastic Container Service (ECS). </p>
<p>In addition to the standard metrics collected for these services by CloudWatch, Container Insights also allows you to capture and monitor diagnostic data giving you additional insights into how to resolve issues that arise within your container architecture. This monitoring and insight data can be analyzed at the cluster, node, pod, and task level making it a valuable tool to help you understand your container applications and services.</p>
<p>As you may have guessed by now, this feature provides you the opportunity to gain a deeper understanding of your applications using AWS Lambda. Working on the principles as we have seen with the previous 2 insight features, it gathers and aggregates system and diagnostic metrics related to AWS Lambda to help you monitor and troubleshoot your serverless applications.</p>
<p>To enable Lambda Insights, you need to enable the feature per Lambda function that you create within Monitoring Tools section of your function:</p>
<p>This ensures that a CloudWatch extension is enabled for your function allowing it to collate system-level metrics which are recorded every time the function is invoked.</p>
<h1 id="What-is-AWS-CloudTrail"><a href="#What-is-AWS-CloudTrail" class="headerlink" title="What is AWS CloudTrail?"></a>What is AWS CloudTrail?</h1><p>Hello, and welcome to this lecture. In this lecture, I will explain the basic fundamentals of AWS CloudTrail to give you an overview of the service, before we look deeper at the inner workings revealing how the different elements work together.</p>
<p>So what is CloudTrail and what does it do? CloudTrail is a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface, from within the AWS Management Console, or even from a request made by another AWS service.</p>
<p>For example, when Auto Scaling automatically sends an API request to launch or terminate an instance, these API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file, which is then stored on S3. Each API call represents a new event within the log file.</p>
<p>CloudTrail also records and associates other identifying metadata with all the events. For example, the identity of the caller, the timestamp of when the request was initiated, and the source IP address. For greater management, new log files are typically created every five minutes, which are then delivered and stored within an S3 bucket that is defined by you during your CloudTrail configuration. This allows you to easily go back and review the history of all API requests made.</p>
<p>There’s also an option to have these logs delivered to a CloudWatch Logs log file as well. Having this association with CloudWatch enables custom metrics to be converted, to monitor specific API requests. Thresholds can be set against these metrics and when crossed the Simple Notification Service, SNS, can be triggered to notify your security teams to investigate. That, at a very high level, is the overall function of the AWS CloudTrail service.</p>
<p>Now let’s take a look at the CloudTrail architecture to understand where it can be implemented from an AWS region standpoint and which services can be supported. AWS CloudTrail is a global service with support for all regions.</p>
<p>In addition to this worldwide coverage CloudTrail also provides support for over 60 AWS services and features across a wide range of service categories. As you can imagine with this extensive coverage, CloudTrail can capture a vast amount of data if you have a multi-region, multi-service infrastructure environment deployed.</p>
<p>So armed with this information, what can you do with it? How can you use this data to help you manage and support your AWS infrastructure? Well there are a number of ways you can use the data captured by CloudTrail to help you enhance your AWS environment.</p>
<p>Firstly, it can be used very effectively as a security analysis tool. CloudTrail events provide very specific information about where an API call originated from and who or what initiated the request. As a result, if malicious activity was detected via irregular trends or restricted API call thresholds with the use of CloudWatch, then a number of security controls can be quickly implemented to prevent the user from causing additional damage.</p>
<p>Another common use for CloudTrail is to help resolve and manage day-to-day operational issues and problems. Using built-in filtering mechanisms it’s possible to quickly find who, what, and when a particular API was used, which could have potentially caused an outage or service interruption. This enables quicker root cause identification resulting in a speedy resolution. Appropriate actions could then be taken to ensure the incident does not reoccur in your environment. As API calls to add, modify, or delete resources are captured, CloudTrail can be an effective method of tracking changes to resources within your environment.</p>
<p>There is another AWS service that is specifically designed to audit and track changes to resources, which is called AWS Config, which CloudTrail interacts with. However, CloudTrail can be used to capture the actual API request and all associated data, which made the change. And if you’re not using AWS Config, then this at least provides some base level of monitoring and tracking.</p>
<p>From a governance and security legislation perspective, many certifications require the ability to recall and provide evidence of log files relating to specific changes to resources. CloudTrail provides all of this by default through the use of capturing events and writing them to a log file, which is then stored on S3. AWS has a great white paper on achieving compliance using CloudTrail entitled “Logging in AWS How AWS CloudTrail can help you achieve compliance by logging API calls and changes to resources.” The following <a target="_blank" rel="noopener" href="https://d1.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf">URL</a> will take you to that white paper.</p>
<p>If you need to be able to capture and track API requests within your AWS account, for any of these reasons mentioned, or perhaps for other reasons you may have of your own, then CloudTrail can do this for you and deliver the output as a log file into an S3 bucket of your choice.</p>
<h1 id="AWS-CloudTrail-Operations"><a href="#AWS-CloudTrail-Operations" class="headerlink" title="AWS CloudTrail Operations"></a>AWS CloudTrail Operations</h1><p>Hello, and welcome to this lecture. Where we are to discuss the different features and components of CloudTrail and how they work together to provide a customizable API call tracking and monitoring solution. </p>
<p>Firstly, let’s look under the hood of CloudTrail to see what makes up the core features and components that create the service.</p>
<ul>
<li>Trails. These are the building blocks of the service. You can create many different Trails containing different configurations related to API requests that you want to capture.</li>
<li>S3. S3 is used by default to store the CloudTrail log files and a dedicated S3 bucket is required during the creation of a new Trail.</li>
<li>Logs. Logs are created by AWS CloudTrail and record all events captured. A new log file is created approximately every five minutes and once processed, it is delivered to an S3 bucket as defined by its Trail configuration. If no API calls to be made, then no logs will be delivered.</li>
<li>KMS. The use of AWS KMS is an optional element of CloudTrail, but it allows additional encryption to be added to your log files when stored on S3.</li>
<li>SNS. SNS is also an optional component for CloudTrail, but it allows for you to create notifications. For example, when a new log file is delivered to S3 SNS could notify someone or a team via an email, or it could be used in conjunction with CloudWatch. When metric thresholds have been reached.</li>
<li>CloudWatch logs. Again, this is another optional component. For AWS CloudTrail allows you to deliver its logs to AWS as CloudWatch logs, as well as S3 for specific monitoring metrics to take place.</li>
<li>Events Selectors. Events Selectors allowed to add a level of customization to the type of API requests, you want the corresponding trails to capture.</li>
<li>Tags, Tags allow you to assign your own metadata to your Trail. For example, you could add a project or department tag indicating which project or department the trail relates to.</li>
<li>Events. For every API request that is captured by CloudTrail it is recorded as an event in a CloudTrail log file. API Activity Filters. These are search filters that can be applied against your API activity history in the management console for create, modify and delete API calls. These events are held in the management console for seven days. Even if the Trails itself is stopped or deleted.</li>
</ul>
<p>Okay, so I’ve now covered the different components that essentially build CloudTrail. Let me now introduce you to the process at a high level of how all of this fits together and in what order. The very first step is to create a Trail. If no Trail exists, then CloudTrail does not know what API cost to capture from which region and which services, global or otherwise. During this Trail creation, you will need to specify an S3 bucket for your log files to be delivered to you. This can be an existing bucket or new bucket, which can be created at this stage.</p>
<p>You will then have an option to encrypt your log files with the key management service KMS, if required. Also if required, you can configure the Simple Notification Service SNS to notify you when new log files are delivered. If you want to add another layer of security, integrity, then you can enable log file validation, which will ensure your logs have not been modified or tampered with since they delivered to S3. Your Trail is now ready to be turned on and created.</p>
<p>Once it has been created, you are able to make further configurational changes that are not available during the Trail creation itself. Upon selection of your new Trail you will have the option to configure CloudWatch logs, which allows you to deliver your cloud Trails log files to CloudWatch in addition to S3. This allows you to create CloudWatch monitoring metrics against specific API calls and will receive notification from SNS when custom thresholds are reached.</p>
<p>Another optional configurable element is that of CloudTrail Event Selectors. This allows you to specify the types of events, management or data that CloudTrail logs. Finally, at the last element of your newly created Trail configuration, there is the ability to add tags just as you would with other resources within AWS. At this point, your Trail is configured and actively recording API calls as per your configuration.</p>
<p>For every API call that matches the requirement of your Trail, it will be captured and recorded in a log file as an event. Each API call will be recorded as a new event. Once you’ve captured the data, you may need to find a particular event quickly, maybe for security reasons. This can be achieved using API Activity Filters, which can be found within the CloudTrail service from the management console.</p>
<p>So from a high-level perspective, we know how a Trail is configured, but what happens when an API matching a Trail is called upon by user or service? Let’s take a quick look.</p>
<p>A user or service calls upon an API. Next CloudTrail checks to see if this API call matches any configured Trails. If a match is found CloudTrail records the API as an event within its current log file. It also associates other identical metadata mentioned earlier. Eventually, the event with the log file will be delivered to S3 and possibly CloudWatch logs, depending on the Trail configuration. If it is sent to CloudWatch logs, the log file will be monitored by any configured metrics. When in S3, the log file will be stored and the default server-side encryption SSE, and unless KMS has been configured for increased security measures with the associated Trail. If any S3 lifecycle cycle rules are applied to the bucket, then over time, the log file may be archived to a different storage class or even glassier.</p>
<p>That essentially concludes the elements of CloudTrail and how they work together.</p>
<h1 id="What-is-AWS-Config"><a href="#What-is-AWS-Config" class="headerlink" title="What is AWS Config?"></a>What is AWS Config?</h1><p>Hello, and welcome to this lecture where we will talk about the AWS Config Service itself, what it is and what it does. So let’s get started. As many of you will be aware, one of the biggest headaches in any organization when it comes to resource management of IT infrastructure is understanding the following, what resources do we have? What devices are out there within our infrastructure performing function? Do we have resources that are no longer needed? And therefore, can we be saving money by switching them off? What is the status of their current configuration? Are there any security vulnerabilities we need to worry about? How are our resources linked within the environment? What relationships are there and are there any dependencies? If we make a change to one resource, will this affect another? What changes have occurred on the resources and by whom? Do we have a history of changes for this resource that shows us how the resource has changed over time? Is the infrastructure compliant with specific governance controls? And how can we check to ensure that this configuration is meeting specific internal and external requirements? And do we have accurate auditing information that can be passed to external auditors for compliance checks?</p>
<p>Depending on the size of your deployment with AWS, trying to answer some of these questions can be very time consuming and laborious. Some of this information can be captured via the AWS CLI by performing a describe or list against the specific resource. But implementing a system to capture those results and output them into a readable format could be very resource intensive. And of course, this will only help you with a small piece of the puzzle.</p>
<p>AWS is aware that due to the very nature of the cloud and its benefits, the resources within an AWS environment are likely to fluctuate frequently along with the configurations of the resources. The cloud by its very nature is designed to do so. And so trying to keep up with the resource management can be a struggle. Because of this, AWS released AWS Config to help with this very task. The service has been designed to record and capture resource changes within your environment, allowing you to perform a number of actions against the data that helps to find answers to the questions that we highlighted previously.</p>
<p>So what did AWS design AWS Config to do? Well, in a nutshell, AWS Config can capture resource changes. So any change to a resource supported by Config can be recorded which will record what changed along with other useful metadata all held within a file known as a Configuration Item, a CI. It can act as a resource inventory.</p>
<p>AWS Config can discover supported resources running within your environment allowing you to see data about that resource type. You can store configuration history for individual resources. The service will record and hold all existing changes that have happened against the resource providing a useful history record of changes. It can provide a snapshot in time of current resource configurations.</p>
<p>An entire snapshot of all supported resources within a region can be captured that will detail their current configurations with all related metadata. Enable notifications of when a change has occurred on a resource. The Simple Notification Service, SNS is used with AWS Config to capture a configuration stream of changes enabling you to process and analyze the changes to resources.</p>
<p>It can provide information on who made the change and when through AWS CloudTrail integration. AWS CloudTrail is used with AWS Config to help you identify who made the change and when and with which API. You can enforce rules that check the compliancy of your resource against specific controls.</p>
<p>Predefined and custom rules can be configured with AWS Config allowing you to check resources compliance against these rules. You can perform security analysis within your AWS environment. A number of security resources can be recorded. And when this is coupled with rules relating to security such as encryption checks, this can become a powerful analysis tool. And it can provide relationship connectivity information between resources.</p>
<p>The AWS Management Console provides a great relationship query, allowing you to quickly see and identify which resources are related to any other resource. For example, when looking at an EBS volume, you’ll be able to see which EC2 instance it is connected to. And it does all of this and presents the data in a friendly format. This is a lot of incredibly useful data that can be used across a range of different scenarios.</p>
<p>Now, unfortunately, at the time of writing this course, the AWS Config Service does not capture this information for all services, but it certainly captures data for the most common services and resources which you would want to hold information for. Services such as EC2, RDS, IAM, and VPC. And it’s great to see that within each of these, there are specific security resources that are covered such as security groups and custom IAM policies.</p>
<p>This makes AWS Config very useful when it comes to carrying out a security analysis. For more information on the latest resources that AWS Config supports. Please see the link on screen. AWS Config is region specific, meaning that if you have resources in multiple regions, then you will have to configure AWS Config for each region you want to record resource changes for.</p>
<p>When doing so, you are able to specify different options for each region. For example, you could configure Config in one region to record all supportive resources across all services within that region. And that at a predefined AWS managed Config rule that will check if EBS volumes are encrypted.</p>
<p>In another region, you could select to only record a specific type of resource such as security groups with no predefined rules allocated. Some of you may be wondering what if the service you want to monitor is not region specific such as IAM? well, in this case, there was a separate option to include Global Services, which IAM falls under.</p>
<h1 id="Key-Components-of-AWS-Config"><a href="#Key-Components-of-AWS-Config" class="headerlink" title="Key Components of AWS Config"></a>Key Components of AWS Config</h1><p>Hello, and welcome to this lecture. Where we will be taking a look at the components that make up the AWS Config Service and the functions that each of them carry in delivering the service. To understand how to get the most of the service, it’s important to understand how the services piece together and how it works. So let’s start by identifying the key components. Following this, we will then break each of them down to understand their role and how they sit within the service.</p>
<p>The following identifies the main components to the service. AWS resources, configuration items, configuration streams, configuration history, configuration snapshots, configuration recorder, config rules, resource relationships, SNS topics, S3 buckets and AWS Config permissions. Let’s now take a look at each of these and turn in more detail. Starting with AWS resources.</p>
<p>AWS resources are typically classed as objects that can be created, updated or deleted from within the AWS Management Console or programmatically, using the AWS CLI or supported STK. AWS Config records changes to supported AWS resources within a specific Region. For the current list of supported services and resources, please follow the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-resources">link</a> on screen.</p>
<p>A configuration item or CI as is known, is a key component of AWS Config. It is comprised of a JSON file that holds the configuration information, relationship information, and other metadata as a point in time snapshot view of a supported resource. All the information that AWS Config can record for resource, is captured within the CI. A CI is created every time a supported resource has a change made to its configuration in any way.</p>
<p>In addition to recording the details of the affected resource, AWS Config will also record CIs for any directly related resources to ensure the change did not affect those resources too. For example, if there was a rule change to a security group, additional rules would be added with new parts. AWS Config would record all CI information for that resource, but it would also give a CI information for any instances that were a part of that security group. The CIs would then be sent to a configuration stream, which we will cover next.</p>
<p>Because it captures so much data, these CIs are used by other features and components of AWS Config, such as configuration history. CIs are used to look up all the changes that have been made to a resource. </p>
<p>Configuration Streams. CIs are sent to an SNS topic to enable analysis of the data. Configuration snapshots. CIs are used to create a point in time snapshot of all supported resources. Following CIs, let’s move on to configuration streams. When a create, update or delete API call is made against a supported AWS Config resource, as we now know a new CI is created along with additional CIs for any resources that are related to the original modified resource.</p>
<p>These CIs are then sent to a configuration stream, and this stream is in the form of an SNS topic. However, this stream is also used by AWS Config to send information when other events occur such as when the configuration history for a resource was delivered to your account, when a configuration snapshot was started and delivered to your account, when the state of your resource compliance changes against any config rules that have been configured, when evaluation begins for rules against resources, and when AWS Config found to deliver notifications to your account.</p>
<p>The configuration history uses configuration items to collate and produce a history of changes to a particular resource. This allows you to see the complete set of changes made to resource over a set period of time. The information can be accessed either programmatically through the AWS CLI using the following command. You can also specify the resource type. So for example, if you wanted to look at the configuration history for a Subnet, you could enter the following into the AWS CLI. Or you could access the history via the AWS Management Console.</p>
<p>Additionally, AWS Config also sends a configuration history file for each resource type to an S3 bucket that is selected during the setup of AWS Config. This configuration file is typically delivered every six hours. Unlike our site, it contains all CI changes for all resources of a particular type. For example, there would be one configuration history file covering six hours for all RDS DB instance changes in one Region.</p>
<p>The configuration snapshot also uses configuration items in its production. The configuration snapshot will take a point in time snapshot on all supported resources configured for that Region. It will generate CIs for each resource in your AWS account for a specific Region, and this configuration snapshot can then be sent to an S3 bucket. Alternatively, this information can be viewed by the AWS Management Console.</p>
<p>Now let’s take a look at the configuration recorder. So this component of AWS Config can be seen as the engine of the service. This element is responsible for recording all of the changes to the supported resources within your account, and generating the configuration items. By default, the configuration recorder is automatically enabled and started when you first configure AWS Config. However, it is something that you can stop and then will start again at a later point. When you stop it, AWS Config will no longer track and record changes to your supported of resources.</p>
<p>When you first configure AWS Config in the AWS Management Console, you are asked which resource types you would like to record. This is essentially setting up and creating the configuration recorder, as this information is required to enable the configuration recorder to start. If you only select to record specific resource types, then it will capture all creates, changes and deletes for those resource types, and will create a CI record for each occurrence. However, the configuration recorder will still record all creates and deletes of supported resource types within that Region, but no changes to those resources will be recorded.</p>
<p>Also, if you stop and change your configuration recorder settings to, perhaps, remove certain resource types from being recorded, then all captured information for those resources up to that point will remain, and you will be able to view the history with all data from the previous CIs that have been captured.</p>
<p>AWS Config rules are a great way to help you enforce specific compliance checks and controls across your resources and allows you to adopt an ideal deployment specification for each of your resource types. Each rule is essentially a Lambda function, that when called upon, evaluates the resource and carries out some simple logic to determine the compliance result with the rule.</p>
<p>Each time a change is made to one of your supportive resources, AWS Config will check the compliance against any config rules that you have in place. If there was a violation against these rules, then AWS Config will send a message to the configuration stream by SNS, and the resource will be marked as noncompliant.</p>
<p>It’s important to note that this does not mean the resource will be taken out of service or it will stop working, it will continue to operate exactly as it is with its new configuration. AWS Config simply alerts you that there’s a violation and it’s up to you to take the appropriate action. These rules can be custom defined or selected from a predefined list of AWS managed rules that AWS has created on your behalf.</p>
<p>Being able to create your own rules, allows you to adopt best practices that you may have internally within your own enterprise or with other security best practices. By allowing AWS Config to monitor resources at this level, it adds another level of automation, helping to prevent misconfigurations made by human error being left, which could lead to a security risk or even worse, a breach. I highly recommend using config rules for maintaining security checks and configurations.</p>
<p>AWS have a number of predefined rules that fall under the security umbrella that are ready to use. For example, Rds-storage-encrypted, this checks whether storage encryption is activated by your RDS database instances. Encrypted-volumes, this checks to see if any EBS volumes that are in attached state, are encrypted. Root-account-mfa-enabled, this checks whether your root account of your AWS account requires multi-factor authentication for console sign-in. IAM-user-no-policy-check, this checks that none of your IAM users have policies attached.</p>
<p>Best practice dictates that permission should be provided by roles or groups. The great thing about these predefined rules is that you can also edit them to make subtle parameter changes as needed. As you can see, being able to ask either AWS Config to check your resources compliance with rules such as these are invaluable. And when you couple this with being able to create your own custom rules, the scope and potential of automating compliance is huge across your supported resources.</p>
<p>As a part of the CI for a particular resource, for example, in EBS volume, AWS Config identifies relationships with other resources from that resource. In this case, it might be the EC2 instance that the volume is attached to. This allows AWS Config to build a logical mapping of resources and how they connect. This mapping of relationships allows you to quickly jump to other linked resources within the AWS Management Console to view their configuration history and CI data. This is very useful if you’re trying to troubleshoot an issue and pinpoint where the source of an incident may be. For a full listing of available relationship types, see the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-relationships">link</a> on screen.</p>
<p>As we have already seen, an SNS topic is used as a configuration stream for notifications of various events triggered by AWS Config. You can have various end points associated to the SNS stream. Best practice in the case that you should use SQS and then programmatically analyze the results via SQS. The S3 bucket that was selected at the time of configuration, is used to store all that configuration history files that are generated for each resource type, which happens every six hours.</p>
<p>Also, any configuration snapshots that are taken are also stored within the same S3 bucket. The configuration details used for both SNS and S3 are classed as the AWS Config delivery channel, by which data can be sent to other services. When setting up AWS Config, you’re required to select an IAM role. This role is required to allow AWS Config to obtain encrypt permissions to carry out and perform a number of functions.</p>
<p>For example, AWS Config will need read only access to all the supported resources within your account so it can retrieve data for the configuration items. Also, we now know that AWS Config uses SNS and S3, both the streams and storage of the configuration history files and snapshots. So AWS Config requires the relevant permission to allow it to send data to these services.</p>
<p>That now covers the key elements of AWS Config. And so I hope it makes it clear as to how each part plays a role within the service. To reiterate, let’s take a bulleted point look at how it all comes together.</p>
<p>When you first turn on AWS Config, you will configure the elements required for the configuration recorder to begin capturing and recording data. AWS Config will then discover all of your supported resources based upon the details entered within the configuration recorder within that Region. When a create, change or delete against a resource is made, a CI will be created for this change, and a notification will be sent the configuration stream regarding the new CI. Also, following the CI, AWS Config will check its current config rules to evaluate if the changes made the resource noncompliant. If the evaluate state change for the resource, a notification will be sent to the configuration stream. Your config rules can also be configured to run periodically, and so that will run at a set given time period regardless if there have been any changes to resources. If a configuration snapshot is taken, AWS Config will create a point in time snapshot of the resources with new CIs and deliver this configuration to your specified S3 bucket within the configuration recorder. After six hours has passed from turning on AWS Config, a configuration history file will be created for each resource type, and again, sent to your specified S3 bucket. The configuration file will be sent every six hours from that point.</p>
<h1 id="AWS-Organizations"><a href="#AWS-Organizations" class="headerlink" title="AWS Organizations"></a>AWS Organizations</h1><p>Hello and welcome to this lecture where I will start this course by providing an overview of AWS Organizations as a foundation before focusing on the service control policies.</p>
<p>As businesses expand their footprint on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and utilize more services to build and deploy their applications, it will soon become apparent that the need for multiple AWS accounts is required to manage the environment and infrastructure effectively.</p>
<p>This multi-account strategy is beneficial for a number of reasons as your organization scales, and for example, multi-account strategies include cost optimization and billing, security and governance, management and control of workloads, resource grouping, and helping to define business units.</p>
<p>As you begin to expand with multiple accounts, it will become increasingly more difficult to manage them as separate entities. The more accounts you have, the more distributed your environment becomes and the associated security risks and exposures increase and multiply.</p>
<p>However, with AWS Organizations, it can provide a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization, which helps to maintain your AWS environment from a security, compliance, and account management perspective.</p>
<p>To understand how AWS Organizations operates, we first need to be aware of the hierarchy of the service’s components.</p>
<p>AWS Organizations uses the following components to help you manage your accounts: Organizations, Root, Organizational Units, Accounts, Service Control Policies.</p>
<p>An Organization is an element that serves to form a hierarchical structure of multiple AWS accounts. You could think of an organization as a family tree which provides a graphical view of your entire AWS account structure. At the very top of this Organization, there will be a Root container.</p>
<p>The Root object is simply a container that resides at the top of your Organization. All of your AWS accounts and Organizational units will then sit underneath this Root. Within any Organization, there will only be one single Root object.</p>
<p>Organizational Units (OUs) provide a means of categorizing your AWS Accounts. Again, like the Root, these are simply containers that allow you to group together specific AWS accounts. An organizational unit (or OU) can connect directly below the Root or even below another OU (which can be nested up to 5 times). This allows you to create a hierarchical structure as I mentioned previously.</p>
<p>Accounts. These are your AWS accounts that you use and create to be able to configure and provision AWS resources. Each of your AWS accounts has a 12 digit account number.</p>
<p>Service control policies, or SCPs, allow you to control what services and features are accessible from within an AWS account. These SCPs can either be associated with the Root, Organizational Units, or individual accounts. When an SCP is applied to any of these objects, its associated controls are fed down to all child objects. Think of it as a permission boundary that sets the maximum permission level for the objects that it is applied to.</p>
<p>Now we have an understanding of what AWS Organizations is exactly, what benefits can this bring to your AWS environment?</p>
<p>The primary benefit that this service brings is its ability to centrally manage multiple Accounts from a single AWS account, known as the master account. You can start by inviting your existing accounts to an Organization and then create new accounts directly from the Master Account.</p>
<p>Greater control of your AWS environment. Through the use of Service Control Policies attached to the Root, Organizational Units or individual accounts, administrators of the master account gain powerful control over which services and features—even down to specific API calls—that an IAM user within those accounts can use, regardless of the user’s identity-based or resource-based permissions. Consolidated Billing. The master account of your AWS Organization can be used to consolidate the billing and costs from all member AWS accounts. This allows for greater overall cost management across your individual AWS accounts. Categorization and grouping of accounts. By leveraging Organizational Units, you can segregate and group-specific AWS accounts together, applying different SCPs associated to each OU. For example, you may have a number of AWS accounts that must not have the ability to access any AWS Analytical services. In this case, you could place these accounts into a single OU and assign an SCP that denies this functionality.</p>
<h1 id="Implementing-AWS-Organizations"><a href="#Implementing-AWS-Organizations" class="headerlink" title="Implementing AWS Organizations"></a>Implementing AWS Organizations</h1><p>Hello and welcome to this lecture which will explain how to initially set up and configure AWS organizations. Setting up an organization is a very simple process that starts from a master AWS account. Your master account is a standard <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account that you have chosen to create the AWS organization. It’s best practice to use this AWS account solely as a master account, and not to use it to provision any other resources such as EC2 instances, et cetera. This allows you to restrict access to the master account at a greater level. The few users who need access to it, the better, and you need to do this because the master account carries certain administrative level capabilities such as being able to create additional AWS accounts within your organization, invite other accounts to join your organization, remove AWS accounts from your organization, and apply security features via policies to different levels within your organization.</p>
<p>Once you have selected your AWS account to be used as a master account, you can create an organization. From here, you have two choices when creating an organization type: enable all features or enable only consolidated billing. If you want to set up service control policies, then you need to select enable all features.</p>
<p>The second option allows you to control payments and manage costs centrally from that master account across all associated AWS accounts within the organization. When the organization is created, the master account can create organizational units for AWS account management as required. The master account can also invite other member AWS accounts to join the organization. During this invitational process, the account owner of these invited AWS accounts will receive an email requesting that their AWS account join the organization. Once the accounts have joined the organization, the master account can then move these accounts into the corresponding OUs that have been created and associate relevant service control policies with them.</p>
<p>Let me now show you via demonstration on how to create a new organization and invite an existing account to join it. Now I’m logged into my AWS management console in the AWS account that I want to be the master account, and the first thing I need to do is go to AWS organizations, which is under the management and governance category, and you can see, it’s just at the top here.</p>
<p>So if I go into organizations, and at the moment, I don’t have any organizations set up or created. So the first thing I need to do is click on create organization, and this gives you a quick, high-level screenshot just to explain what creating an organization does. So it provides single payer and centralized cost tracking, it lets you create and invite accounts, it allows you to apply policy-based controls, and it helps you simplify organization-wide management of AWS services.</p>
<p>Now, as I mentioned previously, there’s two options when you create your organization. You can only create it with all features enabled, which is what I just listed, or as you can see here, you can just create your organization to consolidate your billing features. With this demonstration, I’m going to create it with all features. So let’s go ahead and create our organization, and that’s effectively it. So it’s very easy to create your AWS organization to start with, and because this is a brand new organization, this is my master account, which is signified by this star here, and this is my account name, and my account ID.</p>
<p>So, to actually create the organization is very simple, but now I want to add another account as a member account, so let me go ahead and do that. So if I select add account, now I have two options here. I can invite an existing account or create a new account. Now I already have another AWS account, so I’m going to invite an existing account. Now I need to enter the email or account ID, so I’ll just paste in my account, and you can add any notes here, for example, please join my organization, and then you select invite.</p>
<p>Okay, now we can see that we have a request that’s been sent as an invitation. The status is currently open. So now the email address that was registered with this account will get an invitation and they must accept that invite into this organization. So let’s take a look and see if I got that email. So here we can see the email that’s been sent to the owner of that member account, and it says, Stuart would like to add your AWS account to their organization as a member account, and then it just gives some additional blurb about AWS organizations, but to accept the invitation, and to understand what features have been enabled, we need to click on this link here.</p>
<p>So if I select that link, and sign in to my account using my details and MFA code, then I can see that I have an invitation from AWS organizations. We can see the organization ID, the master account name, and the requested controls, which is enable all features. So here, I can either accept or decline and I’m going to accept. I just need to confirm the confirmation message about joining the organization.</p>
<p>Okay, now this member account is now a part of that organization. So if I go back to my master account now, I can see now that within my AWS organization of my master account, I have the CA demo account, which is the name of my other account, and we can see that it’s not a master because it hasn’t got the star whereas this account has the, this is the master account. So as you can see, it’s a very simple process to invite other accounts to your organization.</p>
<p>Now I also mentioned previously about organizing accounts and using organizational units. So if we select organize accounts, at the moment, we only have the root in here. So I can create the new organizational unit and assign each of these accounts into those. So, for example, let me create a new organizational unit called production.</p>
<p>Now I’m also going to create a second organizational unit called test. So let me create another one. At the moment, under root, we have our two accounts. So we have our master account and our member account here. Now I want to move my master account into the production organizational unit, just to make things a little more organized. So I can select the account, click on move, and then simply select where I want it to reside within the tree, and then click move, and we can see, it’s now been removed from the root location, and I want to do the same with the member account, but this time, I want to move that into the test OU. So now, if I click on production over here, this organizational unit, we can see the account that it has inside it, and again, if we go back to the root and click on test, we can see that we have the member account. So I just wanted to show you that quickly just to show you how you can easily and quickly organize your different AWS accounts.</p>
<p>Okay, and that’s the end of the demonstration.</p>
<h1 id="Securing-Your-Organizations-with-Service-Control-Policies"><a href="#Securing-Your-Organizations-with-Service-Control-Policies" class="headerlink" title="Securing Your Organizations with Service Control Policies"></a>Securing Your Organizations with Service Control Policies</h1><p>Hello and welcome to this lecture which will dive into Service Control Policies to understand how they can be used to secure your AWS Organization. SCPs are different from both identity-based and resource-based policies, which grant permissions to users, groups, and roles. However, SCPs do not actually grant permission themselves. Restrictions made within an SCP set a boundary of permissions for <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> accounts.</p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS, and EC2 via an identity-based policy. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2, despite having full access to S3. The SCP would serve to prevent that service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<p>So to be clear, an SCP does not grant access. They add a guardrail to define what is allowed. You will still need to configure your identity-based or resource-based policies to identities, granting permission to carry out actions within your accounts. If you want to use Service Control Policies to help you manage your security at an account level, then you need to ensure that you deploy AWS Organizations using the enable all features setting.</p>
<p>Before you start using SCPs, you first need to enable them from the root account of your organization. However, you need to ensure that you have the following permissions. From within your AWS Organizations console, navigate to the Policies tab and select the Service Control Policies option that will show the status of Disabled. Then, select the option to enable the SCPs as shown. At this point, SCPs will now be enabled at the root level of your organization and you can now begin to use Service Control Policies within your organization.</p>
<p>I now want to perform a demonstration showing you how to create a Service Control Policy and attach it to an account. In this demonstration, I will have two accounts; my master account called Stuart and another account called CA-Demo. In the CA-Demo account, I have a user, Alice, who has an IAM policy attached that allows full access to S3. From within my master account, I will create a new Service Control Policy denying access to the Amazon S3 service. I will then attach this new SCP to the CA-Demo account. I will then log in as Alice and try to access Amazon S3 to see the result. Let’s take a look.</p>
<p>Okay, so to start this demonstration, I’m logged in as the user Alice in this CA-Demo account which is my member account to my organization. And as I said previously, Alice only has access to Amazon S3. So if we take a look at S3, we should be able to get into the service without any issues. So we can get into the service which is great and also create a bucket as well, so a bucket for Alice. And there we have it. So this user Alice is able to access S3 and also create buckets as well.</p>
<p>Now what I want to do is swap back over to my master account of my AWS Organization, create a Service Control Policy to block access to Amazon S3 and then apply it to this account. So let’s do that next. Okay, so I’m now back in my master account and what I want to do is to create a new Service Control Policy. So from the AWS Organizations dashboard, if I click on Policies, now I already have my Service Control Policies enabled, if yours says disabled, simply select it and then you’ll have the option to enable it.</p>
<p>Once your Service Control Policies are enabled, if you select them, now we can create a policy. We have a default policy here which is FullAWSAccess which allows access to every operation and this is the default Service Control Policy that’s applied to the root of my AWS Organization. However, what I want to do is deny access to one of those services. So if I select create policy, I’ll just call this Deny S3, give it a description of deny access to S3, and then down here is where we can build our policy. So there’s different options.</p>
<p>Firstly, the statement and then the resource and any conditions. So to start with, we can see that over here our policy is in a deny state. Now if we wanted to allow access to a service, we’ll change that to allow. But I’m gonna leave that as deny and over here I wanna select my service that I want to deny so let me scroll down to S3, if I can just find it in the list, there we go, and I want to prevent all actions of S3. Now as we can see, it’s now updated this policy with the action of all actions to S3, but we don’t have the resource yet. So let’s now scroll down to add resource. So our service is S3 and our resource type would be all resources. So if I select add resource, we can now see that the policy has been updated again. So at the moment, it’s denying any action in S3 for all resources.</p>
<p>Now if I wanted to add a condition, then I could add any conditions in here with the condition key and qualifier, et cetera. For this demonstration, I’m not going to add any conditions.</p>
<p>Now I just need to create the policy. Okay, so that’s our policy created. Now although the policy has been created, it’s not actually attached to any organization unit or account thus yet. So let me go back to my accounts. Now if you remember from a previous demonstration, we had our member account, the CA-Demo account, within the Test OU. So I’m going to select that account, go across the Service Control Policies, and then we can see that we have this policy here that we just created and I want to attach that to this specific account. So if I select attach, we can now see that this account has two Service Control Policies, the FullAWSAccess which is filtered down from the root which allows access to all S3 resources, but we also have a Service Control Policy here that denies access to one of those services and the deny will always overrule an allow.</p>
<p>So now we’ve attached that Service Control Policy to the CA-Demo account which is the account that Alice is a part of. Let me now log back in as Alice into this account to see if I can now access S3 or if the Service Control Policy has been applied. So let’s take a look.</p>
<p>Okay, so I’m back in my CA-Demo account which is the member account. I’m logged in as Alice. So now if I go to S3, let’s see what happens. We have an error of access denied and we can’t see any buckets. So it looks as though that Service Control Policy has taken effect because I can’t access S3 at all. So let me see if I can create a bucket, if I just add in any name, and try to create, again we get an error of access denied. So we can see that the Service Control Policy has in fact now blocked access for Alice despite her having IAM permissions allowing her full access to S3. You would have noticed that in my list of SCPs from the demonstration I just carried out there was a pre-existing SCP there called FullAWSAccess and this was associated with the root object of my Organization, so how does the inheritance of SCPs work? Well, let’s take a look at an example.</p>
<p>Let’s suppose we had the following AWS Organization layout, with the FullAWSAccess SCP at the root. The objects within an Organization follow a parent-child relationship, with the Root being the parent to all other child objects. As you can see at the root level, we have an SCP that allows full access to AWS. I now want to establish what SCPs each of the five AWS accounts, highlighted in orange, is governed by. So looking at Account 1, looking from the root, we have the FullAWSAccess SCP. The next level down to Account 1 is the Dev OU. Now, this has four SCPs associated. However, these are number 2, 3, 5 and 6. So at this stage, only the services and features within these SCPs are allowed at this level.</p>
<p>Now if we go down further to Account #1, we have another OU, the Test OU, which again is governed by a list of SCPs, 3, 5 and 6. We can see at this level of the tree, SCP 2 has been dropped so this OU is now restricted to SCP 3, 5 and 6. Therefore, Account 1 which is a child of the Test OU is restricted and governed by the details set out in these three SCPs. Using this methodology, we can now look at the other accounts. So Account #2 is governed by SCP 2, 3, 5 and 6. Account #3 is governed by 1 and 6. And Accounts 4 and 5 are governed by SCP 4.</p>
<p>Let me now look at a different example, this time I’m going to remove the default SCP of FullAWSAccess at the root and replace it with custom SCPs as shown here. Now I’m going to assume in this scenario that every SCP shown has different service restrictions. So as you can see, at the root level this time we have four SCPs, number 1 through to 4, so let’s see how this affects the results this time for each account. So Account #1, looking from the root, we have SCPs 1 through to 4, the next level down to Account #1 is the Dev OU. Now, this also has four SCPs associated. However, these are number 2, 3, 5 and 6. As a result, this Dev OU is only controlled by SCPs 2 and 3. SCPs 5 and 6 are discarded as they are not a part of the parent relationship with the Root object, and 5 and 6 do not exist at any parent level.</p>
<p>Now if we go down further to Account 1, we have another OU, the Test OU, which again is governed by a list of SCPs, 3, 5 and 6. We already know that 5 and 6 are not allowed as they are not in the parent chain. However, SCP number 3 is. So SCP 3 exists from the root downwards in this OU. Therefore, Account #1 is restricted and governed by the details set out in SCP 3. Using this methodology, we can now look at the other accounts. So Account #2 is governed by SCP 2 and 3. Account #3 is governed by SCP 1. And Account #4 and #5 are governed by SCP 4.</p>
<p>Finally, before I end this lecture, please be aware of some of the characteristics of Service Control Policies. SCPs do not affect resource-based policies. They only affect principals managed by your accounts in your organization. SCPs affect all users and roles, in addition to the root user. However, the root user will still be able to change its own password including MFA settings, manage root access keys, and manage x.509 keys for the root user. If you disable SCPs in your organization, all SCPs are deleted and removed. Re-enabling SCPs again in the same organization will revert to the default SCP allowing FullAWSAccess. The following elements are not affected by SCPs: any actions performed by the master account, SCPs do not affect service-linked roles, and managing Amazon CloudFront keys.</p>
<p>That now brings me to the end of this lecture and to the end of this course. You should now have a greater understanding of how Service Control Policies can be used within your AWS Organization to help you centrally control the different levels of access between multiple accounts. If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="AWS-Control-Tower"><a href="#AWS-Control-Tower" class="headerlink" title="AWS Control Tower"></a>AWS Control Tower</h1><p>There comes a moment in every administrator’s life, when they see the ever-increasing burden of their users piling upon them, more and more accounts just keep joining the company, in ever quickening pace, how am I supposed to keep up with all this? The administrator screams to this guy as they create yet another developer account, trying desperately to wire up all the permissions as fast as possible.</p>
<p>The admin knows that these new database issues won’t fix themselves, so they’ll need to hurry, or how about the poor security auditor who is constantly having to look through production accounts to make sure that we’re following best practices, do they already have an audit log in created for them or will they have to bother the overworked administrator from before? Adding yet another task to the ever-growing pile, and even then is the other supposed to check every production account by hand to see if their EBS volumes are actually encrypted? Why isn’t there an automated way to check for all these things? The poor security person cries into their keyboard for the fourth time today.</p>
<p>And finally, when we have to keep track of all the log files from every security service that touches customer data, where does that all get stored? Are they on separate accounts for each of these production workloads? Do you aggregate everything into a locked vault somewhere? God save us from the crushing burden of all these text files.</p>
<p>Anyways, I think you get the point, there’s an enormous amount of work to consider when you’re administrating, securing or just simply creating an organization within AWS, there have also been a few ways over the years that AWS has recommended doing it, starting originally with just using IAM and hosting new users within your root accounts, creating policies, users and groups, and roles to divvy up permission, we were then given AWS organizations an extremely helpful service that allows us to administer large volumes of these accounts, organizations gave us ability to add accounts into groups and apply policies between them, these service control policies help to control and protect data and our users from portions of AWS, we didn’t want them to have access to, they had the power to override local IAM permissions, regardless of what that particular account actually allowed internally with IAM.</p>
<p>Well, we have now moved on to the next stage of this progress, AWS has evolved something new and I’d like to grab a little bit of your time today, to discuss probably the simplest and most powerful way to date to create, govern and administer large numbers of user accounts within AWS, and that would be with AWS Control Tower. What is AWS Control Tower? AWS Control Tower is a service that offers a larger and more controlled method of creating, distributing, managing, and auditing multiple accounts.</p>
<p>Originally these tasks were relegated to multiple different services that each dealt with an individual piece of the puzzle, some tasks like auditing didn’t even have a specific service to really help drive them forward, creating a large multi-account system from the ground up that deals with these scenarios might take an individual weeks to properly create, with AWS Control Tower we can manage all of these disparate services right within one area.</p>
<p>One of the first main topics we need to discuss for AWS Control Tower is the concept of landing zone What is a landing Zone? A landing zone is a multi-account architecture that follows the well-architected framework and is based around the ideas of security and compliance best practices. Your landing zone will be automatically created by AWS Control Tower and it is inherent part of the service, your landing zone is created from a series of best practice blueprints that help us setting up systems that deal with identity, federated access and overall account structure, these blueprints do the following on your behalf, they create a multi-cloud environment using AWS organizations, there are three organizational units that are provisioned here, the Root OU, this will be the parent OU that contains every other OU, within your landing zone, the core OU this OU contains the log archive and any audit member accounts, we generally refer to these accounts as the shared accounts and a custom OU, this OU another member OUs contain the actual working accounts that your users need to perform whatever duties they do within your AWS environment, for example your developer AWS accounts would sit within this OU.</p>
<p>The service also builds out two shared accounts, a log archive in the account which will be the place where all the logs will be sent between all accounts, it will store the logs of all API calls and resource configurations for every account within the landing zone and an audit account which has a restricted account that has been created to give your security and compliance team members read and write access to any account within your AWS landing zone.</p>
<p>From this account you’ll have programmatic access to review all other accounts, by way of a role that grants use of Lambda functions only, this account does not allow you to log into other accounts manually, which is good. Control tower will also provide identity management with the use of AWS single sign-on, default directory, this directory will house all of the AWS SSO users, you can also use it to define the scope or permissions available for each of those users. It will also provide federated access to those accounts, using AWS SSO directly, control tower then hooks up centralized logging from AWS cloud trail, and into its config, which is stored securely within AWS three and the logging account.</p>
<p>Finally it enables cross account security auditing using AWS IAM an AWS SSO to allow the audit account to perform routine checks as it wishes. In the end AWS Control Tower will have all of that automatically set up for you that is incredible, the amount of time it might take you to create all that by hand would probably be measured in weeks if done by a single person. The service can even create pre-configured groups such as your admins, users, and auditors, you can of course create more designations that provide different levels of access based on your needs.</p>
<p>Additionally, if you have your own active directory service already going, you can plug that into AWS SSO and configure it to work directly with your system which again can be a cloud-based AD or when you’re hosting on-premises, just make sure to use AD connector or the AD service. Safely managing your landings on resources, when you create your new landing zones using AWS Control Tower, there are large number of AWS resources that are created on your behalf, you need to be very careful about deleting or removing these pre-configured resources, if you were to destroy or tamper with these resources you can send control tower into an unknown state, AKA, it can break, so in very simple terms, do not modify or delete any of the AWS identity and access management roles that were created within the shared accounts, the auditing account and the archive account, otherwise bad things can happen.</p>
<p>Now, if you do so accidentally, it’s not the of the world later on, we’ll discuss how to fix these problems, additionally, it is very important that you do not disallow usage of any regions through service control policies or with AWS simple token service. Administration tips for setting up your landing zones. You should be setting up your landing zones in the region where you do most of your work, this region is known as your home region with that in mind make sure you deploy your new accounts from within that home region, if your architecture is multi-region again, keep it simple, put your landing zone and whichever region is the primary region, do not move the audit bucket or other buckets that were created when you launched control tower from your home region, this can cause issues down the line, and when you first launch your landing zones, AWS STS end points need to be activated in the management account in all regions supported AWS Control Tower if not, it might fail midway through the creation process.</p>
<p>Guardrails, even if AWS Control Tower only deployed the landing zones for us with all of its associated accounts and features, I would be happy, however, that is not all that AWS Control Tower helps deploy on your behalf, In addition to everything we’ve just covered, Control tower also deploys guardrails, guardrails is an appropriately named service that helps to keep all of your users accounts and everything under AWS is control tower and compliance with basic security regulations, overall, a guardrail is a fairly high-level security rule that provides continual oversight, they implement preventative or detective based policies that can help you govern your accounts, users, groups, and resources, the best part about these guardrails is that they’re written and meant to be understood in plain English.</p>
<p>Unlike other types of control policies be it service control policies, or IAM user policies, guardrails are very straightforward to understand, here are three simple examples, enable encryption at rest for log archive, enable access logging for log archive, disallow changes to CloudWatch logs, log group, just by reading these titles of the guardrails you already have a great understanding of what they’re supposed to do.</p>
<p>Now, of course, these are all built on top of existing AWS infrastructure and do have an associate SEP lying underneath them, but all of that is managed by AWS and they will keep that site up-to-date for you, here’s an example of what these look like in their normal form. Guardrails are applied to an entire organizational unit just like normal SCPS and therefore are also added to each account underneath that organizational unit, so that means that any accounts you add within your landing zone will be effective by the guardrails that you have in place, in general, guardrail’s there to help express your policy intentions.</p>
<p>Three types of guardrails, when using AWS was control tower we have three types of guardrails that we need to know about, first we have the mandatory set of guardrails, these are automatically added to your landing zone accounts upon creation and can not be disabled, in total there are 22 mandatory guardrails and you can check here for a full list of them and what they do. In general, though the mandatory guardrails are there to deal with basic security concerns, dealing with these shared accounts, to summarize them quickly, they help with enforcing logging, allowing the right services and all regions to facilitate logging, encrypting logs, stopping people from turning off your logs and stopping people from changing anything that AWS Control Tower set up.</p>
<p>Overall the mandatory guard rails don’t do anything super controlling and are defensive in nature. Second, we have a series of strongly recommended guardrails, they are designed to enforce some of the most common best practices for well-architected and multi-account environments, these guard rails are not mandatory and can be enabled and disabled as you please, these optional guardrails are not enabled initially, allowing you to choose what level of security and scrutiny you want for your environment.</p>
<p>Here are a few examples of the strongly recommended guardrails, disallowing creation of access keys for the root user, disallowing public access to Amazon RDS database instances, disallowing public right access to Amazon S3 buckets, there are a total of 13 strongly recommended guardrails in general they try to lock down, read, write, and control access to accounts and services that are commonly compromised.</p>
<p>Feel free to look here for more information about them. Finally, we have the elective guardrails, they continue to help lock down your accounts and can help keep track of the most commonly restricted actions with an enterprise environments, these guardrails are also not enabled by default, there are only five elective guardrails at this moment, so I’ll just name them quickly, disallow cross-region replication for Amazon S3 buckets, disallow delete actions on Amazon S3 buckets without MFA, disallow access to IAM users without MFA, disallow console access to IAM users without MFA and disallow Amazon S3 buckets that are not version enabled.</p>
<p>As you can see, they’re fairly common and highly recommended additions to the security of any architecture, just to finish up about guardrails, it’s important to reiterate that when you turn on these guardrails, they can and will create resources within your AWS account do not modify or delete these resources, otherwise it can set false flags for your guard rails. Provisioning counts for your users, we have spent a lot of time so far looking how control tower helps you set up a robust, secure and easy to audit environment through the use of shared accounts and guardrails, this next topic I wanna cover will explain how to actually add users accounts into the system in the safest and most scalable way possible.</p>
<p>AWS Control Tower has three methods for creating new member accounts, you can add new accounts directly through the enrolled accounts feature which lives directly inside AWS Control Tower, you can create new accounts through the account factory which is included as a part of AWS service catalog, finally, you can create new accounts from your management account using Lambda code and the appropriate IAM roles, I’m not a real fan of this option as it’s not streamlined to my liking, but it does exist and allows you to create an ad accounts programmatically.</p>
<p>Let’s take a moment to look at each of these scenarios to see what is gained or lost in each of these circumstances. Enrolling new accounts, the account enrollment feature of AWS Control Tower provides probably the most straightforward way for your administrators to add new accounts into an organization, these new accounts will be governed by control tower and any guardrails you may have set up.</p>
<p>One caveat for enrolling new accounts is that your landing zone must not be in a state of drift. Drift is where some of the assets within your landing zone have fallen out of compliance and must be updated, this can happen accidentally over time or might have occurred on purpose, to fix a time sensitive issue either way, once you have brought everything back into alignment you can create an account using this method.</p>
<p>To actually create an account, it’s fairly simple, navigate to the account factory page of AWS Control Tower, select enroll account, fill in the appropriate information under the create account section, this will include fields like account email, account name SSO username, and organizational unit, and finally click enroll account, the actual account creation might take up to several minutes to complete, if you do not have this option available make sure you are signed into an IAM user that has access to the AWS service catalog end user full access policy, if you’re still having issues make sure you’re not signed in as root.</p>
<p>Account factory, account factory allows your cloud administrators and your SSO end users to create new accounts within your control tower landing zone AWS refers to this method of account creation as the advanced account provisioning method, they recommend that you use the enrollment feature from earlier.</p>
<p>Account factory creates new accounts so the use of another service called ADSL service catalog, AWS service catalog is an organizational tool developed with the purpose of making provisioning and creation of IT tacks easier for both the end user as well as your IT admins, these stacks can include almost everything under the AWS sun such as easy to instances, databases, software and all the underlying infrastructure to create multi-tiered applications and architectures, service catalog allows your end users to select the content that they need from a list of pre-approved services that your IT or admin teams set up ahead of time, this helps to bring down those barriers of entry for content creation, as well as helping to keep best practices and system security a key component of any development.</p>
<p>Background information aside, to actually go about creating a new account within account factory, we have a few steps to follow, sign into your SSO user portal and from applications select AWS account and from your selection of accounts choose the management account, an AWS service catalog and user access, click on management console, make sure you’re in the correct region, this should be your AWS Control Tower main region search for and choose service catalog, click on the products list, select AWS Control Tower account factory, and then the launch button, now fill out the appropriate user information here, choose next until you get to the review section, now this is very important, do not define tag options and do not enable notifications, this will cause the account provisioning to fail. Review and launch, do not create a resource plan, this will also cause the account provisioning to fail, after that you’re all done.</p>
<p>This method will also take several minutes to complete. Using Lambda, is also possible to do all the above in an automated way, using Lambda, you can have your function create and provision account for you, you must do this for the management account that runs your control tower however, some notes, if you are automating control tower functions like account creation, your identity that is performing the work must have the AWS policy, AWS service catalog, and user full access, as well as the following permission.</p>
<p>This direction is fairly complex and might require a bit of back and fiddling to get working for your specific requirements that aside however, if you do really want to know how to get automatic account creation working just by uploading a CSV file then into an S3 bucket, please check out this blog link by AWS.</p>
<p>Unmanaging a member account, there might come a time after you’ve created an AWS account that need to release it from direct purview of AWS Control Tower, although this might be an infrequent issue, it’s fairly easy to accomplish, releasing an account or unmanaging it, as it officially called, can be done through AWS service catalog, simply log into a SSO user, that is part of the AWS account factory group, from there you can open service catalog, select provision products list, choose the name of the account that you wish to release, choose terminate, this doesn’t mean destroy, it just means removed from the organizational unit and your landing zone, thanks Amazon for making that clear and just give it a few minutes and the account will be unregistered, just to be clear however, this does not remove the users access to the account, it just unregistered it from control towers oversight, if you wish to clear SSO access to that account, you’ll need to change the settings and AWS SSO by changing the user email for that account.</p>
<p>Resolving and detecting drift within AWS Control Tower. When you create your initial setup within AWS Control Tower your landing zones, accounts, and all other organizations will be in compliance with your rules and guardrails that have been set up, as you work with and keep adding new accounts, identities and all the other good stuff to your AWS Control Tower system, you will, at some point experienced drift, some of the drift will be accidental, of course but other drifts might’ve been created on purpose to fix those time-sensitive issues.</p>
<p>AWS control tower is able to detect these abnormalities and lets you know how far from center your setup has officially drifted. Continually resolving drift is the best way to stay up-to-date and in compliance with both your internal corporate policies as well as with any governmental regulations you must comply with, you will receive your drift detection notifications automatically through Amazon SNS, these are aggregated within the audit account that was created in your landing zone.</p>
<p>Remember account admins should subscribe to these SNS drift notifications, even though the drift detection is automatic, you must resolve the problems manually through the console most of the time, luckily most of these drift issues can be repaired by literally pressing a button on the settings page through repair button to be specific, most drift can be taken care of at your leisure, but there are a few examples we call major drift that should be dealt with as soon as possible.</p>
<p>If you delete the organizational unit, originally named core you’ll not be able to perform any additional operations until you prepare it, if any of the IAM roles that check for drift are missing or inaccessible, you will see an error requiring you to repair the landing zone, these roles are the AWS Control Tower admin, AWS was control tower CloudTrail role and the AWS Control Tower stack set role. And finally, if you delete the OU originally named custom, your landing zone will be in a state of major drift but you will still be able to use control tower, only one non-core OU is required for control tower to operate, but you should still fix this.</p>
<p>Edge case guidance for organizations and AWS Control Tower. There’s practices that you should be aware of when working with AWS Control Tower, I have already spoken about a few of these as we’ve gone through the course, but I’ll restate the important ones for extra dramatic flare, do not update any service control policies through AWS organizations that are being managed by AWS Control Tower, if you do, it can throw off the detection systems within guardrails and leave it in an unknown state, this might require you to re-enable those guardrails in order to fix them.</p>
<p>AWS recommends that you instead create a new service control policy and attach those to the organizational units, instead of editing old ones, you should also be careful moving outside accounts into AWS Control Tower doing so will cause drift to occur that must be repaired, to fix this type of drift, simply update the account within the account factory, you can find a full breakdown of that process over here.</p>
<p>Nested OUs are not accessible from AWS Control Tower, the service only deals with top level OUs. And finally, if you rename an account or OU that was created by a control tower, you must repair your landing zone to see the new name displayed in control tower, and again, you just hit the press the button.</p>
<p>Comparisons, one final note I want to touch upon as we come to the end of this lecture is the comparison between ADFS control tower and AWS Security Hub, both of these services deal quite extensively with managing your environment and keeping your services secure, the main distinction that AWS has mentioned between the services is that AWS Control Tower is the primary destination for cloud administrators, helping you set up a secure and well-architected environment that sets your company off on the right path, AWS Security Hub is the primary destination for your security and compliance professionals, it allows you to see a comprehensive and timely analysis of your environments and gives you the option to take actions, based on these findings, these two services also sit on different sides of the security spectrum, AWS Control Tower has a more preventative approach, it tries to stop bad things before they happen through the use of guardrails and limiting your users while AWS security Hub exhibits a more detective-based approach providing reports and an analysis of your security posture allowing you to make decisions and root out possible vulnerabilities.</p>
<h1 id="AWS-License-Manager"><a href="#AWS-License-Manager" class="headerlink" title="AWS License Manager"></a>AWS License Manager</h1><p>Managing licenses in the cloud can become a headache for both asset managers and auditors. AWS License Manager has been designed to make the management and control of licenses with third party vendors such as Microsoft, SAP, Oracle and IBM when they are used both in the cloud and on-premises. It supports and tracks any software where the licensing agreement is set against virtual cores, VCPUs, physical cores, sockets, or a number of machines. AWS License Manager enables you to create license configurations which are made of multiple customizable rules that can be centered around the stipulations made by your different licensing agreements. These different rule types can include license counting type and this defines how your licenses are counted, by vCPU or physical core. Minimum and maximum allowed number of vCPUS or physical cores. This is dependent on the previous rule and the counting type but it essentially sets a threshold value for vCPUs or physical cores. License count, this specifies the number of licenses used within the license configuration. License count hard limit, when I hard limit is set it will block the launch of an instance that would make a breach of license amount. If you set a soft limit it will allow the launch of the instance however it will send a notification alert of the issue. Allowed tenancy, here you can specify which EC2 tenancy can consume a license with the configuration such as shared tenancy, which is the default, dedicated instance or dedicated host. </p>
<p>These rules are evaluated against your EC2 computer resources based on the software running on them to assess if your environment has reached its licensing limits on your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/highly-available-systems-sysops/domain-one-ec2/">EC2 instances</a>. AWS License Manager is currently supported and integrated with EC2 instances, dedicated instances, dedicated hosts, spot instances, spot fleets, and also auto-scaling groups. The customized rules help to minimize licensing breaches and depending on the configuration the EC2 instance can be prevented from being launched if there is a breach or it can send notifications to the appropriate team informing them of the limitations. AWS License Manager is integrated with AWS Systems Manager and AWS Organizations, allowing you to monitor your license requirements across multiple AWS accounts, plus on-premise environments. This allows you to monitor your licenses for your software vendors via a single account in the dashboard view. In addition to this, if you purchase resources from AWS Marketplace then you can also integrate Bring Your Own License to AWS License Manager as well. </p>
<p>So in essence, AWS License Manager provides a means of addressing, tracking, monitoring and managing licenses in a centralized location for on-premises and multi account AWS environments across multiple third party vendors using customized rules. </p>
<p>That brings me to the end of this lecture. Next I will be looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-compute-services-2019-reinvent-reminders/reinvent-c-amazon-elastic-inference/">Amazon Elastic Inference</a>.</p>
<h1 id="AWS-Service-Catalog"><a href="#AWS-Service-Catalog" class="headerlink" title="AWS Service Catalog"></a>AWS Service Catalog</h1><p>AWS service catalog is an organizational tool developed with the purpose of making provisioning and creation of IT stacks easier for both the end user as well as your IT admins. </p>
<p>These stacks can include almost everything under the AWS sun – such as EC2 instances, Databases, software, and all the underlying infrastructures to create multi-tiered applications and architectures. </p>
<p>Service Catalog allows your end users to select the content that they need from a list of preapproved services that your IT or Admin teams set up ahead of time. This helps to bring down those barriers of entry for content creation, as well as helping to keep best practices and system security a key component of any deployment. </p>
<p>Before service Catalog, your End User might have needed to ask their IT or Admin professionals to help them provision or determine what services were and were not within their scope of work. This could of course be achieved by limiting accessibility through IAM on a job role by job role basis. It is a bit tedious, however, and becomes a constantly changing battle, especially if you have many different roles within your company. </p>
<p>Now with AWS Service Catalog, we have the ability to browse through a list of ‘Products’ which are just a set of pre-approved services - and provision what we need from there. By setting up our system in this way, we can create and build with the full confidence that we as developers are creating solutions using only acceptable components that our security, administration, and leadership teams approve of. And on the Admin side, I’m not constantly being bugged with approval requests from every developer and department under the corporate sun. I think you can see the point, so let’s dive in a bit.</p>
<p>This is an important question because ‘products’ are the heart and soul of the AWS Service Catalog. Products are an IT service that you want to make available for deployment on AWS. </p>
<p>A product can consist of just a single AWS Resource or can be comprised of multiple items such as EC2 instances, Their associated EBS volumes, Database that you want them connected to, and all the monitoring capabilities you would come to expect from these services within the cloud.</p>
<p>A product can even be a package listed on the AWS Marketplace. For example, this could be helpful if you were using a database that AWS does not natively support but was available on the marketplace.</p>
<p>In the end, a product is a service. It can range from something as small as a single instance doing basic web hosting to an enormous multi-tiered web application.</p>
<p>Creating a product is actually very simple. AWS service catalog requires you to upload AWS Cloud Formation Templates, and from these templates, the service will add that entire stack into the catalog as a single product. Again that product can be something as small as a single EC2 instance, or a very large multi-tiered web application.</p>
<p>When uploading your products, you have the ability to add detailed product descriptions, any version information, and required support details and tags. These descriptions can be very helpful for your end users, allowing them to know who to contact if something goes wrong with the product.</p>
<p>Tags are very important for keeping track of cost. They can also be used to create metrics and analytics for your most commonly used product, Helping you to determine what your End Users are actually consuming. So, Make sure you appropriately tag your products during this phase. </p>
<p>You also have the ability to update any products you have already created. New versions of a product are added almost exactly how you built the initial product in the first place. Simply go to your product - click add new version - and upload your updated Cloud Formation Template to override the old product.</p>
<p>Once the new version is fully uploaded, and your product is published, your End User will have the option to update their running stacks to this version. You also have the option to deprecate old versions of a product. Your users will still have the ability to update but will not be able to provision more instances of that version.</p>
<p>Products can even be customized and have different deployments based on options you specify. Let’s say you wanted your product to use different ec2 instances based on the number of daily active users expected. When you set up your cloud formation template that describes your product, you can set predefined business-level input parameters. These parameters include questions you can ask your End Users when they select the product for deployment. Each question can have a predefined list of answers. Based on their answers to these questions you could completely change the architectural configuration of the product. That’s pretty Neat.</p>
<p>Now that we have a handle on what a product is, and how to create them, the next step within service catalog is to add these products into portfolios.</p>
<p>A portfolio is a collection of products with configuration information, that helps in determining who can use the products within. Each portfolio requires a name, description, and a product owner. That last one is very important because if something goes wrong with an available product it’s important to know who to send your complaints to. And just like with products, you can also tag each of your portfolios for all the normal good tagging reasons.</p>
<p>Portfolios can also be customized for each type of user within your organization, having individual controls and allowances per individual, group, or team. </p>
<p>When you are ready to publish your portfolios to your end users, you will need to add IAM users, groups, or roles to the portfolio from the console. This will allow them to actually see your finalized products within their Service Catalog.</p>
<p>You can also share portfolios between other AWS accounts, and give the administrators of those accounts the ability to add their own products to your portfolio. This could be useful when you have teams that operate independently, that each deal with creating their own products. You would want their product managers or admins in charge of when new versions are available and to whom they can be provisioned. All you need to do to allow sharing for your portfolio is specify the account id you want to share with (within the service catalog console) and provide the </p>
<p>One of the best features and really the whole goal of service catalog, is that you have full control over what your end users have access to. From an administrator perspective that is incredibly powerful and can help you maintain high levels of both security and credibility.</p>
<p>AWS Service Catalog allows you to apply constraints on the products within your portfolios. These constraints allow you to limit the scope and ability of your products based on pre-defined settings. They also allow you to have additional functionality, at least on the administrative side. When you apply a constraint, they become active as soon as you create them, however, they are not retroactive. </p>
<p>So, what can you do with a constraint? Well, there are a few types of constraints out there that we need to look at. First, we have the Launch constraint. This constraint specifies a specific AWS Identity and access management role that AWS service catalog will use when an End User decides to launch one of your products. By having a launch constraint, you are allowing the service to do the heavy lifting of creating the products instead of your user. This means that your users do not need to have all the permissions required to create your product, such as the ability to use cloud formation - which you probably didn’t want your new junior intern to have access to, for example. </p>
<p>With that in mind of course, the IAM role you apply to the product as a constraint needs to have the following permissions: AWS Cloud Formation, All the services used by that Cloud Formation Template, And read access to s3 bucket that houses the cloud formation template.</p>
<p>Notification Constraint. This constraint allows you to receive Amazon SNS notifications about stack events related to your product. These events are all created from deployment of the cloud formation template that specifies your product.</p>
<p>These are nice to have as an administrator because you can see what is being deployed and where. As well as being able to see if your products are deploying successfully or not. </p>
<p>Tag Update Constraint. This constraint specifies whether or not the user can update the tags on the given resources created by your product. This can allow for more specific tagging per group or user without you having to update the tags for them. However if you have good reasons to have the tags applied to the product or portfolio that are currently on there, maybe avoid letting the users have this ability.</p>
<p>Stack Set Constraints. This constraint gives you the option to configure where you want your products to launch. Specifically, you can control what regions and accounts you would like them available in. From there your End User can manage those accounts and determine where the product deploys and the order of those deployments.</p>
<p>Template Constraints. Template constraints allow you to limit what options that are available to your End User when they are provisioning your products. These options might include forcefully limiting the availability of certain ec2 instances based on the specific user group. You might need to do this if the parameter’s values in your cloud formation template are too broad for the target user group. In this situation, you can define a template constraint to narrow the band of available options.</p>
<p>This can be especially useful when you want your users to be able to use a certain product but to do so within the bounds of some compliance or regulation requirements set by your organization. These template constraints are bound to whatever portfolio they are in and do not cross over to other products in other portfolios.</p>
<p>You can also use template constraints to stop your users from inputting impossible, or obviously incorrect values. For example, you can create a rule that validates that a given subnet is in a particular VPC. This rule will be checked before creating or updating the stack, preventing CloudFormation from failing to create it.</p>
<p>Check out this example that makes sure the users are selecting an appropriate instance type for their workload.</p>
<p>Here we can see this constraint is setting the available instance type based on the environment the user is trying to set up. If they are trying to create a small test environment, the rules limit them to only using m1.small instances. However, if they are going to create a production environment, we don’t want the end users to be subjected to the slow speeds and underperformance of an m1.small, so let’s make sure they can only create m1.large instances.</p>
<p>After your End Users have created their applications using the products you have set up, they might have additional needs and requests. Some of these requests are repeatable and are general ‘day to day’ service level operations that need doing. For example, it would be quite common for someone to need a database backed up, but they don’t have the technical expertise or the trust level appropriate with being given access to the underlying RDS service.</p>
<p>This is where a Service Action can really shine. A service action is an End User available action - that you have explicitly given permission to use. Service actions allow you to give back some control to your end user, and helps to remove blockers from your operation. For this example, the service actions could be starting that RDS backup. </p>
<p>Service actions can enable your users to perform their own operations tasks: such as troubleshooting, running service level commands, or even request permissions to run additional products through service catalog. Service actions are also product-specific, so you can define exactly where you want them available. Behind the scenes, Service actions are based on AWS Systems Manager automation documents.</p>
<p>When working with service catalog, as well as with most aws services, pricing is of course a factor you should be aware of. There is a free tier for this service that lets you have up to 1000 API calls per month at no charge. When you start to exceed this number you will be charged at a rate of 1 cent per 14 API calls.</p>
<p>I think this is a pretty reasonable rate, as this is not a service that will be called all that often. Basically, you will only be paying for when people are thinking about provisioning new resources. Most of the time your users will be developing or actually using the product itself.</p>
<h1 id="Introduction-to-AWS-Systems-Manager"><a href="#Introduction-to-AWS-Systems-Manager" class="headerlink" title="Introduction to AWS Systems Manager"></a>Introduction to AWS Systems Manager</h1><p>Introduction to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/introduction/">AWS Systems Manager</a>. Welcome to this introduction to AWS Systems Manager to gain operational insights. In this segment, you will get an overview of AWS Systems Manager, including features and use cases. Systems Manager is a set of fully managed AWS services that enable automated configuration and ongoing management of systems at scale in a secure and reliable way across all your Linux and Windows instances running on Amazon EC2, your own data center or other cloud platforms. Its focus on automation enables configuration and management of systems where you can select the instances you want to manage and define the tasks you want to perform.</p>
<p>You can also define when modifications are to be applied by configuring a maintenance window. You can create and update system images, collect software inventory, apply system or application patches and configure Linux and Windows operating systems, also manage the state of your instances, all from the same console or the command line interface.</p>
<p>You don’t have to be concerned about setting up and managing different tools for different platforms. You also don’t need to configure secure shell keys, or secure shell or remote desktop ports or bastion hosts in order to establish connectivity to your instances. Systems Manager is built for cloud-type scalability, which uses agility and elasticity, allowing you to manage one or thousands of instances, no matter if they have long running or temporary workloads.</p>
<p>You also get AWS optimized native integration with the rest of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/systems-manager-is-a-good-fit-in-the-aws-tool-set/">AWS management tools</a>, such as Identity and Access Management for access control, CloudTrail for Auditing, CloudWatch events for event driven automation, and many other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> configuration and management tools.</p>
<p>There are no complex licensing models. Most of Systems Manager’s functionality is available at no charge. Systems Manager provides extensive building blocks and services where you can choose to build value of your own on top of these existing services.</p>
<p>In short, with Systems Manager, you can use automation to manage traditional and cloud workloads by performing essential setup, maintenance and management tasks, while maintaining complete visibility and control over your entire machine farm independent of operating system, location and number of instances.</p>
<h1 id="Systems-Manager-is-a-Good-Fit-in-the-AWS-Tool-Set"><a href="#Systems-Manager-is-a-Good-Fit-in-the-AWS-Tool-Set" class="headerlink" title="Systems Manager is a Good Fit in the AWS Tool Set"></a>Systems Manager is a Good Fit in the AWS Tool Set</h1><p>Systems Manager is a good fit in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Tool Set. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/introduction/">Systems Manager</a> fits well into your existing AWS configuration and management tool set. It provides you with the Systems Manager Explorer, which is a customizable operations dashboard that delivers a display of your operational data and work items for all your accounts and across regions. You can use Explorer to configure settings and preferences. You can customize the display using drag and drop and drill down capabilities to prioritize work items.</p>
<p>Finally, with Explorer, you can take action using automated runbooks if needed. Systems Manager also integrates with your Amazon CloudWatch dashboards and your Personal Health dashboard. You can now have a set of fully managed AWS Services that help you enable provisioning and operating of your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/systems-manager-requirements-and-building-blocks/">resources</a> at scale. If you’d like to know more about Amazon CloudWatch Dashboard, you could view our existing course at the link shown here on the screen.</p>
<h1 id="Managing-Resource-Groups"><a href="#Managing-Resource-Groups" class="headerlink" title="Managing Resource Groups"></a>Managing Resource Groups</h1><p>Systems Manager includes over 20 features and integrations, each with their own capabilities and functionality. Some of them are the Fleet Manager, Session Manager, Run Command, Parameter Store, Patch Manager, and State Manager, among others. Most of these features use Systems Manager documents to define the operations to be performed. They also use Maintenance Windows to define the date and time when those operations can take place. Together, they provide you a comprehensive dashboard and essential <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/systems-manager-is-a-good-fit-in-the-aws-tool-set/">tools</a> to set up and manage the life cycle of your instances. You can manage inventory and patch assets, run commands and manage desired state, and even securely connect to EC2 instances in private subnets.</p>
<p>In general, using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/introduction/">Systems Manager</a> entails grouping your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> resources, examining their relevant operational data via dashboards, and finally, take action to mitigate any issues reported. The instances to be operated can be selected using one of three general mechanisms. The first one is manually. This is where you select the instances that you want to register as targets individually, using the Systems Manager console. You can also use instance tags where you specify one or more tag key-value pairs to select the instances that share those tags. You can then save the results as a Resource Group to execute operations on the same set of instances in the future.</p>
<p>Finally, you can use Resource Groups where you can perform a query based on existing resource tags or choose an existing Resource Group that already includes the instances you want to target. Systems Manager operates on logical units of managed instances via Resource Groups. This is the most powerful way to define your targets for AWS Systems Manager to operate. In general, if you work across a range of different AWS resources that are related, AWS Resource Groups can help you organize them for better visibility and aggregation in terms of management, ownership and categories.</p>
<p>Resource Groups begin their life by defining common tags with key-value pairs describing the items in the categorization. A Resource Group is a collection of AWS resources in the same region that match a particular description. Resource Groups can be tag based, which represent a group of resources as being part of a development tier, production tier, a specific owner, a department, or dedicated for a particular project among many other possible categories. Systems Manager can also operate on Resource Groups that are based on CloudFormation stacks. These groups are resources created by the same CloudFormation stack in a particular region. The Resource Group will have the same logical structure as the stack. Systems Manager and Resource Groups allow you to create custom consoles that show organized and consolidated information about Resource Groups, and offer helpful visibility for operation and management.</p>
<p>As a default, the AWS Management Console shows resources organized by service category, as you may have already observed in the EC2 Management Console. The Tag Editor allows you to define tags and what will become a Resource Group. It allows for bulk editing and application of tags to resources in a specific region. The Tag Policy Editor can help enforce tagging across your resources in a particular account or the entire organization. You can manage Resource Groups and find the Tag Editor under the AWS Resource Group service in the Management Tools sections of your AWS Console. Also, as you provision resources on the console, a section of the provisioning will always permit you to define tags.</p>
<p>As you may have noticed, establishing the best practice of tagging your resources becomes essential in order for you to use and take advantage of the features of Systems Manager. As you <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/aws-systems-manager-requirements-and-building-blocks/">build</a> your fleet of instances, it is important to catalog these resources using tags. Later, it becomes significantly easier to group them and operate on them using Systems Manager.</p>
<h1 id="AWS-Systems-Manager-Requirements-and-Building-Blocks"><a href="#AWS-Systems-Manager-Requirements-and-Building-Blocks" class="headerlink" title="AWS Systems Manager Requirements and Building Blocks"></a>AWS Systems Manager Requirements and Building Blocks</h1><p>AWS Systems Manager, Requirements and Building Blocks. The SSM Agent. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-systems-manager-operational-insights-1699/introduction/">AWS Systems Manager</a> requires an agent for its management service. The Systems Manager Agent is the software required to be installed and configured on all instances in order for them to be called managed instances.</p>
<p>A managed instance is an instance with the ability to communicate and be operated by Systems Manager. The agent executes and process tasks you specify through any of the Systems Manager features, like the Run Command. The agent is installed by default on the Amazon Linux AMIs, the AWS Windows AMIs, and available on the Amazon Linux repo. The agent is open-sourced and available on GitHub. You can install the agent on a physical server or a virtual machine in your data center or even another cloud provider. You can manage Windows Server 2003 or later, and Linux distributions like Amazon Linux, Ubuntu, Red Hat Enterprise Linux, SUSE, and CentOS.</p>
<p>Managed Instance Roles. A managed instance will require an Identity and Access Management role applied as an instance profile in order for Systems Manager to be able to interact with the agent and make the instance visible in the Systems Manager Fleet Manager console. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> provides pre-defined managed policies for Systems Manager. They usually have the acronym SSM as part of their name. One of them is called Amazon EC2 Role for SSM, which can save you time in the instance configuration. You can also create your own custom role if needed or use one of the many other SSM-related policies available. To register servers and virtual machines in your data center or other cloud providers outside the scope of Amazon EC2, you can create a hybrid activation and use the activation code and activation ID supplied to configure the agent and centrally manage your hybrid environment and EC2 instances from one location.</p>
<p>Fleet Manager Feature. Once you configure a managed instance, you can go to the Systems Manager console. And under the Node Management section, you will see the Fleet Manager feature. All your managed instances will be displayed in this console. Fleet Manager will give you visibility into the details of each managed instance, including Instance ID, Platform Type, Instance Type, Operating System name, IP Address, and the version of the SSM Agent that is installed among many other features. One interesting item about the Fleet Manager managed instance console is that under instance action, you can connect to the instance using the Session Manager feature of Systems Manager.</p>
<p>The Session Manager feature of Systems Manager is a fully-managed capability that lets you connect to any managed instance using an interactive browser shell login for Linux, Windows, and MacOS instances. It requires no open inbound ports and no need to manage bastion hosts or Secure Shell keys for connectivity to your instance. You also don’t need Secure Shell clients for Linux, or Remote Desktop Protocol clients for Windows when using Session Manager. Communication between Session Manager and instances uses Transport Layer Security version 1.2, or TLS 1.2 for short. Security of the communication can be increased using your own Key Management Service keys. Session Manager tracks all commands and output produced in a session, and also provides full logging and session auditing activity that can be dispatched to CloudTrail, CloudWatch, or an Amazon S3 buckets as a result. Session Manager can control which users can access specific instances by using Identity and Access Management policies. It works through the interactive browser shell or using the AWS Command Line Interface.</p>
<h1 id="The-Benefits-of-Logging"><a href="#The-Benefits-of-Logging" class="headerlink" title="The Benefits of Logging"></a>The Benefits of Logging</h1><p>Hello, and welcome to this short lecture, where I want to discuss a few of the different benefits that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> can bring you and your infrastructure. For some, many people consider logging an afterthought, something that is implemented after it’s too late. This is often the case where an incident or breach of security has occurred that resulted in a delay of resolution and safeguarding of your environment. In hindsight of these situations, logging would have been a great idea to have had running and implemented in the first place to rectify the event quickly and efficiently, or even prevent it from happening in the first place.</p>
<p>So how can logging help?</p>
<p>Generally, logs are created by services and applications which contain a huge amount of information, which is recorded and retained on persistent storage, to be reviewed and analyzed at any time that it might be needed. Some logs can be monitored in real time, allowing automatic responses to be carried out, depending on the data contents of the log. From an auditing perspective, these logs are invaluable. They often contain vast amounts of metadata, including date stamps, source information such as IP address or usernames, and this is especially true when you’re looking at CloudTail logs. These logs can be used to help you achieve specific compliance certifications that require evidence of traceable and auditable actions that have been carried out. </p>
<p>Being able to resolve an incident as quickly as possible is paramount within your organization. Whether it’s a priority one, two, or three, being able to gain as much insight into what happened just before and just after the incident can significantly reduce your time to resolution. Using logs to ascertain the state of your environment before and after and even during the incident provides clarity and enables you to detect where the incident occurred, allowing you to pinpoint your efforts in a specific area. Quicker resolution results in a better customer experience for your organization. </p>
<p>By monitoring the data within your logs, you’re able to quickly identify potential issues that you want to be made aware of as soon as they occur. By combining this monitoring of logs with thresholds and alerts, you are able to receive automatic notifications of potential issues, threats, and incidents, prior to them becoming a production issue. By logging what’s happening within your applications, network, and other cloud infrastructure, you are able to build a baseline of performance and establish what’s routine and what isn’t. By having this baseline, you are able to identify threats and anomalies easier through the use of third party tools and management services. </p>
<p>To have a thorough understanding of what’s happening within your infrastructure provides a huge benefit to your operational teams. Having an inside look of how your infrastructure is performing and communicating helps achieve the previous benefits that I’ve already discussed, and having more data about how your environment is running far outweighs the disadvantage of not having enough information, especially when it really matters to your business in the case of incidents and security breaches. </p>
<p>That now brings me to the end of this lecture. There are many more reasons as to why you should be capturing data that can be logged. But I just wanted to provide a few key points to you. </p>
<h1 id="CloudWatch-Logging-Agent"><a href="#CloudWatch-Logging-Agent" class="headerlink" title="CloudWatch Logging Agent"></a>CloudWatch Logging Agent</h1><p>Hello and welcome to this lecture where I shall explain what CloudWatch Logs are, how they work and how they are configured. As we know CloudWatch is a monitoring service that is used to collate and collect metrics on resources running on your AWS account allowing you to monitor their performance and respond to alerts that meet to find thresholds. In addition to this, Amazon CloudWatch is a powerful tool that allows you to collect logs of your applications and a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. </p>
<p>When data is fed into Cloudwatch Logs you are able to monitor the logstream in real time and set up metric filters to search for specific events that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. Let me explain the different components of this feature to give you a better understanding of how it all fits together. Starting with the Unified CloudWatch Agent. </p>
<p>With the installation of the Unified CloudWatch Agent you are able to collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. Interestingly this metric data is in addition to the D4EZ2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. The agent can be installed on a number of different operating systems and at the time of writing this course the operating systems versions supported are as follows. </p>
<p>To install the agent and configure it requires a number of different steps. To install the agent on your EC2 instances you need to perform the following, firstly you need to create a role and attach it to the instance with permissions allowing CloudWatch to collect data from the instances in addition to interacting with AWS systems manager SSM. You then need to download and install the agent onto the EC2 instance. And lastly configure and start the CloudWatch Agent. The most efficient way of completing this installation and configuration is with the use of the EC2 systems manager service known as SSM. You will need to create two roles. One role will be used to install the agent and also to send the additional metrics gathered to CloudWatch. The other role is used to communicate with the parameter store within SSM, to store a configuration file of the agent which then can be shared with other EC2 instances. </p>
<p>From a security perspective it’s best that only one of your EC2 instances has this permission to write to the parameter store. Once the agent configuration file is stored on SSM there is no need for other EC2 instances to do the same. In fact once the write has been completed, this role should be detached from the EC2 instance and the other role applied which simply allows the agent to send data to CloudWatch. </p>
<p>The role with the additional permissions for SSM needs to be configured as follows when creating a role. The options, ‘select the type of trusted identity’ needs to be ‘AWS service’. The option to ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. The ‘Attach Permissions Policies’ needs to be ‘CloudWatch Agent Admin Policy’ and the ‘Amazon EC2 role for SSM’. </p>
<p>The role that is simply used to install the agent and send data back to CloudWatch needs the following configuration, the ‘select type of trusted identity’ needs to be ‘AWS service’. The option ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. And finally under the ‘Attach Permissions Policies’ it needs to be ‘CloudWatch Agent Server Polic’y and ‘Amazon EC2 Role for SSM’. </p>
<p>Once your roles are created you can then associate the role shown in orange here, which I shall call ‘CloudWatch Agent Admin Role’ to your EC2 instance that will store the configuration file in the parameter store. </p>
<p>From the EC2 instance with additional permissions that will be saved in the configuration file with in the parameter store of SSM, you must then install the agent which can be done using systems manager or it can be downloaded from an S3 public link either for Linux or Windows. However as mentioned earlier I will explain how to do this via SSM in particular the run command function. As a prerequisite to the CloudWatch Agent installation you’ll need to verify that your EC2 instance has access to the internet to communicate with SSM and CloudWatch endpoints. In addition to this you must also have the SSM agent installed. For some AMI’s as stated on the screen the agent may already be installed. For more information on how to install or update your SSM agent on your EC2 instance please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">link</a>. Let me now perform a very quick demonstration to show you how to install the actual CloudWatch agent on an Amazon Linux EC2 instance using the SSM role command. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration 1"></a>Start of demonstration 1</h3><p>Okay so I’m within my AWS management console in the EC2 dashboard and as you can see I have an EC2 instance here named Logging Server and I have it in a VPC that has internet access. I have the SSM agent installed because it’s based on one of the latest Linux AMI’s comes by default with this, and I’ve also attached the CloudWatch Agent Admin Role which I discussed in the previous section so I’ve met all the prerequisites to install the CloudWatch Agent. </p>
<p>Now what I want to do is use the EC2 systems manager to install the CloudWatch Agent itself. So if I load up SSM which now has it’s own console and on the left hand side if I go down to the run command and click on run command to create a new command. What I want to select for the command document is the AWS configure AWS package. So if you just select the entry and then scroll down. And then next I need to select which EC2 instance that I want as the target. So which instance I’m going to install this package on and here we can see the Logging Server so I would just highlight that. And as you can see it’s added our EC2 instance already up there. </p>
<p>Now we come down to the command parameters and the action I want to perform is an install. The name of the package is Amazon CloudWatch Agent and I want to install the latest version. You can add some other comments here and some timeouts, for this demonstration I’m just going to leave those default. If you wanted to perform this same installation via the AWS CLI then you can click on the AWS command line interface command and then based on the above parameters you can simply cut and paste this command and run that command. But for this demonstration I’m going to use the run command within SSM. </p>
<p>So if I click on run we can see here that the command ID was successfully sent. It’s currently in progress and that will just take a moment to process and install the agent, then we should get a return of successful. So if I go back to the main dashboard of the run command we can see here that it was a success. This is the package that we just run using using this command ID, so now the agent is successfully installed. </p>
<p>As you can see here I have a previous command that failed so I just want to show you that worst on here. So if we select that command and go to view details we can drill down to understand why this failed. And if we scroll down to the bottom you can see here that it failed. Select the instance ID and view the output. Now step one, we can dive deeper to see why this failed. Now if we scroll down to the error itself and here it said it failed to retrieve the manifest and it could not find the latest version of this particular package. Now what happened here was I didn’t enter the correct package name. What I entered was CloudWatch Agent instead as we performed in the demonstration just now. The correct name is Amazon CloudWatch Agent. So it couldn’t find the package that was available and that was the reason why it failed. </p>
<p>However, going back to one that was successful, we can see here, we can drill down into this just to make sure everything was okay. Again here, success and we can view the output of this as well like we done with the failed one. And we can see here that it was all installed. It found all the files and it went through and processed everything okay. And that’s it, that’s how you install the CloudWatch Agent using the SSM run command. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration 1"></a>End of demonstration 1</h3><p>On your first instance you’ll need to create the CloudWatch Agent configuration file. Without doing so, you will not be able to start the agent. This file stores configuration parameters that specify which metrics and logs to capture on the instance which are then sent to CloudWatch. It can be created manually or by using a wizard. If you create it manually then you have a much wider scope for capturing elements that are not included within the wizard. Let me show you via another demonstration how to configure the agent using the wizard. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration 2"></a>Start of demonstration 2</h3><p>So I’m now on my Logging Server instance where we installed the agent and now need to configure it. So to configure it we’ll run this command here and this will launch the Amazon CloudWatch Agent configuration wizard. And it just goes through a number of questions before it completes so let’s take a look. So the first question it asks which operating system your using Linux or Windows, take the default choice of Linux. And if we’re going to try and collect logs on EC2 instances or On-Premises So it’s EC2 for us. </p>
<p>We then have a question if we want to monitor any host metrics such as CPU, memory, etc, I’m going to set a default of yes. And if we want to monitor CPU metrics per core and here additional CloudWatch charges may apply. For this demonstration I’m going to say no. We then have a question if we want to add any EC2 dimensions such as the instance ID or the instance type into our metrics if the information is available. Say yes. Then we have a question about how often CloudWatch will collect these metrics. Whether that’s one second, 10 seconds 30 seconds or 60. I’ll accept the default of every minute, 60 seconds and then asks which default metric config we want whether that’s basic, standard, advanced or none. I’ll accept the default of basic for this demonstration. It then just shows you an example of that configuration that you just selected. If you’re happy with that you can say one for yes or two for no. And then select a different default metrics config so I will just select yes for this demonstration, I’m happy with that. </p>
<p>Now it asks the question if we have an existing CloudWatch log agent configuration file that we want to import. At the moment we don’t but what we’re trying to do is get in the position of creating one to then import into the parameter store of SSM. So at this moment our default choice is no which is correct. It then asks if we want to monitor any log files on this EC2 instance. I’m going to say no just for this demonstration because all we are trying to do is configure the CloudWatch agent at the moment. Our next question asks if we want to store the config in the SSM parameter store, and here yes we do cause we want to upload this file to the parameter store to allow all other EC2 instances to connect to the SSM parameter store to download the configuration file to prevent us from doing this on each and every instance. So I’m going to say yes which is number one that we do want to store the config in the SSM parameter store. </p>
<p>It then asks what default name you want to give this configuration file. And it just gives you a little message there saying you should use the prefix of Amazon CloudWatch hyphen if you’re using any of the AWS managed policies. So I would go with their suggested name of Amazon CloudWatch Linux and you shouldn’t run into any issues there. It then asks you which region you want to store the config in the parameter store in and it gives a default choice. I’m just going accept that default. It then asks a question about which credentials should be used to send the config to the parameter store. I’m going accept the default credentials there which should have access and we now have a message that it successfully put the config to the parameter store Amazon CloudWatch-Linux. And that’s it, that’s the end of the wizard so it’s a very simple and quick and easy wizard. And now your CloudWatch agent configuration file is configured and it’s been uploaded to the parameter store so now any other EC2 instances can click to that parameter store and simply download it and have the agent running very quickly and easily. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration 2"></a>End of demonstration 2</h3><p>Now the configuration file is configured and successfully copied to the SSM parameter store I simply now need to start the agent and again I will use the systems manager service to complete this file with the run command. Let’s take a look. </p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration 3"></a>Start of demonstration 3</h3><p>Okay so the final stage is to start the agent and again I’m going do this from the AWS systems manager so I’m at the systems manager dashboard. Again I’m going use the run command so so it’s over on the left hand side here. Click on run command. Then click on the orange run command button and this will allow us to start a new command for the command document. What we need to look for is the following, which is Amazon CloudWatch Manage Agent. And here we have the command document here. So I’m going to select that. </p>
<p>Scroll down, we then need to select our target instance and we have our Logging Server here. Now the command parameters, for the action we want to select configure rather than stop because we’re going to select the configuration file that we uploaded to the parameter store first using the EC2 mode rather than On-Premises. The optional configuration source which is SSM which is where we stored the configuration file so we’ll leave that as SSM. Now in the optional configuration location we need to enter the name of the file that we stored it as. And if you can remember that was Amazon CloudWatch-Linux. So we’ll paste that in. </p>
<p>Now under optional restart we want that as yes because it will then start the agent once it’s pulled the information from SSM. If we scroll down to the bottom simply click on run. And now we have it, the command ID was successfully sent. If we go back to our dashboard we can see that it was a success. And here the document name was the CloudWatch Manage Agent. So the CloudWatch Agent will now be running on that EC2 instance, the Logging Server. And it’s as simple as that. Now we have our logs configured and log data of our EC2 instance is being sent to CloudWatch along with the additional metric information. It’s now possible to search for specific entries within the logs for points of interest. </p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration 3"></a>End of demonstration 3</h3><p>That now brings me to the end of this lecture on CloudWatch logs where I explained how CloudWatch can be used to centralize login for more EC2 instances or applications running on your instances by determining logs past configured by the CloudWatch Agent and log groups within CloudWatch. Coming up next I should be talking about CloudTrail logs.</p>
<h1 id="CloudTrail-Logging"><a href="#CloudTrail-Logging" class="headerlink" title="CloudTrail Logging"></a>CloudTrail Logging</h1><p>Hello, and welcome to this lecture focusing on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> capabilities and configuration of AWS CloudTrail. You should already be familiar with what CloudTrail is and what it does. However, to quickly summarize: It’s a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface: CLI, from within the AWS Management Console, Or even from a request made by another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service. For example, when auto-scaling automatically sends an API request to launch or terminate an instance. These API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the new log file. </p>
<p>CloudTrail also records and associates other identifying metadata with all events. For example, the identity of the caller, which can be the user or the account that made the API call, the timestamp of when the request was initiated, and the source IP address. The logs generated are the output of the CloudTrail service and they hold all of the information relating to the API calls that have been captured. So as a result, it’s important to know what you can do with these logs in order to maximize the benefit of the data they contain. </p>
<p>So, what is a log file and what does it look like? Log files are written in a JSON format. Much like access policies within IAM and S3. Every time an API is captured, it’s associated with an event and written to a log. And new logs are created approximately every five minutes or so, but they are not delivered to a nominated S3 bucket for persistent storage for approximately 15 minutes after the API was called. So if you expect to see the log file for an API called seven minutes ago, then you may not see the log as expected for potentially another eight minutes. The log files are held by the CloudTrail service until final processing has been completed. Only then will it be delivered to S3, and optionally, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a>, depending on your configuration of the trail. </p>
<p>When an event reflecting an API call is written to a log, a number of attributes are also written to the same event capturing key data about that call. As you can see from this example. Without going through every attribute here, I just want to point out some of the more interesting ones. These being eventName. This refers to the name of the actual API that was called. EventSource. This refers to the service as to which the API called was made against. EventTime. This was the time that the call was made. SourceIPAddress. This displays the source IP address of the requester who made the API call. This is a great piece of information when trying isolate an attacker from a security perspective. UserAgent. This is the agent method that the request was made through. Example values of these are: Signin.amazonaws.com and this is what we have in our example and it simply means that user made this request from within the AWS management console. Console.amazonaws.com, this is the same as the previous, however, if this was displayed, it would mean that the request was made by the root user of the account, and lambda.amazonaws.com, this is fairly obvious, this would reflect that the request was made with AWS lambda. UserIdentity. This contains a larger set of attributes that provides information on the identity that made the API request. Once events have been written to the logs and then delivered and saved to S3, they are given a standard name and format, as shown. </p>
<p>The first three elements of this naming structure are self-explanatory. The AccountID, Name of the Service delivering the log, CloudTrail, and the region that it came from. The next part relates to the date and time. The year, months, and days. The T indicates the next part is the time reflecting hour and minutes. The Z simply means that the time is in UTC. The UniqueString value is a random 16 alphanumeric character string that is simply used by CloudTrail as a unique file identifier to ensure that it doesn’t get overwritten with the same name of another file. Currently, the FileNameFormat is defaulted to json.gz which is a compressed GZ version of a JSON text file. While we are looking at structures, let me also talk about the bucket structure way your logs are stored. </p>
<p>You may feel that the logs are all stored in one folder within your S3 bucket. However, there is a lengthy but very useful folder structure as follows: Firstly, you have your dedicated S3 BucketName that you selected during the creation of your Trail. Next, is the prefix that is also configured during Trail creation and is used to help you organize a folder structure for your logs corresponding to different Trails. Following this, is a fixed folder name of AWSLogs. Followed by the originating AWS account ID. Then another fixed folder name of CloudTrail indicating which service has delivered the logs. And after that, the RegionName of where the log file originated from. This is useful for when you have Trails that apply to multiple regions. The last three folders show the year, month and day that the log file was delivered. As you can see, although there are multiple folders underneath your nominated S3 bucket, it does provide an easy navigation method when looking for a specific log file.</p>
<p>This folder structure comes into even greater use if you have multiple AWS accounts delivering logs to the same S3 bucket. Some organizations may be using more than one AWS account, and having CloudTrail logs stored in different S3 buckets across multiple accounts can be inconvenient in certain circumstances and require additional administration to manage. Thankfully, AWS offers the ability to aggregate CloudTrail logs for multiple accounts into a single S3 bucket belonging to one of these accounts. This is why there is an accountID folder within your S3 bucket. Please note that you are unable to aggregate CloudTrail logs for multiple AWS accounts into CloudWatch logs that belongs to a single AWS account. </p>
<p>So to have all your logs from your accounts delivered to just one S3 bucket is a fairly simple process with the end result allowing you to essentially manage all your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/monitoring-cloudtrail-cloudwatch/">CloudTrail logs</a>. Let’s take a look at how this solution is configured. Firstly, you need to enable CloudTrail by creating a Trial in the AWS account that you want all log files to be delivered to. Permissions need to be applied to the destination S3 bucket allowing cross account access for CloudTrail. And once permissions have been applied to your policy, you need to edit the bucket policy and add an additional line for each AWS account requiring access. Then you need to create a new Trial in your other AWS accounts and select to use an existing S3 bucket for the log files. When prompted, add the bucket name used in step one and when alerted, accept the warning that you want to use a bucket from a different AWS account. An important point to make here when configuring the bucket selection is to ensure that you use the same prefix as the one you used when you configured the bucket in the first step. That is unless you intend to edit the bucket policy to allow CloudTrail to write to the location of a new prefix you wish to use. When you have configured your Trail, click create and your new Trail will now deliver it’s log files to the S3 bucket in your AWS account used in the first step. Again, this is a great solution that allows you to essentially manage all of your CloudTrail logs in one single account and S3 bucket. However, there may be uses such as system administrators who manage the other AWS accounts where the logs have come from that might need access to data within these logs. So how would they gain access to the S3 bucket to allow them only to access their CloudTrail logs that originated from their AWS account? </p>
<p>It could be done quite easily by configuring a few elements within IAM. Firstly, in the master account, IAM Roles would need to be created for each of the other AWS accounts requiring read access. Secondly, a policy would need to be assigned to those Roles allowing access to the relevant AWS account logs only. Lastly, users within the requesting AWS accounts would need to be able to assume this Role to gain read access for their CloudTrail logs. The easiest way to show you how to configure the permissions required is by a demonstration while I shall perform the following steps: I shall create a new Role. Apply a policy to this Role to only allow access for AWS account B’s folder in S3. Show the Trust Relationship between AWS account A and B. I will then create a new IAM user in account B. And create a Policy and apply the sts:AssumeRole permissions to this user allowing them to assume the new Role we created in account A. So let’s take a look at how and where we apply these permissions. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so, as I just said the first thing we need to do is create a new Role in our primary account. So, if we go across to IAM, which is under Security, Identity and Compliance and then once that’s loaded, we need to go across to Roles and then Create New Role. So let’s give this Role a name. I’ll call it ‘Cross Account CloudTrail’ Click on Next Step. We then need to select a Role type, and what we want to do is select Role for Coss-Account access because we will allow users in another AWS account to access the log files in this primary AWS account. And this will set up the trust relationship between this account and then my secondary account. So, for that we will select this top option of providing access between AWS accounts you own. Then next, I’ll need to enter the secondary account ID that I want to create the trust relationship with. So I’ll just enter that number. </p>
<p>Okay, then after you have entered your account ID click on next step. And now we need to attach a Policy to this Role. Prior to this demo, I set up my own Policy and this allows cross-account access to read only from my secondary account to the bucket on this primary account. But I’ll explain this Policy in a few moments and I’ll show exactly what it contains. And from here click on next step and this is just a review of the Role. So we have the Role name, the ARN, the Amazon Resource name, the trusted entities. So this is the secondary account ID that I entered, and then the actual Policy and then the link that we can give to users in the secondary account to allow them to switch Roles. So, create Role. And there we go, the cross-account CloudTrail Role that we just created. So, let’s take a look at this.</p>
<p>Firstly, I’ll show the trust relationships. So, because we added a cross-account Role access and then we entered the secondary AWS account ID, we can see that this account is trusted by our primary account and that allows entities in this account to assume this Role. Now, I mentioned earlier that I previously set up a Policy with permissions in. So let’s take a look at that Policy. I named it Cross-Account read only for CloudTrail. So if I show the Policy, as you can see it. I made a very small Policy. Very simple. Now we have an effect of allow which will allow any S3:Get and any S3:List command, so essentially, read only access on this resource here specified by this line. Now this resource links to the bucket and folder where CloudTrail logs are delivered for our secondary account as you can see here. So, essentially, what this Policy does is allow read only access to any folders within the secondary account’s CloudTrail log folders. So this account won’t be able to access any other accounts CloudTrail logs, which is important. So, if we come out of this. So let’s just have a quick recap of what we’ve achieved so far.</p>
<p>So, so far, what we’ve done: We’ve created a Role in our primary account for our secondary account access. And we’ve also assigned an access Policy to this Role in order for the secondary AWS account to access the relevant folder in S3. So now what we need to do is assign a user in the secondary account and then apply the permissions to that user to enable them to assume the new Role in the primary account. So let’s go ahead and do that. </p>
<p>Okay, so I’ve now logged into the secondary account where I’ll need to create a new user and assign the correct permissions. So, to start with, I’m going to set up a permission Policy to assign to the user. So if I go down to Security, Identity and Compliance and select IAM. And then go across to Policies, and from here I want to create a new Policy. And I am going to create my own Policy, so I’m going to select the bottom option. I’m going to call this AssumeRoleforCloudTrail And description will be Assume role in primary AWS account. And for the Policy document, I’m going to paste in a Policy that I’ve already created. As you can see, it’s only a very small Policy again. And we have an allow effect that allows the Assume Role action from the security token service against the following resource, and this resource links back to a Role on our primary account where we created the Role cross-Account CloudTrail. </p>
<p>So this Policy will allow the user to assume this Role in the primary account. So let’s go ahead and create that Policy. Let’s validate it first. And then create. Now what we need to do is to assign a user to use that Policy. Now, I created a new user earlier prior to this demo. So, let’s just find our new Policy that we just created. And here it is at the bottom, AssumeRoleforCloudTrail. And I’m going to attach a user. And I’ve called our user CloudTrailUser1. And then attach Policy. And there we go. </p>
<p>So we now have one user attached to this Policy. So that’s all the actions and steps necessary to allow a user in a secondary account to access CloudTrail log files that have been delivered to an S3 bucket in a primary account. And it would do this by using the permission Policy that we just applied to that user to access the Role in the primary account. And that Role has a Policy attached that allows S3 read access to it’s own CloudTrail logs. </p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>CloudTrail allows you to enable a feature called Log File Integrity Validation. Which simply allows you to verify that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. This is typically used for security and forensic investigations where by the integrity of the log files are critical to confirm that they have not been tampered with in any way. </p>
<p>When a log file is delivered to an S3 bucket a hash is created for it by CloudTrail. A hash file is a set of characters that are unique that are created from a data source. In this case, the log file. The hashing algorithms used by CloudTrail are SHA-256. In addition for a hash for every log file created, </p>
<p>CloudTrail creates a new file every hour, called a digest file, which is used to help verify your log files have not changed. The digest file contains details of all the logs delivered within the last hour along with a hash for each of them. These files are stored in the same bucket as the key pair. When it comes to verifying the integrity of your log files, the public key of the same key pair is used to programmatically check that the logs have not been tampered with in any way. Verification of the log files can be achieved via a programmatic access and not via the console. Using the AWS CLI, this can be checked by issuing the following command. The folder structure for the digest is very similar to the CloudTrail logs, as you can see. But the digest files are clearly distinguishable by the CloudTrail digest folder. </p>
<p>That has now taken me to the end of this lecture. Coming up next, I’ll explain how you can use CloudTrail and CloudWatch together as a monitoring solution.</p>
<h1 id="Monitoring-CloudTrail-with-CloudWatch"><a href="#Monitoring-CloudTrail-with-CloudWatch" class="headerlink" title="Monitoring CloudTrail with CloudWatch"></a>Monitoring CloudTrail with CloudWatch</h1><p>Hello and welcome to this lecture where we will look at how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudtrail-logging/">AWS CloudTrail</a> interacts with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a> and SNS to create a monitoring solution. In addition to S3, the logs from CloudTrail can be sent to CloudWatch Logs, which allows metrics and thresholds to be configured, which in turn, can utilize SNS notifications for specific events relating to API activity. CloudWatch allows for any event created by CloudTrail to be monitored. This enables a whole host of security monitoring checks to be utilized. A great example of this is to be notified when certain API calls requesting significant changes to your security groups or network access control lists within your VPC. Other examples of these checks that are common within organizations are API calls relating to it starting, stopping, rebooting, and terminating EC2 instances. If instances are being created that shouldn’t be, then your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> cost could rise dramatically and quickly. Also, if instances are being rebooted or stopped, this could have a severe impact to your services if they are not configured in a high availability and resilient solution. Changes to security policies within IAM and S3. If changes are being made to your policies that shouldn’t be, access can be inadvertently removed for authorized users and access granted to unauthorized users, having a massive impact on operational services. Even a minor change to a policy can pave the way for an untrusted user to exploit the error. Failed login attempts to the Management Console. Monitoring failed attempts here can help to prevent unauthorized access at your environment’s front door. API calls that result in failed authorization. Not only does CloudTrail track successful API calls whereby the correct authorization was met by the authenticated identify, but it also tracks unsuccessful API requests, too, which would likely be due to the permissions applied. Special attention should be given to these unsuccessful attempts, as this could be a malicious user trying to gain access. However, it could also be a legitimate user trying to access a resource they should have access to for their role, but the incorrect permissions had been applied with their associated IAM policy. </p>
<p>To configure CloudTrail to use CloudWatch, you must first create a trail. Once your trail has been created, you can then configure it to use an existing CloudWatch Log group or have CloudTrail create a new one. Having CloudTrail create a new one for you is recommended if it is your first time doing this, as CloudTrail will take care of all of the necessary roles, permissions, and polices required. You may be wondering why roles and policy are required, so let me give you a high-level overview of the simple process that takes place when sending CloudTrail logs to CloudWatch. When a log file is created by CloudTrail, it is sent to your selected S3 bucket and your chosen CloudWatch Log group, assuming your trail has been configured for this feature. To allow CloudTrail to deliver these logs to CloudWatch, CloudTrail must have the correct permissions and these are gained by assuming a role with the relevant permissions needed to run two CloudWatch APIs. The first being CreateLogStream, and this enables CloudTrail to create a CloudWatch Logs log stream in the log group, and PutLogEvents, and this allows CloudTrail to deliver CloudTrail events to the CloudWatch Logs log stream. CloudWatch then delivers logs to the CloudWatch Logs. </p>
<p>When using the AWS Management Console, you can have the CloudTrail create this role for you, along with the correct policy. By default, the role is called CloudTrail_CloudWatchLogs_Role. For those that are curious, the policy for this role looks as shown. It’s important to point out that CloudWatch Log events have a size limitation of 256 kilobytes on the events that they can process. Therefore, any events that are larger than 256 kilobytes will not be sent to CloudWatch by CloudTrail. </p>
<p>Now that you have your logs with the associated events being sent to CloudWatch, you must then configure CloudWatch to perform analysis of your CloudTrail events within the log files. This is done by configuring and adding metric filters to the log within CloudWatch. These metric filters allow you to search and count a specific value or term within your events in your log file, which then allows for customizable thresholds to be applied against them. When creating these metric filters, you must create a filter pattern which determines what exactly you want CloudWatch to monitor and extract from your files. These filter patterns are usually fully customizable strings but as a result, a very specific pattern syntax is required. So, if you’re creating these for the first time, you must understand the correct syntax. </p>
<p>Just to reiterate what we have spoken about so far, I want to provide a demonstration on how to edit an existing trail to configure it to send logs to CloudWatch Logs. I will then configure a metric filter with the associated metric pattern, and finally, I will set up an SNS alert to notify me when a particular threshold is met. So, let’s take a look. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so what I need to start with is going into CloudTrail to edit an existing trail to enable CloudWatch Logs. So, if I go down to Management Tools and click on CloudTrail and then across to Trails, as you can currently see, under CloudWatch Logs log group, there’s no log group selected. So, if we go into the trail and then scroll down to CloudWatch Logs, click on Configure, and there we can get CloudTrail to automatically set up this group and it’ll create the necessary roles and permissions, etc. So, let’s call this CloudTrail&#x2F;Demo and then click on Continue. So, we’ve given it a name and here it just gives a message to say that for CloudTrail to deliver events and logs to CloudWatch Logs, it needs to assume a role with permissions to run two API calls, which are these two here. And if we go down into the details, you can see that the IAM role that it’s going to use is the CloudTrail_CloudWatchLogs_Role and we’ll ask it to create a new policy. And here’s the policy document. </p>
<p>So, go down to Allow, and then if we scroll down to our CloudWatch Logs section, you can now see that we have a log group created in CloudWatch called CloudTrail&#x2F;Demo. So, if we now go across to CloudWatch and if we click on Logs on the left-hand side here, we can see that we have our log group that was just created by CloudTrail, and it’s CloudTrail&#x2F;Demo. Now, if we go into our log group and select it, you’ll see this log stream, which is the incoming stream of events being sent from CloudTrail. Now, as we’ve only just started, there’s only a few events coming in here, so you might want to wait a few minutes before setting up your metric filters to give you more of a test pattern to search on. So, what I might do is just leave it a couple of minutes for some more events to start streaming in before we set up our metric filters here, just so we have something to search on. </p>
<p>Okay, so I’ve left it a few minutes, so let’s go back into the log group and you can see we’ve now got a couple of streams, and if we go into these, we can see there’s a lot more events. So, if we go back a couple of pages, back to our log group, now we need to create our metric filters to allow us to define what we want to search on within our logs. So, if we select the tick next to our log group and then go up to Create Metric Filter, and here within the metric filter, we need to define a filter pattern. Now, as explained earlier, filter pattern will define what we’re actually searching for within our logs. So, for this example, I’ll keep it fairly simple. I’m going to search for any API call that’s been made from my machine, so from my IP address. So, for that, I need to enter the following command, ( $.sourceIPAddress equals 2.218.11.188, which is my IP address. And now we can test to make sure that that filter pattern’s okay using this Test Pattern box here, and what that does, that’ll run this test filter on some log data we see from this log here and the output of that log is in this box here. So, all we need to do is click on Test Pattern and we can see at the bottom here that it found 47 matches out of 50 events in the sample log. So, we know that the syntax is okay for this filter pattern, so I’m going to go ahead and assign this metric. </p>
<p>And we can see up here that we’ve got our filter name and our filter pattern, and I’m going to create a new name space for this metric and I’ll call it Demo, and the metric name will be IPAddress. And then what we need to do is click on Create Filter. Now, as you can see, our filter has been created and we have the details in this screen here. Now, what we can do at this point is create an SNS alarm so it could be notified if a certain threshold was met. So, let’s go ahead and do that. </p>
<p>So, the first thing that we need to do is add a name, so I’m going to call this SourceIPAddress and description will be Too many calls from my IP. Now, I’m going to set this to be 30. So, whenever my IP address is used as a source IP address that is greater or equal to 30 times for one consecutive period over five minutes, then I want it to set to a state of an alarm. And I want to be notified, so I’m going to enter a new list, give this a new topic, SourceIPAddressAlarm, and I want that to be sent to myself. So, as we can already see, with the current data it’s got that it has already breached the alarm, but it has gone back down below, so we’ll see how this goes and we’ll create the alarm. And this is a message just to say that I need to subscribe to that AWS notification, and I can do that in just a few moments. So, if we go across to our Alarms, we can see that we have our source IP address alarm in the state of OK. </p>
<p>So, at the minute, it’s currently below the 30 threshold. As soon as it goes above that, it will alarm and I will get a notification. Now, over the past few minutes, I’ve just been having some activity within the Management Console, and as we can now see, we do have an alarm on our alert. We can see that it just crossed the threshold, and so, I’ve received an email notification to say that it is now in a state of alarm. And if we take a quick look at that email, we can see here that it was crossed with a data point of 33 and the threshold was 30. So, that is how you set up CloudTrail to use CloudWatch with the inclusion of SNS to create alarms against API activity.</p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="CloudFront-Access-Logs"><a href="#CloudFront-Access-Logs" class="headerlink" title="CloudFront Access Logs"></a>CloudFront Access Logs</h1><h2 id="Resources-Referenced"><a href="#Resources-Referenced" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">Web distribution log file format</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">RTMP distribution log file format</a></p>
<h2 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello, and welcome to this lecture focusing on the access logs generated by Amazon CloudFront. Amazon CloudFront is AWS’s content delivery network that speeds up distribution of your static and dynamic content through its worldwide network of edge locations. When you use a request content that you’re hosting through Amazon CloudFront, the request is routed to the closest edge location which provides it the lowest latency to deliver the best performance. When CloudFront access logs are enabled you can record the request from each user requesting access to your website and distribution. As with S3 access logs, these logs are also stored on Amazon S3 for durable and persistent storage. There are no charges for enabling logging itself, however, as the logs are stored in S3 you will be stored for the storage used by S3. </p>
<p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/introduction/">logging</a> process takes place at the edge location and on a per-distribution basis, meaning that there will not be data written to a log that belongs to more than one distribution. For example, distribution a, b, c, will be saved in a different log to that of distribution d, e, f. When multiple edge locations are used for the same distribution, a single log file is generated for that distribution and all edge locations write to the single file. </p>
<p>The log files capture data over a period of time and depending on the amount of requests that are received by Amazon CloudFront for that distribution will depend on the amount of log fils that are generated. It’s important to know that these log files are not created or written to on S3. S3 is simply where they are delivered to once the log file is full. Amazon CloudFront retains these logs until they are ready to be delivered to S3. Again, depending on the size of these log files this delivery can take between one and 24 hours. </p>
<p>When these log files are delivered they use a standard naming convention as follows. So let’s say for example you had the following settings. The bucket name was access-logs, the prefix was web-app-a, and you had the following distribution ID. Then your name and convention for the log would look something like this. Let me now show you a very simple demonstration on how to enable log in for your CloudFront distribution. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>So setting up access logs for your CloudFront distributions is very simple and easy to do. So let’s go into CloudFront. I’ll just select an existing distribution here, and then if you click on distribution settings and under the general tab you select edit, and then if we scroll down these settings here you’ll see a section where it starts referring to logging. And at the moment I have logging off. So to enable logging I simply click on on and then I select the bucket in S3 where I want the access logs to reside, so I’m going to select CloudFront Access Logs, which is an existing bucket I have set up for this. Now here I can add a log prefix if I want to, if I’ve got different distributions, etc. I’m just going to leave that as blank for this demonstration. And here we can have cooking logging on or off, which will log all cookie data within the request, and it’s as simple as that. And then once you’re happy with that you just click on yes to confirm your changes. And now any access requests that go via your CloudFront distribution will be logged via S3. And that’s it. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>To perform the demonstration that I just completed and to access the logs when they are stored, you will need specific permissions to the S3 bucket designated for logging. To enable the log in for your distribution, the user account activating that feature must have full control on the ACL for the S3 bucket, along with the S3 GetBucketAcl and S3 PutBucketAcl. The reason for this is that during the configuration process, CloudFront will use your credentials to add the AWS data-feeds account to the ACL with full control access. This is an account used by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> which will write the data to the log file and deliver it to your designated S3 login bucket. Therefore, if you’re trying to enable the login feature for your distribution and it’s failing, then you should check your access to ensure you have the required permissions. </p>
<p>Depending on the delivery type of your CloudFront distribution, either WEB or RTMP, the log output will vary. The number of fields within the log files differ between the two types. Web distributions have a total of 26 different fill types for each entry within the log, whereas the RTMP distributions only have 13. I won’t go through every single field explaining their purpose and use, however, I want to highlight a few points of interest starting with the web delivery type. These logs contain information which allow you to identify the following. The date and timestamp of the request of the user and which edge location received this request, source metadata of the requester including IP address details, HTTP access method of the request, such as PUT, DELETE, or GET, etc., the HTTP status codes of the request such as 200, the distribution domain name relating to the request, and the encryption and protocol data used in request such as SSL, V3, or AES256-SHA. For full information on each field and options please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">link</a>. </p>
<p>Now looking at the RTMP delivery type, the points of interest are as follows. Again, a timestamp of the request of the user and which edge location received this request, the source IP address of the requester, the event being carried out by the requester such as play, pause, or stop, and the URL of the page where your SWF file is linked to. Again, for full information on field data captured within RTMP logs you can view the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">here</a>. </p>
<p>One final feature of logging with CloudFront is cooking logging. If you enable this within your distribution, then CloudFront will include all cookie information with your CloudFront access log data. This is only recommended if your origin of your distribution points to anything other than S3 such as an EC2 instance as S3 does not process cookie data. </p>
<p>That now brings me to the end of this lecture covering AWS CloudFront logs. Coming up next I shall be looking at the logs generated at the network level within your VPC with the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC flow logs</a>.</p>
<h1 id="VPC-Flow-Logs"><a href="#VPC-Flow-Logs" class="headerlink" title="VPC Flow Logs"></a>VPC Flow Logs</h1><h2 id="Transcript-1"><a href="#Transcript-1" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture covering VPC Flow Logs. Within your VPC, you could potentially have hundreds or even thousands of resources all communicating between different subnets both public and private and also between different VPCs through VPC peering connections. VPC Flow Logs allows you to capture IP traffic information that flows between your network interfaces of your resources within your VPC. This data is useful for a number of reasons, largely to help you resolve incidents with network communication and traffic flow in addition to being used for security purposes to help spot traffic reaching a destination that should be prohibited. </p>
<p>Unlike S3 access logs and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/cloudfront-access-logs/">CloudFront access logs</a>, the log data generated by VPC Flow Logs is not stored in S3. Instead, the log data captured is sent to CloudWatch logs. Before creating your VPC Flow Logs, you should be aware of some of the limitations which might prevent you from implementing or configuring them. If you are running a VPC peered connection, then you’ll only be able to see flow logs of peered VPCs that are within the same account. Or if you are still running resources within the EC2-Classic environment, then unfortunately you are not able to retrieve information from their interfaces. And once a VPC Flow Log has been created, it cannot be changed. To alter the VPC Flow Log configuration, you need to delete it and then recreate a new one. </p>
<p>In addition to this, the following traffic is not monitored and captured by the logs. DHCP traffic within the VPC, traffic from instances destined for the Amazon DNS Server. However, if you decide to use and implement your own DNS Server within your environment, then the traffic to this will be logged and recorded within the VPC Flow Log. Any traffic destined to the IP address for the VPC default router and traffic to and from the following addresses, 169.254.169.254 which is used for gathering instance metadata, and 169.254.169.123 which is used for the Amazon Time Sync Service. Traffic relating to an Amazon Windows activation license from a Windows instance and finally the traffic between a network load balancer interface and an endpoint network interface. All other traffic both ingress and egress can be captured at a network IP level. </p>
<p>You can set up and create a flow log against three separate resources. These being a network interface on one of your instances, a subnet within your VPC, and your VPC itself. Obviously for option two and three, this will contain a number of different resources. As a result, data is captured for all network interfaces either within the subnet or the VPC respectively. I mentioned earlier that this data is then sent to CloudWatch logs via a CloudWatch log group. For every network interface that publishes data to the CloudWatch log group, it will use a different log stream. And within each of these streams, there will be the flow log event data that shows the content of the log entries. Each of these logs captures data during a window of approximately 10 to 15 minutes. </p>
<p>To enable your flow log data to be pushed to a CloudWatch log group, an IAM role is required for permissions to do so. This role is selected during the setup configuration of the VPC Flow Log. If your role does not have the required permissions, then your log data will not be delivered to the CloudWatch group. At a minimum, the following permissions must be associated to the role. In addition to this, you will also need to ensure that the VPC Flow Log service can assume that IAM role to perform the delivery of logs to CloudWatch. This can be achieved with the following permissions. </p>
<p>While on the topic of permissions, I want to also show you the required permissions for someone to review and access the VPC Flow Logs or indeed be able to create one in the first place. The following three EC2 permissions allows you to create, delete, and describe flow logs. These being ec2:CreateFlowLogs, ec2:DeleteFlowLogs, and ec2:DescribeFlowLogs. The logs:GetLogData permissions is used to enable you to list log events from a data stream. If you wanted to create flow logs, then you need to also grant the use of the IAM permission of iam:passrole which allows the service to assume the role mentioned previously to create these flow logs on your behalf. </p>
<p>Let me now show you how to create a flow log for an interface on an instance, a subnet, and lastly the VPC itself. </p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay so firstly I’m going to set up a VPC Flow Log for the running instance that we’ve used in a previous demonstration which was for the logging server. So what I need to do is go down to our network interfaces under network and security and select the ENI of the logging server. As you can see, it’s this bottom instance here. So if I select that interface, if I just drag this up a little bit, and we have three tabs here, details, flow logs, and tags. If we select the flow logs tab of this interface, we can see that there’s no flow log created as yet. </p>
<p>What we need to do is click on create flow log. Now we can select the filter for this flow log to only log either accepted requests or rejected requests so I’m going to select all so it gets accepted and rejected. We now need to select our role and I created a role earlier and I called that Flow-Logs-Role so that has the required permissions to push data to CloudWatch logs. And here we have the ARN of the role. The destination log group for CloudWatch, I set up a log group prior to this demonstration and I’ve just called this Flow-Logs. And then click on create flow log. And that’s it, it’s as simple as that. So now you can see for this eni interface here, we now have a flow log created. It gives it a flow log ID. Shows the filter which we have ALL here. Their destination log group. The ARN of the role and it’s currently active. So now any traffic going in and out of that interface on that EC2 instance will be captured and the data will be sent to the flow logs log group in CloudWatch. And let’s take a look at how you set up flow logs for a subnet. </p>
<p>So let’s go across to our VPC service. I have a couple of VPCs here and we’ll use our logging VPC. So if we go down to our subnets, and let’s select the public subnet for our logging VPC, now again we have the tabs for this subnet. We have the summary, route table, network ACL, etc, and we also again have the flow logs tab. Very simple process again. Click on create flow log. The same filters. Select the same role and the same log group. And then simply create flow log. And that’s now having the flow logs enabled on this particular subnet so all traffic going in and out of this subnet will be captured and sent to the flow logs log group. </p>
<p>And for the VPC, it’s very similar. So you simply select your VPC so we have our logging VPC here, again we have our flow logs tab. Create flow log. Select the role and the destination log group of flow logs and then create flow log and that’s it. So it’s very easy to set up your flow logs for your EC2 network interface clouds or your subnet or your entire VPC. And that’s it. </p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>Let’s now take a look at a record within one of these flow logs. When you access the logs, you will find each entry has the following syntax. These entries are defined as follows. Version, which is the version of the flow log itself. Account-id, this is your AWS account ID. Interface-id, this is the interface ID of which the log stream data applies to. Source address, this is the IP source address. Destination address, this is the IP destination address. Source port, this is the source port being used for the traffic. And the destination port is the destination port being used for the traffic. The protocol, this defines the protocol number being used for the traffic. Packets, this shows the total number of packets sent during the capture. Bytes, again this shows the total number of bytes sent during the capture. Start and end shows the timestamp of when the capture window started and finished. Action, this shows if the traffic was accepted or rejected by security groups and network access control lists. And the log-status shows the status of the logging through three different codes. OK, where data is being received by CloudWatch logs. NoData, this means there was no traffic to capture during the capture window. And SkipData, where some data within the log was captured due to an error. </p>
<p>One of the key fields from an incident response and troubleshooting perspective is the action field. For example, if you are troubleshooting an issue of traffic not being received by a particular resource, then you could check the VPC Flow Logs to see if the traffic is getting blocked at the subnet level by a network ACL. This will then allow you to review your entries within the NACL to make the changes that’s necessary from a security perspective. </p>
<h1 id="Overview-of-the-AWS-Health-Dashboard"><a href="#Overview-of-the-AWS-Health-Dashboard" class="headerlink" title="Overview of the AWS Health Dashboard"></a>Overview of the AWS Health Dashboard</h1><p><strong>Instructor: Carlos Rivas</strong></p>
<h1 id="Overview-of-the-AWS-Health-Dashboard-1"><a href="#Overview-of-the-AWS-Health-Dashboard-1" class="headerlink" title="Overview of the AWS Health Dashboard"></a>Overview of the AWS Health Dashboard</h1><p>The Health Dashboard is divided into 2 main sections:</p>
<ul>
<li>Events that affect everyone (top left), and </li>
<li>Events that affect your account’s resources, right below that.</li>
</ul>
<p>Let’s go over each one.</p>
<h2 id="Service-Health"><a href="#Service-Health" class="headerlink" title="Service Health"></a>Service Health</h2><p>First, you have <strong>Open and recent issues</strong>, this is where you can see current issues happening in the AWS Platform. More often than not, this option will show as disabled if there’s nothing of interest going on.</p>
<p><strong>Service history</strong> on the other hand, will show a historic view of issues. This is really helpful if something happened over a weekend or holiday and you want to get details about which services and regions were affected.</p>
<p>Let’s look at an example of a possible outage:</p>
<p><img src="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid6-3a3ff769-6937-455b-95ea-fe227962f0ae.png" alt="alt"></p>
<p>Each one of these tickets will have: A header, showing the latest status , in this case “resolved” and a short description of the issue. In this case, “Increased API Error rates.”</p>
<p>You want to pay close attention to the affected services, in this case it’s a list of 20 services and chances are, that if you were using one of these services when this event happened in the US-EAST-1 regions, your application would have experienced similar problems.</p>
<p>Information like this is useful to shorten troubleshooting times and also to consider multi-region solutions if your business suffers a significant impact by an issue like this, in this particular AWS region.</p>
<h2 id="Your-account-health"><a href="#Your-account-health" class="headerlink" title="Your account health"></a>Your account health</h2><p>If we switch over to <strong>Your account health</strong>, this is where the Health Dashboard becomes really useful, because it correlates AWS Global issues with the resources and service that you are currently using. This way, you can see if there’s any impact to your business.</p>
<p>For example, let’s say you are running an EC2 instance and it’s been running nonstop for 12 months… </p>
<p>You may go here under the <strong>Scheduled events</strong> tab ( or, you may get it in an Email from AWS) and see something like this:</p>
<p><img src="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid4-e35d3347-e755-45e7-88d0-dc6c2fb51434.png" alt="alt"></p>
<p>Essentially, this means that the physical hardware running your EC2 server may need to be taken down for repairs, upgrades or simply maintenance. – The solution is simple by he way: simply stop and restart your virtual EC2 instance and it will come online on a different physical computer, therefore allowing AWS to perform maintenance without further interruption to you or any other customers.</p>
<h2 id="Integration-with-EventBridge"><a href="#Integration-with-EventBridge" class="headerlink" title="Integration with EventBridge"></a>Integration with EventBridge</h2><p>It’s totally understandable if you don’t want to have to manually visit a web page to find out if there’s an outage affecting your AWS infrastructure, for this, there’s a solution: EventBridge.</p>
<p>EventBridge can be used to monitor and react to AWS Health Dashboard events, and take certain actions including:</p>
<ul>
<li>Sending a notification to the Ops team</li>
<li>Identifying affected resources, and</li>
<li>Executing custom lambda functions to perform pretty much any task, such as creating a Zendesk or JIRA ticket related to an AWS Scheduled maintenance event.</li>
</ul>
<p>We will be looking at this in more detail, but here’s a pattern to catch Events related to notifications, scheduled changes or issues sent to your account via the Health Dashboard.</p>
<p><strong>{</strong></p>
<p> <strong>“detail”: {</strong></p>
<p>  <strong>“eventTypeCategory”: [</strong></p>
<p>   <strong>“issue”,</strong></p>
<p>   <strong>“accountNotification”,</strong></p>
<p>   <strong>“scheduledChange”</strong></p>
<p>  <strong>],</strong></p>
<p>  <strong>“service”: [</strong></p>
<p>   <strong>“AUTOSCALING”,</strong></p>
<p>   <strong>“VPC”,</strong></p>
<p>   <strong>“EC2”</strong></p>
<p>  <strong>]</strong></p>
<p> <strong>},</strong></p>
<p> <strong>“detail-type”: [</strong></p>
<p>  <strong>“AWS Health Event”</strong></p>
<p> <strong>],</strong></p>
<p> <strong>“source”: [</strong></p>
<p>  <strong>“aws.health”</strong></p>
<p> <strong>]</strong></p>
<p><strong>}</strong></p>
<p>With this pattern in EventBridge you can quickly react to potential issues without human intervention and notify the right folks in order to decide what to do. Also note the Service filter here that includes AUTOSCALING, EC2 and VPC. This is important because if you are not using AWS S3 -for example- you don’t want to send out alerts if this service won’t impact you directly.</p>
<h1 id="Enterprise-Level-Services"><a href="#Enterprise-Level-Services" class="headerlink" title="Enterprise-Level Services"></a>Enterprise-Level Services</h1><p><strong>Instructor: Carlos Rivas</strong></p>
<h1 id="Enterprise-Level-Services-1"><a href="#Enterprise-Level-Services-1" class="headerlink" title="Enterprise-Level Services"></a>Enterprise-Level Services</h1><p>The Health Dashboard is quite useful as it is, specially for those using AWS Organizations. However, if you also subscribe to a Business or Enterprise level support plan, AWS takes it up a notch by providing access to the Heath API.</p>
<p>This enables you to perform lots of additional health-related tasks such as integration with 3rd-party applications, such as: Creating JIRA Tickets, sending notifications to Slack or MS Teams and more.</p>
<p>One feature that I find quite interesting is to be able to use the Open-Source tool called AHA – AWS Health Aware. This tool, created by AWS, depends on the Health API.</p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/mt/aws-health-aware-customize-aws-health-alerts-for-organizational-and-personal-aws-accounts/">https://aws.amazon.com/blogs/mt/aws-health-aware-customize-aws-health-alerts-for-organizational-and-personal-aws-accounts/</a></p>
<p>This tool will perform the monitoring for you and provide you with integrations and notifications of Events that are relevant to your accounts and overall AWS footprint.</p>
<p><em><img src="https://assets.cloudacademy.com/bakery/media/uploads/entity/blobid2-992c4663-2ed4-4f5d-89d1-33e7cfcd817a.png" alt="alt"><br>Image from</em> <a target="_blank" rel="noopener" href="https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2021/09/04/aha-arch-single-region-1.png"><em>https://d2908q01vomqb2.cloudfront.net/972a67c48192728a34979d9a35164c1295401b71/2021/09/04/aha-arch-single-region-1.png</em></a><em>.</em></p>
<h1 id="Bills-and-Cost-Drivers"><a href="#Bills-and-Cost-Drivers" class="headerlink" title="Bills and Cost Drivers"></a>Bills and Cost Drivers</h1><p>Welcome to the first live demo. As you can see here we are in the AWS Management Console. The most fundamental part is to get a good overview of the environment. Therefore you have to get a broad view of all the services, usage, and expenses from the past, and the present.</p>
<p>To set a base for our analysis, we first need to get some numbers. For this we will be using the billing dashboard which is also our main platform for gathering information on expenses. We will deal a lot with the billing and cost management dashboard alongside but the very first thing I like to investigate when I start with an unfamiliar account or environment is to check the last bills.</p>
<p>So to get there, you can either type bill here in the search field to get straight to the billing service or you can just go here over the menu to the billing dashboard. Here, you get a good first overview what is going on in your account. What are the most expensive services? How much have you spent so far? And what is the forecast? The forecast to the current month. But we will talk about this later.</p>
<p>So first thing here is the bill section here on the left side. I wanna get to overview from the last month, how much we spent last month and what were the biggest cost drivers. So as we are currently in January 2021, I go one month back to December 2020, here from the dropdown menu. And as you can see, I have a total amount, how much we spent last month.</p>
<p>I have like different information about taxes and payment summaries. And I can also get a CSV file from my whole bill or just printed as a PDF. So as you can see here we are in a master payer account, that means we’re using AWS Organizations which enables us to link other accounts to this account.</p>
<p>So all the costs that are caused by the under linked accounts are covered by this account that are collected here. You can see there are many accounts but let’s focus on this overview here. So what I wanna get here is the first overview. What are the most expensive services, or like what are the biggest cost drivers.</p>
<p>You can see we’re spending like small amounts, like compared to the total we’re spending small amounts on like different services. For example, here $34 for the API Gateway or 91 for CloudFront. As this is like just like a very very small percentage of our total amount, it would make no sense or like almost no sense to dive into deeper analysis here for CloudFront. It just would make no sense because like the amounts are here so small and this makes also like our whole analysis much easier because we know the only thing we have to focus is Elastic Compute Cloud because this is the biggest cost driver in this case. And when we go here, we can see, okay, we’re using like quite a lot of regions here.</p>
<p>So my first question would be, why do we need so many regions? And as you probably know, the regions in Asia are way more expensive than for example, the regions in Northern Virginia. So my first question would be, why do we have all these different regions? And as you can see this is like just my thinking process, how I would approach such an analysis or such an optimization process but we will talk about this later in detail. Let’s go through the next section.</p>
<h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p>AWS credits are applied to bills to help cover costs that are associated with eligible services. They are applied until they are exhausted or they expire. With credits AWS introduced a sort of reward system for particularly active users and developers. You can use them instead of spending money on certain services.</p>
<p>So you may ask yourself, “How do I get credits?” So, there are multiple ways. For example, developing and publishing a skill for Alexa or by attending webinars and events. As a startup, you can use the AWS Activate program or through the AWS Credit Program for Nonprofits.</p>
<p>In my case, let’s jump here to the credits section, I did a few AWS certifications, and for that I was rewarded by AWS with two $300 credit vouchers which is pretty cool. I used them here for my private account to play around with a few services that I was interested in and as you can see here, you can see a list of which these credits can be applied to. And there are almost all services that are available for AWS. And what’s quite funny is, the AWS cost explorer that can also cost money when you are using the API is not listed here. So, for the billing services and the cost services you can’t use these credits.</p>
<h1 id="Cost-Explorer"><a href="#Cost-Explorer" class="headerlink" title="Cost Explorer"></a>Cost Explorer</h1><p>In this section, we will take a look on how to use the AWS Cost Explorer, one of the most essential tools for our Cloud Financial Management within AWS. Moreover, the Cost Explorer is the very main tool for you to gather information and analyze all the costs and expenses in your environment. So let me introduce you to the tool itself.</p>
<p>To get there, you can go over to your Billing Dashboard. From here, go to the Cost Explorer, and just launch it. See, you’re getting guided to a whole new menu where you can start to analyze your costs. The Cost Explorer is a built-in tool that lets you get deeper insights into the costs and usage of your cloud environment.</p>
<p>With the help of the Cost Explorer, you will be able to identify trends, hunt down the biggest cost drivers and detect anomalies. And the best part, the Cost Explorer, in its base variation, does not cost anything. You can use it for free. However, take note that there can be additional costs when you use tools to request the API of the Cost Explorer.</p>
<p>Every API request costs about one cent, which could drive up your costs very fast when you fire up a lot of API requests. With the Cost Explorer, you can visualize your usage patterns over time and identify underlying cost drivers.</p>
<p>What we see here is the very first view that you get when you open the Cost Explorer. It is grouped by the different services that you use. You get a six-month overview, and the visualization type is bar, which means that the costs are stacked to each other in these kind of bars here. You can also change this to stacks or to lines, what doesn’t make too much sense in this case. So let’s stick to the bar view.</p>
<p>The legend here below shows you which service is illustrated by which color. In this example, we can see over the last six months our EC2 usage has increased quite a lot. If you scroll down a little bit you can see the detailed data table. Below the Cost Explorer here is the data table, which provides a deeper insight into the data seen in your chart. And you can also export this as a CSV file for further processing with a table tool like Excel if you wanna make forecasts or like further analysis. </p>
<p>You can also like, like I said we have the last six months here, you can also like scroll vertically to see all the services and cost drivers and you can scroll horizontally to see each month. It’s basically just a table.</p>
<p>You can choose the exact timeframe you want to see within Cost Explorer. By default, it’s on six months. So you can either select day by day that you wanna see or you can choose to auto select here to see for example like just last seven days, current month, three months, six months, one year, months to date, year to date, or three months forecast or 12 months forecast.</p>
<p>Let’s go for the last seven days here. So we can see how much we spend for the day different services day by day. We can also delete the group by filter here and get like monthly view. It makes no sense because we wanna see days. So you’re gonna select daily here and then we can see how much we spend day by day, if there’s, for example, an anomaly. We can see, oh wow, like way more than the other days. Why is that?</p>
<p>You can see we are here January 1st the first day of the month. And on the first day of the months, you usually have to pay for tags, reserved instances, and all that stuff that incurred like more costs than over the other days. And that could be the explanation for this anomally here.</p>
<p>And you may have noticed you can also go from a daily to an hourly view, but it is deactivated. I can’t select it. Why is that? The reason for that is super easy. You have to pay for the hourly view because you need like much more data records. So you have to enable it first. If you really wanna go in that much detail. I will show you how you can do this.</p>
<p>Hourly view, you have to go to preferences. And this is the point we are looking for, the hourly and resource level data. We can select this and save it. Here you can also see the cost that would occur. And we can see here one cent per 1000 usage records per month.</p>
<p>It depends how much EC2 instances because this just applies to EC2. It depends how much EC2 instances you’re running. Of course, the more instances you’re running the more usage records you are going to record and the more costs you incur, but 1 cent per 1000 records, in this case, it’s not that much. </p>
<p>So we wanna save it here, go back to the Cost Explorer and we can see, oh, the hourly view is available. We can only see last 14 days of usage was the hourly view enabled. In this case because I just enabled it, I won’t see anything because the data needs to be recorded first. So it will take a minimum of four days for Cost Explorer to collect enough data that the hourly view is available here.</p>
<p>About data grouping and filter. We started with the group function, with the grouped by functionality, you can segment your data based on a particular cost or usage. For example, you can choose a region to see in what region you spent the most money on which regions you spent money.</p>
<p>Let’s go with a bar so we can see, let’s take December. We spent the most money in Ireland followed by others because others mean we have like a lot of data here. Well, let’s go to December. We’ve used many different regions here and all the regions that we use that are not shown here are combined into the others bar. If you want to stripe your chart down to make it more granular, you should use the filter function located on the right.</p>
<p>Using the filter panel. You can refine your data set to include or exclude specific filtering dimensions and values. For example, let’s say we wanna only see costs that occurred in Frankfurt. We go to the filter section here search for the Frankfurt region, select it, applied it, then we can just see the costs that applied for the Frankfurt region. And we can use like many combinations here to dive deep within our costs.</p>
<p>For example, if we just wanna see the costs for our EC2 instances, just filter here for EC2, for our compute instances. Nothing else just compute instances.</p>
<p>So just our literal service and we filter it by instance type and then we can see what instances occurred the most. So for example, here, we have a G4DN which is like a graphical, a big graphical instance that is quite expensive in this case.</p>
<p>As you can see here on the right, there are quite a lot of filters that you can use and no worries, we won’t look at all of them, but some of them are quite interesting. Let’s for example talk about usage type, usage types are the unit that each service uses to measure the usage of a specific type of resource.</p>
<p>For example, if I wanna know how long my t2 micro instances we are running in the past, I would just type in here, t2 micro, oops micro, select a filter here. And I can see my t2 micro instances were running 370 hours in July and about 200 hours in August. And you can use this for many other services as well.</p>
<p>Usage type groups also is pretty cool, these are the kind of type of filters that collect a specific category of usage type filter and put it into one. For example, if I wanna know how long all my EC2 instances were running in the past, I would just search for EC2 running hours and I could see the amount all my EC2 instances were running over the last six months.</p>
<p>Another one that is quite useful if you wanna go in more detail is the filter for API operations. So let’s take S3 as an example, we can see here. We don’t have that much S3 costs here, just in December $188. But if I wanna know more about these costs, how these costs are structured, I can look for specific API operations that apply to S3.</p>
<p>For example, if I want to know how much my costs were for like reading files from my S3 three buckets, I would look up GetObject because this is the API operation for reading objects from S3. I apply this filter here and it can see the costs for GetObject from S3. That’s it, that’s how you’re gonna use API operations.</p>
<p>As you can imagine, there are a lot of API operations that are used within AWS and I can highly recommend to have a look at API reference documentation for each service because you can see here, this is like just for S3, all the API operations that you can use for S3. And just by looking at them, you can, for most of them, you can already like kind of guess what they’re doing. Like for example, here, create object, create bucket and you can like just look forward for the API operation that fits for your kind of analysis.</p>
<p>A little bit deeper into Cost Explorer and the advanced options that can also be quite useful. For example, show only untagged resources. This is a pretty important filter because we talk about tags already a lot and little spoiler here we will talk about them way more because this is such a super, super, super important topic. And if you set this filter here you will see all the resources that have no tags attached, super important to see that.</p>
<p>What is also quite cool is the show costs as here. The unblended filter here will be the best for the vast majority of AWS customers. This is the cost dataset presented you on the bill page like for the bills and for the invoices. And it’s also the default option for analyzing costs using AWS Cost Explorer or setting the custom budgets using AWS budgets.</p>
<p>The unblended costs represent your usage costs on the day that they are charged to you or in finance terms they represent your costs on a cash basis of accounting. For most of you this is the only data set that you will ever need. Okay?</p>
<p>Amortized costs are also quite interesting because the amortized cost is useful in cases in which it doesn’t make sense to view your costs on the day that they were charged or as many of finance owners say it’s useful to view costs on an actual basis rather than a cash basis.</p>
<p>This cost dataset is especially useful for those of you who have purchased AWS or reservations such as reserved instances or savings plans. Savings plans and reservations often have upfront or recurring monthly fees associated with them.</p>
<p>Recurrent fees for reservations are charged on the first day of the month that can lead to a spike on one day if you’re using unblended costs as your cost dataset. When you toggle over to amortized costs, these recurring costs, as well as any upfront costs, are distributed evenly across the month.</p>
<p>Armotised costs are a powerful tool if you seek to gain insight into the effective daily costs associated with your reservation portfolio, or when you are looking for an easy way to normalize costs and usage information when operating at scale.</p>
<p>And then there are also like, two more that can be quite helpful. And these are the net unblended costs and the net amortized costs. These are basically the same as the two that I just explained here but they also include discounts like the reserved instance volume discounts. Like these discounts are calculated into these costs.</p>
<h1 id="Reports"><a href="#Reports" class="headerlink" title="Reports"></a>Reports</h1><p>Let’s talk about reports. Reports can be super, super, super useful. For example, in this case, I build a specific view for EC2 instances running in Frankfurt, in the EU region on a monthly base, like on a base for December, in this case. And because I wanna use this view more often, I can save it as a report. And to do this, I just click here on save as. I give this one a title, EU-EC2. I save it, and that’s it.</p>
<p>By clicking here, on recent reports, I can see this report. Or I can just go under report overview and see all the reports that I have here. The reports with a lock here are default AWS reports, and the one without a lock are my own reports.</p>
<p>Some of the AWS reports can be pretty useful. For example, what I use quite a lot is the RI utilization and coverage, or the savings plan utilization and coverage, that basically shows you how effective or efficient your reservations are.</p>
<h1 id="Cost-and-Usage-Reports"><a href="#Cost-and-Usage-Reports" class="headerlink" title="Cost and Usage Reports"></a>Cost and Usage Reports</h1><p>The AWS Cost and Usage Reports or in short the CUR. The CUR is basically the most important thing to capture your AWS billing data. And the CUR is a pretty complex CSV file that stores all details about your cost and usage data of all AWS resources.</p>
<p>Enabling the CUR is super important because it’s the most granular and detailed mechanism to collect data for AWS costs and usage. It offers historical by-the-hour data that can offer clarity on trends and lead to a more accurate data-driven insight. And there’s no looking back. Until the CUR is enabled, you’re losing valuable data about your usage that is older than 12 months.</p>
<p>The CUR can get really big and in large corporations, it can easily get beyond five gigabyte and more with millions over millions of lines. So let’s see how to enable them.</p>
<p>When we are here in the AWS Management Console, we click here on the top menu on the billing dashboard and you can see here on the left menu for the Cost and Usage Report. By default it’s disabled so you need to enable it, enable it first and create a report. We give it a name. Let’s call it test. I would advise you to include the resource IDs because then, every resource get a unique resource ID.</p>
<p>You can enable the automatic refresh. Click Next and then you need to choose an S3 bucket where you put the file. This can either be existing an S3 or a new one. Then we set a path, a prefix pass task costs. You can select the time granularity here. Of course, the more granular the data are, the more data you’re going to produce.</p>
<p>We can create new report versions or we can override the existing ones and we can choose what kind of data integration we need.</p>
<p>And a little side note here maybe. You can export the files for Redshift or QuickSight usage. This will change the output format of the file to be readable for either Athena or Redshift and QuickSight. See, Athena is Parquet and Redshift or QuickSight is the CSV file that comes in a, in a zip file.</p>
<p>With Athena, Athena is a serverless service that allows you to analyze the data stored directly in Amazon S3 using standard SQL. And for that you need, as I just mentioned, the Parquet file While with Redshift and QuickSight, you can manage to see a SWI file as you would do it also like for example, for Excel.</p>
<p>Redshift is a so-called data warehouse service which you would use for querying big data sets with like multiple gigabytes or even up to petabytes. It can help you take wide insights of your own environment and for customers on a very large scale. And QuickSight is a business intelligence service that can combine data from literally any source into a dashboard. It helps you to visualize for any type of audience and it is much more visually driven than Athena or Redshift.</p>
<p>After you set up all these options here, you can click on Next and that’s basically it. You have now configured your Cost and Usage Report. But be aware that it may take up to 24 hours for the first report to be delivered. Also, expect some costs from S3 for storing the CUR data in your S3 bucket. But these costs are like very, very low. Maybe like, I don’t know, a few dollars per months or per year. Depends on how big your file gets of course.</p>
<h1 id="Budgets"><a href="#Budgets" class="headerlink" title="Budgets"></a>Budgets</h1><p>In this section, we will learn how to create a budget, to help manage running expenses. Budgets allow the user to get notified when costs or usage exceed a certain predefined amount. So let’s have look how to set up automatic notifications and actions with AWS Budgets.</p>
<p>So we get there by clicking here on the top menu on the billing dashboard. Here on the left, we have budgets, we click on it, and we can see that there is already a budget predefined here. I set this up in the past to get notified if my monthly budget threshold gets over $150 but let’s create a new budget.</p>
<p>So we can select four different kinds of budgets in this case or like in general. We have cost budgets based on actual costs, usage budgets based on usage like ours and budgets for reserved instances and savings plans. So let’s start with the cost budget.</p>
<p>So first we need to give it a name and we have to select a period in that we want to be notified. We will keep it here on a monthly period and we can also choose if this is a recurring budget or an expiring budget. Does that goes, for example, like just till April but we will stay with a recurrent budget.</p>
<p>So you can either choose if you want to set a fixed budget. Like for example, last month I had costs of $31. I can set this to $35 ‘cause this is quite close. And if I would like reach this threshold here I would get notified. You can also set a monthly budget planning.</p>
<p>For example, let’s say you have a snowboard rental and you know, there won’t be much users on your platform in the warmer month, like for example, in April till October. So you could set the budget here to like just 100 bucks. But you know, in the cooler months when there’s actually actually snow, you will have much, much more users. So you could set a budget here to 1000 bucks but let’s keep it simple and go with a fixed budget.</p>
<p>You have fear like also many other photos that you also know from the cost explorer demo. The last demo that I just showed you to get more information about the costs you had in the previous months. But let’s configure the thresholds.</p>
<p>You can define your budget thresholds and you can set it either to the actual costs like the cost that like actually occurred to a percentage, like an alert threshold for example, like 80%. So in this case, you would get an alert if 80% of the budget that we just defined in the last step, if we reach this threshold. That will be $28 in this case. And you could also set it based on forecast at cost but this is getting too complex.</p>
<p>Let’s keep it with the actual cost. So here, can you set up the notifications. I can like type in here, my email address and whenever I would reach the threshold I would get an email an alarm that I reached this threshold. And since October 2020, it is also possible to set different kinds of triggers for actions like budget actions. These are based either on identity and access management policies, service control policies or you can also target running instances like EC2 or RDS.</p>
<p>For example, you can choose to apply a custom denied EC2, run instance IAM policy to a user, to a group or to a role in your account once your monthly budget for EC2 has been exceeded. With the same budget threshold, you can configure a second action that targets specific EC2 instances, using a particular region. You can choose to execute actions automatically or make use of a workflow approval process before AWS Budgets execute a request on your behalf.</p>
<p>It’s possible to set up five budget thresholds with up to 10 actions for each threshold. IAM and SEP action type reset at the beginning of each budgeted period. Like in our case, monthly while actions target at a specific EC2 or RDS running instances will not reset.</p>
<p>So we’ve clicked here on the budget actions we activated it, and now we can choose an IAM role that allows budget actions to actually do something with the instances that we are going to define here. So let’s just take this open access role that have defined earlier and we can also choose what should happen.</p>
<p>We can say here if our threshold reaches the budget that we just defined, just stop all EC2 or RDS instances. This is of course, like quite radical step but if you are on a budget, well, you have to do what you have to do, right? So when you can choose if you want to stop EC2 or RDS instances, and you can also select a specific region where this should happen. And if I would click here on “Confirm budget,” the budget would be set. And if the threshold is reached, I would get an email and my EC2 instances would be stepped.</p>
<h1 id="Introduction-to-AWS-Budgets"><a href="#Introduction-to-AWS-Budgets" class="headerlink" title="Introduction to AWS Budgets"></a>Introduction to AWS Budgets</h1><p>Before the cloud, companies often had a fixed procurement process. Companies signed contracts upfront and understood how engineering workloads mapped to software and hardware. Because that process was so well understood, the costs associated with it were understood as well - which meant it was easier to track and control costs. </p>
<p>Now, with cloud computing, costs are variable. With variable usage, you gain speed - you can move quicker and procure the hardware and software you need faster. However, it’s now more difficult to understand the costs associated with that procurement. Often, it requires a change in the procurement process, which means application teams and finance teams need to work better together to determine how to improve planning and control costs. </p>
<p>And to do that, these teams need three things: </p>
<ol>
<li>They need to track AWS usage and costs, set appropriate budgets and receive alerts if they’re exceeding those budgets </li>
<li>They need to provide reports to business leaders and engineering managers to better inform future purchasing decisions and</li>
<li>they not only need to see this information, but they also need to take action and automate responses when they do exceed their budget</li>
</ol>
<p>This is where AWS Budgets comes into play. AWS Budgets has tools that map to each of these requirements. For tracking AWS usage and costs, or Savings Plan and Reserved Instance coverage, you can create a Budget.</p>
<p>For business reporting, you can use AWS Budgets Reports to disseminate information to the right people. And for taking action, you can use AWS Budgets Actions to automate responses if you go over your budget. </p>
<p>Let’s see how each of these tools work together at a high level. You’ll first define your budget, by specifying </p>
<ul>
<li>what you want to track, this could be cost - or how much you’re spending, service usage - how much you’re using, or coverage and utilization for Savings plans and Reserved Instances - are you getting the most out of your reservations </li>
<li>Then you will determine your budget amount, </li>
<li>and last, provide the scope of what this budget applies to - does it only apply to a particular project or service or does it apply to all resources in your account?</li>
</ul>
<p>For example, you can specify a cost budget with a $100 monthly spend as your budget amount that applies to all services in your account. </p>
<p>Then you configure an alert, by specifying a threshold. This threshold is where you specify when you want to be notified. For example, you may want to be notified once you spend 80% of your $100 budget. Once that threshold is reached, the alert will notify you through your choice of email, SNS topic or AWS Chatbot notification. </p>
<p>You can optionally also attach a Budget action to this alert. You can configure one of three automated actions: </p>
<ul>
<li>you can change IAM permissions, </li>
<li>change AWS Organizations permissions, </li>
<li>or stop EC2 or RDS instances.</li>
</ul>
<p>So going back to the previous example, if your alert threshold is met after you’ve spent 80% of your $100 budget, it will not only notify you but also trigger the action you selected automatically or with your approval. </p>
<p>Finally, to get a full report on all your budgets and their status, you can create a budget report and send it out to leadership or other interested parties. This will give them a high-level overview of the status of all budgets and enable them to plan for the future based on this data.</p>
<h1 id="Creating-a-Budget-Demo"><a href="#Creating-a-Budget-Demo" class="headerlink" title="Creating a Budget Demo"></a>Creating a Budget Demo</h1><p>In this video, I’ll create a cost budget to monitor the costs in my AWS account. To do this, I’ll search for Budgets in the AWS console, and click Create Budget. </p>
<p>The first thing I’ll select is the type of budget, I have four options: cost, usage, savings plans and Reserved Instances. I’ll select a cost budget.</p>
<p>From there, you can see my screen has been split. On one side, I’m selecting the options for my budget, and on the other side, it generated a chart from AWS Cost Explorer so I can view the historical cost data in my account. This account is fairly new, only a few months old, so it only has data from the past few months and you can see I generally stick around the $15 range monthly. </p>
<p>Using this data, I can gain a solid understanding of my costs and fill out the rest of my information to create my budget. First, I’ll choose a budget name, I’ll call it TakeMyFifteenDollars. Then I’ll select the time period. My options are daily, monthly, quarterly, and annually. I’ll choose monthly. </p>
<p>Then I can choose to have this budget set to recur after my budget period of a month, or I can have it expire after a certain period of time that I choose. I want this to be ongoing, so I’ll select recurring. </p>
<p>Then I select my threshold. I can either choose a fixed amount, such as $15. Or I can select a planned amount for each month in my budget period, in this case, a year. I can input these values manually, or I can let AWS calculate them based on a starting budget and a percentage of budget growth that I’m expecting.</p>
<p>So for example, if I own a consulting company, and I know the beginning of the year is typically tight budget wise, I can start with a low number - let’s say $100. And let’s say I know I’m going to grow at least 1% every month. AWS will use this information to fill in the data for my period of time, starting with $100 and ending with a compounded budget that accounts for my monthly 1% growth - in this case $111.57.</p>
<p>However, there may be times when you know your spending patterns will change over time, but you don’t know the exact percentage increase. So if you want to avoid having to update and maintain your budget yourself, you can choose the auto-adjusting option. </p>
<p>This, as the name suggests, will adjust your budget automatically based on spending patterns over a time range that you specify. You can choose to base your budget on the forecast for the current month, your bill last month, your average spend over 6 months, or a custom time period. </p>
<p>Since my account is new and I only have data for the past four months, I’m going to use the custom time period and select four months and click apply. Using this, it generates a budget amount for me - in this case it recommends $14 as my budget amount. </p>
<p>If AWS chooses to increase this budget threshold for me, it will notify me through email that it did so. And I can always make updates and edit this budget if I need. </p>
<p>Moving on to the budget scope, I can either choose to track costs from any services or filter based on specific resource attributes. With filters, you can get pretty advanced with how you configure your budgets. For example, EC2 instances for my development environment are often the most expensive part of my bill, so I might choose to add a filter to set a budget for my EC2 instances that are tagged with the tag value “dev”. </p>
<p>However, since I want to monitor costs for my entire account, I’ll choose all services. As far as advanced options, you can choose how to aggregate your costs for your budget. You can choose between unblended, blended, or amortized costs. Additionally, you can also specify whether your budget includes credits, discounts, taxes and more.  </p>
<p>I’m going to leave these options on the default settings and click next. From there, I can create my alert threshold where I can specify a percentage or an absolute value to trigger my alert. For example, I can choose to get notified when I reach the $10 value, or if I’ve spent 80% of my budget. Then I can choose if this is based on actual spend or forecasted. I’ll choose forecasted. If I’m forecasted to spend above 80% of my budget, it will notify me and then I can make changes before I’ve actually spent that money. </p>
<p>Then, I can notify myself through email, or choose to integrate Amazon SNS or Amazon Chatbot alerts. </p>
<p>I can also have multiple alert thresholds to receive additional notifications to have a better pulse on my spending at different times. I’ll leave it at one alert, click next, ignore the Budget actions section for this video, and then I can finally click create budget. And I’m done. From there, AWS will monitor all activity in my account, and send me an email notification if I’m forecasted to spend 80% of my $15 budget.</p>
<h1 id="Budget-Actions-Demo"><a href="#Budget-Actions-Demo" class="headerlink" title="Budget Actions Demo"></a>Budget Actions Demo</h1><p>While Budget alerts are helpful in terms of providing information and visibility, it’s often not enough to solve the spending problem. Typically, you will need to follow that notification with some action. These actions can be manual, such as sending out angry emails to users of your AWS accounts telling them to shut down unneeded resources. Or you can automate specific actions using Budget Actions. </p>
<p>There are three types of automated actions you can take once your budget alert is triggered:</p>
<ol>
<li>The first is IAM policies. With this action, you can choose to change the permissions of users and roles in your account. For example, once the alert is triggered, you may choose to decrease the level of permissions of your users or roles, by changing their policies to “read only policies” until you can figure out what’s going on with the budget. </li>
<li>The second is through Service Control Policies. This is a similar action that can help you change permissions at the AWS Organizations level or Organizational Unit level instead. For example, say your sandbox accounts have reached 80% of their budget, you can choose to limit the sandbox accounts permissions until resources are shut down. </li>
<li>And the third is by stopping EC2 and RDS instances by selecting the instances you want to stop once an alert threshold is crossed.</li>
</ol>
<p>For each of these actions, you can choose to apply the action automatically or through a manual approval process. If you choose the manual approval workflow, once your alert threshold has been reached, you will receive an email letting you know you have an action waiting for you. You can then login to the console and execute the action. If you choose to apply the action automatically, it will not wait for your approval and the action will be applied immediately. </p>
<p>So let’s say I’ve already started the process of creating a new budget, and I’ve already created an alert. Now I need to add on an automated response for this alert. To do this, I’ll attach this new action to the alert I’ve already set up by clicking “add action”. </p>
<p>From there, I’ll select an IAM role with appropriate permissions to run an action. This role uses an AWS-managed policy that has appropriate permissions to stop instances, and change permissions.</p>
<p>And then I can select which action to take. I’m going to choose to stop EC2 instances, as my account is just a sandbox and it’s the fastest way for me to save on cost. From here, I’ll choose the Region, which is us-east-1, and then I’ll select the instance I want to shut down. </p>
<p>Next, I can choose if I want this to happen automatically or go through a manual approval workflow. I’m going to choose the manual approval process, as I want to be extra safe and not shut down an instance I might need in the future. </p>
<p>And then I’ll click create budget. Now we’re finally done, but I’m going to wait some time to see what happens when my budget threshold has been exceeded. </p>
<p>When my alert is triggered, I get two notifications in my email. The first is a notification telling me that my budget has been exceeded. The second notification lets me know that an action is waiting for me in the console. Now I can go into the console to execute that action. Click on actions that require my approval. Scroll down to the actions section, and click the checkbox. And then click run action. Once I do that, I can go to the EC2 console, and check on my instance to see if it is in the stopped state. </p>
<p>Looks like it is, so now we know my action worked.</p>
<h1 id="Budget-Reports-Demo"><a href="#Budget-Reports-Demo" class="headerlink" title="Budget Reports Demo"></a>Budget Reports Demo</h1><p>Visibility into the performance of your budgets enables you to better plan and forecast for the future, and determine areas where the budget needs to increase or decrease. To get this visibility, you can use AWS Budget Reports. </p>
<p>Reports provide a high-level overview of the status of all your budgets and can help inform your decisions.</p>
<p>To create a budget report, you can login to the AWS console. Find the AWS Budgets service, and click on budget reports on the left-hand side. From here, you can click “create report” and then select the budgets you’d like to create a report for. </p>
<p>After that, you can select the frequency in which you’d like to receive these reports, whether that’s daily, weekly, or monthly. Keep in mind that each report costs $0.01. Then, you can specify the email addresses that you’d like to receive the reports. You can put up to 50 email addresses in this box but since I don’t know 50 people, I’ll just specify my own email address, <a href="mailto:alana.layton@cloudacademy.com">alana.layton@cloudacademy.com</a>.</p>
<p>Last, you can name your report and then click create report. Once you create this report, your reports will then be delivered to the email addresses you specified at the interval you selected.</p>
<p>Now let’s take a look at one of these emails and see what it looks like. This is an example of a report I created for two budgets in my account. The first budget called “CA Budget” tracks my daily spend. As you can see, I typically spend around $0.49 daily, and my current spend has exceeded that, at $.61. Additionally I have another budget called “EC2 Usage” that tracks my EC2 instance running hours. I budgeted 30 instance running hours, and currently I’m above that at 48 hours. </p>
<p>Now I know the status of each of my budgets. And it’s clear that I need to make some changes, as I have greatly exceeded my expected cost and usage. So, I’m off to go stop a few of my EC2 instances, which will reduce my running hours and my cost and hopefully my report will look better tomorrow. </p>
<p>See you next time! </p>
<h1 id="AWS-Cost-Anomaly-Detection"><a href="#AWS-Cost-Anomaly-Detection" class="headerlink" title="AWS Cost Anomaly Detection"></a>AWS Cost Anomaly Detection</h1><p>Everyone has a story about an AWS bill - and usually, it takes hearing just one of these stories for you to become obsessive about tracking your AWS spend. And while AWS budgets help with this paranoia, it’s not a perfect science. </p>
<p>There may be times when you exceed your budget, but it’s because of business growth. You’re using more and racking up additional costs to better serve your customers - which is a good thing. Or there may be times where you’ve set a budget and you have a spike that’s not normal for your business but it remains under the threshold, so you don’t get alerted…which isn’t a good thing. </p>
<p>This leads us to two fundamental truths: </p>
<ol>
<li>Every business hates surprise spikes in cost and, </li>
<li>Every business contextualizes these spikes in costs and determines if they are “good” or “bad” based on their own unique spending patterns</li>
</ol>
<p>Analyzing these cost spikes used to be a very manual process but now there are tools that can help us automate it, and one of those is called AWS Cost Anomaly Detection. </p>
<p>This service helps you gain an understanding of what is normal or not normal in terms of spending for your AWS accounts. And any time you have a random spike in spending that is deemed “not normal”, you can not only be alerted of that spike, but you can also investigate why it happened.</p>
<p>Like AWS Budgets, the first step in using Cost Anomaly Detection is to create an alert - which is called a cost monitor in this service. You can choose to evaluate your costs based on AWS service, or by linked accounts, cost allocation tags, or cost categories. </p>
<p>This choice is dependent on how you track your costs in your cloud environment. Lets use an example - say you’re in an organization that segments cost by project. You can do this in many different ways in AWS. For example, you may use AWS accounts to segment costs for each project or you may use tags, and tag resources based on a particular project or you may even create your own custom resource groupings using cost categories. Or perhaps you’re more interested in segmenting spend by AWS service, to monitor each service spend individually for your projects instead. So depending on how you filter your spend, you’ll choose the corresponding monitor to match. </p>
<p>When you create a cost monitor, you have to specify a threshold which will determine when the service sends you a notification. This threshold is defined as the difference between actual spend and your normal spend pattern. For example, let’s say you set your threshold at $25. And your normal spend is $50. When your daily spend reaches $75, which is $25 past your normal $50 spend, then you’ll be alerted of the anomaly. </p>
<p>Keep in mind this threshold only defines when to alert you and does not determine what an anomaly looks like for your account. And actually, you don’t have to define that anywhere - the service will define what an anomaly looks like based on your own spending patterns by using machine learning. </p>
<p>From there, you can choose to get notified as soon as the anomaly is detected, or in a daily or weekly summary. Instead of receiving a notification anytime an anomaly is detected, these daily or weekly summaries will consolidate all anomalies that occurred within that day or that week.</p>
<p>Once the anomaly reaches the threshold that you set, you can choose to get notified through email, SNS, or the AWS Chatbot service. </p>
<p>However, occasionally, there may also be times where there is an anomaly in your account but it doesn’t reach the threshold you set to be notified. This is where the detection history section of the Cost Anomaly Detection dashboard comes in handy. You can view your entire history of anomalies.</p>
<p>In detection history, you can inspect each anomaly in further detail. When inspecting an anomaly, you can take several actions, for example: </p>
<ul>
<li>You can view the anomaly in cost explorer to filter out details on a more granular level. </li>
<li>You can view the root cause analysis, which the service identifies as the “best guess” to what may have caused the spike. </li>
<li>And you can also submit assessments of the anomaly to better train the model to better learn your unique patterns of spending.</li>
</ul>
<p>In summary, Cost Anomaly Detection helps detect one time cost spikes and continuous cost increases. This service is technically considered a free service, so consider using this in conjunction with AWS budgets. This combination of services will provide you more visibility into your spending patterns and any strange cost spikes, enabling you to better plan for the future. </p>
<h1 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h1><p>The essence of a cost allocation strategy is the ability to tell how much is spent on which resource on which service. This type of visibility can be best achieved by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/introduction/">tagging</a> every single resource in your cloud. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> enables the user to put tags on every available resource. You can use tags for many things. But for this course, we’re just going to focus on how to use them for cost allocation. So let’s find out what tags actually are. </p>
<p>Tags provide the functionality to define metadata in the form of key and value pairs. These on the other hand are associated with the resources in a cloud account. Let’s have a look at this diagram. In this example, we’re looking at four resources. Don’t mind the details because it doesn’t really matter what kind of resources they are. These are just some key samples. So each of them has a tag, which goes by the key-value environment production. This one is true for every four of them. This means that all of these resources belongs to the production stage of our environment.</p>
<p>The next one distinguishes our resources between the frontend and backend, and basically tells us right away which resources host a front-end service and which hosts a back-end service. Typically, business tags such as cost center, business unit, or project are used to associate AWS costs with traditional financial reporting within an organization. However, a cost allocation report is not static and hence can include any tag. This allows customers to easily link costs to technical or security dimensions, such as specific applications, environments, or compliance programs.</p>
<p>With AWS Cost Explorer and Cost and Usage Report, AWS costs can even be viewed according to tags, providing even more insightful cost visualizations. AWS Cost and Usage report is otherwise known as AWS CUR. Just so you know what the Cost and Usage Report is, I wanted to drop in a short explanation. With the help of AWS Cost and Usage reports, you can track the monthly AWS costs and usage associated with your AWS account. The report includes items for each unique combination of product, usage type, and operation that is used in your AWS environment. It enables you to configure the AWS Cost and Usage report to show only the data that you want, using the AWS Cost and Usage API.</p>
<p>AWS Cost and Usage Reports contain the widest variety of cost and usage data. You can set up the CUR to collect billing data for any given period and push it into an Amazon S3 bucket to store it there for whenever you need it. You can get hourly, daily, or monthly reports. These contain the costs in detail and are sorted by product or resource. If tags are used properly, they are also listed in the report and can provide extra detail to your bill. A report is updated at least once per day and up to three times. They can be stored in an S3 bucket of your choice and be retrieved whenever needed, either manually or by using another service. </p>
<p>After you set up a cost and usage report, you receive the current month’s billing data and daily updates in the same Amazon S3 bucket. The data from the CUR forms the base for a detailed and complete cost analysis. It is often the main part for many business intelligence tools, like Athena and QuickSight, just to name a few. And CUR can also be reached by an API, which you can use for your custom scripts or individual needs.</p>
<p>So, in conclusion, with AWS CUR, you are able to store your report files in Amazon S3 buckets, update the report automatically, up to three times a day, make use of the AWS CUR API for automation or easier management through API calls, and use the CUR for in-depth analysis with business intelligence tools like QuickSight, Athena, and others. That’s about it for the AWS Cost and Usage Reports, so let’s continue. </p>
<p>Keeping cost allocation in mind, the best way to assign business-context details to specific resources is by using tags. Later on in the process, this enables you to carry out a more valuable analysis based on your cost data and facilitates company-specific decision making by a well-evaluated foundation of data. If you take bill analysis into consideration, tags can add business dimension and context to ease the allocation process. </p>
<p>Tags are used to identify which item or resource in your cloud is attributed to each of your business services. So you can always tell exactly which resources are used for which service in your company. Nevertheless, keep in mind that tags are only meaningful to their respective user or a customer. They literally do not have any semantic meaning. You can name them whatever you like and assign a value to them. However, when used correctly, they can help read and analyze your data and even automate your analysis with the right setup.</p>
<p>In AWS, you can manage tags in the service console or accessing the API through AWS CLI, although this limits you to only one resource at a time. If you want to add, edit, or delete tags on multiple resources, it is best to use a service, for example, the AWS Tag editor. Once you have tagged your resources, you can enable Cost Allocation Tags in the Billing and Cost Management sections. We will discuss how to tag and activate the Cost Allocation tags in a minute. One significant thing to note here is that tagging existing resources retroactively is pretty annoying. So make sure to tag your resources from the very beginning.</p>
<p>In the best case, policies can prohibit the deployment of new resources without the appropriate tags. But more about that later. If you want to analyze a cost report after the fact with unlabeled or poorly labeled resources, you will have a hard time understanding the exact usage of each resource, and will likely not be able to identify the exact costs and usage by resource. So, it is advised to start tagging resources as soon as possible and stay consistent with your tagging strategy.</p>
<p>Planning out a tagging strategy or a tagging standard is essential, and the best time to implement one is before a company launches its cloud resources. It’s best to keep tagging simple and easy to grasp. Don’t overdo it for the sake of it. After all, you want to gain visibility, not cause confusion. The best thing to do is to learn about predefined tags and choose the ones suitable for your business. It’s also advisable to adjust your tags to follow your KPIs once you determine them.</p>
<h1 id="Tagging-Best-Practices"><a href="#Tagging-Best-Practices" class="headerlink" title="Tagging Best Practices"></a>Tagging Best Practices</h1><p>We’ve picked a few best practice examples for you to apply for your business or organization. Let’s start with some common tags that are used by most organizations. Of course, these are just some ideas and you need to use tags that fit your business case. Some common examples include Cost Center or Business Unit tag, used to show where resource costs are allocated within the organization, and it also allows correct cost allocation within billing data.</p>
<p>Service&#x2F;Workload name tag. This shows which service the resource belongs to. Resource Owner tag. This is responsible for the resource. Simple Resource Name tag. This is something easier to read and to remember than the default tags. And Environment tag. It determines the cost difference between different environments. For example, dev, test&#x2F;stage, production. Check your cloud and see whether these tags can help you get started with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/tagging/">tagging</a>. Also make sure to check <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> pre-generated tags. They might save you some time.</p>
<p>Now let’s look at some tagging best practices. So, number one, align tags to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/aws-generated-cost-allocation-tags/">cost allocation strategy</a>. Before you start tagging, you should think of a general cost management strategy. Think of tags that help you to track and allocate expenses and make those tags align with your strategy. Next, tag everything. Tag as many resources as possible so that no resource is left untagged. Make this a rule. In fact, you can roll out policies in your cloud environment that will forbid launching resources without tags.</p>
<p>Next, find a purpose for each tag. Think of a certain use case before adding a tag. Otherwise you will have a hard time justifying your tags and you risk running into a mess of baseless tags. That now leads me onto the next point. Limit the number of tags you adopt. Find redundancies and overlapping tags and simplify them. There’s no point in releasing multiple tags that cover the same subject. Look for tags that might logically overlap. See where you might merge them and reduce the number of your overall tags. And keep it manageable. Obviously, the more tags you have, the more tags you have to deal with. Keep the number as low as necessary, but the information value as high as possible.</p>
<p>Next, consistency is key. Use a consistent naming convention. This helps to keep an overview and eases further processing. Giving your tags less abstract names, and instead naming them with descriptive terms also makes them easier to read. Automate tag management. Make use of tools like the AWS tag editor to automate your tagging. Avoid wasting time on repetitive tasks and use automation as much as possible. Set up policies to forbid launching untagged resources. This is an easy way to ensure that no new resources are slipping into your environment without a tag.</p>
<p>And finally, audit and maintain your tags. Make it a habit to review tags from time to time and verify their purpose. Tag maintenance is essential and should involve everyone on the team. So make it a recurring task for everyone and have everyone keep their eyes open for suggestions for improvement.</p>
<h1 id="AWS-Generated-Cost-Allocation-Tags"><a href="#AWS-Generated-Cost-Allocation-Tags" class="headerlink" title="AWS-Generated Cost Allocation Tags"></a>AWS-Generated Cost Allocation Tags</h1><p>The default tags in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> are basically cost allocation tags. They can be activated in the billing section, which we will explain in a minute. Cost allocation tags are special tags that are used by Cost Explorer and other services for allocation and visualization. So they can be explicitly used and displayed in the various views of various services. So take a look at the screenshot. Here we can see the Cost allocation tags section in AWS Billing. We can tell that CostCenter and Name are active and enabled as cost allocation tags, while the others are inactive. These inactive tags were once set for resources that are not in use any longer.</p>
<p>Once the cost allocation tags are activated, the console detects all tag keys used and suggests those for activation as cost allocation tags as well. Otherwise, they won’t show up in Cost Explorer charts. The term User Defined means that a user created these tags and that they are therefore custom tags. AWS generated tags were automatically generated, such as createdBy, the createdBy Amazon WorkSpaces Tag. These were created and applied to support AWS resources for the purpose of cost allocation. They can only be view in the AWS Billing and Cost Management console and reports. They do not appear anywhere else in the AWS console, including the AWS Tag Editor.</p>
<p>Also, note that in this Cost allocation tags section, all the tag keys that are and were used in the account will be shown here. However, if you don’t enable the Cost Allocation Tags option, you will not be able to evaluate certain views in the Cost Explorer and other services. For example, if I want to know what costs have been incurred for all resources with a specific cost allocation tag, then I won’t be able to select or see them.</p>
<p>To prevent you from failing to find your resources with those cost allocation tags, I will show you how to enable this option. Let’s see how you can enable AWS generated Cost allocation tags. First, you need to log in to your Master account as an IAM user with the required permissions. Next, type Billing into the search field and go to the Billing console. Select Cost Allocation Tags on the left side menu. And then, simply click on Activate to enable the tags.</p>
<h1 id="Data-Visualization-Services"><a href="#Data-Visualization-Services" class="headerlink" title="Data Visualization Services"></a>Data Visualization Services</h1><p>Let’s start by introducing the visualization tools and services that we have available to us. AWS provides a suite of tools to help you present and manipulate big data results. Amazon Athena is an interactive query service that makes it easy to analyze data that’s stored in Amazon S3 using standard sequel. Athena is server-less, so there is no infrastructure to manage, and you pay only for the queries that you run. Athena uses Presto with ANSI SQL and works with a variety of standard data formats. They include CSV, JSON, ORC, Avro, and Parquet. </p>
<p>With Athena there is no need for complex ETL jobs to prepare your data for analysis. Now this makes it easy for anyone with SQL skills to quickly analyze large scale data sets. Another benefit is that Athena uses Amazon S3 as it’s underlying data store, so you’re data remains durable and highly available. Amazon Athena is integrated with AWS Glue, so you can use the Glue’s ETO capabilities to transform data or use the Glue data catalog, which is a powerful unification tool. </p>
<p>Now you could look to use this to create a unified MIDI data repository, for example, that could run across a number of data services. Amazon QuickSight makes it easy to build visualizations, perform ad hoc analysis, and quickly get business insides from your data. It has a number of pre-configured reports which take out the undifferentiated heavy lifting of creating visual reports. Let’s think through how QuickSight can be used. Data dashboarding is often a core requirement in business reporting, and a common-use case is business reporting on the data we might have stored in a data store.</p>
<p>Let’s envisage our data warehouse make up new data on a nightly basis from a number of different sources. So we need to ingest and transform that data quickly so the data is ready and consumable in the morning when the CEO and other business users come in and need to generate reports. So we might have a number of transformation jobs that put formatted and clean data into Amazon S3. </p>
<p>Now we use Amazon S3 as our data store, as it is highly durable, and Amazon Redshift can consume this data on multiple threads in parallel from each Redshift node. So data processing will be really fast. It is also for data on Amazon S3 to be consumed by other analytics, tools, or services if we add them. For visualizing analytics, we can use Amazon QuickSight or one of the many partner visualization platforms listed in the marketplace using the ODBC&#x2F;JDBC connection to Amazon Redshift. </p>
<p>Now the benefit of QuickSight is that it’s integrated into our AWS dashboard and our AWS account, and this is where reports and graphs can be viewed by the CEO and his staff. When we create visuals, the style and format of graphs is automatically selected by the QuickSight engine, which saves time and improves the quality of reports and visuals. QuickSight also makes it easy for business teams to create and share interactive graphs and reports as stories, and if we have any additional data sources added in the future, those can just simply be added as Amazon QuickSight database sources. Now once we are in QuickSight we can create visuals and scenes that provide information relevant to different business units or reporting agendas. </p>
<p>QuickSight also enables us to share graphs, reports, or business insights as creative stories. A story is a collection of interactive visuals that can be easily shared with other people. Now at the heart of the QuickSight service is the Spice engine. So QuickSight uses Spice which stands for Super fast Parallel In-memory Calculation Engine, and Amazon has developed this to run natively in AWS. So it has been from the ground up for the AWS cloud. </p>
<p>Now Spice uses a combination of data compression, columnar storage, machine code generation, and in-memory technologies enabled to the latest hardware innovations. Spice automatically replicates data for higher availability, and also enables Amazon QuickSight to support interactive analysis across a wide variety of AWS data sources. Now we don’t need to be an expert in how it all works, but what is does mean is that we can run interactive queries on massive data sets and get really fast results. Spice capacity is allocated by region. So the information displayed is for the currently selected region you have. You can see how much Spice capacity you are using and how much there is overall from the AWS console. </p>
<p>Currently each Amazon QuickSight account receives 10 gigabytes of Spice capacity per paid user, and that is allocated when you log into QuickSight for the first time. This limit will no doubt change over time so do check the current account limits. If space is a concern for your use cases. You get one free user per account, and the Spice capacity is pooled across all users for your Amazon QuickSight account. So each QuickSight account receives one gigabyte of Spice capacity. So if you have four users, say one free and three paid for, you’ll have 31 gigabytes of Spice capacity available. Now that can be utilized by any of the users in the account. </p>
<p>All of your default Spice capacity is allocated to your home region, and the other regions have no Spice capacity unless you choose to purchase some, okay? Now as your usage of QuickSight increases, housekeeping does become important, and you can release purchased spice capacity that you aren’t using to free up capacity. To free up Spice capacity, you delete any unused data sets that you haven’t ported into spice. </p>
<p>Now keep in mind purchasing or releasing Spice capacity only affects the capacity for that currently selected region. You can purchase up to one terabyte of additional Spice capacity per QuickSight account if you need it. If you do find yourself low on Spice Capacity, you can also choose the buy Spice alert that appears on the your data sets and create a data set pages in the console, and if you need more capacity than that, you can submit a limit increase request to AWS support following the AWS service limits instructions. A neat benefit with QuickSight is that it’s very easy to connect QuickSight to data sources. You can upload CSV or Excel files, ingest data from AWS data sources such as Amazon S3, Amazon Redshift, Amazon RDS, or Amazon Aurora, or Amazon Athena, and Amazon Elastic produce, which is Presto, and Apache, and Spark. </p>
<p>Now you can also connect to cloud or on premise databases such as MySQL, Sequel Server, and Postgres, and you can also connect to SAS applications like Sales Force. You can prepare data in any data set to make it more suitable for analysis. You can change field names or add a calculated field. You can also do Joins on database tables using structured query language or SQL. You’ll find it relatively limited if you need to do complex cascades or select into statements or on inner Joins. The Join interface doesn’t let you use any additional SQL statements to refine the data set. </p>
<p>A couple of points to remember, the target of the join has to be a Spice dataset for Joins. For both, datasets have to be based in the same sequel database data source, alright? So you cannot do Joins across two independent data sources. The fields used in the joins cannot be calculated fields. So if you’ve edited a date collect calculation or similar that can’t be part of your Joins statement. If you do need to run a lot of conditional logic then you might want to consider using Amazon Athena as it provides more functional support and flexibility in manipulating data. </p>
<p>You can use calculated fields to use common operators or functions to analyze or transform field data. You can use multiple functions and operators in a calculated field. So for example, you might use the format date function to extract the year from a date field and then the if else function to segment records based on that year. A calculated field has to be from the QuickSight data source, just FYI. A lot of common function types are supported as you can see from this visual on the screen. </p>
<p>Now Amazon QuickSight supports assorted visualizations that facilitate different analytical approaches. To create a visualization you start by selecting the data fields you want to analyze or drag the fields directly onto the visual canvas. We can do a combination of both. QuickSight will automatically select an appropriate visualization to display your data based on the data that you’ve selected. </p>
<p>Now it does this using a proprietary technology called Autograph. Autograph allows QuickSight to select the most appropriate visualizations based on the properties of the data such as cardinality and data type. Pretty clever. The visualization types are chosen to best reveal the data and the relationships in the most effective way. QuickSight is super-intuitive. It selects the display time that best suits the record types you have in your results set. </p>
<p>Now this is a real time saver if you just need to show results quickly. You can alter the visualization type to include fields or views you prefer by adding a field from the field selector. If you add a new view or field it automatically is added to the field types menu which is super-cool. If you add an additional sort field, the graph visual style automatically updates from a bar graph to a line graph to better represent your results set. Brilliant! You can also resort back to the default visualization at any time by clicking the menu option. </p>
<p>Okay, so let’s walk through a demo. So we choose a data set and then we choose create analysis. If we don’t have any data sets yet, we’ll create a new one by choosing new data set. Now at this point you’ll notice there’s an auto-save option up there in the menu bar. Auto-save is on by default when you’re working on an analysis. When it’s on you’re changes are automatically saved every minute or so. I’m not sure exactly what the timing is. When auto-save is off, your changes are not automatically saved, okay? </p>
<p>So that’s useful if you want to try out a different analysis or display style, or perhaps show a certain variation or view without changing your core analysis. The undo feature works when either auto-save mode is on or off. So you can undo or redo any change you make by using undo or redo from the application bar.</p>
<h1 id="AWS-Glue-Data-Catalog-Primer"><a href="#AWS-Glue-Data-Catalog-Primer" class="headerlink" title="AWS Glue Data Catalog Primer"></a>AWS Glue Data Catalog Primer</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="AWS-Glue-Data-Catalog-Primer-1"><a href="#AWS-Glue-Data-Catalog-Primer-1" class="headerlink" title="AWS Glue Data Catalog Primer"></a>AWS Glue Data Catalog Primer</h1><p>AWS Glue historically was only an ETL service. Since then, the service has turned into a suite of data integration tools. Now, AWS Glue is made up of four different services: </p>
<ol>
<li>Glue Data Catalog</li>
<li>Glue Studio</li>
<li>Glue DataBrew, and </li>
<li>Glue Elastic Views. Glue Elastic Views is out of scope for this content, so I won’t be talking about it in this lecture. If you’re interested in Glue Elastic Views, I will link a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-reinvent-2020-aws-glue-elastic-views-1209/">course</a> specifically for that topic.</li>
</ol>
<p>In this lecture, I’ll mainly focus on the Glue Data Catalog aspect of this service.  </p>
<h3 id="AWS-Glue-Data-Catalog"><a href="#AWS-Glue-Data-Catalog" class="headerlink" title="AWS Glue Data Catalog"></a>AWS Glue Data Catalog</h3><p>AWS defines the Glue Data Catalog as a central metadata repository. This means that it stores data about your data. This includes information like data format, data location, and schema. Here’s how it works: </p>
<p>You upload your data to storage like Amazon S3, or a database like Amazon DynamoDB, Amazon Redshift, or Amazon RDS. From there, you can use a Glue Crawler to connect to your data source, parse through your data, and then infer the column name and data type for all of your data. The Crawler does this by using Classifiers, which actually read the data from your storage. You can use built-in Classifiers or custom Classifiers you write to identify your schema. </p>
<p>Once it infers the schema, it will create a new catalog table with information about the schema, the metadata, and where the source data is stored. You can have many tables filled with schema data from multiple sources. These tables are housed in what’s called a database. </p>
<p>Note, that your data still lives in the location where you originally uploaded it, but now you also have a representation of the schema and metadata for that data in the catalog tables. This means your code doesn’t necessarily need to know where the data is stored, and can reference the Data Catalog for this information instead. </p>
<p>That’s it for this one. See you soon!</p>
<h1 id="ETL-with-AWS-Glue-Studio"><a href="#ETL-with-AWS-Glue-Studio" class="headerlink" title="ETL with AWS Glue Studio"></a>ETL with AWS Glue Studio</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="ETL-with-AWS-Glue-Studio-1"><a href="#ETL-with-AWS-Glue-Studio-1" class="headerlink" title="ETL with AWS Glue Studio"></a>ETL with AWS Glue Studio</h1><p>Hello and welcome to this lecture where I’ll be discussing AWS Glue Studio, which is one of the tools available in the AWS Glue ecosystem. AWS Glue Studio is where you create, submit and monitor your ETL jobs. </p>
<p>With AWS Glue Studio, every ETL job consists of at least three things: </p>
<ol>
<li><strong>A data source.</strong> This could be the Data Catalog, or a service like Amazon S3, Amazon Kinesis, Redshift, RDS, DynamoDB or another JDBC source. </li>
<li>Then, you need a <strong>transformation script</strong>. Glue will use the data from your source and process it according to the transformation script you write. You can write these in either Python or Scala. </li>
<li>Last, you need a <strong>target</strong>. Glue will export the output to a target of your choice, such as the Data Catalog, Amazon S3, Redshift or a JDBC source.</li>
</ol>
<p>Let’s look at Glue Studio in the Console. Here I am in the Job dashboard of the service. If I want to create a job, you can see there are many options to do so. However, they are categorized in one of two ways: I can either create a job programmatically or I can use a visual interface. </p>
<p>For example, if I click the visual with a blank canvas option and click create. I can then create graphical relationships between a source, transformation scripts, and a target destination.</p>
<p>Let’s build one quickly. I can use the Data Catalog as my source. For my transformation script, I’ll use a built-in script called Rename Field, that renames a key in my data set to another name. Then, I can output the transformation to an Amazon S3 bucket. I can additionally choose to update my Data Catalog or not. </p>
<p>While this is a pretty simple ETL job, you can create more complex relationships and graphs between services without coding at all, and Glue will generate the Apache Spark code for you behind the scenes. Note, that if you want a true no-code tool for creating ETL jobs, this won’t really provide you with that, as the built-in transformation scripts in Glue Studio are very limited. You only have about 10 options or so here. If you feel comfortable with coding, you can create custom transformation scripts in this interface using Python or Scala as well.</p>
<p>However, there are better places where you can develop your own custom scripts. For example, if I click back, you can see the other options for programmatically creating scripts, such as the </p>
<p>Spark script editor, the Python shell script editor, or the built-in Jupyter Notebook interface to create Python or Scala job scripts.</p>
<p>That’s it for this one - see you next time.</p>
<h1 id="AWS-Glue-DataBrew-vs-Glue-Studio"><a href="#AWS-Glue-DataBrew-vs-Glue-Studio" class="headerlink" title="AWS Glue DataBrew vs. Glue Studio"></a>AWS Glue DataBrew vs. Glue Studio</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="AWS-Glue-DataBrew-vs-Glue-Studio-1"><a href="#AWS-Glue-DataBrew-vs-Glue-Studio-1" class="headerlink" title="AWS Glue DataBrew vs. Glue Studio"></a>AWS Glue DataBrew vs. Glue Studio</h1><p>A few years ago, Glue released another transformation tool called Glue DataBrew. On the surface, DataBrew looks very similar to Glue Studio. So, what is Glue DataBrew?</p>
<p>Glue DataBrew is a true no-code service for transforming data. Here’s how it works: </p>
<p>You first <strong>upload your data</strong>. You can upload it directly to the service, or connect to other data sources like Amazon S3, Amazon Aurora, Amazon Redshift, Glue Data Catalog, or other JDBC Connections. It can additionally connect to AppFlow, Data Exchange and Snowflake. </p>
<p>Once you upload your data, you can preview your data in a visual interface. From there you can <strong>choose from hundreds of built-in transformations</strong>. Some of these transformations include formatting your data, modifying columns, working with duplicate or missing values, encoding data, and more. </p>
<p>Once you apply your transformation, you can <strong>store the output in Amazon S3</strong>. Note that Amazon S3 is the only place you can store your transformed data. </p>
<h3 id="Glue-DataBrew-vs-Glue-Studio"><a href="#Glue-DataBrew-vs-Glue-Studio" class="headerlink" title="Glue DataBrew vs Glue Studio"></a>Glue DataBrew vs Glue Studio</h3><p>So if both of these services provide transformations, function in similar ways, and if Glue Data Studio also provides some no-code options, which service do you use? </p>
<p>Well, there are a four main differences between the two that might help you distinguish when to use each service: </p>
<p>\1. No-Code vs Custom Code </p>
<p>Glue DataBrew is a no-code tool. Unlike Glue Studio, you can’t write your own custom code for transformations even if you wanted to. However, that means that DataBrew provides a lot more options for built-in transformations. DataBrew has over 250+ built-in transformations, while Glue Studio has around 10. These transformations are different as well. Glue Studio built-in transformations focus mostly on ETL, while DataBrew’s transformations mostly prepare data for machine learning.</p>
<h4 id="2-Different-Tools-for-Different-Users"><a href="#2-Different-Tools-for-Different-Users" class="headerlink" title="2. Different Tools for Different Users"></a>2. Different Tools for Different Users</h4><p>These services are meant for different audiences. Glue Studio is meant for ETL engineers and is focused on ETL itself, while Glue DataBrew is mostly for business analysts and data scientists that may not have coding experience. You don’t need specialized expertise to transform data with DataBrew. </p>
<h4 id="3-Programmatic-Creation-of-ETL-Jobs"><a href="#3-Programmatic-Creation-of-ETL-Jobs" class="headerlink" title="3. Programmatic Creation of ETL Jobs"></a>3. Programmatic Creation of ETL Jobs</h4><p>Both services provide a graphical interface for visualizing your transformations. Glue Studio, however, is the only option that provides programmatic opportunities for working with ETL through Jupyter notebooks and shell scripts. </p>
<h4 id="4-Data-Profiling"><a href="#4-Data-Profiling" class="headerlink" title="4. Data Profiling"></a>4. Data Profiling</h4><p>DataBrew has a profiling feature, which enables you to get statistics about your data. For example, with profiling, you can get information about how many rows you have in your data set or how many unique values you have in each column. Glue Studio does not have a data profiling feature. </p>
<p>That’s it for this one - see you next time!</p>
<h1 id="Amazon-EMR-vs-AWS-Glue-for-ETL"><a href="#Amazon-EMR-vs-AWS-Glue-for-ETL" class="headerlink" title="Amazon EMR vs. AWS Glue for ETL"></a>Amazon EMR vs. AWS Glue for ETL</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="Amazon-EMR-vs-AWS-Glue-for-ETL-1"><a href="#Amazon-EMR-vs-AWS-Glue-for-ETL-1" class="headerlink" title="Amazon EMR vs. AWS Glue for ETL"></a>Amazon EMR vs. AWS Glue for ETL</h1><p>In this video, I’ll be comparing Amazon EMR vs AWS Glue for ETL. Before we get deeper into the two services, it’s important to note that Amazon EMR does have multiple deployment options. You can use EMR on EC2, on EKS, or use EMR serverless. In this video, I’ll be focusing mostly on EMR on EC2, with some mentions to EMR serverless. </p>
<p>With that being said, our next fight for the evening is: AWS Glue taking on Amazon EMR. Who will win? </p>
<p>In one corner, we have Amazon EMR, a big data platform that’s designed not only for ETL, but also for machine learning and data analysis. </p>
<p>In the other corner, we have AWS Glue, a data integration service, that provides Glue Studio for ETL. It also includes a Data Catalog, Glue DataBrew for no-code transformations, and Glue Elastic Views. </p>
<p>Let’s look at these two services from three different perspectives</p>
<ol>
<li>Ease of use </li>
<li>Pricing</li>
<li>Limitations</li>
</ol>
<h3 id="Ease-of-Use"><a href="#Ease-of-Use" class="headerlink" title="Ease of Use"></a>Ease of Use</h3><p>Ease of use for any tool in AWS is often inversely related to control. Tools that AWS says are “ easy to use” generally provide the user with less control over the service. The same is true the other way around, tools that provide a lot of control are typically more complex to use. </p>
<p>You can see this clearly with EMR and Glue. For example, EMR on EC2 provides maximum control over the service. You can optimize, manage, and scale your cluster and compute nodes. You can take advantage of EC2 instance types, sizes, and pricing options such as Spot, Reservations, and Savings Plans. You can install a wide range of open source tools, such as Hive, Presto, HBase, Spark, and more to fit your use case. And you can choose how long you run your EMR cluster. It could be a longer running cluster that is available 24&#x2F;7, or it could be a transient cluster that’s provisioned, runs the proposed jobs, and then terminates soon after. You have control over all of it. </p>
<p>However, the freedom of choice can make the service more complex to manage. Configuring and maintaining the engine and the cluster can be a full time job. You may need to dedicate resources in your engineering teams to manage this underlying infrastructure.</p>
<p>With Glue, your choices decrease because it is serverless. You no longer get to manage the underlying EC2 instances and storage. All cluster, node, and engine maintenance disappears. From the infrastructure maintenance perspective, it is simpler. However, that also means you no longer get to choose EC2 instance types, sizes, or pricing options. And you also no longer get to choose from a range of open source engines. Glue can only run your ETL jobs in an Apache Spark environment. The other factor is that Glue terminates as soon as your job finishes executing, so support for longer-running clusters is not possible.</p>
<h3 id="Pricing"><a href="#Pricing" class="headerlink" title="Pricing"></a>Pricing</h3><p>The convenience of serverless is helpful, but there is a price for this convenience. Glue is, at face value, more expensive than EMR on EC2. This is a common tradeoff in AWS, where you have to decide if the convenience of not having to configure and manage a cluster is worth it. However, before you go with the cheaper option, you have to factor in additional costs with EMR, such as what it takes to maintain a cluster. You might need to factor in the cost of a cluster administrator into your total cost analysis. With all costs considered, you might even find that Glue may be cheaper in the long run. </p>
<p>Another factor to consider is that Glue terminates as soon as the job executes. With Glue, you only pay for the time it runs. If you have longer-running EMR clusters, you will pay for idle time where the cluster is sitting there, not performing any work.</p>
<h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>With Glue, there are three limitations you need to be aware of:</p>
<ol>
<li>It has a default limitation on how much CPU and RAM you can use for your jobs. The biggest worker type you can use currently has 8 vCPU and 32 GB of RAM. The number of workers you can scale up to in Glue by default is 100. So if you need more performance than Glue can provide to you, EMR is the better choice. </li>
<li>Glue is a true ETL service. While it can do light machine learning analysis and can be paired with Amazon Athena for data analysis, Amazon EMR outperforms Glue for both Machine Learning and with data analysis using engines like Presto. </li>
<li>Ultimately, if your workload requires any other engine other than Spark, you should use EMR.</li>
</ol>
<h3 id="EMR-Serverless-vs-Glue"><a href="#EMR-Serverless-vs-Glue" class="headerlink" title="EMR Serverless vs. Glue"></a>EMR Serverless vs. Glue</h3><p>EMR on EC2 vs Glue is a fairly straightforward comparison. However, the differences between EMR Serverless and Glue are a little less obvious. Both EMR Serverless and Glue require no infrastructure maintenance and are more expensive than EMR on EC2. The biggest difference is use case. Like EMR on EC2, you can use EMR Serverless for use cases beyond ETL. With Glue, while it does support light machine learning transformations, it is mostly considered an ETL tool. Because of this, Glue Studio offers more ETL tooling that EMR serverless doesn’t natively support, such as a graphical ETL interface, built-in scheduling, and the ability to build pipelines from Glue components. </p>
<p>If you already use EMR and have pre-existing Spark or Hive jobs, it may be worthwhile to consider running these jobs on EMR serverless. This lets you use a familiar tool without the maintenance of cluster management. </p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Ultimately, if you need flexibility with how you manage the engine or the underlying infrastructure, EMR on EC2 is best for you. Otherwise, if you need to run short-lived jobs that will run in an Apache Spark environment, Glue or EMR Serverless will save you time by managing the infrastructure for you.</p>
<h1 id="Orchestrating-ETL-Workflows"><a href="#Orchestrating-ETL-Workflows" class="headerlink" title="Orchestrating ETL Workflows"></a>Orchestrating ETL Workflows</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="Orchestrating-ETL-Workflows-1"><a href="#Orchestrating-ETL-Workflows-1" class="headerlink" title="Orchestrating ETL Workflows"></a>Orchestrating ETL Workflows</h1><p>ETL pipelines can be complex. You may be running multiple ETL jobs at once, at varying intervals of time (maybe hourly, daily, weekly), involving multiple AWS services. It’s important you have a service that not only triggers your pipeline to run, but also automates the movement between services, while also handling basic retry logic and error handling. </p>
<p>To do this, you can use orchestration services. There are three main orchestration services that can be used in combination with ETL services, like Amazon EMR and AWS Glue. These services are:</p>
<ul>
<li>AWS Data Pipeline</li>
<li>AWS Step Functions</li>
<li>And surprisingly, AWS Glue. AWS glue has it’s own orchestration tool called Glue Workflows</li>
</ul>
<h3 id="AWS-Glue"><a href="#AWS-Glue" class="headerlink" title="AWS Glue"></a>AWS Glue</h3><p>Let’s start with the simplest of the three options: Glue Workflows. Glue Workflows provides a visual editor to create relationships between your Glue components, such as your Triggers, Crawlers and your Glue ETL jobs. </p>
<p>For example, let’s say I create a Workflow. This Workflow will first start with a trigger. I can trigger based off a a schedule or an event. I want this Workflow to be triggered daily at 12:00. </p>
<p>Once the workflow is triggered, it will kick off a job to do some light pre-processing of the data. After that is successful, I’ll have a crawler crawl the optimized data set. Once the crawler finishes running, I can then run ETL on that data. </p>
<p>Glue will run this workflow every day at 12:00 without my intervention, completely automating my pipeline. </p>
<p>Glue Workflows has only one drawback: it is very simplistic and can only be integrated with Glue tools. If you use other AWS services within your pipeline, and not just Glue, consider using a service that has better service integration, such as Data Pipeline or AWS Step Functions. </p>
<p>There is no extra cost to Glue Workflows, however, you will pay for the Crawlers, the ETL jobs, and the Data Catalog requests that Workflows triggers on your behalf. </p>
<h3 id="AWS-Data-Pipeline"><a href="#AWS-Data-Pipeline" class="headerlink" title="AWS Data Pipeline"></a>AWS Data Pipeline</h3><p>Next, there’s Data Pipeline. It’s sole purpose is to coordinate data processing from one service to another without human intervention. </p>
<p>The service itself is pretty bare bones, and because of this, it’s very simple in nature. A data pipeline is made up of three core components: </p>
<ol>
<li><strong>Data nodes</strong>: these are storage locations where you house your input data and output data. Data nodes can be S3, Redshift, DynamoDB, RDS or a JDBC connection. </li>
<li><strong>Activities</strong>: this is the work that you want the pipeline to perform on your data. This could be a CopyActivity, that copies data to another location, it could be a SQL activity, that runs a SQL query on a database, or it could be an EMR activity, such as running an EMR cluster, or running a Hive query or Pig script on an EMR cluster.</li>
<li><strong>Preconditions</strong>: these are conditional statements that must be true before an activity can run. For example, you can check whether a data node exists, or run a custom shell script before your activity runs.</li>
</ol>
<p>Data Pipeline also has retry functionality built-in to the service. You can configure up to 5 retries per activity. The pipeline won’t report failure until it goes through the number of retries you set. The higher the number, the longer it will take. </p>
<p>While AWS DataPipeline is simple to get started with, you may find that there are some limitations. For example, DataPipeline has limited data sources. While you might be able to hack around this, you may want to consider using a service called AWS Step Functions for further AWS service integration. </p>
<h3 id="AWS-Step-Functions"><a href="#AWS-Step-Functions" class="headerlink" title="AWS Step Functions"></a>AWS Step Functions</h3><p>This leads us to the last orchestration service: AWS Step Functions. While AWS Step functions isn’t purpose-built for working with data, it does work well with most general workflows. This generic nature provides more flexibility to the user.</p>
<p>With Step Functions, you can integrate with far more services, such as AWS Lambda, API Gateway, Athena, and more. You can call over 200 AWS services from your Step Functions workflows. It additionally can support pipelines that use Amazon EMR and AWS Glue, whereas DataPipeline only supports EMR. </p>
<p>Step Functions coordinates the navigation among services in a serverless workflow and manages retries and errors. It is more robust than DataPipeline in terms of configuration, providing the ability to not only perform tasks, but also embed simple logic for execution in your pipeline. This enables you to make choices between multiple states, pass data between services, use parallel execution, and implement delays in your pipeline. </p>
<p>To get a feel of how it works let’s draw a quick example that uses Step Functions to orchestrate Glue ETL jobs. In this example, I upload my data to Amazon S3, which triggers Step Functions to run. Step Functions first signals to Lambda to validate my data in S3, to ensure that it is the right data type and schema. </p>
<p>If the validation is successful, the data is moved to a staging folder. If the validation fails, it moves to an error folder and sends you a notification using Amazon SNS. </p>
<p>For the successfully validated data, an AWS Glue Crawler runs to infer the schema. Step Functions then triggers a Glue ETL job to run, transforming the file into a different format. Once the glue job is complete, it stores the outputted data in the transformed folder in Amazon S3. I then receive an SNS message stating the ETL job has successfully finished. </p>
<p>This is just an example of what you can do with Step Functions. You can build far more complex ETL processes that include a wide range of AWS services and logic. </p>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>In summary, AWS Glue Workflows is great for creating workflows between different Glue Components. However, it’s not best if you need orchestration that includes other AWS services. </p>
<p>While AWS Data Pipeline is a simplistic way of getting started with building data pipelines on AWS, it does have rigid limitations on what services integrate with it. AWS Step Functions has far greater integration with AWS and provides more sophisticated logic when building a pipeline.</p>
<h1 id="Finding-Compliance-Data-With-AWS-Artifact"><a href="#Finding-Compliance-Data-With-AWS-Artifact" class="headerlink" title="Finding Compliance Data With AWS Artifact"></a>Finding Compliance Data With AWS Artifact</h1><p>Hello, and welcome to this lecture where I will be examining AWS Artifact, a free self-service portal that provides you with immediate access to AWS security and compliance reports. Within AWS Artifact, you also have the ability to view, download, accept, and terminate legal agreements between you and AWS at both the account and organization level.</p>
<p>So you may be asking yourself: why would I ever need to access the information in AWS Artifact? And as it turns out, there could be several reasons. For starters, you might be asked to provide evidence of the current or historical compliance of different AWS services used within your architecture as part of a required audit to ensure that your enterprise may continue to leverage the AWS cloud. And this audit could potentially extend out to include your suppliers as well. Or perhaps you just want to learn more about your responsibilities when it comes to complying with various regulatory standards such as Payment Card Industry, or PCI, or Service Organization Control, or SOC. After all, simply leveraging the AWS cloud does not guarantee that the systems you build within it will be fully secure or compliant. We’ll discuss this more in a moment.</p>
<p>AWS Artifact can be accessed directly from the AWS console by searching “Artifact.” From there, the AWS Artifact home page gives you options to view reports and view agreements, so let’s spend a little time discussing reports and agreements in more detail.</p>
<p>AWS Artifact Reports consist of AWS auditor-issued reports and include everything from ISO certifications to PCI and SOC reports.</p>
<p>These reports, known as audit artifacts, may be shared with auditors and regulators by creating IAM users with an associated identity-based policy that grants access only to the necessary reports. And these audit artifacts allow you to provide evidence of AWS security controls to ensure compliance with any applicable governance, regulations, or frameworks when architecting solutions in the AWS cloud. Now of course this is always done in accordance with the AWS Shared Responsibility Model, where AWS is responsible for the underlying security OF the cloud, but you remain responsible for your own systems’ and applications’ security IN the cloud. Now to learn more about the AWS Shared Responsibility Model, I encourage you to check out this resource. Consequently, the compliance reports provided within AWS Artifact pertain only to AWS and do not in any way certify the security or compliance of your own company, organization, or application. However, these audit artifacts can and should inform the security controls you choose to implement as part of your own cloud architecture and solution design.</p>
<p>In addition to security and compliance reports, AWS Artifact also allows you to view and execute legally binding agreements between you and AWS.</p>
<p>These agreements can be applied at the individual account level, or if you are signed in to the AWS console with the management account of an organization in AWS Organizations, you can also apply an agreement to all member accounts within your organization. One example of a commonly used agreement is the AWS Business Associate Addendum, or BAA, which governs your use of AWS services when storing personal health information, or PHI.</p>
<p>To accept an agreement, you must first accept the AWS Artifact non-disclosure agreement or NDA.</p>
<p>After you have accepted this NDA, then downloaded and reviewed the agreement, you may accept the agreement by checking a box acknowledging that you accept all of its relevant terms and conditions. Note that when accepting an agreement on behalf of all member accounts within an AWS Organization, you must also certify that you have the full power and authority to accept the agreement on behalf of every entity that either currently has, or may ever subsequently have, a member account within your organization at any point in the future.</p>
<p>So that’s how we can use AWS Artifact to not only view compliance reports and agreements but also to help ensure the solutions we architect in the AWS cloud remain secure and compliant with all necessary rules and regulations.</p>
<h1 id="What-is-AWS-CloudFormation"><a href="#What-is-AWS-CloudFormation" class="headerlink" title="What is AWS CloudFormation?"></a>What is AWS CloudFormation?</h1><p>Hello and welcome to this lecture where I am going to introduce you to the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudformation-introduction-infrastructure-code/introduction/">AWS CloudFormation Service</a> and some of the concepts of this service. When you first start using <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and begin getting to grips with some of the core services and features such as VPC’s, EC2’s, RDS, auto-scaling, and elastic load balancing for example. Then it is likely that you would have used specific dashboards within the management dashboard to configure them. The dashboards within the management console allow you to set up and configure the associated resources that you are interested in. Now, the configuration process generally consists of a number of steps where you are required to select different parameters. For example, when creating an EC2 instance you’ll be asked to select your AMI, the EC2 instance type, the type of storage and security groups etc. So when you start to create environments within AWS, for example you may create a Virtual Private Cloud with various subnets both private and public with Network Access Control Lists for security. And then on top of this you might deploy EC2 Instances, for example, web service in the public subnet and application service in your private subnet. And these in turn might connect to a RDS or DynamoDB Database which would also need to be configured and provisioned. You’ll also want to introduce autoscaling and elastic load balancers for higher variability. And this is great but what if you didn’t need to go through the process of configuring each and every resource through the management console selecting the appropriate parameters every time? Or even creating the same resources via the AWS CLI? Wouldn’t it be great if you created a script that created your entire environment for you? From the VPC to the Elastic Load Balancer’s and on top of that compiled the environment in such a way that you didn’t need to worry about dependencies as you would creating each individual resource? For example, you would normally have to configure your subnets after creating your VPC. Now, by using AWS CloudFormation you can provision all of your infrastructure resources that you require via a simple template in a YAML or JSON format. </p>
<p>CloudFormation performs these actions securely and across multiple regions and accounts if configured to do so. The template will describe all of your resources that you need, and their configurations without having to worry about service dependency. AWS CloudFormation will handle the order of deployment for you. You might be thinking If I have to describe and enter all of the configuration of my resources into a text file what true benefit does AWS Cloudformation have by doing it this way? Lets take a look at a few examples of where CloudFormation is extremely useful to you within your environment. </p>
<p>Security: As I explained previously your AWS resources can be provisioned and be deployed by configuring each service and component manually through a series of configurable screens. When carrying out these configurations once, five times, ten, or even twenty times the fact of human error will eventually come into play. And a mistake will be made that could lead to the resulting solution being compromised or vulnerable in some way. With AWS CloudFormation these repeatable steps can be tested, controlled, and rolled back should any issue arise. Once a template is considered error-free, the same resources can be deployed hundreds or even thousands of times without risk of errors. </p>
<p>Infrastructure Replication: AWS CloudFormation is a great tool to allow you to quickly and easily replicate your infrastructure within your AWS account. For example, lets say that you have deployed your application across a single region. Over time, the criticality of the application has increased, and you now need to deploy the same level of infrastructure and resources across multiple regions. With CloudFormation, this is easy with the use of deploying the same template in the other regions required. The alternative, would be to manually identify and understand all resources within the solution and then manually deploy them in the alternative region. Even then, it is likely that you would have missed some configuration. Having a template to deploy your resources enables the template to become the source of truth for your solution. </p>
<p>Simply Code: As the entire infrastructure is deployed via a scripted code It may make it easy for other members of your team or outside of the team to review and verify your code to ensure that it’s correct before deployment. Each author of the code can use their own code editor to construct the environment and implement a method of version control to help determine the latest templates in production. This will also help you to roll back to a previous version if required. </p>
<p>Notification and Automation: As AWS CloudFormation integrates with other management and automation serves it is easy to configure CloudFormation to notify you of the status of deployments through its integration with SNS . This could then provide you and your team the status of changes being made in CloudFormation. These SNS messages could also trigger an AWS Lambda function if you needed to bring another level of automation into your pipeline. </p>
<p>Sample Templates: AWS CloudFormation offers a number of sample templates to get you started off. Lets look at some common deployment options to save you having to create them from scratch yourself as a learning development tool. You can take a look and download these templates <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-sample-templates.html">here</a>. The template for the London Region alone contains sixty-three different templates. An example of some of these can be seen on screen. </p>
<p>That now brings me to the end of this lecture. Coming up next, I shall be looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudformation-introduction-infrastructure-code/components-of-cloudformation/">some of the components</a> that make up this service and what they are used for.</p>
<h1 id="Management-Summary"><a href="#Management-Summary" class="headerlink" title="Management Summary"></a>Management Summary</h1><p>You’ve made it through another section, it’s a great work. You’ve definitely covered the lion’s share of what you need to know in becoming a solutions architect. So, we’ve just covered topics, looking at AWS management services and concepts, and there’s a lot to take in. So, let me break down some of the core elements I believe are a must know for the exam.</p>
<p>So, let’s start with CloudWatch. Now, if you’re presented with any questions that relate to the health of resources or metrics, monitoring or logging, then it’s likely that CloudWatch will be in one of the answers available and for good reason. Now remember, CloudWatch is the go to service to understand the operational performance of your resources and applications. At its core, CloudWatch collects metrics from supported services and any custom metrics that you have added. And it can display these in a visual dashboard. It can help you to detect anomalies, review logs, trigger alarms, and also automate responses to help you optimize your infrastructure. So, it’s a great tool to help you maintain and monitor your environment.</p>
<p>So, for the exam, you need to be familiar with some of the features that it offers, as you might be asked how you could use CloudWatch to detect respond or to identify potential issues that arise in the performance of your infrastructure. So, let’s review some of the important features of the service. So, firstly, dashboards. Now, this is a customizable dashboard that lets you build a visual status of your resources using different types of widgets.</p>
<p>Now, the key point is that they are fully customizable, allowing you to design the dashboard how you need to represent your data. So, if you get a question asking how to best represent the status of many of your resources that perhaps relate to a specific project or region or resource type, then you can build a dashboard in CloudWatch for this. Next, you are 100% going to need to be aware of metrics from an exam point of view at least. If you see a question that mentions metrics, then you will almost definitely see something relating to CloudWatch. It’s CloudWatch metrics that enable you to monitor a specific element of an application resource using time series data points. So, you might see a question relating to EC2 IO Performance and what metrics you could check, perhaps disk reads or disk writes.</p>
<p>Now by default, metrics are collected every five minutes, but detailed monitoring can be enabled for a small cost, and this will collect metrics every minute. Now, what comes hand in hand with metrics is CloudWatch alarms, and these enable you to implement automatic actions based on specific metric thresholds. So, you might be given a scenario asking the best way to notify your engineering team when your EC2 instance reaches 75% CPU utilization.</p>
<p>So, what could you do to achieve this? Well, with CloudWatch, you could set an alarm that monitors for the CPU utilization metric, and then when it reaches 75%, trigger SNS to send an email to the engineering team automatically. So, remember that CloudWatch also has this integration with other services for alerting like SNS. Now, also be sure you’re aware of the different states of alarms as well, of which there are three. You have an OK state, an alarm state, and insufficient data state.</p>
<p>Now, remember CloudWatch EventBridge provides a means of implementing a level of real time monitoring. And this allows you to respond to events that occur in your application as they happen. And lastly, I just want to touch on CloudWatch Logs because these often come up in the exam one way or another. And in fact, logging is assessed at many different levels, which is why we focus on a few logging services in this course. So, what are some of the key things to remember with CloudWatch logs? Well, it acts as a central repository for real time monitoring of log data for different AWS services that provide logs as an output, such as CloudTrail EC2, VPC Flow Logs, etc. in addition to your own applications.</p>
<p>So, data is sent to a log stream within CloudWatch logs to differentiate between different logs. And you can filter for specific entries within these logs to help you identify potential issues. And you can also use the unified CloudWatch agent to collect logs and additional metric data, which is over and above the default metrics collected by CloudWatch against your EC2 instances. Now, this agent is best installed using EC2 Systems Manager known as SSM.</p>
<p>Okay, let’s leave CloudWatch logs there and move onto CloudTrail. Now, the key thing to know about CloudTrail is that it’s used to log, record, and track all API calls in your environment. If you remember that, you’ll be able to eliminate a couple of wrong answers if anything comes up about tracking API calls. Now, API calls are pretty much made for every action either made by you or made by another AWS service. They are all recorded with CloudTrail. And it’s a great tool for auditing because of this. Now, in addition to tracking the API it also tracks the user or service who initiated it, the time, date, and other metadata, such as source, IP address, etc. And where would CloudTrail send all this data? Well, to S3 of course as logs. Now S3 is used by many services for storing data and you probably know that by now anyway.</p>
<p>Now, a quick point worth mentioning is that CloudTrail logs can also be sent to CloudWatch logs for additional review, triggers, and automated responses that CloudWatch can provide like we’ve already talked about. So, this is another great integration between two services. Sometimes CloudTrail can be used as a security analysis tool, for example, identifying APIs that shouldn’t be called, or it can be used to assist with auditing as I mentioned previously.</p>
<p>Okay, so next up we looked at AWS Config. And I see CloudTrail and Config appear in the same question or the same set of answers for a particular question. So, it’s certainly worth noting the main difference between each of them and what each service is used for. So, let’s take a look at config. So, AWS Config is designed to record and capture resource changes within your environment. It’s a great service for helping you collate and review data about a specific resource type within your environment. Now you can check its configuration history to see all of the changes that have occurred on the resource since you first provisioned it, or you can see a snapshot in time of its current configuration.</p>
<p>Now, again, this also has integration with SNS and CloudTrail to offer automated notifications of any resource Config changes and which APIs triggered those. Now, one great benefit of AWS Config is its ability to implement Config rules which ensure that your resources are meeting a specific specification. Now, this is great if you get any questions that relate to compliance. So, let’s say the question was asking for ways of ensuring that your EFS file system were encrypted with KMS at all times, which was perhaps needed to meet specific regulatory requirements. What service could you use to help maintain this?</p>
<p>Well, the answer here would be AWS Config using managed rules. So, Config would assess your EFS file systems and alert you if an EFS file system was deployed without encryption. Now, this would allow you to correct the non-compliance that it’s met.</p>
<p>Okay, so we also looked at management from an AWS account level perspective as well,  and this focused on AWS organizations. Now this service is certainly mentioned in the exam, so make sure you know when you choose it and what it does and also some of the components that are used which make it a really effective account management service. And I’m going to tell you some of the main points to remember. Now the primary benefit that this service brings is its ability to essentially manage multiple accounts from a single AWS account known as the master account. Now, of course, by doing this, it helps to maintain security compliance and account management under a single umbrella.</p>
<p>Now there are two options to deploy AWS organizations, you can either deploy all with all features, which is the default, and this uses enhanced account management features, or just with consolidating billing features enabled. And this just gives a subset of features providing basic management tools enabling you to manage billing essentially across all of your accounts. So, the organization of your accounts essentially forms a family tree structure allowing you to group certain accounts with others. </p>
<p>So, know the difference between the root object, the organizational unit objects and also account objects as well. Now one benefit of organizations is that you can use service control policies or SCPs to control what services and features are accessible from within an AWS account or group of accounts. So, when a service control policy is applied to an organizational unit, all child accounts that fall under that OU will be under the same controls that are applied within the SCP.</p>
<p>Now, some people think SCPs as permission policies, however, they don’t actually give permissions, rather they just limit what permissions can be given within the corresponding account. So, they act as a permission boundary instead. So, if the SCP denied all S3 access, then no one in the associated account would be allowed to use S3. Even if their IAM permissions allowed it, it would be denied at the SCP level. So, if you get any questions relating to multi-account permissions and restrictions, then it’s likely that AWS organizations will be mentioned, specifically regarding service control policies.</p>
<p>Okay. Lastly, I just want to make sure that you know what a VPC Flow Log is and where you can use them. I’ve seen this topic come up on the exam a couple of times before. Now you might be asked how to monitor specific network traffic between your subnets or different interfaces within your infrastructure, and what would be the best solution to do this? Well, VPC Flow Logs will certainly help you here. You’ll need to have a basic understanding of what a VPC Flow Log is, what it can capture, and when they can be used to help you answer questions relating to this topic. </p>
<p>So, VPC Flow Logs capture all the IP traffic flowing between your network interfaces on your resources within your VPC, and then this log data is then sent to CloudWatch logs. Now, once the VPC Flow Log has been created, it can’t be changed. The only way you could change it would be to delete it and then recreate another. So, VPC Flow Logs can be configured against a network interface, one of your subnets in your VPC, or the VPC itself. So, just remember those three levels of application.</p>
<p>Okay, so we’ve now reached the end of this course, so a few things not to forget. So, if you need to monitor health of different resources, set alerts, gather metrics, then use CloudWatch. If you need to capture API calls being made across your AWS account, then AWS CloudTrail is your go to answer. If you need to monitor, manage, and assess the configuration state of your resource, then AWS Config can help you here with managed Config rules. If you have to set up management and security controls across a multi-AWS account level, then AWS Organizations is the service you need. And lastly, if you need to capture network traffic at an interface, subnet, or VPC level and review the logs in CloudWatch, then VPC Flow Logs should be at the forefront of your mind. Okay, that’s me done. Let’s take a step away from the keyboard and take a break before tackling the next section.</p>
<h1 id="2What-is-Amazon-CloudWatch"><a href="#2What-is-Amazon-CloudWatch" class="headerlink" title="2What is Amazon CloudWatch?"></a>2<strong>What is Amazon CloudWatch?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/course-introduction/">Data Visualization: How to Convey your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">List of metrics</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-sqs-sns-ses/">Using SQS, SNS and SES in a Decoupled and Distributed Environment</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">List of targets</a></p>
<h1 id="18CloudWatch-Logging-Agent"><a href="#18CloudWatch-Logging-Agent" class="headerlink" title="18CloudWatch Logging Agent"></a>18<strong>CloudWatch Logging Agent</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">Metrics collected by the CloudWatch Agent</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent Linux</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/windows/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent for Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">How to install&#x2F;update your SSM Agent</a></p>
<h1 id="21CloudFront-Access-Logs"><a href="#21CloudFront-Access-Logs" class="headerlink" title="21CloudFront Access Logs"></a>21<strong>CloudFront Access Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">Web distribution log file format</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">RTMP distribution log file format</a></p>
<h1 id="22VPC-Flow-Logs"><a href="#22VPC-Flow-Logs" class="headerlink" title="22VPC Flow Logs"></a>22<strong>VPC Flow Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/whats-new/2018/08/amazon-vpc-flow-logs-can-now-be-delivered-to-s3/">Flow Logs delivery to S3</a></p>
<h1 id="45Finding-Compliance-Data-With-AWS-Artifact"><a href="#45Finding-Compliance-Data-With-AWS-Artifact" class="headerlink" title="45Finding Compliance Data With AWS Artifact"></a>45<strong>Finding Compliance Data With AWS Artifact</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/resource/aws-shared-responsibility-model">AWS Shared Responsibility Model</a></p>
<h1 id="46What-is-AWS-CloudFormation"><a href="#46What-is-AWS-CloudFormation" class="headerlink" title="46What is AWS CloudFormation?"></a>46<strong>What is AWS CloudFormation?</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-sample-templates.html">AWS CloudFormation Templates</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-2-of-2-42/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-2-of-2-42/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-2-of-2-42</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:14:00" itemprop="dateCreated datePublished" datetime="2022-11-18T22:14:00-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:12:56" itemprop="dateModified" datetime="2022-11-27T20:12:56-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-2-of-2-42/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-2-of-2-42/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Security-SAA-C03-2-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-1-of-2-41/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-1-of-2-41/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-1-of-2-41</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:59" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:59-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:08:42" itemprop="dateModified" datetime="2022-11-27T20:08:42-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-1-of-2-41/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Security-SAA-C03-1-of-2-41/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Security-SAA-C03-1-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Advanced-Roles-and-Groups-Management-Using-IAM-40/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Advanced-Roles-and-Groups-Management-Using-IAM-40/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Advanced-Roles-and-Groups-Management-Using-IAM-40</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:57" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:57-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:12:02" itemprop="dateModified" datetime="2022-11-27T20:12:02-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Advanced-Roles-and-Groups-Management-Using-IAM-40/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Advanced-Roles-and-Groups-Management-Using-IAM-40/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-IAM-39/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-IAM-39/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Introduction-to-IAM-39</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:56" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:56-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:17:48" itemprop="dateModified" datetime="2022-11-27T20:17:48-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-IAM-39/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-IAM-39/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Security-SAA-C03-38/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Security-SAA-C03-38/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Security-SAA-C03-38</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:54" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:54-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 19:59:00" itemprop="dateModified" datetime="2022-11-27T19:59:00-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Security-SAA-C03-38/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Security-SAA-C03-38/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Security-SAA-C03-Introduction"><a href="#Security-SAA-C03-Introduction" class="headerlink" title="Security (SAA-C03) Introduction"></a>Security (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on AWS security services, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the various security services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#112;&#x70;&#x6f;&#x72;&#x74;&#64;&#99;&#108;&#x6f;&#x75;&#x64;&#97;&#x63;&#x61;&#100;&#x65;&#x6d;&#x79;&#46;&#99;&#111;&#109;">&#x73;&#117;&#112;&#x70;&#x6f;&#x72;&#x74;&#64;&#99;&#108;&#x6f;&#x75;&#x64;&#97;&#x63;&#x61;&#100;&#x65;&#x6d;&#x79;&#46;&#99;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various security services in AWS in preparation for the exam. The objective of this course is to provide an introduction to the various security services in AWS and to introduce important security concepts, including how to manage users, groups, and roles with AWS Identity and Access Management, known as IAM.</p>
<p>We’ll also introduce the AWS Web Application Firewall, or WAF, which, along with other services such as the AWS Firewall Manager and AWS Shield, can help you create a comprehensive security solution for your web applications.</p>
<p>You’ll learn about the AWS Security Hub as well as many other AWS security services and their features, including Amazon Inspector, Amazon GuardDuty, and Amazon Macie. Finally, you’ll learn about Amazon Cognito, including Cognito User Pools and Identity Pools. And we’ll wrap up the course with an introduction to AWS Identity Federation and how it can be used to simplify access at scale.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#117;&#x70;&#112;&#111;&#x72;&#x74;&#x40;&#99;&#x6c;&#111;&#x75;&#100;&#x61;&#x63;&#97;&#x64;&#x65;&#x6d;&#x79;&#46;&#x63;&#x6f;&#109;">&#115;&#117;&#x70;&#112;&#111;&#x72;&#x74;&#x40;&#99;&#x6c;&#111;&#x75;&#100;&#x61;&#x63;&#97;&#x64;&#x65;&#x6d;&#x79;&#46;&#x63;&#x6f;&#109;</a>. Thank you!</p>
<h1 id="What-is-Identity-and-Access-Management"><a href="#What-is-Identity-and-Access-Management" class="headerlink" title="What is Identity and Access Management?"></a>What is Identity and Access Management?</h1><p>Hello and welcome to this lecture where I shall provide an overview of what the Identity &amp; Access Management service is, and what IAM actually means.</p>
<p>Firstly I want to define what is meant by Identity &amp; Access Management and I shall break this down into two parts, starting with Identity Management. </p>
<p>Identities, such as AWS usernames are required to authenticate you to your AWS account, and this authentication process is managed in 2 stages.</p>
<ol>
<li>The first part of this process is to define who you are, effectively presenting your identity, so for example your AWS username. This identification is a unique value within IAM for your account, so this means IAM would prevent you from having 2 identical user accounts with the same name within the same AWS account.</li>
<li>The second part of the authentication process is to verify that you are who you say you are. This is achieved by supplying additional data, and when using our AWS usernames we can verify this by supplying a password</li>
</ol>
<p>Now, Access Management relates to authorization and access control. Authorization determines what an identity can access within your AWS account once it’s been authenticated to it. An example of this authorization would be the user’s list of permissions to access specific AWS resources, for example, they might have Full Access to EC2 or Read Only to RDS.</p>
<p>Access Control can be classed as the mechanism of accessing a secured resource. For example, using the following:</p>
<ul>
<li>Username and password (Authentication and Verification)</li>
<li>Multi-Factor Authentication (MFA, used as an additional verification step following a valid password)</li>
<li>Or Federated Access, which allows users external to AWS to access resources securely without having to supply AWS user credentials from a valid IAM user account. Instead, these credentials are supplied from identity providers. For more information on Identity Federation, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/">https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/</a></li>
</ul>
<p>So essentially IAM can be defined by its ability to manage, control, and govern authentication, authorization, and access control mechanisms of identities to your resources within your AWS Account.</p>
<p>Having an understanding of the different security controls from an authentication and authorization perspective can help you design the correct level of security for your infrastructure.</p>
<h1 id="IAM-Features"><a href="#IAM-Features" class="headerlink" title="IAM Features"></a>IAM Features</h1><p>It’s critical to understand how IAM works and what can be achieved via the service, but it’s even more important to know how to implement its features. Without IAM, there would be no way of maintaining security or control of who or what could access your resources and what they could do with them, both internally and externally. IAM provides the components to maintain this management of access, but it is only as strong and secure as you configure it. </p>
<p>The responsibility of implementing secure, robust, and tight security within your AWS account using IAM is yours, you are the owner of the AWS account. You must define how secure your access control procedures must be, how much you want to restrict users from accessing certain resources, how complex a password policy must be, and if users should be using Multi-factor authentication. All of this is and much more is down to you to architect and implement, and much of it will likely depend on your own security standards and policies within your Information Security Management System (ISMS).</p>
<p>In this lecture, I want to talk about some of the different features and components that AWS IAM uses to centrally manage and control security permissions for any identity requiring access to your AWS Account and its resources. </p>
<p>From with the AWS Management Console, the IAM service can be found under the ‘Security, Identity &amp; Compliance’ category, and when accessed it will take you to the IAM Dashboard. </p>
<p>The initial dashboard of the IAM Console will display the following information.</p>
<p>This is a URL link that you can send to users who you will need to gain access to your AWS Management Console. This sign-in link can be customized by clicking on the ‘customize’ button to make it easier to remember and read. If you have multiple AWS accounts, this customization would help to distinguish between your accounts</p>
<p>IAM Resources. This section provides a summary overview of your IAM resources using a simple count of the number of users, user groups, roles, customer-managed policies, and identity providers you have configured within IAM. </p>
<p>Best Practices. This is populated with a list of IAM security best practices that AWS recommends you implement with links on how to implement them. I strongly recommend you try to adopt these best practices at your earliest opportunity. Maintaining tight security is paramount with working within a cloud environment.</p>
<p>In addition to this dashboard, you will also see 2 categories on the left menu, Access Management and Access Reports each listing a number of components underneath. I want to start by reviewing each of these components under Access Management to give you an insight into what each are used for, starting with Users.</p>
<p>User objects are created to represent an identity, this could be a real person within your organization who requires access to operate and maintain your AWS environment, or it could be an identity that is used by an application to interact with your AWS resources programmatically.  Users are simply objects representing an identity which are used in the authentication process to your AWS account. Being unique values, every user has an Amazon Resource Name, an ARN which it can be referenced by. An example of a user’s ARN could be as shown.</p>
<p>When configuring your Users you can set them up for Multi-Factor Authentication. Configuring MFA allows for an additional level of verification to be applied, the user will have to enter a random 6 digit number from a linked MFA device after their usual password. MFA should be used for the AWS Account owner and any other users who have elevated privileges. </p>
<p>IAM USer Groups are objects much like user objects, however, they are not used in any authentication process, instead, they are used to authorize members of the group access to AWS resources.</p>
<p>So, the IAM USer Groups contain IAM Users, and these groups have IAM policies associated that will either allow or explicitly deny access to AWS resources. These policies are either pre-existing AWS Managed policies, customer-managed policies that are created by you, the customer, or in-line policies which are embedded explicitly to the group itself.</p>
<p>IAM Roles allow Users, other AWS services, and applications to adopt a set of temporary IAM permissions to access AWS resources. Roles essentially operate the same as User objects do in that it’s an identity with associated permissions to allow it access to different resources. The difference being however is that roles don’t define a single person, they are designed to be assumed by any identity or service that needs to temporarily acquire a set of permissions. Additionally, roles don’t have passwords associated with their identities, instead, like I just mentioned, roles are assumed as long as you have the correct permissions to assume it.</p>
<p>Policies used within IAM are written as JSON documents and these define what can and can’t be accessed. These policies can be attached to Users, User Groups, or Roles. </p>
<p>When working with policies, you can use Managed policies or In-line policies. Managed policies are viewed as a library of usable policies and come in 2 different flavors which can be applied to multiple Users, User Groups, and Roles:</p>
<ol>
<li>AWS Managed Policies: These are a list of predefined policies granting varied access to different AWS services</li>
<li>Customer Managed Policies: These are policies created and written by you as the customer</li>
</ol>
<p>Unlike Managed policies, Inline policies are not stored in a library, instead, they have to be written and explicitly embedded within a User, User Group, or Role, as a result, the same policy can’t easily be applied to another identity like Managed policies can.</p>
<p>If you are looking to provide federated access to your AWS resources, then you must add an identity provider. </p>
<p>Federated Access allows credentials external to AWS to be used as a means of authentication to your AWS resources. For example, you could establish a trust between your Active Directory Federation server on premises and your AWS account. Or, you could establish a trust between your AWS account and Google account. Either way, in these instances your ADFS server or Google account could be configured as Identity Providers allowing those authenticated by Active Directory or Google to access your resources in your AWS account. This prevents you from having to create individual IAM user accounts.</p>
<p>Under Account Settings, you can enforce a password policy, and it’s always best practice to do so. The Password policy should be used to enforce the minimum security requirements that need to be met for any password standards that your organization might have to adhere to. The password policy applies to all IAM users within your account, and as you can see here, you can be very specific by enabling&#x2F;disabling and configuring different controls.</p>
<p>The STS service is used to allow you to request temporary, limited-privilege credentials for both IAM users and federated users. The STS endpoints provide a list of Regions that are either activated or deactivated for STS. By default, all Regions are activated, however, it is recommended you deactivate the regions you do not intend to use. </p>
<p>By default STS uses the global endpoint of  <a target="_blank" rel="noopener" href="https://sts.amazonaws.com/">https://sts.amazonaws.com</a>, however, when using regional endpoints it can help to reduce latency.</p>
<p>So that has given a quick overview of the component covered by Access Management, I now want to look at the features that come under the category of Access Reports, starting with Access Analyzer</p>
<p>Access Analyzer. This is used to generate findings when a policy on a resource within your zone of trust allows access from outside your zone of trust. So this could be from a number of different sources, such as an IAM role allowing cross-account access, or a Bucket that allows a different account to upload objects into the bucket, basically any resource that allows external access to your resources will be flagged and highlighted to ensure you are aware of the access. This is a great tool to help reduce security risks and threats as you or your team may have unintentionally allowed access to a resource you shouldn’t have. Any issues are recorded by Access Analyzer as a finding allowing you to review the access </p>
<p>The credential report is a great tool that allows you to generate and download a *.csv file containing a list of all of your IAM users and their credentials. This provides a quick and easy way to review your accounts and the last time they were used, in addition to identifying when a user’s password was last changed and if they have Multi-Factor Authentication enabled.</p>
<p>If using AWS Organizations, it allows you to Select an organizational unit (OU) or account to view its service activity over the previous 365 days. By drilling down into the accounts in this section you can see which users have had activity, in addition to which AWS services they have accessed over a given time frame. </p>
<p>Service Control Policies. This links in with the previous component, ‘Organization Activity’, and lists any Service Control Policies that are applicable to the account and the number of identities affected. Service Control Policies, or SCPs, are different from identity-based policies which grant permissions to users, user groups, and roles as SCPs do not actually grant permission themselves. Instead, SCPs are used with AWS Organizations to implement and set a boundary of permissions for AWS accounts. </p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS, and EC2 via an identity-based policy in IAM. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2, despite having full access to S3. The SCP would prevent that service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<h1 id="Overview-of-the-User-Dashboard"><a href="#Overview-of-the-User-Dashboard" class="headerlink" title="Overview of the User Dashboard"></a>Overview of the User Dashboard</h1><p>Hello and welcome to this lecture where I’m going to review the dashboard of the user Console. In IAM, we can create user objects to represent an identity with long-term credentials, this could be a real person within your organization who requires access to operate and maintain your AWS environment, or it could be an account to be used by an application that may require permissions to access your AWS resources programmatically. Users are simply objects representing an identity which are used to authenticate to your AWS account with an associated set of permissions.</p>
<p>When you open the AWS IAM dashboard and select users, you will be presented with a screen similar to the following. This screen provides a summary of the key points of interest relating to your IAM users and it can quickly help you identify any potential security risks that you might encounter. As we can see, we have a number of different warnings on this screen in different colors. We have a green tick which symbolizes activity was measured within the last 90 days An amber exclamation mark showing activity was last measured between 91 and 365 days ago. And a red exclamation mark, which highlight anything older than 365 days ago. Anything either amber or red should be looked at and addressed as a priority to reduce potential security issues.</p>
<p>Let’s take a closer look at this table and its columns to understand exactly what it’s trying to show you. The complete list of field names are shown here, and you can toggle them on an off as you need. Let me go through each of them one-by-one to explain what they represent. So username. This column is self-explanatory, it provides the username of that user.</p>
<p>Path. If you create your users using the IAM API or the AWS Command Line Interface, the CLI, then you can specify a path structure for your users. This is useful when you have a large organization and you want to define a management structure to help you organize your users more effectively.</p>
<p>For example, you could add a path for a user Stuart of &#x2F;ContentTeam&#x2F;AWS&#x2F;UK&#x2F;Stuart. This would help to define where your users might be in accordance with your organization structure. In addition to this, you can also reference paths in your access policies, so for example you could allow all users in the &#x2F;ContentTeam&#x2F;AWS&#x2F;UK&#x2F; path to have full access to S3 with a simple policy like this.</p>
<p>Groups. This shows the different groups that the user belongs to. In the screenshot, you can see that the user Stuart belongs to the Admin group. Last activity. This column is really useful to show the last time your users logged into the AWS Management Console or if they accessed any of your AWS resources programmatically using their access keys. As we can see from my example, the user Alice hasn’t accessed anything for 467 days, as a result I can say with some confidence that this user can be removed and deleted from IAM. So it’s a great way to ensure that you don’t have any unused user accounts, and deleting unused accounts is an IAM best practice.</p>
<p>MFA. MFA stands for Multi-Factor Authentication, and so this will highlight any users that have been configured with MFA which adds an additional level of verification, enhancing security. The users in my example that show virtual mean that these users are using MFA on a virtual device, and in this example it’s the Google Authenticator app on a mobile phone. For those unfamiliar with MFA, let me give a brief explanation of what it is. Typically, when a user logs into the AWS Management Console, they will authenticate to your AWS account by providing their identification, typically their username, and then verify this identification, usually with a password. These two elements, the identification and verification, allow the user to authenticate. For many cases, the verification, the password, is sufficient enough to confirm the identity, the username. However, for users who have an elevated level of privileges, for example the users may have full access to a large number of AWS services, then you may want to, or be required to due to governance controls, implement an additional verification step within the authentication process. This adds another layer of security attached to the identity, and this is where Multi-Factor Authentication comes in, MFA. The name explains itself, it’s used to create an additional factor for authentication in addition to your existing methods, such as a password. Therefore creating a multi-factor level of authentication. So what is this additional level of security that MFA brings with AWS? MFA utilizes a random six-digit number that is only available for a very short time period before the number changes again which is generated by an MFA device. There is no additional charge for this level of authentication, however, you will need your own MFA device, which can be a physical token or a virtual device. AWS provides a summary of all supported devices here. Personally, I use Google Authenticator on my phone because it is simple and easy to set up and configure. Password age. This shows how long ago it was until the password of the user was changed. As a security best practice, your passwords should be changed regularly.</p>
<p>Console last sign-in. This column simply shows the last time a user signed in to the management console.</p>
<p>Access key ID. The access key ID will identify if it’s active or inactive and will display the key in use. Before I continue, let me just quickly explain a little more about Access Keys. Access Keys are used for programmatic access to your AWS resources, and they are comprised of two elements. An access key ID and a Secret Access Key ID The Access Key ID is made up of 20 random uppercase alphanumeric characters, such as the one shown here. The Secret Access Key ID is made up of 40 random upper and lowercase alphanumeric and non-alphanumeric characters, such as the one displayed on the screen. During the creation of a user who requires programmatic access, you are prompted to download and save these details and it’ll only be displayed once, and if you lose it, then you will have to delete the associated Access Key ID and recreate new keys for the user. It’s not possible to retrieve lost Secret Access Key IDs as AWS does not retain copies. These access keys must then be applied and associated with the application that you are using that requires the relevant access. For example, if you were using the AWS CLI to access AWS resources, you would first have to instruct the AWS CLI to use these Access Keys to authenticate you providing authorization. The method of performing this association varies based on the application and system that you are using. However, once this association has taken place it ensures that all API requests made to AWS are signed with this digital signature.</p>
<p>Active key age. The active key age refers to the age of the access keys that are associated with the user which, as I explained, are required for programmatic access. Access key last used. This column just shows the last time that the user used their Access Keys within your AWS account. As a security best practice, it’s a good idea to remove any access keys that haven’t been used for quite some time. This removes possible entry points for malicious users to gain access to your environment.</p>
<p>The ARN. The ARN, or Amazon Resource Name provides the unique identifier for referencing that particular user. For example, the one showne here for the user Stuart. Creation time. This is a simple time metric to show you how long ago the user was created Console access. The console access helps you to determine which users have access to the Management console. When creating a new user, you have the option to configure their access type as seen here. Each user can be configured to have Programmatic Access, or AWS Management Console Access, or both. If they have access to the Console, then this column will show as Enabled for the user and Disabled if the user does not have console access.</p>
<p>Signing certs. Finally, the last column of signing certs will show if the user has a Signing Certificate or X.509 Certificate associated with them for secure access to certain AWS product interfaces. For example, EC2 uses a Signing Certificate for access to its SOAP, Simple Object Access Protocol, and command line utilities. Finally, before we finish this lecture, you can also create and delete users as required, and search for any of your users via their username or access key from within the user dashboard.</p>
<h1 id="Creating-IAM-Users"><a href="#Creating-IAM-Users" class="headerlink" title="Creating IAM Users"></a>Creating IAM Users</h1><p>When creating a new user, you have the option to create it via the AWS Management Console or programmatically via the AWS CLI, Tools for Windows PowerShell, or using the IAM HTTP API. For this lecture, I should be using the AWS Management Console to demonstrate how to configure Users. User object creation is a simple process. Firstly, you will set the user details by creating a username, which can be up to 64 characters in length. Next you’ll select the AWS access type, either AWS Management Console access or programmatic access.</p>
<p>For programmatic access, an access key ID and secret access key ID will be issued to be used with the AWS CLI, SDKs or other development tools. If console access is required, you will need to define a console password for the user. Permission assignment through the use of policies can be attached to the user or inherited from a group that the user can be assigned to. And permission boundaries can also be applied to the user, controlling their maximum permission level. You can assign any tags to the user as you would with any other AWS resource. And then you must review and confirm the information that has been submitted before you create the user.</p>
<p>Once the user is created, you can download the security credentials via a CSV file. And that will contain the username, access keys required for programmatic access and the console login link. So let me now jump into the console to demonstrate how to create a new user. Okay, so I’ve just logged into my AWS Management Console. And the first thing I want to do is to go to IAM. And that can be found under the Security, Identity and Compliance category.</p>
<p>So if we select IAM, and that will take us straight to the IAM dashboard. And this is where we can start creating our users and groups and roles, et cetera, and anything else that we need to manage within IAM. So to create a new user, I need to go across to Users on the left here. And then from here, I can select Add users.</p>
<p>Now the first thing I need to do is to create the username of the user. So I’m going to call this user Patricia. And then we can select the access types. So we have the programmatic access here, all the AWS Management Console access here. So for this user, let me add both. So I want them to have programmatic access and also AWS Management Console access. So I’m going to select both. So for the AWS Management Console access, we need to enable a password so we can either have IAM auto-generated password or I can select my own. And for this demonstration, I’m going to add in my own password.</p>
<p>Now if you tick this option here, when the user signs in, they will be asked to generate their own password once they’ve used your initial password to login. And that’s a great idea, just to enhance security there. So if we click on Next, then we can assign permissions. And here we have a couple of options so we can add the user to an existing group. We can copy permissions from another user or attach existing policies directly to the user. So for best practice, I’m just going to add this user to a couple of different groups. So I’m gonna add them to the CloudAcademy group, and also to the RDSFullAccess.</p>
<p>So once you’ve selected the groups that you want the user to have, you can click on Next to go to tags. And this is an optional step. You can add any key value tags here for that user if you want to. Just gonna leave that blank for this demonstration. Then if we go to Next to Review, then we can review all the options that we’ve set. So we’ve given the username. We specify the access types. So you’ve got programmatic and AWS Management Console access. We’ve not set any permission boundaries. We’ve added the groups that we want the user to belong to and we haven’t applied any tags.</p>
<p>So now, we can go to Create User. Now we’ve successfully created the IAM user, but because we specified that we wanted programmatic access, we need to copy the access key ID and also the secret access key ID as well. If we download the CSV file of that user and take a look at that, we can see here that this CSV file shows the access key ID and also the secret access key ID, and also the console link as well to allow that user to login. So if we go back to the AWS Console, we can also email those login instructions to the user as well if we need to. Once you’ve taken a copy of the access key ID and the secret access key, then you can close this window. Remember, you will only be given one opportunity to take these details and download the CSV that contains that information, so make sure you do that. Then click on Close. And we can now see that that user, Patricia, has been set up as a user with the CloudAcademy and RDSFullAccess groups. So it’s very simple. It’s very quick. It’s very easy to set up a new user within IAM.</p>
<h1 id="Managing-IAM-Users"><a href="#Managing-IAM-Users" class="headerlink" title="Managing IAM Users"></a>Managing IAM Users</h1><p>Once you have created IAM users, you can view their details to configure additional security options or review permissions and change access. In this lecture, I want to cover these additional features. This will be easiest to explain via a demonstration, and I can explain each point as we go through, so let’s take a look. So in this demonstration, I just want to select a user and just to show you some of the different elements that you can change of that user once it has been created, so let’s take a look.</p>
<p>So I’m in the Identity and Access Management Dashboard at the moment and you can see I’m in the Users section. So let’s take a look at this user, Patricia. So if I select the user and we can have a look at some of the options that we can see about this user and some of the things that we can change, et cetera. So this is a summary screen of the user. We have the users ARN at the top here, and we can also see the creation time of that user. And then we have a number of different tabs.</p>
<p>So start with the permissions tab. We can see that this user is getting permissions from two policies at the moment here, the Amazon S3 Full Access policy, and also the Amazon RDS Full Access. And if we wanted to, we can just take a quick look at these groups. We can have a look at the policy summary, or we can take a look at the JSON as well. So that’s the policy and the JSON format. And then if we look at the policy summary, we can see here that this allows full access to S3 and S3 Object Lambda. Here we can set her permissions boundary. Currently there’s not one set, but if we wanted to, we can set one to control the maximum permissions that this user can have. And also there’s a feature here to generate policy based on CloudTrail events.</p>
<p>So what this will do, it will generate a policy looking at the user’s activity. And then based on what the user has been accessing, it can generate a policy based on what services this user has been accessing. Also at the top here, we can add an inline policy for this user. So if we’d done that, then that will be a policy that is embedded within the user object itself. So it’s not taken from a role, it’s not taken from a group. The policy is attached within the user.</p>
<p>Okay, if we take a look at the groups, we can just see a quick breakdown of any groups that the user belongs to, and the policies that are attached to them, which we covered just a moment ago in the summary. The tags is what you’d expect. If there’s any tags here for the user, then they would be listed, or if you wanted to add any tags, then you can do so here. So for example, we can add a key of location and say, UK, Save Changes. And then we can see this tag for this user. Under security credentials, we could see console link that this user can use and we can manage the user’s password. And if we want to change the password, we can simply click on manage, and we can either disable the console access or generate a new password, or ask the user to create a new password at the next sign-in. We also have here, the assigned MFA device, the multifactor authentication.</p>
<p>At the moment, it’s not assigned, but we can go ahead and set up MFA for this user. So let’s go ahead and do that quickly. So if we click on manage, we have a couple of options here, virtual MFA device, U2F security key, or another hardware MFA device. For this, I’m just going to use a virtual MFA device and I’ll use the Google Authenticator app on my phone to do this. If I click on continue. So, first of all, you need to make sure you have an app on your mobile phone or your computer. Like I say, I’m going to use the Google Authenticator app on my phone.</p>
<p>So what I need to do is to show the QR code, and now on my phone, I’m going to add this as a new entry in my Google Authenticator app, so I’m going to click on Scan QR code. And then we can see at the bottom there is added the user, Patricia. And then we add in that code, so 074720. And then what we need to do is to add the second code that comes in when it appears on the Google Authenticator app. So we’re just waiting for that to come around and then I can add in the second code and then it’ll be synchronized and configured. So, we can see it’s about to change, and now I can add in the next code 185887, and then I click on Assign MFA. And that’s it, so you have successfully assigned a virtual MFA device to that user. Click on Close, and there we can see here that there’s an assigned MFI device. We can see that this user also had programmatic access ‘cause there’s access keys that have been generated.</p>
<p>Now, if we wanted to, we can make this access key ID inactive. So if wanted to do that, simply click on Make inactive. And it’ll explained that once you’ve done this, you can’t then use these keys to form any programmatic access. Click on deactivate. And you can see here, the status is now inactive. So any access keys that were used before for this user will no longer be allowed to make any kind of requests. If we wanted to generate new access keys, simply click on Create access key. And again, you’ll have a new access key ID and a new secret access key. And if you wanted to, you can download the CSV file, so you don’t forget those keys. Click on Close.</p>
<p>Now, if we go back up to the top to Access Advisor, I just wanted to show you this quickly. So what this does, it will basically show you which services that this user can access based on their current permissions, and also the last time that these services were accessed. So if you scroll down here, we can see that this user has access to a whole different range of services. And it’ll show you which policies are actually granting these permissions.</p>
<p>So this access to EC2 in IAM is being granted through the RDS Full Access policy, and access to S3 is being granted through the Amazon S3 Full Access. So this is great to review to identify if there’s any users there that do have access to services that they probably shouldn’t do. So you can then modify the policies accordingly just to make sure that the users are only accessing what they are supposed to access. So that was a very quick demonstration of some of the key points that you can change within a user’s properties once you have created an IAM user.</p>
<h1 id="Managing-Multiple-Users-with-IAM-User-Groups"><a href="#Managing-Multiple-Users-with-IAM-User-Groups" class="headerlink" title="Managing Multiple Users with IAM User Groups"></a>Managing Multiple Users with IAM User Groups</h1><p>In this lecture, I want to talk about how IAM User Groups can be used to manage multiple users. IAM User Groups do not signify a single user and they can’t be referenced as a principal in any AWS access policy like a User or a Role can. However, they are used to authorize access for members of the group to AWS resources through the use of AWS Policies attached to the User Groups. So User Groups are objects that contain IAM Users, and these User Groups will have IAM policies associated that will allow or explicitly deny access to AWS resources.</p>
<p>These policies can be AWS Managed policies that can be selected from within IAM, customer-managed policies that are created by you, or in-line policies, which are written and embedded directly into the group. User Groups are a great user management feature and they are normally created to directly relate to a specific requirement or job role. For example, you could have a group called Developers, and then attach policies to that group that allow access to AWS resources required by your development team.</p>
<p>Any users that are then a member of that group will automatically inherit the permissions applied to the group. By applying permissions to a group instead of individual users, it makes it easy to modify permissions for multiple users at once, simplifying access management at scale. It’s a security best practice to apply permissions to User Groups and then associate users to that group than to associate policies to individual Users. This prevents you having to update permissions for each and every user.</p>
<p>For example, if you needed to change access for all the individual developers that had policies assigned to them directly, and this can be very time intensive and prone to human error, especially in an enterprise environment. If using groups and additional access is required for your Developer User Group, all you would need to do is to modify the permissions of the Developer Group and all your associated developers would inherit the new access.</p>
<p>Creating a group is very simple and is essentially a three-step process. You must give your group a meaningful name, add users to the group, attach permissions via the policies. Once you have created a user group, you can then review its configuration, edit the permissions and see other details such as the ARN of the user group. Let me show you via a quick demonstration on how to create an IAM group and then how to modify the permissions of the group once it’s been created.</p>
<p>Okay, so I’ve logged into my AWS Management Console and I’ve gone to the IAM dashboard. Now, from here, to access and create groups, under access management on the left, you can see user groups. So if you select that, and then will show you any groups that you currently have. And I only have one group, which is Admin. So to create a new group, you’ve gotta cross to the right-hand side here, click on create group. And the first thing you need to do is to give the group a name. So I’m going to call this MyS3andEC2Group. And then after that, you need to select the users that you want to be a part of the group.</p>
<p>So if you already have the IAM users there, you can add them at this stage. So let’s just go ahead and add in Stuart. And then at the bottom, you can then attach any policies that you want to be associated with the group. So if I type in S3, and it’ll pick up any policies that we have associated with S3. And I’m going to select this one here, this AmazonS3andEC2FullAccess policy. And if I click on the little plus sign here, it’ll give you a JSON view of the actual policy itself. So you can see exactly what’s happening.</p>
<p>So now I’ve selected that policy. If we just go down to create group at the bottom, and it’s as simple as that. So it’s very easy to create a group. You simple give it a name, specify the users if you need to at that stage and also add any permissions if you want to at that stage as well.</p>
<p>Now, once we’ve created our group, if we select it, we can see the user list here. Now, if you need to add any additional users, simply click on add users, select the users that you’d like and click on add users. And then they’ll be immediately added to the group. You can also look at the permissions. So here’s the policy that we added. Now, we can if we want to, add additional permissions by clicking on this button here. And you can either attach an existing policy or create an inline policy that’s directly embedded into the group.</p>
<p>So for example, if we go to attach policies, and we want RDS access as well for the people in this group, have a quick search and we’ll select this AWS managed policy for RDSFullAccess. Click on add permissions. And we can now see that this group has two permissions policies. And again, we can view the JSON details of those policies if we want to. And then we have the Amazon RDS policy here. So it’s very easy to set up groups and only literally takes just a few clicks.</p>
<p>So that’s how you create a new group, add users and also change the permissions as well. And also, just before we finish, if you want to delete the group, you can click on delete here. And we just need to type in the name of the group to confirm the deletion. So I’ll just go ahead and do that. Then click on delete. And that deletes the group. Finally, from a limitation perspective, your AWS account has a default maximum limit of 300 groups. To increase this you’ll need to contact AWS using the appropriate limit increase forms. Also, a user can only be associated with 10 groups, so bear this in mind when assigning permissions and each group can contain 10 different policies attached at once. Limitations on AWS services is fluctuating all of the time, so for the latest information on Group limitations, please see the following URL here.</p>
<h1 id="IAM-Roles"><a href="#IAM-Roles" class="headerlink" title="IAM Roles"></a>IAM Roles</h1><p>IAM Roles allow trusted Users, AWS services and applications to adopt a set of temporary IAM credentials to access your AWS resources. Roles act as identities, much like Users do, and have permissions assigned to them defining what resources the Roles can and can’t access. Unlike Users though, which represent a single identity, IAM Roles are designed to be assumed by multiple different entities as and when required. Like I say, Roles are used for temporary access to gain access to resources, and each time the role is assumed by a User, an AWS service or an application, a new set of credentials is dynamically created for the duration of that session. As a result, Roles do not have any long term credentials associated, so there is no password for console access, nor are there any access keys for programmatic access that are explicitly associated with the Role.</p>
<p>For every role there will be associated policies controlling access as to what can and can’t be accessed when the role is assumed. There will also be a Trust Relationship, and this Trust Relationship defined who or what can assume the role, for example a User, an AWS account, or an AWS Service. IAM Roles are generally used: if you need to grant temporary access for Users to AWS resources that they don’t normally require access to or you can use a Role to grant access for an IAM user in one account to access resources in another AWS account or perhaps an AWS service needs to access resources on your behalf or if an application requires access to resources you can use a Role instead of embedding credentials into the software itself. Or you might have federated users who require access to specific resources, perhaps those authenticated via Active Directory So, as a result, Roles can be assumed by the following: A user that’s in the same AWS account as the where the role has been created, a user that’s in a different AWS account than where the the role has been created, an AWS service, such as EC2, or an external federated users to your AWS account</p>
<h1 id="Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf"><a href="#Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf" class="headerlink" title="Using AWS Service Roles to Access AWS Resources on Your Behalf"></a>Using AWS Service Roles to Access AWS Resources on Your Behalf</h1><p>An AWS Service Role allows an AWS service to assume a role to access other AWS resources within your own account on your behalf. This is commonly used for EC2 instances, whereby you could create a role for an EC2 instance to assume to gain access to AWS resources on your behalf. Let’s look at an example of when you might use this.</p>
<p>Consider the following scenario. You have an EC2 instance running an application that requires access to Amazon S3 to Put and Get objects using the relevant API calls. To allow access to S3, a set of credentials could be stored on the EC2 instance within the application code allowing it to use those credentials to gain access to the relevant S3 Bucket for any Put or Get API requests. However, in this scenario, you would need to manage these credentials manually including the rotation of access keys, which is obviously an administrative burden and open to the possibility of being compromised by a malicious attacker.</p>
<p>To alleviate this issue, the EC2 instance could be assigned an IAM Role, which in turn would have the relevant permissions associated granting the EC2 instance and its application to access S3 to perform the Put and Get API calls using existing AWS managed or customer manager policies. EC2 instances can be assigned a role during its creation, or to a running instance. You can also replace a role that is already associated with an EC2 instance with a new role. From a security best practice perspective you should always associate a Role to an EC2 instance for accessing AWS resources instead of storing local credentials on the instance itself.</p>
<p>There is also another great advantage of using Service Roles. Let’s now imagine we have a fleet of EC2 instances all using the same application and performing the same task using the same role, but now consider that your existing application, which was used to perform Put and Get requests is now only required to perform Put requests only, and Get requests must be denied.</p>
<p>To make the change, all you’d to do is to alter the permissions assigned to the IAM Role and all EC2 instances associated with that Role would now have the correct permissions. If this same scenario happened by embedding credentials locally on the EC2 instance, then it would take a long time to replicate the change on every instance accurately.</p>
<p>When creating a Service Role, there’s a number of AWS Services that integrate with IAM that support roles. This is a screenshot at the time of writing this course showing the supported AWS services, but for the latest information, you should always check the AWS documentation found here. Before we move on from AWS Service Roles, I want to mention service-linked roles.</p>
<p>A number of different AWS services require roles to perform functions requiring very specific permissions, and in these instances AWS allows you to create service-linked Roles. These are often created the first time that you use a service. Service-linked Roles come pre-configured with the relevant AWS Managed policies, trusts and permissions allowing only that Service to carry out the required operations with other AWS resources that it needs to interact with. Some examples of these roles include AWS ServiceRoleForAmazonSSM.</p>
<p>So AWS Systems Manager uses this IAM service role to manage AWS resources on your behalf. AWS ServiceRoleForCloudTrail. So this service linked role is used for supporting the organization trail feature with AWS CloudTrail. And AWS ServiceRoleForCloudWatchEvents. CloudWatch uses this service-linked role to perform Amazon EC2 alarm actions.</p>
<p>So if we look closer at the AWSServiceRoleForAmazonSSM in IAM, we will find that the trusted identity to use this role is ssm.amazonaws.com, AWS Systems Manager. and with it having an AWS Managed policy already configured, we’re unable to edit and update this policy. This policy is specifically designed to provide access to AWS Resources managed or used by Amazon SSM, and this role is created when you configure SSM. So the difference between AWS Service and AWS Service-Linked Roles is that AWS Service roles allow you to apply your own customer managed or AWS Managed policies, whereas service-linked roles come pre-configured with a specific set of read-only AWS managed policies that can only be used by that particular service.</p>
<h1 id="Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users"><a href="#Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users" class="headerlink" title="Using IAM User Roles to Grant Temporary Access for Users"></a>Using IAM User Roles to Grant Temporary Access for Users</h1><p>There might be circumstances where you will need to grant temporary access to AWS resources for a particular user and the best way to do this would be to allow the user to assume a role with these new permissions. The alternative would be to either add a policy associated with the user and then remember to remove the policy again afterwards, which would likely lead to a security risk, and as we know, attaching policies to a specific user isn’t considered a security best practice, we should use user groups instead. However, if we added these permissions to a user group that the user belonged to, then all users that are a part of that same user group would receive those permissions, so that’s definitely a security risk.</p>
<p>So in this instance, creating a new role with the relevant permissions and allowing only specific users to assume that role, is the best approach to the problem. When a user assumes a role, it replaces all other permissions that the user has. You might think that the user keeps their current permissions and just inherits the additional permissions set out by the role, but that’s not the case. When you assume a role, your existing permissions are temporarily replaced.</p>
<p>I mentioned previously that every role has a trust relationship and so as a user, you can only assume roles where they have been added as a trusted entity. However, in addition to being trusted, the user also has to have the relevant permissions to assume the role as well and this is done via an access policy.</p>
<p>So let’s assume we have a role called RoleS3FullAccess with a permission policy attached as shown which grants full access to Amazon S3. And the trusted entity of this role was defined as the user specified in this ARN. So this means this role trusts the user, Stuart, to assume the role to gain full access to S3. However, Stuart’s permissions do not currently allow him to assume this role. For Stuart to be able to assume the RoleS3FullAcess role, Stuart will need to have the following policy associated with his user, where the text in red is the ARN of the role. When this policy is associated with Stuart, he then has permission to assume the role which is trusted to him as defined in the trusted entity section of the role allowing him to have full access to S3.</p>
<p>Let me now provide a demonstration on how you could create this example that we just ran through. Okay, so I’m in my AWS Management Console and I’m at the IAM dashboard. So in this demonstration, we’re going to create a role with a policy attached and then add an inline policy to the user, Stuart, to ensure that Stuart can assume the role.</p>
<p>So firstly, let’s create our role. So on the left here, we have roles and then over in the top right, we have Create Role. So because we want this role to assumed by a user, we have to pick the correct type of trusted entity. So it’s not going to be assumed by an AWS Service and it’s not to with Federation either, so the option we need here, is this one relating to an AWS account. Now although says another AWS account, this option is used to creating cross-account access, but you can also create roles to be assumed by users within your own account as well. And to do that, all we need to do in the account ID, is to enter our our account ID, so if I just add that in there, and you can also add a couple of additional options, such as requiring multifactor authentication, et cetera.</p>
<p>So once we’ve put in the account ID of where the users are that we want to assume this role, we can then click on next, permissions. And I want the policy that’s attached to this role to have full S3 access, so let’s have a quick search. And here we have it here, Amazon S3 full access, which is an AWS managed policy. Then click on next for tags. Add any tags in that you’d like for the role as an optional step. I’m just gonna leave it blank for this demonstration. And this is where we can give the role a name, so I’m going to call this RoleS3FullAccess. Give it a description if you need to and here we have the trusted entity, which is my own account and then all we need to do from there, is click on Create Role.</p>
<p>Okay, so if we search for that. RoleS3, here we have our role here. So if we go into the role, we can have a look at our trust relationships, so this is essentially saying that anyone within this account is trusted to assume this role and we can also edit this trust relationship as well. So this shows the policy that relates to that trust relationship. So this essentially allows anyone in this account, access to the role, as long as they have that assume role permissions that we’ll be looking at in just a moment. But I want to make this a bit more secure, so I’m going to change it from root to user, Stuart.</p>
<p>So now the trusted relationship for this role will only allow the user, Stuart, within this account to assume the role, given the correct permissions. So I’m gonna update that trust policy and now we can see the trusted entity has been changed. And if we take a look at the permissions, we can see that it has the policy attached there as well. So now we’ve got roles set up with the correct permissions, we now need to edit the user, Stuart with an inline policy to allow that user to assume the role. So I’ve just gone with IAM user of Stuart. If we click on add inline policy and go across to JSON.</p>
<p>Now, here I’m just going to paste in a policy that I’ve already created. So will allow the user, Stuart to use the secure token service AssumeRole action against the role that we just created and that’s the ARN in the role. So now Stuart will have access to assume that role. Let’s go to review policy. Give this a name. I’m just going to call this, AssumeRoleS3. Create policy. And there we have the inline policy that we just created, attached to the user, Stuart.</p>
<p>So now what I want to do, is to log in as the user, Stuart and show you how I can switch to this role. Okay, so I’ve now signed in as the user, Stuart and to switch roles, all I need to do, is to select on the top right up there, select Switch Role. And again, Switch Role. Type in the account. So I’ll just paste that in quickly and then type the role name, which we called RoleS3FullAccess and then click on Switch Role. And that’s it, we’ve now switched roles and if you look in the top right, we can see that it says, RoleS3FullAccess at this account number and that signifies that you swapped your permissions for that role that the user, Stuart has just assumed.</p>
<p>Once you finish what you need to do with the permissions given by the role, you can simply select the top right area again and say Switch Back. And that will revert you back to your original user with your original permissions. IAM user roles are often used to create a cross-account access role, allowing users in one AWS account to access resources in a different AWS account. For a full explanation on how to create cross-account roles, please see our existing course found here.</p>
<h1 id="Using-Roles-for-Federated-Access"><a href="#Using-Roles-for-Federated-Access" class="headerlink" title="Using Roles for Federated Access"></a>Using Roles for Federated Access</h1><p>In this lecture, I want to discuss how users who have been federated can access your resources using roles. When doing so you have two options. Firstly, Web Identity, this allows users federated by a specified external web identity or OpenID Connect provider, to assume this Role to perform actions in your account. And secondly, SAML 2.0 federation, this allows users that are federated with SAML 2.0, to assume this Role to perform actions in your account.</p>
<p>So let’s first look at Web Identity and a scenario where you might need to create a Role. You’ve just created a new mobile application that requires access to Amazon S3 to store media such as photos and videos from users worldwide. As a part of the operations of your application, it will require permissions to S3, to upload and download this media from tens of thousands of users. Embedding long-term credentials into your application code to do this goes against all security best practices. And so instead, you should design your application to request temporary credentials from authenticated users through web identity federation. </p>
<p>Before I go any further, let me explain what identity federation is. It’s basically a method of authentication where two different providers can establish a level of trust, allowing users to authenticate from one, which authorizes them to access resources in the other. During the federation process, one party would act as an Identity Provider, known as an IdP, and the other would be the service provider, an SP. The identity provider authenticates the user and the service provider controls access to their service or resources based on the IdP’s authentication.</p>
<p>You’ve probably all been to websites where it presents you with a login page, where you can either login using existing credentials, native that service, or you might have an option to authenticate using credentials from a third party provider, such as Facebook, Google, Twitter, or LinkedIn, et cetera. So going back to our example, our application run on AWS would be the service provider, and the Web Identity provider could be Google or Facebook, for example.</p>
<p>So when users authenticate to your app via web identity federation, they will receive an authentication token. This token is then exchanged for temporary security credentials in AWS, which can be mapped to your IAM Role, using the AssumeRoleWithWebIdentity API. This then allows the relevant access to Amazon S3 provided by the role, to carry out the operations needed by the application.</p>
<p>Generally, when working with mobile applications, the preferred and recommended method for managing access, would be via Amazon Cognito, to manage his federation process. For more information related to Amazon Cognito, please see our existing course here.</p>
<p>Let’s now take a look at SAML 2.0 federation. Whereas web identity federation is generally used for large, wide scale of access from unknown users, SAML 2.0 is generally used to authenticate your employees using existing directory services that you might already be using. SAML, which stands for Security Assertion Markup Language, is a standard that’s used to exchange authentication and authorization identities between different security domains, which uses security tokens containing assertions to pass information about a user between a SAML Identity Provider and a SAML consumer.</p>
<p>For example, you might already to be using Microsoft Active Directory to authenticate your employees to your internal network. And so you might not want to or need to create lots of users in IAM with their own set of credentials. Instead, it would be easier to allow them to federate their access through via SAML, integrating with your ADFS server. The benefits of this are twofold. It minimizes the amount of administration required within IAM and it allows for a single sign on solution.</p>
<p>As the vast majority of organizations today are using Microsoft Active Directory, using MSAD is an effective way of granting access to your AWS resources without going through the additional burden of creating potentially hundreds of IAM user accounts. Let’s take a high-level look at how active directory authentication mechanism is established. This example will assume the user within an organization requires API access to S3, EC2, and RDS. This scenario will also include the use of an AWS service called Security Token Service, STS. The Security Token Service allows you to gain temporary security credentials for federated users via IAM, associated with IAM roles and policies. Let’s look at this in more detail via a diagram.</p>
<p>A user within an internal organization initiates a request to authenticate against the Active Directory Federated Service, an ADFS server, via a web browser using a single sign on URL. If their authentication is successful by using their Active Directory credentials, SAML will then issue a successful authentication assertion back to the user’s client, requesting federated access. The SAML assertion is then sent to the AWS Security Token Service, to assume a role within IAM using the AssumeRoleWithSAML API. STS then responds to the user requesting federated access with temporary security credentials, with an assumed role and associated permissions, allowing S3, EC2, and RDS access as per our example, the user then has federated access to the necessary AWS services as per the role’s permissions.</p>
<p>This is a very simple overview of how federation is instigated from the user for API access to specific AWS services. Corporate identity federation is always authenticated internally first by Active Directory before AWS, when creating your role for users federating via SAML, you can specify if you want to provide programmatic access only, or programmatic and AWS management Console access, in addition to specific permissions to access other AWS resources.</p>
<h1 id="IAM-AWS-Policy-Types"><a href="#IAM-AWS-Policy-Types" class="headerlink" title="IAM AWS Policy Types"></a>IAM AWS Policy Types</h1><p>Hello and welcome to this lecture where I want to talk about four different policy types that you can expect to see when working with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">AWS IAM</a>. Identity-based policies. These policies can be attached to users, user groups or roles within IAM. Essentially, any entity that depicts an identity. Resource-based policies. Instead of being associated with an identity, these policies are attached in line to resources themselves. An example of a resource-based policy in IAM will be that of a role trust policy where the policy defines the principle that can assume the role.</p>
<p>Permission boundaries. These policies can be associated with a role or user, but they don’t actually grant permissions themselves, instead they define the maximum level of permissions that can be granted to an entity. Organization service control policies, SCPs. These are very similar to permission boundaries in the fact that they do not grant permissions. They define a boundary of maximum permissions. However, these service control policies are associated with an AWS account or organizational unit, an OU, when working with AWS organizations and govern the maximum permissions to the members of those accounts.</p>
<p>With all of these policy types, it’s easy to see why some people can get confused as to which type of policy is being used or should be used. So let me run through each of these policies in greater detail, starting with the identity-based policies.</p>
<p>As already highlighted, these policies can be attached to users, user groups, or roles and they control what permissions each of those entities have. Identity-based policies can either be managed or inline policies, but what does this mean? Well, managed policies are saved within the IAM library of policies and can be attached to any user, user group or role as and when required, and the same policy can be attached to multiple entities.</p>
<p>Managed policies also come in two different flavors, AWS managed policies and customer managed policies. AWS managed policies are policies that have been pre-configured by AWS and made available to you to help you manage some of the most common permissions that you may wish to assign. Some examples of AWS managed policies can be seen here. From the policy name, you can usually tell what access is being given, although you can expand upon each policy to see the JSON document that is associated.</p>
<p>Customer managed policies are those that you have created yourself, which can then be associated with a user, user group or role. You might want to create customer managed policies when the AWS managed policies do not meet your security requirements. For example, you might want to add additional granularity to the policy to restrict access at a specific API call level.</p>
<p>Let me now explain how inline policies contrast against managed policies. So inline policies are embedded directly into the entity, either the user, user group or role. The policy is not saved and stored in the IAM library policy, its only existence is within the associated entity. As a result, it can’t easily be replicated to other entities, it’s specific to that one user, user group or role, creating a one-to-one relationship. It’s not always best practice to use inline policies as they take a lot of administration to keep on top of and should only be used if absolutely necessary. For example, you might have some elevated permissions that you don’t want to have mistakenly given to someone else that they weren’t intended for.</p>
<p>Okay, let me now move onto resource-based policies. So resource-based policies are effectively inline policies that are associated with a resource instead of an identity. If you have been using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon S3</a> for any extended period of time, then you may have come across S3 bucket policies, which is a common resource-based policy where permissions to the bucket are defined at the resource level and defined which principle or principles can access the bucket based upon different actions.</p>
<p>When using roles within IAM, the role has a trust relationship policy, which is a resource-based policy. As a result, the permissions of the trust are embedded inline within the role itself. So in this instance, the resource is the role and the resource-based policy is the trust relationship. This example shows the resource-based policy of the trust relationship of one of my roles. In this policy, the user, Stuart is the principle and has the ability to assume the role. It is this parameter of principle which signifies the difference between an identity-based policy and a resource-based policy.</p>
<p>Identity-based policies don’t have that principle parameter as the policy is already associated within the identity. Resource-based policies must have the principle parameter to determine which identity that the policy permissions relate to. Next up, we have permission boundaries. So permission boundaries can only be associated with a user or role. It’s not possible to add a boundary to a group. They differ from both identity-based and resource-based policies in the fact that permission boundaries don’t grant permissions themselves. They act as a guide rail to limit the maximum level of permissions that the user or role can be given. The policy configured for the boundary can be an AWS managed or customer managed policy.</p>
<p>So let’s assume for example, that our user, Stuart has an identity-based policy associated with him that allows full access to S3 and full access to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-elastic-compute-cloud-ec2/amazon-ec2/">EC2</a> using the following AWS managed policies. However, if a permission boundary was also configured for Stuart using the following AWS managed policies, which only grants read only access to S3, but still full access to EC2, then the resulting access for the user, Stuart would be full access to EC2, but only read only to Amazon S3 as the maximum permission defining by the permissions boundary was limited to read only despite the user having an identity-based policy associated granting full S3 access.</p>
<p>So the last policy I want to reference are the SCPs, the service control policies. So service control policies are used by AWS organizations and attached to AWS accounts or organizational units, OUs, to define the maximum permissions allowed for the members of the associated account or OU. So in a way they act in a similar fashion to that of permission boundaries, but at the account level and affect all members of those accounts.</p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS and EC2 via an identity-based policy. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2 despite having full access to S3. The SCP would serve to prevent service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<p>So to be clear, an SCP does not grant access, they add a guide route to define what is allowed. You’ll still need to configure your identity-based or resource-based policies to identities, granting permission to carry out actions within your accounts. If you want to use service control policies to help you manage your security at an account level, then you need to ensure that you deploy AWS organizations using the enable all feature setting.</p>
<p>Within IAM, you have the ability to view any SCPs that are applicable to your AWS account, the policy statement itself, it’s ARN, the number of entities it affects and you can also review access activity to learn when a principal within the organization last accessed a service. However, if you wanted to update or edit the SCP, then you’d have to do that from within the AWS organization service itself. The SCP can’t be edited from within IAM. For more information relating to how to implement, manage and secure AWS organizations using SCPs, then please see our existing course here.</p>
<h1 id="Examining-the-JSON-Policy-Structure"><a href="#Examining-the-JSON-Policy-Structure" class="headerlink" title="Examining the JSON Policy Structure"></a>Examining the JSON Policy Structure</h1><p>In this lecture, I will be taking a deep look at the syntax and structure of IAM policies. For both identity-based and resource-based policies, the syntax is the same, but you might use slightly different parameters. If you’ve worked with IAM at all before, then I am sure you have come across an IAM policy. After all, it is the component that defines what an identity can or can’t access. But what do these policies actually look like? IAM policies are formatted as JSON documents, JavaScript Object Notation, and each policy will have at least one statement where the structure may look like this example.</p>
<p>Let me break the structure of this policy down to allow you to understand each element. Version. This specifies the policy language version and specifies the language syntax used, and at the time of writing this course, the current policy version is 2012-10-17. Statement. This defines the main element of the policy, which will includes other sub-elements, including the Sid, Effect, Action, Resource, and Condition. These elements will identify the level of access granted or denied and to which resource. A policy must contain at least one statement, but it can also contain an array of statements. For each statement block, it must be enclosed within curly braces, but if you use an array of statements, then you must enclose the entire array within square brackets. And we shall look at an example of this during this lecture.</p>
<p>Sid. This is the statement ID, and it’s an optional parameter that allows you to set a unique identifier within the statement. As you add more arrays within your policy, it can be a good idea to include a Sid for each one, allowing you to name them appropriately, making them more easily identifiable. For example, AllowGetObjectForS3. Without reading the rest of the statement, you can get a good idea of what the permissions in this statement allows.</p>
<p>Effect. This element can be set to either Allow or Deny, which either grant or restrict access for the actions defined in the statement. By default, all access to your resources are denied and so, therefore, if this is set to Allow, it replaces the default denied access. Similarly, if this was set to Deny, it would override any previous Allow. An explicit Deny in a policy will always take precedence over any Allow.</p>
<p>Principal. This parameter defines which principal the policy relates to. The Principal is only used for resource-based policies, for example, those policies attached to S3 Buckets. When using identity-based policies, this parameter is not required within the policy as the policy itself is associated with the principal and not a resource. As an alternative to the Principal parameter, this could be replaced with the NotPrincipal parameter, which would specify the user, role, or AWS account that is not allowed or denied access to the associated resource.</p>
<p>Action. This is the action that will either be allowed or denied, depending on the value entered for the Effect parameter. Actions are effectively API calls for different services. As a result, different actions are used for each service. For example the DeleteBucket action is available for S3 but not EC2, and likewise, the CreateKeyPair action is available for EC2 but not S3. The action is prefixed with the associated AWS service. This example defines two actions, CreateTrail and DeleteTrail, for the CloudTrail service. As another example, you can see this one here with the asterisk, and this acts as a wildcard which represents all actions for the CloudTrail service, essentially granting full access to the service.</p>
<p>Similarly, as we had with the Principal parameter, we could replace the Action parameter with the NotAction instead, and this could help you optimize your policy by creating a shorter version, listing just a limited set of actions that should not match instead of creating a longer policy listing all that actions that should. An example of using the NotAction is shown here. This policy essentially allows all actions for CloudTrail apart from the DeleteTrail API, and this is because the NotAction parameter is being used.</p>
<p>Resource. This element specifies the actual resource you wish the Action and Effect to be applied to. AWS uses identifiers known as ARNs, Amazon Resource Names, to specify specific resources. Typically, ARNs follow a syntax of, arn, partition, service, region, account-id, and then resource. So the Partition element, this relates to the partition that the resource is found in. For standard AWS regions, this would be AWS. Service. This reflects the specific AWS service, for example, S3 or EC2.</p>
<p>Region. This is the region where the resource is located. Some services do not need a region specified, so this can sometimes be left blank. Account-ID. This is your AWS Account ID, without hyphens. Again, some services do not need this information and so can be left blank. Resource. The value of this field will depend on the AWS service you are using. For example, if I were using the action, Action:s3:PutObject, then I could use the bucket name that I wanted those permissions to apply to. Again, we also have a NotResource parameter that can be used to explicitly match all other resources except those specified. For example, this policy will allow access to all S3 buckets other than those specified by the NotResource parameter.</p>
<p>Condition. This is an optional element that allows you to control when the permissions will be effective based upon set criteria. The element itself is made up of a condition and a key value pair and all elements of the condition must be met for the permissions to be active. Let’s take a look at an example condition. In the example, the IP address is the condition itself, which the key value pair will be effective against. The aws:SourceIp is the Key and the 10.10.0.0&#x2F;16 is the value element of the key. So effectively, what this is saying is, if the Source IP address of the user who is requesting access via the policy is within the 10.10.0.0&#x2F;16 network range, then implement the permissions in the policy statement.</p>
<p>Now that I’ve gone through the core parameters of an IAM policy, let’s take a look at a couple of examples to ensure we can understand the permissions that are being presented within the policy. As I mentioned previously, you can have multiple Sids within a statement, each granting a different level of access. The example below demonstrates this, and I have highlighted each a different color to show the separation.</p>
<p>So looking at this example policy, let’s determine what access is being granted. So in StatementBlock1, this allows any resource full access to CloudTrail on the condition that their source IP address is within the 10.10.0.0&#x2F;16 network range. StatementBlock2. This allows full access to all RDS databases using all API calls except for that of the rds:DeleteDBInstance due to the NotAction parameter being used instead of the Action being used. And in StatementBlock3, this allows the creation and deletion of S3 Buckets within the cloudacademy bucket on S3.</p>
<p>Let’s take a look at another example policy, this time, a resource-based policy, and in this instance, it’s from an S3 bucket. So in this policy, it allows the user, Stuart, as highlighted by the Principal parameter, to create and delete buckets in addition to deleting the bucket policy. Stuart can also delete and put objects within the bucket, and the bucket that this policy refers to is the bucket named ca-bucket-uk, as defined by the Resource parameter. However, there is a condition bound to this policy that states that Stuart can only do this if he was authenticated via Multi-Factor Authentication, MFA. That should now give you more of an understanding of how JSON IAM policies work and what they look like.</p>
<h1 id="Creating-an-AWS-IAM-Policy"><a href="#Creating-an-AWS-IAM-Policy" class="headerlink" title="Creating an AWS IAM Policy"></a>Creating an AWS IAM Policy</h1><p>Hello and welcome to this lecture where I will show you how to create your IAM policies. Let’s start by creating an identity-based, customer managed policy, and there are a number of ways to create a customer managed policy, these being, copy an existing AWS Managed Policy. So we can copy an existing policy and then edit that policy to ensure it meets the requirements we need, and this can save us some time.</p>
<p>We can use the Policy Generator, and this allows you to create a customer managed policy by selecting options from a series of dropdown boxes. And we can also create our own policy. So if you’re proficient in JSON and the syntax of IAM policy writing, then you can write your own policies from scratch or paste in a JSON policy from another source. So I now want to give you a quick demonstration on how to create a policy in each of these three ways. And the demonstration will include, how to create a customer managed policy by editing an existing AWS managed policy, how to create a customer managed policy using the Policy Generator, and how to create a customer managed policy from scratch. So let’s take a look.</p>
<p>Okay, so I’ve logged into my AWS Management Console, and I’m at the IAM dashboard, and I’ve gone into Policies. So from here, we can create a number of different IAM policies. And the first method I’m going to show you is how to copy and import an existing AWS managed policy to create your own, if you need to make a few changes to it. So let’s go ahead and do that now. So from here, we need to go across to Create Policy.</p>
<p>Now, from here, we have an option on the top right here called Import Managed Policy, and that’s what we want to do. I want to import an AWS managed policy and then make a couple of small changes to it. So if I select Import Managed Policy and then find the policy that I want to find. So let’s, for example, look at AmazonS3FullAccess, import that, and we can see here that it’s imported the data. And if we look at the JSON tab, we can actually see the JSON format of that policy, and from here, we can directly make changes.</p>
<p>So for this quick demonstration, I want to copy this existing policy, but instead of allowing any resource, I want to specify my own resource for this, which will mean instead of this policy being, allow full access to Amazon S3, it allow full access to a specific bucket on Amazon S3. So let me go ahead and make those changes now. So I’m gonna change the asterisks from Resource and put in my own specific ARN of a bucket. So now, I’ve changed the resource to my own ARN.</p>
<p>So now, all I need to do is click on Next, go to Tags, and I can add any optional tags if I want. I’m just gonna leave that blank for this demonstration. Go across to Review, and here I can give this new policy a name. So I can call it S3FullAccessToMyBucket. And description, this allows full S3 access to ca-bucket-uk. And it gives you a summary of the policy here. So we can see the service that it’s using, the access level, and also the resource. Once we’re happy with that, we can simply click on Create Policy. And we can see that that policy has now been created.</p>
<p>Now, if we have a look at that policy, just by clicking on it there, it’ll take us straight to it, and we can see the JSON version of the policy. So that’s a very quick and easy way if you want to save yourself some time by copying existing S3 managed policies that are already there. Now, I chose a fairly simple policy just for demonstration, but there are some quite complex policies that AWS already have that you might need to just tweak a few changes to so you can simply import those existing managed policies, make your changes, and then save it as a new customer managed policy.</p>
<p>Okay, so that’s the first method covered. Now, next, I want to show you how to create a policy using something called the AWS Policy Generator. Now, if you simply go to Google and type in the AWS Policy Generator, then it’ll come up straight away, and you’ll be brought to a page like this. Now, the actual URL of this Policy Generator, if you’d prefer to type it in, is awspolicygen.s3.amazonasw.com&#x2F;policygen.html. So this is a Policy Generator, and it allows you to easily create different types of AWS policies.</p>
<p>So if we look at this drop down list here, we can create an SQS policy, an S3 bucket policy, which, as we know, is a resource-based policy, a VPC endpoint policy, an IAM policy, and an SNS topic policy. We’re interested in the IAM policy. So if we select that, now, in step two, we can add our statements. Now, here, we have our effect, which can be allow or deny. So let’s say allow for this example. And then, we can select the service that we’re interested in. Let’s select Amazon S3 to keep it nice and simple. And now, we can select our actions.</p>
<p>Now, we can select individual actions here just through these tick boxes for any actions that we’re interested in. So I’ll just select a number of different ones there. And you can see here that it’s selected five actions. If you wanted all actions, you would simply tick this box. And then, you’d put in the ARN of the bucket. So let’s just put in that same bucket that we used in the previous example. And then, here, we can also add in any conditions that we’d like. So just through a series of dropdown boxes, you can specify any conditions that you’d like in there as well. And then, once you’re happy with your policy, simply select Add Statement. And here, it breaks down a summary as well. So it shows the effect, the action, the actions that we selected, the resource, and if there’s any conditions, which we didn’t specify any.</p>
<p>Now, if you wanted to, we can now add an additional statement. So for example, if we wanted to add some RDS elements in here as well, we can select AWS Service, select a load of actions, put in the ARN of your RDS database, and also add that statement to the same policy. Then, once you’re happy with your policy and the number of statements that you’ve added, simply click Generate Policy. And here, it shows you the JSON view of the policy that you would need based on your dropdown selection. So what you can do now is simply copy that, go back to IAM, go to your policies, Create Policy, go to the JSON tab, and simply paste it in. So it’s a very quick way of creating a policy through a series of dropdown boxes. And again, you can just progress through, adding any tags that you need to do, give this a name, ThisIsMyPolicy, and then click Create Policy. And again, you can see that this has been created. You can select that. And again, we can see the JSON statement there.</p>
<p>So now, we’ve looked at how to create a policy by copying an existing AWS managed policy. We’ve looked at how to use the AWS Policy Generator. Now, let’s just quickly review how to create a policy from scratch. So again, go back to Policies and then Create Policy. Now, you can either use this visual editor or go straight to JSON, and you can start typing out your policy in here as and how you need it, or you can use the visual editor, which is very similar to the Policy Generator we just looked at, where, again, you can choose a service, then the specific actions, say all S3 actions, then you can specify the resources, whether you want this, again, as an access point, a bucket, a job, an object. So you can say any object. Then, you can specify any conditions, if you need MFA or specific source IP, for example. And again, you can also add additional permissions here.</p>
<p>So almost like another statement. It’s very similar to the Policy Generator. But for me, personally, I think the Policy Generator is slightly easier to understand. And then, once you have your settings as you want them, and this has all been pre-filled from the options from the visual editor, and, again, click Tags, then Review, and then give this a name, MyPolicy, and then go to Create Policy. And if we take a look at the policy, we can just see its details again. And again, this shows the JSON view, et cetera. And if we wanted to edit the policy directly here, then we can. Click on Edit. Then, we can either edit through the visual editor, or we can directly edit the JSON view as well. So that’s just a couple of ways of creating your IAM policies in a few very simple steps.</p>
<h1 id="Policy-Evaluation-Logic"><a href="#Policy-Evaluation-Logic" class="headerlink" title="Policy Evaluation Logic"></a>Policy Evaluation Logic</h1><p>Every time someone tries to access a resource within AWS, the request is processed through a series of steps. One of which involves evaluating the level of permissions based upon the policies that are used. So let’s take a look at the whole process to understand how access is either granted or denied. And we can start with a simple four step process. So firstly authentication. We must first ensure that the principle sending the request is authenticated as a valid user.</p>
<p>Next, the context. Once authentication of the principle has been established, AWS then needs to determine the context of the request that is being asked, for example, what service or action is being requested. And this ensures that the relevant policies can be highlighted based on the request. We then have policy evaluation, and this is the part that we are interested in. Based on the request, there may be multiple policy types that need to be reviewed to determine the level of access, and I shall cover this in greater detail as we go through this lecture. And then finally, the result. AWS will determine if access is allowed or denied based upon the evaluation of all policies used.</p>
<p>So for this lecture, I want to focus purely on the third point, the policy evaluation and how that process is carried out. The rules for reviewing permissions across multiple policies in a single account is actually quite simple and can be summarized like this: by default, all access to a resource is denied. Access will only be allowed if an Allow has been specified within a policy associated with the principle. If a single Deny exists within any policy associated with the same principle against the same resource then that Deny will overrule any previous Allow that might exist for the same resource and action. So to reiterate, an explicit Deny will always take precedence over an Allow.</p>
<p>Now, there is an order in which policies are evaluated, and the following list of policies are shown in the order of evaluation. So firstly, we have any Organizational Service Control Policies. Then any Resource-based policies, then IAM permission boundaries, and then finally Identity-based policies. So let’s look at an example scenario. Let’s assume that the user, Stuart, is requesting to upload an object to the s3 bucket of ca-bucket-uk using the s3:PutObject API.</p>
<p>With this in mind, let’s assume we have the following policies in place to see what happens at each step of the evaluation. So firstly, the evaluation will review any organization SCPs in place, and here is our example SCP. So this SCP will simply deny all access to RDS. So there is no Deny in place that affects the s3:PutObject requested by Stuart so the evaluation continues. Next, the evaluation will identify any resource-based policies, and here we have a Bucket Policy associated with the ca-bucket-uk as shown.</p>
<p>Again, there is no Deny here for the request in question, so the evaluation continues. Next, we have IAM Permission Boundaries. And this IAM Permission Boundary Policy is set on the user, Stuart. So this policy sets out a maximum permission boundary of full access to s3. Remember, permission boundaries do not actually grant permissions, they set the maximum privilege level, as full access to s3 allowed, the evaluation continues.</p>
<p>Finally, we have the evaluation of any Identity-based Policies, and this policy is associated to the group that the user, Stuart, belongs to. So as we can see, this policy allows any s3 action to the ca-bucket-uk. As a result, this permits Stuart to upload an object using s3:PutObject to the s3 bucket of ca-bucket-uk. So the final decision upon the policy evaluation is to allow the request.</p>
<h1 id="Implementing-Cross-Account-Access-Using-IAM"><a href="#Implementing-Cross-Account-Access-Using-IAM" class="headerlink" title="Implementing Cross-Account Access Using IAM"></a>Implementing Cross-Account Access Using IAM</h1><p>Welcome to this lecture covering the principles of cross-account access. Firstly, let me quickly define what cross-account access is. Put simply, it allows IAM users from one AWS account to access services within a different AWS account through the use of IAM roles. Now, IAM roles allow users and other AWS services and applications to adopt a set of temporary IAM permissions to access AWS resources.</p>
<p>But why would you want to implement cross-account access in the first place? Well, essentially it comes down to architecture and security best practices. For example, most organizations using AWS will probably have a production and a dev or test environment. For ease of management and isolation, it’s likely that each of these environments will be in separate AWS accounts. With this in mind, your development team who primarily work in the dev account may occasionally require access to resources in the production account. Instead of creating additional user accounts in the production environment, which would only be required on a rare occasion, roles could be used with the required permissions instead. The development team could then be granted permission to assume this role to then access the resources in the production environment. This helps to adhere to the principle of least privilege.</p>
<p>As this access is not always required, the developer must consciously switch to and assume the role to access the other resources. For additional security, you could also add multi-factor authentication to the role before it can be assumed, adding another verification step.</p>
<p>Let me now look at the key steps required to implement cross-account access. To understand this process, you need to be aware of two key terms that I will be using. These being the <em>trusted</em> account, and the <em>trusting</em> account. Using our previous example of production and dev, the production account will be the trusting account and the development account would be the trusted account, as the development users will be trusted to access the resources in the production account, which is trusting those users.</p>
<p>Let me break the process down. Firstly, you must create a role from within the trusting account, which in our example, would be the production account. This is to establish a trust between the two accounts. This role will define the development account as a trusted entity. Next, you must specify the permissions attached to this newly created role which the users in the development account would assume to carry out their required actions and tasks. Next, you must switch to the trusted account, in this scenario the development account, to grant permissions to your developers to allow them to assume the newly created role in the trusted account. Finally, you can test the configuration by switching to the role.</p>
<p>Now we know the fundamentals of this process, let me provide a demonstration on how to configure this using the AWS Management Console. Okay, so I’m logged into my AWS account, as you can see I’m on the AWS Console, and this is going to be my trusting account. So from here, I need to select IAM to allow me to create the role. So under Security, Identity, &amp; Compliance, we can see the IAM service, so if we select that. Go across to the left-hand side and select roles. And at the top you can see a blue button called Create role, so let’s click on that. And here I need to select the type of trusted entity. Now, because I want this role to be assumed by another account, I’ll select this option here, the second option where it says Another AWS account. And underneath here you have a couple of options, and here we need to enter to account ID and this is the account ID of the trusted account. So let me just go ahead and enter the account number of what will be the trusted account. And if you wanted to add multi-factor authentication then you can select this tick box here. For the sake of this demonstration, I’m just going to leave that unticked. Once we have the trusted account ID entered then we click on next for permissions. And here we specify what permissions we want assigned to this role to allow the users in the trusted account to assume. So let’s just enter EC2 and we’ll give them EC2FullAccess. Once you’ve assigned your permissions, select Next. </p>
<p>If you want to add any tags for this role then you can enter that here. For this demonstration, I’m just going to click on Next. And finally, we get to give our role a name, so I’m just going to call this CrossAccountDemo. We can add a description if we want to. And we can see here that the trusted entities is the account ID that I entered. So this is a different account from the one I’m in at the moment. And also you can see the policy that I assigned here. </p>
<p>Okay, let’s create this role. And there you see it, the role CrossAccountDemo has now been created. So now we’ve set up the role and we’ve added the trusted account as the entity of this role, let’s now swap over to that account to allow the users in that account to assume this role within this account. So let me just flip over to that account now.</p>
<p>Okay, so I’m now in the other account, as you can see up here it’s a different account number. And this is the account number that I added as the trusted entity on the role. So if we go down to the IAM service from here. And I want to assign some permissions to one of my groups, and I’m going to assign it to the developer group. So if I select this group here, we can already see that this developer group has a number of policies that allow specific access within this account, but I want all users in this group, and if we just click on the Users tab, we can see that there’s two users here, and one of those users is the account that I’m logged in as at the moment.</p>
<p>So, I want all users of this group to be able to assume that role that we created in the trusting account. So I’m going to create an inline policy. So if I click on Inline Policies, it says there are no inline policies to show, to create one click here. I’m going to create a Custom Policy, click on Select. Give the policy a name and I’ll call this CrossAccountAccess. And I’ve already created a policy for this, so let me just paste this in and I’ll just run through it. So, what this policy does, it allows the AssumeRole action for the role that I created in the other account, and as we can see, that is the account number of the trusting account where the role exists, and that’s the role name. So when you’re setting this up on yours, just make sure you have the account number of the trusting account and also the correct role name that you configured for your own.</p>
<p>So, once that policy’s created, if we just click on Validate Policy, we can see that the policy is valid, and then click Apply Policy. And if we have a look, we can now see the policy under Inline Policies. So again, just to reiterate, now any user within this group is able to assume the following role, and that’s the role that we created in the trusting account.</p>
<p>So now that permission is set, let me see if I can switch roles. Now, to do that, if you go to the top of the screen here where it shows who you’re logged in as under what account and click on the dropdown list, and then at the bottom we can see Switch Role. So if I click on Switch Role. So if I enter the account number of the trusting account, and then the role name, which was CrossAccountDemo, and then select Switch Role, we can see at the top here that we’re now logged in under the CrossAccountDemo at the trusting account.</p>
<p>Now, when you assume a role, the permissions you already had are replaced with temporary permissions of this role. As a result, I can see a lot of errors within IAM here, because I don’t have access to IAM. If you remember, the only permissions we allowed on this role on the CrossAccountDemo role, was full access to EC2. So that’s why I’m getting all these errors on IAM. However, if I go across to EC2, we should have permissions to launch an EC2 instance. So if I go down to launch instance, and go through the process, we can see that it’s allowing us to go through. Now, on this page, you’ll notice that there’s an error here as well; it says I do not have permissions to list instance profiles, and this is because this element here is related to IAM and I don’t have access to IAM with the CrossAccountDem role - I only have access to EC2.</p>
<p>So I’m not allowed to select any IAM roles for this EC2 instance. However, I can continue to go through and accept all the defaults and launch an instance. And as you can see, that instance has now been launched. So I’ve got access as per the permissions of the role. And remember, this instance is actually running in the trusting account because that’s the role that I assumed.</p>
<p>So if I logged back into the other account now, we should see this same instance running. And let’s just give this a name: Cross Account Instance. So if I go back to my original account which is CloudAcademy, we’ll see that this EC2 instance isn’t running because it’s in the wrong account. So let’s take a look. And now I’ve come out of assuming that role, we can see that in the trusted account there isn’t actually any EC2 instances launched because it was in the trusting account. So let me now log back into the trusting account and we should see that EC2 instance that we launched when we assumed the role. So let me flip over to that now. So I’m now back in the trusting account and here we can see that the instance is here that we created when was in the other account and assumed the role. So that’s how you create and configure cross-account access.</p>
<h1 id="An-Overview-of-AWS-WAF"><a href="#An-Overview-of-AWS-WAF" class="headerlink" title="An Overview of AWS WAF"></a>An Overview of AWS WAF</h1><p>Hello, and welcome to this lecture where I shall give an introduction to the AWS WAF service. If you are delivering any kind of web content, either through CloudFront Distributions, Amazon API Gateway REST APIs, Application Load Balancers, or via AWS AppSync GraphQL APIs, then I would recommend you implement the AWS Web Application Firewall service as an additional layer of security.</p>
<p>Without using a web application firewall you could be exposing your websites and web apps to potentially harmful and malicious traffic, which could lead to security risks within your environment. This could have a significant detrimental impact on your business from both a financial and reputation perspective. The AWS Web Application Firewall is a service that helps to prevent websites or web applications from being maliciously attacked by common web attack patterns. Many of which are outlined in the OWASP top 10 list, such as SQL injection and cross-site scripting.</p>
<p>In addition to your own custom criteria, such as perhaps filtering request based on source IP address or country of origin. OWASP, the Open Web Application Security Project is a not-for-profit organization that is dedicated to helping others improve security and software. They provide a top 10 list of the most critical security vulnerabilities and risks surrounding application architecture. For the <a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">latest OWASP top 10 list</a>, please visit the following <a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">link</a>.</p>
<p>If you can implement a WAF within your architecture to mitigate against some of these vulnerabilities, then that’s a huge asset to your web application architecture and a great relief to the security officers within your organization. If you then compare the implementation and administration time needed to deploy AWS WAF to a standard WAF solution, then it’s by far quicker. Further, AWS WAF is far simpler and easy to manage as well. And there are currently two versions of AWS WAF. AWS WAF classic, and AWS WAF, sometimes referred to as new AWS WAF.</p>
<p>Now you should only use AWS WAF classic if you created AWS WAF resources prior to November, 2019. So in this course, I shall be focusing on new AWS WAF. The AWS WAF service interacts with Amazon API Gateway, Amazon CloudFront for its distributions, Application Load Balances, and AWS AppSync, ensuring only filtered web requests that meet specific conditions are forwarded to be processed by these services. WAF works with these other services to filter both HTTP and HTTPS requests by distinguishing between legitimate and harmful inbound requests that will then either be allowed or blocked.</p>
<p>Let’s take a closer look at what the WAF service is composed of to allow you to understand how it works. So there are a number of essential components relating to WAF. These being Web ACLs, Rules and Rule Groups. Let me explain each of these individually to see what part they play within the service, starting with the main building block of the configuration, the Web ACL. So Web Access Control lists, or web ACL, are the main building block of the WAF service. And it’s used as the component that is associated with one of the supported resources to determine which web requests are considered safe and which ones are not.</p>
<p>At the time of writing this course, the supported resources include Amazon CloudFront Distributions, Amazon API Gateway REST APIs, Application Load Balances, and AWS AppSync GraphQL APIs. The Web ACL contains rules, which contains specific controls and criteria checks that assess each web request to determine whether it should be allowed or blocked. Also for each Web ACL, there is a default action that traffic should take if the criteria set out in the rules are not met by the incoming request, and the options for this are either allow or block.</p>
<p>Rules. Each rule contains statements and actions, which focus on specific criteria that the web request will be inspected against. If the inspected request matches the criteria set out in the statement, then that is considered a match. The result of this match can then follow an action of allow, block, or count. Allow means the request is forwarded onto the resource. Block means the request is dropped and a response is sent back to the requester, informing them that the request was denied, and count simply counts the number of matching requests.</p>
<p>Rule Groups. A Rule Group is essentially a collection of rules that you can apply to different Web ACLs as you need to. AWS WAF also comes pre-configured with a number of AWS manageable groups that have been built and designed to protect your resources against some common attack patterns. In addition to this, you can also get access to other Rule Groups created and sold by other vendors on the AWS marketplace. So from a logical architecture perspective, let’s assume that WAF has been associated with a CloudFront Distribution with its origin pointing to S3. The network diagram would look as follows at a high level.</p>
<p>So firstly, a user would initiate a request to the web content being served by the CloudFront Distribution. Next, although logically AWS WAF sits in front of CloudFront, the request will be received by the CloudFront Distribution first. Then it’s immediately forwarded to your associated WAF Web ACL. The AWS WAF Web ACL associated with the distribution would filter the incoming web traffic using the Rules or Rule Groups. So before it’s even traversed your CloudFront environment and network, you have the ability to detect, analyze, and either allow or block the incoming request.</p>
<p>If the criteria highlighted by the Rules deemed that the request should be blocked, then the traffic would be stopped and prevented from progressing further. The requester would then be informed the request was denied. If the traffic met the criteria, allowing the traffic to pass, then the request would be forwarded to CloudFront. CloudFront would then serve the content as required. You might already have other security detection mechanisms within your organization that operate deeper within your infrastructure, perhaps at the web server layer, to mitigate against some of the same risks that WAF does.</p>
<p>And so you may be thinking, why should I implement WAF if I have this existing solution, which is working okay? If you have existing detection systems within your infrastructure, then that’s great. However, the closer they are logically implemented to your web application, then the greater the risk of additional vulnerabilities occurring towards the edge of your infrastructure. It’s best to mitigate vulnerability risks as close to the perimeter of your network environment as possible. By doing so, reduces the chances of other infrastructure and systems being compromised.</p>
<h1 id="Understanding-Rules-and-Rule-Groups"><a href="#Understanding-Rules-and-Rule-Groups" class="headerlink" title="Understanding Rules and Rule Groups"></a>Understanding Rules and Rule Groups</h1><p>In this lecture, I shall be discussing rules and rule groups within your AWS WAF web ACLs. These rules are used to define inspection criteria that will determine if the web traffic will be allowed or blocked. As a result, they play a fundamental role in making sure your web ACLs are secure and effective. So it’s important that you understand them in detail. When you come to build your web ACLs and you have defined the resource that you want it to be associated with, you’ll be asked to add a rule or rule group to the web ACL.</p>
<p>As you can see in this screenshot, there were two options. Add manageable groups and add my own rules and rule groups. Let me take a look at each of these in more detail, starting with managed rule groups. So manage role groups are essentially a set of predefined rules that have already been created by AWS and other AWS marketplace sellers for you to use.</p>
<p>At the time of writing this course, I have the following manageable group vendors that appear in the AWS Managed Console. AWS, Cyber Security Cloud, f5, FORTINET, GEOGUARD, imperva and ThreatSTOP. This list will be constantly changing with new vendors. So please review the available manageable groups as and when you need to apply them.</p>
<p>These screenshots here show you a couple of examples from FORTINET and GEOGUARD. As you can see, they provide a set of defined rules that are already configured to monitor and detect anomalies in your web requests against specific vulnerabilities. The example from FORTINET allows you to use a rule group to protect against the OWASP Top 10 vulnerabilities. Whereas the GEOGUARD rule groups contain criteria to help you detect IP fraud using geolocation.</p>
<p>Now, the benefit of using these rule groups is that they have been tried and tested and have been designed to help protect against a specific type of vulnerability or risk. And this can save you a lot of time and effort in trying to recreate the same with your own rules. You may also notice that each rule group also contains a description of the rule group, in addition to the link to subscribe to the rule group, which contains the pricing, reviews and more detailed information.</p>
<p>You may also notice that there is a capacity rating. For FORTINET, the rating is 1,000 and for GEOGUARD, the capacity rating is 100, but what does this mean? Well, for each web ACL, there is a limit of 1,500 capacity units known as WCUs. And these can be used by rules and rule groups. These WCUs are used to control the amount of operating resources that are needed by AWS WAF to run your rules and the inspection criteria set out by those rules.</p>
<p>Now, depending on the rule and its complexity of statements, it will directly relate to and determine how many WCUs will be consumed by that particular rule. So effectively the more intricate the rule is from an inspection perspective, the more WCUs will be consumed. When creating a rule group, you must stipulate an immutable capacity at the time of its creation. This will then set the maximum WCUs that the rules within the group can reach. This is because when you alter the rules within a rule group, it ensures that it doesn’t exceed the maximum WCUs of 1500, set out within any web ACLs that are sharing that rule group.</p>
<p>Consider the following scenario. There are two web ACLs, each with a WCU limit of 1500, both sharing a manageable group. Let’s call this rule group one. Our web ACLs are configured as shown. If the rule groups did not have an immutable limit and rule group one had its rules changed, which meant the WCU rating changes from 1,000 to 1200 due to additional complexity, then web ACL1 would continue to operate as the total would be 1300 WCUs. However, web ACL2 would fail as the total would change to 1600 WCUs, which exceeds the maximum limit of 1500 for the web ACL. Therefore by ensuring that rule groups have an immutable WCU unit limit set, it would prevent your web ACLs from failing if the rules within the rule group were modified. Any changes to the rule group would have to ensure that the combined rule WCU remained at or below the immutable limit set.</p>
<p>Okay, so now we know what a manageable group is and the benefits that they bring, but let’s now go back to looking at creating our own rules and our own rule groups. So custom rules. Let’s assume we didn’t choose to use manageable groups and instead we wanted to add our own rules. The first thing I need to select is a rule type. So I need to select either IP set, rule builder, or rule group.</p>
<p>The IP set. By selecting IP set, it allows you to configure the rule criteria based upon either the source IP address or the IP address in header. However, before you can select an IP set, you must first create it from outside of the web ACL. To do this, you must select IP sets from the AWS WAF dashboard. From here, you can create an IP set by providing an IP set name, an optional description, a region in which the IP set should exist, or you can split the Global CloudFront Region, the IP version of addresses to be added to the IP set, which can be IPv4 or IPv6, the IP addresses to be attached to the IP set list, And you must add these addresses in a sider format with each entry on a different line. And this allows you to add individual IP addresses or a network range. For example, if using IPv4, then you could add 17.6.7.8&#x2F;32 to identify a single host address, or something like 107.11.0.0&#x2F;16 to identify a network range.</p>
<p>Once you have created an IP set, it can then be added within your web ACLs. So jumping back to configuring IP sets in your web ACL. After giving your rule name and selecting your IP set from the dropdown menu, you need to decide if you want to use the source IP address or the IP address in header from the request in your rule. If you decide to configure the IP set on source IP address, then all that remains for you to define is if you want the action of the web ACL that matches the IP source to allow, block or count the request.</p>
<p>When a request is allowed, it is forwarded onto the relevant associated resource, for example, a CloudFront distribution. When a request is blocked, the request is terminated there and no for the processing of that request is taken. A count action will do exactly that. It will count the number of requests that meet the conditions within that rule. So this is a good option to select when testing your rules to ensure that the rule is picking up the requests as expected before setting it to either allow or block.</p>
<p>If an incoming request does not meet any rule within that web ACL, then the request takes the action associated with the default option specified, which can either be, allow or block. If you select the option of the IP address to be based upon the IP address in header, then you must also specify the header field name and how you want to treat missing IP addresses in the header. And then finally, the action required for a match of the rule. Whichever option you choose, source IP address, or IP address in header, then you can enable a custom response to be sent to the requester if the action of block is used. The rule builder.</p>
<p>Let’s now take a look at the rule builder option. Here you can create your rules using the rule visual editor, allowing you to create your rule from a series of dropdown menus and options, or you can create your rule statements using the rule JSON builder. If you are experienced with WAF and JSON and are looking to create more complex nested statements within your rule, then the JSON editor is probably the easiest way to do this. By nesting statements, you can implement logic within your rules that allow you to use arguments such as, and, or, and not between the nested statements within your rule.</p>
<p>Okay, let’s turn back to the rule visual editor for those who are less familiar with JSON. When building a rule, you first need to give it a name. Next, you need to define the rule type, which can either be regular or rate-based. The only difference between rate-based rules and regular rules is that rate-based rules count the number of requests that are being received based on the source IP address, or the IP address in the header over a time period of five minutes.</p>
<p>When you select a rate-based rule option, and as you can see from the image, you are asked to enter the maximum number of requests from a single IP within a five minute timeframe. When the count limit is reached, the action of the rule is triggered until the request rate falls back below the rate limit specified. This can be used as a temporary block of request from a specific source if they are sending an excessive amount of requests within a short period of time.</p>
<p>It’s worth pointing out that the only actions allowed on rate-based rules are either block or count. If you’re not looking to monitor the amount of requests from a single source, then simply select a regular rule type. Regular rules are effectively if&#x2F;then statements. So as you can see, if you need to define a value from the dropdown menu of if a request, then your options, are matches the statement, matches all the statements, matches at least one of the statements, or doesn’t match the statement. And these options allow you to implement and build and, or and not statements within your rules.</p>
<p>Depending on the option from this menu will dictate the rest of the options you have within your statement. For example, if you select the matches all the statement, then you will be able to add additional statements to build your rule with a logical and between each statement. When you have finished creating your rule using a single statement or using the logical arguments, then you must define the then part of the rule, which as you are probably familiar with now, are allow, block or count. So here we are allowed to specify an allow action, whereas when we use the rate-based rule, the allow action was not an option. Once your rule has been created, you can see the amount of WCUs that has been assigned to your statement.</p>
<p>Okay, so now we have looked at IP sets, the rule builder. So the last point to look at is adding a rule group to the web ACL. So rule groups. Your rule groups need to be built outside of your web ACL creation. You can’t build them on the fly like you can with adding new rules. They are created from the main dashboard menu in your WAF management console. So from here, you can then create your rule groups and each rule group you create can exist in a particular region or the Global CloudFront Region. So bare this in mind when creating your rule groups, that you set the correct location dependent on your use case.</p>
<p>When creating your rule group, you will need to give it a name in addition to a CloudWatch metric name. Now this metric will be created within CloudWatch and allow you to monitor activity relating to the rule group that you’ve created. It’s helpful from a management perspective to keep your CloudWatch metric name the same name as the rule group. You can then create and add your rules as you normally would, and as I previous explained in this lecture, based upon the rule that you create within your rule group, you will be told how many WC Units the rule group currently has.</p>
<p>However, you must also consider future plans for the rule group if you anticipate that you’ll add more rules to it going forward. So as a result, you must specify the maximum capacity WCUs for the rule group, as once this option is set, it can’t be changed. Once your rule group has been created, this can then be selected from within your web ACL during it’s configuration. So rule priority.</p>
<p>I now want to talk to you about rule priority, and this comes up when creating rules within your web ACL and also, when creating your rule groups. During both of their configurations, the web ACL or rule group, you’ll be asked to verify the rule priorities of the rules that have been added. And this is an important point as rules are executed in the order that they are listed. So be careful to architect this order correctly for your rule base. Typically, these are ordered as shown.</p>
<p>Firstly, your Whitelisted IPs are allowed, you then have your Blacklisted IPs, which are blocked, and then any Bag signatures, which are also blocked. So Whitelisted IP addresses are a list of IP addresses that are allowed to access the resource. If you’re finding that a known customer is getting blocked within your rule base that shouldn’t be, then you could simply add their IP address to the Whitelist and they would then gain access.</p>
<p>Blacklisted IP addresses are a list of IP addresses that are explicity blocked for known reasons and Bad signatures would be rules that relate to attack patterns, such as SQL Injections and Cross-Site Scripting vulnerabilities. When an incoming request comes into WAF, it will be matched against the associated rule groups and rules within the web ACL in the order they appear. As soon as the request matches all of your criteria within a rule, the associated action will be carried out for that rule, regardless if there is another rule further down that would also be a match.</p>
<h1 id="Creating-a-Web-ACL-Demo"><a href="#Creating-a-Web-ACL-Demo" class="headerlink" title="Creating a Web ACL Demo"></a>Creating a Web ACL Demo</h1><p>In this lecture, I want to provide a demonstration showing you how to create the following: an IP set, a rule group and its associated rules, and a web ACL that is associated with a CloudFront distribution. Okay, so I’m in my AWS console and we can find AWS WAF under the Security and Identity and Compliance category here.</p>
<p>So once we get to the WAF dashboard, what I want to do first is to create an IP set, and we can see the IP sets on the left here. So I’m going to create an IP set, and I’m also going to create a rule group. And then I’m going to create my web ACL using the IP set that I’m going to create now within a rule. And then I’m gonna add that rule into the rule groups, and then attach that to the web ACL to a CloudFront distribution. So if we select IP Sets, we can see here that we have no IP sets found. So I haven’t got any created at the moment.</p>
<p>What I need to do is click on Create IP set. And I can give my IP set a name. So I’m just gonna call this MyIPSet. Add an optional description, and also the region in which you want to create this IP set in. I’m just gonna put it in the global CloudFront region, because that’s what I’m going to be associating the web ACL to. Now you can select here IPv4, IPv6. And this is where you enter your IP addresses that you want to be a part of your IP set. So I’m just going to copy in a couple of IP addresses I’ve got here.</p>
<p>So the first one I’ve copied here is a single IP address, and we know that because it has a mask of 32. So that’s just a single IP. But you can also add network ranges as well. So for example, this one underneath is a network range with a subnet mask of &#x2F;24. And for each IP address or network address that you want to add in, you have to add it on a separate line. So once you’ve created your IP set, simply click on Create IP set. Okay, that’s now created in the global CloudFront region. So if we change the location there, we’ll be able to see the IP set in the list. So there it is, MyIPSet.</p>
<p>Now what I want to do is to create a rule group. So if I go across the rule groups, and we can see here that there are currently no rule groups found. So if I click on Create rule group, give this rule group a name. I’m gonna call this MyRuleGroup. Again, an optional description. And it also adds its own CloudWatch metric name as well, which matches the name of the rule group. The region, I’m going to keep it in the global CloudFront region. Click on Next. And this is where we can start adding our rules to the rule group.</p>
<p>So let me add my first rule. So let me call this MyFirstRule. And we have our different types of rules, the regular rule or the rate-based rule. I’m gonna stick with the regular rule. So let’s start building the rule. So if a request matches a statement, or we can have an and statement here, where it matches all the statements, or an or statement, or a not. Let’s go for an or. So for the first statement, I’m going to say if a request originates from a country in the United States or the United Kingdom, you can see this added them in here, using the source IP address to determine the country of origin, or, and this is where the second statement comes in, we can inspect the originating IP address, and this is where we can select our IP set that we created just now.</p>
<p>Again, using the source IP address as the originating address. Then as an action, I want to block that. So let’s take a look at this rule. So we have a regular rule where if a request matches at least one of the statements, so either that the source IP address originates from the UK or the US or the IP address matches one of those in the IP set that we created, then block the traffic. So let’s add that rule. Okay, so we can see it there, MyFirstRule.</p>
<p>Let’s add another rule. Let’s call this MySecondRule. Again, I’m gonna add a regular rule. This time if a request matches the statement, so I’m not gonna use an and or or. And for the inspection type, I’m going to say HTTP method. And match type, if it contains a SQL injection attack. So if the request matches a SQL injection attack, then I also want to block that traffic. So Add rule. So now I have two rules here, NyFirstRule and MySecondRule.</p>
<p>The first rule relates to the country of origin and my IP set, and the second rule relates to any SQL injection attacks. And we can see here that the capacity has been identified as two for the first rule and 20 for the second rule. So the minimum required capacity is 22, but I can enter the maximum capacity up to 1500 for this rule group. So if you envisage you’re going to add more rules to this at a later stage, then you should increase this capacity. So I’m just gonna change that to 500. And this would give me plenty of allowance to add additional rules or modify the rules that might increase the capacity limit of this rule group.</p>
<p>Click on Next. And here you can change the rule priority. So you can move it up or down depending on how many rules you have. I’m just gonna leave it as what we had. Click on Next. And then here is a quick review of our rule group. So we have the rule group name, and we also have the rules that we created and the actions. So Create rule group. And there we have it. We can see MyRuleGroup. So now we’ve created the IP set. We’ve created a rule group, which contains two rules, and one of those rules contains the IP set that we created.</p>
<p>Now we need to attach this rule group to a web ACL. So if we go across to Web ACLs, again, we don’t have any created at the moment, say Create web ICL. Give this a name. I’m gonna call it MyWebACL, add an optional description. Again, CloudWatch will create an automatic metric for this web ACL. And then we can select our resource type if we want it associated with the CloudFront distribution or an application load balancer, APIGateway, or AppSync, et cetera. But I’m gonna associate this to a CloudFront distribution in the global CloudFront region.</p>
<p>So down here, where it says Associated AWS resources, I’m gonna add a resource. I’m gonna select my CloudFront distribution. So this web ACL will now be associated with this CloudFront distribution. Click on Next. Now here we can add any rules, so we can just add a rule from here, or we can add managed rule groups or add my own rules and rule groups. So as we created our rule group earlier, I want to add that in here. So if we go across to Rule group. We’ll give this rule a name within the web ACL, MyRules, select the rule group, and we have the MyRuleGroup option that we had here, and then click on Add rule.</p>
<p>So we’ve just added a rule within this web ACL, which is associated to the rule group. Now here we can see it’s picked up the maximum capacity of that rule group of 500. So it will take up 500 WCUs of the maximum 1500 allowed for the web ACL. Even though it’s only using 22, it will take the maximum. So just be aware of that when creating your rule groups. And then we also have a default web ACL action for requests that don’t match any rules.</p>
<p>So in this demonstration, I’m just going to allow everything through that isn’t picked up by any of my rules. So effectively what I’m saying there is, is that if any traffic comes from any other country other than the UK or the US, or sits outside of the IP address ranges that I specified in my IP set, and isn’t a SQL injection attack, then I’m happy for that traffic to come through. Click on Next. Again, we can set the rule priority. </p>
<p>Click on Next. You can change the CloudWatch metric name of the rule that you just added if you want to. And you also have the option of running some sample options here as well on your web ACL. I’m just going to leave it as default. Click on Next. And this is where we can review the details from the web ACL that we’ve just created. So it shows the name, the scope, which is CloudFront, the region, and the CloudWatch metrics, the WCU capacity of your rules in your web ACL, and the default action as well.</p>
<p>So once you are happy with everything, just click on Create web ACL. And there we have it. So that’s a very quick demonstration on how to create an IP set, how to create a rule within a rule group, using the IP sets as well. And then also how to create a web ACL associated to a CloudFront distribution using your own rule groups.</p>
<h1 id="AWS-Firewall-Manager-and-Prerequisites"><a href="#AWS-Firewall-Manager-and-Prerequisites" class="headerlink" title="AWS Firewall Manager and Prerequisites"></a>AWS Firewall Manager and Prerequisites</h1><p>Hello, and welcome to this lecture, where I should provide an overview of AWS Firewall Manager, so, you can understand what the service is used for. The core function of AWS Firewall Manager is to help you simplify the management of being able to provide security protection to a range of different resources, between multiple AWS accounts. It’s the fact that it works across multiple account infrastructure, that gives this service a lot of power from a security perspective. So, it’s a great tool to become familiar with, if you are responsible for security across more than one AWS account.</p>
<p>Once your configured security policies to govern the protections that you require for your resources, AWS Firewall Manager, will then automatically apply this protection in addition to managing this protection for any newly creative resources, that match your configuration across any of your accounts that it has responsibility for. So, once it’s set up, the management and protection efforts are simplified dramatically, across your entire organization.</p>
<p>The current AWS services and resources that Firewall Manager provides protection for and integrate with, include the following; <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/protecting-web-apps-aws-waf-shield-firewall-manager/introduction/">AWS WAF, AWS Shield Advanced, AWS Network Firewall</a>, VPC Security Groups and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/amazon-route-53/">Amazon Route 53</a> Resolver DNS Firewall. In addition to these resources that are protected, Firewall Manager is also closely integrated with AWS Organizations. In fact, running AWS Organizations is a prerequisite of using Firewall Manager. For those I’m familiar with AWS Organizations, it’s a service which provides a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization.</p>
<p>Let’s look at the prerequisites of Firewall Manager in a little more detail, to allow you to begin using the service. So, the first step is to decide which AWS account will be used as your Firewall Manager Administrator account. And this account will be used to essentially manage your security policies. Next, you must ensure that this account is a part of an AWS Organization. However, the that it joins must be configured with all features enabled, and not just consolidated billing.</p>
<p>When your account has successfully joined an AWS Organization, you must then configure AWS Firewall Manager within that account, as the Firewall Manager Administrator Account. And this administrator account is used to create a manager security policies. To delegate your account as the administrator, open the Firewall Manager Console, select, get started and enter the account number of your AWS account. Once you’ve added your AWS account to an AWS Organization and designated the Firewall Manager administrative account, you’ll see confirmation ticked on the Firewall Manager dashboard as seen to reflect that you have met these prerequisites.</p>
<p>Next, you must enable AWS config for your account, and for any other account in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/securing-aws-organizations-with-service-control-policies-scps/introduction/">AWS Organization</a> that you want to manage resource security for. And it must be enabled for each region in that account, in which the resources reside. If you don’t want to enable AWS conflict for all resources in each of your accounts, then you must ensure that you enable the following depending on which resources you want Firewall Manager to secure. The next step is optional, depending on if you are looking to apply security policies for all Network Firewalls and DNS Firewalls.</p>
<p>Then you must enable sharing with AWS Organizations in AWS Resource Access Manager. By doing so, it allows you to deploy security policies to these resource types, using Firewall Manager across your accounts in your organization. To complete this configuration, you must open the settings page in the AWS Resource Access Manager Console, and then from here, select, enable sharing with AWS Organizations, and then select, safe settings.</p>
<p>The final step allows Firewall Manager to manage resources in regions, that might be disabled by default. So, you must enable these regions before you can create and managed resources within them. These regions must being enabled in the AWS management account, for your AWS Organization, in addition to the AWS account designated as your Firewall Administrator account. Enabling a region is a simple process. From within the AWS Management Console, navigate to the top right corner and select your account, and then select my account, scroll down to regions section and select, enable in the action column, for the regions that you would like to enable. Once you’ve completed these initial steps you are ready to begin configuring AWS Firewall Manager and its policies.</p>
<h1 id="Policies"><a href="#Policies" class="headerlink" title="Policies"></a>Policies</h1><p>In this lecture I want to dive into the configuration of AWS Firewall Manager so you can see how to create policies that are used to manage and secure your resources across multiple accounts. Before we look at some of this configurations in detail, let me talk a little more about these policies. For each type of resource that you want to protect, there is a different policy, each with a slightly different configuration. Also, note that you can create more than one policy for the same resource type which might be useful depending on your use case.</p>
<p>At the time of writing this course, Firewall Manager allows you to create policies protecting the following resources. An AWS WAF Policy, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/protecting-web-apps-common-exploits-using-aws-waf-1883/an-overview-of-aws-waf/">AWS WAF</a> is a service that helps to prevent websites or web applications from being maliciously attacked by common web attack patterns, and it uses Web ACL’s as the main building block of the service to determine which web requests are considered safe and which ones are not. So by using this WAF policy you can create a set of Firewall Manager Rule Groups to run at the beginning and the end of your Web ACLs that you have configured.</p>
<p>The rules or rule groups that run in between these two can be configured as required from within WAF itself. Shield Advanced Policy, the AWS Shield services is designed to help protect your infrastructure against distributed denial of service attacks, commonly known as DDoS. This policy allows you to apply Shield advanced protection across your accounts and resources. Network Firewall Policy, AWS Network Firewalls allow you to protect your VPCs from common network threats by implementing fine-grained firewall rules enabling you to control which traffic is permitted and which should be blocked. Using this policy allows you to manage these firewall rules across your VPCs running in multiple AWS accounts.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-what-vpc/">Amazon VPC</a> Security Group Policy, security groups are used to control traffic at the instance level based upon port and protocol types. By using this policy, it allows you to manage all of your security groups across your entire AWS organization, giving you centralized control. An Amazon Route 53 Resolver DNS Firewall Policy. This resolver DNS Firewall is a managed firewall that allows you to block DNS queries made against known malicious domains, in addition to allowing queries for your trusted domains. As a result, this Policy enables you to control these Route 53 resolver DNS Firewall protections across all the VPCs in your AWS organization.</p>
<p>So now we have more of an understanding on what each of the policies are being used for, let’s examine some of them in a bit more detail as to their configuration. The creation of each policy type is generally a five step process, apart from the Network Firewall Policy which contains an extra step.</p>
<p>So step one, you must choose your policy and region. In this step you must select which policy you’d like in addition to the region. Step two, describe Policy. So here you need to define the details of the policy, which is dependent on which policy you selected. Step three, define the policy scope. So this step defines which resources and accounts are covered by the policy that you’re creating. Step four, configure policy tags. This is an optional step allowing you to associate a resource tag to the policy. Step five, review and create policy. So this is the final step allowing you to review the configuration you made in the previous four steps before creating the policy. However, do bear in mind that generally the costs of Firewall Manager policies are charged at $100 per policy for each region. For the latest information on Firewall Manager pricing, please see the AWS documentation here to avoid any unexpected costs.</p>
<p>Also, another point when it comes to charges for Firewall Manager policies is that, for each policy you create it will also create <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-config/what-is-aws-config/">AWS Config</a> rules, and in turn these rules will also incur additional charges. For more information on AWS Config pricing, please see the following URL. Okay, so let’s quickly review a couple of these policies to see how they differ, starting with the AWS WAF Policy. So to create a policy from within the AWS Management Console you need to select “Security policies” from the AWS Firewall Manager dashboard and then choose “Create Policy”.</p>
<p>So step one, this is where you choose policy and region. From here you must select which policy you’d like in addition to the region. For this walkthrough, I’m going to select AWS WAF and the Global region. When you have selected the options you require, click next. So step two, describe the policy. In this step we need to define the policy name, policy rules and policy action. But firstly we need to name our policy something appropriate for the use case.</p>
<p>Next we need to configure the Rule Groups of the Web ACL that will be associated with all resources defined in the policy scope. So effectively, at this stage, the Web ACL that you create in this policy will contain a list of Rule groups that will appear at the beginning, in addition to a list of Rule groups that will appear at the end. This Web ACL will then be replicated to all accounts and resources in scope. And the administrator of each of those accounts can then add their own rules and rule groups between the start and end rule groups already defined by this policy. Also as a part of configuring the policy rules, we must select the default action of the Web ACL, either Allow or Block, and if we want to enable the capture of logging information to Kinesis Firehose.</p>
<p>Lastly, we need to indicate a policy action. So we can either use the policy to detect resources that don’t comply with the policy rules without any remediation, or use the policy to automatically remediate any noncompliant resources. Step three, define the policy scope. As we move into the 3rd step, we are able to drill down into the scope, defining which resources and accounts are covered by the policy. At the account level we can apply the policy to all accounts in the AWS organization. We can include selected accounts and organization units or we can exclude selected accounts and organization units, including all others.</p>
<p>From a resource perspective, in the WAF policy you can select a CloudFront Distribution as the resource and you can choose if you want the policy to include all resources that match the resource type, and in this instance that will be CloudFront Distributions. Include only resources that have a specific tag assigned to them. Or exclude resources that have a specific tag, and include all others. Additionally when defining the policy scope, we can choose if we want Firewall Manager to clean up resources by removing protections from resources if they leave the scope of the policy.</p>
<p>For example, you might have selected to include resources that have a resource tag of TeamA, if that resource then had it’s tag changed from TeamA to TeamB, then Firewall Manager could remove the protection offered on that resource automatically as it no longer aligned with the required resource tags. Step four, configure policy tags. This step simply allows you to assign a resource tag to the policy itself using a key value pair and this is an optional step. And then step five, review and create policy.</p>
<p>This final step allows you to review all of your options from steps one to four, before confirming you want to create the policy. However, do bear in mind the associated cost before creating your policy, which I will reiterate are generally charged at $100 per policy for each region. Now before we look at the other policies, step one, step four and step five are generally all the same for every policy. So for the remaining policies, I shall just be looking at step two and three.</p>
<p>So let’s jump straight to step two for the AWS Shield advanced policy, which allows you to manage Distributed Denial of Service, DDoS protections for your applications. So step two for describing the policy. When compared to the AWS WAF policy that we just looked at, there aren’t really many options here for AWS Shield. All we need to do here is to give the policy a name and then define a policy action. Either use the policy to detect resources that don’t comply with the policy rules without any remediation, or use the policy to automatically remediate any noncompliant resources.</p>
<p>And then step three, defining the policy scope. This step is exactly the same as what we covered previously when we looked at AWS WAF. We configure which accounts and resources that this policy applies to by selecting the appropriate options. Again, this relates to CloudFront distributions as Firewall Manager doesn’t currently support Amazon Route 53 or the AWS Global Accelerator. So if you want to protect either of resources with this service, then you will need to manage this directly from within AWS Shield.</p>
<p>Okay, let’s take a look at one more policy before we move on, so the next policy I want to show is the Security Group Policy. Now interestingly, this option has an additional step of configuration options during step one. In addition to selecting the policy detail of Security Group and the Region, you will also be asked to specify the Security Group Policy Type. As you can see, there are three options for this. Common security groups, auditing and enforcement of security group rules. And auditing and clean up unused and redundant security groups.</p>
<p>So your selection will depend on the use case in which you would like Firewall Manager to manage your security groups. If you want to add new security groups to your Organization, select common security groups. If you want to enforce specific rules in existing security groups, choose the 2nd option and lastly, if you want to remove redundant security groups then select the last option. So let’s look at the configuration of adding new Security Groups to your organization. So with that in mind, let’s move onto step two.</p>
<p>Now in this step there are three elements we need to configure. Policy name, policy rules and policy action. As with any policy, we need to issue the policy with a relevant name. When it comes to configuring the Policy Rules, you will need to ensure that you have already created a security group within your VPC. Once you have created the security group that you want to apply across your organization, you can then add it to the policy rule. In this example, you can see that I have created a security group called, MyFirewallSG, which I have now added to this policy.</p>
<p>Finally, select your option for the Policy Action. Again, this is where we can define the scope of which accounts and resources this policy is associated. The main difference on this screen from previous policies we’ve looked at is the Resource Type. Here you can specify which type of resource, whether that be an EC2 instance, an ENI, an ALB or a Classic load balancer, all of which can be associated with a security group. So again, it depends on what you are using your Security Groups for and for which resource you want to protect.</p>
<p>Also, at the bottom of this screen shot, you can also see that by enabling the checkbox, you can apply the policy to resources against shared VPCs too. So compared with the WAF and Shield policies, there is a wider variety of options when it comes to resource type, and its simply because security groups have the capability of being associated with a variety of different resources within your organization. So we’ve looked at some of the policies that Firewall Manager can create for you, allowing you to govern and manage access for a number of different resources across your organization.</p>
<p>It really helps if you have a working knowledge of these resources and services before you begin trying to create policies to offer protection for them. As we’ve seen, in some circumstances you need to have created certain resources and elements prior to creating a firewall manager policy. Once you have created the policies that you need, AWS Firewall Manager will deploy these within the accounts and against the scope of resources we have identified.</p>
<p>Any new resources that are created from this point will automatically fall under the protection of these same policies as long as they fit within the scope outlined of the policy. This automatic protection reduces the amount of administration required in configuring and setting up protection on an individual basis.</p>
<h1 id="What-is-AWS-Shield"><a href="#What-is-AWS-Shield" class="headerlink" title="What is AWS Shield?"></a>What is AWS Shield?</h1><p>Hello and welcome to this section of the course focusing on the third and final service, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Shield. AWS Shield is closely related to both AWS WAF and also the AWS Firewall Manager. So what is it used for? Well, AWS Shield has been designed to help protect your infrastructure against distributed denial of service attacks, commonly known as DDoS. These attacks are very common, and the attack itself targets a host which might be running a website or web application, and it receives a huge number of requests simultaneously sent maliciously by an attacker from multiple distributed sources. This increase and flood of traffic aims to prevent legitimate requests getting through to host and being processed, while at the same time severely hindering the performance of the application or website. So much so in fact, that users often think the site is down. </p>
<p>There are a number of different types of DDoS takes that can take place, for example, a SYN Flood. In a Syn Flood attack, a large number of connections are made to the host under attack. The host will then respond accordingly with an SYN&#x2F;ACK packet, at which point the client sending the original connection request would normally respond with another SYN, completing the three-way handshake to allow communications to begin. However, this final SYN packet is not sent to the host, and this leaves a huge number of open connections on the host, resulting in diminished resources available to process legitimate requests. DNS Query Flood. By using multiple DNS queries an attacker can drain the resources against a DNS server, such as Route 53 in AWS. HTTP flood and cache-busting attacks. These attacks operate at layer seven, the application layer. And during an HTTP flood attack an attacker sends a large amount of HTTP requests, which may include POST and GET requests to a host, consuming the resources available. Cache-busting attacks are similar to HTTP floods, however, by using the HTTP request query string they are able to force content to be retrieved from the originating server, rather than from an edge location, which impacts the performance of the source servers available resources unnecessarily. AWS Shield itself is available at two different levels of features, AWS Shield Standard and AWS Shield Advanced, and AWS shield advanced has a lot more power and protection on offer than standard. AWS Shield Standard is free to everyone, well, at least anyone who has an AWS account, and it offers DDoS protection against some of the more common layer three, the network layer, and layer four, transport layer, DDoS attacks. This protection is integrated with both CloudFront and Route 53. </p>
<p>AWS Shield advanced offers a greater level of protection for DDoS attacks across a wider scope of AWS services for an additional cost. This advanced level offers protection against your web applications running on EC2, CloudFront, ELB and also Route 53. In addition to these additional resource types being protected, there are enhanced levels of DDoS protection offered compared to that of Standard. And you will also have access to a 24-by-seven specialized DDoS response team at AWS, known as DRT. With these additional features, the advanced level also provides an enhanced monitoring capability allowing you to view real-time metrics of any attacks against your resources. Whereas the Standard version of Shield offered protection against layer three and layer four, Advanced also offers protection against layer seven, application, attacks. Another great advantage is the fact that you also get cost protection as a part of the plan, whereby your resources may scale suddenly and unexpectedly to cope with the rise in traffic. From a cost perspective, if your decide to go with AWS Shield Advanced then you also get AWS WAF included in the same price, and this price is currently $3,000 a month, plus data transfer fees. As you can see from this image, there are a significant amount of advantages with the Advanced version of AWS Shield over Standard. That now brings me to the end of this lecture.</p>
<h1 id="Configuring-Shield"><a href="#Configuring-Shield" class="headerlink" title="Configuring Shield"></a>Configuring Shield</h1><p>Hello and welcome to this lecture where I want to cover how to configure and set up AWS Shield Advanced. There are a number of different steps involved if you want to make use of the benefits and features discussed in the previous lecture. Let’s take a look at them. Firstly, we need to activate AWS Shield advanced, which can be done via the Management Console using the WAF and Shield Service. When you go into the service, you are presented with the dashboard, you can simply select Summary from the AWS Shield menu on the left-hand side of the dashboard. This will then present you with a screen which we also saw in the previous lecture as shown. At the bottom of the screen, you can see in blue, a button that says Activate AWS Shield Advanced. It’s worth noting that AWS Shield is AWS account specific, so you will need to perform this step on each AWS account that you want to use it within. You must then accept a number of terms and conditions before you commit to activating the service. Once you have activated AWS Shield with your AWS account, you are then ready to define which resources you want to protect with the service. This is a manual process and is not done automatically. You may think Shield self-discovers resources, however, you need to manually select the resources needing protection. You can select the resources using ARNs providing the resource is within the same account, so simply select all supported resources from a dropdown list. If one of your resource is an EC2 instance, then you must first associate an EIP, an Elastic IP Address, to that instance for it to be protected as AWS Shield protects whatever resource is associated to that EIP. Once your resources are defined and selected, you must then add rate-based rules. Having these configured, it can be a primary indicator that a DDoS attack is in progress. </p>
<p>You may remember from a previous lecture in this course that a rate-based rule counts the number of requests that are being received from a particular IP address over a time period of five minutes. If there is a surge in requests from a particular IP address out of the ordinary, then these rate-based rules can alert you of this behavior. These rate-based rules are only associated with CloudFront distributions and application load balancers and so are not required or available for other resource types, such as EC2 instances. For each supported resource in the list, it is recommend you associate a Web ACL with a rate-based rule. If you have any resources already in the list being protected by a Web ACL, then you can’t change that Web ACL for that resource. If you want it, you must first remove it from the Web ACL within WAF before associating it to a new one. Following your rate-based rule configuration of your resources you then have the opportunity to pre-authorize the AWS DDoS Response team, the DRT, to have the ability to review, update and modify your Web ACLs and Shield configurations during an attack to help you resolve your issues quickly and effectively. If you are not happy to authorize the DRT team to access your resources, then you can select the option of Do not grant the DRT access to my account. If you decide you do want the assistance of the DRT team, you must be subscribed to either the business or enterprise support plans. The authorization process is governed by an IAM role where you can either create a new or select an existing role. </p>
<p>Creating a new role will set up all the relevant permissions automatically. If you wish to select an existing role, you must ensure that it has the AWSShieldDRTAccessPolicy managed policy attached and that you trust the service principal of drt.shield.amazonaws.com to use that role. If you need more information relating to IAM policies and permissions, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">here</a>. It’s also possible to allow the DRT team to access flow log information stored in an S3 bucket, you just need to supply the name of the bucket and the DRT team will be given permissions of GetBucketLocation, GetObject, and ListBucket to review the flow log information. Following this step, it’s recommended that you set up some CloudWatch alarms and use the SNS service to notify you about your resources. AWS Shield will configure the SNS topic for each region specified and it will also configure CloudWatch metrics to notify you of any potential DDoS activity. It’s also possible to configure a CloudWatch Dashboard of the data collected by Shield Advanced. To learn more about CloudWatch and Dashboards, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/">here</a>. Once this is done, then your configuration of AWS Shield Advanced is then complete. However, it’s also worth noting and viewing the Global Threat Environment Dashboard which can help provide an overview of the top attacks, and the number of attacks across the AWS landscape.</p>
<h1 id="AWS-Security-Hub-Features"><a href="#AWS-Security-Hub-Features" class="headerlink" title="AWS Security Hub Features"></a>AWS Security Hub Features</h1><p>AWS Security Hub allows you to start consolidating security findings and alerts across accounts and provider products and display results into a single dashboard. It gives you a way to centrally view and manage security alerts and automate security checks to quickly understand the situation and respond to security issues as they take place. AWS Security Hub integrates with multiple AWS services like Amazon GuardDuty, Amazon Inspector and Amazon Macie.</p>
<p>There are more services that integrate with Security Hub including third party products. The following services are able to send findings to Security Hub, and they are: AWS Firewall Manager, Amazon GuardDuty, Identity and Access Management Access Analyzer, Amazon Inspector, Amazon Macie, AWS Systems Manager Patch Manager, AWS Systems Manager Explorer and OpsCenter, and Amazon Detective.</p>
<p>The Amazon Detective integration allows you to switch back and forth from Security Hub to Detective and investigate a security finding. Security Hub is also able to integrate with security solutions by members of the AWS Partner Network. You will want to visit the integrations section in the Security Hub service documentation. As of this recording, there are over 60 integrations available for the Security Hub service from vendors like Symantec, Sumo Logic, Splunk, Atlassian, Slack, ServiceNow, RackSpace, Palo Alto Networks, PagerDuty, CloudStrike and many more. Security Hub runs continuous, account-level configuration and security checks based on AWS best practices and industry standards. It provides the result of these checks as a readiness score, and identifies specific accounts and resources that require attention. The result is reduced effort to collect, prioritize and remediate security findings.</p>
<h1 id="AWS-Security-Hub-Standards-Implemented"><a href="#AWS-Security-Hub-Standards-Implemented" class="headerlink" title="AWS Security Hub Standards Implemented"></a>AWS Security Hub Standards Implemented</h1><p>AWS Security Hub automatically implements three fundamental standards as defined by the Center for Internet Security, or CIS. CIS is helping make the connected world a safer place by publishing guidelines to safeguard public and private organizations against cyber threats. This includes benchmarks for AWS implementations. The reference URL for the Center for Internet Security is <a target="_blank" rel="noopener" href="http://www.cisecurity.org/">www.cisecurity.org</a> Once enabled, Security Hub begins to monitor CIS benchmarks in order to provide you visibility as to what you’re doing right and what needs remediation.</p>
<p>The first standard supported by Security Hub is called the AWS Foundational Security Best Practices Version 1.0. This is a standard defined and curated by AWS security experts and represents the most basic actions you need to take on a new account in order to make sure everything else you build is secure. Details as simple as enabling multifactor authentication for all account users, defining a robust password policy, the use of groups for applying permissions to users and making sure you never have publicly accessible Amazon S3 buckets will ensure a high score on this fundamental benchmark. We will discuss how to get a good score on this set of best practices in the next section.</p>
<p>The second standard supported by AWS Security Hub is the CIS AWS Foundations Benchmark Version 1.2. This is a set of best practices for AWS configuration in terms of security. It includes details related to monitoring and logging, which are essential to identify security issues. AWS Security Hub automatically checks for your compliance with these benchmarks once you enable it.</p>
<p>The third standard supported by Security Hub is the PCI DSS Version 3.2.1. This stands for Payment Card Industry Data Security Standards and it applies to the storing and processing of credit card information in your AWS implementation. AWS Security Hub is your central point of access to verify AWS security and take appropriate action when needed.</p>
<p>AWS Security is to be taken seriously and best practices need to be defined by your organization, implemented and reviewed regularly as the security landscape continues to evolve. The importance of security in your AWS infrastructure cannot be overstated. A fundamental error in your security implementation can potentially compromise and cost your business significant losses in time, effort, and money. Cloud security is complicated to implement. AWS Security Hub helps you manage the complexity of collecting and remediating security issues in your AWS infrastructure.</p>
<h1 id="AWS-Security-Hub-Operation"><a href="#AWS-Security-Hub-Operation" class="headerlink" title="AWS Security Hub Operation"></a>AWS Security Hub Operation</h1><p>In addition to best practices, the information generated by security services is significant in terms of volume and categories. Gathering security findings in a single place is difficult and the information requires close attention and analysis in order to guarantee a secure and compliant environment at all times. This is where AWS Security Hub becomes essential by providing a single point of access to your security implementation and automatically verifying you are applying best practices as defined by AWS security experts. Let’s discuss some of these foundational best practices for new AWS accounts. </p>
<p>Whether you’re just getting started or have some experience already with AWS, there’s no doubt you have to interact with the AWS Identity and Access Management service. This is the service to provision users, define access controls for those users including passwords and access keys. Identity and Access Management is one of the many security services provided by AWS in order to assist you in the provisioning of a secure and compliant implementation.</p>
<p>This diagram represents three best practices regarding AWS Identity and Access Management. As you can see, it seems a little complicated. Let’s break it down step by step into some of the foundational best practices that Security Hub verifies.</p>
<p>Number one, the Root User is the owner of AWS account, and it has access to everything including the credit card used to pay for services. As a best practice, always assign a multifactor authentication to the root user. That way, the root account is protected at all times. After that, create a group and apply administrator permissions to it. Then create a user, also assign an MFA to that user and provision keys if needed. Finally, add the user to the group, sign off the root account, and try never to use it again unless strictly necessary.</p>
<p>Your implementation will begin by leveraging the administrator user that you just created. Groups are an efficiency mechanism to apply permissions. It is easier to define a group, apply permissions, and then add or remove users as needed. You should rarely have to apply permissions to a user directly. If required, you can also provision keys to a user. Finally, also if required, you can allow the user to assume roles. For item number three, roles represent a way to gain permissions on a temporary basis.</p>
<p>The primary use of roles is to allow services to interact with each other. Users can also assume a role if needed. The permissions attributed to a role should be what is needed and nothing in addition to that. This is called the principle of least privilege, sometimes also called the principle of maximum security. Roles are commonly associated with EC2 instances and services. As a general security best practice, never store access keys in your code on EC2 instances or Amazon S3 buckets.</p>
<p>Fortunately for us, AWS offers a variety of tools for automating provisioning, monitoring, and auditing, many of them using machine learning in order to help us get our Cloud Security compliant with best practices. We just implemented some of the foundational security best practices checked by AWS Security Hub. We’re gonna see the results later on.</p>
<h1 id="AWS-Security-Hub-Integrations"><a href="#AWS-Security-Hub-Integrations" class="headerlink" title="AWS Security Hub Integrations"></a>AWS Security Hub Integrations</h1><p>With AWS Security Hub enabled, Amazon GuardDuty, Amazon Inspector and Amazon Macie findings are automatically sent to Security Hub as the single point of access and remediation for security issues. On the first item, we notice that Security Hub uses AWS Config rules to perform most of its security checks for controls. On the second item, we notice AWS Security Hub automatically runs continuous, account-level configuration and security checks based on AWS best practices and industry standards.</p>
<p>On the third item we note, AWS Security Hub supports integration with Amazon EventBridge to automatically send notifications and remediation details of security findings. You can use custom actions to send results to a ticketing system or to an automated remediation service.</p>
<p>Now in this next diagram, things are a little more advanced. It’s important to note that AWS Security Hub findings can display results from AWS services that include GuardDuty, Inspector, Macie, AWS Firewall, AWS Systems Manager Patch Manager and AWS Identity and Access Management Access Analyzer among many others. There’s a total of over a dozen services that AWS Security Hub is able to integrate into a single point of viewing for security findings. I have no doubt, the list will continue to grow as time goes by.</p>
<p>For now, it’s important, especially for beginners, to have a sense of the benefit delivered by the individual services that AWS Security Hub aggregates. Let’s briefly discuss the first three services that integrated with security hub. They are Amazon Inspector, Amazon GuardDuty and Amazon Macie. We’re going to keep things simple, so please don’t worry. We’ll discuss the main benefits and take a look at some sample findings from screens for each of them. We’ll return to our discussion of AWS Security Hub shortly after.</p>
<h1 id="Amazon-Inspector-Basic-Features"><a href="#Amazon-Inspector-Basic-Features" class="headerlink" title="Amazon Inspector Basic Features"></a>Amazon Inspector Basic Features</h1><p>You need to be responsible for the security of applications, processes, and tools that run on EC2 instances. Amazon Inspector lets you analyze your deployed EC2 instances and helps you identify potential security issues. Some of the basic features include a knowledge base with hundreds of rules that are mapped to common security compliance standards and vulnerability definitions. These rules are regularly updated by AWS security experts. You can install an agent in the operating system of an Amazon EC2 instance to monitor behavior like network, file system, and process activity.</p>
<p>You can also automate vulnerability assessments to make security testing of EC2 instances a regular part of your cloud operations. As a result, Amazon Inspector gives you a prioritized list of findings. A sample list of findings is shown. Amazon inspector is all about protecting the security of your EC2 instances.</p>
<h1 id="Amazon-Guard-Duty-Basic-Features"><a href="#Amazon-Guard-Duty-Basic-Features" class="headerlink" title="Amazon Guard Duty Basic Features"></a>Amazon Guard Duty Basic Features</h1><p>Amazon GuardDuty is an intelligent threat detection service that provides you with an accurate way to consistently monitor and protect your AWS accounts and workloads for suspicious activity. We’re talking about intelligent threat identification for your accounts, data, and workflows. It uses trained machine learning models to identify suspicious user and resource behaviors. It also learns from your environment to eliminate false positive identifications.</p>
<p>Amazon GuardDuty is able to analyze CloudTrail logs, VPC flow logs, and DNS query logs to identify issues worth looking into. An interesting item about GuardDuty is that sample findings help you analyze the type of results that GuardDuty delivers. When you generate sample findings, GuardDuty populates your current findings list with one sample finding of each type.</p>
<p>Don’t forget, GuardDuty is able to display its results to AWS Security Hub. Generating sample findings will allow you to verify AWS Security Hub’s functionality sooner more than later. Let’s take a look at some sample results from the GuardDuty screens. As a result, Amazon GuardDuty gives you a listing of findings classified under three categories, low, medium, and high severity.</p>
<p>In this screen of the AWS console, low severity findings are marked in blue with a small circle next to the finding. Medium severity findings are marked by GuardDuty in orange with a small square next to the finding. High severity findings are marked by GuardDuty in red with a small triangle next to the finding. Also notice how, on the top right, you have a findings summary showing the total for each of the severity categories.</p>
<h1 id="Amazon-Macie-Basic-Features"><a href="#Amazon-Macie-Basic-Features" class="headerlink" title="Amazon Macie Basic Features"></a>Amazon Macie Basic Features</h1><p>Amazon Macie uses machine learning to do its work and helps you discover and analyze sensitive data stored in Amazon S3 buckets, including personal identifiable information, or PII, such as names, addresses, credit card numbers, API Keys, and access credentials among many others. Macie scans S3 buckets and recognizes critical private information. It also automatically tracks changes to buckets and only evaluates new or modified objects in future scans. That way, it doesn’t have to review objects that have not changed and makes the discovery job significantly more efficient and scalable. You can run one-time or automated data discovery and display the results to AWS Security Hub.</p>
<p>Amazon Macie provides a list of findings where the severity and finding type are clearly displayed. In this case, we created a bucket called academy-ca-macie and uploaded a file with disabled user keys, an RDS SQL Query, a credit card list in CSV format, and a few other files. Notice the severity as medium or high. Also, notice the finding type for the S3 objects include Personal, Financial, and Credentials. It also points to the resource that is affected. Finally, it shows when the object was last scanned. In the future, unless there is a change, these objects will not be re-evaluated.</p>
<h1 id="Overview-of-Amazon-Cognito"><a href="#Overview-of-Amazon-Cognito" class="headerlink" title="Overview of Amazon Cognito"></a>Overview of Amazon Cognito</h1><p>Amazon Cognito, one of the most annoying parts of building and creating applications, either mobile or web, is dealing with user authentication. Being able to determine who is and who is not allowed to operate specific services or aspects in application is extremely important. However, it can be a tedious and time consuming operation to set up.</p>
<p>In the past, all of that important user information would have been stored in a garden variety user database. This database might’ve been hosted onsite or even in the cloud. Either way, it was probably a relational database holding tables of usernames with associated permissions.</p>
<p>When working in AWS land, this means the database would either be hosted on RDS, the Relational Database Service, or by running your own database on EC2 instances. The trouble with using either of these methods is it requires a lot of work to get everything set up and maintain the system. We also have the familiar scenario of people working in the corporate environment.</p>
<p>All of their information is already stored in a directory service like Microsoft Active Directory and we don’t want them to have to create yet another login and password for our new custom application. We would prefer that they could sign in with their day-to-day corporate username and password. Well, all these pain points can be resolved by using Amazon Cognito, a fairly small service that can do quite a lot of heavy lifting.</p>
<h1 id="The-Basics-of-Cognito"><a href="#The-Basics-of-Cognito" class="headerlink" title="The Basics of Cognito"></a>The Basics of Cognito</h1><p>The Basics, at its core, Amazon Cognito is an authentication and user management service. It has a strong integration with third-party identity providers such as Apple, Facebook, Google, and Amazon. It also allows you to federate identities from your own active directory services so that your AD users can have access to your own external web and mobile applications. Amazon Cognito’s features can easily be broken down into two different topics, Amazon Cognito User Pools and Amazon Cognito Identity Pools. Everything else that you can do with the service is derived from these two things. So let’s take a look at Amazon Cognito User Pools first and then we can take a look at Identity Pools after.</p>
<h1 id="User-Pools"><a href="#User-Pools" class="headerlink" title="User Pools"></a>User Pools</h1><p>The primary concern of Cognito User Pools is to create and maintain a directory of your users for your mobile or web applications. This means dealing with both signing up, and signing in, your new and returning users. </p>
<p>When signing up new users, Cognito lets you customize what is important for you and your application. And there is a lot of information you can have your perspective users submit when they are signing up.</p>
<p>If you want all of your new user to submit their emails, addresses, pictures, what have you - you can set that all up while creating your user pool. </p>
<p>You also can create custom attributes if you need something specific from your users. A custom attribute can be a string or a number and allows you to set minimum and maximum values you will accept.</p>
<p>This information is all stored within the Cognito User pool and can be accessed by your application when you need it.</p>
<p>You can also specify how stringent you want to be with your passwords that users can create. </p>
<p>Cognito gives you all the normal password functionality like min length, requiring numbers, special characters, upper and lowercase. Yadda yadda.</p>
<p>Amazon Cognito also has the functionality to require multi-factor authentication(MFA), which I would recommend for any financial services, or high-value information like medical, credit card, or anything really with in-app purchases that a user might have invested a significant amount of money towards.</p>
<p>The service even includes account recovery features that can be quite annoying to set up on your own, and would normally require another backend service to handle for you: Including email and phone.</p>
<p>Additionally, if you don’t want your users to have to go through all that hassle, Cognito User Pools gives you the ability to have social sign-in. This means your users can also sign in for your application by using third-party ID providers.</p>
<p>This path does require you (the application developer) to first set up a developer account with those external third-party providers and set up your application with them. It’s not a particularly difficult task, but it can be time-consuming.</p>
<p>Take a look over here for a step by step walkthrough of the process: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-social-idp.html">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-social-idp.html</a></p>
<p>Finally, You can also sign in with any SAML (Security Assertion Markup Language) identity provider. In case you are unaware of SAML, It is a XML-based markup language for security assertions. It is an important tool for single sign-on through the web. For example, your SAML ID provider might be an active directory federation service. This provider could be your on-premises AD or one you are even hosting on an EC2 server.</p>
<p>Please be aware that if you do use this route you will need a domain name that you own.</p>
<p>The service also provides a way to create your own customizable web UI to handle the Sign in and sign up services. Using this customizable UI provides you with an OAuth 2.0 compliant authorization server. OAuth is an open-standard authorization protocol that helps regulate how servers or services can safely allow authenticated access to each other, without sharing SSO credentials.</p>
<p>The user experience of the provided web UI is customizable and allows you to add your own brand logos and to change the look and feel of the webpage.</p>
<p>You don’t have to use this of course and can create your own UI. You would just in charge of calling the appropriate API calls for the service yourself, and running your own OAuth server - which can be difficult for some people, and might be out of scope for what you are trying to achieve.</p>
<p>User pools also have integrations with AWS lambda and gives you the option to trigger functions based on user flow.</p>
<p>For example, if you wanted a lambda function to trigger right after a user has successfully signed up - maybe to send an email, or to create some backend functionality for that user, you have that ability. Or whenever someone successfully signs in, you could have lambda check some backend information about that user, and prepare their environment based on that.</p>
<p>As a final note: you can also add an entire listing of users and accounts via CSV file if you already have that information available.</p>
<p>I think the big thing to notice about this service in general, is that it is just trying to remove all of the extra obstacles out of the way and let you just get into developing your application.</p>
<p>There are plenty of steps required to make even simple sign and authentication work with modern applications, so to have all of these extraneous things dealt with for you is super powerful.</p>
<h1 id="User-Pools-Authentication-Flow"><a href="#User-Pools-Authentication-Flow" class="headerlink" title="User Pools Authentication Flow"></a>User Pools Authentication Flow</h1><p>Now that we know a little bit more about User pools, let’s quickly examine how the authentication flow is handled by your application and the service.</p>
<p>Your user will be presented with a login screen or terminal of some sort from within your application. They will submit their credentials, either from having created an account directly or by logging into a third-party provider. </p>
<p>The application will call the <strong>InitiateAuth</strong> operation with those credentials. This API call kicks off the authentication flow. It directly indicates to amazon Cognito that you want to authenticate. </p>
<p>If the call is successful - Cognito will respond either with a token or with a challenge.</p>
<p>A challenge can include CAPTCHAs or Dynamic challenge questions. These are normally used to help screen for bots. You can insert your own custom challenges if you wish. This will be sent back to the client and it now becomes <em>Their problem</em>.</p>
<p>When the client is ready to respond back to the server(cognito), they can reply with <strong>RespondToAuthChallenge</strong> and provide whatever information the challenge requires back. If the user fails the challenge, you can have Cognito set up to resend a new one. This can include multiple rounds until the user is successful or fails out.</p>
<p>If successful Cognito will shoot back some tokens for the client to use - Hurray authentication!</p>
<p>For a deeper dive into the process please take a look over <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/mobile/customizing-your-user-pool-authentication-flow/">here</a>.</p>
<h1 id="Identity-Pools"><a href="#Identity-Pools" class="headerlink" title="Identity Pools"></a>Identity Pools</h1><p>The Amazon Cognito Identity pools - also known as Federated Identities, Helps to provide temporary access AWS Credentials for your users or guests that need access to AWS services.</p>
<p>Identity pools can work in tandem with Amazon Cognito user pools, allowing your users to operate and access whatever specific feature they need from AWS. </p>
<p>Additionally, just like with User pools, you can federate with public providers such as Amazon, Facebook, and Google.</p>
<p>The Identity pools help to define two types of identities - Authenticated identities and unauthenticated identities.</p>
<p>Each identity within your identity pool has to be in one of these two states. </p>
<p>To gain the authenticated state, a user must be authenticated by a public login provider. This can be your Amazon Cognito user pool from early, or can also be any of those other public ID providers like Amazon, Apple, Facebook, Google, SAML, and even an Open ID connect provider.</p>
<p>You can also have Unauthenticated identities. This can be useful for a number of reasons, but the primary ones might be for allowing users to see various AWS resources before they are completely logged in. Giving them some visibility into dashboards for example - so they could at a glance see if something was wrong.</p>
<p>You can also use Unauthenticated identities to act as a sort of guest pass for when you want people to have some access to basic services and later prompting them to sign in or sign up.</p>
<p>Each type of identity has a role that goes along with it. Roles have policies attached to them, that set the permissions for what that user is allowed to do within AWS. Roles help to define boundaries and allow you to explicitly state what an authenticated or unauthorized user can, and can not, modify or even see.</p>
<p>If you need a refresher on roles beyond what I’ve just described, please take a look over here for an in-depth look at this feature: </p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/users-groups-and-roles/">https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/users-groups-and-roles/</a></p>
<p>The big thing I want you to think about when differentiating between Identity pools and User pools is that Identity pools are used for authentication and access control ( specifically for AWS services). While user pools are designed for sign-up and sign-in type operations.</p>
<h1 id="Identity-Pools-Authentication-Flow"><a href="#Identity-Pools-Authentication-Flow" class="headerlink" title="Identity Pools Authentication Flow"></a>Identity Pools Authentication Flow</h1><p>Alrighty, now let’s take a look at how authentication works with Identity pools.</p>
<p>The first part we should already be familiar with. Your application needs to have the user sign in with Cognito user pools and that can take place using either the user pool itself, a social sign-in, your SAML-backed authentication service or something of that ilk.</p>
<p>At this stage, an Identity Token is sent by the IDP back to the Cognito user Pool. Now Cognito does not store the credentials by the IDP or forward this to your mobile app, instead, the IDP token will be normalized into a standard token, called a Cognito User Pool Token, or CUP token and this will be used and stored by Cognito. This essentially means that it doesn’t matter if the user authenticated via an account in the User Pool, or federated access, all tokens will be standardized that are sent back to your app.</p>
<p>The actions carried out by the user on your app might also need to access back-end services or APIs that you have created, for example, you might be using API Gateway or Lambda - which accepts these CUP tokens, so Cognito will use the same CUP token to authenticate and authorize you to use those APIs with API Gateway, etc.</p>
<p>Now some services do not allow you to use the CUP tokens for authentication. If your mobile app requires access to services such as S3 or DynamoDB on the user’s behalf, then you will need to use the Identity Pool to authenticate. </p>
<p>The CUP Token can be sent to the Identity Pool, where an STS Token (Security Token Service) will be created based off of your CUP token, and this will be sent back to your application. </p>
<p>With these AWS credentials, your application will now be allowed to call upon those other AWS services.</p>
<p>These credentials will be linked to an AWS role you have associated with your users within the identity pool.</p>
<h1 id="Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale"><a href="#Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale" class="headerlink" title="Using AWS Identity Federation to Simplify Access at Scale"></a>Using AWS Identity Federation to Simplify Access at Scale</h1><p>Hello, my name is Stuart Scott and today I want to speak to you about AWS Identity Federation. I want to explain what it is, some of the AWS services that can be involved in federation and also highlight some scenarios where you might want to implement it.</p>
<p>If you have any feedback on this course, positive or negative, it would be greatly appreciated if you can contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>So, firstly, what is Identity Federation? </p>
<p>It’s basically a method where two different providers can establish a level of trust allowing users to authenticate from one, which authorizes them to access resources in the other. During the federation process, one party would act as an Identity Provider, known as an IdP, and the other would be the Service Provider, an SP.  The identity provider authenticates the user, and the service provider controls access to their service or resources based on IdPs authentication.</p>
<p>You’ve probably all been to websites where it presents you with a log in page, where you can either log in using existing credentials native for that service, or you might have an option to authenticate using credentials from a 3rd party provider, such as Facebook, Google, Twitter or LinkedIn etc. </p>
<p>On the screen here you can see the login page for medium.com you can use existing credentials from identity providers including Google, Facebook, Apple, Twitter and Gmail to create a new Medium account, or log back in to Medium if you’ve already created an account. In this scenario, Medium would be the Service Provider.</p>
<p>So if you didn’t have an account with Medium already, you could select one of these providers to create one. So as an example if you already had a Google account and were signed in with it, and selected ‘Sign in with Google’ you’d be presented with the following screen</p>
<p>Now here Medium has detected that I didn’t have a Medium account associated with my current Google Account, you can see it says “Sorry, we didn’t recognize that account’, and so you are asked if you would like to create one. By selecting Sign up with Google, Medium authenticates my access via my Google credentials, and so trusting the Google IdP, and this enables me to create a new Medium account based off of that authentication. </p>
<p>So in this example, Google was used as the Identity Provider to authenticate you to Medium with a new account. The next time I then need to log in to Medium, all I would have to do is select ‘Sign in with Google’ and as long as I were logged in to my Google account already, I would be logged straight into Medium with 1-click, and this directly correlates to the term ‘Single-Sign on’, or SSO. SSO relates to the method of signing in to one system, which can then be used to authenticate you into another without having to resupply additional credentials. </p>
<p>Now, the process behind this is fairly simple, when the identity provider has been selected, and the user is authenticated by that provider, an assertion is sent back to the service provider, in this case Medium. And this assertion is a message that contains metadata and attributes about the user such as their username. This assertion allows the service provider to authorize access to their services.**<br>**</p>
<p>So identity federation provides a great way to easily set up access control systems with flexibility and ease for the users and service providers. </p>
<p>So using this same principle AWS has a variety of services and methods that allows you to access your AWS services via federated access, meaning you don’t need to have a specific identity &amp; access Management user configured in AWS if you already have a user directory that is managed elsewhere that could be used as an Identity Provider. </p>
<p>Using many different identity standards, including OAuth 2.0, OIDC, which is OpenID Connect, and SAML 2.0, (Security Assertion Markup Language), AWS enables you to easily configure access for contractors and other 3rd parties, in addition to allowing scalable access to authenticate to your mobile and web applications.</p>
<p>Let’s take a quick look at the definitions of these standards according to Wikipedia:</p>
<p><strong>“OAuth</strong> <em>is an open standard for access delegation, commonly used as a way for Internet users to grant websites or applications access to their information on other websites but without giving them the passwords”</em> - Wikipedia</p>
<p><strong>“OpenID Connect</strong> <em>is a simple identity layer on top of the OAuth 2.0 protocol, which allows computing clients to verify the identity of an end-user based on the authentication performed by an authorization server, as well as to obtain basic profile information about the end-user.”</em> - Wikipedia</p>
<p><em>“</em><strong>Security Assertion Markup Language 2.0</strong> <em>(<em><strong>SAML 2.0</strong></em>) is a standard for exchanging authentication and authorization identities between security domains. SAML 2.0 is an XML-based protocol that uses security tokens containing assertions to pass information about a principal (usually an end user) between a SAML authority, named an Identity Provider, and a SAML consumer, named a Service Provider.” -</em> Wikipedia</p>
<p>Now we have more of an understanding of federation, I want to take a high level look at some of the services offered by AWS and how they fit into AWS Identity Federation, including</p>
<ul>
<li>AWS Single Sign-On, known as SSO</li>
<li>AWS Identity &amp; Access Management (IAM)</li>
<li>And Amazon Cognito</li>
</ul>
<p>Now, by the name alone we can safely assume that this has something to do with federated access. </p>
<p>This service has primarily been designed for users to easily access multiple accounts within their AWS Organization enabling a single sign-on approach negating the need to provide credentials for each account. For those unfamiliar with AWS Organizations, they provide a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization, helping to maintain your AWS environment from a security, compliance, and account management perspective.</p>
<p>You can either use the AWS SSO own built in user directory which allows you to create unique users based on e-mail addresses, or alternatively connect AWS SSO to one of the supporting identity providers, such as your own corporate MS Active Directory if you are using one. </p>
<p>Either way, AWS SSO enables you to create different groups while leveraging the power of IAM roles and permissions allowing you to control what access users or groups have in specific AWS Accounts within your organization. </p>
<p>When Users have been configured, they can connect via their own portal which will list all AWS accounts they have access to, along with roles they have access to adopt, and if they would like to access the Management console or use temporary credentials for AWS CLI access.</p>
<p>AWS SSO can also be used to manage access to cloud-based apps using SAML 2.0 such as Office 365 and Salesforce. </p>
<p>As I mentioned previously, Identity federation allows you to access and manage AWS resources even if you don’t have a user account within IAM. </p>
<p>Whereas AWS SSO allows you to create a single sign-on approach to multiple accounts in your AWS organization using the in-built user directory or MS-AD, AWS IAM allows you to configure federated access to your AWS accounts and resources using different identity providers for each AWS account. Examples include both SAML 2.0 for enterprise federation using MS-AD, and OpenID Connect, classed as web identity federation, such as Google, PayPal and Amazon. As a result you could allow access to your environment using these Identity Providers instead of setting up users with a new identity within AWS IAM.</p>
<p>The benefits of this is twofold. It minimizes the amount of administration required within IAM and it allows for a single sign-on solution. </p>
<p>As a part of the configuration process to implement federated authentication, a trust relationship between the identity provider and your AWS account is established. AWS supports two types of identity providers, OpenID Connect, also often referred to as web identity federation, and SAML 2.0.</p>
<p>Amazon Cognito has been built purely to enable the simplification of enabling secure authentication and access control for new and existing users accessing your web or mobile applications, rather than your AWS account. It not only integrates well with SAML 2.0 but also web identity federation as well. One of the biggest features of Amazon Cognito is that it has the capability to scale to millions of new users which is great when working with mobile applications.</p>
<p>Cognito also allows you to use a custom portal allowing you to add a personalized sign-in page with branding and your own logo.</p>
<p>There are 2 main components of Amazon Cognito, these being ‘User Pools’ and ‘Identity Pools’ and they perform different actions.</p>
<p>User Pools are essentially a scalable user directory that allows new users to sign up, or existing users to log in to your mobile application using their native credentials from the user pool, or they can alternatively federate their access via a web or enterprise IdP. </p>
<p>Identity Pools are different to user pools in that they actually provide you with the ability of accessing AWS resources called upon by your web or mobile app by using temporary credentials and the Security token service.</p>
<p>So let me summarize the key difference between these 3 AWS options to implement federated access to AWS.</p>
<ul>
<li>AWS SSO allows you to create a Single sign-on approach to access multiple AWS accounts within an AWS Organization using a single identity provider for all.</li>
<li>AWS IAM allows you to configure different OpenID or SAML identity providers for each of your AWS accounts.</li>
<li>Amazon Cognito enables secure authentication to your web or mobile applications using both SAML 2.0 and web identity federation.</li>
</ul>
<p>That brings me to the end of this introductory course covering AWS identity Federation. You should now have a greater understanding of what it is and some of the services that can be used to implement different kinds of federated Access.</p>
<p>If you have any feedback on this course, positive or negative, please send an e-mail to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="Using-AWS-SSO-to-Simplify-Access-Across-Your-AWS-Organization"><a href="#Using-AWS-SSO-to-Simplify-Access-Across-Your-AWS-Organization" class="headerlink" title="Using AWS SSO to Simplify Access Across Your AWS Organization"></a>Using AWS SSO to Simplify Access Across Your AWS Organization</h1><p>Hello, my name is Stuart Scott and today I want to introduce you to AWS SSO and perform a demonstration on how to create and configure a single sign-on portal for users to access multiple AWS accounts within a single AWS Organization without the users having an IAM account. By the end of this course, you will have a greater understanding of the benefits of AWS SSO and how it can be used to simplify user access at scale.</p>
<p>To get the most from this course, it would be beneficial if you have a basic understanding of AWS Organizations as this tightly integrates with the AWS SSO service. If you have any feedback on this course, positive or negative, it would be greatly appreciated if you can contact support at cloudacademy.com.</p>
<p>So what is SSO? For those unfamiliar with the AWS SSO service, let me briefly explain what the service is used for and some of its key features. AWS SSO, which stands for Single Sign-On, is used to help you implement a federated access control system providing a portal to your users allowing them to access multiple accounts within your AWS Organization without having to supply IAM credentials for each one. It can also be used to federated access to cloud applications, such as Microsoft Office 365 and Salesforce.</p>
<p>For those unfamiliar with AWS Organizations, it provides a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization, helping to maintain your AWS environment from a security, compliance, and account management perspective. For more information on this service, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/securing-aws-organizations-with-service-control-policies-scps/introduction/">here</a>.</p>
<p>So some key features of AWS SSO include the following: It’s a highly available managed AWS service. It has personalized portals for your end-users to centrally gain access to your AWS accounts and cloud applications. It comes with full integration with AWS CloudTrail for auditing SSO activity between your accounts. As already highlighted, it integrated with AWS Organizations enabling SSO for multiple accounts. It also has support for Azure Active Directory and Okta Universal Directory. It also supports multi-factor authentication for additional security protection, and it has the ability to use IAM fine-grained controls to manage access to resources.</p>
<p>So as you can see from these features alone it has some great advantages to help you manage and implement a single-sign on approach in your organization. Now there are a number of prerequisites that you must meet before you configure and enable AWS SSO within your accounts. The first of which requires you to configure AWS Organizations for your accounts using the ‘All features’ option rather than just ‘Consolidated billing features’.</p>
<p>You must also use the Management AWS account of your AWS Organization to enable and configure AWS SSO. The configuration and implementation can’t be done using one of your member accounts. And then once you have performed these two simple steps, you then have the decision of deciding which identity pool you want to have AWS SSO use as your source of users.</p>
<p>You can either use an external identity pool supported by AWS SSO with SAML two, such as Azure Active Directory or Okta Universal Directory, or you can choose to manage your users with the default identity user store that comes natively with AWS SSO, and it’s this option that I am going to focusing on today in our demonstration.</p>
<p>Before I start the demonstration, I just want to highlight what I shall be doing. So the aim of this demonstration is to show you how to configure AWS SSO enabling full S3 and RDS access for a user in two different AWS accounts using a customized user portal. The steps required to achieve this will include: Selecting my identity source, and this is where I can select to use the native AWS SSO user directory, or Active Directory, or another supported SAML 2.0 based identity provider. And I shall be using the AWS SSO user directory.</p>
<p>I shall customize the URL of the portal that I want users of AWS SSO to use to gain access to my AWS Accounts I will then add a new user to the AWS SSO directory using an e-mail address to uniquely identify them as a user. I shall create a new group and add the user to that group, and I must also create a permission set defining full S3 and RDS access for the user to inherit. I will then associate this permission set to the new group which will be applied to specific AWS accounts in my Organization. And I will then test access by signing in to the AWS SSO user portal</p>
<p>Okay, so now we have a basic understanding of what we are going to do. Let’s go ahead and try it out!</p>
<p>Okay, so I’m in the AWS management console. So the first thing I need to do is find AWS SSI and that can be found under the security identity and compliance category. So here we have AWS single sign-on. So if I select that, and this will take me to the dashboard.</p>
<p>Now the thing I want to do is to select an identity source. So if I go into the identity source and here we can see that currently the identity source is configured as AWS SSI. Now, if I didn’t want to use the inbuilt identity directory offered by SSI, then I could go in here and change it and select active directory or an external identity provider. But for this demonstration, I’m just going to use the AWS SSO directory itself.</p>
<p>Okay, then further down we can configure the user portal. So this will be the link that everyone will receive, who’s set up on AWS SSO to access the AWS account. And you can customize this to whatever you’d like it to. So let me just click on customize and I’ll just enter my name Stuart Scott and then save. And now my link is customized, making it much more user friendly.</p>
<p>Okay, so I’ve now selected my identity source and also customize my user portal URL. Let’s now go and create a user. So on the left-hand side here, you can select users. At the moment you can see I don’t have any users configure at all. If I click on add user, now I can set up a user name. I’ll just call this Stuart Scott. And for password, I can select to send an email to the user with a password to set them instructions or generate a one-time password that I can then share with the user. I’m just gonna leave it as a default send an email to a user. I’ll put in an email address, then just confirm those details, add in the extra details and that’s it. And then further down this optional other metadata and attributes that you can add if needed, but for now I’m just going to leave it as there, the setup of the user itself.</p>
<p>If I click on next groups, now here, I can create a group for easier management of the older different users that you have. So let’s go ahead and do that. And this helps with permission access as well as we’ll see as we go through the demonstration. So if I select create group and I’ll just call this S3 and RDS access, ‘cause this is what I want the group permissions to have.</p>
<p>So let’s make it nice and easy. If I select create and I can select that group to make sure the user is a part of that group and then say, add user. So that user is now added to our group and we have the user Stuart Scott added to this AWS SSO directory. Now, what I need to do is create permissions for that user. So if we go over to our AWS accounts on the left here, and I can see the different accounts that I have in my AWS organization.</p>
<p>So, like I said, as a prerequisite, when you use AWS SSO, you need to make sure that you have AWS organizations already set up. And these are my two accounts that I have in my organization. Now at the top here, you can see two different tabs, your AWS organization, and also permission sets. So if I select permission sets, at the moment I don’t have any permission sets. So if I create one, and I can use an existing job function policy or create a custom permission set.</p>
<p>For this demonstration, I want to create a custom permission set. Click on next. And I’ll just call this S3 and RDS access again. And you can configure session durations, which will define the length of time a user can be logged on before the console logs him out of this session. So you have a number of different options here. I’m just gonna leave it as a default one hour. And I want to attach AWS managed policies. So let me just type in S3, ‘cause I want full S3 access and also full RDS access. So here we have the AWS managed policy of S3 and I also want RDS as well. And here we have Amazon RDS full access.</p>
<p>So if I then click on next, we can add any tags if you want to. I’m just gonna leave that blank for now. Then in the final review screen, we can see the policies that I’ve attached, the S3 Full Access and RDS Full Access, and also the name of this permission set. And then click on create.</p>
<p>Now the next thing I want to do is associate this permission set with the group and assign that group to the AWS accounts in my organization. So I go across to AWS organization here, and if I select the two accounts that I want to add the users to, then I can click on assign users. Now I can either split the use individually or the group, and the best practice would be to use the group for easier managements. I’m going to select the S3 and RDS access because we know that the user is already a part of this group.</p>
<p>Now, from here, we can click on next to assign the permission sets. And here we have our permission set that we would like. So this will associate that permission set with the group on those two AWS accounts. Now we can see here that this is now successfully configured for my AWS accounts and our users can use these accounts with the permissions that we assigned. So if we click on proceed to AWS accounts and it takes us back to this screen here.</p>
<p>Now let’s look at from the user perspective. So now in my inbox of that user that was created. So this is the automatic email that is sent out to say that we’ve been invited to use the AWS SSO user portal and we can see the URL here, and we need to accept the invitation first. So let’s go ahead and do that.</p>
<p>Now I’m prompted to enter a new password, so let me just go ahead and do that. Making sure it meets all the recommendations, set new password. And then once that’s done, we can then click on our user portal here. So let’s go ahead and try that. So we have our username of Stuart Scott. If I enter my password, and I’m taken to this user portal here and we can see here that it’s got that URL that we initially set up.</p>
<p>Now, what we have here is our AWS accounts. Now, if I select this account, it shows the two accounts that this user has access to. And if I select one of those accounts, it then shows the permissions and roles that I effectively set up, the S3 and RDS access. And from here, I can either select the management console to gain access to the management console or use the CLI to gain programmatic access.</p>
<p>So let’s just go ahead and click on the management console. And that takes me straight into that specific account. So from here, this user can now administer S3 and RDS as well. And I can go back to the portal and also select the secondary account and do exactly the same thing.</p>
<p>So that’s how you can easily use AWS SSO to set up single sign on access to multiple AWS accounts when you’re running AWS organizations. That brings me to the end of this course which demonstrated how the AWS SSO service can be configured and implemented to help you manage a single sign-on portal to access multiple AWS accounts with a single AWS organization using the default user directory provided.</p>
<p>If you have any feedback on this course, positive or negative, please send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="Security-Summary"><a href="#Security-Summary" class="headerlink" title="Security Summary"></a>Security Summary</h1><p>So you’ve now completed the theory stage of probably my favorite topic of AWS, that being security. As you’ve probably realized by now, security is interwoven in so many parts of AWS and its services. For example, we touched on Compute Security with key pairs. We mentioned security during our storage section referencing encryption of data. We even touched on encryption during networking with network ACLs and security groups. So security is everywhere. </p>
<p>So it’s no surprise that AWS has a focus on specific security services, but the ones that will primarily show up in the exam are AWS IAM, WAF, Shield, and Cognito. So in this summary, I want to rinse and repeat some of the key elements that are essential in helping you prepare to answer questions that come up relating to these services. Okay, so let’s get started. First off, AWS Identity and Access Management, also known as IAM. So first and foremost, you’re going to need to know the difference between users, groups, and roles, and the best practices when configuring each of them. For example, generally, you should add users to groups and then apply permissions to those groups. </p>
<p>Another example would be to ensure you configure multi-factor authentication to any users that have enhanced and privileged permissions to reduce the blast radius should the credentials become compromised. Let me quickly mention a few points over each that you definitely should know about before sitting the exam. So from a user perspective, this is where you can configure and assign multi-factor authentication settings, also known as MFA. Also, this is where you can configure a user for programmatic access, as well as access to the management console. Can you remember what a user needs if they want programmatic access to your resources? Well, they need their access key ID and secret access key which are associated with their user and applied to the configuration of the AWS CLI.</p>
<p>So next we have IAM groups, and these are simply objects that your users can be a part of and any users in that group inherit permissions associated with that group. Now, when it comes to permissions, and I can’t emphasize this enough, it’s imperative that you have a basic awareness and understanding of IAM policies. You need to be able to look at a policy to determine what access it is allowing or denying. And this will help you breeze through a couple of questions on the exam, at least. Now, remember when it comes to permissions, a deny always overwrite any allow.</p>
<p>So I mentioned roles just a moment ago. So roles and policy permissions is always a hot topic in the AWS Solutions Architect Associate. So let me just run through a few points in relation to these. So roles allow users and other AWS services to adopt a set of temporary IAM permissions to access AWS resources. And the keyword here is temporary. Remember that for any questions that come up relating to temporary access, if you see temporary then look out for roles, but to adopt a role, you must have the right permissions to do so. Now much like groups, roles have associated permission sets and any user adopting the role will receive those permissions. And roles can also be used by EC2 instances, allowing that instance to access resources on your behalf, which is a great way to allow instances to access other resources, rather than embedding credentials on the EC2 instance itself, which you should never do. </p>
<p>Now you should also be aware of the different types of roles that are available, which includes service roles, service linked roles, roles for cross-account access. And actually on this point, I recommend that you learn and understand the steps involved in creating cross-account access, as there is usually at least one question on this. And then finally, roles for identity provider access which relates to federated access. But what is federated access, I hear you ask? Well, federated access is when you provide authentication to your applications in AWS Services via an external identity provider, such as your own Microsoft Active Directory or social identity, such as Facebook, Amazon, or Google. Now a great service to help you manage this for your web and mobile applications is Amazon Cognito. And I’ll come on to this later.</p>
<p>Now, earlier I mentioned how important it is to understand how to read IAM policies, which are written in a JSON format. Now the main elements of a policy you need to know are the action, and this is the action of the permissions being requested. For example, being able to delete objects in S3 or to create instances for EC2. Also, we have the effect, and this element can either be set to allow or deny the actions that are stipulated. Then we have the resource, and the defines the resource you wish the action and effect to be applied to. For example, the ARN of a specific S3 bucket. And also we have the principal element as well. And this specifies the identity that the permissions apply to. And this is usually used in a resource-based policy instead of an identity-based policy. And then finally, the condition. This is an optional element that allows you to control when the permissions will be effective, based upon set criteria. For example, you could ensure that the permissions are only applied if the user has a specific source IP address.</p>
<p>Okay, so moving on from IAM, which is quite a big topic in itself, I now want to mention a few points relating to the AWS Web Application Firewall Service, Firewall Manager, and Shield. Okay, so firstly, what is WAF? While the main function of the AWS WAF service is to provide protection of your web applications from malicious attacks from a wide variety of attack patterns. It’s worth noting as well that it’s also used in conjunction with Amazon CloudFront distributions, the Application Load Balancer and the API gateway to analyze requests over HTTP or HTTPS, to help distinguish between harmful and legitimate requests to your applications and site. So AWS WAF will block and restrict access that is detected as forbidden. Now it will help you to understand how WAF fits together and some of its components. Ensure you know the differences between conditions, rules, and Web Access Control List for the exam.</p>
<p>So to use WAF, you need to create a Web Access Control List and this will be associated to a resource, for example, a CloudFront distribution. And in this Web ACL, you will include both conditions and rules. Now the primary function of the condition is to specify what element of the incoming request should be analyzed by WAF, and examples of these include cross-site scripting, geo-match or IP address, et cetera. So it’s a condition as to which the incoming request will be assessed upon. Now rules contain the conditions that you want to use to filter the incoming web requests. So using both the rules and conditions, you can specify an action, which will either be allow, block, or count. </p>
<p>And this will depend on the condition configuration. And these rules are then added to the Web Access Control List. Now it’s worth pointing out that WAF rules are executed in the order that they appear within a Web ACL. And as soon as the match is found, no other rules are checked for that request. So ensure you set these rules in the correct order to filter your request properly. So that’s an overview of WAF. Now let’s see how Firewall Manager fits in. So Firewall Manager closely relates to WAF, but Firewall Manager has been designed to help you manage WAF in a multi-account environment with simplicity and control, with the help of AWS Organizations. So just remember, when it comes to WAF and multi-accounts, you should be using Firewall Manager. </p>
<p>Now, before you use Firewall Manager, you must ensure that your AWS Account is a part of an AWS Organization with all features enabled. You must also define which AWS Account should act as a Firewall Manager Admin account. And lastly, ensure that you have AWS Config enabled. So that’s just some prerequisites of using Firewall Manager to be aware of. Now, Firewall Manager uses rule groups to group together one or more WAF rules that you have, and these will all have the same action applied when the conditions are met within a rule. Now also remember that Firewall Manager policies contain rule groups that you want to assign to your AWS resources. Now, the final service I want to talk about that relates to WAF as well is AWS Shield.</p>
<p>So AWS Shield is designed to help to protect your infrastructure against distributed denial of service attacks, commonly known as DDoS, and these attacks target hosts which might be running a website or web application, and the increase and flood of traffic from a DDoS attack aims to prevent legitimate requests getting through to the host and being processed, and so it hinders the whole performance of the application or website. So if you get any questions relating to DoS attacks or DDoS attacks, then you want to be looking for AWS Shield within your answers. So AWS Shield itself is available at two different levels. We have AWS Shield Standard, which is free and available to everyone, whereas AWS Shield Advanced has a lot more power and protection to offer than Standard. For example, with Advanced you get access to a 24 by 7 specialized DDoS response team at AWS, known as DRT. Now also remember that the protection that Shield offers is integrated both with CloudFront and also Route 53 as well.</p>
<p>Okay, let’s move on to Amazon Cognito. And this is the last security service I want to mention. So let’s put together some of our key highlights here. So Amazon Cognito is an authentication and user management service, which works great with web and mobile applications. So it integrates with third party identity providers, such as Apple, Facebook, Google, and Amazon, in addition to being able to federate identities from your own Active Directory service. Now I recommend you become familiar with the differences between Amazon Cognito User Pools and Identity Pools. Now these are the two main points of this service. So as long as you understand those, then it should put you in good stead to answer any questions that come up about managing authentication and verification at scale to your web applications or your mobile applications. </p>
<p>Now just remember that User Pools are used to create and maintain a directory of your users for your mobile or web applications. Now this means dealing with both signing up and signing into your new and returning users. Now you can also offer sign-in for your application by using third party identity providers, such as Facebook, Amazon, et cetera, or by using SAML, for example, using your own Active Directory. Now Cognito Identity Pools help to provide temporary access AWS credentials for your authenticated users or unauthenticated guests that need access to AWS services. Now they can work in tandem with Amazon Cognito User Pools, allowing your users to operate and access whatever specific feature they need from AWS. </p>
<p>Now, additionally, just like with User Pools, you can federate with public providers, such as Amazon, Facebook and Google as well. So the main difference between User Pools and Identity Pools is that User Pools provide a method of authentication through identity verification, allowing them to sign into your web or mobile applications using an identity provider or Cognito’s local directory, whereas Identity Pools are typically used to help control access using temporary credentials to access AWS services on your application’s behalf.</p>
<p>Okay, so that brings us to the end of the security summary. Now there’s a lot to take in. However, you are going to need to know about IAM, the difference between users, groups, and roles. So have a play with them using our labs, and it will really help you. Also spend some time looking at IAM policies to ensure you understand their layout and what they tell you. Now if you get any questions relating to protecting your web apps or CloudFront distributions from common attack patterns, then think of WAF. If you get any questions about managing WAF between multiple accounts using AWS Organizations, then think Firewall Manager. And if you get anything asking how to protect against DoS or DDoS, then AWS Shield is your go-to answer. Okay, it’s time to take a break, rest up, retain what we’ve covered, and then move on to the next section.</p>
<h1 id="2What-is-Identity-and-Access-Management"><a href="#2What-is-Identity-and-Access-Management" class="headerlink" title="2What is Identity and Access Management?"></a>2<strong>What is Identity and Access Management?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/">Identity Federation course</a></p>
<h1 id="3IAM-Features"><a href="#3IAM-Features" class="headerlink" title="3IAM Features"></a>3<strong>IAM Features</strong></h1><p><a target="_blank" rel="noopener" href="https://sts.amazonaws.com/">STS Global Endpoint</a></p>
<h1 id="7Managing-Multiple-Users-with-IAM-User-Groups"><a href="#7Managing-Multiple-Users-with-IAM-User-Groups" class="headerlink" title="7Managing Multiple Users with IAM User Groups"></a>7<strong>Managing Multiple Users with IAM User Groups</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-quotas.html">Group limitations</a></p>
<h1 id="9Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf"><a href="#9Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf" class="headerlink" title="9Using AWS Service Roles to Access AWS Resources on Your Behalf"></a>9<strong>Using AWS Service Roles to Access AWS Resources on Your Behalf</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html">AWS services that work with IAM</a></p>
<h1 id="10Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users"><a href="#10Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users" class="headerlink" title="10Using IAM User Roles to Grant Temporary Access for Users"></a>10<strong>Using IAM User Roles to Grant Temporary Access for Users</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-cross-account-access-using-iam/">Implementing cross-account roles using IAM</a></p>
<h1 id="11Using-Roles-for-Federated-Access"><a href="#11Using-Roles-for-Federated-Access" class="headerlink" title="11Using Roles for Federated Access"></a>11<strong>Using Roles for Federated Access</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-cognito-manage-authentication-authorization-mobile-web-apps-1560/">Using Amazon Cognito to Manage Authentication &amp; Authorization to your Mobile and Web Apps</a></p>
<h1 id="17An-Overview-of-AWS-WAF"><a href="#17An-Overview-of-AWS-WAF" class="headerlink" title="17An Overview of AWS WAF"></a>17<strong>An Overview of AWS WAF</strong></h1><p><a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">OWASP Top Ten List</a></p>
<h1 id="21Policies"><a href="#21Policies" class="headerlink" title="21Policies"></a>21<strong>Policies</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/firewall-manager/pricing/">Firewall Manager pricing</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/config/pricing/">AWS Config pricing</a></p>
<h1 id="23Configuring-Shield"><a href="#23Configuring-Shield" class="headerlink" title="23Configuring Shield"></a>23<strong>Configuring Shield</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Course: AWS: Overview of AWS Identity &amp; Access Management (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/">Course: Amazon Web Services CloudWatch</a></p>
<h1 id="25AWS-Security-Hub-Standards-Implemented"><a href="#25AWS-Security-Hub-Standards-Implemented" class="headerlink" title="25AWS Security Hub Standards Implemented"></a>25<strong>AWS Security Hub Standards Implemented</strong></h1><p><a target="_blank" rel="noopener" href="https://www.cisecurity.org/">Center for Internet Security</a></p>
<h1 id="34User-Pools-Authentication-Flow"><a href="#34User-Pools-Authentication-Flow" class="headerlink" title="34User Pools Authentication Flow"></a>34<strong>User Pools Authentication Flow</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/mobile/customizing-your-user-pool-authentication-flow/">Customizing Amazon Cognito User Pool Authentication Flow</a></p>
<h1 id="35Identity-Pools"><a href="#35Identity-Pools" class="headerlink" title="35Identity Pools"></a>35<strong>Identity Pools</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/users-groups-and-roles/">Course: AWS: Overview of AWS Identity &amp; Access Management (IAM)</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-2-of-2-37/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-2-of-2-37/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-2-of-2-37</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:53" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:53-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:00:10" itemprop="dateModified" datetime="2022-11-27T20:00:10-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-2-of-2-37/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-2-of-2-37/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Architecture-SAA-C03-2-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-1-of-2-36/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-1-of-2-36/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-1-of-2-36</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:51" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:51-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:00:04" itemprop="dateModified" datetime="2022-11-27T20:00:04-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-1-of-2-36/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Architecture-SAA-C03-1-of-2-36/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Architecture-SAA-C03-1-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Fan-Out-Orders-using-Amazon-SNS-and-SQS-35/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Fan-Out-Orders-using-Amazon-SNS-and-SQS-35/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Fan-Out-Orders-using-Amazon-SNS-and-SQS-35</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:50" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:50-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:12:38" itemprop="dateModified" datetime="2022-11-27T20:12:38-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Fan-Out-Orders-using-Amazon-SNS-and-SQS-35/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Fan-Out-Orders-using-Amazon-SNS-and-SQS-35/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Manage-Message-Queues-Using-Amazon-SQS-34/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Manage-Message-Queues-Using-Amazon-SQS-34/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Manage-Message-Queues-Using-Amazon-SQS-34</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:48" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:48-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:16:48" itemprop="dateModified" datetime="2022-11-27T20:16:48-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Manage-Message-Queues-Using-Amazon-SQS-34/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Manage-Message-Queues-Using-Amazon-SQS-34/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/48/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/48/">48</a><span class="page-number current">49</span><a class="page-number" href="/page/50/">50</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/50/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
