<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/45/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/45/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:57" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:57-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:56" itemprop="dateModified" datetime="2022-11-20T19:01:56-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Cert-Prep-Certified-Cloud-Practitioner-for-AWS.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:55" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:55-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:54" itemprop="dateModified" datetime="2022-11-20T18:58:54-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Billing, Pricing, and Support in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce some Billing, Pricing, and Support resources currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Billing, Pricing, and Support resources in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to important Billing, Pricing, and Support resources in AWS, including:</p>
<ul>
<li>AWS Cost Management tools such as Cost Explorer and AWS Cost and Usage Reports;</li>
<li>The role of tags in cost allocation; and</li>
<li>AWS customer support resources and support plans.</li>
</ul>
<p>These objectives are covered by Domain 4 in the official AWS Certified Cloud Practitioner exam blueprint: Billing and Pricing, which accounts for 16% of the exam content. The other courses in this learning path will cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="Cloud-Economics-Basics-Part-One"><a href="#Cloud-Economics-Basics-Part-One" class="headerlink" title="Cloud Economics Basics Part One"></a>Cloud Economics Basics Part One</h1><p>Welcome to your very first Lecture “Understand how AWS Billing works.” Here we’ll give you the basics you need to understand AWS Billing.</p>
<p>We take you on a journey through the development from the classic data center to today’s public cloud. </p>
<p>In addition to important information about the modern Cloud Economy, we also show you the problems with the Public Cloud. </p>
<p>Cloud costs affect everyone, so it is important that IT, Finance, and Executive Departments understand each other. We convey the most important terminology from these areas and ensure mutual understanding. </p>
<p>Even though we are fans of practical learning, this will be a theoretical lesson to lay the foundations for the rest of the course.</p>
<p>Please join me into the first lecture.</p>
<p>Let’s take a look at where we actually come from. Until the year 2006, companies still relied on their own IT. IT costs were fixed, also called Capital Expenses or CapEx. </p>
<p>That has changed with the move to the cloud. IT and cloud costs are now much more variable and difficult to plan than before. We have moved to operational expenses, also known as OpEx. No worries if you don’t know exactly what these terms mean, we’ll cover them later.</p>
<p>We are currently at a tipping point where more is already being spent on cloud resources than on-premise. And the trend is rising. Forecasts predict that by 2022 we will have a total cloud spend of around 360 billion dollars.</p>
<p>In addition to many advantages, the cloud also has its downsides and we can see what these are in this graphic. At re:Invent 2019 310 business and IT executives were asked where their biggest problems lie in cloud operations. </p>
<p>The graphic speaks for itself, 29% stated that cost management is their biggest challenge. </p>
<p>This is still underpinned by a survey by Gartner. According to the survey, 80% of companies will overrun their Infrastructure as a service budget in 2020. </p>
<p>Reason enough to bring the knowledge about cloud cost optimization to the world, right?</p>
<p>Working in the Cloud comes with many benefits and variable expense control. However, due to the often new and unfamiliar environment, many users end up with paying more and struggling with cost management.</p>
<h1 id="Why-Cost-Optimization-Matters"><a href="#Why-Cost-Optimization-Matters" class="headerlink" title="Why Cost Optimization Matters"></a>Why Cost Optimization Matters</h1><p>Starting early with cost optimization is essential and can save a lot of trouble.</p>
<p>Many AWS cost projects do not start when you migrate to AWS, but costs get out of hand. And then it usually becomes even more difficult to change entrenched processes and behaviors. But do not be discouraged by this if you are in such a situation. This is why we have created this course, to help you relax, even in such difficult situations.</p>
<p>AWS offers fantastic opportunities for faster development, lower TCO, and increased agility.</p>
<p>No matter if a startup or large corporation, AWS grows with the requirements and always provides exactly the services that are currently needed - of course, only when used correctly.</p>
<p>Cost Optimization helps you meet your financial and business objectives while only paying for what you need and and use.</p>
<p>AWS is not a traditional data center and therefore should not be used as such. </p>
<p>AWS allows us to be much more agile and instead of spending time on administrative overhead, we can invest that time in innovation.</p>
<p>Companies see the greatest potential for savings by taking advantage of the agility of AWS and developing applications that scale and are highly automated.</p>
<p>Later in the course, we will cover the five pillars of the Well-Architected Framework, clarifying the key concepts and design principles of cloud workloads.</p>
<p>Cost optimization will help you manage your resources and money appropriately and in time to make economically wise decisions before expenses get out of hand and need to be fixed.</p>
<h1 id="Cloud-Economics-Basics-Part-Two"><a href="#Cloud-Economics-Basics-Part-Two" class="headerlink" title="Cloud Economics Basics Part Two"></a>Cloud Economics Basics Part Two</h1><p>Welcome to the section about the basics of cloud economics. The value of cloud extends beyond cost savings for your infrastructure. Cloud users can see significant improvements in other areas, including staff productivity, operational resilience, and business agility.</p>
<p>Let’s dig a little deeper into cloud economics to see what exactly is part of it, and where companies can improve their business. Of course, there’s cost savings or a lower TCO. Or in other words, infrastructure cost savings, or avoidance, from moving to the cloud. This can be reached through: A better ability to match supply and demand, and improving utilization; Elastic cost base driven by usage patterns; And through the elimination of hardware refresh and maintenance programs. Example for this is 50% reduction to total TCO.</p>
<p>Then there’s also the staff productivity, or efficiency improvement by function on a task-by-task basis. This can be reached through a higher maintenance efficiency through automation, the elimination of hardware-related tasks, and increased developer productivity. An example for this would be over 500 hours per year of server configuration time saved.</p>
<p>There’s also operational resilience, or the benefit of improved availability, security, and compliance. This can be reached through the reduced cost of planned and unplanned outages, a reduced risk profile, or cost of risk mitigation, and an improved service level agreement. And an example for this is the critical workload that is run in multiple AZs and Regions for strong disaster recovery.</p>
<p>Last but not least, there is business agility, or the faster deployment of new features and applications, while reducing errors. And this is reached through reduced time to market, increased operational agility, and the reduce costs, and increased pace of innovation. An example for this is a 75% faster launch of a new product.</p>
<p>Now let’s take a look at some real numbers to see what this means exactly. In 2018, IDC, one of the largest market research firms in the world, asked 27 major corporations how their use of AWS has impacted their business. And as we can see here, they had a 62% more efficient IT infrastructure staff, nearly three times more features delivered, and 94% less time lost to unplanned downtime.</p>
<p>This is of course only a sample of 27 companies. And of course, the perspectives are not supplied everywhere. But you get a good overview of what is possible when using cloud properly. Cloud economics is not only about cost savings. However, with the right usage the cloud can help your entire business become more efficient.</p>
<p>In the past, engineering teams needed the approval of the finance team to get new resources. This process works, but it’s incredibly slow, and would destroy all of the agility in the cloud. So how can this process be adapted to the cloud? The biggest hurdle here is not to get financial issues in the way of IT teams, and to involve financial teams in planning, procurement, and forecasting.</p>
<p>Based on many companies’ experience, AWS has created a high-level best practice framework that is also called the five pillars of cost optimization. The five cost optimization pillars apply, regardless of your workload or infrastructure across nearly all environments.</p>
<p>The pillars of cost optimization are defined by AWS as follows:</p>
<p>Right-sizing: Ensure that what you provision matches what you need. For example, for compute, you provision for CPU, memory, storage, and network throughput.</p>
<p>Increase elasticity: Traditional IT costs and hardware requirements are tailored for peak usage and are rarely turned off. In the cloud, you can optimize costs to meet dynamic needs and turn resources off when they are not needed. For example, you can usually turn off non-production instances for 70% or more of any given week.</p>
<p>Leverage the right pricing model: AWS provides a range of pricing models. For example, On-Demand and Spot Instances for variable workloads, and Reserved Instances for predictable workloads. Choose the right pricing model to optimize costs based on the nature of your workload.</p>
<p>Optimize storage: AWS provides multiple storage tiers at prices designed to meet performance. By identifying the most appropriate destination for specific data types, you can reduce Amazon Elastic Block Store and Amazon Simple Storage Service while maintaining the required performance and availability. For example, where performance requirements are lower, using Amazon EBS Throughput Optimized HDD typically cost half as much as the default General Purpose SSD.</p>
<p>Measure, monitor, and improve: To ensure that you extract the full economic potential of the AWS Cloud at any scale, you want to: Define and enforce cost allocation tagging. And you will hear me talking about tagging a lot within this course.</p>
<p>Forecast usage and costs: Define metric set targets and review at a reasonable cadence. Enable teams to architect for costs via training, visualization of progress goals, and a balance of incentives, and assign optimization responsibility to an individual or to a team.</p>
<h1 id="Total-Cost-of-Ownership"><a href="#Total-Cost-of-Ownership" class="headerlink" title="Total Cost of Ownership"></a>Total Cost of Ownership</h1><p>Welcome to this section about TCO or Total Cost of Ownership. TCO was already mentioned a few times. TCO stands for Total cost of ownership and is a comprehensive assessment of IT’s total costs or other costs over time.</p>
<p>For IT, TCO includes hardware and software acquisition, management and support, communications, end-user expenses, and the opportunity cost of downtime, training, and other productivity losses.</p>
<p>Comparing the TCO of different environments is not an easy task. Especially in the “old world,” the on-premise world, many factors play a role in this assessment, as this overview shows.</p>
<p>Let’s take the facility costs for example. Something like space, power, cooling. It’s nothing you have to worry about when using AWS resources. </p>
<p>Using the cloud has greatly simplified the TCO assessment.</p>
<p>Because most additional costs are included in AWS’s service prices, there are far fewer factors to take into account.</p>
<p>As someone who started his IT career by assembling servers and configuring switches, I can assure you that physical IT operation involves a lot of effort, costs, and stress.</p>
<p>Today, an EC2 instance can be started with just a few clicks; in the past, a server had to be assembled, installed, mounted in the data center, and wired. Also, there were costs for maintenance, cooling, space, power, licenses, and other factors.</p>
<p>Understanding TCO improves the overview on the business and helps you recognize issues and opportunities at an early stage.</p>
<h1 id="Economies-of-Scale"><a href="#Economies-of-Scale" class="headerlink" title="Economies of Scale"></a>Economies of Scale</h1><h1 id="Welcome-to-this-section-about-economies-of-scale-Every-time-you-attend-AWS-Re-Invent-the-largest-AWS-conference-worldwide-you-can-see-this-slide"><a href="#Welcome-to-this-section-about-economies-of-scale-Every-time-you-attend-AWS-Re-Invent-the-largest-AWS-conference-worldwide-you-can-see-this-slide" class="headerlink" title="Welcome to this section about economies of scale. Every time you attend AWS Re:Invent, the largest AWS conference worldwide, you can see this slide."></a>Welcome to this section about economies of scale. Every time you attend AWS Re:Invent, the largest AWS conference worldwide, you can see this slide.</h1><p>The only thing that changes every year is the number of price reductions. Currently, this number is 69. </p>
<p>How does AWS manage to reduce prices continuously?</p>
<p>The answer is quite simple: Economies of scale. But what does that mean?</p>
<p>Let’s take a look at this cycle. More customers lead to higher AWS usage which leads to more infrastructure. The economies of scale effect occurs because costs can now be spread over a larger number of customers. The infrastructure becomes cheaper, prices can be reduced and new customers benefit from the lower prices. And then the cycle starts all over again. </p>
<p>With AWS you’re likely to experience lower prices over time as their infrastructure grows and the economies of scale takes place.</p>
<h1 id="Pricing-Calculator"><a href="#Pricing-Calculator" class="headerlink" title="Pricing Calculator"></a>Pricing Calculator</h1><p>With the AWS Pricing Calculator, you have a powerful tool to estimate costs, reduce running expenses and find the cheaper, better suited solution for your environment. Welcome to the section about the AWS Pricing Calculator.</p>
<p>The Pricing Calculator has undergone many name changes and innovations over the years. The current version as of fall 2020 is a great help to estimate the total cost of your AWS environment, find the right instance sizes and compare services across different regions.</p>
<p>Let’s have a look at how the tools work exactly. When you generate an estimate, you can either add services directly to your estimate or create a group and add the services to your group. This time I will show to set up a group with an Amazon EC2 instance that you can use to perform tasks, such as, run a small program or host a website.</p>
<p>We’ll now create an estimate and assign it to a region. So we click on, Create Estimate and we need to look up the service we’re looking for. Wanna estimate an EC2 instance, so we’re looking here for EC2 and click on, Configure. We have to choose a region.</p>
<p>Bear in mind, every region has a different pricing. There are cheaper and there are more expensive regions. We will take Frankfurt as this is where I’m sitting currently. You can either create quick estimate or an advanced estimate. We will go for the advanced estimate, because this gives us the option to also choose the kind of workload that we have.</p>
<p>Let’s assume we have an online blog and we have many users or visitors over the weekends. So we choose daily spike traffic. And we assume that we have a lot of visitors on Friday, Saturday and Sunday. We have a baseline of, let’s say, three instances and during peak times over the weekend, we have 10. Peak times are eight hours and we need to find an instance.</p>
<p>Let’s say we need two CPUs and four gig of RAM and the tool tells us which instance would be the cheapest, as you can see here. Let’s say we take the t3a. The A stands for an AMD chip set. We can see we have four gigs of memory. Two CPUs. Enough network traffic. And that’s good enough.</p>
<p>Here you can choose the pricing models. We will cover this later within the course. So we’ll just go for On-Demand here. 10 gigs of storage is enough. Daily, two daily snapshots. We also have some data transfer. Let’s say, 10 gigs. Intra-region transfer, no. Outbound, yeah, let’s say also 10 gigs, just to get some numbers in here. And you can see here, we would have EC2 costs monthly of 72 and some storage costs of $50. </p>
<p>We have $87 total cost for our EC2 instance and bare in mind that we have a spikey workload. So these with these usage patterns are already calculated in here. We can now add this calculation to our estimate and see how much our website or our blog would cost us for 12 total months.</p>
<p>We could now save our estimate or add another service. For example, let’s say we also need a database, maybe like MySQL database. Notice that’s a huge instance here. Let’s go for a small one, m5.2xlarge. It’s also a big a one, but it’s just for numbers, right? On-Demand is fine. 20 gigs of storage.</p>
<p>Then we can see the database would cost us $592, plus a little bit of storage. We can also add this to our estimate and we can now see the total price here and so it goes on. You can add the services and tools that you need here and get a very accurate estimate for your total costs.</p>
<h1 id="AWS-Well-Architected-Framework"><a href="#AWS-Well-Architected-Framework" class="headerlink" title="AWS Well-Architected Framework"></a>AWS Well-Architected Framework</h1><p>AWS has developed a structure of best practices and strategies when building software in the cloud, called the AWS Well-Architected Framework. </p>
<p>Based on knowledge collected and gained by AWS solution architects and AWS users over the past years it is made to provide strategies and proven concepts for cloud systems.</p>
<p>The framework is based on five pillars that ensure a secure, reliable, efficient, and cost-effective system.</p>
<p><strong>The 5 Pillars of the AWS Well-Architected Framework - Overview:</strong></p>
<ul>
<li><p>Operational excellence</p>
</li>
<li><ul>
<li>aims at constant improvement and efficient managing of workloads, as well as gaining operational insights and continuous improvement of processes and procedures to support business value</li>
</ul>
</li>
<li><p>Security</p>
</li>
<li><ul>
<li>describes how to protect data, systems, and components using cloud technologies, how user rights and privileges are correctly managed, and how integrity and conformity of information is maintained</li>
</ul>
</li>
<li><p>Reliability</p>
</li>
<li><ul>
<li>focuses on the ability of a workload to perform correctly and as intended at the expected time. Including quickly recovery and prevention from failures</li>
</ul>
</li>
<li><p>Performances and efficiency</p>
</li>
<li><ul>
<li>Efficient allocation and right-sizing of computing resources by the system requirements and interception of demand changes</li>
</ul>
</li>
<li><p>Cost optimization</p>
</li>
<li><ul>
<li>Understanding and controlling expenses, avoiding unnecessary costs, and analyzing spend in detail</li>
</ul>
</li>
</ul>
<p>Furthermore, AWS provides the AWS Well-Architected Tool, which is based on the framework‘s principals. The tool helps to review the user workloads and compare them to the latest architectural best practices.</p>
<p>In general, the AWS Well-Architected Framework and the tool act as general guidelines that can be used without additional costs.</p>
<p>As a user, you will benefit from free architectural guidance, proven and tested strategies, and best practices directly from AWS for your workloads.</p>
<p>By making use of the AWS Well-Architected Framework and its tool, you can avoid pitfalls and failures by relying on proven strategies and professional practices for your own system, without any additional costs. The framework guarantees you improvements on security, reliability, efficiency, and on the financial end.</p>
<h1 id="Working-with-the-Pillars-of-the-AWS-Well-Architected-Framework"><a href="#Working-with-the-Pillars-of-the-AWS-Well-Architected-Framework" class="headerlink" title="Working with the Pillars of the AWS Well-Architected Framework"></a>Working with the Pillars of the AWS Well-Architected Framework</h1><p>Striving for excellence by applying as many best practices as possible into the workloads, with the aim to reduce human error, save time and resources by automation and continually improve processes.</p>
<p>Principles:</p>
<ol>
<li>Operations-as-Code</li>
</ol>
<p>The entire workload, i.e., the development of applications together with the infrastructure, should consist exclusively of code. This way, operational errors can be reduced, and the execution, as well as updates or changes, can be automated, which saves time and resources.</p>
<ol>
<li>Frequent but small and reversible changes</li>
</ol>
<p>The development and infrastructure should be designed for small and light updates that are made more frequently and should be easily reversible if necessary, without breaking anything.</p>
<ol>
<li>Evolve procedures alongside the workload</li>
</ol>
<p>As software development progresses, so should the associated processes. Regular and dedicated routines can help to find improvement opportunities, like finding processes that can be automated, and validate the effectiveness of the procedures.</p>
<ol>
<li>Failure prevention</li>
</ol>
<p>Identify sources of failures and remove the cause before issues reoccur by performing preventive exercises. Regular events and simulated exercises will increase awareness inside the team and improve reaction times.</p>
<ol>
<li>Learning from operational failures</li>
</ol>
<p>Every incident and failure is a lesson to analyze and learn from. Lessons and appropriate improvements should be well documented and shared throughout the whole company.</p>
<p>As customer needs and business demands can change rapidly, it is wise to design the development infrastructure to support agility and possible changes in advance. **<br>**</p>
<p>Additionally, keeping lessons on success and failure well documented and easily accessible helps maintain the best possible performance and reduce spending time on recurring decision-making.</p>
<p>That was the first pillar, operational excellence. The next one will be Security.</p>
<p>The goal of the security pillar is to provide the highest possible security on data, systems, and components.</p>
<ol>
<li>Implementation of a strong identity foundation</li>
</ol>
<p>Going by the principle of least privilege, users are granted only as much access as they need to fulfill a task. Appropriate authorization should be a major requirement for all resources in the cloud system.</p>
<ol>
<li>Traceability</li>
</ol>
<p>Real-time monitoring, alerting, and auditing actions and changes in the environment should be logged and made available for automatic investigation and action routines.</p>
<ol>
<li>Apply security at all levels</li>
</ol>
<p>Defensive strategies should be applied to all possible levels.</p>
<ol>
<li>Security by automation</li>
</ol>
<p>By using automated security mechanisms, architectures gain the ability to scale faster and more cost-effectively.</p>
<ol>
<li>Data protection</li>
</ol>
<p>Make the use of data encryption, tokenization, and access control mandatory.</p>
<ol>
<li>Access</li>
</ol>
<p>Reduce the risk of improper handling or modification of sensitive data by users by preventing direct access when not needed.</p>
<ol>
<li>Incident preparation</li>
</ol>
<p>Prepare incident management and investigation policy in advance to handle incidences when they occur.</p>
<p>When designing the system architecture, identity control, and access on multiple layers should be prioritized from the beginning to avoid major changes at a later state.</p>
<p>Recognize demand changes as well as disruptions at an early state to acquire resources in time and recover from failures automatically.</p>
<ol>
<li>Automatic recovery</li>
</ol>
<p>Identify potential outages by using Key Performance Indicators (KPIs) on the workload that will trigger the monitoring system. This allows you to take appropriate action to either prevent the outage or automatically begin remediation.</p>
<ol>
<li>Test recovery procedures</li>
</ol>
<p>Develop recovery strategies by testing different failure scenarios. Other than an On-Premise environment, the cloud allows simulating variously scaled scenarios.</p>
<ol>
<li>Horizontal scaling for better availability</li>
</ol>
<p>Prevent common points of failure by scaling the infrastructure horizontally, i.e., distributing requests across multiple resources rather than hoarding the entire workload on a single resource.</p>
<ol>
<li>Stop guessing capacity</li>
</ol>
<p>Use monitoring as a tool to detect demand and scale the environment accordingly by automated addition or removal.</p>
<ol>
<li>Manage changes in automation</li>
</ol>
<p>Changes to the infrastructure should only be made by automated and trackable actions.</p>
<p>Monitoring and logging is the fundament of a reliable system. By analyzing logged metric data and responding accordingly in time, failures can be detected beforehand and automatically repair themselves.</p>
<p>Efficient usage of computer resources to meet system requirements while demand and technological advances may change.**<br>**</p>
<ol>
<li>Make use of advanced technologies</li>
</ol>
<p>In times of cloud computing, many software solutions come as a service. Using software as a service instead of hosting, operating, and managing a tool provides more free time and resources for the development team.</p>
<ol>
<li>Global in minutes</li>
</ol>
<p>With AWS Regions, the workload can be deployed all around the world and at various scales, which allows the reduction of latency for customers.</p>
<ol>
<li>Serverless architectures</li>
</ol>
<p>Today, many traditional computing activities can be realized through serverless solutions that can be maintained without the need for a physical server, thus saving costs for operation, management, and operations.</p>
<ol>
<li>Experiment more</li>
</ol>
<p>Various tests can be performed with virtual resources to determine which service or resource and which type of configuration is best suited for individual requirements.</p>
<ol>
<li>Know the options - make the right choices</li>
</ol>
<p>Make sure to get to know about the service that aligns best with individual workload goals. For example, when making decisions, consider the appropriate database or storage concept that best suits the needs.</p>
<p>Use a data-driven approach to select high-performance designs. Collect data on all areas of your architecture, monitor deviations from expected performance, and then take action.</p>
<p>Deliver business value at the lowest possible price. Reduce upfront fixed costs and profit from controllable and small ongoing expenses.</p>
<ol>
<li>Implement Cloud Financial Management</li>
</ol>
<p>Build capabilities to manage and spread awareness of costs and expenses in the cloud environment.</p>
<ol>
<li>Adopt a consumption model</li>
</ol>
<p>Pay as you go. Analyze the actual business needs and match resources to current requirements.</p>
<ol>
<li>Measure overall efficiency</li>
</ol>
<p>Measure workload business performance and the costs associated with deployment. Use these metrics to determine the gains you make by increasing performance and reducing costs.</p>
<ol>
<li>Stop spending money on data center operations</li>
</ol>
<p>AWS takes over traditional data center tasks and does not charge extra to manage and update operating systems or applications with managed services. </p>
<ol>
<li>Analyze and attribute expenses</li>
</ol>
<p>AWS makes it easy to identify system usage and costs and transparently allocate IT costs to individual workload owners based on this data. This helps you measure return on sales (ROI) and enables workload owners to optimize resources and reduce costs.</p>
<p>Getting the best performance and value while spending as little as possible can be achieved by applying the whole AWS Well-Architected Framework correctly on the individual business and cloud environment. The most vital part is to spread awareness of expenses and correct resource usage across the team to get rid of inefficiency</p>
<h1 id="Understanding-the-Other-Departments"><a href="#Understanding-the-Other-Departments" class="headerlink" title="Understanding the Other Departments"></a>Understanding the Other Departments</h1><p>Every department and every job has its own language. When people from different areas meet, communication is often difficult because important terms are not familiar to the other side.</p>
<p>Take the example of IT and finance. The topics of these two areas are far apart.</p>
<p>But as we have already learned, it is important to establish communication between these areas and this is exactly what we want to help you with in this lesson.</p>
<p>We will introduce you to the most important terms from AWS but also from software development and finance.</p>
<p>AWS has hundreds of different services and you can probably fill a whole dictionary with terms. Here we would like to explain the most important terms that are also important for the Cloud Financial Management area.</p>
<h1 id="General-AWS-Terminology"><a href="#General-AWS-Terminology" class="headerlink" title="General AWS Terminology"></a>General AWS Terminology</h1><p>Account (AWS) - AWS services are housed within an Account. Accounts can be Master Payer accounts that contain billing data or Linked Accounts which do not. AWS Organizations and other services can be used to manage Accounts within AWS. Many AWS services can span Account boundaries.</p>
<p>RI -Reserved Instance - a commitment to use a cloud resource, usually of a specific type, location, and size, for some period of time, usually 1 or 3 years, in exchange for a discounted rate.</p>
<p>SP - Savings Plans are very similar to Reserved Instances but more flexible and can only be applied to compute usage.</p>
<p>AURI, PURI, NURI &#x2F; SP (SavingsPlans)</p>
<p>All Upfront Reserved instance, Partial Upfront Reserved Instance and No Upfront Reserved Instance. Some people use these acronyms when referring to reserved instances, in case you hear them.</p>
<p>EC2 (AWS) Elastic Compute Cloud AWS’ virtual computer cloud offering</p>
<p>AWS supports a variety of instance, Instance Type, Family, Generation, Size (AWS) - Instance refers to a specific EC2 virtual machine. Instance Families, designated by letter, an instance Generation designated by a number and optionally other letters, and instance sizes which follow a structure of nano, micro, small, medium, large, xlarge, 2xlarge, etc. The Instance type includes the entire ndesignation, such as m5a.4xlarge which would be an “m” family, 5th generation, “a” for AMD chipset, 4xlarge sized instance. </p>
<p>IAM - Identity and Access Management - is the way that AWS refer to their system of granting and governing permissions within their cloud platforms.</p>
<p>Tags are metadata attached to a specific ressource running in AWS. They are meant to provide contextual information about the resource. Tags can be created with the resource in most cases or added after the fact manually or systematically. Tags are useful for identifying the type of resource, the environment it supports, the owner, the cost center, etc. </p>
<p>Tags can be queried or accessed in a wide variety of ways and can be used to drive automation, divide costs, or for other important purposes. Most large cloud-using organizations will at some point establish governance policies around tag use and require specific tags be used on all resources.</p>
<p>Console</p>
<p>Web-based portal from where you can manage your accounts or access AWS services. </p>
<p>Convertible &#x2F; Standard</p>
<p>AWS terms referring to the ability to convert reserved instances for some resources to different specifications. Standard RIs cannot be converted or changed for their entire term. Convertibility reduces the discount offered by AWS.</p>
<p>Region </p>
<p>AWS has the concept of a Region, which is a physical location around the world where data centers are clustered. An AWS Region consists of multiple, isolated, and physically separate AZ’s within a geographic area. Regions are generally guaranteed to be more than a minimum distance from one another to satisfy disaster recovery requirements.</p>
<p>Availability Zones (AZ) are sub-units of a Region, there are typically multiple AZs per Region. AZs are made up of multiple physical data centers but can generally be thought of as being very closely situated from a network latency and performance perspective.</p>
<h1 id="Terminology-from-Software-Development-amp-Operations"><a href="#Terminology-from-Software-Development-amp-Operations" class="headerlink" title="Terminology from Software Development &amp; Operations"></a>Terminology from Software Development &amp; Operations</h1><p>You’ve probably heard this one here before: DevOps. DevOps is a set of practices that intends to break down traditional silos between developers and operators of computer systems, allowing combined teams to collaborate and deliver software in a more consistent, efficient and automated fashion.</p>
<p>Enterprise Architecture, or EA, groups are traditionally tasked with outlining the structure of the systems an enterprise will build and maintain to achieve its business goals. Like physical architects, they provide the blueprints for how the various systems should be put together, the “materials” or software concepts that should be used to build them, and how the end results should look.</p>
<p>Lift &amp; Shift. Lift &amp; Shift is a method of migration involving moving an application as currently architected and built from one environment (an on-premises data center) to another (usually a public cloud). Lift &amp; Shift migrations can usually be done more quickly as they often do not require substantial change to the application code or configuration.</p>
<p>However, because they do not modify applications to use cloud-native services, they tend to create situations where the cloud system is more expensive or difficult to run than the on-premises system had been. </p>
<p>Lift &amp; Shift migrations are typically used when time pressure to close a data center or other need outweighs the cost and quality issues. A remediation period for the environment should always be planned After a Lift &amp; Shift migration to address issues.</p>
<p>Workload is a generic name for an application or software system running on a computing or other platform.</p>
<p>In a traditional website, there might be a web server, an application server and a database server, each running on an individual hardware-based server, or virtual machine in my data center, each of those three elements of the application would be a workload running on that virtual server. </p>
<p>On-Premises (or On-Prem) is a term used to refer to company-owned or company-controlled data center space. Usually used to differentiate from public cloud environments where application migrations are targeting workloads.</p>
<p>Companies have an extensive On-premises infrastructure built over many years when they begin using the cloud, and there are often difficulties using systems, infrastructure or processes developed for the on-premises environment in the public cloud.</p>
<p>Rightsizing is a form of optimization where measurements are taken over time to match workloads to a virtual resource sized to run it efficiently with a minimum of waste. Rightsizing can be used as a technique to save costs but must always involve technology oversight as well.</p>
<p>Agile is a method of project management, used primarily for software development characterized by division of tasks to short phases of work (into sprints) and frequent assessment of priorities and plans. Generally, leads to development of products or software incrementally beginning with a minimum viable product and then continually enhancing it from a backlog of requirements.</p>
<h1 id="Terminology-from-the-Finance-amp-Accounting-World"><a href="#Terminology-from-the-Finance-amp-Accounting-World" class="headerlink" title="Terminology from the Finance &amp; Accounting World"></a>Terminology from the Finance &amp; Accounting World</h1><p>We are now getting to the last part of our terminology section, Now with terms from the finance and accounting world. And we’re starting with amortization. This means retiring a payment of capital gradually over time on a schedule which reflects the benefits the capital provides in each period. Like depreciation, amortization typically applies to the retirement of cash payments, where depreciation tends to apply to physical capital equipment. An upfront RI payment can be amortized over the usual lifetime (1 or 3 years) of the RI itself.</p>
<p>Variable Costs are costs that varies according to the business volume it supports. A company hosting websites would need to pay for more computers to host more websites, and so that cost per website is a variable cost.</p>
<p>Upfront Charges for Reserved instances or savings plans. Reserved instances or service reservations, in general, can typically be purchased with a full upfront payment (All Upfront), a partial upfront payment plus a reduced periodic charge (Partial-upfront), or with no upfront charge (No-Upfront). The upfront charge may be amortized over the life of the RI. Upfront charges might be treated as Prepaid Expenses on the Balance Sheet but check this with your accountants!</p>
<p>Opex or Operating Expenditure is a category of business expense made in a specific accounting period which provide benefits only in that accounting period. Purchasing on demand cloud services is considered an Operating Expenditure. Operating expenditures require no long-term tracking of depreciation or amortization but are subtracted from earnings in the period incurred.</p>
<p>ROI - Return on Investment is the amount of profit from an investment made, usually expressed as a percentage of the original total cost invested. In a cloud rightsizing business case, the ROI might be calculated as the savings in cloud expenditure expected less the engineering and other costs required to take the rightsizing action.</p>
<p>The Income Statement (sometimes referred to as a P&amp;L statement) is a statement showing the company’s net profit or loss over a period of time (a month, a quarter, a year, etc.) The income statement would show expenses and amortization incurred during the period, so in year two of a 3-year RI, the amortization for the second year would show up as an expense against earnings in the period covered.</p>
<p>NPV or Net Present Value. An assessment used to calculate the long-term profitability of a project made by adding together all the revenue it can be expected to achieve over its whole life and deducting all the costs involved, discounting both future costs and revenue at an appropriate rate. In a cloud business case, the net present value of all the cash flows of a no-upfront RI might be compared to the current cash value of the all upfront RI for determining which is better for the business.</p>
<p>Depreciation means retiring the cost of an asset gradually overtime on a schedule which reflects the provision of benefits. Often this reflects the decrease in value of an asset over time due to wear and tear or usefulness because of continued use in out periods.</p>
<p>EBITDA. Earnings Before Interest, Taxes, Depreciation, and Amortization. An assessment of the earnings expected when subtracting only the cost of goods sold from the revenue achieved. Tracking the prepaid expenses of a 3-year all-upfront Reserved Instance as a cash outlay that can be amortized over three years would affect EBITDA differently than if the resources were purchased using cash at on-demand rates. </p>
<p>Capitalization is the ability to treat an investment or outlay as a capital item which will be depreciated or amortized in future periods. Cost Allocation is in FinOps, the ability to identify and allocate costs to the appropriate cost categories in use. Ideally direct costs, amortized costs, and shared costs can be allocated to individual budgeting categories for a clear view of the entire cost of running my application or workload in the cloud.</p>
<p>Unit Economics is the ability to directly compare my overall cost to the overall business benefit l am creating on a per unit basis. For example, if I understand that the overall cost of running my website infrastructure is $5Mil per month and is able to support 10,000,000 paid hosted web pages, then I can track a Webpage&#x2F;$ metric of “2” which indicates how efficiently I run my service. Any future modifications to my cloud infrastructure can then be expressed in terms of the Webpage&#x2F;$ metric to determine if they are helping or hurting, and opportunities for cost savings can be expressed in terms of how they impact Webpages&#x2F;$.</p>
<p>Fixed Cost. A cost which does not change with changes in business volume. The cost of a data center building mortgage is a fixed cost in that it does not vary regardless of whether there it is supporting 1 web server or 1,000,000 web servers driving the company’s revenue.</p>
<p>Balance Sheet. A statement of financial position of the business on a specific date which indicates the value of all assets and liabilities as of that date, including the retained value of any undepreciated or unamortized capitalizable items. A company purchasing a 3-year RiI at the beginning of a year would show that RI with % of its original value on the Balance Sheet on the last day of that year.</p>
<p>Capex Capital Expenditure - the purchase of a capitalizable asset, such as a building or equipment meant to provide value over a long term and thus to be depreciated or amortized over that term. Purchasing a data center and using it over 30 years is considered a Capital Expenditure while paying to run a virtual server in the cloud for this month is not.</p>
<p>Congratulations! You passed a heavy theoretical part and learned about the most important terms and their meanings in the cloud and finance context!</p>
<h1 id="Bills-and-Cost-Drivers"><a href="#Bills-and-Cost-Drivers" class="headerlink" title="Bills and Cost Drivers"></a>Bills and Cost Drivers</h1><p>Welcome to the first live demo. As you can see here we are in the AWS Management Console. The most fundamental part is to get a good overview of the environment. Therefore you have to get a broad view of all the services, usage, and expenses from the past, and the present.</p>
<p>To set a base for our analysis, we first need to get some numbers. For this we will be using the billing dashboard which is also our main platform for gathering information on expenses. We will deal a lot with the billing and cost management dashboard alongside but the very first thing I like to investigate when I start with an unfamiliar account or environment is to check the last bills.</p>
<p>So to get there, you can either type bill here in the search field to get straight to the billing service or you can just go here over the menu to the billing dashboard. Here, you get a good first overview what is going on in your account. What are the most expensive services? How much have you spent so far? And what is the forecast? The forecast to the current month. But we will talk about this later.</p>
<p>So first thing here is the bill section here on the left side. I wanna get to overview from the last month, how much we spent last month and what were the biggest cost drivers. So as we are currently in January 2021, I go one month back to December 2020, here from the dropdown menu. And as you can see, I have a total amount, how much we spent last month.</p>
<p>I have like different information about taxes and payment summaries. And I can also get a CSV file from my whole bill or just printed as a PDF. So as you can see here we are in a master payer account, that means we’re using AWS Organizations which enables us to link other accounts to this account.</p>
<p>So all the costs that are caused by the under linked accounts are covered by this account that are collected here. You can see there are many accounts but let’s focus on this overview here. So what I wanna get here is the first overview. What are the most expensive services, or like what are the biggest cost drivers.</p>
<p>You can see we’re spending like small amounts, like compared to the total we’re spending small amounts on like different services. For example, here $34 for the API Gateway or 91 for CloudFront. As this is like just like a very very small percentage of our total amount, it would make no sense or like almost no sense to dive into deeper analysis here for CloudFront. It just would make no sense because like the amounts are here so small and this makes also like our whole analysis much easier because we know the only thing we have to focus is Elastic Compute Cloud because this is the biggest cost driver in this case. And when we go here, we can see, okay, we’re using like quite a lot of regions here.</p>
<p>So my first question would be, why do we need so many regions? And as you probably know, the regions in Asia are way more expensive than for example, the regions in Northern Virginia. So my first question would be, why do we have all these different regions? And as you can see this is like just my thinking process, how I would approach such an analysis or such an optimization process but we will talk about this later in detail. Let’s go through the next section.</p>
<h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p>AWS credits are applied to bills to help cover costs that are associated with eligible services. They are applied until they are exhausted or they expire. With credits AWS introduced a sort of reward system for particularly active users and developers. You can use them instead of spending money on certain services.</p>
<p>So you may ask yourself, “How do I get credits?” So, there are multiple ways. For example, developing and publishing a skill for Alexa or by attending webinars and events. As a startup, you can use the AWS Activate program or through the AWS Credit Program for Nonprofits.</p>
<p>In my case, let’s jump here to the credits section, I did a few AWS certifications, and for that I was rewarded by AWS with two $300 credit vouchers which is pretty cool. I used them here for my private account to play around with a few services that I was interested in and as you can see here, you can see a list of which these credits can be applied to. And there are almost all services that are available for AWS. And what’s quite funny is, the AWS cost explorer that can also cost money when you are using the API is not listed here. So, for the billing services and the cost services you can’t use these credits.</p>
<h1 id="Cost-Explorer"><a href="#Cost-Explorer" class="headerlink" title="Cost Explorer"></a>Cost Explorer</h1><p>In this section, we will take a look on how to use the AWS Cost Explorer, one of the most essential tools for our Cloud Financial Management within AWS. Moreover, the Cost Explorer is the very main tool for you to gather information and analyze all the costs and expenses in your environment. So let me introduce you to the tool itself.</p>
<p>To get there, you can go over to your Billing Dashboard. From here, go to the Cost Explorer, and just launch it. See, you’re getting guided to a whole new menu where you can start to analyze your costs. The Cost Explorer is a built-in tool that lets you get deeper insights into the costs and usage of your cloud environment.</p>
<p>With the help of the Cost Explorer, you will be able to identify trends, hunt down the biggest cost drivers and detect anomalies. And the best part, the Cost Explorer, in its base variation, does not cost anything. You can use it for free. However, take note that there can be additional costs when you use tools to request the API of the Cost Explorer.</p>
<p>Every API request costs about one cent, which could drive up your costs very fast when you fire up a lot of API requests. With the Cost Explorer, you can visualize your usage patterns over time and identify underlying cost drivers.</p>
<p>What we see here is the very first view that you get when you open the Cost Explorer. It is grouped by the different services that you use. You get a six-month overview, and the visualization type is bar, which means that the costs are stacked to each other in these kind of bars here. You can also change this to stacks or to lines, what doesn’t make too much sense in this case. So let’s stick to the bar view.</p>
<p>The legend here below shows you which service is illustrated by which color. In this example, we can see over the last six months our EC2 usage has increased quite a lot. If you scroll down a little bit you can see the detailed data table. Below the Cost Explorer here is the data table, which provides a deeper insight into the data seen in your chart. And you can also export this as a CSV file for further processing with a table tool like Excel if you wanna make forecasts or like further analysis. </p>
<p>You can also like, like I said we have the last six months here, you can also like scroll vertically to see all the services and cost drivers and you can scroll horizontally to see each month. It’s basically just a table.</p>
<p>You can choose the exact timeframe you want to see within Cost Explorer. By default, it’s on six months. So you can either select day by day that you wanna see or you can choose to auto select here to see for example like just last seven days, current month, three months, six months, one year, months to date, year to date, or three months forecast or 12 months forecast.</p>
<p>Let’s go for the last seven days here. So we can see how much we spend for the day different services day by day. We can also delete the group by filter here and get like monthly view. It makes no sense because we wanna see days. So you’re gonna select daily here and then we can see how much we spend day by day, if there’s, for example, an anomaly. We can see, oh wow, like way more than the other days. Why is that?</p>
<p>You can see we are here January 1st the first day of the month. And on the first day of the months, you usually have to pay for tags, reserved instances, and all that stuff that incurred like more costs than over the other days. And that could be the explanation for this anomally here.</p>
<p>And you may have noticed you can also go from a daily to an hourly view, but it is deactivated. I can’t select it. Why is that? The reason for that is super easy. You have to pay for the hourly view because you need like much more data records. So you have to enable it first. If you really wanna go in that much detail. I will show you how you can do this.</p>
<p>Hourly view, you have to go to preferences. And this is the point we are looking for, the hourly and resource level data. We can select this and save it. Here you can also see the cost that would occur. And we can see here one cent per 1000 usage records per month.</p>
<p>It depends how much EC2 instances because this just applies to EC2. It depends how much EC2 instances you’re running. Of course, the more instances you’re running the more usage records you are going to record and the more costs you incur, but 1 cent per 1000 records, in this case, it’s not that much. </p>
<p>So we wanna save it here, go back to the Cost Explorer and we can see, oh, the hourly view is available. We can only see last 14 days of usage was the hourly view enabled. In this case because I just enabled it, I won’t see anything because the data needs to be recorded first. So it will take a minimum of four days for Cost Explorer to collect enough data that the hourly view is available here.</p>
<p>About data grouping and filter. We started with the group function, with the grouped by functionality, you can segment your data based on a particular cost or usage. For example, you can choose a region to see in what region you spent the most money on which regions you spent money.</p>
<p>Let’s go with a bar so we can see, let’s take December. We spent the most money in Ireland followed by others because others mean we have like a lot of data here. Well, let’s go to December. We’ve used many different regions here and all the regions that we use that are not shown here are combined into the others bar. If you want to stripe your chart down to make it more granular, you should use the filter function located on the right.</p>
<p>Using the filter panel. You can refine your data set to include or exclude specific filtering dimensions and values. For example, let’s say we wanna only see costs that occurred in Frankfurt. We go to the filter section here search for the Frankfurt region, select it, applied it, then we can just see the costs that applied for the Frankfurt region. And we can use like many combinations here to dive deep within our costs.</p>
<p>For example, if we just wanna see the costs for our EC2 instances, just filter here for EC2, for our compute instances. Nothing else just compute instances.</p>
<p>So just our literal service and we filter it by instance type and then we can see what instances occurred the most. So for example, here, we have a G4DN which is like a graphical, a big graphical instance that is quite expensive in this case.</p>
<p>As you can see here on the right, there are quite a lot of filters that you can use and no worries, we won’t look at all of them, but some of them are quite interesting. Let’s for example talk about usage type, usage types are the unit that each service uses to measure the usage of a specific type of resource.</p>
<p>For example, if I wanna know how long my t2 micro instances we are running in the past, I would just type in here, t2 micro, oops micro, select a filter here. And I can see my t2 micro instances were running 370 hours in July and about 200 hours in August. And you can use this for many other services as well.</p>
<p>Usage type groups also is pretty cool, these are the kind of type of filters that collect a specific category of usage type filter and put it into one. For example, if I wanna know how long all my EC2 instances were running in the past, I would just search for EC2 running hours and I could see the amount all my EC2 instances were running over the last six months.</p>
<p>Another one that is quite useful if you wanna go in more detail is the filter for API operations. So let’s take S3 as an example, we can see here. We don’t have that much S3 costs here, just in December $188. But if I wanna know more about these costs, how these costs are structured, I can look for specific API operations that apply to S3.</p>
<p>For example, if I want to know how much my costs were for like reading files from my S3 three buckets, I would look up GetObject because this is the API operation for reading objects from S3. I apply this filter here and it can see the costs for GetObject from S3. That’s it, that’s how you’re gonna use API operations.</p>
<p>As you can imagine, there are a lot of API operations that are used within AWS and I can highly recommend to have a look at API reference documentation for each service because you can see here, this is like just for S3, all the API operations that you can use for S3. And just by looking at them, you can, for most of them, you can already like kind of guess what they’re doing. Like for example, here, create object, create bucket and you can like just look forward for the API operation that fits for your kind of analysis.</p>
<p>A little bit deeper into Cost Explorer and the advanced options that can also be quite useful. For example, show only untagged resources. This is a pretty important filter because we talk about tags already a lot and little spoiler here we will talk about them way more because this is such a super, super, super important topic. And if you set this filter here you will see all the resources that have no tags attached, super important to see that.</p>
<p>What is also quite cool is the show costs as here. The unblended filter here will be the best for the vast majority of AWS customers. This is the cost dataset presented you on the bill page like for the bills and for the invoices. And it’s also the default option for analyzing costs using AWS Cost Explorer or setting the custom budgets using AWS budgets.</p>
<p>The unblended costs represent your usage costs on the day that they are charged to you or in finance terms they represent your costs on a cash basis of accounting. For most of you this is the only data set that you will ever need. Okay?</p>
<p>Amortized costs are also quite interesting because the amortized cost is useful in cases in which it doesn’t make sense to view your costs on the day that they were charged or as many of finance owners say it’s useful to view costs on an actual basis rather than a cash basis.</p>
<p>This cost dataset is especially useful for those of you who have purchased AWS or reservations such as reserved instances or savings plans. Savings plans and reservations often have upfront or recurring monthly fees associated with them.</p>
<p>Recurrent fees for reservations are charged on the first day of the month that can lead to a spike on one day if you’re using unblended costs as your cost dataset. When you toggle over to amortized costs, these recurring costs, as well as any upfront costs, are distributed evenly across the month.</p>
<p>Armotised costs are a powerful tool if you seek to gain insight into the effective daily costs associated with your reservation portfolio, or when you are looking for an easy way to normalize costs and usage information when operating at scale.</p>
<p>And then there are also like, two more that can be quite helpful. And these are the net unblended costs and the net amortized costs. These are basically the same as the two that I just explained here but they also include discounts like the reserved instance volume discounts. Like these discounts are calculated into these costs.</p>
<h1 id="Reports"><a href="#Reports" class="headerlink" title="Reports"></a>Reports</h1><p>Let’s talk about reports. Reports can be super, super, super useful. For example, in this case, I build a specific view for EC2 instances running in Frankfurt, in the EU region on a monthly base, like on a base for December, in this case. And because I wanna use this view more often, I can save it as a report. And to do this, I just click here on save as. I give this one a title, EU-EC2. I save it, and that’s it.</p>
<p>By clicking here, on recent reports, I can see this report. Or I can just go under report overview and see all the reports that I have here. The reports with a lock here are default AWS reports, and the one without a lock are my own reports.</p>
<p>Some of the AWS reports can be pretty useful. For example, what I use quite a lot is the RI utilization and coverage, or the savings plan utilization and coverage, that basically shows you how effective or efficient your reservations are.</p>
<h1 id="Cost-and-Usage-Reports"><a href="#Cost-and-Usage-Reports" class="headerlink" title="Cost and Usage Reports"></a>Cost and Usage Reports</h1><p>The AWS Cost and Usage Reports or in short the CUR. The CUR is basically the most important thing to capture your AWS billing data. And the CUR is a pretty complex CSV file that stores all details about your cost and usage data of all AWS resources.</p>
<p>Enabling the CUR is super important because it’s the most granular and detailed mechanism to collect data for AWS costs and usage. It offers historical by-the-hour data that can offer clarity on trends and lead to a more accurate data-driven insight. And there’s no looking back. Until the CUR is enabled, you’re losing valuable data about your usage that is older than 12 months.</p>
<p>The CUR can get really big and in large corporations, it can easily get beyond five gigabyte and more with millions over millions of lines. So let’s see how to enable them.</p>
<p>When we are here in the AWS Management Console, we click here on the top menu on the billing dashboard and you can see here on the left menu for the Cost and Usage Report. By default it’s disabled so you need to enable it, enable it first and create a report. We give it a name. Let’s call it test. I would advise you to include the resource IDs because then, every resource get a unique resource ID.</p>
<p>You can enable the automatic refresh. Click Next and then you need to choose an S3 bucket where you put the file. This can either be existing an S3 or a new one. Then we set a path, a prefix pass task costs. You can select the time granularity here. Of course, the more granular the data are, the more data you’re going to produce.</p>
<p>We can create new report versions or we can override the existing ones and we can choose what kind of data integration we need.</p>
<p>And a little side note here maybe. You can export the files for Redshift or QuickSight usage. This will change the output format of the file to be readable for either Athena or Redshift and QuickSight. See, Athena is Parquet and Redshift or QuickSight is the CSV file that comes in a, in a zip file.</p>
<p>With Athena, Athena is a serverless service that allows you to analyze the data stored directly in Amazon S3 using standard SQL. And for that you need, as I just mentioned, the Parquet file While with Redshift and QuickSight, you can manage to see a SWI file as you would do it also like for example, for Excel.</p>
<p>Redshift is a so-called data warehouse service which you would use for querying big data sets with like multiple gigabytes or even up to petabytes. It can help you take wide insights of your own environment and for customers on a very large scale. And QuickSight is a business intelligence service that can combine data from literally any source into a dashboard. It helps you to visualize for any type of audience and it is much more visually driven than Athena or Redshift.</p>
<p>After you set up all these options here, you can click on Next and that’s basically it. You have now configured your Cost and Usage Report. But be aware that it may take up to 24 hours for the first report to be delivered. Also, expect some costs from S3 for storing the CUR data in your S3 bucket. But these costs are like very, very low. Maybe like, I don’t know, a few dollars per months or per year. Depends on how big your file gets of course.</p>
<h1 id="Budgets"><a href="#Budgets" class="headerlink" title="Budgets"></a>Budgets</h1><p>In this section, we will learn how to create a budget, to help manage running expenses. Budgets allow the user to get notified when costs or usage exceed a certain predefined amount. So let’s have look how to set up automatic notifications and actions with AWS Budgets.</p>
<p>So we get there by clicking here on the top menu on the billing dashboard. Here on the left, we have budgets, we click on it, and we can see that there is already a budget predefined here. I set this up in the past to get notified if my monthly budget threshold gets over $150 but let’s create a new budget.</p>
<p>So we can select four different kinds of budgets in this case or like in general. We have cost budgets based on actual costs, usage budgets based on usage like ours and budgets for reserved instances and savings plans. So let’s start with the cost budget.</p>
<p>So first we need to give it a name and we have to select a period in that we want to be notified. We will keep it here on a monthly period and we can also choose if this is a recurring budget or an expiring budget. Does that goes, for example, like just till April but we will stay with a recurrent budget.</p>
<p>So you can either choose if you want to set a fixed budget. Like for example, last month I had costs of $31. I can set this to $35 ‘cause this is quite close. And if I would like reach this threshold here I would get notified. You can also set a monthly budget planning.</p>
<p>For example, let’s say you have a snowboard rental and you know, there won’t be much users on your platform in the warmer month, like for example, in April till October. So you could set the budget here to like just 100 bucks. But you know, in the cooler months when there’s actually actually snow, you will have much, much more users. So you could set a budget here to 1000 bucks but let’s keep it simple and go with a fixed budget.</p>
<p>You have fear like also many other photos that you also know from the cost explorer demo. The last demo that I just showed you to get more information about the costs you had in the previous months. But let’s configure the thresholds.</p>
<p>You can define your budget thresholds and you can set it either to the actual costs like the cost that like actually occurred to a percentage, like an alert threshold for example, like 80%. So in this case, you would get an alert if 80% of the budget that we just defined in the last step, if we reach this threshold. That will be $28 in this case. And you could also set it based on forecast at cost but this is getting too complex.</p>
<p>Let’s keep it with the actual cost. So here, can you set up the notifications. I can like type in here, my email address and whenever I would reach the threshold I would get an email an alarm that I reached this threshold. And since October 2020, it is also possible to set different kinds of triggers for actions like budget actions. These are based either on identity and access management policies, service control policies or you can also target running instances like EC2 or RDS.</p>
<p>For example, you can choose to apply a custom denied EC2, run instance IAM policy to a user, to a group or to a role in your account once your monthly budget for EC2 has been exceeded. With the same budget threshold, you can configure a second action that targets specific EC2 instances, using a particular region. You can choose to execute actions automatically or make use of a workflow approval process before AWS Budgets execute a request on your behalf.</p>
<p>It’s possible to set up five budget thresholds with up to 10 actions for each threshold. IAM and SEP action type reset at the beginning of each budgeted period. Like in our case, monthly while actions target at a specific EC2 or RDS running instances will not reset.</p>
<p>So we’ve clicked here on the budget actions we activated it, and now we can choose an IAM role that allows budget actions to actually do something with the instances that we are going to define here. So let’s just take this open access role that have defined earlier and we can also choose what should happen.</p>
<p>We can say here if our threshold reaches the budget that we just defined, just stop all EC2 or RDS instances. This is of course, like quite radical step but if you are on a budget, well, you have to do what you have to do, right? So when you can choose if you want to stop EC2 or RDS instances, and you can also select a specific region where this should happen. And if I would click here on “Confirm budget,” the budget would be set. And if the threshold is reached, I would get an email and my EC2 instances would be stepped.</p>
<h1 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h1><p>The essence of a cost allocation strategy is the ability to tell how much is spent on which resource on which service. This type of visibility can be best achieved by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/introduction/">tagging</a> every single resource in your cloud. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> enables the user to put tags on every available resource. You can use tags for many things. But for this course, we’re just going to focus on how to use them for cost allocation. So let’s find out what tags actually are. </p>
<p>Tags provide the functionality to define metadata in the form of key and value pairs. These on the other hand are associated with the resources in a cloud account. Let’s have a look at this diagram. In this example, we’re looking at four resources. Don’t mind the details because it doesn’t really matter what kind of resources they are. These are just some key samples. So each of them has a tag, which goes by the key-value environment production. This one is true for every four of them. This means that all of these resources belongs to the production stage of our environment.</p>
<p>The next one distinguishes our resources between the frontend and backend, and basically tells us right away which resources host a front-end service and which hosts a back-end service. Typically, business tags such as cost center, business unit, or project are used to associate AWS costs with traditional financial reporting within an organization. However, a cost allocation report is not static and hence can include any tag. This allows customers to easily link costs to technical or security dimensions, such as specific applications, environments, or compliance programs.</p>
<p>With AWS Cost Explorer and Cost and Usage Report, AWS costs can even be viewed according to tags, providing even more insightful cost visualizations. AWS Cost and Usage report is otherwise known as AWS CUR. Just so you know what the Cost and Usage Report is, I wanted to drop in a short explanation. With the help of AWS Cost and Usage reports, you can track the monthly AWS costs and usage associated with your AWS account. The report includes items for each unique combination of product, usage type, and operation that is used in your AWS environment. It enables you to configure the AWS Cost and Usage report to show only the data that you want, using the AWS Cost and Usage API.</p>
<p>AWS Cost and Usage Reports contain the widest variety of cost and usage data. You can set up the CUR to collect billing data for any given period and push it into an Amazon S3 bucket to store it there for whenever you need it. You can get hourly, daily, or monthly reports. These contain the costs in detail and are sorted by product or resource. If tags are used properly, they are also listed in the report and can provide extra detail to your bill. A report is updated at least once per day and up to three times. They can be stored in an S3 bucket of your choice and be retrieved whenever needed, either manually or by using another service. </p>
<p>After you set up a cost and usage report, you receive the current month’s billing data and daily updates in the same Amazon S3 bucket. The data from the CUR forms the base for a detailed and complete cost analysis. It is often the main part for many business intelligence tools, like Athena and QuickSight, just to name a few. And CUR can also be reached by an API, which you can use for your custom scripts or individual needs.</p>
<p>So, in conclusion, with AWS CUR, you are able to store your report files in Amazon S3 buckets, update the report automatically, up to three times a day, make use of the AWS CUR API for automation or easier management through API calls, and use the CUR for in-depth analysis with business intelligence tools like QuickSight, Athena, and others. That’s about it for the AWS Cost and Usage Reports, so let’s continue. </p>
<p>Keeping cost allocation in mind, the best way to assign business-context details to specific resources is by using tags. Later on in the process, this enables you to carry out a more valuable analysis based on your cost data and facilitates company-specific decision making by a well-evaluated foundation of data. If you take bill analysis into consideration, tags can add business dimension and context to ease the allocation process. </p>
<p>Tags are used to identify which item or resource in your cloud is attributed to each of your business services. So you can always tell exactly which resources are used for which service in your company. Nevertheless, keep in mind that tags are only meaningful to their respective user or a customer. They literally do not have any semantic meaning. You can name them whatever you like and assign a value to them. However, when used correctly, they can help read and analyze your data and even automate your analysis with the right setup.</p>
<p>In AWS, you can manage tags in the service console or accessing the API through AWS CLI, although this limits you to only one resource at a time. If you want to add, edit, or delete tags on multiple resources, it is best to use a service, for example, the AWS Tag editor. Once you have tagged your resources, you can enable Cost Allocation Tags in the Billing and Cost Management sections. We will discuss how to tag and activate the Cost Allocation tags in a minute. One significant thing to note here is that tagging existing resources retroactively is pretty annoying. So make sure to tag your resources from the very beginning.</p>
<p>In the best case, policies can prohibit the deployment of new resources without the appropriate tags. But more about that later. If you want to analyze a cost report after the fact with unlabeled or poorly labeled resources, you will have a hard time understanding the exact usage of each resource, and will likely not be able to identify the exact costs and usage by resource. So, it is advised to start tagging resources as soon as possible and stay consistent with your tagging strategy.</p>
<p>Planning out a tagging strategy or a tagging standard is essential, and the best time to implement one is before a company launches its cloud resources. It’s best to keep tagging simple and easy to grasp. Don’t overdo it for the sake of it. After all, you want to gain visibility, not cause confusion. The best thing to do is to learn about predefined tags and choose the ones suitable for your business. It’s also advisable to adjust your tags to follow your KPIs once you determine them.</p>
<h1 id="Tagging-Best-Practices"><a href="#Tagging-Best-Practices" class="headerlink" title="Tagging Best Practices"></a>Tagging Best Practices</h1><p>We’ve picked a few best practice examples for you to apply for your business or organization. Let’s start with some common tags that are used by most organizations. Of course, these are just some ideas and you need to use tags that fit your business case. Some common examples include Cost Center or Business Unit tag, used to show where resource costs are allocated within the organization, and it also allows correct cost allocation within billing data.</p>
<p>Service&#x2F;Workload name tag. This shows which service the resource belongs to. Resource Owner tag. This is responsible for the resource. Simple Resource Name tag. This is something easier to read and to remember than the default tags. And Environment tag. It determines the cost difference between different environments. For example, dev, test&#x2F;stage, production. Check your cloud and see whether these tags can help you get started with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/tagging/">tagging</a>. Also make sure to check <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> pre-generated tags. They might save you some time.</p>
<p>Now let’s look at some tagging best practices. So, number one, align tags to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/aws-generated-cost-allocation-tags/">cost allocation strategy</a>. Before you start tagging, you should think of a general cost management strategy. Think of tags that help you to track and allocate expenses and make those tags align with your strategy. Next, tag everything. Tag as many resources as possible so that no resource is left untagged. Make this a rule. In fact, you can roll out policies in your cloud environment that will forbid launching resources without tags.</p>
<p>Next, find a purpose for each tag. Think of a certain use case before adding a tag. Otherwise you will have a hard time justifying your tags and you risk running into a mess of baseless tags. That now leads me onto the next point. Limit the number of tags you adopt. Find redundancies and overlapping tags and simplify them. There’s no point in releasing multiple tags that cover the same subject. Look for tags that might logically overlap. See where you might merge them and reduce the number of your overall tags. And keep it manageable. Obviously, the more tags you have, the more tags you have to deal with. Keep the number as low as necessary, but the information value as high as possible.</p>
<p>Next, consistency is key. Use a consistent naming convention. This helps to keep an overview and eases further processing. Giving your tags less abstract names, and instead naming them with descriptive terms also makes them easier to read. Automate tag management. Make use of tools like the AWS tag editor to automate your tagging. Avoid wasting time on repetitive tasks and use automation as much as possible. Set up policies to forbid launching untagged resources. This is an easy way to ensure that no new resources are slipping into your environment without a tag.</p>
<p>And finally, audit and maintain your tags. Make it a habit to review tags from time to time and verify their purpose. Tag maintenance is essential and should involve everyone on the team. So make it a recurring task for everyone and have everyone keep their eyes open for suggestions for improvement.</p>
<h1 id="AWS-Generated-Cost-Allocation-Tags"><a href="#AWS-Generated-Cost-Allocation-Tags" class="headerlink" title="AWS-Generated Cost Allocation Tags"></a>AWS-Generated Cost Allocation Tags</h1><p>The default tags in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> are basically cost allocation tags. They can be activated in the billing section, which we will explain in a minute. Cost allocation tags are special tags that are used by Cost Explorer and other services for allocation and visualization. So they can be explicitly used and displayed in the various views of various services. So take a look at the screenshot. Here we can see the Cost allocation tags section in AWS Billing. We can tell that CostCenter and Name are active and enabled as cost allocation tags, while the others are inactive. These inactive tags were once set for resources that are not in use any longer.</p>
<p>Once the cost allocation tags are activated, the console detects all tag keys used and suggests those for activation as cost allocation tags as well. Otherwise, they won’t show up in Cost Explorer charts. The term User Defined means that a user created these tags and that they are therefore custom tags. AWS generated tags were automatically generated, such as createdBy, the createdBy Amazon WorkSpaces Tag. These were created and applied to support AWS resources for the purpose of cost allocation. They can only be view in the AWS Billing and Cost Management console and reports. They do not appear anywhere else in the AWS console, including the AWS Tag Editor.</p>
<p>Also, note that in this Cost allocation tags section, all the tag keys that are and were used in the account will be shown here. However, if you don’t enable the Cost Allocation Tags option, you will not be able to evaluate certain views in the Cost Explorer and other services. For example, if I want to know what costs have been incurred for all resources with a specific cost allocation tag, then I won’t be able to select or see them.</p>
<p>To prevent you from failing to find your resources with those cost allocation tags, I will show you how to enable this option. Let’s see how you can enable AWS generated Cost allocation tags. First, you need to log in to your Master account as an IAM user with the required permissions. Next, type Billing into the search field and go to the Billing console. Select Cost Allocation Tags on the left side menu. And then, simply click on Activate to enable the tags.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-IAM-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-IAM-24/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Introduction-to-IAM-24</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:53" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:53-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:42" itemprop="dateModified" datetime="2022-11-20T19:08:42-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-IAM-24/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-IAM-24/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-What-is-AWS-KMS-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-What-is-AWS-KMS-23/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-What-is-AWS-KMS-23</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:51" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:51-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:50" itemprop="dateModified" datetime="2022-11-20T19:08:50-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-What-is-AWS-KMS-23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-What-is-AWS-KMS-23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:50" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:50-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:20" itemprop="dateModified" datetime="2022-11-20T18:58:20-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Security and Compliance in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce some Security and Compliance services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#117;&#x70;&#112;&#x6f;&#114;&#x74;&#x40;&#x63;&#108;&#x6f;&#x75;&#x64;&#x61;&#99;&#97;&#x64;&#101;&#x6d;&#121;&#x2e;&#99;&#111;&#x6d;">&#115;&#117;&#x70;&#112;&#x6f;&#114;&#x74;&#x40;&#x63;&#108;&#x6f;&#x75;&#x64;&#x61;&#99;&#97;&#x64;&#101;&#x6d;&#121;&#x2e;&#99;&#111;&#x6d;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Security and Compliance services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to various Security and Compliance services in AWS and to introduce important security concepts, including:</p>
<ul>
<li>Finding compliance data with AWS Artifact;</li>
<li>Managing users, groups, and roles with AWS Identity and Access Management, or IAM; and</li>
<li>Evaluating the security of your AWS environment using AWS Trusted Advisor.</li>
</ul>
<p>We’ll also introduce the AWS Web Application Firewall, or WAF, which, along with other services such as the AWS Firewall Manager and AWS Shield, can help you create a comprehensive security solution for your web applications.</p>
<p>These objectives are covered by Domain 2 in the official AWS Certified Cloud Practitioner exam blueprint: Security and Compliance, which accounts for 25% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#x73;&#117;&#112;&#x70;&#111;&#114;&#116;&#64;&#x63;&#x6c;&#x6f;&#x75;&#100;&#97;&#x63;&#x61;&#x64;&#101;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;">&#x73;&#117;&#112;&#x70;&#111;&#114;&#116;&#64;&#x63;&#x6c;&#x6f;&#x75;&#100;&#97;&#x63;&#x61;&#x64;&#101;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;</a>. Thank you!</p>
<h1 id="Finding-Compliance-Data-With-AWS-Artifact"><a href="#Finding-Compliance-Data-With-AWS-Artifact" class="headerlink" title="Finding Compliance Data With AWS Artifact"></a>Finding Compliance Data With AWS Artifact</h1><p>Hello, and welcome to this lecture where I will be examining AWS Artifact, a free self-service portal that provides you with immediate access to AWS security and compliance reports. Within AWS Artifact, you also have the ability to view, download, accept, and terminate legal agreements between you and AWS at both the account and organization level.</p>
<p>So you may be asking yourself: why would I ever need to access the information in AWS Artifact? And as it turns out, there could be several reasons. For starters, you might be asked to provide evidence of the current or historical compliance of different AWS services used within your architecture as part of a required audit to ensure that your enterprise may continue to leverage the AWS cloud. And this audit could potentially extend out to include your suppliers as well. Or perhaps you just want to learn more about your responsibilities when it comes to complying with various regulatory standards such as Payment Card Industry, or PCI, or Service Organization Control, or SOC. After all, simply leveraging the AWS cloud does not guarantee that the systems you build within it will be fully secure or compliant. We’ll discuss this more in a moment.</p>
<p>AWS Artifact can be accessed directly from the AWS console by searching “Artifact.” From there, the AWS Artifact home page gives you options to view reports and view agreements, so let’s spend a little time discussing reports and agreements in more detail.</p>
<p>AWS Artifact Reports consist of AWS auditor-issued reports and include everything from ISO certifications to PCI and SOC reports.</p>
<p>These reports, known as audit artifacts, may be shared with auditors and regulators by creating IAM users with an associated identity-based policy that grants access only to the necessary reports. And these audit artifacts allow you to provide evidence of AWS security controls to ensure compliance with any applicable governance, regulations, or frameworks when architecting solutions in the AWS cloud. Now of course this is always done in accordance with the AWS Shared Responsibility Model, where AWS is responsible for the underlying security OF the cloud, but you remain responsible for your own systems’ and applications’ security IN the cloud. Now to learn more about the AWS Shared Responsibility Model, I encourage you to check out this resource. Consequently, the compliance reports provided within AWS Artifact pertain only to AWS and do not in any way certify the security or compliance of your own company, organization, or application. However, these audit artifacts can and should inform the security controls you choose to implement as part of your own cloud architecture and solution design.</p>
<p>In addition to security and compliance reports, AWS Artifact also allows you to view and execute legally binding agreements between you and AWS.</p>
<p>These agreements can be applied at the individual account level, or if you are signed in to the AWS console with the management account of an organization in AWS Organizations, you can also apply an agreement to all member accounts within your organization. One example of a commonly used agreement is the AWS Business Associate Addendum, or BAA, which governs your use of AWS services when storing personal health information, or PHI.</p>
<p>To accept an agreement, you must first accept the AWS Artifact non-disclosure agreement or NDA.</p>
<p>After you have accepted this NDA, then downloaded and reviewed the agreement, you may accept the agreement by checking a box acknowledging that you accept all of its relevant terms and conditions. Note that when accepting an agreement on behalf of all member accounts within an AWS Organization, you must also certify that you have the full power and authority to accept the agreement on behalf of every entity that either currently has, or may ever subsequently have, a member account within your organization at any point in the future.</p>
<p>So that’s how we can use AWS Artifact to not only view compliance reports and agreements but also to help ensure the solutions we architect in the AWS cloud remain secure and compliant with all necessary rules and regulations.</p>
<h1 id="What-is-Identity-and-Access-Management"><a href="#What-is-Identity-and-Access-Management" class="headerlink" title="What is Identity and Access Management?"></a>What is Identity and Access Management?</h1><p>Hello and welcome to this lecture where I shall provide an overview of what the Identity &amp; Access Management service is, and what IAM actually means.</p>
<p>Firstly I want to define what is meant by Identity &amp; Access Management and I shall break this down into two parts, starting with Identity Management. </p>
<p>Identities, such as AWS usernames are required to authenticate you to your AWS account, and this authentication process is managed in 2 stages.</p>
<ol>
<li>The first part of this process is to define who you are, effectively presenting your identity, so for example your AWS username. This identification is a unique value within IAM for your account, so this means IAM would prevent you from having 2 identical user accounts with the same name within the same AWS account.</li>
<li>The second part of the authentication process is to verify that you are who you say you are. This is achieved by supplying additional data, and when using our AWS usernames we can verify this by supplying a password</li>
</ol>
<p>Now, Access Management relates to authorization and access control. Authorization determines what an identity can access within your AWS account once it’s been authenticated to it. An example of this authorization would be the user’s list of permissions to access specific AWS resources, for example, they might have Full Access to EC2 or Read Only to RDS.</p>
<p>Access Control can be classed as the mechanism of accessing a secured resource. For example, using the following:</p>
<ul>
<li>Username and password (Authentication and Verification)</li>
<li>Multi-Factor Authentication (MFA, used as an additional verification step following a valid password)</li>
<li>Or Federated Access, which allows users external to AWS to access resources securely without having to supply AWS user credentials from a valid IAM user account. Instead, these credentials are supplied from identity providers. For more information on Identity Federation, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/">https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/</a></li>
</ul>
<p>So essentially IAM can be defined by its ability to manage, control, and govern authentication, authorization, and access control mechanisms of identities to your resources within your AWS Account.</p>
<p>Having an understanding of the different security controls from an authentication and authorization perspective can help you design the correct level of security for your infrastructure.</p>
<h1 id="IAM-Features"><a href="#IAM-Features" class="headerlink" title="IAM Features"></a>IAM Features</h1><p>It’s critical to understand how IAM works and what can be achieved via the service, but it’s even more important to know how to implement its features. Without IAM, there would be no way of maintaining security or control of who or what could access your resources and what they could do with them, both internally and externally. IAM provides the components to maintain this management of access, but it is only as strong and secure as you configure it. </p>
<p>The responsibility of implementing secure, robust, and tight security within your AWS account using IAM is yours, you are the owner of the AWS account. You must define how secure your access control procedures must be, how much you want to restrict users from accessing certain resources, how complex a password policy must be, and if users should be using Multi-factor authentication. All of this is and much more is down to you to architect and implement, and much of it will likely depend on your own security standards and policies within your Information Security Management System (ISMS).</p>
<p>In this lecture, I want to talk about some of the different features and components that AWS IAM uses to centrally manage and control security permissions for any identity requiring access to your AWS Account and its resources. </p>
<p>From with the AWS Management Console, the IAM service can be found under the ‘Security, Identity &amp; Compliance’ category, and when accessed it will take you to the IAM Dashboard. </p>
<p>The initial dashboard of the IAM Console will display the following information.</p>
<p>This is a URL link that you can send to users who you will need to gain access to your AWS Management Console. This sign-in link can be customized by clicking on the ‘customize’ button to make it easier to remember and read. If you have multiple AWS accounts, this customization would help to distinguish between your accounts</p>
<p>IAM Resources. This section provides a summary overview of your IAM resources using a simple count of the number of users, user groups, roles, customer-managed policies, and identity providers you have configured within IAM. </p>
<p>Best Practices. This is populated with a list of IAM security best practices that AWS recommends you implement with links on how to implement them. I strongly recommend you try to adopt these best practices at your earliest opportunity. Maintaining tight security is paramount with working within a cloud environment.</p>
<p>In addition to this dashboard, you will also see 2 categories on the left menu, Access Management and Access Reports each listing a number of components underneath. I want to start by reviewing each of these components under Access Management to give you an insight into what each are used for, starting with Users.</p>
<p>User objects are created to represent an identity, this could be a real person within your organization who requires access to operate and maintain your AWS environment, or it could be an identity that is used by an application to interact with your AWS resources programmatically.  Users are simply objects representing an identity which are used in the authentication process to your AWS account. Being unique values, every user has an Amazon Resource Name, an ARN which it can be referenced by. An example of a user’s ARN could be as shown.</p>
<p>When configuring your Users you can set them up for Multi-Factor Authentication. Configuring MFA allows for an additional level of verification to be applied, the user will have to enter a random 6 digit number from a linked MFA device after their usual password. MFA should be used for the AWS Account owner and any other users who have elevated privileges. </p>
<p>IAM USer Groups are objects much like user objects, however, they are not used in any authentication process, instead, they are used to authorize members of the group access to AWS resources.</p>
<p>So, the IAM USer Groups contain IAM Users, and these groups have IAM policies associated that will either allow or explicitly deny access to AWS resources. These policies are either pre-existing AWS Managed policies, customer-managed policies that are created by you, the customer, or in-line policies which are embedded explicitly to the group itself.</p>
<p>IAM Roles allow Users, other AWS services, and applications to adopt a set of temporary IAM permissions to access AWS resources. Roles essentially operate the same as User objects do in that it’s an identity with associated permissions to allow it access to different resources. The difference being however is that roles don’t define a single person, they are designed to be assumed by any identity or service that needs to temporarily acquire a set of permissions. Additionally, roles don’t have passwords associated with their identities, instead, like I just mentioned, roles are assumed as long as you have the correct permissions to assume it.</p>
<p>Policies used within IAM are written as JSON documents and these define what can and can’t be accessed. These policies can be attached to Users, User Groups, or Roles. </p>
<p>When working with policies, you can use Managed policies or In-line policies. Managed policies are viewed as a library of usable policies and come in 2 different flavors which can be applied to multiple Users, User Groups, and Roles:</p>
<ol>
<li>AWS Managed Policies: These are a list of predefined policies granting varied access to different AWS services</li>
<li>Customer Managed Policies: These are policies created and written by you as the customer</li>
</ol>
<p>Unlike Managed policies, Inline policies are not stored in a library, instead, they have to be written and explicitly embedded within a User, User Group, or Role, as a result, the same policy can’t easily be applied to another identity like Managed policies can.</p>
<p>If you are looking to provide federated access to your AWS resources, then you must add an identity provider. </p>
<p>Federated Access allows credentials external to AWS to be used as a means of authentication to your AWS resources. For example, you could establish a trust between your Active Directory Federation server on premises and your AWS account. Or, you could establish a trust between your AWS account and Google account. Either way, in these instances your ADFS server or Google account could be configured as Identity Providers allowing those authenticated by Active Directory or Google to access your resources in your AWS account. This prevents you from having to create individual IAM user accounts.</p>
<p>Under Account Settings, you can enforce a password policy, and it’s always best practice to do so. The Password policy should be used to enforce the minimum security requirements that need to be met for any password standards that your organization might have to adhere to. The password policy applies to all IAM users within your account, and as you can see here, you can be very specific by enabling&#x2F;disabling and configuring different controls.</p>
<p>The STS service is used to allow you to request temporary, limited-privilege credentials for both IAM users and federated users. The STS endpoints provide a list of Regions that are either activated or deactivated for STS. By default, all Regions are activated, however, it is recommended you deactivate the regions you do not intend to use. </p>
<p>By default STS uses the global endpoint of  <a target="_blank" rel="noopener" href="https://sts.amazonaws.com/">https://sts.amazonaws.com</a>, however, when using regional endpoints it can help to reduce latency.</p>
<p>So that has given a quick overview of the component covered by Access Management, I now want to look at the features that come under the category of Access Reports, starting with Access Analyzer</p>
<p>Access Analyzer. This is used to generate findings when a policy on a resource within your zone of trust allows access from outside your zone of trust. So this could be from a number of different sources, such as an IAM role allowing cross-account access, or a Bucket that allows a different account to upload objects into the bucket, basically any resource that allows external access to your resources will be flagged and highlighted to ensure you are aware of the access. This is a great tool to help reduce security risks and threats as you or your team may have unintentionally allowed access to a resource you shouldn’t have. Any issues are recorded by Access Analyzer as a finding allowing you to review the access </p>
<p>The credential report is a great tool that allows you to generate and download a *.csv file containing a list of all of your IAM users and their credentials. This provides a quick and easy way to review your accounts and the last time they were used, in addition to identifying when a user’s password was last changed and if they have Multi-Factor Authentication enabled.</p>
<p>If using AWS Organizations, it allows you to Select an organizational unit (OU) or account to view its service activity over the previous 365 days. By drilling down into the accounts in this section you can see which users have had activity, in addition to which AWS services they have accessed over a given time frame. </p>
<p>Service Control Policies. This links in with the previous component, ‘Organization Activity’, and lists any Service Control Policies that are applicable to the account and the number of identities affected. Service Control Policies, or SCPs, are different from identity-based policies which grant permissions to users, user groups, and roles as SCPs do not actually grant permission themselves. Instead, SCPs are used with AWS Organizations to implement and set a boundary of permissions for AWS accounts. </p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS, and EC2 via an identity-based policy in IAM. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2, despite having full access to S3. The SCP would prevent that service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<h1 id="Overview-of-the-User-Dashboard"><a href="#Overview-of-the-User-Dashboard" class="headerlink" title="Overview of the User Dashboard"></a>Overview of the User Dashboard</h1><p>Hello and welcome to this lecture where I’m going to review the dashboard of the user Console. In IAM, we can create user objects to represent an identity with long-term credentials, this could be a real person within your organization who requires access to operate and maintain your AWS environment, or it could be an account to be used by an application that may require permissions to access your AWS resources programmatically. Users are simply objects representing an identity which are used to authenticate to your AWS account with an associated set of permissions.</p>
<p>When you open the AWS IAM dashboard and select users, you will be presented with a screen similar to the following. This screen provides a summary of the key points of interest relating to your IAM users and it can quickly help you identify any potential security risks that you might encounter. As we can see, we have a number of different warnings on this screen in different colors. We have a green tick which symbolizes activity was measured within the last 90 days An amber exclamation mark showing activity was last measured between 91 and 365 days ago. And a red exclamation mark, which highlight anything older than 365 days ago. Anything either amber or red should be looked at and addressed as a priority to reduce potential security issues.</p>
<p>Let’s take a closer look at this table and its columns to understand exactly what it’s trying to show you. The complete list of field names are shown here, and you can toggle them on an off as you need. Let me go through each of them one-by-one to explain what they represent. So username. This column is self-explanatory, it provides the username of that user.</p>
<p>Path. If you create your users using the IAM API or the AWS Command Line Interface, the CLI, then you can specify a path structure for your users. This is useful when you have a large organization and you want to define a management structure to help you organize your users more effectively.</p>
<p>For example, you could add a path for a user Stuart of &#x2F;ContentTeam&#x2F;AWS&#x2F;UK&#x2F;Stuart. This would help to define where your users might be in accordance with your organization structure. In addition to this, you can also reference paths in your access policies, so for example you could allow all users in the &#x2F;ContentTeam&#x2F;AWS&#x2F;UK&#x2F; path to have full access to S3 with a simple policy like this.</p>
<p>Groups. This shows the different groups that the user belongs to. In the screenshot, you can see that the user Stuart belongs to the Admin group. Last activity. This column is really useful to show the last time your users logged into the AWS Management Console or if they accessed any of your AWS resources programmatically using their access keys. As we can see from my example, the user Alice hasn’t accessed anything for 467 days, as a result I can say with some confidence that this user can be removed and deleted from IAM. So it’s a great way to ensure that you don’t have any unused user accounts, and deleting unused accounts is an IAM best practice.</p>
<p>MFA. MFA stands for Multi-Factor Authentication, and so this will highlight any users that have been configured with MFA which adds an additional level of verification, enhancing security. The users in my example that show virtual mean that these users are using MFA on a virtual device, and in this example it’s the Google Authenticator app on a mobile phone. For those unfamiliar with MFA, let me give a brief explanation of what it is. Typically, when a user logs into the AWS Management Console, they will authenticate to your AWS account by providing their identification, typically their username, and then verify this identification, usually with a password. These two elements, the identification and verification, allow the user to authenticate. For many cases, the verification, the password, is sufficient enough to confirm the identity, the username. However, for users who have an elevated level of privileges, for example the users may have full access to a large number of AWS services, then you may want to, or be required to due to governance controls, implement an additional verification step within the authentication process. This adds another layer of security attached to the identity, and this is where Multi-Factor Authentication comes in, MFA. The name explains itself, it’s used to create an additional factor for authentication in addition to your existing methods, such as a password. Therefore creating a multi-factor level of authentication. So what is this additional level of security that MFA brings with AWS? MFA utilizes a random six-digit number that is only available for a very short time period before the number changes again which is generated by an MFA device. There is no additional charge for this level of authentication, however, you will need your own MFA device, which can be a physical token or a virtual device. AWS provides a summary of all supported devices here. Personally, I use Google Authenticator on my phone because it is simple and easy to set up and configure. Password age. This shows how long ago it was until the password of the user was changed. As a security best practice, your passwords should be changed regularly.</p>
<p>Console last sign-in. This column simply shows the last time a user signed in to the management console.</p>
<p>Access key ID. The access key ID will identify if it’s active or inactive and will display the key in use. Before I continue, let me just quickly explain a little more about Access Keys. Access Keys are used for programmatic access to your AWS resources, and they are comprised of two elements. An access key ID and a Secret Access Key ID The Access Key ID is made up of 20 random uppercase alphanumeric characters, such as the one shown here. The Secret Access Key ID is made up of 40 random upper and lowercase alphanumeric and non-alphanumeric characters, such as the one displayed on the screen. During the creation of a user who requires programmatic access, you are prompted to download and save these details and it’ll only be displayed once, and if you lose it, then you will have to delete the associated Access Key ID and recreate new keys for the user. It’s not possible to retrieve lost Secret Access Key IDs as AWS does not retain copies. These access keys must then be applied and associated with the application that you are using that requires the relevant access. For example, if you were using the AWS CLI to access AWS resources, you would first have to instruct the AWS CLI to use these Access Keys to authenticate you providing authorization. The method of performing this association varies based on the application and system that you are using. However, once this association has taken place it ensures that all API requests made to AWS are signed with this digital signature.</p>
<p>Active key age. The active key age refers to the age of the access keys that are associated with the user which, as I explained, are required for programmatic access. Access key last used. This column just shows the last time that the user used their Access Keys within your AWS account. As a security best practice, it’s a good idea to remove any access keys that haven’t been used for quite some time. This removes possible entry points for malicious users to gain access to your environment.</p>
<p>The ARN. The ARN, or Amazon Resource Name provides the unique identifier for referencing that particular user. For example, the one showne here for the user Stuart. Creation time. This is a simple time metric to show you how long ago the user was created Console access. The console access helps you to determine which users have access to the Management console. When creating a new user, you have the option to configure their access type as seen here. Each user can be configured to have Programmatic Access, or AWS Management Console Access, or both. If they have access to the Console, then this column will show as Enabled for the user and Disabled if the user does not have console access.</p>
<p>Signing certs. Finally, the last column of signing certs will show if the user has a Signing Certificate or X.509 Certificate associated with them for secure access to certain AWS product interfaces. For example, EC2 uses a Signing Certificate for access to its SOAP, Simple Object Access Protocol, and command line utilities. Finally, before we finish this lecture, you can also create and delete users as required, and search for any of your users via their username or access key from within the user dashboard.</p>
<h1 id="Creating-IAM-Users"><a href="#Creating-IAM-Users" class="headerlink" title="Creating IAM Users"></a>Creating IAM Users</h1><p>When creating a new user, you have the option to create it via the AWS Management Console or programmatically via the AWS CLI, Tools for Windows PowerShell, or using the IAM HTTP API. For this lecture, I should be using the AWS Management Console to demonstrate how to configure Users. User object creation is a simple process. Firstly, you will set the user details by creating a username, which can be up to 64 characters in length. Next you’ll select the AWS access type, either AWS Management Console access or programmatic access.</p>
<p>For programmatic access, an access key ID and secret access key ID will be issued to be used with the AWS CLI, SDKs or other development tools. If console access is required, you will need to define a console password for the user. Permission assignment through the use of policies can be attached to the user or inherited from a group that the user can be assigned to. And permission boundaries can also be applied to the user, controlling their maximum permission level. You can assign any tags to the user as you would with any other AWS resource. And then you must review and confirm the information that has been submitted before you create the user.</p>
<p>Once the user is created, you can download the security credentials via a CSV file. And that will contain the username, access keys required for programmatic access and the console login link. So let me now jump into the console to demonstrate how to create a new user. Okay, so I’ve just logged into my AWS Management Console. And the first thing I want to do is to go to IAM. And that can be found under the Security, Identity and Compliance category.</p>
<p>So if we select IAM, and that will take us straight to the IAM dashboard. And this is where we can start creating our users and groups and roles, et cetera, and anything else that we need to manage within IAM. So to create a new user, I need to go across to Users on the left here. And then from here, I can select Add users.</p>
<p>Now the first thing I need to do is to create the username of the user. So I’m going to call this user Patricia. And then we can select the access types. So we have the programmatic access here, all the AWS Management Console access here. So for this user, let me add both. So I want them to have programmatic access and also AWS Management Console access. So I’m going to select both. So for the AWS Management Console access, we need to enable a password so we can either have IAM auto-generated password or I can select my own. And for this demonstration, I’m going to add in my own password.</p>
<p>Now if you tick this option here, when the user signs in, they will be asked to generate their own password once they’ve used your initial password to login. And that’s a great idea, just to enhance security there. So if we click on Next, then we can assign permissions. And here we have a couple of options so we can add the user to an existing group. We can copy permissions from another user or attach existing policies directly to the user. So for best practice, I’m just going to add this user to a couple of different groups. So I’m gonna add them to the CloudAcademy group, and also to the RDSFullAccess.</p>
<p>So once you’ve selected the groups that you want the user to have, you can click on Next to go to tags. And this is an optional step. You can add any key value tags here for that user if you want to. Just gonna leave that blank for this demonstration. Then if we go to Next to Review, then we can review all the options that we’ve set. So we’ve given the username. We specify the access types. So you’ve got programmatic and AWS Management Console access. We’ve not set any permission boundaries. We’ve added the groups that we want the user to belong to and we haven’t applied any tags.</p>
<p>So now, we can go to Create User. Now we’ve successfully created the IAM user, but because we specified that we wanted programmatic access, we need to copy the access key ID and also the secret access key ID as well. If we download the CSV file of that user and take a look at that, we can see here that this CSV file shows the access key ID and also the secret access key ID, and also the console link as well to allow that user to login. So if we go back to the AWS Console, we can also email those login instructions to the user as well if we need to. Once you’ve taken a copy of the access key ID and the secret access key, then you can close this window. Remember, you will only be given one opportunity to take these details and download the CSV that contains that information, so make sure you do that. Then click on Close. And we can now see that that user, Patricia, has been set up as a user with the CloudAcademy and RDSFullAccess groups. So it’s very simple. It’s very quick. It’s very easy to set up a new user within IAM.</p>
<h1 id="Managing-IAM-Users"><a href="#Managing-IAM-Users" class="headerlink" title="Managing IAM Users"></a>Managing IAM Users</h1><p>Once you have created IAM users, you can view their details to configure additional security options or review permissions and change access. In this lecture, I want to cover these additional features. This will be easiest to explain via a demonstration, and I can explain each point as we go through, so let’s take a look. So in this demonstration, I just want to select a user and just to show you some of the different elements that you can change of that user once it has been created, so let’s take a look.</p>
<p>So I’m in the Identity and Access Management Dashboard at the moment and you can see I’m in the Users section. So let’s take a look at this user, Patricia. So if I select the user and we can have a look at some of the options that we can see about this user and some of the things that we can change, et cetera. So this is a summary screen of the user. We have the users ARN at the top here, and we can also see the creation time of that user. And then we have a number of different tabs.</p>
<p>So start with the permissions tab. We can see that this user is getting permissions from two policies at the moment here, the Amazon S3 Full Access policy, and also the Amazon RDS Full Access. And if we wanted to, we can just take a quick look at these groups. We can have a look at the policy summary, or we can take a look at the JSON as well. So that’s the policy and the JSON format. And then if we look at the policy summary, we can see here that this allows full access to S3 and S3 Object Lambda. Here we can set her permissions boundary. Currently there’s not one set, but if we wanted to, we can set one to control the maximum permissions that this user can have. And also there’s a feature here to generate policy based on CloudTrail events.</p>
<p>So what this will do, it will generate a policy looking at the user’s activity. And then based on what the user has been accessing, it can generate a policy based on what services this user has been accessing. Also at the top here, we can add an inline policy for this user. So if we’d done that, then that will be a policy that is embedded within the user object itself. So it’s not taken from a role, it’s not taken from a group. The policy is attached within the user.</p>
<p>Okay, if we take a look at the groups, we can just see a quick breakdown of any groups that the user belongs to, and the policies that are attached to them, which we covered just a moment ago in the summary. The tags is what you’d expect. If there’s any tags here for the user, then they would be listed, or if you wanted to add any tags, then you can do so here. So for example, we can add a key of location and say, UK, Save Changes. And then we can see this tag for this user. Under security credentials, we could see console link that this user can use and we can manage the user’s password. And if we want to change the password, we can simply click on manage, and we can either disable the console access or generate a new password, or ask the user to create a new password at the next sign-in. We also have here, the assigned MFA device, the multifactor authentication.</p>
<p>At the moment, it’s not assigned, but we can go ahead and set up MFA for this user. So let’s go ahead and do that quickly. So if we click on manage, we have a couple of options here, virtual MFA device, U2F security key, or another hardware MFA device. For this, I’m just going to use a virtual MFA device and I’ll use the Google Authenticator app on my phone to do this. If I click on continue. So, first of all, you need to make sure you have an app on your mobile phone or your computer. Like I say, I’m going to use the Google Authenticator app on my phone.</p>
<p>So what I need to do is to show the QR code, and now on my phone, I’m going to add this as a new entry in my Google Authenticator app, so I’m going to click on Scan QR code. And then we can see at the bottom there is added the user, Patricia. And then we add in that code, so 074720. And then what we need to do is to add the second code that comes in when it appears on the Google Authenticator app. So we’re just waiting for that to come around and then I can add in the second code and then it’ll be synchronized and configured. So, we can see it’s about to change, and now I can add in the next code 185887, and then I click on Assign MFA. And that’s it, so you have successfully assigned a virtual MFA device to that user. Click on Close, and there we can see here that there’s an assigned MFI device. We can see that this user also had programmatic access ‘cause there’s access keys that have been generated.</p>
<p>Now, if we wanted to, we can make this access key ID inactive. So if wanted to do that, simply click on Make inactive. And it’ll explained that once you’ve done this, you can’t then use these keys to form any programmatic access. Click on deactivate. And you can see here, the status is now inactive. So any access keys that were used before for this user will no longer be allowed to make any kind of requests. If we wanted to generate new access keys, simply click on Create access key. And again, you’ll have a new access key ID and a new secret access key. And if you wanted to, you can download the CSV file, so you don’t forget those keys. Click on Close.</p>
<p>Now, if we go back up to the top to Access Advisor, I just wanted to show you this quickly. So what this does, it will basically show you which services that this user can access based on their current permissions, and also the last time that these services were accessed. So if you scroll down here, we can see that this user has access to a whole different range of services. And it’ll show you which policies are actually granting these permissions.</p>
<p>So this access to EC2 in IAM is being granted through the RDS Full Access policy, and access to S3 is being granted through the Amazon S3 Full Access. So this is great to review to identify if there’s any users there that do have access to services that they probably shouldn’t do. So you can then modify the policies accordingly just to make sure that the users are only accessing what they are supposed to access. So that was a very quick demonstration of some of the key points that you can change within a user’s properties once you have created an IAM user.</p>
<h1 id="Managing-Multiple-Users-with-IAM-User-Groups"><a href="#Managing-Multiple-Users-with-IAM-User-Groups" class="headerlink" title="Managing Multiple Users with IAM User Groups"></a>Managing Multiple Users with IAM User Groups</h1><p>In this lecture, I want to talk about how IAM User Groups can be used to manage multiple users. IAM User Groups do not signify a single user and they can’t be referenced as a principal in any AWS access policy like a User or a Role can. However, they are used to authorize access for members of the group to AWS resources through the use of AWS Policies attached to the User Groups. So User Groups are objects that contain IAM Users, and these User Groups will have IAM policies associated that will allow or explicitly deny access to AWS resources.</p>
<p>These policies can be AWS Managed policies that can be selected from within IAM, customer-managed policies that are created by you, or in-line policies, which are written and embedded directly into the group. User Groups are a great user management feature and they are normally created to directly relate to a specific requirement or job role. For example, you could have a group called Developers, and then attach policies to that group that allow access to AWS resources required by your development team.</p>
<p>Any users that are then a member of that group will automatically inherit the permissions applied to the group. By applying permissions to a group instead of individual users, it makes it easy to modify permissions for multiple users at once, simplifying access management at scale. It’s a security best practice to apply permissions to User Groups and then associate users to that group than to associate policies to individual Users. This prevents you having to update permissions for each and every user.</p>
<p>For example, if you needed to change access for all the individual developers that had policies assigned to them directly, and this can be very time intensive and prone to human error, especially in an enterprise environment. If using groups and additional access is required for your Developer User Group, all you would need to do is to modify the permissions of the Developer Group and all your associated developers would inherit the new access.</p>
<p>Creating a group is very simple and is essentially a three-step process. You must give your group a meaningful name, add users to the group, attach permissions via the policies. Once you have created a user group, you can then review its configuration, edit the permissions and see other details such as the ARN of the user group. Let me show you via a quick demonstration on how to create an IAM group and then how to modify the permissions of the group once it’s been created.</p>
<p>Okay, so I’ve logged into my AWS Management Console and I’ve gone to the IAM dashboard. Now, from here, to access and create groups, under access management on the left, you can see user groups. So if you select that, and then will show you any groups that you currently have. And I only have one group, which is Admin. So to create a new group, you’ve gotta cross to the right-hand side here, click on create group. And the first thing you need to do is to give the group a name. So I’m going to call this MyS3andEC2Group. And then after that, you need to select the users that you want to be a part of the group.</p>
<p>So if you already have the IAM users there, you can add them at this stage. So let’s just go ahead and add in Stuart. And then at the bottom, you can then attach any policies that you want to be associated with the group. So if I type in S3, and it’ll pick up any policies that we have associated with S3. And I’m going to select this one here, this AmazonS3andEC2FullAccess policy. And if I click on the little plus sign here, it’ll give you a JSON view of the actual policy itself. So you can see exactly what’s happening.</p>
<p>So now I’ve selected that policy. If we just go down to create group at the bottom, and it’s as simple as that. So it’s very easy to create a group. You simple give it a name, specify the users if you need to at that stage and also add any permissions if you want to at that stage as well.</p>
<p>Now, once we’ve created our group, if we select it, we can see the user list here. Now, if you need to add any additional users, simply click on add users, select the users that you’d like and click on add users. And then they’ll be immediately added to the group. You can also look at the permissions. So here’s the policy that we added. Now, we can if we want to, add additional permissions by clicking on this button here. And you can either attach an existing policy or create an inline policy that’s directly embedded into the group.</p>
<p>So for example, if we go to attach policies, and we want RDS access as well for the people in this group, have a quick search and we’ll select this AWS managed policy for RDSFullAccess. Click on add permissions. And we can now see that this group has two permissions policies. And again, we can view the JSON details of those policies if we want to. And then we have the Amazon RDS policy here. So it’s very easy to set up groups and only literally takes just a few clicks.</p>
<p>So that’s how you create a new group, add users and also change the permissions as well. And also, just before we finish, if you want to delete the group, you can click on delete here. And we just need to type in the name of the group to confirm the deletion. So I’ll just go ahead and do that. Then click on delete. And that deletes the group. Finally, from a limitation perspective, your AWS account has a default maximum limit of 300 groups. To increase this you’ll need to contact AWS using the appropriate limit increase forms. Also, a user can only be associated with 10 groups, so bear this in mind when assigning permissions and each group can contain 10 different policies attached at once. Limitations on AWS services is fluctuating all of the time, so for the latest information on Group limitations, please see the following URL here.</p>
<h1 id="IAM-Roles"><a href="#IAM-Roles" class="headerlink" title="IAM Roles"></a>IAM Roles</h1><p>IAM Roles allow trusted Users, AWS services and applications to adopt a set of temporary IAM credentials to access your AWS resources. Roles act as identities, much like Users do, and have permissions assigned to them defining what resources the Roles can and can’t access. Unlike Users though, which represent a single identity, IAM Roles are designed to be assumed by multiple different entities as and when required. Like I say, Roles are used for temporary access to gain access to resources, and each time the role is assumed by a User, an AWS service or an application, a new set of credentials is dynamically created for the duration of that session. As a result, Roles do not have any long term credentials associated, so there is no password for console access, nor are there any access keys for programmatic access that are explicitly associated with the Role.</p>
<p>For every role there will be associated policies controlling access as to what can and can’t be accessed when the role is assumed. There will also be a Trust Relationship, and this Trust Relationship defined who or what can assume the role, for example a User, an AWS account, or an AWS Service. IAM Roles are generally used: if you need to grant temporary access for Users to AWS resources that they don’t normally require access to or you can use a Role to grant access for an IAM user in one account to access resources in another AWS account or perhaps an AWS service needs to access resources on your behalf or if an application requires access to resources you can use a Role instead of embedding credentials into the software itself. Or you might have federated users who require access to specific resources, perhaps those authenticated via Active Directory So, as a result, Roles can be assumed by the following: A user that’s in the same AWS account as the where the role has been created, a user that’s in a different AWS account than where the the role has been created, an AWS service, such as EC2, or an external federated users to your AWS account</p>
<h1 id="Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf"><a href="#Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf" class="headerlink" title="Using AWS Service Roles to Access AWS Resources on Your Behalf"></a>Using AWS Service Roles to Access AWS Resources on Your Behalf</h1><p>An AWS Service Role allows an AWS service to assume a role to access other AWS resources within your own account on your behalf. This is commonly used for EC2 instances, whereby you could create a role for an EC2 instance to assume to gain access to AWS resources on your behalf. Let’s look at an example of when you might use this.</p>
<p>Consider the following scenario. You have an EC2 instance running an application that requires access to Amazon S3 to Put and Get objects using the relevant API calls. To allow access to S3, a set of credentials could be stored on the EC2 instance within the application code allowing it to use those credentials to gain access to the relevant S3 Bucket for any Put or Get API requests. However, in this scenario, you would need to manage these credentials manually including the rotation of access keys, which is obviously an administrative burden and open to the possibility of being compromised by a malicious attacker.</p>
<p>To alleviate this issue, the EC2 instance could be assigned an IAM Role, which in turn would have the relevant permissions associated granting the EC2 instance and its application to access S3 to perform the Put and Get API calls using existing AWS managed or customer manager policies. EC2 instances can be assigned a role during its creation, or to a running instance. You can also replace a role that is already associated with an EC2 instance with a new role. From a security best practice perspective you should always associate a Role to an EC2 instance for accessing AWS resources instead of storing local credentials on the instance itself.</p>
<p>There is also another great advantage of using Service Roles. Let’s now imagine we have a fleet of EC2 instances all using the same application and performing the same task using the same role, but now consider that your existing application, which was used to perform Put and Get requests is now only required to perform Put requests only, and Get requests must be denied.</p>
<p>To make the change, all you’d to do is to alter the permissions assigned to the IAM Role and all EC2 instances associated with that Role would now have the correct permissions. If this same scenario happened by embedding credentials locally on the EC2 instance, then it would take a long time to replicate the change on every instance accurately.</p>
<p>When creating a Service Role, there’s a number of AWS Services that integrate with IAM that support roles. This is a screenshot at the time of writing this course showing the supported AWS services, but for the latest information, you should always check the AWS documentation found here. Before we move on from AWS Service Roles, I want to mention service-linked roles.</p>
<p>A number of different AWS services require roles to perform functions requiring very specific permissions, and in these instances AWS allows you to create service-linked Roles. These are often created the first time that you use a service. Service-linked Roles come pre-configured with the relevant AWS Managed policies, trusts and permissions allowing only that Service to carry out the required operations with other AWS resources that it needs to interact with. Some examples of these roles include AWS ServiceRoleForAmazonSSM.</p>
<p>So AWS Systems Manager uses this IAM service role to manage AWS resources on your behalf. AWS ServiceRoleForCloudTrail. So this service linked role is used for supporting the organization trail feature with AWS CloudTrail. And AWS ServiceRoleForCloudWatchEvents. CloudWatch uses this service-linked role to perform Amazon EC2 alarm actions.</p>
<p>So if we look closer at the AWSServiceRoleForAmazonSSM in IAM, we will find that the trusted identity to use this role is ssm.amazonaws.com, AWS Systems Manager. and with it having an AWS Managed policy already configured, we’re unable to edit and update this policy. This policy is specifically designed to provide access to AWS Resources managed or used by Amazon SSM, and this role is created when you configure SSM. So the difference between AWS Service and AWS Service-Linked Roles is that AWS Service roles allow you to apply your own customer managed or AWS Managed policies, whereas service-linked roles come pre-configured with a specific set of read-only AWS managed policies that can only be used by that particular service.</p>
<h1 id="Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users"><a href="#Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users" class="headerlink" title="Using IAM User Roles to Grant Temporary Access for Users"></a>Using IAM User Roles to Grant Temporary Access for Users</h1><p>There might be circumstances where you will need to grant temporary access to AWS resources for a particular user and the best way to do this would be to allow the user to assume a role with these new permissions. The alternative would be to either add a policy associated with the user and then remember to remove the policy again afterwards, which would likely lead to a security risk, and as we know, attaching policies to a specific user isn’t considered a security best practice, we should use user groups instead. However, if we added these permissions to a user group that the user belonged to, then all users that are a part of that same user group would receive those permissions, so that’s definitely a security risk.</p>
<p>So in this instance, creating a new role with the relevant permissions and allowing only specific users to assume that role, is the best approach to the problem. When a user assumes a role, it replaces all other permissions that the user has. You might think that the user keeps their current permissions and just inherits the additional permissions set out by the role, but that’s not the case. When you assume a role, your existing permissions are temporarily replaced.</p>
<p>I mentioned previously that every role has a trust relationship and so as a user, you can only assume roles where they have been added as a trusted entity. However, in addition to being trusted, the user also has to have the relevant permissions to assume the role as well and this is done via an access policy.</p>
<p>So let’s assume we have a role called RoleS3FullAccess with a permission policy attached as shown which grants full access to Amazon S3. And the trusted entity of this role was defined as the user specified in this ARN. So this means this role trusts the user, Stuart, to assume the role to gain full access to S3. However, Stuart’s permissions do not currently allow him to assume this role. For Stuart to be able to assume the RoleS3FullAcess role, Stuart will need to have the following policy associated with his user, where the text in red is the ARN of the role. When this policy is associated with Stuart, he then has permission to assume the role which is trusted to him as defined in the trusted entity section of the role allowing him to have full access to S3.</p>
<p>Let me now provide a demonstration on how you could create this example that we just ran through. Okay, so I’m in my AWS Management Console and I’m at the IAM dashboard. So in this demonstration, we’re going to create a role with a policy attached and then add an inline policy to the user, Stuart, to ensure that Stuart can assume the role.</p>
<p>So firstly, let’s create our role. So on the left here, we have roles and then over in the top right, we have Create Role. So because we want this role to assumed by a user, we have to pick the correct type of trusted entity. So it’s not going to be assumed by an AWS Service and it’s not to with Federation either, so the option we need here, is this one relating to an AWS account. Now although says another AWS account, this option is used to creating cross-account access, but you can also create roles to be assumed by users within your own account as well. And to do that, all we need to do in the account ID, is to enter our our account ID, so if I just add that in there, and you can also add a couple of additional options, such as requiring multifactor authentication, et cetera.</p>
<p>So once we’ve put in the account ID of where the users are that we want to assume this role, we can then click on next, permissions. And I want the policy that’s attached to this role to have full S3 access, so let’s have a quick search. And here we have it here, Amazon S3 full access, which is an AWS managed policy. Then click on next for tags. Add any tags in that you’d like for the role as an optional step. I’m just gonna leave it blank for this demonstration. And this is where we can give the role a name, so I’m going to call this RoleS3FullAccess. Give it a description if you need to and here we have the trusted entity, which is my own account and then all we need to do from there, is click on Create Role.</p>
<p>Okay, so if we search for that. RoleS3, here we have our role here. So if we go into the role, we can have a look at our trust relationships, so this is essentially saying that anyone within this account is trusted to assume this role and we can also edit this trust relationship as well. So this shows the policy that relates to that trust relationship. So this essentially allows anyone in this account, access to the role, as long as they have that assume role permissions that we’ll be looking at in just a moment. But I want to make this a bit more secure, so I’m going to change it from root to user, Stuart.</p>
<p>So now the trusted relationship for this role will only allow the user, Stuart, within this account to assume the role, given the correct permissions. So I’m gonna update that trust policy and now we can see the trusted entity has been changed. And if we take a look at the permissions, we can see that it has the policy attached there as well. So now we’ve got roles set up with the correct permissions, we now need to edit the user, Stuart with an inline policy to allow that user to assume the role. So I’ve just gone with IAM user of Stuart. If we click on add inline policy and go across to JSON.</p>
<p>Now, here I’m just going to paste in a policy that I’ve already created. So will allow the user, Stuart to use the secure token service AssumeRole action against the role that we just created and that’s the ARN in the role. So now Stuart will have access to assume that role. Let’s go to review policy. Give this a name. I’m just going to call this, AssumeRoleS3. Create policy. And there we have the inline policy that we just created, attached to the user, Stuart.</p>
<p>So now what I want to do, is to log in as the user, Stuart and show you how I can switch to this role. Okay, so I’ve now signed in as the user, Stuart and to switch roles, all I need to do, is to select on the top right up there, select Switch Role. And again, Switch Role. Type in the account. So I’ll just paste that in quickly and then type the role name, which we called RoleS3FullAccess and then click on Switch Role. And that’s it, we’ve now switched roles and if you look in the top right, we can see that it says, RoleS3FullAccess at this account number and that signifies that you swapped your permissions for that role that the user, Stuart has just assumed.</p>
<p>Once you finish what you need to do with the permissions given by the role, you can simply select the top right area again and say Switch Back. And that will revert you back to your original user with your original permissions. IAM user roles are often used to create a cross-account access role, allowing users in one AWS account to access resources in a different AWS account. For a full explanation on how to create cross-account roles, please see our existing course found here.</p>
<h1 id="Using-Roles-for-Federated-Access"><a href="#Using-Roles-for-Federated-Access" class="headerlink" title="Using Roles for Federated Access"></a>Using Roles for Federated Access</h1><p>In this lecture, I want to discuss how users who have been federated can access your resources using roles. When doing so you have two options. Firstly, Web Identity, this allows users federated by a specified external web identity or OpenID Connect provider, to assume this Role to perform actions in your account. And secondly, SAML 2.0 federation, this allows users that are federated with SAML 2.0, to assume this Role to perform actions in your account.</p>
<p>So let’s first look at Web Identity and a scenario where you might need to create a Role. You’ve just created a new mobile application that requires access to Amazon S3 to store media such as photos and videos from users worldwide. As a part of the operations of your application, it will require permissions to S3, to upload and download this media from tens of thousands of users. Embedding long-term credentials into your application code to do this goes against all security best practices. And so instead, you should design your application to request temporary credentials from authenticated users through web identity federation. </p>
<p>Before I go any further, let me explain what identity federation is. It’s basically a method of authentication where two different providers can establish a level of trust, allowing users to authenticate from one, which authorizes them to access resources in the other. During the federation process, one party would act as an Identity Provider, known as an IdP, and the other would be the service provider, an SP. The identity provider authenticates the user and the service provider controls access to their service or resources based on the IdP’s authentication.</p>
<p>You’ve probably all been to websites where it presents you with a login page, where you can either login using existing credentials, native that service, or you might have an option to authenticate using credentials from a third party provider, such as Facebook, Google, Twitter, or LinkedIn, et cetera. So going back to our example, our application run on AWS would be the service provider, and the Web Identity provider could be Google or Facebook, for example.</p>
<p>So when users authenticate to your app via web identity federation, they will receive an authentication token. This token is then exchanged for temporary security credentials in AWS, which can be mapped to your IAM Role, using the AssumeRoleWithWebIdentity API. This then allows the relevant access to Amazon S3 provided by the role, to carry out the operations needed by the application.</p>
<p>Generally, when working with mobile applications, the preferred and recommended method for managing access, would be via Amazon Cognito, to manage his federation process. For more information related to Amazon Cognito, please see our existing course here.</p>
<p>Let’s now take a look at SAML 2.0 federation. Whereas web identity federation is generally used for large, wide scale of access from unknown users, SAML 2.0 is generally used to authenticate your employees using existing directory services that you might already be using. SAML, which stands for Security Assertion Markup Language, is a standard that’s used to exchange authentication and authorization identities between different security domains, which uses security tokens containing assertions to pass information about a user between a SAML Identity Provider and a SAML consumer.</p>
<p>For example, you might already to be using Microsoft Active Directory to authenticate your employees to your internal network. And so you might not want to or need to create lots of users in IAM with their own set of credentials. Instead, it would be easier to allow them to federate their access through via SAML, integrating with your ADFS server. The benefits of this are twofold. It minimizes the amount of administration required within IAM and it allows for a single sign on solution.</p>
<p>As the vast majority of organizations today are using Microsoft Active Directory, using MSAD is an effective way of granting access to your AWS resources without going through the additional burden of creating potentially hundreds of IAM user accounts. Let’s take a high-level look at how active directory authentication mechanism is established. This example will assume the user within an organization requires API access to S3, EC2, and RDS. This scenario will also include the use of an AWS service called Security Token Service, STS. The Security Token Service allows you to gain temporary security credentials for federated users via IAM, associated with IAM roles and policies. Let’s look at this in more detail via a diagram.</p>
<p>A user within an internal organization initiates a request to authenticate against the Active Directory Federated Service, an ADFS server, via a web browser using a single sign on URL. If their authentication is successful by using their Active Directory credentials, SAML will then issue a successful authentication assertion back to the user’s client, requesting federated access. The SAML assertion is then sent to the AWS Security Token Service, to assume a role within IAM using the AssumeRoleWithSAML API. STS then responds to the user requesting federated access with temporary security credentials, with an assumed role and associated permissions, allowing S3, EC2, and RDS access as per our example, the user then has federated access to the necessary AWS services as per the role’s permissions.</p>
<p>This is a very simple overview of how federation is instigated from the user for API access to specific AWS services. Corporate identity federation is always authenticated internally first by Active Directory before AWS, when creating your role for users federating via SAML, you can specify if you want to provide programmatic access only, or programmatic and AWS management Console access, in addition to specific permissions to access other AWS resources.</p>
<h1 id="IAM-AWS-Policy-Types"><a href="#IAM-AWS-Policy-Types" class="headerlink" title="IAM AWS Policy Types"></a>IAM AWS Policy Types</h1><p>Hello and welcome to this lecture where I want to talk about four different policy types that you can expect to see when working with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">AWS IAM</a>. Identity-based policies. These policies can be attached to users, user groups or roles within IAM. Essentially, any entity that depicts an identity. Resource-based policies. Instead of being associated with an identity, these policies are attached in line to resources themselves. An example of a resource-based policy in IAM will be that of a role trust policy where the policy defines the principle that can assume the role.</p>
<p>Permission boundaries. These policies can be associated with a role or user, but they don’t actually grant permissions themselves, instead they define the maximum level of permissions that can be granted to an entity. Organization service control policies, SCPs. These are very similar to permission boundaries in the fact that they do not grant permissions. They define a boundary of maximum permissions. However, these service control policies are associated with an AWS account or organizational unit, an OU, when working with AWS organizations and govern the maximum permissions to the members of those accounts.</p>
<p>With all of these policy types, it’s easy to see why some people can get confused as to which type of policy is being used or should be used. So let me run through each of these policies in greater detail, starting with the identity-based policies.</p>
<p>As already highlighted, these policies can be attached to users, user groups, or roles and they control what permissions each of those entities have. Identity-based policies can either be managed or inline policies, but what does this mean? Well, managed policies are saved within the IAM library of policies and can be attached to any user, user group or role as and when required, and the same policy can be attached to multiple entities.</p>
<p>Managed policies also come in two different flavors, AWS managed policies and customer managed policies. AWS managed policies are policies that have been pre-configured by AWS and made available to you to help you manage some of the most common permissions that you may wish to assign. Some examples of AWS managed policies can be seen here. From the policy name, you can usually tell what access is being given, although you can expand upon each policy to see the JSON document that is associated.</p>
<p>Customer managed policies are those that you have created yourself, which can then be associated with a user, user group or role. You might want to create customer managed policies when the AWS managed policies do not meet your security requirements. For example, you might want to add additional granularity to the policy to restrict access at a specific API call level.</p>
<p>Let me now explain how inline policies contrast against managed policies. So inline policies are embedded directly into the entity, either the user, user group or role. The policy is not saved and stored in the IAM library policy, its only existence is within the associated entity. As a result, it can’t easily be replicated to other entities, it’s specific to that one user, user group or role, creating a one-to-one relationship. It’s not always best practice to use inline policies as they take a lot of administration to keep on top of and should only be used if absolutely necessary. For example, you might have some elevated permissions that you don’t want to have mistakenly given to someone else that they weren’t intended for.</p>
<p>Okay, let me now move onto resource-based policies. So resource-based policies are effectively inline policies that are associated with a resource instead of an identity. If you have been using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon S3</a> for any extended period of time, then you may have come across S3 bucket policies, which is a common resource-based policy where permissions to the bucket are defined at the resource level and defined which principle or principles can access the bucket based upon different actions.</p>
<p>When using roles within IAM, the role has a trust relationship policy, which is a resource-based policy. As a result, the permissions of the trust are embedded inline within the role itself. So in this instance, the resource is the role and the resource-based policy is the trust relationship. This example shows the resource-based policy of the trust relationship of one of my roles. In this policy, the user, Stuart is the principle and has the ability to assume the role. It is this parameter of principle which signifies the difference between an identity-based policy and a resource-based policy.</p>
<p>Identity-based policies don’t have that principle parameter as the policy is already associated within the identity. Resource-based policies must have the principle parameter to determine which identity that the policy permissions relate to. Next up, we have permission boundaries. So permission boundaries can only be associated with a user or role. It’s not possible to add a boundary to a group. They differ from both identity-based and resource-based policies in the fact that permission boundaries don’t grant permissions themselves. They act as a guide rail to limit the maximum level of permissions that the user or role can be given. The policy configured for the boundary can be an AWS managed or customer managed policy.</p>
<p>So let’s assume for example, that our user, Stuart has an identity-based policy associated with him that allows full access to S3 and full access to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-elastic-compute-cloud-ec2/amazon-ec2/">EC2</a> using the following AWS managed policies. However, if a permission boundary was also configured for Stuart using the following AWS managed policies, which only grants read only access to S3, but still full access to EC2, then the resulting access for the user, Stuart would be full access to EC2, but only read only to Amazon S3 as the maximum permission defining by the permissions boundary was limited to read only despite the user having an identity-based policy associated granting full S3 access.</p>
<p>So the last policy I want to reference are the SCPs, the service control policies. So service control policies are used by AWS organizations and attached to AWS accounts or organizational units, OUs, to define the maximum permissions allowed for the members of the associated account or OU. So in a way they act in a similar fashion to that of permission boundaries, but at the account level and affect all members of those accounts.</p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS and EC2 via an identity-based policy. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2 despite having full access to S3. The SCP would serve to prevent service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<p>So to be clear, an SCP does not grant access, they add a guide route to define what is allowed. You’ll still need to configure your identity-based or resource-based policies to identities, granting permission to carry out actions within your accounts. If you want to use service control policies to help you manage your security at an account level, then you need to ensure that you deploy AWS organizations using the enable all feature setting.</p>
<p>Within IAM, you have the ability to view any SCPs that are applicable to your AWS account, the policy statement itself, it’s ARN, the number of entities it affects and you can also review access activity to learn when a principal within the organization last accessed a service. However, if you wanted to update or edit the SCP, then you’d have to do that from within the AWS organization service itself. The SCP can’t be edited from within IAM. For more information relating to how to implement, manage and secure AWS organizations using SCPs, then please see our existing course here.</p>
<h1 id="Examining-the-JSON-Policy-Structure"><a href="#Examining-the-JSON-Policy-Structure" class="headerlink" title="Examining the JSON Policy Structure"></a>Examining the JSON Policy Structure</h1><p>In this lecture, I will be taking a deep look at the syntax and structure of IAM policies. For both identity-based and resource-based policies, the syntax is the same, but you might use slightly different parameters. If you’ve worked with IAM at all before, then I am sure you have come across an IAM policy. After all, it is the component that defines what an identity can or can’t access. But what do these policies actually look like? IAM policies are formatted as JSON documents, JavaScript Object Notation, and each policy will have at least one statement where the structure may look like this example.</p>
<p>Let me break the structure of this policy down to allow you to understand each element. Version. This specifies the policy language version and specifies the language syntax used, and at the time of writing this course, the current policy version is 2012-10-17. Statement. This defines the main element of the policy, which will includes other sub-elements, including the Sid, Effect, Action, Resource, and Condition. These elements will identify the level of access granted or denied and to which resource. A policy must contain at least one statement, but it can also contain an array of statements. For each statement block, it must be enclosed within curly braces, but if you use an array of statements, then you must enclose the entire array within square brackets. And we shall look at an example of this during this lecture.</p>
<p>Sid. This is the statement ID, and it’s an optional parameter that allows you to set a unique identifier within the statement. As you add more arrays within your policy, it can be a good idea to include a Sid for each one, allowing you to name them appropriately, making them more easily identifiable. For example, AllowGetObjectForS3. Without reading the rest of the statement, you can get a good idea of what the permissions in this statement allows.</p>
<p>Effect. This element can be set to either Allow or Deny, which either grant or restrict access for the actions defined in the statement. By default, all access to your resources are denied and so, therefore, if this is set to Allow, it replaces the default denied access. Similarly, if this was set to Deny, it would override any previous Allow. An explicit Deny in a policy will always take precedence over any Allow.</p>
<p>Principal. This parameter defines which principal the policy relates to. The Principal is only used for resource-based policies, for example, those policies attached to S3 Buckets. When using identity-based policies, this parameter is not required within the policy as the policy itself is associated with the principal and not a resource. As an alternative to the Principal parameter, this could be replaced with the NotPrincipal parameter, which would specify the user, role, or AWS account that is not allowed or denied access to the associated resource.</p>
<p>Action. This is the action that will either be allowed or denied, depending on the value entered for the Effect parameter. Actions are effectively API calls for different services. As a result, different actions are used for each service. For example the DeleteBucket action is available for S3 but not EC2, and likewise, the CreateKeyPair action is available for EC2 but not S3. The action is prefixed with the associated AWS service. This example defines two actions, CreateTrail and DeleteTrail, for the CloudTrail service. As another example, you can see this one here with the asterisk, and this acts as a wildcard which represents all actions for the CloudTrail service, essentially granting full access to the service.</p>
<p>Similarly, as we had with the Principal parameter, we could replace the Action parameter with the NotAction instead, and this could help you optimize your policy by creating a shorter version, listing just a limited set of actions that should not match instead of creating a longer policy listing all that actions that should. An example of using the NotAction is shown here. This policy essentially allows all actions for CloudTrail apart from the DeleteTrail API, and this is because the NotAction parameter is being used.</p>
<p>Resource. This element specifies the actual resource you wish the Action and Effect to be applied to. AWS uses identifiers known as ARNs, Amazon Resource Names, to specify specific resources. Typically, ARNs follow a syntax of, arn, partition, service, region, account-id, and then resource. So the Partition element, this relates to the partition that the resource is found in. For standard AWS regions, this would be AWS. Service. This reflects the specific AWS service, for example, S3 or EC2.</p>
<p>Region. This is the region where the resource is located. Some services do not need a region specified, so this can sometimes be left blank. Account-ID. This is your AWS Account ID, without hyphens. Again, some services do not need this information and so can be left blank. Resource. The value of this field will depend on the AWS service you are using. For example, if I were using the action, Action:s3:PutObject, then I could use the bucket name that I wanted those permissions to apply to. Again, we also have a NotResource parameter that can be used to explicitly match all other resources except those specified. For example, this policy will allow access to all S3 buckets other than those specified by the NotResource parameter.</p>
<p>Condition. This is an optional element that allows you to control when the permissions will be effective based upon set criteria. The element itself is made up of a condition and a key value pair and all elements of the condition must be met for the permissions to be active. Let’s take a look at an example condition. In the example, the IP address is the condition itself, which the key value pair will be effective against. The aws:SourceIp is the Key and the 10.10.0.0&#x2F;16 is the value element of the key. So effectively, what this is saying is, if the Source IP address of the user who is requesting access via the policy is within the 10.10.0.0&#x2F;16 network range, then implement the permissions in the policy statement.</p>
<p>Now that I’ve gone through the core parameters of an IAM policy, let’s take a look at a couple of examples to ensure we can understand the permissions that are being presented within the policy. As I mentioned previously, you can have multiple Sids within a statement, each granting a different level of access. The example below demonstrates this, and I have highlighted each a different color to show the separation.</p>
<p>So looking at this example policy, let’s determine what access is being granted. So in StatementBlock1, this allows any resource full access to CloudTrail on the condition that their source IP address is within the 10.10.0.0&#x2F;16 network range. StatementBlock2. This allows full access to all RDS databases using all API calls except for that of the rds:DeleteDBInstance due to the NotAction parameter being used instead of the Action being used. And in StatementBlock3, this allows the creation and deletion of S3 Buckets within the cloudacademy bucket on S3.</p>
<p>Let’s take a look at another example policy, this time, a resource-based policy, and in this instance, it’s from an S3 bucket. So in this policy, it allows the user, Stuart, as highlighted by the Principal parameter, to create and delete buckets in addition to deleting the bucket policy. Stuart can also delete and put objects within the bucket, and the bucket that this policy refers to is the bucket named ca-bucket-uk, as defined by the Resource parameter. However, there is a condition bound to this policy that states that Stuart can only do this if he was authenticated via Multi-Factor Authentication, MFA. That should now give you more of an understanding of how JSON IAM policies work and what they look like.</p>
<h1 id="Creating-an-AWS-IAM-Policy"><a href="#Creating-an-AWS-IAM-Policy" class="headerlink" title="Creating an AWS IAM Policy"></a>Creating an AWS IAM Policy</h1><p>Hello and welcome to this lecture where I will show you how to create your IAM policies. Let’s start by creating an identity-based, customer managed policy, and there are a number of ways to create a customer managed policy, these being, copy an existing AWS Managed Policy. So we can copy an existing policy and then edit that policy to ensure it meets the requirements we need, and this can save us some time.</p>
<p>We can use the Policy Generator, and this allows you to create a customer managed policy by selecting options from a series of dropdown boxes. And we can also create our own policy. So if you’re proficient in JSON and the syntax of IAM policy writing, then you can write your own policies from scratch or paste in a JSON policy from another source. So I now want to give you a quick demonstration on how to create a policy in each of these three ways. And the demonstration will include, how to create a customer managed policy by editing an existing AWS managed policy, how to create a customer managed policy using the Policy Generator, and how to create a customer managed policy from scratch. So let’s take a look.</p>
<p>Okay, so I’ve logged into my AWS Management Console, and I’m at the IAM dashboard, and I’ve gone into Policies. So from here, we can create a number of different IAM policies. And the first method I’m going to show you is how to copy and import an existing AWS managed policy to create your own, if you need to make a few changes to it. So let’s go ahead and do that now. So from here, we need to go across to Create Policy.</p>
<p>Now, from here, we have an option on the top right here called Import Managed Policy, and that’s what we want to do. I want to import an AWS managed policy and then make a couple of small changes to it. So if I select Import Managed Policy and then find the policy that I want to find. So let’s, for example, look at AmazonS3FullAccess, import that, and we can see here that it’s imported the data. And if we look at the JSON tab, we can actually see the JSON format of that policy, and from here, we can directly make changes.</p>
<p>So for this quick demonstration, I want to copy this existing policy, but instead of allowing any resource, I want to specify my own resource for this, which will mean instead of this policy being, allow full access to Amazon S3, it allow full access to a specific bucket on Amazon S3. So let me go ahead and make those changes now. So I’m gonna change the asterisks from Resource and put in my own specific ARN of a bucket. So now, I’ve changed the resource to my own ARN.</p>
<p>So now, all I need to do is click on Next, go to Tags, and I can add any optional tags if I want. I’m just gonna leave that blank for this demonstration. Go across to Review, and here I can give this new policy a name. So I can call it S3FullAccessToMyBucket. And description, this allows full S3 access to ca-bucket-uk. And it gives you a summary of the policy here. So we can see the service that it’s using, the access level, and also the resource. Once we’re happy with that, we can simply click on Create Policy. And we can see that that policy has now been created.</p>
<p>Now, if we have a look at that policy, just by clicking on it there, it’ll take us straight to it, and we can see the JSON version of the policy. So that’s a very quick and easy way if you want to save yourself some time by copying existing S3 managed policies that are already there. Now, I chose a fairly simple policy just for demonstration, but there are some quite complex policies that AWS already have that you might need to just tweak a few changes to so you can simply import those existing managed policies, make your changes, and then save it as a new customer managed policy.</p>
<p>Okay, so that’s the first method covered. Now, next, I want to show you how to create a policy using something called the AWS Policy Generator. Now, if you simply go to Google and type in the AWS Policy Generator, then it’ll come up straight away, and you’ll be brought to a page like this. Now, the actual URL of this Policy Generator, if you’d prefer to type it in, is awspolicygen.s3.amazonasw.com&#x2F;policygen.html. So this is a Policy Generator, and it allows you to easily create different types of AWS policies.</p>
<p>So if we look at this drop down list here, we can create an SQS policy, an S3 bucket policy, which, as we know, is a resource-based policy, a VPC endpoint policy, an IAM policy, and an SNS topic policy. We’re interested in the IAM policy. So if we select that, now, in step two, we can add our statements. Now, here, we have our effect, which can be allow or deny. So let’s say allow for this example. And then, we can select the service that we’re interested in. Let’s select Amazon S3 to keep it nice and simple. And now, we can select our actions.</p>
<p>Now, we can select individual actions here just through these tick boxes for any actions that we’re interested in. So I’ll just select a number of different ones there. And you can see here that it’s selected five actions. If you wanted all actions, you would simply tick this box. And then, you’d put in the ARN of the bucket. So let’s just put in that same bucket that we used in the previous example. And then, here, we can also add in any conditions that we’d like. So just through a series of dropdown boxes, you can specify any conditions that you’d like in there as well. And then, once you’re happy with your policy, simply select Add Statement. And here, it breaks down a summary as well. So it shows the effect, the action, the actions that we selected, the resource, and if there’s any conditions, which we didn’t specify any.</p>
<p>Now, if you wanted to, we can now add an additional statement. So for example, if we wanted to add some RDS elements in here as well, we can select AWS Service, select a load of actions, put in the ARN of your RDS database, and also add that statement to the same policy. Then, once you’re happy with your policy and the number of statements that you’ve added, simply click Generate Policy. And here, it shows you the JSON view of the policy that you would need based on your dropdown selection. So what you can do now is simply copy that, go back to IAM, go to your policies, Create Policy, go to the JSON tab, and simply paste it in. So it’s a very quick way of creating a policy through a series of dropdown boxes. And again, you can just progress through, adding any tags that you need to do, give this a name, ThisIsMyPolicy, and then click Create Policy. And again, you can see that this has been created. You can select that. And again, we can see the JSON statement there.</p>
<p>So now, we’ve looked at how to create a policy by copying an existing AWS managed policy. We’ve looked at how to use the AWS Policy Generator. Now, let’s just quickly review how to create a policy from scratch. So again, go back to Policies and then Create Policy. Now, you can either use this visual editor or go straight to JSON, and you can start typing out your policy in here as and how you need it, or you can use the visual editor, which is very similar to the Policy Generator we just looked at, where, again, you can choose a service, then the specific actions, say all S3 actions, then you can specify the resources, whether you want this, again, as an access point, a bucket, a job, an object. So you can say any object. Then, you can specify any conditions, if you need MFA or specific source IP, for example. And again, you can also add additional permissions here.</p>
<p>So almost like another statement. It’s very similar to the Policy Generator. But for me, personally, I think the Policy Generator is slightly easier to understand. And then, once you have your settings as you want them, and this has all been pre-filled from the options from the visual editor, and, again, click Tags, then Review, and then give this a name, MyPolicy, and then go to Create Policy. And if we take a look at the policy, we can just see its details again. And again, this shows the JSON view, et cetera. And if we wanted to edit the policy directly here, then we can. Click on Edit. Then, we can either edit through the visual editor, or we can directly edit the JSON view as well. So that’s just a couple of ways of creating your IAM policies in a few very simple steps.</p>
<h1 id="Policy-Evaluation-Logic"><a href="#Policy-Evaluation-Logic" class="headerlink" title="Policy Evaluation Logic"></a>Policy Evaluation Logic</h1><p>Every time someone tries to access a resource within AWS, the request is processed through a series of steps. One of which involves evaluating the level of permissions based upon the policies that are used. So let’s take a look at the whole process to understand how access is either granted or denied. And we can start with a simple four step process. So firstly authentication. We must first ensure that the principle sending the request is authenticated as a valid user.</p>
<p>Next, the context. Once authentication of the principle has been established, AWS then needs to determine the context of the request that is being asked, for example, what service or action is being requested. And this ensures that the relevant policies can be highlighted based on the request. We then have policy evaluation, and this is the part that we are interested in. Based on the request, there may be multiple policy types that need to be reviewed to determine the level of access, and I shall cover this in greater detail as we go through this lecture. And then finally, the result. AWS will determine if access is allowed or denied based upon the evaluation of all policies used.</p>
<p>So for this lecture, I want to focus purely on the third point, the policy evaluation and how that process is carried out. The rules for reviewing permissions across multiple policies in a single account is actually quite simple and can be summarized like this: by default, all access to a resource is denied. Access will only be allowed if an Allow has been specified within a policy associated with the principle. If a single Deny exists within any policy associated with the same principle against the same resource then that Deny will overrule any previous Allow that might exist for the same resource and action. So to reiterate, an explicit Deny will always take precedence over an Allow.</p>
<p>Now, there is an order in which policies are evaluated, and the following list of policies are shown in the order of evaluation. So firstly, we have any Organizational Service Control Policies. Then any Resource-based policies, then IAM permission boundaries, and then finally Identity-based policies. So let’s look at an example scenario. Let’s assume that the user, Stuart, is requesting to upload an object to the s3 bucket of ca-bucket-uk using the s3:PutObject API.</p>
<p>With this in mind, let’s assume we have the following policies in place to see what happens at each step of the evaluation. So firstly, the evaluation will review any organization SCPs in place, and here is our example SCP. So this SCP will simply deny all access to RDS. So there is no Deny in place that affects the s3:PutObject requested by Stuart so the evaluation continues. Next, the evaluation will identify any resource-based policies, and here we have a Bucket Policy associated with the ca-bucket-uk as shown.</p>
<p>Again, there is no Deny here for the request in question, so the evaluation continues. Next, we have IAM Permission Boundaries. And this IAM Permission Boundary Policy is set on the user, Stuart. So this policy sets out a maximum permission boundary of full access to s3. Remember, permission boundaries do not actually grant permissions, they set the maximum privilege level, as full access to s3 allowed, the evaluation continues.</p>
<p>Finally, we have the evaluation of any Identity-based Policies, and this policy is associated to the group that the user, Stuart, belongs to. So as we can see, this policy allows any s3 action to the ca-bucket-uk. As a result, this permits Stuart to upload an object using s3:PutObject to the s3 bucket of ca-bucket-uk. So the final decision upon the policy evaluation is to allow the request.</p>
<h1 id="What-is-AWS-Trusted-Advisor"><a href="#What-is-AWS-Trusted-Advisor" class="headerlink" title="What is AWS Trusted Advisor?"></a>What is AWS Trusted Advisor?</h1><p>Hello and welcome to this lecture where I am going to be looking at AWS Trusted Advisor, explaining what it is and the different components that make up this service. </p>
<p>Trusted Advisor plays an integral part in helping you to optimize your infrastructure across a number of key areas, allowing you to make decisions upon recommendations made by the service which follow and best practices that have been honed over the years by AWS.</p>
<p>The service itself can be found within the AWS Management Console under the Management &amp; Governance category, alongside services such as Amazon CloudWatch, Control Tower and Systems Manager. </p>
<p>The main function of Trusted Advisor is to recommend improvements across your AWS account to help optimize and streamline your environment based on these AWS best practices. These recommendations cover 5 distinct categories:</p>
<ol>
<li><strong>Cost optimization</strong> - Helps to identify ways in which you could optimize your resources to help you reduce costs by implementing features such as reserved capacity and removing unused capacity</li>
<li><strong>Performance</strong> - This reviews your resources to highlight any potential performance issues across your infrastructure, determining if you could take benefits from performance-enhancing capabilities such as provisioned throughput</li>
<li><strong>Security</strong> - This analyses your environment for any potential security weaknesses or vulnerabilities that could potentially lead to a breach.</li>
<li><strong>Fault Tolerance</strong> - This helps to suggest best practices to maintain service operations by increasing resiliency, should a fault or incident occur across your resources.</li>
<li><strong>Service Limit</strong> - This identifies and warns you when your resources reach 80% capacity of their service limit quota.</li>
</ol>
<p>Within each of these 5 categories, Trusted Advisor has a list of control points and checks to see how your account, resources and architecture is implemented to determine if you’re aligned with best practice. So it essentially acts as an automatic auditor across your account, which can save you money, increase the efficiency of your resources, maintain a tighter and more secure environment, help to ensure your resources remain operational should a failure occur and that you remain in line with your service limitations, allowing you to request an increase where possible.</p>
<p>Between the 5 different categories and at the time of writing this course, there are over 115 different checks. Please note, that the number of these checks are constantly changing, so for the most up to date figures, please review the following link: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>
<p>Although there are a lot of these checks that Trusted Advisor can perform, not all of them are freely available to anyone with an AWS account. The list of checks that you have access to is very dependent on the support agreement with hold with AWS.</p>
<p>The full power and potential of AWS Trusted Advisor is only available if you have a Business or Enterprise Support Plan with AWS. Without either of these plans then you will only have access to 6 core checks in the security category and all the Service Limits </p>
<p>The 6 checks within security are as follows:</p>
<ul>
<li>S3 Bucket permissions</li>
<li>Security Groups - Specific Ports Unrestricted</li>
<li>EBS Public Snapshots</li>
<li>RDS Public Snapshots</li>
<li>IAM Use</li>
<li>MFA on root account</li>
</ul>
<p>At the time of writing this course, here are the available service limit checks.</p>
<p>Now if you compare this to the full list of checks here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>
<p>….that are included with Business and Enterprise support plans, you will see that the full checklist can provide a huge wealth of valuable information to help you optimise your infrastructure. </p>
<p>In addition to these extra checks that these support plans offer, you will also get the additional benefit of being able to administer certain functions of Trusted Advisor, such as:</p>
<ul>
<li>being able to track the most recent changes to your AWS account by bringing them to the top of your AWS Trusted Advisor dashboard.</li>
<li>using the AWS Support API to retrieve and refresh trusted advisor results. </li>
<li>Also you’ll have the added advantage of having Amazon CloudWatch integration to detect and react to changes made to your Trusted Advisor checks</li>
</ul>
<p>There are also a number of features that everyone has access to, including those outside of the Enterprise and Business support plans, these being:</p>
<ul>
<li><p><strong>Trusted Advisor Notifications</strong> - This is an opt-in or opt-out feature which is completely free to everyone and can be configured within the preferences pane of the Trusted Advisor console. It tracks your resource check changes and cost saving estimates over the course of a week and it will then email up to 3 recipients, for billing, operations and security notifications with a report.</p>
</li>
<li><p><strong>Exclude Items</strong> - This allows you to select specific resources to be excluded from appearing in the console within a specific check. You may want to do this if you are not interested in the reporting for that particular resource and so you decide to exclude it. You can decide to include it again at any point if you do change your mind. This feature can make viewing and managing your checks easier by eliminating some resources within the console.</p>
</li>
<li><p><strong>Action Links</strong> - Many of the items identified within the Checks against resources have hyperlinks associated, these are known as Action Links which allow quick access to the resource in question allowing you to remediate the issue identified. For example, if you reached 80% of the number of VPC’s within a Region, the <strong>‘VPC’</strong> Service Limit Check would highlight this as an issue. The Action Link against the resource would lead you to an AWS Support Center page to create a case to increase the quantity of VPCs you’re allowed within a single region.</p>
</li>
<li><p><strong>Access Management</strong> - AWS Trusted Advisor is tightly integrated within Identity &amp; Access Management. You can grant different levels of access to Trusted Advisor, including Full Access, Read Only, or even restrict access down to specific Categories, Checks and Actions. For example, the following IAM policy allows access to AWS Trusted Advisor, but denies the user from performing a refresh and updating notification preferences.</p>
</li>
</ul>
<p>For a full list of IAM permissions using the trustedadvisor namespace please see the following AWS reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html">https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html</a></p>
<ul>
<li><strong>Refresh</strong> - The data within Trusted Advisor is automatically refreshed if the data is more than 24 hours old when you view it within the console. However, after any refresh, you can perform a manual refresh 5 minutes after the previous refresh. You can either choose to perform a refresh against individual checks or against all checks.</li>
</ul>
<p>Before I finish this lecture I just want to give a high level overview of how Trusted Advisor works in a few simple steps:</p>
<ul>
<li>Once you connect to AWS Trusted Advisor, the service will scan your infrastructure </li>
<li>It will then compare the state of your infrastructure against best practices defined within the 5 categories of Cost Optimization, Security, Performance, Fault Tolerance and service limits</li>
<li>The output of this scan will generate a number of recommendations of how your infrastructure could be optimised with a priority factor</li>
<li>This then allows you to optimize your resources based on the recommendations</li>
</ul>
<p>AWS Trusted Advisor uses a service-linked IAM role to access you resources, named <strong>AWSTrustedAdvisorServiceRolePolicy.</strong> This is a predefined role created by AWS and allows the services to call other services on your behalf. The policy summary of this role is as shown here and helps to define which AWS services that Trusted Advisor communicates with.</p>
<p>Please be aware that this list will change over time, so for an updated list please refer to the role within IAM to determine which services <strong>AWSTrustedAdvisorServiceRolePolicy</strong> has access to.</p>
<h1 id="AWS-Organizations"><a href="#AWS-Organizations" class="headerlink" title="AWS Organizations"></a>AWS Organizations</h1><p>Hello and welcome to this lecture where I will start this course by providing an overview of AWS Organizations as a foundation before focusing on the service control policies.</p>
<p>As businesses expand their footprint on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and utilize more services to build and deploy their applications, it will soon become apparent that the need for multiple AWS accounts is required to manage the environment and infrastructure effectively.</p>
<p>This multi-account strategy is beneficial for a number of reasons as your organization scales, and for example, multi-account strategies include cost optimization and billing, security and governance, management and control of workloads, resource grouping, and helping to define business units.</p>
<p>As you begin to expand with multiple accounts, it will become increasingly more difficult to manage them as separate entities. The more accounts you have, the more distributed your environment becomes and the associated security risks and exposures increase and multiply.</p>
<p>However, with AWS Organizations, it can provide a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization, which helps to maintain your AWS environment from a security, compliance, and account management perspective.</p>
<p>To understand how AWS Organizations operates, we first need to be aware of the hierarchy of the service’s components.</p>
<p>AWS Organizations uses the following components to help you manage your accounts: Organizations, Root, Organizational Units, Accounts, Service Control Policies.</p>
<p>An Organization is an element that serves to form a hierarchical structure of multiple AWS accounts. You could think of an organization as a family tree which provides a graphical view of your entire AWS account structure. At the very top of this Organization, there will be a Root container.</p>
<p>The Root object is simply a container that resides at the top of your Organization. All of your AWS accounts and Organizational units will then sit underneath this Root. Within any Organization, there will only be one single Root object.</p>
<p>Organizational Units (OUs) provide a means of categorizing your AWS Accounts. Again, like the Root, these are simply containers that allow you to group together specific AWS accounts. An organizational unit (or OU) can connect directly below the Root or even below another OU (which can be nested up to 5 times). This allows you to create a hierarchical structure as I mentioned previously.</p>
<p>Accounts. These are your AWS accounts that you use and create to be able to configure and provision AWS resources. Each of your AWS accounts has a 12 digit account number.</p>
<p>Service control policies, or SCPs, allow you to control what services and features are accessible from within an AWS account. These SCPs can either be associated with the Root, Organizational Units, or individual accounts. When an SCP is applied to any of these objects, its associated controls are fed down to all child objects. Think of it as a permission boundary that sets the maximum permission level for the objects that it is applied to.</p>
<p>Now we have an understanding of what AWS Organizations is exactly, what benefits can this bring to your AWS environment?</p>
<p>The primary benefit that this service brings is its ability to centrally manage multiple Accounts from a single AWS account, known as the master account. You can start by inviting your existing accounts to an Organization and then create new accounts directly from the Master Account.</p>
<p>Greater control of your AWS environment. Through the use of Service Control Policies attached to the Root, Organizational Units or individual accounts, administrators of the master account gain powerful control over which services and features—even down to specific API calls—that an IAM user within those accounts can use, regardless of the user’s identity-based or resource-based permissions. Consolidated Billing. The master account of your AWS Organization can be used to consolidate the billing and costs from all member AWS accounts. This allows for greater overall cost management across your individual AWS accounts. Categorization and grouping of accounts. By leveraging Organizational Units, you can segregate and group-specific AWS accounts together, applying different SCPs associated to each OU. For example, you may have a number of AWS accounts that must not have the ability to access any AWS Analytical services. In this case, you could place these accounts into a single OU and assign an SCP that denies this functionality.</p>
<h1 id="Implementing-AWS-Organizations"><a href="#Implementing-AWS-Organizations" class="headerlink" title="Implementing AWS Organizations"></a>Implementing AWS Organizations</h1><p>Hello and welcome to this lecture which will explain how to initially set up and configure AWS organizations. Setting up an organization is a very simple process that starts from a master AWS account. Your master account is a standard <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account that you have chosen to create the AWS organization. It’s best practice to use this AWS account solely as a master account, and not to use it to provision any other resources such as EC2 instances, et cetera. This allows you to restrict access to the master account at a greater level. The few users who need access to it, the better, and you need to do this because the master account carries certain administrative level capabilities such as being able to create additional AWS accounts within your organization, invite other accounts to join your organization, remove AWS accounts from your organization, and apply security features via policies to different levels within your organization.</p>
<p>Once you have selected your AWS account to be used as a master account, you can create an organization. From here, you have two choices when creating an organization type: enable all features or enable only consolidated billing. If you want to set up service control policies, then you need to select enable all features.</p>
<p>The second option allows you to control payments and manage costs centrally from that master account across all associated AWS accounts within the organization. When the organization is created, the master account can create organizational units for AWS account management as required. The master account can also invite other member AWS accounts to join the organization. During this invitational process, the account owner of these invited AWS accounts will receive an email requesting that their AWS account join the organization. Once the accounts have joined the organization, the master account can then move these accounts into the corresponding OUs that have been created and associate relevant service control policies with them.</p>
<p>Let me now show you via demonstration on how to create a new organization and invite an existing account to join it. Now I’m logged into my AWS management console in the AWS account that I want to be the master account, and the first thing I need to do is go to AWS organizations, which is under the management and governance category, and you can see, it’s just at the top here.</p>
<p>So if I go into organizations, and at the moment, I don’t have any organizations set up or created. So the first thing I need to do is click on create organization, and this gives you a quick, high-level screenshot just to explain what creating an organization does. So it provides single payer and centralized cost tracking, it lets you create and invite accounts, it allows you to apply policy-based controls, and it helps you simplify organization-wide management of AWS services.</p>
<p>Now, as I mentioned previously, there’s two options when you create your organization. You can only create it with all features enabled, which is what I just listed, or as you can see here, you can just create your organization to consolidate your billing features. With this demonstration, I’m going to create it with all features. So let’s go ahead and create our organization, and that’s effectively it. So it’s very easy to create your AWS organization to start with, and because this is a brand new organization, this is my master account, which is signified by this star here, and this is my account name, and my account ID.</p>
<p>So, to actually create the organization is very simple, but now I want to add another account as a member account, so let me go ahead and do that. So if I select add account, now I have two options here. I can invite an existing account or create a new account. Now I already have another AWS account, so I’m going to invite an existing account. Now I need to enter the email or account ID, so I’ll just paste in my account, and you can add any notes here, for example, please join my organization, and then you select invite.</p>
<p>Okay, now we can see that we have a request that’s been sent as an invitation. The status is currently open. So now the email address that was registered with this account will get an invitation and they must accept that invite into this organization. So let’s take a look and see if I got that email. So here we can see the email that’s been sent to the owner of that member account, and it says, Stuart would like to add your AWS account to their organization as a member account, and then it just gives some additional blurb about AWS organizations, but to accept the invitation, and to understand what features have been enabled, we need to click on this link here.</p>
<p>So if I select that link, and sign in to my account using my details and MFA code, then I can see that I have an invitation from AWS organizations. We can see the organization ID, the master account name, and the requested controls, which is enable all features. So here, I can either accept or decline and I’m going to accept. I just need to confirm the confirmation message about joining the organization.</p>
<p>Okay, now this member account is now a part of that organization. So if I go back to my master account now, I can see now that within my AWS organization of my master account, I have the CA demo account, which is the name of my other account, and we can see that it’s not a master because it hasn’t got the star whereas this account has the, this is the master account. So as you can see, it’s a very simple process to invite other accounts to your organization.</p>
<p>Now I also mentioned previously about organizing accounts and using organizational units. So if we select organize accounts, at the moment, we only have the root in here. So I can create the new organizational unit and assign each of these accounts into those. So, for example, let me create a new organizational unit called production.</p>
<p>Now I’m also going to create a second organizational unit called test. So let me create another one. At the moment, under root, we have our two accounts. So we have our master account and our member account here. Now I want to move my master account into the production organizational unit, just to make things a little more organized. So I can select the account, click on move, and then simply select where I want it to reside within the tree, and then click move, and we can see, it’s now been removed from the root location, and I want to do the same with the member account, but this time, I want to move that into the test OU. So now, if I click on production over here, this organizational unit, we can see the account that it has inside it, and again, if we go back to the root and click on test, we can see that we have the member account. So I just wanted to show you that quickly just to show you how you can easily and quickly organize your different AWS accounts.</p>
<p>Okay, and that’s the end of the demonstration.</p>
<h1 id="Securing-Your-Organizations-with-Service-Control-Policies"><a href="#Securing-Your-Organizations-with-Service-Control-Policies" class="headerlink" title="Securing Your Organizations with Service Control Policies"></a>Securing Your Organizations with Service Control Policies</h1><p>Hello and welcome to this lecture which will dive into Service Control Policies to understand how they can be used to secure your AWS Organization. SCPs are different from both identity-based and resource-based policies, which grant permissions to users, groups, and roles. However, SCPs do not actually grant permission themselves. Restrictions made within an SCP set a boundary of permissions for <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> accounts.</p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS, and EC2 via an identity-based policy. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2, despite having full access to S3. The SCP would serve to prevent that service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<p>So to be clear, an SCP does not grant access. They add a guardrail to define what is allowed. You will still need to configure your identity-based or resource-based policies to identities, granting permission to carry out actions within your accounts. If you want to use Service Control Policies to help you manage your security at an account level, then you need to ensure that you deploy AWS Organizations using the enable all features setting.</p>
<p>Before you start using SCPs, you first need to enable them from the root account of your organization. However, you need to ensure that you have the following permissions. From within your AWS Organizations console, navigate to the Policies tab and select the Service Control Policies option that will show the status of Disabled. Then, select the option to enable the SCPs as shown. At this point, SCPs will now be enabled at the root level of your organization and you can now begin to use Service Control Policies within your organization.</p>
<p>I now want to perform a demonstration showing you how to create a Service Control Policy and attach it to an account. In this demonstration, I will have two accounts; my master account called Stuart and another account called CA-Demo. In the CA-Demo account, I have a user, Alice, who has an IAM policy attached that allows full access to S3. From within my master account, I will create a new Service Control Policy denying access to the Amazon S3 service. I will then attach this new SCP to the CA-Demo account. I will then log in as Alice and try to access Amazon S3 to see the result. Let’s take a look.</p>
<p>Okay, so to start this demonstration, I’m logged in as the user Alice in this CA-Demo account which is my member account to my organization. And as I said previously, Alice only has access to Amazon S3. So if we take a look at S3, we should be able to get into the service without any issues. So we can get into the service which is great and also create a bucket as well, so a bucket for Alice. And there we have it. So this user Alice is able to access S3 and also create buckets as well.</p>
<p>Now what I want to do is swap back over to my master account of my AWS Organization, create a Service Control Policy to block access to Amazon S3 and then apply it to this account. So let’s do that next. Okay, so I’m now back in my master account and what I want to do is to create a new Service Control Policy. So from the AWS Organizations dashboard, if I click on Policies, now I already have my Service Control Policies enabled, if yours says disabled, simply select it and then you’ll have the option to enable it.</p>
<p>Once your Service Control Policies are enabled, if you select them, now we can create a policy. We have a default policy here which is FullAWSAccess which allows access to every operation and this is the default Service Control Policy that’s applied to the root of my AWS Organization. However, what I want to do is deny access to one of those services. So if I select create policy, I’ll just call this Deny S3, give it a description of deny access to S3, and then down here is where we can build our policy. So there’s different options.</p>
<p>Firstly, the statement and then the resource and any conditions. So to start with, we can see that over here our policy is in a deny state. Now if we wanted to allow access to a service, we’ll change that to allow. But I’m gonna leave that as deny and over here I wanna select my service that I want to deny so let me scroll down to S3, if I can just find it in the list, there we go, and I want to prevent all actions of S3. Now as we can see, it’s now updated this policy with the action of all actions to S3, but we don’t have the resource yet. So let’s now scroll down to add resource. So our service is S3 and our resource type would be all resources. So if I select add resource, we can now see that the policy has been updated again. So at the moment, it’s denying any action in S3 for all resources.</p>
<p>Now if I wanted to add a condition, then I could add any conditions in here with the condition key and qualifier, et cetera. For this demonstration, I’m not going to add any conditions.</p>
<p>Now I just need to create the policy. Okay, so that’s our policy created. Now although the policy has been created, it’s not actually attached to any organization unit or account thus yet. So let me go back to my accounts. Now if you remember from a previous demonstration, we had our member account, the CA-Demo account, within the Test OU. So I’m going to select that account, go across the Service Control Policies, and then we can see that we have this policy here that we just created and I want to attach that to this specific account. So if I select attach, we can now see that this account has two Service Control Policies, the FullAWSAccess which is filtered down from the root which allows access to all S3 resources, but we also have a Service Control Policy here that denies access to one of those services and the deny will always overrule an allow.</p>
<p>So now we’ve attached that Service Control Policy to the CA-Demo account which is the account that Alice is a part of. Let me now log back in as Alice into this account to see if I can now access S3 or if the Service Control Policy has been applied. So let’s take a look.</p>
<p>Okay, so I’m back in my CA-Demo account which is the member account. I’m logged in as Alice. So now if I go to S3, let’s see what happens. We have an error of access denied and we can’t see any buckets. So it looks as though that Service Control Policy has taken effect because I can’t access S3 at all. So let me see if I can create a bucket, if I just add in any name, and try to create, again we get an error of access denied. So we can see that the Service Control Policy has in fact now blocked access for Alice despite her having IAM permissions allowing her full access to S3. You would have noticed that in my list of SCPs from the demonstration I just carried out there was a pre-existing SCP there called FullAWSAccess and this was associated with the root object of my Organization, so how does the inheritance of SCPs work? Well, let’s take a look at an example.</p>
<p>Let’s suppose we had the following AWS Organization layout, with the FullAWSAccess SCP at the root. The objects within an Organization follow a parent-child relationship, with the Root being the parent to all other child objects. As you can see at the root level, we have an SCP that allows full access to AWS. I now want to establish what SCPs each of the five AWS accounts, highlighted in orange, is governed by. So looking at Account 1, looking from the root, we have the FullAWSAccess SCP. The next level down to Account 1 is the Dev OU. Now, this has four SCPs associated. However, these are number 2, 3, 5 and 6. So at this stage, only the services and features within these SCPs are allowed at this level.</p>
<p>Now if we go down further to Account #1, we have another OU, the Test OU, which again is governed by a list of SCPs, 3, 5 and 6. We can see at this level of the tree, SCP 2 has been dropped so this OU is now restricted to SCP 3, 5 and 6. Therefore, Account 1 which is a child of the Test OU is restricted and governed by the details set out in these three SCPs. Using this methodology, we can now look at the other accounts. So Account #2 is governed by SCP 2, 3, 5 and 6. Account #3 is governed by 1 and 6. And Accounts 4 and 5 are governed by SCP 4.</p>
<p>Let me now look at a different example, this time I’m going to remove the default SCP of FullAWSAccess at the root and replace it with custom SCPs as shown here. Now I’m going to assume in this scenario that every SCP shown has different service restrictions. So as you can see, at the root level this time we have four SCPs, number 1 through to 4, so let’s see how this affects the results this time for each account. So Account #1, looking from the root, we have SCPs 1 through to 4, the next level down to Account #1 is the Dev OU. Now, this also has four SCPs associated. However, these are number 2, 3, 5 and 6. As a result, this Dev OU is only controlled by SCPs 2 and 3. SCPs 5 and 6 are discarded as they are not a part of the parent relationship with the Root object, and 5 and 6 do not exist at any parent level.</p>
<p>Now if we go down further to Account 1, we have another OU, the Test OU, which again is governed by a list of SCPs, 3, 5 and 6. We already know that 5 and 6 are not allowed as they are not in the parent chain. However, SCP number 3 is. So SCP 3 exists from the root downwards in this OU. Therefore, Account #1 is restricted and governed by the details set out in SCP 3. Using this methodology, we can now look at the other accounts. So Account #2 is governed by SCP 2 and 3. Account #3 is governed by SCP 1. And Account #4 and #5 are governed by SCP 4.</p>
<p>Finally, before I end this lecture, please be aware of some of the characteristics of Service Control Policies. SCPs do not affect resource-based policies. They only affect principals managed by your accounts in your organization. SCPs affect all users and roles, in addition to the root user. However, the root user will still be able to change its own password including MFA settings, manage root access keys, and manage x.509 keys for the root user. If you disable SCPs in your organization, all SCPs are deleted and removed. Re-enabling SCPs again in the same organization will revert to the default SCP allowing FullAWSAccess. The following elements are not affected by SCPs: any actions performed by the master account, SCPs do not affect service-linked roles, and managing Amazon CloudFront keys.</p>
<p>That now brings me to the end of this lecture and to the end of this course. You should now have a greater understanding of how Service Control Policies can be used within your AWS Organization to help you centrally control the different levels of access between multiple accounts. If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="An-Overview-of-AWS-WAF"><a href="#An-Overview-of-AWS-WAF" class="headerlink" title="An Overview of AWS WAF"></a>An Overview of AWS WAF</h1><p>Hello, and welcome to this lecture where I shall give an introduction to the AWS WAF service. If you are delivering any kind of web content, either through CloudFront Distributions, Amazon API Gateway REST APIs, Application Load Balancers, or via AWS AppSync GraphQL APIs, then I would recommend you implement the AWS Web Application Firewall service as an additional layer of security.</p>
<p>Without using a web application firewall you could be exposing your websites and web apps to potentially harmful and malicious traffic, which could lead to security risks within your environment. This could have a significant detrimental impact on your business from both a financial and reputation perspective. The AWS Web Application Firewall is a service that helps to prevent websites or web applications from being maliciously attacked by common web attack patterns. Many of which are outlined in the OWASP top 10 list, such as SQL injection and cross-site scripting.</p>
<p>In addition to your own custom criteria, such as perhaps filtering request based on source IP address or country of origin. OWASP, the Open Web Application Security Project is a not-for-profit organization that is dedicated to helping others improve security and software. They provide a top 10 list of the most critical security vulnerabilities and risks surrounding application architecture. For the <a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">latest OWASP top 10 list</a>, please visit the following <a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">link</a>.</p>
<p>If you can implement a WAF within your architecture to mitigate against some of these vulnerabilities, then that’s a huge asset to your web application architecture and a great relief to the security officers within your organization. If you then compare the implementation and administration time needed to deploy AWS WAF to a standard WAF solution, then it’s by far quicker. Further, AWS WAF is far simpler and easy to manage as well. And there are currently two versions of AWS WAF. AWS WAF classic, and AWS WAF, sometimes referred to as new AWS WAF.</p>
<p>Now you should only use AWS WAF classic if you created AWS WAF resources prior to November, 2019. So in this course, I shall be focusing on new AWS WAF. The AWS WAF service interacts with Amazon API Gateway, Amazon CloudFront for its distributions, Application Load Balances, and AWS AppSync, ensuring only filtered web requests that meet specific conditions are forwarded to be processed by these services. WAF works with these other services to filter both HTTP and HTTPS requests by distinguishing between legitimate and harmful inbound requests that will then either be allowed or blocked.</p>
<p>Let’s take a closer look at what the WAF service is composed of to allow you to understand how it works. So there are a number of essential components relating to WAF. These being Web ACLs, Rules and Rule Groups. Let me explain each of these individually to see what part they play within the service, starting with the main building block of the configuration, the Web ACL. So Web Access Control lists, or web ACL, are the main building block of the WAF service. And it’s used as the component that is associated with one of the supported resources to determine which web requests are considered safe and which ones are not.</p>
<p>At the time of writing this course, the supported resources include Amazon CloudFront Distributions, Amazon API Gateway REST APIs, Application Load Balances, and AWS AppSync GraphQL APIs. The Web ACL contains rules, which contains specific controls and criteria checks that assess each web request to determine whether it should be allowed or blocked. Also for each Web ACL, there is a default action that traffic should take if the criteria set out in the rules are not met by the incoming request, and the options for this are either allow or block.</p>
<p>Rules. Each rule contains statements and actions, which focus on specific criteria that the web request will be inspected against. If the inspected request matches the criteria set out in the statement, then that is considered a match. The result of this match can then follow an action of allow, block, or count. Allow means the request is forwarded onto the resource. Block means the request is dropped and a response is sent back to the requester, informing them that the request was denied, and count simply counts the number of matching requests.</p>
<p>Rule Groups. A Rule Group is essentially a collection of rules that you can apply to different Web ACLs as you need to. AWS WAF also comes pre-configured with a number of AWS manageable groups that have been built and designed to protect your resources against some common attack patterns. In addition to this, you can also get access to other Rule Groups created and sold by other vendors on the AWS marketplace. So from a logical architecture perspective, let’s assume that WAF has been associated with a CloudFront Distribution with its origin pointing to S3. The network diagram would look as follows at a high level.</p>
<p>So firstly, a user would initiate a request to the web content being served by the CloudFront Distribution. Next, although logically AWS WAF sits in front of CloudFront, the request will be received by the CloudFront Distribution first. Then it’s immediately forwarded to your associated WAF Web ACL. The AWS WAF Web ACL associated with the distribution would filter the incoming web traffic using the Rules or Rule Groups. So before it’s even traversed your CloudFront environment and network, you have the ability to detect, analyze, and either allow or block the incoming request.</p>
<p>If the criteria highlighted by the Rules deemed that the request should be blocked, then the traffic would be stopped and prevented from progressing further. The requester would then be informed the request was denied. If the traffic met the criteria, allowing the traffic to pass, then the request would be forwarded to CloudFront. CloudFront would then serve the content as required. You might already have other security detection mechanisms within your organization that operate deeper within your infrastructure, perhaps at the web server layer, to mitigate against some of the same risks that WAF does.</p>
<p>And so you may be thinking, why should I implement WAF if I have this existing solution, which is working okay? If you have existing detection systems within your infrastructure, then that’s great. However, the closer they are logically implemented to your web application, then the greater the risk of additional vulnerabilities occurring towards the edge of your infrastructure. It’s best to mitigate vulnerability risks as close to the perimeter of your network environment as possible. By doing so, reduces the chances of other infrastructure and systems being compromised.</p>
<h1 id="AWS-Firewall-Manager-and-Prerequisites"><a href="#AWS-Firewall-Manager-and-Prerequisites" class="headerlink" title="AWS Firewall Manager and Prerequisites"></a>AWS Firewall Manager and Prerequisites</h1><p>Hello, and welcome to this lecture, where I should provide an overview of AWS Firewall Manager, so, you can understand what the service is used for. The core function of AWS Firewall Manager is to help you simplify the management of being able to provide security protection to a range of different resources, between multiple AWS accounts. It’s the fact that it works across multiple account infrastructure, that gives this service a lot of power from a security perspective. So, it’s a great tool to become familiar with, if you are responsible for security across more than one AWS account.</p>
<p>Once your configured security policies to govern the protections that you require for your resources, AWS Firewall Manager, will then automatically apply this protection in addition to managing this protection for any newly creative resources, that match your configuration across any of your accounts that it has responsibility for. So, once it’s set up, the management and protection efforts are simplified dramatically, across your entire organization.</p>
<p>The current AWS services and resources that Firewall Manager provides protection for and integrate with, include the following; <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/protecting-web-apps-aws-waf-shield-firewall-manager/introduction/">AWS WAF, AWS Shield Advanced, AWS Network Firewall</a>, VPC Security Groups and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/amazon-route-53/">Amazon Route 53</a> Resolver DNS Firewall. In addition to these resources that are protected, Firewall Manager is also closely integrated with AWS Organizations. In fact, running AWS Organizations is a prerequisite of using Firewall Manager. For those I’m familiar with AWS Organizations, it’s a service which provides a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization.</p>
<p>Let’s look at the prerequisites of Firewall Manager in a little more detail, to allow you to begin using the service. So, the first step is to decide which AWS account will be used as your Firewall Manager Administrator account. And this account will be used to essentially manage your security policies. Next, you must ensure that this account is a part of an AWS Organization. However, the that it joins must be configured with all features enabled, and not just consolidated billing.</p>
<p>When your account has successfully joined an AWS Organization, you must then configure AWS Firewall Manager within that account, as the Firewall Manager Administrator Account. And this administrator account is used to create a manager security policies. To delegate your account as the administrator, open the Firewall Manager Console, select, get started and enter the account number of your AWS account. Once you’ve added your AWS account to an AWS Organization and designated the Firewall Manager administrative account, you’ll see confirmation ticked on the Firewall Manager dashboard as seen to reflect that you have met these prerequisites.</p>
<p>Next, you must enable AWS config for your account, and for any other account in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/securing-aws-organizations-with-service-control-policies-scps/introduction/">AWS Organization</a> that you want to manage resource security for. And it must be enabled for each region in that account, in which the resources reside. If you don’t want to enable AWS conflict for all resources in each of your accounts, then you must ensure that you enable the following depending on which resources you want Firewall Manager to secure. The next step is optional, depending on if you are looking to apply security policies for all Network Firewalls and DNS Firewalls.</p>
<p>Then you must enable sharing with AWS Organizations in AWS Resource Access Manager. By doing so, it allows you to deploy security policies to these resource types, using Firewall Manager across your accounts in your organization. To complete this configuration, you must open the settings page in the AWS Resource Access Manager Console, and then from here, select, enable sharing with AWS Organizations, and then select, safe settings.</p>
<p>The final step allows Firewall Manager to manage resources in regions, that might be disabled by default. So, you must enable these regions before you can create and managed resources within them. These regions must being enabled in the AWS management account, for your AWS Organization, in addition to the AWS account designated as your Firewall Administrator account. Enabling a region is a simple process. From within the AWS Management Console, navigate to the top right corner and select your account, and then select my account, scroll down to regions section and select, enable in the action column, for the regions that you would like to enable. Once you’ve completed these initial steps you are ready to begin configuring AWS Firewall Manager and its policies.</p>
<h1 id="What-is-AWS-Shield"><a href="#What-is-AWS-Shield" class="headerlink" title="What is AWS Shield?"></a>What is AWS Shield?</h1><p>Hello and welcome to this section of the course focusing on the third and final service, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Shield. AWS Shield is closely related to both AWS WAF and also the AWS Firewall Manager. So what is it used for? Well, AWS Shield has been designed to help protect your infrastructure against distributed denial of service attacks, commonly known as DDoS. These attacks are very common, and the attack itself targets a host which might be running a website or web application, and it receives a huge number of requests simultaneously sent maliciously by an attacker from multiple distributed sources. This increase and flood of traffic aims to prevent legitimate requests getting through to host and being processed, while at the same time severely hindering the performance of the application or website. So much so in fact, that users often think the site is down. </p>
<p>There are a number of different types of DDoS takes that can take place, for example, a SYN Flood. In a Syn Flood attack, a large number of connections are made to the host under attack. The host will then respond accordingly with an SYN&#x2F;ACK packet, at which point the client sending the original connection request would normally respond with another SYN, completing the three-way handshake to allow communications to begin. However, this final SYN packet is not sent to the host, and this leaves a huge number of open connections on the host, resulting in diminished resources available to process legitimate requests. DNS Query Flood. By using multiple DNS queries an attacker can drain the resources against a DNS server, such as Route 53 in AWS. HTTP flood and cache-busting attacks. These attacks operate at layer seven, the application layer. And during an HTTP flood attack an attacker sends a large amount of HTTP requests, which may include POST and GET requests to a host, consuming the resources available. Cache-busting attacks are similar to HTTP floods, however, by using the HTTP request query string they are able to force content to be retrieved from the originating server, rather than from an edge location, which impacts the performance of the source servers available resources unnecessarily. AWS Shield itself is available at two different levels of features, AWS Shield Standard and AWS Shield Advanced, and AWS shield advanced has a lot more power and protection on offer than standard. AWS Shield Standard is free to everyone, well, at least anyone who has an AWS account, and it offers DDoS protection against some of the more common layer three, the network layer, and layer four, transport layer, DDoS attacks. This protection is integrated with both CloudFront and Route 53. </p>
<p>AWS Shield advanced offers a greater level of protection for DDoS attacks across a wider scope of AWS services for an additional cost. This advanced level offers protection against your web applications running on EC2, CloudFront, ELB and also Route 53. In addition to these additional resource types being protected, there are enhanced levels of DDoS protection offered compared to that of Standard. And you will also have access to a 24-by-seven specialized DDoS response team at AWS, known as DRT. With these additional features, the advanced level also provides an enhanced monitoring capability allowing you to view real-time metrics of any attacks against your resources. Whereas the Standard version of Shield offered protection against layer three and layer four, Advanced also offers protection against layer seven, application, attacks. Another great advantage is the fact that you also get cost protection as a part of the plan, whereby your resources may scale suddenly and unexpectedly to cope with the rise in traffic. From a cost perspective, if your decide to go with AWS Shield Advanced then you also get AWS WAF included in the same price, and this price is currently $3,000 a month, plus data transfer fees. As you can see from this image, there are a significant amount of advantages with the Advanced version of AWS Shield over Standard. That now brings me to the end of this lecture.</p>
<h1 id="Amazon-Inspector-Basic-Features"><a href="#Amazon-Inspector-Basic-Features" class="headerlink" title="Amazon Inspector Basic Features"></a>Amazon Inspector Basic Features</h1><p>You need to be responsible for the security of applications, processes, and tools that run on EC2 instances. Amazon Inspector lets you analyze your deployed EC2 instances and helps you identify potential security issues. Some of the basic features include a knowledge base with hundreds of rules that are mapped to common security compliance standards and vulnerability definitions. These rules are regularly updated by AWS security experts. You can install an agent in the operating system of an Amazon EC2 instance to monitor behavior like network, file system, and process activity.</p>
<p>You can also automate vulnerability assessments to make security testing of EC2 instances a regular part of your cloud operations. As a result, Amazon Inspector gives you a prioritized list of findings. A sample list of findings is shown. Amazon inspector is all about protecting the security of your EC2 instances.</p>
<h1 id="Amazon-Guard-Duty-Basic-Features"><a href="#Amazon-Guard-Duty-Basic-Features" class="headerlink" title="Amazon Guard Duty Basic Features"></a>Amazon Guard Duty Basic Features</h1><p>Amazon GuardDuty is an intelligent threat detection service that provides you with an accurate way to consistently monitor and protect your AWS accounts and workloads for suspicious activity. We’re talking about intelligent threat identification for your accounts, data, and workflows. It uses trained machine learning models to identify suspicious user and resource behaviors. It also learns from your environment to eliminate false positive identifications.</p>
<p>Amazon GuardDuty is able to analyze CloudTrail logs, VPC flow logs, and DNS query logs to identify issues worth looking into. An interesting item about GuardDuty is that sample findings help you analyze the type of results that GuardDuty delivers. When you generate sample findings, GuardDuty populates your current findings list with one sample finding of each type.</p>
<p>Don’t forget, GuardDuty is able to display its results to AWS Security Hub. Generating sample findings will allow you to verify AWS Security Hub’s functionality sooner more than later. Let’s take a look at some sample results from the GuardDuty screens. As a result, Amazon GuardDuty gives you a listing of findings classified under three categories, low, medium, and high severity.</p>
<p>In this screen of the AWS console, low severity findings are marked in blue with a small circle next to the finding. Medium severity findings are marked by GuardDuty in orange with a small square next to the finding. High severity findings are marked by GuardDuty in red with a small triangle next to the finding. Also notice how, on the top right, you have a findings summary showing the total for each of the severity categories.</p>
<h1 id="Amazon-Macie-Basic-Features"><a href="#Amazon-Macie-Basic-Features" class="headerlink" title="Amazon Macie Basic Features"></a>Amazon Macie Basic Features</h1><p>Amazon Macie uses machine learning to do its work and helps you discover and analyze sensitive data stored in Amazon S3 buckets, including personal identifiable information, or PII, such as names, addresses, credit card numbers, API Keys, and access credentials among many others. Macie scans S3 buckets and recognizes critical private information. It also automatically tracks changes to buckets and only evaluates new or modified objects in future scans. That way, it doesn’t have to review objects that have not changed and makes the discovery job significantly more efficient and scalable. You can run one-time or automated data discovery and display the results to AWS Security Hub.</p>
<p>Amazon Macie provides a list of findings where the severity and finding type are clearly displayed. In this case, we created a bucket called academy-ca-macie and uploaded a file with disabled user keys, an RDS SQL Query, a credit card list in CSV format, and a few other files. Notice the severity as medium or high. Also, notice the finding type for the S3 objects include Personal, Financial, and Credentials. It also points to the resource that is affected. Finally, it shows when the object was last scanned. In the future, unless there is a change, these objects will not be re-evaluated.</p>
<h1 id="2Finding-Compliance-Data-With-AWS-Artifact"><a href="#2Finding-Compliance-Data-With-AWS-Artifact" class="headerlink" title="2Finding Compliance Data With AWS Artifact"></a>2<strong>Finding Compliance Data With AWS Artifact</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/resource/aws-shared-responsibility-model">AWS Shared Responsibility Model</a></p>
<h1 id="3What-is-Identity-and-Access-Management"><a href="#3What-is-Identity-and-Access-Management" class="headerlink" title="3What is Identity and Access Management?"></a>3<strong>What is Identity and Access Management?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/">Identity Federation course</a></p>
<h1 id="4IAM-Features"><a href="#4IAM-Features" class="headerlink" title="4IAM Features"></a>4<strong>IAM Features</strong></h1><p><a target="_blank" rel="noopener" href="https://sts.amazonaws.com/">STS Global Endpoint</a></p>
<h1 id="8Managing-Multiple-Users-with-IAM-User-Groups"><a href="#8Managing-Multiple-Users-with-IAM-User-Groups" class="headerlink" title="8Managing Multiple Users with IAM User Groups"></a>8<strong>Managing Multiple Users with IAM User Groups</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-quotas.html">Group limitations</a></p>
<h1 id="10Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf"><a href="#10Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf" class="headerlink" title="10Using AWS Service Roles to Access AWS Resources on Your Behalf"></a>10<strong>Using AWS Service Roles to Access AWS Resources on Your Behalf</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html">AWS services that work with IAM</a></p>
<h1 id="11Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users"><a href="#11Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users" class="headerlink" title="11Using IAM User Roles to Grant Temporary Access for Users"></a>11<strong>Using IAM User Roles to Grant Temporary Access for Users</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-cross-account-access-using-iam/">Implementing cross-account roles using IAM</a></p>
<h1 id="12Using-Roles-for-Federated-Access"><a href="#12Using-Roles-for-Federated-Access" class="headerlink" title="12Using Roles for Federated Access"></a>12<strong>Using Roles for Federated Access</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-cognito-manage-authentication-authorization-mobile-web-apps-1560/">Using Amazon Cognito to Manage Authentication &amp; Authorization to your Mobile and Web Apps</a></p>
<h1 id="17What-is-AWS-Trusted-Advisor"><a href="#17What-is-AWS-Trusted-Advisor" class="headerlink" title="17What is AWS Trusted Advisor?"></a>17<strong>What is AWS Trusted Advisor?</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">Best Practice Checklist</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html">Full list of IAM permissions</a></p>
<h1 id="21An-Overview-of-AWS-WAF"><a href="#21An-Overview-of-AWS-WAF" class="headerlink" title="21An Overview of AWS WAF"></a>21<strong>An Overview of AWS WAF</strong></h1><p><a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">OWASP Top Ten List</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:48" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:48-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:24" itemprop="dateModified" datetime="2022-11-20T19:08:24-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:47" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:47-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:34" itemprop="dateModified" datetime="2022-11-20T19:08:34-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:45" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:45-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:00:22" itemprop="dateModified" datetime="2022-11-20T19:00:22-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Management-Fundamentals-CLF-C01-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Management Fundamentals in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce some management services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#112;&#112;&#x6f;&#x72;&#x74;&#x40;&#99;&#108;&#x6f;&#117;&#x64;&#x61;&#99;&#97;&#100;&#x65;&#109;&#121;&#46;&#x63;&#111;&#109;">&#x73;&#x75;&#112;&#112;&#x6f;&#x72;&#x74;&#x40;&#99;&#108;&#x6f;&#117;&#x64;&#x61;&#99;&#97;&#100;&#x65;&#109;&#121;&#46;&#x63;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various management services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to various management services in AWS that can assist with auditing, reporting, and monitoring within your AWS environments, including:</p>
<ul>
<li>AWS CloudTrail,</li>
<li>AWS Config, and</li>
<li>Amazon CloudWatch.</li>
</ul>
<p>These services are covered by Domain 2 in the official AWS Certified Cloud Practitioner exam blueprint: Security and Compliance, which accounts for 25% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="AWS-CloudTrail"><a href="#AWS-CloudTrail" class="headerlink" title="AWS CloudTrail"></a>AWS CloudTrail</h1><p>Hello, and welcome to this lecture. In this lecture I will explain the basic fundamentals of AWS CloudTrail to give you an overview of the service before we look deeper at the inner workings, revealing how the different elements work together. So what is CloudTrail and what does it do? CloudTrail is a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface, from within the AWS management console, or even from a request made by another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service.</p>
<p>For example, when auto scaling automatically sends and API request to launch or terminate an instance. These API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file, which is then stored on S3. Each API call represents a new event within the log file. CloudTrail also records and associates other identifying metadata with all the events.</p>
<p>For example, the identity of the caller, the timestamp of when the request was initiated, and the source IP address. In a later lecture entitled Insight Into CloudTrail Logs, I will look at these log files deeper, while I’ll provide an example of a log file showing the different attributes recorded. For greater management, new log files are typically created every five minutes. Which are then delivered and stored within an S3 bucket that is defined by you during your CloudTrail configuration.</p>
<p>This allows you to easily go back and review the history of all API requests made. There’s also an option to have these logs delivered to a CloudWatch Logs log file as well. Having this association with CloudWatch enables custom metrics to be converted to monitor specific API requests. Thresholds can be set against these metrics and when crossed, the simple notification service, SNS, can be triggered to notify your security teams to investigate. That, at a very high level, is the overall function of the AWS CloudTrail service.</p>
<p>Now let’s take a look at the CloudTrail architecture to understand where it can be implemented from an AWS region standpoint, and which services can be supported. AWS CloudTrail is a global service with support for all regions. Support for the latest region, EU London was added in mid December, 2016. In addition to this worldwide coverage, CloudTrail also provides support for over 60 AWS services and features across a wide-range of service categories.</p>
<p>As you can imagine, with this extensive coverage, CloudTrail can capture a vast amount of data if you have a multi-region, multi-service infrastructure environment deployed. So armed with this information, what can you do with it? How can you use this data to help you manage and support your AWS infrastructure? Well there are a number of ways you can use the data captured by CloudTrail to help you enhance your AWS environment. Firstly, and as mentioned earlier, it can be used very effectively as a security analysis tool. CloudTrail events provide very specific information about where an API call originated from and who, or what initiated the request.</p>
<p>As a result, if malicious activity was detected via irregular trends or restricted API call thresholds, with the use of CloudWatch, then a number of security controls can be quickly implemented to prevent the user from causing additional damage. Another common use for CloudTrail is to help resolve and manage day to day operational issues and problems. Using built-in filtering mechanisms, it’s possible to quickly find who, what and when a particular API was used, which could have potentially caused an outage or service interruption. This enables quicker root cause identification, resulting in a speedy resolution.</p>
<p>Appropriate actions can then be taken to ensure the incident does not reoccur in your environment. As API calls to add, modify or delete results are captured CloudTrail can be an effective method of tracking changes to resources within your environment. There is another AWS service that is specifically designed to order and track changes to resources, which is called AWS Config, which CloudTrail interacts with, however, CloudTrail can be used to capture the actual API request, and all associated data which made the change. And if you’re not using AWS Config then this at least provides some base level of monitoring and tracking.</p>
<p>From a governance and security legislation perspective, many certifications require the ability to recall and provide evidence of log files relating to specific changes to resources. CloudTrail provides all of this by default, through the use of capturing events and writing them to a log file, which is then stored on S3. AWS has a great whitepaper on achieving compliance, using CloudTrail entitled Logging in AWS, How AWS CloudTrail Can Help You Achieve Compliance by logging API calls and Changes to Resources.</p>
<p>The following URL will take you to that whitepaper. If you need to be able to capture and track API requests within your AWS account, for any of these reasons mentioned, or perhaps for other reasons you may have of your own, then CloudTrail can do this for you and deliver the output as a log file into an S3 bucket of your choice.</p>
<p>Lectures:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/introduction-15/">Management fundamentals for AWS</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-config-2/">AWS Config</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-cloudtrail-1/">AWS CloudTrail</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/amazon-trusted-advisor-2/">Amazon Trusted Advisor</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/management-fundamentals-of-aws-for-sol-arch/amazon-clouwatch-1/">Amazon CloudWatch</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-personal-health-dashboard-2/">AWS Personal Health Dashboard</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/summary-36/">Summary</a></li>
</ul>
<h1 id="AWS-Config"><a href="#AWS-Config" class="headerlink" title="AWS Config"></a>AWS Config</h1><p>Hello and welcome to this lecture where we will talk about the AWS Config service, itself, what it is, and what it does. So let’s get started. As many of you will be aware, one of the biggest headaches in any organization when it comes to resource management of IT infrastructure is understanding the following: What resources do we have? What devices are out there within our infrastructure performing functions? Do we have resources that are no longer needed and therefore can we be saving money by switching them off? What is the status of their current configuration?</p>
<p>Are there any security vulnerabilities we need to worry about? How are our resources linked within the environment? What relationships are there and are there any dependencies? If we make a change to one resource, will this effect another? What changes have occurred on the resources and by whom? Do we have a history of changes for this resource that shows us how the resource has changed over time? Is the infrastructure compliant with specific governance controls and how can we check to ensure that this configuration is meeting specific internal and external requirements?</p>
<p>And do we have accurate auditing information that can be passed to external auditors for compliance checks? Depending on the size of your deployment with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, trying to answer some of these questions can be very time-consuming and laborious. Some of this information can be captured by the AWS CLI by performing a describe, or list, against the specific resource. But implementing a system to capture those results and output them into a readable format could be very resource-intensive.</p>
<p>And of course, this will only help you with a small piece of the puzzle. AWS is aware that due to the very nature of the cloud and its benefits, the resources within an AWS environment are likely to fluctuate frequently, along with the configurations of the resources. The cloud by its very nature is designed to do so, and so trying to keep up with the resource management can be a struggle. Because of this, AWS released AWS Config to help with this very task.</p>
<p>The service has been designed to record and capture resource changes within your environment, allowing you to perform a number of actions against the data that helps to find answers to the questions that we highlighted previously. So what did AWS design AWS Config to do? Well, in a nutshell, AWS Config can capture resource changes. So any change to a resource supported by Config can be recorded, which will record what change along with other useful metadata all held within a file known as a configuration item, a CI.</p>
<p>It can act as a resource inventory. AWS Config can discover supported resources running within your environment, allowing you to see data about that resource type. You can store configuration history for individual resources. The service will record and hold all existing changes that have happened against the resource, providing a useful history record of changes. It can provide a snapshot in time of current resource configurations. An entire snapshot of all supported resources within a region can be captured that will detail their current configurations with all related metadata.</p>
<p>Enable notifications of when a change has occurred on a resource. The Simple Notification Service, SNS, is used with AWS Config to capture a configuration stream of changes, enabling you to process and analyze the changes to resources. It can provide the information on who made the change and when, through AWS CloudTrail integration. AWS CloudTrail is used with AWS Config to help you identify who made the change and when, and with which API. You can enforce rules that check the compliancy of your resource against specific controls. Predefined and custom rules and be configured with AWS Config, allowing you to check resources’ compliance against these rules. You can perform security analysis within your AWS environment.</p>
<p>A number of security resources can be recorded and when this is coupled with rules relating to security, such as encryption checks, this can become a powerful analysis tool. And it can provide relationship connectivity information between resources. The AWS management console provides a great relationship query, allowing you to quickly see and identify which resources are related to any other resource. For example, when looking at an EBS volume, you’ll be able to see which EC2 instance it is connected to.</p>
<p>And it does all of this and presents the data in a friendly format. This is a lot of incredibly useful data that can be used across a range of different scenarios, some of which we will cover later in this course. Now unfortunately at the time we’re writing this course, the AWS Config service does not capture this information for all services. But it certainly captures data for the most common services and resources, which you would want to hold information for.</p>
<p>Services such as: EC2, RDS, IAM, and VPC. And it’s great to see that within each of these, there are specific security resources that are covered, such as security groups and custom IAM policies. This makes AWS Config very useful when it comes to carrying out a security analysis, which we will cover in a later lecture. For more information on the latest resources that AWS Config supports, please see the link on-screen.</p>
<p>AWS Config is region-specific, meaning that if you have resources in multiple regions, then you will have to configure AWS Config for each region you want to record resource changes for. When doing so, you are able to specify different options for each region. For example, you could configure Config in one region to record all supported resources across all services within that region and add a pre-defined AWS manage config rule that would check if EBS volumes are encrypted.</p>
<p>In another region, you could select to only record a specific type of resource, such as security groups, with no pre-defined rules allocated. Some of you may be wondering, what if the service you want to monitor is not region-specific, such as IAM? Well in this case, there is a separate option to include global services, which IAM falls under.</p>
<p>Lectures:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/introduction-15/">Management fundamentals for AWS</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-config-2/">AWS Config</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-cloudtrail-1/">AWS CloudTrail</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/amazon-trusted-advisor-2/">Amazon Trusted Advisor</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/management-fundamentals-of-aws-for-sol-arch/amazon-clouwatch-1/">Amazon CloudWatch</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/aws-personal-health-dashboard-2/">AWS Personal Health Dashboard</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-management-fundamentals/summary-36/">Summary</a></li>
</ul>
<h1 id="What-is-Amazon-CloudWatch"><a href="#What-is-Amazon-CloudWatch" class="headerlink" title="What is Amazon CloudWatch?"></a>What is Amazon CloudWatch?</h1><p>Hello and welcome to this lecture which will provide you with a high-level overview of what Amazon CloudWatch is and does.</p>
<p>Amazon CloudWatch is a global service that has been designed to be your window into the health and operational performance of your applications and infrastructure. It’s able to collate and present meaningful operational data from your resources allowing you to monitor and review their performance. This gives you the opportunity to take advantage of the insights that CloudWatch presents, which in turn can trigger automated responses or provide you with the opportunity and time to make manual operational changes and decisions to optimize your infrastructure if required. </p>
<p>Understanding the health and performance of your environment is one of the fundamental operations you can do to help you minimize incidents, outages and errors. As a result Amazon CloudWatch is heavily used by those in an operational role and site reliability engineers. </p>
<p>There are a wide range of components to Amazon CloudWatch, making this an extremely powerful service. Let me now run through at a high level some of these features and what they allow you to do, including CloudWatch Dashboards, CloudWatch Metrics and Anomaly Detection, CloudWatch Alarms, CloudWatch EventBridge, CloudWatch Logs, CloudWatch Insights.</p>
<p>Using the AWS Management console, the AWS CLI, or the PutDashboard API, you can build and customize a page using different visual widgets displaying metrics and alarms relating to your resources to form a unified view. These dashboards can then be viewed from within the AWS Management Console.</p>
<p>Here is an example of the different types of widgets you can select to build your dashboard.</p>
<p>The resources within your customized dashboard can be from multiple different regions making this a very useful feature. Being able to build your own views, you can quickly and easily design and configure different dashboards to represent the data that you need to see from a business and operational perspective. For example, you might need to view all performance metrics and alarms from resources relating to a particular project, or a specific customer. Or you might want to create a different dashboard for a specific region or application deployment. The key point is that they are fully customizable to be designed how YOU want to represent your data.  </p>
<p>For more information of selecting the right chart type to visualize data, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/">https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/</a></p>
<p>Once you have built your Dashboards, you can easily share them with other users, even those who may not have access to your AWS account. This allows you to share the findings gathered by CloudWatch with those who may find the results interesting and beneficial to their day-to-day operational role, but don’t necessarily require the need to access your AWS account.</p>
<p>Metrics are a key component and fundamental to the success of Amazon CloudWatch, they enable you to monitor a specific element of an application or resource over a period of time while tracking these data points. For example, the number of DiskReads or DiskWrites on an EC2 instance, these are just 2 metrics relating to EC2 that you can monitor. Different services will offer different metrics, for example, there is no DiskReads for Amazon S3 as it’s not a compute service, and so instead metrics relevant to the service are available, such as NumberOfObjects, which tracks the number of objects in a specified bucket.</p>
<p>By default when working with Amazon CloudWatch, everyone has access to a free set of Metrics, and for EC2, these are collated over a time period of 5 minutes. However, for a small fee, you can enable detailed monitoring which will allow you to gain a deeper insight by collating data across the metrics every minute. In addition to detailed monitoring, you can also create your own custom metrics for your applications, using any time-series data points that you need, but be aware that when you create a metric they are regional, meaning that any metrics created in 1 region will not be available in another.</p>
<p>CloudWatch metrics also allow you to enable a feature known as anomaly detection. This allows CloudWatch to implement machine learning algorithms against your metric data to help detect any activity that sits outside of the normal baseline parameters that are generally expected. Advance warning of this can help you detect an issue long before it becomes a production problem.</p>
<p>Amazon CloudWatch Alarms tightly integrate with Metrics that I just discussed and they allow you to implement automatic actions based on specific thresholds that you can configure relating to each metric.</p>
<p>For example, you could set an alarm to activate an auto scaling operation, such as provisioning another instance if your CPUUtilization of an EC2 instance peaked at 75% for more than 5 minutes. You could also configure an alarm to send a message to an SNS Topic when the same instance drops back below the 75% threshold, causing it to come out of an ‘alarm’ state notifying engineers of the change. </p>
<p>For more information on SNS, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/">https://cloudacademy.com/course/using-sqs-sns-ses/</a></p>
<p>Speaking of Alarm states, there are 3 different states for any alarm associated with a metric, these being OK – The metric is within the defined configured threshold, ALARM – The metric has exceeded the thresholds set, and INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.</p>
<p>CloudWatch alarms are also easily integrated with your dashboards as well, allowing you to quickly and easily visualize the status of each alarm. When an alarm is triggered into a state of ALARM, it will turn red on your dashboard, giving a very obvious indication.</p>
<p>CloudWatch EventBridge is a feature that has evolved from an existing feature called Amazon Events. So if you have any prior experience working with CloudWatch Events then this will be fairly familiar to you.  </p>
<p>CloudWatch EventBridge provides a means of connecting your own applications to a variety of different targets, typically AWS services, to allow you to implement a level of real-time monitoring, allowing you to respond to events that occur in your application as they happen.  </p>
<p>But what is an event? Basically, an event is anything that causes a change to your environment or application.</p>
<p>The big benefit of using CloudWatch EventBridge is that it offers you the opportunity to implement a level of event-driven architecture in a real-time decoupled environment. </p>
<p>EventBridge establishes a connection between your applications and specified targets to allow a data stream of events to be sent. Currently, there is a wide range of targets that can be used as a destination for events as you can see here.</p>
<p>For the latest list of targets, please see the relevant documentation here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html</a></p>
<p>Let me provide a quick level overview of some of the elements of this feature, and these include Rules, Targets, and Event Buses.</p>
<p>So starting with Rules. A rule acts as a filter for incoming streams of event traffic and then routes these events to the appropriate target defined within the rule. The rule itself can route traffic to multiple targets, however the target must be in the same region. </p>
<p>Next, we have Targets. We saw a list of these just a few moments ago, so targets and where the events are sent by the Rules, such as AWS Lambda, SQS, Kinesis or SNS. All events received by the target are done os in a JSON format</p>
<p>Now finally, Event Buses. An Event Bus is the component that actually receives the Event from your applications and your rules are associated with a specific event bus. CloudWatch EventBridge uses a default Event bus that is used to receive events from AWS services, however, you are able to create your own Event Bus to capture events from your own applications. </p>
<p>CloudWatch Logs gives you a centralized location to house all of your logs from different AWS services that provide logs as an output, such as CloudTrail, EC2, VPC Flow logs, etc, in addition to your own applications.</p>
<p>When log data is fed into Cloudwatch Logs you can utilize CloudWatch Log Insights to monitor the logstream in real time and configure filters to search for specific entries and actions that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. </p>
<p>An added advantage of CloudWatch logs comes with the installation of the Unified CloudWatch Agent, which can collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. This metric data is in addition to the default EC2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. </p>
<p>There are now 3 different types of insights within CloudWatch, there are Log Insights, Container Insights, and Lambda Insights.</p>
<p>But what exactly are insights? Well as the name suggests, they provide the ability to get more information from the data that CloudWatch is collecting. So let’s look at each of these at a high level to understand the role that they perform, starting with Log Insights.</p>
<p>This is a feature that can analyze your logs that are captured by CloudWatch Logs at scale in seconds using interactive queries delivering visualizations that can be represented as bar, line, pie, or stacked area charts. The versatility of this feature allows you to work with any log file formats that AWS services or your applications might be using.</p>
<p>Using a flexible approach, you can use Log insights to filter your log data to retrieve specific data allowing you to gather insights that you are interested in. Also using the visual capabilities of the feature, it can display them in a visual way.</p>
<p>Much like Log insights, Container Insights allow you to collate and group different metric data from different container services and applications within AWS, for example, the Amazon Elastic Kubernetes Service, (EKS) and the Elastic Container Service (ECS). </p>
<p>In addition to the standard metrics collected for these services by CloudWatch, Container Insights also allows you to capture and monitor diagnostic data giving you additional insights into how to resolve issues that arise within your container architecture. This monitoring and insight data can be analyzed at the cluster, node, pod, and task level making it a valuable tool to help you understand your container applications and services.</p>
<p>As you may have guessed by now, this feature provides you the opportunity to gain a deeper understanding of your applications using AWS Lambda. Working on the principles as we have seen with the previous 2 insight features, it gathers and aggregates system and diagnostic metrics related to AWS Lambda to help you monitor and troubleshoot your serverless applications.</p>
<p>To enable Lambda Insights, you need to enable the feature per Lambda function that you create within Monitoring Tools section of your function:</p>
<p>This ensures that a CloudWatch extension is enabled for your function allowing it to collate system-level metrics which are recorded every time the function is invoked.</p>
<h1 id="4What-is-Amazon-CloudWatch"><a href="#4What-is-Amazon-CloudWatch" class="headerlink" title="4What is Amazon CloudWatch?"></a>4<strong>What is Amazon CloudWatch?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/course-introduction/">Data Visualization: How to Convey your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-sqs-sns-ses/">Using SQS, SNS and SES in a Decoupled and Distributed Environment</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">List of metrics</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">List of targets</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:44" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:44-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:00:10" itemprop="dateModified" datetime="2022-11-20T19:00:10-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Decoupled-and-Serverless-Architectures-CLF-C01-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Decoupled and Serverless Architectures in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the concepts of decoupled and event-driven architectures, as well as some serverless services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about decoupled and serverless architectures in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to decoupled and event-driven architectures in AWS. We’ll see how services such as the Amazon Simple Queue Service, or SQS; the Amazon Simple Notification Service, or SNS; and Amazon Kinesis can be used within these architectures. And finally, we’ll provide a survey-level overview of serverless services in AWS.</p>
<p>These topics and services are covered by Domains 1 and 3 in the official AWS Certified Cloud Practitioner exam blueprint: Cloud Concepts and Technology. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="What-is-a-Decoupled-and-Event-Driven-Architecture"><a href="#What-is-a-Decoupled-and-Event-Driven-Architecture" class="headerlink" title="What is a Decoupled and Event-Driven Architecture?"></a>What is a Decoupled and Event-Driven Architecture?</h1><p>Hello and welcome to this lecture where I want to explain what we mean by decoupled and event-driven architectures.</p>
<p>Firstly, let me focus on decoupled architecture, and to understand decoupling, we first need to address monolithic architectures which is how applications have been done in the past. Monolithic applications were built with a close and tight-knit relationship to each other, for example, between the front end and back end of an application. If a change was made the back end, it could easily disrupt services and operation in the front end, and that’s because they were very tightly coupled together and had a lot of built-in dependencies against each other. Although this had some advantages, it wasn’t able to offer what a decoupled architecture could.</p>
<p>When you implement and design a solution using a decoupled architecture you are building a solution put together using different components and services that can operate and execute independently of one another, instead of being closely coupled to each of its connecting components to operate and function. Each component in a decoupled solution is effectively unaware of any other changes to other components due to the segregation of boundaries applied.</p>
<p>Each service within a decoupled environment communicates with others using specific interfaces which remain constant throughout its development regardless of what configurations are made. By having this layered and independent approach, you are able to design, develop and configure each component without worrying about any dependencies within your solution. This allows your development teams to work faster and more efficiently as their scope of operation is refined on a particular service or component. They can make changes to a specific area of the application without having to worry about affecting other components, this helps to drive innovation and progress at a far greater rate.</p>
<p>As you go through this course, I will introduce you to an AWS service that is commonly used in a decoupled architecture, this being Amazon SQS, the Simple Queue Service.</p>
<p>Let me now look at Event-Driven Architectures.</p>
<p>Event-driven architectures closely relate and interact with decoupled architectures, however, services in an event-driven architecture are triggered by events that occur within the infrastructure. So what is an event? Well, an event can be a number of things, for example, a change of state, so a resource such as an EC2 instance changing from ‘running’ to ‘stopped’—that is a change of state, or perhaps an order has been placed on your website and an item has been moved from for sale to sold, that could be a change of state within your application.</p>
<p>When utilizing and implementing event-driven architectures in AWS, they will typically have three components: a producer, an event router, and consumers.</p>
<p>A producer is the element within the infrastructure that will push an event to the event router. The event router then processes the event and takes the necessary action in pushing the outcome to the consumers. By having the event router sat between both the producer and consumers, each of these two components are decoupled from each other and carry the benefits of a decoupled architecture that I discussed previously.</p>
<p>As we go through this course, I will introduce you to a number of different event-driven services, which act as the event routers, and these include Amazon SNS (the Simple Notification Service), Amazon Kinesis, and AWS Lambda.</p>
<h1 id="Introduction-to-the-Simple-Queue-Service"><a href="#Introduction-to-the-Simple-Queue-Service" class="headerlink" title="Introduction to the Simple Queue Service"></a>Introduction to the Simple Queue Service</h1><p>Hello and welcome to this lecture, which will cover the SQS Service, Simple Queue Service. With the continuing growth of microservices and the cloud best practice of designing decoupled systems, it’s imperative that developers have the ability to utilize a service, or system, that handles the delivery of messages between components. And this is where SQS comes in. SQS is a fully managed service offered by AWS that works seamlessly with serverless systems, mock services, and any distributed architecture. Although it’s simply a queueing service for messages between components, it does much more than that. It has the capability of sending, storing, and receiving these messages at scale without dropping message data, as well as utilizing different queue types depending on requirements. And it includes additional features, such as dead-letter queues. It is also possible to configure the service using the AWS Management Console, the AWS Sarli, or using the AWS SDKs. Let me focus on some of the components to allow to understand how the service is put together. The service itself uses three different elements. Two of which are a part of your distributed system, these being the producers and the consumers, and the third part is the actual queue, which is managed by SQS and is managed across a number of SQS servers for resiliency. Let me explain how these components work together.</p>
<p> The producer component of your architecture is responsible for sending messages to your queue. At this point, the SQS service stores the message across a number of SQS servers for resiliency within the specified queue. This ensures that the message remains in the queue should a failure occur with one of the SQS servers. Consumers are responsible for processing the messages within your queue. As a result, when the consumer element of your architecture is ready to process the massage from the queue, the message is retrieved and is then marked as being processed by activating the visibility timeout on the message. This timeout ensures that the same message will not be read and processed by another consumer. When the message has been processed, the consumer then deletes the message from the queue. Before moving on, I just want to point out a little more relating to the visibility timeout. As I said, when a message is retrieved by a consumer, the visibility timeout is started. The default time is 30 seconds, but it can be set up to as long as 12 hours. During this period, the consumer processes the message. If it fails to process a message, perhaps due to a communication error, the consumer will not send a delete message request back to SQS. As a result, if the visibility timeout expires and it doesn’t receive the request to delete the message, the message will become available again in the queue for other consumers to process. This message will then appear as a new message to the queue. The value of your visibility timeout should be longer than it takes for your consumers to process your messages. I mentioned earlier that there were different types of queues. </p>
<p>These being standard queues, first-in, first-out queues, and dead-letter queues. Standard queues, which are the default queue type upon configuration, support at-least-once delivery of messages. This means that the message might actually be delivered to the queue more than once, which is largely down to the highly distributive volume of SQS servers, which would make the message appear out of its original order or delivery. As a result, the standard queue will only offer a best-effort on trying to preserve the message ordering from when the message are sent by the producers. Standard queues also offer an almost unlimited number of transactions per second, TPS, making this queue highly scalable. If message ordering is critical to your solution, then standard queues might not be the right choice for you. Instead, you would need to use first-in, first-out queues. This queue is able to ensure the order of messages is maintained, and that there are no duplication of messages within the queue. Unlike standard queues, FIFO queues do have a limited number of transactions per second. These are defaulted to 300 per second for all send and receive and delete operations. If you use batching with SQS, then this changes to 3,000. Batching essentially allows you to perform actions against 10 messages at once within a single action. So, the key takeaways between the two queues are for standard queues, you have unlimited throughput, at-least-once delivery, and best-effort ordering. And for first-in, first-out queues, you have high throughput, first-in, first-out delivery, and exactly-once processing. For both queues, it is also possible to enable encryption using server-side encryption via KMS. A dead-letter queue differs to the standard and FIFA queues as this dead-letter queue is not used as a source queue to hold messages submitted by producers. Instead, the dead-letter queue is used by the source queue to send messages that fail processing for one reason or another. This could be the result of cloud enabling your application, corruption within the message, or simply missing information within a database that no message data relates to.</p>
<p>By the way, if the message can’t be processed by a consumer after a maximum number of tries specified, the queue will send the message to a dead-letter queue. This allows engineers to assess why the message failed, to identify where the issue is, to help prevent further messages from falling into the dead-letter queue. By viewing and analyzing the content of these messages, it might be possible to identify the problem and ascertain if the issue exists from the producer or consumer perspective. A couple of points to make with a dead-letter queue is that is must be configured as the same queue type as the source is used against. For example, if the source queue is a standard queue, the dead-letter queue must also be a standard queue type. And similarly, for FIFA queues, the dead-letter queue must also be configured as a FIFA queue. Before I end this lecture, I just want to show a quick demonstration on how to set up a queue and some of the configuration options available during this process. That now brings me to the end of this lecture which covered an introduction to the Simple Queue Service.</p>
<h1 id="Introduction-to-the-Simple-Notification-Service"><a href="#Introduction-to-the-Simple-Notification-Service" class="headerlink" title="Introduction to the Simple Notification Service"></a>Introduction to the Simple Notification Service</h1><p>Hello, and welcome to this lecture, covering the SNS service. It’s likely that out of the free services covered within this course, this is a service you may have come across the most, simply due to its integration with a number of other AWS services. The Simple Notification Service is used as a publish and subscribe messaging service. But what does this mean? SNS is centered around topics, and you can think of a topic as a group for collecting messages. Users or endpoints can then subscribe to this topic, and messages or events are then published to that particular topic. When a message is published, all subscribers to that topic receive a notification of that message. This helps to implement event-driven architectures within a decoupled environment.</p>
<p>Again, much like SQS, SNS is a managed service and highly scalable, allowing you to distribute messages automatically to all subscribers across your environment, including mobile devices. It can be configured with the AWS management console, the CLI, or the AWS SDK. As mentioned, SNS uses a concept of publishers and subscribers, which can also be classed as consumers and producers, and works in the same principle as SQS, from this perspective. The producers or publishers send messages to a topic, which is used as the central communication control point. Consumers or subscribers of the topic are then notified of this message by one of the following methods, HTTP, HTTPS, email, email JSON, Amazon SQS, application, AWS Lambda, or SMS.</p>
<p>Subscribers don’t just have to be users. For example, it could be a web server, and they may be notified of the message via the HTTP protocol. Or if it was a user, you could use the email notification method and enter their email address. SNS offers methods of controlling specific access to your topics through a topic policy. For example, you might want to restrict which protocols subscribers can use, such as SMS, or HTTPS, or only allow access to this topic for a specific user. The policy themselves follow the same format as IAM policies. For more information on IAM policies, please see our existing IAM course which is available within our library of content. I will now perform a demonstration, showing you how topic policies are configured and the different options within them, which allows you to apply access security to your topics. </p>
<p>So in this demonstration, I’m just gonna show you where to find your SNS policies and how to edit them, et cetera. So if we go to our SNS service, and if we create a new topic, go down to create topic here. And if we give it a topic name of Cloud Academy, and then go to create topic. From here, we can now edit the topic policy. So if we go to other topic actions, we can see here, the second option down is edit topic policy. Now there’s two views here, a basic view and an advanced view. The basic view shows the policy is very much a point and click overview, so it’s very simple to understand.</p>
<p>So this top section here allow these users to publish messages to this topic. We can say only myself, everyone or only these users, and we can enter users in this box here. I’m just gonna leave it as only me for this demonstration. And at the bottom here, you can see, allow these users to subscribe to the topic. So again, you can restrict it to only yourself, everyone or only specific AWS users. And down here, you can also specify different delivery protocols as well that I spoke about earlier. So again, for this demonstration, I’m just gonna leave it as only me. If we go to the advanced view, we can see the policy as a JSON view.</p>
<p>Now I explain that if you are familiar with IAM policies, this is laid out in exactly the same fashion. So we have the version at the top. We have a statement. We have a statement ID. And then also we have the usual parameters of effect principle, an action and resource. So if you look at this, for example, we can see that this will allow any principle with the following actions. For example, we can publish the SNS. We can delete the topic, et cetera. Now if we scroll down, we can see resource. And this resource line shows that it’s the Cloud Academy topic that I just created. Now you can edit this policy directly in here and simply click on update policy if you’re confident to do so. But like I say, it follows the same principles as IAM policies. And it’s through these policies that you can control access to your SNS topics. And that’s it.</p>
<p>Both SNS and SQS integrate with each other, which makes sense as both of these services are designed to run in a highly distributed and decoupled environment. By working together, a solution can be designed to send messages to subscribers through a push method, or SQS handles incoming messages and waits for consumers to pull data. Therefore, being able to use SNS as a producer for an SQS queue makes perfect sense from a development perspective. To do this, you’ll need to have your SQS queue subscribed to the SNS topic. And this can be achieved by performing the following steps within this demonstration.</p>
<p>So in this demonstration, I’m gonna show you how you can use SNS as a producer for an SQS queue. So from within the management console, if we go to our SQS service again, and we can find the queue that we had earlier. So if I just select that, and then go to queue actions, then you can see down here, you have subscribe queue to an SNS topic. If I select that, now I can select the topic region and the topic that I want. Now the topic that I created earlier was in the EU Ireland region, so by selecting this drop down list, I should be able to find the Cloud Academy topic. And there it is. And that gives our topic ARN. If we say subscribe, we get a confirmation message to say that this queue has successfully subscribed to the following SNS topic, which is the Cloud Academy topic. And it also set up the relevant permissions as well to allow that to happen. So if we click on okay, now we can test this out. If we go through to SNS, and if we go to our topics and Cloud Academy, and the first thing that we can see is the SQS queue is actually subscribed to this topic. So that was the action we just carried out.</p>
<p>Now I wanna test this by publishing a message to this topic to make sure that SQS receives a copy of it. So if I click on publish to topic, and I’ll just create a message here just called, Cloud Academy Test. This is a test, and click on publish message. Now that has published a message to that topic, so any subscribers should receive a copy, which means our queue should also receive a copy. So if we go back to SQS, we can see here that messages available is one. So if we go across and select our queue, and go to view&#x2F;delete messages, click on start polling for messages, we can see here straight away that we have our message that we received from the topic. And if we go to more details, we can see here the subject of Cloud Academy Test, and also the message, “This is a test.” So this proves that we have successfully used SNS as a producer for this SQS queue. Much like SQS, SNS also integrates well with AWS Lambda, a key serverless computer service. To learn more about serverless technologies, you can view our existing learning path, entitled, “<a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/serverless-computing-aws-developers-45/">Serverless Computing on AWS for Developers</a>,” which can be found <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/serverless-computing-aws-developers-45/">here</a>.</p>
<p>This integration allows SNS notifications to invoke existing Lambda functions. Like SQS, the Lambda function has to be subscribed to the topic. Then when a message is sent to the topic, the message is pushed out to the Lambda function to invoke it. The function itself uses the payload of the message as an input parameter, where it can then alter the message if required, or forward the message onto another AWS service, or indeed to another SNS topic.</p>
<p>To configure AWS Lambda to work with the topic, you can perform the following steps. From within the SNS dashboard of the AWS management console, select topics. Select the topic that you want to subscribe to with the Lambda function. Select actions and subscribe to topic. Using the protocol menu, select the AWS Lambda option. Then you must select the Lambda function to be used from the endpoint dropdown box.</p>
<p>Finally, you can select the version or alias of the function, and select the latest version of the function. Choose the latest option. Select create subscription. To gain more insight into this process and to see an example of how this can be used to create a sample message history store using SNS Lambda and Amazon DynamoDB, you can view this blog post made by AWS found here. We also have a lab which would teach you how to process SNS notifications with a Lambda function. As a simple example, the lab uses Python to log custom metrics to CloudWatch based on the message payload. That now brings me to the end of this lecture, which covered an introduction to the Simple Notification Service.</p>
<h1 id="Introduction-to-Amazon-Kinesis"><a href="#Introduction-to-Amazon-Kinesis" class="headerlink" title="Introduction to Amazon Kinesis"></a>Introduction to Amazon Kinesis</h1><p>Hello and welcome to this lecture introducing Amazon Kinesis.</p>
<p>Amazon Kinesis makes it easy to collect, process, and analyze real-time streaming data so you can get timely insights and react quickly to new information. With Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT to imagery data, and more, into your databases, your data lakes and data warehouses.</p>
<p>It enables you to process and analyze data as it arrives and responds to it in real-time, instead of having to wait until all your data is collected before the processing can begin.</p>
<p>Amazon Kinesis can continuously capture terabytes of data per hour from hundreds or thousands of sources, such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events.</p>
<p>From a storage perspective, Amazon Kinesis does not store persistent data itself, unlike many of the other Amazon big data services. As a result, Amazon Kinesis needs to be deployed as part of a larger event-driven solution.</p>
<p>Amazon Kinesis provides three different solution capabilities. Amazon Kinesis Streams: This enables you to build custom applications that process or analyze streaming data for specialized needs. This comes in 2 different variations, Kinesis Data Streams, and Kinesis Video streams. Data Streams offer a real-time data streaming service capable of elastically scaling to support hundreds of thousands of data feeds to help you build real-time solutions, such as live dashboards or identifying any security anomalies. Video Streams are designed to securely elastically scale and ingest video streams on a massive scale, connecting to millions of video streaming devices, where it can then store, and encrypt the data ready for processing by your data analytics solutions. Amazon Kinesis Data Firehose. This enables you to load streaming data into Amazon Kinesis Analytics, Amazon S3, Amazon RedShift, Amazon Elastic Search, and Splunk. Amazon Kinesis Analytics. This enables you to write standard SQL queries on streaming data.</p>
<p>Amazon Kinesis Streams is based on a platform as a service style architecture where you determine the throughput of the capacity you require and the architecture and components are automatically provisioned and stored and configured for you. You have no need or ability to change the way these architectural components are deployed.</p>
<p>An Amazon Kinesis stream is an ordered sequence of data records. A record is the unit of data in an Amazon Kinesis stream. Each record in the stream is composed of a sequence number, a partition key, and a data blob. The data blob is the data of interest that your data producer adds to a stream.</p>
<p>So what is a Producer? A producer is an entity that is continuously pushing data to Kinesis Streams, for example, a web service sending log data to a stream is a producer.</p>
<p>And then we have Consumers, now a consumer receives records from Amazon Kinesis Streams and processes them in real-time. Consumers can store their results using an AWS service, such as Amazon DynamoDB, Amazon Redshift, or Amazon S3. These consumers are known as Amazon Kinesis Streams applications and typically run on a fleet of EC2 instances. You need to build your applications using either the Amazon Kinesis API or the Amazon Kinesis Client Library.</p>
<p>Okay, let’s have a look at the architecture that underpins the Amazon Kinesis Firehose. While still under the Kinesis moniker, the Amazon Kinesis Firehouse architecture is different to that of Amazon Kinesis Streams.</p>
<p>Amazon Kinesis Firehose is a fully-managed service for delivering real-time streaming data to destinations such as Amazon S3, Amazon Redshift, Amazon Elasticsearch Service and Splunk.</p>
<p>With Kinesis Firehose, you do not need to write applications as your consumers. Instead, you configure your data producers to send data to Kinesis Firehose, where the service then automatically delivers the data to the destination that you specify. You can also configure Amazon Kinesis Firehose to transform your data before data delivery.</p>
<p>A delivery stream is the underlying entity of Kinesis Firehose. You use Kinesis Firehose by creating a Firehose delivery stream and then sending data to it, which means each delivery stream is effectively defined by the target system that receives the streamed data. Firehose can also invoke an AWS Lambda function to transform incoming data before delivering it to the selected destination. You can configure a new Lambda function using one of the Lambda blueprints AWS provides or you can choose on of your existing Lambda functions.</p>
<p>Let’s have a quick look at the difference between Amazon Kinesis Streams and Firehose. Amazon Kinesis Streams is a service for workloads that require custom processing, per incoming record, with sub-one-second processing latency, and a choice of stream processing frameworks.</p>
<p>Amazon Kinesis Firehose is a service for workloads that require zero administration, with data latency of 60 seconds or higher. You use Firehose by creating a delivery stream to a specified destination and send data to it, you do not have to create a stream or create a custom application as the destination. But Firehose is limited to S3, Redshift, and Elasticsearch and Splunk as the data destinations.</p>
<p>Amazon Kinesis Analytics is a fully managed service that enables you to quickly author SQL code that continuously reads, processes and stores data. With Amazon Kinesis Analytics, you can ingest in real-time billions of small data points. Each and every individual data point can then be aggregated to provide intelligent business insights, which in turn can be used to continually optimize and improve business processes.</p>
<p>Working with Kinesis Analytics requires you to perform three steps. You must create an input stream. Input streams typically come from streaming data sources such as Kinesis streams. Create SQL processing logic, a series of SQL statements that process input and produce output. The SQL code will typically perform aggregations and generate insights. And finally, create an output stream. Output streams can be configured to hold intermediate results that are used to feed into other queries or be used to stream out the final results. Output streams can be configured to write out to destinations such as S3, Redshift, Elasticsearch and&#x2F;or other Kinesis streams.</p>
<p>What is the benefit of using Kinesis Analytics, well, the ability to maintain peak performance of a business is often related to the ability to make timely decisions. The earlier we can make informed and actionable decisions, the quicker we can adjust and maintain optimal performance, and hence highlights the importance of being able to process data in near to real-time.</p>
<p>The type of decision making we can make is based on the age of the data itself. Considering this, we can see that data processed within real-time allows us to take preventative and&#x2F;or predictive decisions.</p>
<p>Your SQL querying statements that you author represent the most important part of your Kinesis Analytics application as they generate the actual analytics that you wish to derive. Your analytics are implemented using one or several SQL statements, used to process and manipulate input and produce output.</p>
<p>This process can involve intermediary steps, whereby the outputs of one query feed into a second in-application stream. This process can be repeated multiple times until a final desired result is achieved persisted to an output stream.</p>
<h1 id="A-Survey-of-Serverless"><a href="#A-Survey-of-Serverless" class="headerlink" title="A Survey of Serverless"></a>A Survey of Serverless</h1><p>Hello, my name is Will Meadows and I would like to welcome you to this survey of the serverless services. </p>
<p>Today we’re going to go over all the different serverless categories and services that are available within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Initially serverless was a very small domain that only offered Lambda as the sole option for someone getting into the serverless category. However, At this point in time, there are over 12 different serverless services for you to choose, each providing a unique benefit and solving a specific problem. </p>
<p>That’s why I think it’s important for someone just getting in the domain to take a survey of all of these options to get a real understanding of what it is that they want to learn about. This lecture will spend just a few minutes on each of the 12 different services to help you drive forward your learning and education. And for each service where we have a course, I will link to that lecture so that way you can continue your education on anything that you deem fit.</p>
<p>So grab a coffee, sit back and relax, and let’s take a look at all the different serverless options that AWS has available.</p>
<p>To get us started there are currently three different categories of serverless services. These categories include: serverless compute services, serverless application integration services, and serverless database services.</p>
<p>I think we should start off by looking at something you’re probably the most familiar with, which is the serverless compute services.</p>
<p>Compute is probably one of the most important things that any application will have to deal with. It is the way that the actual work gets performed. </p>
<p>Traditionally this job was completed by fleets of servers all running specific applications or code to complete their jobs. These jobs might range from running websites, big data analytics, or even dealing with long-running applications like video game servers.</p>
<p>In this category of serverless compute there are currently two different services that can complete these jobs in varying ways. Each of the options have some positives and negatives that lend themselves to some use cases over others. Let’s start off with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">AWS Lambda</a> and take a look at where it might shine.</p>
<p>Lambda was released in 2014 as the first serverless service. It offered the ability to run code, without needing to manage a server, for up to 5 minutes at a time. </p>
<p>This new paradigm of compute, the idea of functions as a service Is what really pushed forward the category of serverless as a whole.</p>
<p>Lambda is an event-driven service. This means that it needs some kind of action, or event, to trigger the code you wish to run. In many situations, the event or action is just a change of state. This change of state could be something as simple as when an image is placed into an S3 bucket.</p>
<p>These days Lambda has increased its execution time to up to 15 minutes, and has greatly increased its range of functionality from its 2014 introduction.</p>
<p>If you are just starting your journey into this space, getting a deeper understanding of lambda will really help your learning and understanding of every other serverless offering.</p>
<p>We have a course that covers AWS Lambda in-depth over here if you want to learn more: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/</a></p>
<p>This service allows you to run serverless containers on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/ecs-ec2-container-service/">Amazon ECS</a> (the elastic container service). If you are currently running a container-based application, and would like to convert to a serverless format, this is the perfect solution for you! </p>
<p>One of the coolest aspects about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/course-automatically-created-2018-10-02-113135226141/c3l4-terraform1/">Fargate</a> is that it allows you to get over the 15 minutes execution hurdle that plagues AWS lambda. You are allowed to run your Fargate tasks (containers) for an unlimited amount of time. The serverless component of AWS Fargate is in regard to where those containers actually live.</p>
<p>If you would like to learn more about ECS, Fargate, and Microservices - we have a course that covers all of that right here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/">https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/ </a></p>
<p>Application integration services are a part of the interstitial glue that helps weave everything together. They allow you to connect to and send data to other applications and AWS services. There are a number of these types of offering available nowadays, and they all fill a slightly different niche. </p>
<p>Amazon has a serverless event-based service called Amazon Eventbridge that functions as a serverless event bus. Think of an event bus as a kind of event coordinator. It allows you to intake information from both external SAAS providers, AWS services (over 90 different ones are supported), and even your own custom applications. With these input events, Amazon Eventbridge can filter, manage, and direct them to other systems that listen for them, and are able to take action based on their content. It does this filtering process with routing rules, which give you full control over your event bus.</p>
<p>If you would like to learn more about this service, please check out this course over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/">https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/</a></p>
<p>AWS Step Functions can best be described as a serverless state machine service. For those who don’t know what a state machine is, think of your standard vending machine. </p>
<p>A vending machine sits there waiting for a customer to come up to it and input money (that’s its idle state). Once money has been added into the machine, it movies onto the next state, which would be item selection. The user inputs their choice, and the machine moves into the final state of vending the product. After the workflow has been completed it returns back to the idle state, waiting for another customer.</p>
<p>AWS Step Functions allow you to create serverless workflows just like the vending machine, where you can have your system wait for inputs, make decisions, and process information based on the input variables.</p>
<p>If you would like to know more about AWS Step Functions please check out this course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-step-functions-1117/introduction/">https://cloudacademy.com/course/aws-step-functions-1117/introduction/</a></p>
<p>Amazon SQS (the simple queue service) is a messaging queue system. It can help you decouple your applications by providing a system for sending, storing, and receiving messages between multiple software components. SQS is a managed service that offers two types of queues: FIFO and Standard (which is a best-effort ordering queue, with at least once delivery).</p>
<p>Amazon SQS is a fantastically useful service that works splendidly in most serverless applications but can also find a home in many standard architectures.</p>
<p>For more on SQS please check out this course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/">https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/</a></p>
<p>This service is a Pub-Sub notification service that provides both application-to-application or application-to-person communication. This communication works well for high-throughput applications as well as many-to-many messaging between distributed systems. SNS can also act as an event-driven hub similar to Amazon Eventbridge - It’s just more bare bones.</p>
<p>For more info on Amazon SNS take a look over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/">https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/</a></p>
<p>AWS has created a fully managed service called Amazon API Gateway which helps deal with: building, publishing, monitoring, securing, and maintaining API within AWS. It works quite well at any scale, and is able to support serverless, generic web applications, and even containerized workloads on the back end. </p>
<p>You can build your APIs for public use, for private use, or even for third-party developers. The best part about it is that it is entirely serverless, and does not require you to manage any infrastructure and you pay just for what you use.</p>
<p>The service is also able to handle accepting and processing hundreds of thousands of concurrent requests. If things start to get out of hand, API Gateway is able to monitor all traffic and can throttle requests as desired.</p>
<p>Please check out this introductory course on API gateway if you are interested in learning more: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/">https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/</a></p>
<p>AWS app sync allows you to manage and synchronize data across multiple mobile devices and users. The service also allows users to modify data while offline, and having those changes be automatically synced when the device reconnects to the internet. </p>
<p>This functionality allows you to build real-time, multi-user collaborative tools and applications that work between browsers, mobile applications, and even Amazon Alexa skills. </p>
<p>AWS AppSync uses GraphQL to enable clients to fetch, change, and subscribe to data from databases, microservices, and APIs all from a single GraphQL endpoint.</p>
<p>Setting up and managing databases is a huge challenge for many organizations. Having to deal with scalability and right-sizing can take a lot of knowledge, time, and money to get set up just right. Having data storage be serverless can greatly increase your productivity as well as reduce a lot of headaches. There are a number of fantastic data storage services available serverlessly these days, so let’s take a moment to look at each of them.</p>
<p>Amazon s3 is an object-based serverless storage system that is able to handle a nearly unlimited amount of data. S3 provides great scalability, availability, and speedy performance for many different use cases. Amazon S3 is able to support files as small as zero bytes, and tops out at five terabytes. </p>
<p>Objects stored in S3 have a durability of 11 nines (99.999999999%) and so, the likelihood of losing data is extremely rare.</p>
<p>S3 has native integrations with AWS lambda, allowing you to create event-based workflows with ease. Additionally, s3 provides some of the cheapest data storage available through the use of the S3 Glacier storage class. </p>
<p>Please take a look over here at this course if you wish to learn more about Amazon S3: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/">https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/</a></p>
<p>DynamoDB is a fully managed serverless, NoSQL database that has been built to run high-performance applications at any scale. The service can operate at single-digit millisecond latency which is very valuable for time-sensitive applications that require the fastest response times. DynamoDB is a key-value store database that has no strict design schema that it needs to conform to.</p>
<p>DynamoDB is designed to be highly available. Your data is automatically replicated across three different availability zones within a geographic region. In the case of an outage or an incident affecting an entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>For more information about DynamoDB please take a look at this course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/">https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/</a></p>
<p>Amazon RDS Proxy is a fully managed, serverless, highly available database proxy for Amazon RDS. The proxy allows you to build serverless applications that are more scalable than your standard direct to RDS implementations.</p>
<p>If you are opening many new connections to your RDS databases through lambda functions or other serverless methods - you might have issues when large surges of connections are required.</p>
<p>RDS Proxy allows you to pool and share already established database connections, reducing the latency of your applications when establishing a new connection. Additionally RDS Proxy helps the availability of your serverless application by denying access to unserviceable connections that may degrade your database’s performance. </p>
<p>Aurora serverless is a fully on-demand SQL database configuration for Amazon Aurora. It automatically starts up, shuts down, and scales its capacity based on the application’s needs. It operates on a pay-per-second basis while the database is active, and can be used through a simple database endpoint. If you ever need to switch over to standard workload and leave the realm of serverless, you can do so with the click of a button.</p>
<p>Aurora is built to be highly available, fault-tolerant, and self-healing as it replicates your data 6 ways across multiple availability zones.</p>
<p>If you would like to learn more about Aurora serverless please take a look at this course over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/">https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/</a></p>
<p>AWS has developed and positioned a well-thought-out series of serverless services that can help almost any application. These days there’s pretty much an in-place stand-in for every piece of normally servered component, that can be run serverlessly.</p>
<p>I would highly recommend looking at the coursework for any of the services covered today that interested you. Each course dives directly into the service and can really help explain where it fits in an architecture and what problems it can help you solve.</p>
<p>Hopefully, you’ve enjoyed this survey of all the service services and categories within AWS. My goal was to give you just a little taste of each facet of serverless so that way you could explore and find what you want to learn about.</p>
<p>My name is Will Meadows and I’d like to thank you for spending your time here learning about Serverless. If you have any feedback, positive or negative, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated, thank you!</p>
<h1 id="4Introduction-to-the-Simple-Notification-Service"><a href="#4Introduction-to-the-Simple-Notification-Service" class="headerlink" title="4Introduction to the Simple Notification Service"></a>4<strong>Introduction to the Simple Notification Service</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/serverless-computing-aws-developers-45/">Serverless Computing on AWS for Developers</a></p>
<h1 id="6A-Survey-of-Serverless"><a href="#6A-Survey-of-Serverless" class="headerlink" title="6A Survey of Serverless"></a>6<strong>A Survey of Serverless</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/introduction/">Understanding AWS Lambda to Run &amp; Scale Your Code</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-microservices-containers-ecs-1828/introduction-to-microservices-containers-and-ecs/">Introduction to Microservices, Containers, and ECS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/connecting-application-data-using-amazon-eventbridge-1301/introduction/">Connecting Application Data using Amazon EventBridge</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-step-functions-1117/introduction/">AWS Step Functions</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-queue-service/">Introduction to the Simple Queue Service</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-simple-notification-service/">Introduction to the Simple Notification Service</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-api-gateway-2140/introduction-to-api-gateway/">Introduction to API Gateway</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-storage-fundamentals-2016/introduction-32/">Storage Fundamentals for AWS</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/working-with-amazon-dynamodb/introduction-31/">Working with DynamoDB</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-aurora-high-availability/aurora-serverless/">Amazon Aurora High Availability</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:42" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:42-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:07:48" itemprop="dateModified" datetime="2022-11-20T19:07:48-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-Virtual-Private-Cloud-VPC-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/44/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><span class="page-number current">45</span><a class="page-number" href="/page/46/">46</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/46/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
