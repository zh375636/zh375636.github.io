<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/51/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/51/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-RDS-Database-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-RDS-Database-23/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Create-Your-First-Amazon-RDS-Database-23</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:32" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:32-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:16:56" itemprop="dateModified" datetime="2022-11-27T20:16:56-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-RDS-Database-23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-RDS-Database-23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Databases-SAA-C03-22</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:31" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:31-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 19:59:24" itemprop="dateModified" datetime="2022-11-27T19:59:24-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Databases-SAA-C03-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Databases-SAA-C03-Introduction"><a href="#Databases-SAA-C03-Introduction" class="headerlink" title="Databases (SAA-C03) Introduction"></a>Databases (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various database services currently available in AWS that may be covered on the exam.</p>
<p>Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#x70;&#112;&#x6f;&#114;&#116;&#64;&#x63;&#x6c;&#x6f;&#117;&#100;&#x61;&#x63;&#x61;&#x64;&#101;&#109;&#121;&#46;&#x63;&#111;&#109;">&#x73;&#x75;&#x70;&#112;&#x6f;&#114;&#116;&#64;&#x63;&#x6c;&#x6f;&#117;&#100;&#x61;&#x63;&#x61;&#x64;&#101;&#109;&#121;&#46;&#x63;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various database services in AWS in preparation for the exam.</p>
<p>The objective of this course is to provide an introduction to database services in AWS for solution architects, including:</p>
<ul>
<li>Managed relational databases using the Amazon Relational Database Service, or RDS, along with purchasing options and pricing; and</li>
<li>Managed NoSQL databases including Amazon DynamoDB.</li>
</ul>
<p>You’ll also learn about several other data structure stores and database services offered by AWS and see demonstrations of each of the following in action:</p>
<ul>
<li>Amazon ElastiCache;</li>
<li>Amazon Neptune;</li>
<li>Amazon Redshift;</li>
<li>Amazon DocumentDB;</li>
<li>Amazon Keyspaces; and</li>
<li>The Amazon Quantum Ledger Database, or QLDB.</li>
</ul>
<p>Finally, we’ll wrap up the course with a discussion of data lakes on AWS, including defining what a data lake is, as well as explaining the differences between data lakes and data warehouses.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#x75;&#x70;&#x70;&#111;&#x72;&#116;&#x40;&#99;&#x6c;&#111;&#117;&#x64;&#97;&#99;&#97;&#x64;&#101;&#x6d;&#121;&#x2e;&#x63;&#111;&#x6d;">&#115;&#x75;&#x70;&#x70;&#111;&#x72;&#116;&#x40;&#99;&#x6c;&#111;&#117;&#x64;&#97;&#99;&#97;&#x64;&#101;&#x6d;&#121;&#x2e;&#x63;&#111;&#x6d;</a>. Thank you!</p>
<h1 id="Amazon-Relational-Database-Service"><a href="#Amazon-Relational-Database-Service" class="headerlink" title="Amazon Relational Database Service"></a>Amazon Relational Database Service</h1><p>Hello and welcome to this lecture, where I shall be discussing the first of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-part-one-1064/course-introduction/">AWS database services</a> that I will be covering in this course, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-part-one-1064/demo-creating-an-amazon-rds-database/">Amazon Relational Database Service</a>, commonly known as RDS. I will look at a number of different common features of the service to give you a general idea of how it’s configured. So as the name suggests, this is a relational database service that provides a simple way to provision, create, and scale a relational database within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. It’s a managed service, which takes many of the mundane administrative operations out of your hands, and it’s instead managed by AWS, such as backups and the patching of both the underlying operating system in addition to the database engine software that you select.</p>
<p>Amazon RDS allows you to select from a range of different database engines. These currently include MySQL, and this is considered the number one open source relational database management system. MariaDB. This is the community-developed fork of MySQL. PostgreSQL. This database engine comes in a close second behind MySQL as the preferred open source database. Amazon Aurora. Amazon Aurora is AWS’s own fork of MySQL, which provides ultrafast processing and availability, as it has its own cloud-native database engine. Oracle. The Oracle database is a common platform in corporate environments. And SQL Server. This is a Microsoft database with a number of different licensing options.</p>
<p>In addition to so many different database engines, you also have a wide choice when it comes to selecting which compute instance you’d like to run your database on. The varying different options offer different performance and allowed to architect your environment based on your expected load. When you create your RDS database, you must select an instance to support your database from a processing and memory perspective, as shown here in this screenshot, using the MySQL database engine. Currently, these are the different instance types available to you for each of the database engines, which are categorized between general purpose and memory-optimized.</p>
<p>For a breakdown of the performance of each of these instance types, please refer to the following <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">AWS documentation</a>. For each of these instance types, you also have various instance sizes, each equating to a different performance level from a vCPU and memory perspective. For example, when looking at the T3 instance, we have the following sizes available.</p>
<p>You can deploy your RDS instance in a single availability zone. However, if high availability and resiliency is of importance when it comes to your database, then you might want to consider a feature known as Multi AZ, which stands for multi availability zones. When Multi AZ is configured, a secondary RDS instance is deployed within a different availability zone within the same region as the primary instance. The primary purpose of the second instance is to provide a failover option for your primary RDS instance. The replication of data between the primary RDS database and the secondary replica instance happens synchronously.</p>
<p>Let’s look at how Multi AZ would work in a production environment. If you have configured Multi AZ and an incident occurs which causes an outage to the primary RDS instance, then the RDS failover process takes over automatically. This process is managed by AWS, and it’s not something that you need to manually perform or trigger. RDS will update the DNS record to point to the secondary instance. This process can typically take between 60 and 120 seconds. The length of time is very dependent on the size of the database, its transactions, and the activity of the database at the time of failover. This automatic changeover enables you to continue using the database without the need of an engineer making any changes to your environment. The failover process will happen in the following scenarios. If patching maintenance has been performed in the primary instance, if the instance of the primary database has a host failure, if the availability zone of the primary database fails, if the primary instance was rebooted with failover, and if the primary database instance class on the primary database is modified.</p>
<p>As you can see, activating Multi AZ is an effective measure and precaution to implement to ensure you have resiliency built in should an outage occur. For detailed information on RDS Multi AZ, please refer to our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">here</a>.</p>
<p>Over time, your workloads on your database will fluctuate. And so how can you optimize your RDS database to ensure it is capable of meeting the demands of your load, both from a storage and compute perspective? When it comes to scaling your storage, you can use a feature called storage autoscaling. MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server all use Elastic Block Store, EBS volumes, for both data and log storage. However, Amazon Aurora uses a shared cluster storage architecture and does not use EBS. The database engines that use EBS support general purpose SSD storage, provisioned IOPS SSD storage, and magnetic storage.</p>
<p>The general purpose SSD storage is a good option for a broad range of use cases which provides single-digit millisecond latencies and offers a cost-effective storage solution. The minimum SSD storage volume for your primary dataset is 20 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle. However, the maximum for SQL Server is 16 tebibytes. Provisioned IOPS SSD. Now, this option is great for when you have workloads that operate at a very high I&#x2F;O. You can provision a minimum of 8,000 IOPS and a maximum of 80,000 for MySQL, PostgreSQL, MariaDB, and Oracle, but the maximum for SQL Server is 40,000.</p>
<p>In addition to being able to provision the IOPS needed for your workload, the minimum storage for your primary dataset is a 100 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle, and 16 tebibytes for SQL Server. Magnetic storage is simply supported to provide backwards compatibility. And so instead, AWS recommends that you select general purpose. The following screenshot shows the configuration screen when setting your storage requirements during the creation of a MySQL RDS database. </p>
<p>n this example, general purpose has been selected as the storage with a minimum of 100 gibibytes of primary storage. Under the storage autoscaling section, I’ve enabled the feature with the checkbox and set a maximum storage threshold of 1000 gibibytes. This means that one storage will start at 100 gibibytes when the database is created and the storage will automatically scale with demand up to a maximum of 1000 gibibytes without you having to intervene in any way. The maximum storage threshold can be set at 65,536 gibibytes. Aurora doesn’t use EBS and instead uses a shared cluster storage architecture which is managed by the service itself.</p>
<p>When configuring your Aurora database in the console, the option to configure and select storage options like we saw previously does not exist. Your storage will scale automatically as your database grows. To scale your compute size, which is effectively your instance, is easy to do in RDS both vertically and horizontally. Vertical scaling will enhance the performance of your database instance. For example, scaling up from an m4.large to an m4.2xlarge. Horizontal scaling will see an increase in the quantity of your current instance. For example, moving from a single m4.large to three m4.large instances in your environment through the means of read replicas.</p>
<p>At any point you can scale your RDS database vertically, changing the size of your instance. When doing so, you can select to perform the change immediately or wait for a scheduled maintenance window. For horizontal scaling, read replicas can be used by application and other services to save read only access to your database data via a separate instance. So, for example, let’s assume we have a primary RDS instance which serves both read and write traffic. Due to the size of the instance and the amount of read-intensive traffic being directed to the database for queries, the performance of the instance has taken a hit. So to help resolve this, you can create a read replica. </p>
<p>A snapshot will be taken of your database, and if you’re using Multi AZ, then this snapshot will be taken of your secondary database instance to ensure that there are no performance impacts during this process. Once the snapshot is complete, a read replica instance is created from this data. The read replica then maintains a secure asynchronous link between itself and the primary database. At this point, read only traffic can be directed to the read replica to serve queries. Before implementing read replicas, please check with the latest AWS documentation to identify database engine read replica compatibility.</p>
<p>As I mentioned previously, many of the administrative tasks for RDS are taken care of by AWS. For example, patching and automated backups. As Amazon RDS is a managed service, and from a shared responsibility model is considered a container service where you have no access to the underlying operating system on which your database runs on. As you can see, both platform and application management and operating systems falls under the realm of AWS responsibilities. As a result, AWS is responsible for both the patching of the operating system and any patching for the database engine themselves. More information on the AWS shared responsibility model can be found in my existing blog post found <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">here</a>.</p>
<p>From a backup perspective, by default, Amazon RDS provides an automatic feature seen here. This is enabled on all new RDS databases, which backs up your RDS instance to Amazon S3. You are able to configure the level of retention in days from zero to 35 and implement a level of encryption using the key management service, or KMS. More information on KMS can be found in our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>You can also perform manual backups anytime you need to, which are known as snapshots. However, these snapshots are not bound by the retention period set in the automatic backup configuration and are only deleted through a manual process. When using a MySQL-compatible Aurora database, you can also use a feature called backtrack, and this allows you to go back in time on the database to recover from an error or incident without having to perform a restore or create another database cluster.</p>
<p>As you can see from the configuration page during the Aurora database creation process, it is enabled via a checkbox and allows you to enter a number of hours of how far you would like to backtrack to with a maximum of 72 hours. In this example, I’ve entered 12 hours, and so Aurora will retain log data of all changes for 12 hours as specified. We’ve now covered the fundamentals of Amazon RDS and some of the key features to be aware of. If you’d like to get some hands-on experience of using Amazon RDS, feel free to check out the following labs.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="DEMO-Creating-an-Amazon-RDS-Database"><a href="#DEMO-Creating-an-Amazon-RDS-Database" class="headerlink" title="DEMO: Creating an Amazon RDS Database"></a>DEMO: Creating an Amazon RDS Database</h1><p>Hello, and welcome to this lecture, which will be a demonstration on how to create an RDS database. So let’s get straight into it. So as you can see, I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console. And the first thing I need to do is to go to RDS. Now you can find RDS under the Database category, and you can see that it’s the first database option. So if you select the RDS service, and this is the dashboard for Amazon RDS. As you can see, I don’t have any database instances running. So let’s go ahead and create our first database.</p>
<p>So under the Create database section here, we can either restore an existing database from Amazon S3, from a backup, or we can create a new database. For this demonstration, I’m going to create a new database. So let’s click that option. Now, firstly, we need to choose a database creation method. We can either do a standard create or an easy create. Now the standard create allows us to set more-configurable options. So for this demonstration, that’s the option I’m going to select.</p>
<p>Then we have our database engine types. As I explained previously, we have Amazon Aurora, mySQL, MariaDB, Postgres, Oracle and Microsoft SQL Server. I’m just going to select mySQL for this demonstration. So scrolling down again, we can then select our version of mySQL, whichever version we’d like. I’ll just leave it as the default option. And then we have something called Templates.</p>
<p>Now, depending on what template we select here will predefine a list of other configurable components. So here that’s highlighted is Production and this uses defaults for high availability and fast and consistent performance. The Dev&#x2F;Test template is intended for development use outside of a production environment. And the Free Tier is simply to allow you to get hands-on experience with RDS and doesn’t really use many of the features. But I want to show you the full feature set. So I’m going to select Production.</p>
<p>Now, if we go down to Settings. Here, we can enter a database instance identifier. So this is the name of the database instance, not the actual name of a database table. So I’m happy to call it database one, just leave that as default. Now we have our credentials. Now we have a master user name to connect to the database instance, so we can have admin. We can either auto-generate a password or we can select our own. So I’ll just go ahead and enter my own one.</p>
<p>Next, we have the option to select the database instance size. So we have our Standard classes or Memory Optimized or even Burstable classes. So I’m going to select the Standard class and using this dropdown, we can select the size of the instance that we want. And as you can see, there’s a different number of VCPUs and RAM. And so I’m just going to select the smallest instance size.</p>
<p>Now, if we go down to Storage, we can select our storage type. So we have General Purpose or Provisioned IOPS. If we look at the Provisioned IOPS, we can define the allocated storage and then also the number of IOPS as well, which is the input and output operations per second. For this demonstration, I’m just gonna leave it as General Purpose. I’ll just accept the storage defaults there of 20. Here we have Storage autoscaling. If we want to enable that or not, it’s just a simple tick box. RDS will automatically scale up to whatever value we put in here. So for example, 100 gig. That will give us the flexibility of starting at 20 and scaling all the way up to 100 automatically. </p>
<p>Now, if we go down to Availability and durability. Here, we have a Multi-AZ deployment. So it’s enabling this, will create another standby instance in a different availability zone to create high availability and data redundancy. For this demonstration, I’m just gonna leave it as a single AZ deployment. So I’m gonna select Do not create a standby instance. If we go down to Connectivity, we can select the VPC that we’d like this RDS instance to reside in. And you can see here, after a database is created, you can’t change the VPC selection. If you expand this option here for additional connectivity configuration, we can see a few additional options.</p>
<p>So we can select a database subnet group. And this simply defines which subnets and IP ranges the database instance can use in the VPC. Have an option here, if the database should be publicly accessible or not. If you select yes, then it will be issued a public IP address and devices and instances outside of your VPC will be able to try and connect to your database, if the VPC security groups allow it. For this demonstration, I’m just going to keep it a private RDS database, so it won’t assign any kind of public IP address. And only instances inside the VPC will be able to connect. Here, we can choose our security group, which will essentially define which resources can talk to our RDS instance.</p>
<p>Now, if we select an existing, then we can use this dropdown box here to select which security group that we’d like to use. I’m just going to select the default. I haven’t set any kind of security groups up for this as this is just a demonstration, but that’s where you would apply your security groups for your RDS instance. And as you can see, it’s added it in there. If you’d like to deploy your RDS instance in a particular availability zone, then you can select one. If you have no preference, simply select no preference. And also what port the database will be using.</p>
<p>If you go down to Database authentication. We have two options here for mySQL. Password authentication. Now this will allow anyone to connect to the instance just using the database passwords. If you want it more secure, then you can use the database password in addition to verifying that the user has permissions to access the RDS database, through permissions that were assigned directly to the user or group or role. So that just offers an additional level of security. If we go down further to Additional configuration, we can configure additional options.</p>
<p>So here we have our database options. You can enter an initial database name that will run on your database instance. Let’s just say my database. You can select a parameter group. Parameter groups is essentially a grouping of configurable parameters that operate at the database engine level. You’re able to create different parameter groups that contain different settings for the same database engine, depending on your use case and how you’d like these parameters to be configured. Now the parameter group itself sits outside of the database, and this means that the same parameter group can be applied to multiple databases. So if you update the values within the parameter group, then this will update all the databases that use that same parameter group. Depending on which database engine you select, you are able to select an option group. And these option groups allow you to configure additional features to help you manage and secure your databases. Again, like parameter groups, they sit outside of the database itself.</p>
<p>Here we have our Backup section, so we can enable our automatic backups. If you don’t want this, you can simply un-tick it, but it’s pretty useful, so I tend to leave that enabled. And here we have our backup retention period. And you can select the number of days, up to 35 days. Just leave that at seven. We have a backup window. We can select a window, select in the time and the duration. So if you have a particular time that you’d like to run your backups, you can simply add in the hours and minutes, and also how long it should run for. If you don’t have a specific window, you can simply select no preference. If you have any tags for your database, you can copy that to your backup snapshots as well.</p>
<p>When it comes to encryption, you can either encrypt your database. The default is to have your database encrypted, and then you can select your key here. Now, this is the default AWS managed key for RDS, which is used by KMS, the key management service. or you can select your own CMK, your own customer master key, if you have a different one yourself. I’m just gonna leave it as the default AWS RDS managed key.</p>
<p>Down here, we have performance insights. Performance insights allows you to implement a level of performance tuning and monitoring, which enables you to see and review the load on your database, and if any actions should be taken. Here we can make some additional monitoring changes. We can enable enhanced monitoring, and we can set the granularity of this monitoring from anything from 60 seconds to one second. I’ll leave that as a default of 60 seconds for enhanced monitoring. And I’m just going to leave RDS to create the default role for that enhanced monitoring.</p>
<p>We can export our logs to Amazon CloudWatch Logs. Either the error, the general or the slow query log or all of them, or any combination. So if you want to export any of your logs to CloudWatch Logs for additional monitoring and queries then you can do so.</p>
<p>Then we have maintenance. We can enable or disable auto minor version upgrade. And here we can see that by enabling auto minor version upgrade, it will automatically upgrade to new minor versions as they are released. And the automatic upgrades occur during the maintenance window that we’ve scheduled for the database.</p>
<p>Now, speaking of maintenance window, we can select one here. So we can select a window. We can say on a particular day, that’s good for us, Saturday at four o’clock in the morning for two hours, for example, that could be our maintenance window. So if there’s any auto minor version upgrades to take place, then these will be scheduled during our maintenance window. And then finally you have deletion protection. And this simply prevents a database from being deleted accidentally.</p>
<p>Now at the very bottom, it has your estimated monthly costs. So we can see the cost of the database instance and also for the storage. Once you’re happy with all your options, simply click Create database. And now we can see that here’s our database instance, and we can see the status is Creating. And we have a message up here saying that the database might take a few minutes to launch. Okay, we now have a message that says the database has successfully been created. And it’s as simple as that.</p>
<h1 id="RDS-vs-EC2"><a href="#RDS-vs-EC2" class="headerlink" title="RDS vs. EC2"></a>RDS vs. EC2</h1><h3 id="Instructor-Danny-Jessee"><a href="#Instructor-Danny-Jessee" class="headerlink" title="Instructor: Danny Jessee"></a>Instructor: Danny Jessee</h3><p>Hello, and welcome to this lecture, where I will discuss how to choose between using the Amazon Relational Database Service, or RDS, or Elastic Compute Cloud, or EC2 instances for your relational database deployments in the AWS Cloud. Both RDS and EC2 instances allow you to host databases securely within your own Virtual Private Cloud, or VPC. Both give you the ability to scale your databases to meet varying levels of storage, throughput, and performance demand, and both can be configured to operate within the AWS free tier. But that’s where the similarities end. In order to decide which option is best for your database, it helps to have a better understanding of the various benefits each approach has to offer. So let’s begin by discussing the advantages and disadvantages of both approaches, beginning with RDS.</p>
<h2 id="Amazon-RDS"><a href="#Amazon-RDS" class="headerlink" title="Amazon RDS"></a>Amazon RDS</h2><p>Amazon RDS is an AWS-managed service that allows you to automatically install and provision relational databases in the AWS Cloud using popular open-source and commercial database engines including MySQL, MariaDB, PostgreSQL, and even Oracle and Microsoft SQL Server. RDS also offers Amazon Aurora, which is a MySQL and PostgreSQL-compatible database with up to 5 times the throughput of MySQL, up to 64 TB of auto-scaling SSD storage, and 6-way replication of your data across 3 availability zones, all at a fraction of the cost of a traditional database deployment.</p>
<p>Now what makes RDS appealing for so many use cases is just how simple it is to quickly spin up a new database without having to install or configure any operating systems or database software. And since AWS is fully responsible for the management of your RDS instance, you never have to worry about any of the administrative tasks typically associated with managing a database like configuring backups, security, or high availability. You also never need to worry about patching or hardening the underlying operating system of the servers that host your databases. This makes RDS ideally suited for smaller development shops or any organization that can’t afford the expense of a dedicated database administrator, or DBA function. And although you don’t have access to the server that hosts your RDS database, you can still use most standard database administration tools to connect to your database and perform any necessary actions.</p>
<p>So let’s quickly run through some of the other advantages of using RDS for your database deployments in the AWS Cloud. When you choose RDS, AWS will automatically provision your database instance along with its associated storage, configure automatic backups, and even automatic minor version upgrades if you choose.</p>
<p>RDS also takes care of encryption for your databases, both in transit and at rest. And when we talk about encryption at rest, this extends beyond just your primary instance storage and also includes your database backups and snapshots as well.</p>
<p>You even have the option to choose between preconfigured templates for production or dev&#x2F;test deployments. So again, when you don’t have the time or resources for a dedicated DBA, or if you just want to get up and running as quickly as possible with your new database, it’s really great to let RDS handle all of these things for you.</p>
<h2 id="High-availability-and-scalability"><a href="#High-availability-and-scalability" class="headerlink" title="High availability and scalability"></a>High availability and scalability</h2><p>And beyond all of this, many of the supported database engines in RDS will also allow you to configure a highly available multi-AZ deployment with just a single click. And with a multi-AZ deployment, you get a 99.95% uptime SLA. So in a multi-AZ deployment, RDS will automatically create a standby, synchronous replica instance in a different availability zone than your primary instance. RDS will then automatically fail over to that standby instance in the event an infrastructure failure is ever detected on your primary instance. And these failures could include anything from an outage within an availability zone, to a loss of network connectivity, or even a compute or storage failure. And best of all, when a failure does happen, AWS will automatically update the URL associated with your database to point to your standby instance during a failover, then back to your primary instance once it’s up and running again. And all of this happens without requiring any manual intervention from an administrator whatsoever.</p>
<p>Now when it comes to scalability, RDS makes it easy to scale out your database deployments by adding asynchronous read replica instances, which can help offload read demand from your primary instance. And you can imagine how much time and effort would be involved for a DBA to have to spin up additional EC2 instances and then configure a solution like database mirroring or a failover cluster, so RDS definitely offers some advantages when it comes to configuring high availability and scalability. To learn more about when to use Multi-AZ deployments and read replicas with RDS, please check out this course:</p>
<p><em>When to use RDS Multi-AZ &amp; Read Replicas</em></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/rds-multi-az/"><em>https://cloudacademy.com/course/using-rds-multi-az-read-replicas/rds-multi-az/</em></a> </p>
<p>And to create your first RDS database, check out this hands-on lab:</p>
<p><em>Create Your First Amazon RDS Database</em></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/"><em>https://cloudacademy.com/lab/create-your-first-amazon-rds-database/</em></a> </p>
<h2 id="Amazon-EC2"><a href="#Amazon-EC2" class="headerlink" title="Amazon EC2"></a>Amazon EC2</h2><p>So as you’ve seen, RDS offers a lot of inherent advantages. It’s extremely easy to set up, it’s cost effective, and it offloads the responsibility for your more time and labor-intensive database administration tasks to AWS, allowing you to focus on your own business requirements. But there are still scenarios where it makes more sense to leverage EC2 instances to host your relational databases instead of RDS. So now, let’s talk about some of the reasons why you may want to choose EC2 instances over RDS.</p>
<p>Amazon EC2 allows you to provision virtual servers on demand that can run a variety of different operating system types and versions, whereas RDS abstracts away the underlying operating system of the database instance. So if your application requires a specific OS type or version for your database, or if you need to run a database that isn’t currently supported by RDS such as IBM DB2 or SAP HANA, you’ll need to provision your own EC2 instances. You’ll also need to use EC2 instances if your application requires you to configure any operating system-specific settings, or install any additional software directly on your database server.</p>
<p>It might also make sense for you to leverage EC2 instances if your organization already has full-time DBAs on staff who need to have access to advanced features and configuration settings within the database. For instance, RDS doesn’t support some of the more advanced features in Microsoft SQL Server such as persistent memory devices or its Resource Governor. If your application requires any of these features, you’ll have no choice but to provision SQL Server on an EC2 instance.</p>
<p>So as you’ve seen, most of the reasons you’d want to choose EC2 instances over RDS involve needing to have a greater level of control over your database or the underlying server operating system than what RDS provides. And this could involve anything from controlling exactly when or how you apply patches and perform maintenance on your database, to which ports and protocols you want your database to use. Perhaps you need to configure multiple versions or multiple instances of a particular database engine on a single server. You may even need to set up an advanced RAID and striping configuration for your database’s storage using EBS volumes. While all of these would be non-starters with RDS, they’re entirely possible when using EC2 instances.</p>
<p>Now just because you choose to use EC2 instances instead of RDS doesn’t mean you can’t still configure some of the native functionality that RDS provides such as high availability, automatic failover, and scheduled backups for your databases. Just know that setting all of this up with EC2 instances will require significantly more effort than if you just used RDS. But depending on your specific requirements, this may be a worthwhile tradeoff for you.</p>
<p>So that covers the reasons you might want to use EC2 instances for your database deployments instead of RDS. To create your first EC2 instance, please check out these hands-on labs:</p>
<p><em>Create Your First Amazon EC2 Instance (Linux)</em></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance/"><em>https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance/</em></a></p>
<p><em>Create Your First Amazon EC2 Instance (Windows)</em></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/"><em>https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/</em></a></p>
<h2 id="Final-Thoughts"><a href="#Final-Thoughts" class="headerlink" title="Final Thoughts"></a>Final Thoughts</h2><p>So to wrap things up here, you’re probably wondering: with all the benefits that RDS has to offer, surely all of this added functionality comes at a much higher cost when compared to just running your database on EC2 instances. Surprisingly, though, costs for database deployments using RDS and EC2 may be more comparable than you might expect. While RDS deployments are typically more expensive than comparable EC2 instance deployments, the difference in cost will vary depending on the specific instance and storage types you choose for your deployments. It’s also important to remember that the additional costs associated with those manual database administration tasks will become your responsibility if you choose to use EC2 instances instead of RDS.</p>
<p>So if you’re interested in getting a better sense of what these various database deployment options might cost for your applications in the AWS Cloud, you can input your specific numbers into the AWS Pricing Calculator at <a target="_blank" rel="noopener" href="https://calculator.aws/">calculator.aws</a>, where you can configure pricing estimates for different types of deployments by searching for things like RDS, Oracle, or EC2.</p>
<p>And just a couple of final thoughts: if you already have a dedicated DBA on staff, you’ll probably want to use EC2 instances for the cost savings you’ll get compared to RDS. You’ll also want to use EC2 instances if you need to provision a database type or version that isn’t supported by RDS. But in many other cases, RDS will probably be your best option. By reducing the management overhead associated with running your relational databases in the AWS Cloud, RDS allows you to worry less about tedious, labor-intensive administrative tasks, enabling you to focus on the things that deliver value to your business instead.</p>
<h1 id="RDS-Instance-Purchasing-Options"><a href="#RDS-Instance-Purchasing-Options" class="headerlink" title="RDS Instance Purchasing Options"></a>RDS Instance Purchasing Options</h1><p>Hello and welcome to this lecture where I want to cover the number of different purchasing options that are available and the associated costs. </p>
<p>You can purchase database instances through a variety of different payment plans. These have been designed to help you save costs by selecting the most appropriate option for your deployment. The different payment options within <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">RDS</a> include on-demand instances, on-demand instances (BYOL), reserved instances, reserved instances (BYOL), and serverless.</p>
<p>Currently, only the Oracle database engine uses the BYOL purchase options, all other DB engines only use on-demand instances and reserved instances, with the added exception of Aurora also using serverless as an additional purchasing option. </p>
<p>So to make things a little clearer you can see in this table which DB engines support each type of purchasing option.</p>
<p>It’s good to be aware of these different options as having an understanding of these can help you save a considerable amount of money depending on your use case. Let me run through each option to help explain. Starting with on-demand instances.</p>
<p>On-demand instances can be launched at any time and be provisioned and available to you within minutes. You can use this instance for a shorter time or for as long as you need before terminating the instance.</p>
<p>When you create an RDS database you are required to select the appropriate instance to support your DB from a processing and memory perspective, as shown here in this screenshot from the MySQL DB engine.</p>
<p>Depending on the options you select here will depend on what the instance price will be. The larger and more powerful the instance is, the more it will cost you. The cost of this compute capacity is calculated hourly with no minimum usage term. Any partial DB instance hours used are cost on per second increments. For any database changes that alter the running costs, such as modifying the instance or creating the DB instance, then a minimum of a 10-minute charge will be applied even if the DB is terminated or altered again before 10 minutes has passed.</p>
<p>At the time of writing this course, the following instance types are available for use.</p>
<p>For a breakdown of the performance of each of these instance types, please refer to the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation: <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">https://aws.amazon.com/ec2/instance-types/</a>. </p>
<p>As an example, the following table shows the pricing for each of the available instances with the MySQL DB engine within the London region, based on the deployment of an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/database-storage-and-i-o-pricing/">RDS database</a> within a single availability zone.</p>
<p>As you may or may not know, you can also deploy your RDS database within a single availability zone, or use a Multi-AZ deployment to add a layer of high availability. This will, of course, incur additional costs for your standby instance to run in another availability zone should your primary instance fail. </p>
<p>The following table shows the total costs for your multi-az deployment, which is typically double the cost of single deployment highlighted previously, again this is based upon the London region using the MySQL DB engine.</p>
<p>As well as both single-AZ and multi-AZ, some of the DB engines offer alternative options.</p>
<p>Amazon Aurora: When selecting Amazon Aurora as your DB engine, you can either select to deploy it with:</p>
<ul>
<li>Amazon Aurora with MySQL compatibility </li>
<li>Amazon Aurora with PostgreSQL compatibility</li>
</ul>
<p>Each of these options have a different price for the on-demand instances. </p>
<p>Also, you also have an option to Amazon Aurora Serverless which ultimately means there are no instances to manage. With this in mind, the on-demand pricing is not applicable, serverless pricing will be used instead and is measured in Aurora Capacity Units (ACUs). Each ACU consists of 2GB of memory in addition to any associated CPU and networking requirements. As an example, each ACU within the London region is charged at $0.07 per ACU Hour</p>
<p>Oracle: When selecting Oracle as your DB engine, your on-demand instance price will be dependant on which edition of Oracle you choose, the options available to you are:</p>
<ul>
<li>Standard Edition One (SE1)</li>
<li>Standard Edition Two (SE2)</li>
</ul>
<p>SQL Server: Much like with Oracle, the SQL Server DB engine is also dependent on the edition your select:</p>
<ul>
<li>Express</li>
<li>Web</li>
<li>Standard</li>
</ul>
<p>Again, each of these options will offer a different on-demand instance price.</p>
<p>Currently, the bring your own license options are only available with the Oracle DB engine. When using a bring your own license, you are able to use RDS with one of your pre-existing licenses that you already own, in this instance, one of your Oracle database software licenses. This differs from on-demand instances where your software licenses are already included in the price.</p>
<p>Again, much like on-demand instances that we discussed previously, you are charged by the hour with no long-term commitments of any sort.</p>
<p>Before you decide to use a BYOL license instance you need to ensure that your current license includes software update license and support for the particular instance that you are looking to create. </p>
<p>Also with BYOL for Oracle you have additional editions for deployment. This means that BYOL supports the following Oracle Editions:</p>
<ul>
<li>Standard Edition</li>
<li>Standard Edition One (SE1)</li>
<li>Standard Edition Two (SE2)</li>
<li>Enterprise Edition</li>
</ul>
<p>As you are only paying for the compute instances when using BYOL, there is no variation in prices between the different editions of Oracle being used.</p>
<p>The following tables shows the pricing for single-az deployment using Oracle with BYOL option with T3 and M5 instances.</p>
<p>The pricing for multi-az will be double the prices you see here.</p>
<p>Reserved instances allow you to purchase a discount for an instance type with set criteria for a set period of time in return for a reduced cost compared to on-demand instances. This reduction can be as much as 75%. These reservations against instances must be purchased in either one or three-year time frames. </p>
<p>Further reductions can be achieved with reserved instances depending on which payment methods you select. There are three options available to you, firstly:</p>
<ul>
<li>All upfront. The complete payment for the one or three-year reservation is paid. This offers the largest discount and no further payment is required regardless of the number of hours the instance is used. </li>
<li>Partial upfront, here a smaller upfront payment is made and then a discount is applied to any hours used by the instance during the term. </li>
<li>No upfront, no upfront or partial payments are made and the smallest discount of the three models is applied to any hours used by the instance.</li>
</ul>
<p>The following is an example of the pricing for MySQL reserved instance pricing in the London Region for the db.t3.medium instance type</p>
<p>When compared to the on-demand pricing there is a big cost-savings, especially when you factor in the difference in term length, plus any upfront payments. In this example, the on-demand pricing for the same instance type is $0.076 per hour, now if you compare this to the 3 year term, full upfront payment option of just $0.037 per hour, that’s a saving of just over 51%!</p>
<p>Again, we also have the option to implement Multi-AZ with reserved instances, and for consistency, here is the pricing of Multi-AZ with the same instance type as the example above.</p>
<p>Again, much like the on-demand pricing there are some additional deployment options when using reserved instances.</p>
<p>Amazon Aurora: When selecting Amazon Aurora as your DB engine, you can either select to deploy it with:</p>
<ul>
<li>Amazon Aurora with MySQL compatibility </li>
<li>Amazon Aurora with PostgreSQL compatibility</li>
</ul>
<p>Each of these options have a different price for the reserved instances. </p>
<p>Oracle: When selecting Oracle as your DB engine, your reserved instance price will be dependant on which edition of Oracle you choose, the options available to you are:</p>
<ul>
<li>Standard Edition One (SE1)</li>
<li>Standard Edition Two (SE2)</li>
</ul>
<p>SQL Server: Much like with Oracle, the SQL Servers DB engine is also dependent on the edition your select:</p>
<ul>
<li>Express</li>
<li>Web</li>
<li>Standard</li>
</ul>
<p>Again, each of these options will offer different reserved instance prices.</p>
<p>This follows the same principles I covered earlier when discussing BYOL for on-demand instance pricing. At the time of writing this course, it is only available for Oracle, and the same prerequisites apply, in addition to the same 4 editions being available:</p>
<ul>
<li>Standard Edition</li>
<li>Standard Edition One (SE1)</li>
<li>Standard Edition Two (SE2)</li>
<li>Enterprise Edition</li>
</ul>
<p>As you are only paying for the compute instances when using BYOL, there is no variation in prices between the different editions of Oracle being used.</p>
<p>The following table shows the reserved pricing for single-az deployment using Oracle with BYOL option for the t3.micro instance. </p>
<p>If you compare this to the pricing below which shows the same instance types, but using a standard reserved instance where the price of the license is included, you can see there is a substantial difference. </p>
<p>Again, the pricing for multi-az will be double the prices you see here.</p>
<h1 id="Database-Storage-and-I-x2F-O-Pricing"><a href="#Database-Storage-and-I-x2F-O-Pricing" class="headerlink" title="Database Storage and I&#x2F;O Pricing"></a>Database Storage and I&#x2F;O Pricing</h1><p>Hello and welcome to this lecture where I shall be looking at how both primary storage and I&#x2F;O pricing is configured.</p>
<p>We have looked at the pricing options for your database compute instances themselves, I now want to focus on the storage aspects of your databases and how these charges are calculated across the different DB engines.</p>
<p>MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server all use Elastic Block Store (EBS) volumes for both data and log storage. However, Aurora on the other hand uses a shared cluster storage architecture and does not use EBS.</p>
<p>So let me first look at the pricing for the majority of the DB engines using EBS.</p>
<p>When configuring your storage requirements, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">RDS</a> supports 3 different types:</p>
<ul>
<li>General Purpose SSD Storage</li>
<li>Provisioned IOPS (SSD) storage</li>
<li>Magnetic Storage</li>
</ul>
<p>Let’s take a closer look at each, starting with General purpose SSD storage:</p>
<ul>
<li><p>General Purpose SSD storage: This is a good option for a broad range of use cases which provides single-digit millisecond latencies and offers a cost-effective storage solution. The minimum SSD storage for your primary data set is 20 GiB, with a maximum of 64 TiB for MySQL, PostgreSQL, MariaDB, and Oracle, however, the maximum for SQL Server is 16 TiB. When using SSD, you are charged for the amount of storage provisioned and not for the number of I&#x2F;Os processed</p>
</li>
<li><p>Provisioned IOPS (SSD) storage: This option is great for when you have workloads that operate at a very high I&#x2F;O. You can provision a minimum of 8000 IOPS, and a maximum of 80000 for MySQL, PostgreSQL, MariaDB, and Oracle, but the maximum for SQL Server is 40000. In addition to being able to provision the IOPS needed for your workload, the minimum storage for your primary data set is 100 GiB, with a maximum of 64 TiB for MySQL, PostgreSQL, MariaDB and Oracle, and 16 TiB for SQL Server. The charges for this option are based upon the amount of storage provisioned in addition to the IOPS throughput selected, again, you are charged not for the total number of I&#x2F;Os processed.</p>
</li>
<li><p>Magnetic storage: Finally magnetic storage is simply supported to provide backward compatibility, and so <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> recommends that you select General Purpose instead</p>
</li>
</ul>
<p>The following screenshot shows the configuration screen when determining your storage requirements for MySQL. In this example Provisioned IOPS (SSD) has been selected as the storage, with a minimum of 100 GiB of primary storage, and a 1000 provisioned IOPS as throughput.</p>
<p>Let’s now take a look at the pricing structure for the storage.</p>
<p>The costs for your database storage has 2 different price points depending on whether it has been configured as a single-AZ or Multi-AZ deployment. Much like the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/rds-instance-purchasing-options/">instance pricing</a>, the Multi-AZ is typically twice the value of single-AZ deployment. </p>
<p>Each of the storage types, General Purpose SSD, Provisioned IOPS (SSD), and Magnetic all come at a different price. For each type of storage used, it is priced at per GB-Month, but what is the metric of GB-Month exactly? </p>
<p>Essentially this defines how many GBs of storage have been provisioned and for how long. Let me give an example. Assume we are working on a 30 day month and we receive a bill for 10GB-Months of storage, this could be the result of the following scenarios:</p>
<ul>
<li>You have 300-GB of storage running for just 24 hours </li>
<li>You have 10-GB of storage running for 720 hours </li>
<li>You have 40-GB of storage running for 180 hours</li>
</ul>
<p>These are calculated using the following formula: </p>
<p>Total provisioned storage &#x2F; (720&#x2F;number of hours running)</p>
<p>Where 720 &#x3D; number of hours in a 30-day month</p>
<p>So, looking at the examples above the calculations would be as follows:</p>
<ul>
<li>300 &#x2F; (720&#x2F;24)</li>
<li>10 &#x2F; (720&#x2F;720)</li>
<li>40 &#x2F; (720&#x2F;180)</li>
</ul>
<p>The following screenshots show the costs for each of the storage types for the MySQL DB engine in the London region under a single-az deployment:</p>
<p>I now want to revisit the storage for the Aurora DB engine. As I explained earlier, Aurora uses a shared cluster storage architecture which is managed by the service itself. When configuring your Aurora database in the console, the option to configure and select storage options like we saw previously does not even exist. Your storage will scale automatically as your database grows. As a result the pricing structure for your Aurora database is priced differently.</p>
<p>Again, the pricing metric used is GB-Months, in addition to the actual number of I&#x2F;Os processed, which are billed per million requests. The great thing about using Aurora is that you are only billed for the storage used and IOs processed, whereas with MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server you are billed for the storage provisioned regardless of if your use all of it or just a part of it. </p>
<p>As an example, the following shows the costs for both the used in GB-Months, plus the I&#x2F;Os processed (per million requests) within the London region.</p>
<h1 id="Backup-Storage-Pricing"><a href="#Backup-Storage-Pricing" class="headerlink" title="Backup Storage Pricing"></a>Backup Storage Pricing</h1><p>Now that we have looked at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/database-storage-and-i-o-pricing/">primary storage and I&#x2F;O costs</a> for your database, let’s now take a look at what costs you will incur for backups that you take of your database. Firstly, I want to identify which actions and considerations need to be taken into account and that will affect how your backup storage fluctuates:</p>
<ul>
<li><p>The first point I want to make, and it might surprise you, is that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">Amazon RDS</a> does not charge any backup storage costs that equates to the total sum of provisioned storage used with your databases within a specific region. So for example, Assume you had a MySQL DB with provisioned storage of 150GiB-Month DB, plus a MariaDB DB with 450GiB in a single region. Amazon RDS would not charge you for any storage utilised up to 600GiB-Month.</p>
</li>
<li><p>Any backup storage used over this ‘free’ tier is charged at $0.10 per GiB-Month, regardless of the region for:</p>
</li>
<li><ul>
<li>MySQL</li>
<li>PostgreSQL</li>
<li>MariaDB</li>
<li>Oracle</li>
<li>SQL Server</li>
</ul>
</li>
<li><p>Any backup storage used over this ‘free’ tier for Aurora is charged differently based upon the region. Currently at the time of writing this course, it is $0.022 per GiB-Month in the London region</p>
</li>
<li><p>Any automated backups taken use backup storage</p>
</li>
<li><p>Any manual snapshots that are taken of your database will also use backup storage</p>
</li>
<li><p>By extending your backup retention periods (how long you’d like to keep your backups for) will increase the amount of storage required for backups</p>
</li>
<li><p>Backup storage is calculated based upon the total amount of backup storage consumed within a specific region across all your RDS databases</p>
</li>
<li><p>If you copy your backups to another region, this will also increase the amount of backup storage used within that new region</p>
</li>
</ul>
<p>So all in all, backup storage for your RDS databases is relatively simple to calculate and inexpensive bearing in mind it is free to backup 100% of your provisioned database storage. </p>
<p>Let me now move onto <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/backtrack-storage-pricing/">Backtrack storage costs</a> with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> Aurora.</p>
<h1 id="Backtrack-Storage-Pricing"><a href="#Backtrack-Storage-Pricing" class="headerlink" title="Backtrack Storage Pricing"></a>Backtrack Storage Pricing</h1><p>Let me now move on to backtrack storage costs with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> Aurora. Backtrack is a feature that is only currently available for a MySQL-compatible Aurora database, using and is configured at the time of the database creation. Essentially, backtrack allows you to go back in time on the database to recover from an error or incident without having to perform a restore or create another DB cluster. For a deeper dive on on Backtrack storage, take a look at this AWS blog post found here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/">https://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/</a></p>
<p>As you can see from the configuration page during the Aurora database creation process, it is enabled via a checkbox and allows you to enter a number in hours of how far you would like to ‘backtrack’ to, with a maximum of 72 hours. In this example, I have entered 12 hours, and so Aurora will retain log data of all changes 12 hours as specified. The number of changes made directly relates to how much the Backtrack feature is going to cost you. </p>
<p>The pricing shown here is based upon a set cost per 1 million change records per hour for the London region.</p>
<p>So let’s look at an example. If you had built an Aurora database with a 12 hour backtrack setting like I had in my previous example that was generating 50,000 change records per hour the calculation would be as follows:</p>
<p>Obtain the total number of change records for your backtrack time period:</p>
<p>50,000 (change records&#x2F;hour) x 12 hours &#x3D; 600,000 change records </p>
<p>Calculating total costs based upon the London region:</p>
<p>(600,000 &#x2F; 1,000,000) x $0.014 &#x3D; $0.0084&#x2F;hour</p>
<p>To help you keep an accurate record of the number of change records, you can use Amazon CloudWatch to help you monitor the number of change records that are being generated each hour. </p>
<h1 id="Snapshot-Export-Pricing"><a href="#Snapshot-Export-Pricing" class="headerlink" title="Snapshot Export Pricing"></a>Snapshot Export Pricing</h1><p>Snapshots in RDS are your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/backup-storage-pricing/">backups</a> of your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/database-storage-and-i-o-pricing/">database</a> tables and instances, and these snapshots can then be exported out of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">Amazon RDS</a>, to Amazon S3. You might want to do this perform analysis of the data held within your database using more specific tools, for example, Amazon Athena, which is an interactive query service that allows you to perform analysis on data that you have stored in Amazon S3 using a standard SQL. </p>
<p>You could also use other services, such as Amazon SageMaker and Amazon Elastic Map Reduce (EMR), however, the point is, the data can be exported from your RDS database to S3 for further in-depth analysis as required.</p>
<p>Also, during an export of a snapshot, you can decide, through filtering, to simply export specific databases, tables, or even schemas.</p>
<p>The cost associated with performing snapshot exports are again based on a region by region basis. The example here shows the costs for the Ireland Region when using MySQL and is based upon a cost per GB of snapshot</p>
<p>For additional security and protection of your exported data, you can encrypt the data using the Key Management Service (KMS). As a result, additional KMS costs would be incurred depending on the KMS key selected. For more information on the Key Management Service, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/">https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/</a></p>
<p>Also, you will also be charged costs associated with Amazon S3 for the storage and your PUT requests. For more information on how Amazon S3 costs are calculated, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/">https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/</a></p>
<h1 id="Data-Transfer-Pricing"><a href="#Data-Transfer-Pricing" class="headerlink" title="Data Transfer Pricing"></a>Data Transfer Pricing</h1><p>When we talk about data transfer there are a number of different data paths where you might be charged for transferring data IN to and OUT of your RDS database depending on the source and destination, for example:</p>
<ul>
<li>Data transferred IN to your RDS database from the internet</li>
<li>Data transferred OUT from your RDS database to the internet</li>
<li>Data transferred OUT to Amazon CloudFront</li>
<li>Data transferred OUT to <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Regions</li>
<li>Data transferred OUT to EC2 instances in the same availability zone</li>
<li>Data transferred between availability zones for multi-az replication</li>
<li>Data transferred between an EC2 instance and an RDS instance in different availability zones of the same region</li>
<li>Data transferred when a snapshot copy is transferred to a different region</li>
</ul>
<p>Across the different DB engines, there is consistency when it comes to data transfer costs, so let’s take a look at each of the price points individually. In the following examples, I will be taking the pricing from the MySQL DB engine.</p>
<p>Firstly, let me show you under which circumstances you will NOT be charged for data transfer and so it completely free, this includes:</p>
<ul>
<li>Any data that is transferred IN to your RDS database from the internet</li>
<li>Any Data that is transferred OUT to Amazon CloudFront</li>
<li>Any data that is transferred OUT to EC2 instances in the same availability zone</li>
<li>Data transferred between availability zones for multi-az replication - For those who might be new to Multi-AZ, it simply means Multi-Availability Zone, which is a feature that is used to help with resiliency and business continuity. When Multi-AZ is configured, a secondary RDS instance, known as a replica, is deployed within a different availability zone within the same region as the primary instance. That’s its single and only purpose, to provide a failover option for a primary RDS instance.  As a result, there is a requirement for data replication between the primary and the secondary.</li>
</ul>
<p>Data transferred OUT from your RDS database to the internet: This element of data transfer is very dependent on how much data you transfer per month and is charged per GB. As a result, the resulting pricing is reflected as a tiered structure as you can see.</p>
<p>The first GB each month is free, and then as the amount of data transferred increases in Terabytes, the amount charged in GB decreases. The more you transfer out to the internet, the cheaper per GB your data transfer becomes.</p>
<p>If data is transferred from RDS out to another region, then you will incur a small charge per GB, and as it stands at the time of writing this course, this is a flat fee of $0.02 per GB for ANY region that the data is transferred to. </p>
<p>In this instance, charges will apply for Amazon EC2 regional data transfer both sides of the transfer and is charged at $0.01&#x2F;GB in each direction.</p>
<p>As explained earlier, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/snapshot-export-pricing/">Snapshots</a> in RDS are your backups of your database tables and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/rds-instance-purchasing-options/">instances</a>, and these snapshots can be both exported out of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-costs-associated-with-amazon-rds-1050/course-introduction/">Amazon RDS</a> and copied to another region. These Cross-region snapshots are a great way to help you implement a disaster recovery strategy across your RDS database infrastructure.</p>
<p>When copying your snapshots across regions you are just charged for the amount of data transferred based on the size of the snapshot, based on $0.02 per GB.</p>
<h1 id="Amazon-DynamoDB"><a href="#Amazon-DynamoDB" class="headerlink" title="Amazon DynamoDB"></a>Amazon DynamoDB</h1><p>Hello and welcome to this lecture covering Amazon DynamoDB. Amazon DynamoDB is a NoSQL database, which means that it doesn’t use the common Structured Query Language, SQL. It falls into a category of databases known as key-value stores. A key value store is simply a collection of items or records, and you can look up data by using a primary key for each item or through the use of indexes.</p>
<p>Amazon DynamoDB is designed to be used for ultra high performance, which could be maintained at any scale with single-digit latency, making this a very powerful database choice used commonly for gaming, web, mobile and IoT applications to name but a few. Much like Amazon RDS, DynamoDB is also a fully managed service, taking many of the day-to-day administration operations out of your hands, giving you more time to focus on the business logic of your database. That’s one of the great things about Amazon DynamoDB, there’s no database administration required by us as a customer, no service to manage and nothing to back up. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> handles all of this for you. This makes the creation of a DynamoDB database very easy. All you have to do is set up your tables and configure the level of provision throughput that each table should have. Provision throughput refers to the level of read and write capacity that you want AWS to reserve for your table. You are charged for the total amount of throughput that you configure for your tables plus the total amount of storage space used by your data.</p>
<p>If we actually look at the configuration screen when creating a new DynamoDB database, as seen here, you can see that there are very few options required to create a new database. And in fact, in its simplest form, you can just provide a table name and a primary key, which is used to partition data across hosts for scalability and availability. You can then accept any remaining defaults and create your database, it’s as simple as that. DynamoDB tables are considered schemaless because there’s no strict design and schema that every record must conform to. As long as each item has an appropriate primary key, the item can contain varying sets of attributes. The records in a table do not need to have the same attributes or even the same number of attributes. This can be very convenient for rapid application development and if you want to add a new column to a table, you don’t need to alter the table, you can just start including the new field as an attribute when you insert new records. Likewise, you never need to adjust the data type for a column as DynamoDB generally isn’t interested in data types for individual attributes.</p>
<p>If when creating your DynamoDB database, you choose not to reset all the defaults, what other options exist? Let’s take a look. Unchecking the use default settings from the Table settings section provides you with the following. Firstly, you’ll be asked about secondary indexes, which allow you to perform queries on attributes that are not part of the table’s primary key. The default option provides no secondary index. However, you can add them here if required. DynamoDB lets you create additional indexes so that you can run queries to search your data by other attributes. If you’ve worked with relational databases, you’ve probably used indexes with those, but there are a couple of big differences in how indexes operate in DynamoDB.</p>
<p>First, each query can only use one index. If you want to query and match on two different columns, you need to create an index that can do that properly. Second, when you write your queries, you need to specify exactly which index should be used for each query. It’s not like a relational database that has a query analyzer, which can decide which indexes to use for our query. Here you need to be explicit and tell DynamoDB what index to use. DynamoDB has two different kinds of secondary indexes, global indexes let you query across the entire table to find any record that matches a particular value and by contrast, local secondary indexes can only help find data within a single partition key.</p>
<p>Following secondary indexes, you can modify the default settings applied to your table’s read&#x2F;write capacity mode. When you create a table in DynamoDB, you need to tell AWS how much capacity you want to reserve for the table. You don’t need to do this for disk space as DynamoDB will automatically allocate more space for your table as it grows. However, you do need to reserve capacity for input and output for reads and writes. Amazon charges you based on the number of read capacity units and write capacity units that you allocate. It’s important to allocate enough for your workload, but don’t allocate too much or DynamoDB could become prohibitively expensive.</p>
<p>By default, when you create a table in the AWS Console, Amazon will configure your table with five read capacity units and five write capacity units. There are two modes that you can choose from, provisioned and on-demand. Provisioned mode allows you to provision set read and writes allowed against your database per second by your application and is measured in capacity units, RCUs for reads and WCUs for writes. Depending on the transaction, each action will use one or more RCUs or WCUs. Provisioned mode is used generally when you have a predicted and forecasted workload of traffic. On-demand mode does not provision any RCUs or WCUs, instead they are scaled on demand. The downside is that it is not as cost effective as provisioned. This mode is generally used if you do not know how much workload you are expected to experience. Over time, you are likely to get more of an understanding of load and you can change your mode across to provisioned.</p>
<p>Once you have selected the provisioned mode, you will then have the opportunity to add configuration information relating to how your RCU and WCU are scaled as demand increases and decreases. As you can see, by entering your minimum and maximum provisioned capacity along with your target threshold utilization as a percentage, you can confidently rely on Amazon DynamoDB to manage the scaling operations of your throughput.</p>
<p>The last main point of the configuration allows you to set encryption of your tables, which is enabled by default for data at rest. Through the use of the key management service, KMS, you are able to select either a customer managed or AWS managed CMK to use for the encryption of your table instead of the default keys used by DynamoDB. For more information on CMKs and the key management service in general, please refer to our existing course found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Before I finish this lecture covering DynamoDB, I just want to cover some of its advantages and also what can be considered disadvantages. Some of the advantages of DynamoDB is that it’s fully managed by AWS, you don’t have to worry about backups or redundancy, although you’re welcome to set up these kinds of safeguards using some more advanced DynamoDB features.</p>
<p>As mentioned previously, DynamoDB tables are schemaless so you don’t have to define the exact data model in advance, the data model can change automatically to fit your application’s needs.</p>
<p>DynamoDB is designed to be highly available and your data is automatically replicated across three different availability zones within a geographic region. In the case of an outage or an incident affecting the entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>DynamoDB is designed to be fast, read and writes take just a few milliseconds to complete and DynamoDB will be fast no matter how large your table grows, unlike relational database, which can slow down as the table gets large. DynamoDB performance is constant and stays consistent even with tables that are many terabytes in size. You don’t have to do anything to handle this, except adjusting the provisioned throughput levels to make sure you’ve preserved enough read and write capacity for your transaction volume.</p>
<p>There are also some downsides to using DynamoDB too. As I just mentioned, your data is automatically replicated. Three copies are stored in three different availability zones and that replication usually happens quickly in milliseconds, but sometimes it can take longer and this is known as eventual consistency. This happens transparently and many operations will make sure that they’re always working on the latest copy of your data, but there are certain kinds of queries and table scans that may return older versions of data before the most recent copy. You need to be aware of how this works and you may need to adjust certain queries to require strong consistency.</p>
<p>DynamoDB’s queries aren’t as flexible as what you can do with SQL. If you are used to writing advanced queries with joins and groupings and summaries, you won’t be able to do that with DynamoDB. You’ll have to do more of the computation in your application code. This is done for performance reasons to ensure that every query finishes quickly and that complicated queries can’t hog the resources on a database server. </p>
<p>DynamoDB also has some strict limitations in the way you’re allowed to work with it. Two important limitations are the maximum record size of 400 kilobytes and the limit of 20 global indexes and five secondary indexes per table. There are other limitations that can be adjusted by contacting AWS customer support like the maximum number of tables in an AWS account.</p>
<p>Finally, although DynamoDB performance can scale up as your needs grow, your performance is limited to the amount of read and write throughput that you’ve provisioned for each table. If you expect a spike of the database use, you’ll need to provision more throughput in advance or database requests will fail with a ProvisionedThroughputExceededException message. Fortunately, you can adjust throughput at any time and it only takes a couple of minutes to adjust. Still, this means that you’ll need to monitor the throughput being used in each table or you’ll risk running out of throughput if your usage grows.</p>
<h1 id="DEMO-Creating-a-DynamoDB-Database"><a href="#DEMO-Creating-a-DynamoDB-Database" class="headerlink" title="DEMO: Creating a DynamoDB Database"></a>DEMO: Creating a DynamoDB Database</h1><p>Hello, and welcome to this lecture. This is going to be a demonstration on how to quickly, and easily create a DynamoDB database. Now, first I’ll need to go to the database category, and here we can see DynamoDB. Now I don’t have any DynamoDB databases sets up yet. So if you select create table, and you’ll be presented with this screen.</p>
<p>So first we’ll need to give it a table name, just call this my database and also a primary key. Now the primary key is essentially used to uniquely identify each item in the table. And the primary key is essentially comprised of a partition key. So let me just add one in. I’ll just call this product ID, and we can slate either a string, binary, or a number. I’ll leave that as a string. If we need to, we can also add in a sort key as well. And as we can see here the sort key simply allows you to search within a partition. Just remove that sort key.</p>
<p>Now essentially you can now create your table simply from providing that information, because this tick box here allows you to use lots of default settings that essentially fills in the rest of the configuration for you. So if you’re happy with your table name and primary key, with these default settings for your table, you can simply click create and it’s done. However, I want to uncheck the default settings so you can see the different configurable components used. So let’s take a look.</p>
<p>Now, firstly we have our secondary indexes, so you can add a secondary index, and these allow you to perform queries on attributes that are not part of the table’s primary key. Next, we have our read and write capacity mode, either provisioned or on demand. If we select the provisioned capacity mode, then we can select our read capacity units, and also our write capacity units.</p>
<p>Now scrolling down a bit further to auto scaling. We can set up auto scaling for our read and write capacity units. So when the read capacity gets to 70% utilization, we can scale up to a maximum of 40,000 units, and the same with the write capacity. So you can alter these figures if you need to, and change them to whatever values you need. As a part of that auto scaling process, DynamoDB needs an auto-scaling service link role to give it permission to do so.</p>
<p>Once you’re happy with your read and write capacity units, we can then scroll down to encryption at rest. Now by default encryption is enabled. The default option uses a key that’s owned by DynamoDB, and you are not charged for the use of any encryption keys in this default setting. However, you can use a KMS custom managed CMK, which is the CMK that you may have created, and you can select it from this box here, if you have any and enter the ARN, or you can use the KMS <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> managed CMK, which is this key here.</p>
<p>So it depends on the level of control that you want for the encryption key. First demonstration I’m just gonna leave it as the default. At the bottom here, you can add any text to a database if you’d like. So once you’re happy with the configuration, you simply click on create. As we can see the table is being created. And once it’s created, you can then use these tabs along the top to set up any alarms and review your capacity units set up your indexes, backups, etc, etc, etc. But for this demonstration, I simply wanted to show you how quickly and easy it is to set up and configure a DynamoDB table.</p>
<h1 id="DynamoDB-Accelerator-DAX"><a href="#DynamoDB-Accelerator-DAX" class="headerlink" title="DynamoDB Accelerator (DAX)"></a>DynamoDB Accelerator (DAX)</h1><p>Before we get into DAX itself, I just want to take a minute to cover a 10,000-foot overview of DynamoDB.</p>
<p>Amazon DynamoDB is a fully managed NoSQL database service. There’s no database administration required on your end, no servers to manage and nothing to back up. All of this is handled for you by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. All you have to do is set up your tables and configure the level of provisioned throughput that each table should have.</p>
<p>As DynamoDB is designed to be highly available your data is automatically replicated across multiple availability zones within a geographic region. In the case of an outage or an incident affecting an entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>DynamoDB is designed to be fast. Read and writes take just a few milliseconds to complete, and DynamoDB will be fast no matter how large your tables grow. Unlike a relational database, which can slow down as the table gets large, DynamoDB performance is constant and stays consistent even with tables that are many terabytes in size. You don’t have to do anything to handle this, except adjusting the provisioned throughput levels to make sure you’ve reserved enough read and write capacity for your transaction volume.</p>
<p>Despite its many benefits, there can be some downsides to using DynamoDB too. As I mentioned, your data is automatically replicated to different AZs, and that replication usually happens quickly, in milliseconds, but sometimes it can take longer. This is known as eventual consistency. This happens transparently and many operations will make sure that they’re always working on the latest copy of your data. But there are certain kinds of queries and table scans that may return older versions of data before the most recent copy.</p>
<p>This can be a problem in certain circumstances, and sometimes milliseconds might not be fast enough for your use case. You might have a requirement where you need microsecond response times in read-heavy workloads, and this is where DynamoDB Accelerator (DAX) comes in to play. By combining DynamoDB with DAX, you end up with a NoSQL database solution offering extreme performance.</p>
<p>So what is DAX? DAX is an in-memory cache delivering a significant performance enhancement, up to 10 times as fast as the default DynamoDB settings, allowing response times to decrease from milliseconds to microseconds. It is a fully managed feature offered by AWS and as a result, is also highly available.</p>
<p>Dax is also highly scalable making it capable of handling millions of requests per second without any requirement for you to modify any logic to your applications or solutions as its fully compliant with all DynamoDB API calls, and so it seamlessly fits into your existing architecture without any redesign and effort from your developer teams.</p>
<p>Your DAX deployment can start with a multi-node cluster, containing a minimum of 3 nodes, which you can quickly and easily modify and expand, reaching a maximum of 10 nodes, with 1 primary and 9 read replicas. This provides the ability to handle millions of requests per second.</p>
<p>Another benefit of using DAX is that it can also enable you to reduce your provisioned read capacity within DynamoDB. This is because of the fact that data is cached by DAX and so reduces the impact and amount of read requests on your DB tables, instead, these will be served by DAX from the in-memory cache. As we know, reducing the provisioned requirements on your DynamoDB database, will also reduce your overall costs.</p>
<p>From a security perspective, DAX also supports encryption at rest, this ensures that any cached data is encrypted using the 256-bit Advanced Encryption Standard algorithm with the integration of the Key Management Service (KMS) to manage the encryption keys.</p>
<p>If you are new to KMS, then you can find more on this service from our existing course found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>DAX is a separate entity to DynamoDB and so architecturally it sits outside of DynamoDB itself and is instead placed within your VPC, whereas DynamoDB sits outside of your VPC and is accessed via an endpoint.</p>
<p>During the creation of your DAX Cluster, you will be asked to select a subnet group, this is simply a grouping of subnets across different availability zones. This is to allow DAX to deploy a node in each of the subnets of the subnet group, with one of those nodes being the primary and the remaining nodes will act as read replicas.</p>
<p>To allow your EC2 instances to interact with DAX you will need to install a DAX Client on those EC2 instances. This client then intercepts with and directs all DynamoDB API calls made from your client to your new DAX cluster endpoint, where the incoming request is then load balanced and distributed across all of the nodes in the cluster.</p>
<p>To allow your EC2 instances to communicate with your DAX Cluster you must ensure that the security group associated with your DAX Cluster is open to TCP port 8111 on the inbound rule set.</p>
<p>If a request received by DAX from your client is a read request, such as a <code>GetItem</code>, <code>BatchGetItem</code>, <code>Query</code> or <code>Scan</code>, then the DAX cluster will try and process the request if it has the data cached. If DAX does not have the request in its cache (a cache miss) then the request will be sent to DynamoDB for the results to be returned to the client. These results will also then be stored by DAX within its cache and distributed to the remaining read replicas in the DAX cluster.</p>
<p>With regards to any write requested made by the client, the data is first written to DynamoDB before it is written to the cache of the DAX cluster.</p>
<p>One final point I want to make is that DAX does not process any requests relating to table operations and management, for example, if you wanted to create, update or delete tables. In this instance, the request is passed through directly to DynamoDB.</p>
<h1 id="Amazon-ElastiCache"><a href="#Amazon-ElastiCache" class="headerlink" title="Amazon ElastiCache"></a>Amazon ElastiCache</h1><p>Hello and welcome to this lecture dedicated to an overview of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> ElastiCache. Amazon ElastiCache is a service that makes it easy to deploy, operate, and scale open-source, in-memory data stores in the cloud. This service improves the performance through caching, where web applications allow you to retrieve information from fast managed in-memory data stores instead of relying entirely on slower disk-based solutions.</p>
<p>Let me take a closer look at caching and then expand our discussion to in-memory caching environments in the cloud.</p>
<p>If you think about your own computing devices, your laptop, for example, you may have a huge capacity from a persistent storage perspective via your hard drives with lots of applications installed, allowing you to work on a variety of documents at the same time. However, over time, you may notice a degradation in performance, and certain operations might start to become sluggish, especially as more and more files are opened and accessed to carry out different functions and tasks. When this happens, how can we make the computer perform faster and serve up our information quicker and enhance our response times? One solution is to install additional random access memory, known as RAM. With this additional memory, it enables our device to store frequently accessed information in memory instead of having to request the information from the hard drive, which is much slower than RAM. This process is known as caching.</p>
<p>Now let’s take this approach and relate it to how a web application uses in-memory caching. To be clear, ElastiCache is not just used for web applications. It can be used for any application that can benefit from increased performance using in-memory cache. But to explain the process, I shall focus on a web application as an example.</p>
<p>A common scenario is to have a web application that reads and writes data to persistent storage, for example, to a relational database such as RDS or a NoSQL database such as DynamoDB. However, persistent storage, like hard disk storage, tends to experience some fluctuations in latency as each piece of data needs to be written to or retrieved from a permanent media store, and this can affect the overall performance.</p>
<p>This is where an in-memory cache is useful. It’s generally used to improve read-only performance. Many websites get a high percentage of read hits, but less write hits. An in-memory cache can store frequently accessed read-only information and serve it up much quicker than having the application continually request it from a persistent data store.</p>
<p>Now imagine that your app becomes more popular and you need to scale up. Adding more web servers is not that difficult. However, instead of vertically scaling your data store for your database, you could instead add an in-memory caching layer with sub-millisecond response times, which will of course make a considerable difference. By adding more web servers along with an ElastiCache cluster, you can automatically grow your caching layer based on the increased demand. This can eliminate or reduce the need to scale up on your persistent data store.</p>
<p>ElastiCache supports both Memcached and the Redis engines, so existing applications can be easily moved to ElastiCache. But what is Memcached and Redis and which cache engine should we choose? Amazon ElastiCache for Memcached is a high-performance sub-millisecond latency Memcached-compatible in-memory key store service that can either be used as a cache in addition to a data store. Amazon ElastiCache for Redis is purely an in-memory data store designed for high performance and again providing sub-millisecond latency on a huge scale to real-time applications.</p>
<p>As we can see, both engines provide a great cache solution and are both easily provisioned using the ElastiCache service. In this table, we can see some of the primary use cases for each.</p>
<p>Generally Redis offers a more robust set of features to that of Memcached. In broad terms, Redis has more features, whereas Memcached is often recognized for its simplicity and speed of performance. Memcached really suits workloads where memory allocation is going to be consistent and the increased performance is more important than the additional features that Redis offers.</p>
<p>As you can see from this screenshot during the configuration of ElastiCache, Redis also offers an option to enable cluster mode. When Redis cluster mode is disabled, each cluster will have just a single shard. However, with cluster mode enabled, each cluster can have up to 90 shards. So now’s a good time to run through at a high level some of these fundamental elements of ElastiCache. These include nodes, shards, and clusters.</p>
<p>ElastiCache nodes. A cache node is a fixed sized chunk of secure network attached RAM, essentially the building block of the ElastiCache service, and supports a clustered configuration. As you can see here, the nodes themselves can be launched using a variety of different instance types.</p>
<p>ElastiCache for Redis shards. A Redis shard, also known as a node group when working with the API and CLI, is a group of up to 6 ElastiCache nodes.</p>
<p>ElastiCache for Redis clusters. A Redis cluster contains between one and 90 Redis shards, depending on if cluster mode is enabled or disabled. Data is then partitioned across all of the shards in that cluster. </p>
<p>ElastiCache for Memcached clusters. Clusters are a collection of one or more cache nodes. Once you’ve provisioned a cluster, Amazon ElastiCache automatically detects and replaces failed nodes, which helps reduce the risk of overloaded database, and therefore reduces the website and application load times.</p>
<p>Before I finish this lecture covering ElastiCache, I just want to point out some of the common use cases where you might use Amazon ElastiCache. Due to its incredibly fast performance and scaling abilities, this is commonly used in the online gaming industry where it’s vital that statistical information like a scoreboard is presented as quickly and as consistently as possible to all the players in the game.</p>
<p>Another common use is for social networking sites, where they need a way to store temporary session information in session management.</p>
<p>Real-time analytics is also a great use for ElastiCache, as it can be used in conjunction with other services such as Amazon Kinesis to ingest, process, and analyze data at scale. This could help with personalized ads and recommendations with sub-millisecond latency.</p>
<p>As you can see, these are applications that have lots of read-only content that would benefit from an in-memory cache. Often users are scanning these websites for information like who currently has the high score in a game, what are your friends up to, looking for how-tos, or guidance on the best restaurant. Obviously there is information written to these sites, as well, like when something is updated or changed, so that information will be sent to a permanent data store.</p>
<p>Now we’ve discussed when ElastiCache is a great solution, but when is it best to consider using something else? ElastiCache should never be used to store your only version of data records, since a cache is designed to be a temporary data store. So when data persistence is necessary, like when working with primary data records, or when we need write performance rather than read performance, a persistent data store should be used instead of an ElastiCache.</p>
<p>Amazon ElastiCache can be a very useful tool for developers. Application developers are always looking for ways to ensure the best performance and availability for their application. Databases and other permanent storage options are relatively slow and users are impatient today. Waiting a few more seconds for an app to respond can mean losing business to a competitor. Developers are thrilled when an app becomes popular, but then there is a challenge of scaling. Adding front end support like more web servers is fairly simple, but when it comes to scaling persistent storage, it starts getting complicated and messy. Then there are a number of management tasks to consider. Updating, patching, monitoring, and securing data takes time. Using ElastiCache allows the developer to focus on building and improving apps by simplifying and reducing administrative tasks.</p>
<h1 id="DEMO-Creating-an-ElastiCache-Cluster"><a href="#DEMO-Creating-an-ElastiCache-Cluster" class="headerlink" title="DEMO: Creating an ElastiCache Cluster"></a>DEMO: Creating an ElastiCache Cluster</h1><p>Hello and welcome to this lecture, which is going to be a demonstration on how to create an Amazon ElastiCache cluster. So let’s get straight into it. So I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console, and you can find ElastiCache under the database category down here. So if we select on ElastiCache, we’re then presented with this screen if you haven’t got any ElastiCache clusters already, which I haven’t.</p>
<p>So to get started, simply click on the get started now button. And now we’re in the configuration page of creating our Amazon ElastiCache cluster. So the first option we have is to choose our cluster engine, which is either Redis or Memcached.</p>
<p>Now, if you remember back to the lecture we just covered, Redis generally offers a more robust set of features to that of Memcached, whereas Memcached is recognized for its simplicity and speed of performance. And for this demonstration, I’m just gonna stick with the Redis cluster engine. If you want to enable cluster mode, we can do so here through the tick box. With it enabled, we can have up to 90 shards per cluster. For this demonstration, I’m just going to leave that unchecked.</p>
<p>Now, if we go down to the Redis settings, we can enter our name here. Just put the name of demo. Same for the description. Then we have our engine version compatibility, which is essentially the version of Redis that you’ll install on your nodes. Just leave that as the default. The port that Redis will use. This is the port that the node will accept connections from.</p>
<p>Here we have a parameter group, and much like RDS, this just contains configurational settings about your engine. So if you have a different parameter group that you’d like to use for your cluster, then you can select it here. I’m just gonna use the default.</p>
<p>Scroll down to the node type. If we have a look at these, now, these are all the different nodes that you can have within your cluster. So the different sizes offering different memory and network performance, and we have different instance families along the top here, depending on what your requirements are. So I’m just gonna leave that as the default.</p>
<p>You can then select the number of replicas that you’d like to use and also if you’d like to configure multi-availability zone, which is used for high availability, and also as an automatic failover should your primary node fail. For this demonstration, I’m going to uncheck multi-AZ.</p>
<p>Now if we scroll down to the advanced Redis settings, here we can create a subnet group. Now, this subnet group is essentially used to define where your nodes can reside. So let’s create a new subnet group. So I’ll just call this Subnet1. And the same for the description. And then we can select the VPC that we want to use for our ElastiCache cluster.</p>
<p>So let’s just select the top one here, and then we can select which subnets that we want to include in our subnet group. So I’m just going to select two subnets. We can then select our availability zone placement. We can either have no preference, or if we have specific zones that we’d like to place these, to place our nodes in, then we can do so here. I’m gonna select no preference.</p>
<p>Down to security. Down here we can add any security groups which will control access to our clusters, and so you can add any security groups in that you need. If you want to apply encryption at rest, simply tick the box, and you can either use the default encryption key, which is managed by ElastiCache, or if you have your own CMK, then you can select it from here, for example.</p>
<p>So I have my own KMS key, which is my CMK. So if I wanted to, then I can select that own customer managed CMK. For this demonstration, I’m just going to select the default encryption key. And similarly, if you want encryption in transit, then you can do so by enabling the tick box there.</p>
<p>Now, if you want to import any data to your cluster from the creation, then you can select an S3 folder that might contain an RDS backup that will populate your cluster with data. And then we have our backups. If you want to enable automatic backups, simply tick the check box and then select the number of days for your retention period and if you have a backup window and what time that would be if you want one. I’m gonna select no preference.</p>
<p>And then finally, maintenance. If there’s any maintenance to take place, you can specify a maintenance window, again with day and time and duration. For this demonstration, I’m just gonna select no preference, and you can either have a topic for any notifications with regards to your, with regards to any maintenance carried out. So you can select one of your existing SNS topics or disable notifications. Then once you’re happy with the configuration, simply click create. As you can see, the status is creating, and that will just take a couple of minutes to run through.</p>
<p>Okay, so that took a few minutes to come up, but we can now see the status is available. And if we click on the name of the cluster, then we can see a number of different nodes that have been created. We have the primary and two replicas. If we look at the description, then we can see all of the information about this cluster. For example, the primary endpoint, the reader endpoint, the engine that it’s running, the availability zones, how many nodes, etc, and also the security groups and parameter groups, et cetera. Basically all the configuration information that we went through in its creation. And that’s it. And that’s how you set up an ElastiCache cluster.</p>
<h1 id="Amazon-Neptune"><a href="#Amazon-Neptune" class="headerlink" title="Amazon Neptune"></a>Amazon Neptune</h1><p>Hello, and welcome to this lecture on Amazon Neptune. Amazon Neptune may not be as widely utilized as perhaps <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> RDS or Amazon DynamoDB, simply due to what it is designed for. Amazon Neptune is a fast, reliable, secure, and fully managed graph database service.</p>
<p>For those who are unfamiliar with what a graph database is useful, they are essentially used to help you both store and navigate relationships between highly connected data which could contain billions of separate relationships. As a result, graph databases are ideal if their focus is on being able to identify these relationships of interconnected data, rather than the actual data itself. Trying to perform queries against complex relationships will be very difficult in a normal relational database model. And so graph databases are recommended in this scenario instead.</p>
<p>Before I continue, let me run through some of the use cases to help you understand when and where you might use Amazon Neptune as a graph database to solidify the importance of being able to query complex relationships.</p>
<p>Social networking. Graph databases are a powerful asset when used within a social networking environment. As you can imagine, there are vast webs of tightly networked data that run across social networking platforms, and understanding these relationships and being able to query against them is vital to being able to build and maintain effective social network applications. For example, being able to present the latest feed updates to your end users with relevant news from all the groups that they belong to can be easily generated using graph databases, thanks to the highly scalable and enhanced performance of Amazon Neptune.</p>
<p>Fraud detection. Security should always be a number one priority in any cloud deployment solution, and using Amazon Neptune can help you from a security standpoint, using its high performance capabilities. If you are carrying out financial transactions within your environment, then you can build applications that allow Neptune to analyze the financial relationships of transactions to help you detect potential fortunate activity patterns in near real time response times. For example, you might be able to detect that multiple parties are trying to use the same financial details, all from various different locations.</p>
<p>Recommendation engines. Recommendation engines are widely used across many different websites, largely, eCommerce sites that recommend products based upon your search and purchase history. Using Neptune as a key component within your recommendation engine allows it to perform complex queries based upon various different activities and operations made by the user that will help determine recommendations of what your customer may like to purchase next.</p>
<p>I’ve simply highlighted some of the common scenarios where you might use Amazon Neptune within your solutions. However, there are many, many more use cases available that focus on the relationships between vast amounts of highly interconnected data sets.</p>
<p>Now we have more of an understanding of when and where you might use Amazon Neptune, let’s take a look at some of its components.</p>
<p>Amazon Neptune uses its own graph database engine and supports two graph query frameworks. These being Apache Tinkerpop Gremlin, and this allows you to query your graph running on your Neptune database, using the Gremlin traversal language. And we have the Worldwide Web Consortium Sparql. The Sparql query language has been designed to work with the internet and can be used to run queries against your Neptune database graph.</p>
<p>When creating your Amazon Neptune database, you must create a name for your Neptune database cluster, but what is a cluster?</p>
<p>An Amazon Neptune database cluster is comprised of a single, or if required, multiple database instances across different availability zones, in addition to a virtual database cluster volume which contains the data across all instances within the cluster. The single cluster volume consists of a number of Solid State Discs, SSDs. As your graph database grows, your shared volume will automatically scale an increase in size as required to a maximum of 64 terabytes.</p>
<p>To ensure high availability is factored into Neptune, each cluster maintains a separate copy of the shared volume in at least three different availability zones. This provides a high level of durability to the data. </p>
<p>From a storage perspective, Amazon Neptune has another great feature to help with the durability and reliability of data being stored across your shared cluster, this being Neptune Storage Auto-Repair.</p>
<p>Storage Auto-Repair will automatically find and detect any segment failures that are present in the SSDs that make up the shared volume, and then automatically repair that segment using the data from the other volumes in the cluster. This ensures that the data loss is minimized and the need to restore from a failure is drastically reduced.</p>
<p>Similarly to other AWS database services, Amazon Neptune also has the capability to implement and run replica instances. If replicas are used, then each Neptune cluster will contain a primary database instance, which will be responsible for any read and write operations. The Neptune replicas, however, are used to scale your read operations, and so support read-only operations to the same cluster volume that the primary database instance connects to. As the replicas connect to the same source data as the primary, any read query results served by the replicas have minimal lag, typically less than a 100 milliseconds after new data has been written to the volume.</p>
<p>A maximum limit of 15 replicas per crust exists which can span multiple availability zones. And this ensures that should have failure occur in the availability zone hosting the primary database, one of the Neptune read replicas in a different AZ will be promoted to the primary database instance, and adopt both read and write operations. This process usually takes about 30 seconds.</p>
<p>Data is synchronized between the primary database instance and each replica synchronously. And in addition to providing a failover to your primary database instance, they offer support to read only queries. These queries can be served by your replicas, instead of utilizing resources on your primary instance. When you have created your Amazon Neptune database, you need to understand how to connect to it, and this is achieved through endpoints. An endpoint is simply a URL address and a port that points to your components. There are three different types of Amazon Neptune endpoints, these being cluster endpoint, reader endpoint, and instance endpoint.</p>
<p>Let me take a quick look at each of these endpoints individually, starting with a cluster endpoint. For every Neptune database cluster that you create, you will have a cluster endpoint, and this points directly to the current primary database instance of that cluster. This endpoint should be used by applications that required both read and write access to the database. Earlier, I explained that if a primary instant fails and you have a read replica available, then Neptune will automatically failover to one of these replicas and act as the primary, providing read and write access. When this happens, the cluster endpoint will then point to the new primary instance without any changes required by your applications accessing the database.</p>
<p>Reader endpoints. As you might expect by the name, this endpoint is purely used to connect to any read replicas you might have configured. This is used to allow applications to access your database on a read only basis for queries. Only a single reader endpoint exists, even if you have multiple read replicas. Connection served by the read replicas will be performed on a round-robin basis, and it’s important to point out that the endpoint does not load balance your traffic in any way across the available replicas in your cluster.</p>
<p>Instance endpoints. For every instance within your cluster, including your primary and read replica instances, they will each have their own unique instance endpoint that will point to itself. This allows you to direct certain traffic to specific instances within the cluster. You might want to do this for load balancing reasons across your applications reading from your replicas.</p>
<h1 id="DEMO-Creating-an-Amazon-Neptune-Database"><a href="#DEMO-Creating-an-Amazon-Neptune-Database" class="headerlink" title="DEMO: Creating an Amazon Neptune Database"></a>DEMO: Creating an Amazon Neptune Database</h1><p>Hello, and welcome to this lecture, which is going to be a demonstration on how to create an Amazon Neptune database. So, I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Management Console, and we need to go down to Databases and then select Amazon Neptune. If you don’t have any Neptune databases created as yet, then you’ll be presented with this screen. Simply launch Amazon Neptune. And now we’re down to the configuration screen of our database.</p>
<p>Now, the first option we have is the engine options. So this is essentially the version of Neptune that you can use. Just going to select the latest one, which is the default. Then down to Settings, this is the database cluster identifier. So this will be the name of our database cluster. I’m just gonna leave it as a default as database-1 for this demonstration. But just be aware that the name must be unique within the region for all your database clusters that you own.</p>
<p>Next, we have templates. So we have either production or development and testing. The production one uses defaults for high availability and fast, consistent performance, whereas the dev and test won’t have as many high availability features. So, let’s go with the production template.</p>
<p>We can then select our database instance size. So we have a number of different instances here, all offering different quantities of VCPUs and memory, etc. So I’m just going to select the smallest one for this demonstration.</p>
<p>Down to Availability &amp; durability, we can either create a multi-AZ deployment of Amazon Neptune, or have a single availability zone deployment. If you want high availability, then you’d create the read replica in different zones. For this demonstration, I’m just going to select no to multi-AZ.</p>
<p>When we come down to connectivity, we can select our VPC that we want to deploy the Neptune database in, so select your VPC. If we expand this additional connectivity configuration, we can then select a subnet group which will define which subnets within your VPC that Amazon Neptune can use. In this VPC, I only have the default subnet group. Then we have our VPC security group which acts as a virtual firewall, which will define what traffic can talk to your database and over which ports as well. So you can select an existing security group, or you can create a new security group. I’m just going to select the default security group just for this demonstration.</p>
<p>As we can see, it’s added it in there. You can then decide which zone you’d like to place your database in, if you have a preference, that is, or you can select no preference. And also the port that it will use for application connections. If you’d like to, you can add a tag for your database.</p>
<p>And then finally, if we look at additional configuration, we have a number of database options, so we could provide a name for the actual instance of your database. Again, we have parameter groups that we’ve discussed in the previous lectures, and we have a parameter group for the actual cluster itself, and also for the individual database.</p>
<p>For authentication, you can enable IAM database authentication as well, which will manage access through users and roles. For security purposes, I recommend enabling that. You can define a failover priority, and the failover priority allows you to define on your Neptune replicas which one should be promoted to be prime instance should your prime instance fail. And the priorities range from zero for the highest priority to 15 to the lowest priority. And you can configure each replica with a different priority. I’ll just leave that as a default, no preference.</p>
<p>Down to Backup, these are our automatic backups. And again, we can choose our attention period, which is essentially the number of days that it should keep its automated backups. Encryption is enabled by default through KMS, and here it’s using the AWS RDS KMS key as its default key, which is fine. But if you’d like to select a different one, then you can select any CMKs that you have created. As we can see here, we have one I’ve created called MyCMK. But I’ll just leave it as the default AWS managed KMS key. If you’d like to export your logs, then you can tick the audit log and have that exported to CloudWatch Logs for further analysis.</p>
<p>When it comes to maintenance, you can enable auto minor version upgrade, and this will automatically upgrade to any new minor versions as they are released. And again, we have a maintenance window. So you can select a predefined window where you’d like any maintenance to be scheduled, or select no preference. And finally, you can enable deletion protection. And this simply stops anyone from going ahead and accidentally deleting your database. If you want to delete the database, then you have to modify the settings, uncheck that check box, and then you can delete the database. So like I say, it prevents any accidental deletion, you have to do it with intent. Just gonna leave that unchecked. And then once you’ve set all your settings, simply select Create database.</p>
<p>As we can see, it’s creating our database. If we went for the multi-AZ option, then we’d have another instance under this cluster as well. Now, that will just take a few minutes to start up and create. We can now see that the cluster is available, but it’s still creating our database instance. We can now see the availability zone that it’s placed that instance in, so it’s eu-west-1b. Okay, we can now see that the database instance is also available.</p>
<p>So let’s just take a quick look at these. So firstly, the cluster. As we can see here, we can see the connectivity of the cluster, so we have the cluster end point that I mentioned in the previous lecture, and also the reader end point as well.</p>
<p>For the monitoring, we can see some of the CloudWatch metrics that have been used. And then if we look at logs and events, we can see any logs that are being generated. Configuration is essentially the different options that we selected during the creation of the cluster. Similarly with the maintenance and backups, any maintenance or backups that are scheduled. And also our tags. And it’s a similar story for the actual instance itself. So let’s take a look at that.</p>
<p>We have the same options, connectivity and security, so we can see which VPC it resides in, the subnet group, and the subnets associated with that subnet group. We have the end point. Again, different CloudWatch metrics that are being captured. Any logs and events. Configuration, again, this is a lot of the configuration options that we defined during its creation, such as the KMS key and the instance size, etc, and any maintenance windows that have been scheduled.</p>
<p>So it’s as simple as that, it’s very easy to set up, and many of the configuration options are similar to the previous databases that we’ve also set up within this course.</p>
<h1 id="Amazon-Redshift"><a href="#Amazon-Redshift" class="headerlink" title="Amazon Redshift"></a>Amazon Redshift</h1><p>Hello, and welcome to this lecture where I will look at Amazon Redshift. Amazon Redshift is a fast, fully-managed, petabyte-scale data warehouse. And it’s designed for high performance and analysis of information capable of storing and processing petabytes of data and provide access to this data, using your existing business intelligence tools, using standard SQL. It operates as a relational database management system, and therefore is compatible with other RDBMS applications. Redshift itself is based upon PostgreSQL 8.0.2, but it contains a number of differences from PostgreSQL. These differences are out of scope for this course, but for more information, please refer to the documentation <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">here</a>.</p>
<p>A data warehouse is used to consolidate data from multiple sources to allow you to run business intelligent tools, across your data, to help you identify actionable business information, which can then be used to direct and drive your organization to make effective data-driven decisions to the benefit of your company.</p>
<p>As a result, using a data warehouse is a very effective way to manage your reporting and data analysis at scale. A data warehouse, by its very nature, needs to be able to store huge amounts of data and its data may be subjected to different data operations such as data cleansing, which as an example, may identify, correct, replace or remove incomplete records from a table or recordset.</p>
<p>This can be expanded upon for the need to perform an extract, transform and load or an ETL job. This is the common paradigm by which data from multiple systems is combined to a single database data store or warehouse for legacy storage or analytics.</p>
<p>Extraction is the process of retrieving data from one or more sources. Either online, brick &amp; mortar, legacy data, Salesforce data and many others. After retrieving the data, ETL is to compute work that loads it into a staging area and prepares it for the next phase.</p>
<p>Transformation is the process of mapping, reformatting, conforming, adding meaning and more to prepare the data in a way that is more easily consumed. One example of this is the transformation and computation where currency amounts are converted from US dollars to euros.</p>
<p>Loading involves successfully inserting the transform data into the target database data store, or in this case, a data warehouse. All of this work is processed in what the business intelligent developers call an ETL job.</p>
<p>Now we have an understanding of what Amazon Redshift is. Let’s move on to looking at the architecture of the service and the components that is built upon.</p>
<p>Let me start with clusters and nodes. A cluster can be considered the main or core component of the Amazon Redshift service. And in every cluster, it will run its own Redshift engine, which will contain at least one database. As the name implies, a cluster is effectively a grouping of another component, and these being compute nodes.</p>
<p>Each will contain at least one compute node. However, if the cluster is provisioned with more than one compute node, then Amazon Redshift will add another component called a leader node.</p>
<p>Compute nodes all contain their own quantity of CPU attached storage and memory. And there are different nodes that offer different performances. For example, the following RA3 node types. Also, as you can see here, the dense compute node types.</p>
<p>The leader node of the cluster has the role of coordinating communication between your compute nodes in your cluster and your external applications accessing your Redshift data warehouse. So the leader node is essentially gateway into your cluster from your applications. When external applications are querying the data in your warehouse, the leader node will create execution plans, containing code to return the required results from the database.</p>
<p>If the query from the external application references tables associated with the compute nodes, then this code is then distributed to the compute nodes in the cluster to obtain the required data, which is then sent back to the leader node. If the query does not reference tables stored on the compute nodes, then the query will run on the leader node only.</p>
<p>Each compute node itself is also split into slices, known as node slices. A node slice is simply a partition of a compute node where the nodes memory and disk spaces split. Each node slice then processes operations given by the leader node where parallel operations can then be performed across all slices and all nodes at once for the same query. As I mentioned previously, compute nodes can have different capacities and these capacities determine how many slices each compute node can be split into.</p>
<p>When creating a table, it is possible to distribute rows of that table across different nodes slices based upon how the distribution case is defined for the table. For a deeper understanding on how to select the best distribution style, please see the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">here</a>.</p>
<p>When your Amazon Redshift database is created, you will of course connect to it using your applications. Typically these applications will be your analytic and business intelligence tools, that you’re running with your organization. Communication between your BI applications and Redshift, will use industry standard open database connectivity, ODBC. And Java database conductivity, JDBC drivers for PostgreSQL.</p>
<p>The performance that Amazon Redshift can generate is a huge benefit to many organizations. In fact, at the time of writing this course, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> currently boasts that it’s three times faster than other cloud data warehouses.</p>
<p>From a query perspective, Amazon Redshift has a number of features to return results quickly and effectively. Let’s take a look at a few of them.</p>
<p>Firstly, massively parallel processing. As highlighted in the previous section by associating rows from tables across different nodes slices and nodes. It allows the node leader to generate execution plans, to distribute crews from external applications across multiple compute nodes at once, allowing them to work together to generate the end result, which is an aggregated by the leader node.</p>
<p>Columnar data storage. This is used as a way of reducing the number of times the database has to perform disk I&#x2F;O, which helps to enhance query performance. Reducing the data retrievals from the disk means there is more memory capacity to carry out in memory processing of the query results. Result caching. Caching in general is a great way to implement a level of optimization.</p>
<p>Result caching helps to reduce the time it takes to carry out queries by caching some results of the queries in the memory of the leader node in a cluster. As a result, when a query is submitted, the leader node will check its own cache copy of the results and if a successful match is found, the cached results are used instead of executing another query on your Redshift cluster.</p>
<p>Amazon Redshift also integrates with Amazon CloudWatch, allowing you to monitor the performance of your physical resources, such as CPU utilization and throughput. In addition to this, Redshift also generates query and load performance data that enables you to track overall database performance. Any data relating to query and load performance is only accessible from within the Redshift console itself and not Amazon CloudWatch.</p>
<p>During the creation of your Redshift cluster, you can as an optional element, select up to 10 different IAM roles to associate with your cluster. This allows you to grant the Amazon Redshift principle, redshift.amazonaws.com access to other services on your behalf, for example, Amazon S3 where you might have a data lake. Accessing data within S3 will require a set of credentials to authorize Redshift access to S3. And the best way to do that is by using an IAM role. Therefore, if you intend to perform actions such as this when using your Amazon Redshift cluster, you might need to consider which access you need and what roles you will need to create.</p>
<p>To learn more about IAM and roles, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">here</a>. In the next lecture, I want to show you how to create a new Redshift cluster.</p>
<h1 id="DEMO-Creating-an-Amazon-Redshift-Cluster"><a href="#DEMO-Creating-an-Amazon-Redshift-Cluster" class="headerlink" title="DEMO: Creating an Amazon Redshift Cluster"></a>DEMO: Creating an Amazon Redshift Cluster</h1><p>Hello and welcome to this lecture. This is going to be a quick demonstration on how to set up an Amazon Redshift cluster. So as you can see, I’m in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console at the moment, and to find Redshift, we can scroll down to the database category and we can see Amazon Redshift here. So if we select on that, we’re then taken to this splash screen here. So I don’t have any Redshift clusters at the moment.</p>
<p>So to start with, all I need to do is to click on the orange create cluster button, and this takes us to the configuration page. And the first thing we need to do is give it a name. So I’m just going to call this my cluster. Then we can choose our node type. So we have the RA3 nodes and the dense compute nodes here. Now, AWS recommends the RA3 nodes due to the high performance and the managed storage aspect, or you have the dense compute nodes here.</p>
<p>Now, for this demonstration, I’m just going to select a dense compute node. Now, if you scroll down, we can select the number of nodes that we would like. As you can see, it ranges from one to 32, so we can scroll up or down. And also if you see in this configuration summary section the cost per month, and the total compressed storage will also change with the amount of nodes that I select. So there you can see it is scrolling up and down. I’m just going to leave it as two nodes.</p>
<p>Now I can scroll down to the database configurations. We can give the database a name, and also the port that it’s going to be using, and also a master username and password. So let me just enter a password. And then we have cluster permissions. Now, this is an optional step. So if you want your AWS Redshift cluster to interact with other AWS services on your behalf, for example, maybe Amazon S3, you might want to import data, then you can associate an IAM role that has access to S3 to allow that process to happen. But as I said, this is an optional component.</p>
<p>Now, at the very bottom here, we have additional configuration. Now, these are the default settings. So we have a default network, default backup options, maintenance, default security groups, and also a parameter group, as well. But if you turn off those default settings, then you can go through and modify any of those components. For example, network and security. You can select the VPC for it to run in. You can select the security groups that are associated with your clusters to define what resources can access it. You can also define a subnet group which defines what subnets that the clusters will be launched in and also any availability zones. You can also specify if you want any cluster traffic to purely route through your VPC and if you want your cluster to be publicly accessible or not. So there’s a few network and security features that you can change there.</p>
<p>Looking at database configurations, here you can select a parameter group if you have any configured, and you can also configure any encryption using AWS KMS, and if you want to use the default Redshift key, or if you want to use one of your own CMKs, for example, I have a CMK here in my account. For this demonstration, I’m just gonna disable encryption.</p>
<p>Under maintenance, you can set a maintenance window so that the day and time of the week that any maintenance will be carried out to your cluster. And also you can specify which cluster version you’d like, and you have three options. Either use the most current approved cluster version, use the cluster version before the current version, or use the cluster version with beta releases of new versions. I’ll just leave that as current.</p>
<p>Under monitoring, you can have CloudWatch alarms. So for example, you can create a new alarm for disk usage threshold when that reaches 80%, and then you can notify people via an SNS topic that you might already have configured. I’ll say no alarms. And finally, backup. And also you can specify your snapshot retention, which is how long you’ll keep the backups for. And finally, if you want to configure cross-region snapshot, you can either enable that or disable it. And this will back up your cluster to a different region. So if you enable it, you can then select an alternate region to where your cluster currently resides. I’m just going to disable that.</p>
<p>So there are the different options that are available, but I’m just going to select the defaults that it already suggested. And then once you’re happy with your settings, simply click create cluster. As we can see here now, it’s now creating our cluster. This might take a few minutes, so I’ll come back when that’s done.</p>
<p>Okay, as you can see, the cluster is now available. If we select the dashboard, then we can see that we have one new cluster in the Ireland region with two nodes, and we can see that it’s already taken an automated snapshot, as well.</p>
<p>So cluster overview here, so we can see a number of queries, any database connections, disk space used, CPU utilization. As you can see, there’s not much going on at the moment. We’ve simply just created it. If we have any alarms, and down here, any events, and also a query overview here. So I won’t go into any more detail than that.</p>
<p>This is just a very high-level, quick introduction on how to create an Amazon Redshift cluster. And that’s it.</p>
<h1 id="Amazon-DocumentDB-With-MongoDB-Compatibility"><a href="#Amazon-DocumentDB-With-MongoDB-Compatibility" class="headerlink" title="Amazon DocumentDB (With MongoDB Compatibility)"></a>Amazon DocumentDB (With MongoDB Compatibility)</h1><p>Hello and welcome to this lecture looking at Amazon DocumentDB. This <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/working-with-aws-databases-274/">database service</a> runs in a Virtual Private Cloud and is a non-relational fully managed service, which again is highly scalable, very fast, and much like many other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services conforms to levels maintaining high availability. It will be of no surprise, as the name implies, that DocumentDB is a document database, which provides the ability to quickly and easily store any JSON-like document data which can then be queried and indexed. Indexing enhances the speed of retrieving data within a database thanks to an indexing data structure stored within the database.</p>
<p>The ability to scale within AWS is important across all services, and DocumentDB has the ability to scale both its compute and storage separately from each other. This decoupled approach creates a flexible scaling pattern allowing you to scale the resource as when you need to.</p>
<p>As your database grows in size, Amazon DocumentDB will automatically increase the size of your storage by 10G each time, up to a maximum of 64TB to ensure that you do not run out of storage space.</p>
<p>Amazon DocumentDB has full compatibility with MongoDB, which again is another document database, meaning that if required, you can easily migrate any existing MongoDB databases you might have into Amazon DocumentDB using the Database Migration Service . With this compatibility with MongoDB, it means you don’t have to update any of your code in your applications or modify any toolsets that you are using, making this a simple transition in Amazon DocumentDB if you decide to migrate your database.</p>
<p>The AWS Database Migration Service allows you to connect to a source database, read the source data, format the data for consumption by a target database, and then load the data into that target database. The AWS Database Migration Service can migrate your data to and from commercial and open-source databases.</p>
<p>For more information on the AWS Database migration service, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-the-aws-database-migration-service/dms-introduction/">here</a>.</p>
<p>Let me now explain the architecture of Amazon DocumentDB. Firstly, I want to cover the base architecture of the service, and If you are familiar with Amazon Neptune, then architecturally Amazon DocumentDB is similar in many ways.</p>
<p>The database itself is comprised of a core component, a cluster, and this cluster is composed of a single or multiple DB instances, up to 16 in total, which can span across different availability zones within a single region. Across this cluster is a shared cluster storage volume that supports every instance within the cluster, meaning that every instance sees the same storage volume.</p>
<p>The instances backing the DocumentDB cluster provide the processing power to support read and write requests against the cluster storage volume, and as I mentioned you can have up to 16 DB instances. There will only ever be a single primary DB instance performing write operations in the cluster at any one time. The remaining instances within the cluster if they have been configured will all be read replica instances, serving only read requests.</p>
<p>Read replicas, as we have seen in previous AWS database services in this course series helps to reduce the load on the primary database instances by processing read requests from clients. As a result, DocumentDB is able to process a very high-volume of these kinds of requests.</p>
<p>DocumentDB supports up to 15 read replicas across different availability zones within the same region, much like Amazon Neptune, and also shares the same underlying storage of the primary instance across a shared volume.</p>
<p>The Primary DocumentDB instance will be responsible for both read and write operations. However, the replicas will only process read requests to the cluster volume. As the replicas connect to the same source data as the primary, any read query results served by the replicas have minimal lag, typically down to single-digit milliseconds. Data is synchronized is maintained synchronously between both the primary DB instance and each replica in the region.</p>
<p>DocumentDB uses a principal of endpoints to connect to different components of your database. An endpoint is a URL address with an identified port that points to your infrastructure. There are three different types of DocumentDB endpoints, these being: Cluster endpoint, Reader endpoint, and Instance endpoint.</p>
<p>Cluster Endpoints: Each DocumentDB database will have a cluster endpoint that is associated with the current primary DB instance of the cluster. This endpoint should be used by any applications that require both read and write access to the database. Should a failure of your primary DB instance occur, then DocumentDB will promote either a read replica to the primary DB or if no read replica is configured, DocumentDB will create a new primary instance. When this happens, the cluster endpoint will then point to the new primary instance without any changes required by your applications accessing the database.</p>
<p>Reader Endpoints: A Reader endpoint allows connectivity to any read replicas that you have configured within the region. Applications can use this endpoint to access your database for read requests, typically when performing a query. Only a single reader endpoint will exist, even if you have multiple read replicas. as a result, DocumentDB will manage the forwarding of any read requests onto a specific read replica associated with the primary DB.</p>
<p>Instance endpoints: For every instance within your cluster, including your primary and read replica instances, they will each have their own unique instance endpoint that will point to its own host. This allows you to direct certain traffic to specific instances within the cluster, you might want to do this for load-balancing reasons across your applications reading from your replicas.</p>
<p>DocumentDB performs automatic backups for you based upon a schedule created during the creation of your database. A feature of these automatic backups allows you to restore back to any point in time during your retention period, known as point-in-time-recovery. As a part of the automated daily backup process, DocumentDB captures any transaction logs that have been created as and when updates to your Database were made, this is to ensure it can perform a point-in-time-recovery. These backups are automatically stored on Amazon S3 for durability and availability.</p>
<p>The automated backups themselves are performed daily, and the backup retention period determines how long DocumentDB will keep and maintain your backups for and can be set between 0 and 35 days. For automatic backups to take place, the retention period must be set to at least one day. If the retention is set to 0 then automatic backups will not take place and you will not be able to perform point-in-time restores. If you did have it configured, then your point-in-time restores can take place for any duration within the retention period.</p>
<p>The backup window allows you to define a time period in which the backup snapshot could be taken. This allows you to set it at a time when the database itself will be of low utilization to prevent the backup process impacting the performance of the database itself.</p>
<p>In the next lecture, I will be looking at how to create a new Document DBCluster, which will cover these backup settings in addition to further configurational changes, so let’s move on and take a look.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="DEMO-Creating-an-Amazon-DocumentDB-Cluster"><a href="#DEMO-Creating-an-Amazon-DocumentDB-Cluster" class="headerlink" title="DEMO: Creating an Amazon DocumentDB Cluster"></a>DEMO: Creating an Amazon DocumentDB Cluster</h1><p>Hello and welcome to this lecture. We’re all going to be showing you how to set up an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> document DB database. From the management console, we need to select Amazon DocumentDB from the Database category.</p>
<p>Now as you can see I don’t have any clusters set up at the moment and it’s very simple to get going to create one. So let me go ahead up to create and the first thing I need to do is to enter a cluster identifier. So this is just a unique cluster name so I’ll just call this MyCluster. And then we can select our instance class. So we have a number or different instances down here all different sizes with different number of VCPUs and RAM quantity. I’m just gonna select the smallest. Then we can have the number of instances. Anything from one to 16. So there’ll be one primary and the others will be replicas. Let’s just leave that as three. </p>
<p>Then we need to enter a master username and password so we just add that in now. And it gives us an estimated hourly cost for the instance size that we selected. So here we can see that it’s 92 cents per hour. If we select down here show advanced settings then we have more configuration options that we can change. So we can select which VPC that we would like it in. We can select the subnet group to define which subnets the instances will be launched in. And also associate any VPC security groups for those instances as well.</p>
<p>If we go down to cluster options we can specify the port and if we have any parameter groups configured we can select parameter group there. Over here you can see parameter groups where you can specify a template of pre-configured options for your database as explained previously. We can look at encryption-at-rest. So we can enable encryption using a default master key, or we can select one of our pre-configured CMKs that we have with KMS. And we can see the account and KMS key ID that’s being used. So we can have that enabled or disable the encryption. I’ll leave that enabled and use the default master key.</p>
<p>If we look at the backup options we can specify retention period. Anything up to 35 days. And also the backup window as well which is the daily time range in which automated backups will be created. We can also export different logs over to Amazon CloudWatch and this will use the predefined IAM role.</p>
<p>For maintenance, we can specify a specific maintenance window to start at a specific day and time and duration. If you have any preference simply select no preference. Again you can add any tags that you’d like for your cluster and enable deletion protection which prevents the cluster from being accidentally deleted. Once you’re happy with your configure options simply click create cluster. As we can see the status is currently creating. That will just take a couple of minutes so I’ll come back when that is active.</p>
<p>Okay, you can now see the status is available. We have three instances. If we take a look at the cluster we can see the engine version. We have some details here on how to connect the database and we have our security group here.</p>
<p>If we look at configuration we can see the ARN details of the database. We can see when it was created, the cluster endpoints, and the reader endpoints, and many other relevance of configuration that we defined during the creation.</p>
<p>If we look at monitoring we can see some CloudWatch statistics, but as we’ve only just created this database, there’s not enough data available, and finally any events and tags that have been detected. If we go to instances we can see the three instances that have been created for my cluster. As you can see these instances are still being created even if a cluster has been created. So that will just take a few more minutes.</p>
<p>I’ve just waited a few minutes and we can see that we now have two of the instances available and it’s just creating the last one. So as you can see it’s a fairly simple process and follows a lot of the previous databases that we’ve looked at in its general configuration set up. We created the cluster and also as part of the process, created three different instances. One as a writer which is the primary, and then two reader replicas.</p>
<h3 id="Lectures-1"><a href="#Lectures-1" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="Amazon-Keyspaces-for-Apache-Cassandra"><a href="#Amazon-Keyspaces-for-Apache-Cassandra" class="headerlink" title="Amazon Keyspaces (for Apache Cassandra)"></a>Amazon Keyspaces (for Apache Cassandra)</h1><p>Hello, and welcome to this short lecture, in which we’ll look into the final <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/working-with-aws-databases-274/">database service</a> of this course series, Amazon Keyspaces for Apache Cassandra. Firstly, let’s answer the question that some people ask when seeing this service. What is Apache Cassandra? To summarize it quickly, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Apache_Cassandra">Wikipedia</a> explains that “Apache Cassandra is a free, open-source, distributed, wide column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure.” </p>
<p>So now we have a high-level awareness of Amazon Cassandra. Let’s see how Amazon Keyspaces fits into this. Keyspaces is a serverless, fully-managed service designed to be highly scalable, highly available, and importantly, compatible with Apache Cassandra, meaning you can use all the same tools and code as you do normally with your existing Apache Cassandra databases.</p>
<p>Being a serverless service. It removes the need for you to provision, patch, and manage instances yourself. Instead, all of this is taken care of by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> on your behalf. Boasting unlimited throughput, Amazon Keyspaces is designed for massive scale solutions, allowing you to service business-critical workloads requiring thousands of requests per second. The key features of Amazon Keyspaces is that it can offer extreme performance, scalability, and elasticity, and grows at the rate of demand for your applications, ensuring you only pay for what you use.</p>
<p>Traditionally, Cassandra architectures are comprised of a cluster of nodes, which have to be created, provisioned, managed, patched, and backed up by you. As your Cassandra database grows, so does the amount of nodes, leading to greater administrative resources in managing the infrastructure. Using Amazon Keyspaces removes the need for you to manage this infrastructure, and instead you can focus on the business logic of the database and your applications that interact with it to ensure you are getting the best performance possible.</p>
<p>Amazon Keyspaces is a great choice if you’re looking to build applications where low latency is essential, for example, route optimization applications or trade monitoring. And of course, if you’re looking for an easier way of managing your existing Cassandra databases prices in the cloud without the burden of maintaining your own infrastructure.</p>
<p>To help understand the service in greater detail, let’s look at some of the components of the service.</p>
<p>First let me explain the difference between keyspaces and tables. In Cassandra, a keyspace is essentially a grouping of tables that are related and are used by your applications to read and write data. Also, the keyspace in Cassandra also helps to define how your tables are replicated across multiple nodes in the cluster. However, because Amazon Keyspaces is fully managed and serverless, the entire storage layer is abstracted from being administered and configured by us as customers. Instead, it is managed by AWS. And so here, the keyspace component in Amazon Keyspaces exist in their logical meaning rather than holding the responsibility for us to manage any kind of replication.</p>
<p>Tables are where your database writes are stored, effectively, the data that is held within your database. In each table, there will be a primary key that consists of a partition key and one or more columns. When a new table is created, encryption at rest is automatically enabled, and any clients that want to connect to your tables will require a transport layer security connection for encrypted in transit connectivity.</p>
<p>In the next lecture, I will show you how to set up a keyspace and then a table that will reside within that keyspace. Much like Amazon DynamoDB, Keyspaces offers two different throughput capacity modes when working with your read and writes to and from your tables. These options allow you to customize how your throughput is managed, helping you to optimize it for your workloads.</p>
<p>The options available are on-demand and provisioned. On-demand throughput capacity is a default option when creating your tables and is capable of processing thousands of requests per second. The pricing for this option is based upon the number of read and writes made against your tables by your applications, meaning you only pay for what you’re using.</p>
<p>As your workload fluctuates, it is able to scale to any increased throughput that the database has previously reached instantaneously. However, if additional throughput is required above and beyond existing thresholds, then Amazon Keyspaces works quickly to respond to meet the needs required by your applications.</p>
<p>As a result, this can be a good selection for your throughput if you’re dealing with unknown or unpredictable workloads.</p>
<p>Provisioned throughput capacity is a better choice for you if you are dealing with more predictable workloads, which allows you to specify your predicted number of reads and writes per second, which would enable your tables to meet those throughput speeds faster than on-demand would. You can also use automatic scaling to alter the change of throughput if you experience fluctuation, or as your database naturally grows, using upper and lower the thresholds.</p>
<p>When working with Amazon Keyspaces, you’ll need to use CQL, the Cassandra Query Language, which is the language you use to communicate with your Amazon Keyspaces. In many respects, it is similar to SQL, structured query language. And as a result, this helps to reduce the learning curve when moving from a relational database using SQL, such as MySQL.</p>
<p>There are a number of ways to run queries using CQL. Firstly, from within the Amazon Keyspaces dashboard within the AWS management console, you can use the CQL editor, which can return as many as a thousand records per query. If you are querying more than a thousand records, then you will need to run multiple queries together. You can run them on a CQLSH client, and more information on this can be found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.cqlsh.html">here</a>, or you can run them programmatically using an Apache 2 licensed Cassandra client driver (more info <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.html">here</a>).</p>
<p>In the next lecture, I’ll be demonstrating how to create a keyspace and then a table within that keyspace, so let’s take a look.</p>
<h3 id="Lectures-2"><a href="#Lectures-2" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="DEMO-Creating-a-Keyspace-and-Table-in-Amazon-Keyspaces-for-Apache-Cassandra"><a href="#DEMO-Creating-a-Keyspace-and-Table-in-Amazon-Keyspaces-for-Apache-Cassandra" class="headerlink" title="DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)"></a>DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</h1><p>Hello and welcome to this lecture, which will be a demonstration on how to create an Amazon Keyspaces database. And what I shall do, I shall create a keyspace and then a table that resides in that keyspace. So let’s take a look.</p>
<p>So firstly, scroll down to the database category and then select Amazon keyspaces. As you can see, I don’t have any keyspaces set up at the moment so I just need to click get started. Now much like in the QLDB demonstration, Amazon keyspaces also has a quick getting started guide that gives you a quick tutorial on how to create a keyspace, then how to create a table in your keyspace and then how to populate it with data and query your data.</p>
<p>As this is a fundamental level course, we’re just going to create the keyspace and then create a table. Now I’m not going to use the getting started guide but if you’d like to explore this service further, then I recommend you going through those four steps.</p>
<p>Okay, so to create a keyspace we can select keyspaces on the left side here. Now remember a keyspace is essentially going to be a grouping of tables, that will be used by your applications to read all my data to. And it also defines how the replication is managed as well.</p>
<p>So from here, we simply click create keyspace. Now we need to give it a name, I’m just gonna call this; my keyspace. We can add any optional tabs if we’d like to. And this is the query that will be executed to create the keyspace that we specified above. So we can say this is going to create the name of my keyspace and it will also manage the replication across a single region strategy. So <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> will configure this replication for us across multiple availability zones. Click create keyspace and then we can see it’s active. So it’s very easy to create the keyspace.</p>
<p>Now we need to create a table within that keyspace. So if we now select that keyspace, we can see that it doesn’t have any tables. Now the table itself, is where your database rights are stored. So now we need to create a table within the keyspace. So select create table. We can select the keyspace that we want this table to belong to. As we only have one, that’s the my keyspace that we just created.</p>
<p>Now we can enter a table name. I’ll just call this my table. Here, we can enter the schema information. So we can enter any columns that we want, for example; product name and then the type field for that column. As you can see, there’s lots of different types that you can use. For this, I’ll just use the character string. And if we want more than one column, we can add another column. For example; product ID, and this can be a decimal field.</p>
<p>Now we can add our partition key and we can see that a partition key is composed of one or more columns that are used to uniquely identify rows within a table. So we can select the columns that we’d like to act as the partition key. So as you can see, you can add more than one column. We then have clustering columns, which is optional, and this helps you to sort data within a partition.</p>
<p>For this demonstration, I’m not gonna have any clustered columns. Then if you scroll down to the read and write capacity settings, here we have the on-demand and provision modes that I discussed in a previous lecture. So the on-demand option simplifies billing for the actual read and writes that the application performs. Whereas provisioned, you can manage and optimize the reads and writes in advance.</p>
<p>So as you can see for read capacity, we can have this automatically scaling. I can set the minimum capacity units and the maximum capacity units. And when the utilization gets to 70%, it will automatically scale up. Similarly with the write capacity. You can copy the settings that you have above for the read scaling or you can set it manually.</p>
<p>So you can have different scaling patterns for both read and write capacity. I’m just gonna leave this as on-demand capacity mode, just for simplification. And then if we go down even further, we can see our point in time of recovery. We can either enable this or disable. It’s always best to keep this enabled, just to allow you to recover from any incident at any point. Then you can add any optional tags. And finally, this just shows you the query that will be executed to create the table in keyspace, along with the column names and the types and also the primary key. And it also sets your capacity modes and the fact that we have point-in-time recovery enabled.</p>
<p>Once you’re happy with all your options, simply select create table. And that has now created the my table in my keyspace. As you can see, the status is now active and if you select the table, you can see the partition key and your columns as well. And if you need to add any more columns, then you can do so here.</p>
<p>So that was just a very quick overview of how to firstly create a keyspace and then create a table. And finally, remember you can add additional tables to a keyspace. So if we go back into our keyspace, you can then go ahead and create an additional table within the same keyspace. And that’s it for this demonstration.</p>
<h3 id="Lectures-3"><a href="#Lectures-3" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="Amazon-Quantum-Ledger-Database-QLDB"><a href="#Amazon-Quantum-Ledger-Database-QLDB" class="headerlink" title="Amazon Quantum Ledger Database (QLDB)"></a>Amazon Quantum Ledger Database (QLDB)</h1><p>Hello and welcome to this lecture, covering Amazon Quantum Ledger Database, which is also known as Amazon QLDB. This was released in September of 2019.</p>
<p>So to start with, what actually is Amazon QLDB? It’s yet another fully managed and serverless <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/working-with-aws-databases-274/">database service</a>, which has been designed as a ledger database. This has a whole host of use cases. One quick example would be for recording financial data over a period of time. QLDB would allow to maintain a complete history of accounting and transactional data between multiple parties in an immutable, transparent and cryptographic way through the use of the cryptographic algorithm, SHA-256, making it highly secure.</p>
<p>This means you can rest assured that nothing has changed or can be changed through the use of a database journal, which is configured as append-only. Essentially, the immutable transaction log that records all entries in a sequenced manner over time. This service therefore negates the need for an organization to develop and implement their own ledger applications.</p>
<p>This may sound similar to blockchain technology where a ledger is also used. However, in blockchain, that ledger is distributed across multiple hosts in a decentralized environment, whereas QLDB is owned and managed by a central and trusted authority. This removes the requirement of a consensus of everyone across the network, which is required with blockchain.</p>
<p>Often, ledger applications to fulfill these requirements are added to relational databases, and this quickly becomes difficult to manage since they are not immutable, which makes errors difficult to trace, especially during audits.</p>
<p>I mentioned earlier that QLDB is serverless. So again, the administration of having to maintain the underlying infrastructure is removed and all scaling is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, which includes any read and write limitations of the database.</p>
<p>Amazon QLDB is great for scenarios where you can maintain an accurate record of changes requiring the utmost integrity assurance. So to help get an understanding of how QLDB is used across different industries, let me take a look at a couple of examples and use cases for the service.</p>
<p>So QLDB would be a great fit within the insurance industry, which a claim by its nature can be a long winded and extensive process involving many different parties and operations over a long time period. You could implement different processes, systems and applications to track, audit and record the claim history via relational databases and custom auditing mechanisms verifying the validity of the records. However, this could all be replaced with Amazon QLDB. Using an immutable append-only framework, prevents the ability to manipulate previous data entries, which helps to prevent fraudulent activity.</p>
<p>Another use case where QLDB would be a good fit is within the human resources field, including payroll processes. Accuracy and auditing is essential when it comes to employee data, which is often confidential, including payroll information. HR tracks and records an employee’s history covering their performance, benefits, training, remuneration and much more. Having a clearly defined verifiable employment history that can be encrypted, trusted, and reliable containing all elements of the individual held centrally makes Amazon QLDB a great fit.</p>
<p>So we can see that Amazon QLDB is really about maintaining an immutable ledger with cryptographic abilities to enable the verifiable tracking of changes over time. There are many different use cases where this can be used, and I’ve just highlighted a couple here to provide more of an understanding of how this would be used.</p>
<p>Let’s now take a look at some of the concepts and components that make up the service.</p>
<p>Firstly, and as I’ve mentioned a few times already during this lecture, QLDB is based upon a ledger. So let’s look at the structure of a ledger database.</p>
<p>Data for your QLDB database is placed into tables of Amazon ion documents. Now these ion documents have been created internally at Amazon and on open-source, self-describing data serialization format, which is a superset of JSON, JavaScript Object Notation. This means that any JSON document is also classed as a valid Amazon ion document. The document is also allowed store by structured and unstructured data.</p>
<p>So going back to the tables, they all effectively compromise of a group of Amazon ion documents and their revisions. As with most documents, when a revision is made, it usually signifies a change, an update, a replacement. Basically, something changes to the document, making a revision. Now, as we know, QLDB by design maintains an audit history of all changes and so that revision is saved in addition to all previous versions of the same ion document. This journal of transactional changes allows you to easily query document history across all document iterations.</p>
<p>Changes to these documents are done so via database transactions. In doing this transaction, Amazon QLDB will read data from its ledger, perform the update as required, and then save the changes to the journal. To be clear, the journal acts as an append-only transactional log and maintains the source of truth for that document and the entire history of changes to that document, ensuring that it remains immutable.</p>
<p>Each time a change is committed to the journal, a sequence number is added to identify its place in the change history. In addition to this, an SHA-256 bit hash is used for verification purposes, which creates a cryptographic digest file of the journal. Now this identifies an encrypted signature of the changes made to your document and the history of your entire document at that point in time. This can then be used to verify the integrity of the changes made in relation to the digest file created. This helps to ensure that the data within your document has not been altered or changed in any way since it was first written to in QLDB.</p>
<p>For a deeper understanding of how this whole process works, please review the following: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/qldb/latest/developerguide/verification.html">https://docs.aws.amazon.com/qldb/latest/developerguide/verification.html</a></p>
<p>Now we have a base understanding of the ledger itself through the use of tables and documents, including the ledger. Let me now take a look at how storage is used for QLDB.</p>
<p>Amazon QLDB uses two different methods of storage, each for very different uses, these being journal storage and index storage.</p>
<p>Journal storage is the storage that is used to hold the history of changes made within the ledger database. So this will hold all of the immutable changes and history to the ion documents within your table.</p>
<p>Index storage on the other hand is the storage that is used to provision the tables and indexes within the ledger database and it’s optimized for querying.</p>
<p>With QLDB being a fully managed serverless service, this storage is managed for you and there are no specifications to select or make during the creation of your ledger database. In the next lecture, I will show you how simple it is to create a ledger and load some sample data into the database.</p>
<p>The final point I want to cover with Amazon QLDB is, is integration with Amazon Kinesis through the use of QLDB streams.</p>
<p>Amazon Kinesis makes it easy to collect, process, and analyze real-time streaming data so you can get timely insights and react quickly to new information. With Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, your data lakes and data warehouses, or build your own real-time applications using this data. Kinesis enables you to process and analyze data as it arrives and respond in real-time instead of having to wait until all your data is collected before the processing can begin.</p>
<p>Using QLDB streams, you are able to capture all changes that are made to the journal and feed this information into an Amazon Kinesis data stream in near real-time. This allows you to architect solutions whereby other AWS services could process this data from Kinesis to provide additional benefit. For example, this is a great way to implement event-driven architectures. Event-driven architectures are triggered by events that occur within the infrastructure.</p>
<p>So in this case, suppose your ledger contained financial accounts recording transactions and in this instance, an event could be a Lambda function that triggers an SNS notification to an account owner when a finance balance drops below a certain threshold following an update that has been made to the journal.</p>
<h3 id="Lectures-4"><a href="#Lectures-4" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-ledger-using-amazon-qldb/">DEMO: Creating a Ledger using Amazon QLDB</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="DEMO-Creating-a-Ledger-using-Amazon-QLDB"><a href="#DEMO-Creating-a-Ledger-using-Amazon-QLDB" class="headerlink" title="DEMO: Creating a Ledger using Amazon QLDB"></a>DEMO: Creating a Ledger using Amazon QLDB</h1><p>Hello and welcome to this lecture. This is going to be a quick demonstration to show you how to set up an Amazon QLDB Ledger database. It’s very quick and it’s very simple. So let’s take a look.</p>
<p>Firstly if we scroll down to the database category and select Amazon QLDB. Now before I create a new ledger I just want to show you the getting started option. So if you select on getting started on the left here, it comes with a tutorial to help you get set up for your first QLDB database. So it will show you how to create your first ledger, how to load some sample application data, how to query and modify your data, and also verify a document. So that might be something that you want to look into just to have a play with and get used to the database. However, let me show you how to create a ledger.</p>
<p>So if we go back to ledgers, as you can see we have none here. Select create ledger. Let’s give it a name. I’ll just call it MyLedger and then you can add any optional tags if you’d like to. Then simply click on create ledger. And as you can see the status is creating. That’ll only take a few seconds. Okay, now you can see this is now active.</p>
<p>So our ledger is active. If we take a look we can see the ID, the journal size, the index storage size, and the ARN and the region, et cetera. And also in the CloudWatch metrics. But as we’ve only just set this up, there’s no data available at the moment.</p>
<p>Now your ledger is created. You can then start creating tables, etc. Now if we go back to the getting started section under the sample application data you can select your ledger that you just created and load some sample data in there or you can use a manual option to create tables, indexes, etc. So let me just show you that manual option.</p>
<p>So if you select the link it will actually take you to some <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation and it will show you the automatic option where it will pre fill your ledger with some vehicle registration data or you can select the manual option as well. So this will just guide you through on how to create tables using QLDB and populate your database.</p>
<p>So as you can see it’s very easy to get started with QLDB and there’s some great tutorials here for you as well to guide you through exactly how to set it up, run queries, modify documents, et cetera. But for this demonstration, I just wanted to show you the dashboard and how to create that initial ledger which as you can see was a very simple process. Creating the tables and queries and using all the functions of QLDB is out of scope of this fundamentals course.</p>
<h3 id="Lectures-5"><a href="#Lectures-5" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-redshift/">Amazon Redshift</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-redshift-cluster/">DEMO: Creating an Amazon Redshift Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-quantum-ledger-database-qldb/">Amazon Quantum Ledger Database (QLDB)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-documentdb-with-mongodb-compatibility/">Amazon DocumentDB (With MongoDB Compatibility)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-an-amazon-documentdb-cluster/">DEMO: Creating an Amazon DocumentDB Cluster</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/amazon-keyspaces-for-apache-cassandra/">Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/demo-creating-a-keyspace-and-table-in-amazon-keyspaces-for-apache-cassandra/">DEMO: Creating a Keyspace and Table in Amazon Keyspaces (for Apache Cassandra)</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/database-fundamentals-aws-part-2-1063/course-summary/">Course Summary</a></p>
<h1 id="What-is-a-Data-Lake"><a href="#What-is-a-Data-Lake" class="headerlink" title="What is a Data Lake?"></a>What is a Data Lake?</h1><p>What is a data lake? A data lake is a place for your business or enterprise to store and collect data. The data you store in your data lake may be structured or unstructured, meaning it can have a defined schema or not. </p>
<p>The goal of our data lake is to have a single place where all of our business information can exist, and eventually, we can have some type of analytics performed on it. This data can be from our transactional systems and line of business applications. It could also be from various IoT devices, mobile applications, and even social media.</p>
<p>Companies that are able to aggregate, and work on their data, and derive meaning from it will be able to outperform their peers. These companies might do so through the use of generic data analytics or even by using machine learning to provide valuable insights.</p>
<p>This is why it is important to manage and create a safe place for all your data to live, A.K.A. a data lake.</p>
<h1 id="What-is-the-Difference-between-a-Data-Lake-and-a-Data-Warehouse"><a href="#What-is-the-Difference-between-a-Data-Lake-and-a-Data-Warehouse" class="headerlink" title="What is the Difference between a Data Lake and a Data Warehouse?"></a>What is the Difference between a Data Lake and a Data Warehouse?</h1><p>What is the difference between a data lake and a data warehouse?</p>
<p>When first getting into this space there might be some confusion between data lakes and data warehouses. That is fairly common.</p>
<p>The main difference between a data lake and a data warehouse is specificity and structure. </p>
<p>A data lake is a formless blob of information, it is a pool of knowledge where we try to capture any relevant data from our business so that we can perform analytics on it.</p>
<p>A data warehouse is a specialized tool that allows you to perform analysis on a portion of that data, so you can make meaningful decisions from it. Generally, it is a subset of the data from the data lake with a specialized purpose. Your data warehouse Is an optimized database that is dealing with normalized, transformed, and cleaned-up versions of the data from the data lake.</p>
<h1 id="Why-Can’t-We-Just-Store-All-This-Information-Into-a-Data-Warehouse"><a href="#Why-Can’t-We-Just-Store-All-This-Information-Into-a-Data-Warehouse" class="headerlink" title="Why Can’t We Just Store All This Information Into a Data Warehouse?"></a>Why Can’t We Just Store All This Information Into a Data Warehouse?</h1><p>Why can’t we just store all this information into a data warehouse?</p>
<p>Well, This was exactly what was happening for a long period of time. Unfortunately, as the speed of business has increased, and so has the sheer volume of data. Data warehouses were unable to keep up with the amount of curating and scaling required to support such volumes of data. It was becoming cost-prohibitive and slowing down query speed to try and maintain all of this data in an active database.</p>
<p>So the advent of the data lake became a necessity. We needed a place where we could store large volumes of information for cheap.</p>
<h1 id="What-Makes-Up-a-Good-Data-Lake"><a href="#What-Makes-Up-a-Good-Data-Lake" class="headerlink" title="What Makes Up a Good Data Lake?"></a>What Makes Up a Good Data Lake?</h1><p>What makes up a good data lake?</p>
<p>A good data lake will deal with these five challenges well: Storage (the lake itself), Data Movement (how the data gets to the lake), Data Cataloging and Discovery(finding the data and classifying it), Generic Analytics (making sense of that data), and Predictive analytics ( making educated guesses about the future based on the data).</p>
<p>Storage: Let’s take a look at storage first. The reason people moved into using data lakes was that storage costs were becoming burdensome because the sheer volume of data was starting to crush people. What service does AWS offer that can easily deal with large and crushing volumes of raw data? Well, the first thing I would think about would be something like S3. </p>
<p>S3 is particularly good in this scenario, not only because it can deal with the large volume of data, but also because it can handle unstructured data. You could fill it with log files, json transaction documents, blobs of binary output, it takes anything. A normal database would not be particularly suitable for this task.</p>
<p>The other benefit of using S3 is that we can set up lifecycle policies to help deal with the cost of the ever-increasing data burden. This allows us to put infrequently accessed data into a cheaper storage tier, and even to eventually put it into glacier ( the deep archival service) when we are fairly certain that, that data isn’t going to be used for a long while. We can of course return the cold data back into S3 standard if we ever need to.</p>
<p>Data Movement: Another important thing to figure out when building your data lakes is how the heck are you planning to actually get your data into it. We know that S3 is what we should use for storage, but what mechanisms do we want to use to get all the stuff… into s3?</p>
<p>We could of course manually move large folders of archived log data into whatever bucket we are using for our data lake, but that idea is not super scalable and honestly just feels bad. It would be great to automatically push our business data into this bucket.</p>
<p>There are a few ways of getting your data into your bucket, be it from actively streaming your data with kinesis, to using a direct connection from on-premises to bring in large quantities of data, or using the database migration service to move your database information into s3, or you might even have to have snowball devices delivered to some faraway outpost once a month to collect research data to have sent back to AWS. </p>
<p>Whatever your method, you will need a way to move your data into AWS, and you will prefer that whatever way you use is automated.</p>
<p>Data Cataloging and Discovery: Once you have all of your data within your data lake (your s3 bucket of choice) It becomes necessary to start cataloging and understanding the types of data you have. If we do not spend at least a little time working through our data and managing it, we will quickly turn our data lake into a data swamp.</p>
<p>Think about what would happen as you add terabytes to petabytes of data, folders, and folders of the stuff, into the same bucket. As you do this over long periods of time, your knowledge of what is what and where it lives will fade. This makes it near impossible for anyone else to find specific data sets they want to work with. </p>
<p>This is why we need to catalog our data. We need to create some data about the data - metadata. This will help future persons discover what it is they need from our data lake, without them having to spend hours, days, or weeks trying to figure out where or what it is.</p>
<p>Things that might be helpful to know for example is what formats are the various data stored in - is it mostly JSON, CSV, Parquet… it is compressed data, is it sensitive data? And maybe you might want to add additional tags, like this is data from Twitter, or from customer reviews.</p>
<p>There are many ways you can go about this to create your own data catalog. For example, you might have an upload event on your s3 bucket that triggers a lambda function to store some metadata information in DynamoDB about the new data that was just uploaded. </p>
<p>From there we could push that information into ElasticSearch to browse through and query that data. This is a very do-it-yourself approach and could be a little tricky to get set up correctly.</p>
<p>I would recommend instead that you take a look at AWS glue. This service is a managed transform engine that allows you to run ELT pipelines - but for our uses, it also contains a very robust data catalog that we can leverage. </p>
<p>The glue data catalog even contains built-in crawlers that can crawl through various data sources and automatically populate the catalog for you. This includes your S3 buckets, databases, and data warehouses. They can be scheduled to run at certain times or based on events like new upload into that s3 bucket.</p>
<p>Analytics: Why would we be collecting all this data if we did not want to know information about that data. Our data is a record of the past and that record can give us great insights into what was successful and what was a failure for our business.</p>
<p>There are a number of great AWS services that can help you start to make sense of your data. These services range in their analytical ability and what their goals are. </p>
<p>For example, if you wanted to get some real-time information about your data lake, or at least the information being streamed into it from kinesis or Amazon MSK for example, you can use Kinesis Data Analytics to get a real time feed of what your streaming data is up to.</p>
<p>If you were looking to interactively scrub through your data we have Amazon Athena, a purposely built service that makes it easy to analyze data in Amazon S3 using standard SQL.</p>
<p>If you have some section of your data that you want to create dashboards and graphs for, that’s where something like Amazon Quicksight can be added to your solution.</p>
<p>And, we also have data warehousing services like redshift that you can place a subset of our data lake within to perform general analytics on to try and derive some meaning from that data.</p>
<p>Predictive analytics: Being able to perform predictive analytics will allow you to gain some possible future insight into your business though your data. You can start to build out systems that help with this through the use of machine learning services.</p>
<p>One of the most important things for machine learning is having a robust data set to work with. This is why it works so well to have a data lake where you can pull subsets of data from.</p>
<p>Amazon offers AWS sage maker as a quick way to get into creating, training, and running your own models within AWS. </p>
<p>Additionally, aws has a series of deep learning AMI that come pre-configured with popular deep learning frames and interfaces. This included TensorFLow, PYtorch, Apache MXNet, Chainer, Gluon, Horovod, and Kera. There are no additional charges for using these AMIs, they are still pay-as-you-go like other instance types.</p>
<h1 id="How-do-I-Actually-Build-a-Data-Lake"><a href="#How-do-I-Actually-Build-a-Data-Lake" class="headerlink" title="How do I Actually Build a Data Lake?"></a>How do I Actually Build a Data Lake?</h1><p><strong>Ok, so how do I actually build a data lake?</strong></p>
<p>So there are two ways you can actually go about creating your data lake. You can try to assemble all of these interconnected data lake pieces by hand; which can take quite a bit of know-how and a lot of time.</p>
<p>There are also a few deployable templates floating around from AWS that can help with this process - take a look over here to see a template and an architecture build guide: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/solutions/latest/data-lake-solution/architecture.html">https://docs.aws.amazon.com/solutions/latest/data-lake-solution/architecture.html</a></p>
<p>Or we can use the AWS Lake formation service, which promises to make setting up your secure data lake take only a matter of days, instead of weeks or months.</p>
<p>It does this by identifying existing data sources within Amazon S3, relational databases, and NoSQL databases that you want to move into your data lake. It then will crawl and catalog and prepare all that data for you to perform analytic on. You can also target log files from things like CloudTrail, Kinesis Fire Hose, Elastic Load Balancers, and CloudFront. All this data can be grabbed all at once, or it can be taken incrementally. </p>
<p>All of this functionally is managed by using ‘blueprints’ where you simply:</p>
<ol>
<li>Point to the source data</li>
<li>Point where you want to load that data in the data lake</li>
<li>Specify how often you want to load that data</li>
</ol>
<p>And the blueprint:</p>
<ol>
<li>Discover the sources table schema</li>
<li>Automatically converts to a new target format</li>
<li>Partitions the data based on partitioning schema</li>
<li>Keeps track of the data that was already processed.</li>
<li>Allows you to customize all the above</li>
</ol>
<p>AWS Lake formation will take care of user security by creating self-service access to that data through your choice of analytic services.</p>
<p>It does this by setting up users’ access within lake formation, by tying data access with access control policies within the data catalog instead of with each individual service. So when a user comes to lake formation to see some data - their credentials and access roles are sent to lake formation, lake formation digests that and determines what data that person is allowed to access, and gives them a new token to carry with them that services like Athena, Redshift, and EMR will honor.</p>
<p>This allows you to define permissions once, and then open access to a range of managed services and have those permissions enforced. </p>
<p>There is no additional pricing for using the Lake Formation service, but you do have to pay for all the services it uses though. This means you have to pay for any AWS Glue usage during the crawling and cataloging phases. You will have to pay for the data residency within S3. You will have to pay for any Athena queries you might make on the data when looking up information.</p>
<p>So while the orchestration of all the services doesn’t cost anything, there are many fees that you should be aware of when architecting your solutions if cost is a concern.</p>
<h1 id="Databases-Summary"><a href="#Databases-Summary" class="headerlink" title="Databases Summary"></a>Databases Summary</h1><p>You’re deep into the learning path now, and we’ve covered off compute, storage, networking, and now databases. So congratulations on getting this far. So again, let’s take a recap of some of the most important elements that you need to be aware of for the exam. For me personally, when I was studying for the AWS Solutions Architect Associate exam, I found databases the hardest aspect to grasp and I’m sure I’m not alone from that perspective but you’ve got through the theory which is the hardest part. Okay, let’s focus on the essential must-know points for the certification, and this mainly covers RDS, DynamoDB, and ElastiCache. So RDS, let’s hit the key points. So what is RDS? It’s a managed relational database service, which means all of the patching required of the underlying instance and software that runs RDS is going to be managed by AWS. </p>
<p>In addition to automated backups of your database. So that again will be managed by AWS as well. So this takes a lot of the administrative burden out allowing you to just work on the data within your databases. So even though automatic backups are taken care of you can still perform your own manual snapshot backups. Now, because it’s managed, it’s quick and easy to set up. It has a range of different database engines available to suit your needs, and if you couple this with a scalable and resizable capacity delivery of high performance then this is a great choice for mobile and web applications in addition to e-commerce applications as well. Now, remember, if you need to build in high availability to your audience database you’ll need to use the Multi-AZ feature of RDS to offer additional resiliency in case your primary instance fails. Now, we will talk more about high availability in a later course in this learning path, but Multi-AZ and read replicas will very likely come up in the exam. </p>
<p>So the previous course gave a good introduction to these features, but we will dive into them more in more detail in a later course. I want to point out Aurora which is one of the database engines of RDS and its compatibility with MySQL and PostgreSQL. It’s highly likely that Aurora will be mentioned in the exam and it’s usually in relation to performance and cost-related questions. Now, this is because it can run fives times faster than MySQL and three times faster than PostgreSQL while still maintaining tight security, reliability, and availability, and its also extremely cost-effective operating at just one 10th of the cost of other databases. Now it’s also capable of running 15 read replicas and it can operate across three different availability zones while continuously backing up allowing to recover to any point in time, so it’s super powerful. So let’s see how RDS compares to DynamoDB. So the first big difference to RDS is that it’s not a relational database service. </p>
<p>It’s a NoSQL key-value database. It’s super fast designed for ultra-high performance and capable of handling up to 20 million requests per second. It’s massively scalable, multi-regional, and can scale with single-digit latency regardless of how big the database gets. Whereas with RDS, the larger the database the slower it can get, but with DynamoDB it maintains the ultra-low latency so make sure you bear that in mind. Much like RDS though, it’s fully managed. So again, you just need to focus on your data as all patch management and backups are taken care of by AWS but instead of having to select an instance like RDS, there is no need to with DynamoDB as it’s serverless. Now, you will find that due to these features it’s used for applications related to gaming, media and entertainment, finance and banking, and any solutions that require that single-digit millisecond latency. </p>
<p>Okay, so let’s now take a look at Amazon ElastiCache. So ElastiCache makes it easy to deploy, operate, and scale open-source, in-memory data stores in the cloud. So it basically improves performance of your applications through caching. Now, one key point to note for the exam is that it supports two different engines, Memcached and Redis. So I recommend you have a solid understanding of when to use each engine. Now, genuinely if you are looking for an in-memory key store and or cache that offers high-performance sub-millisecond latency, then Memcached is your best choice. </p>
<p>And it’s also recognized for its speed, performance, and also its simplicity as well. Whereas Redis offers a more robust set of features of that of Memcached while still offering that ultra high performance. So a common scenario might come across could be that you have a web application that reads and writes data to a persistent storage, for example, RDS, but however persistent storage tends to experience some fluctuations in latency as each piece of data needs to be written to or retrieved and this is affecting the overall performance. </p>
<p>So what could you do to enhance the performance of read requests? Well, the answer would be to implement an in-memory cache like ElastiCache, and this is especially true in industries such as online gaming and social networking sites.</p>
<p>So that’s some of the top points I suggest you must note when looking at databases for the exam. So to recap at a high level, RDS is a managed relational database service that supports multiple database engines specifically Amazon Aurora, which is highly performance and cost-effective, and it’s a PostgreSQL and MySQL compatible engine. Whereas DynamoDB is a managed serverless, NoSQL database service designed for ultra-high performance, providing single-digit latency. And ElastiCache is used as an in-memory data store to improve the read performance of your applications through caching and is supported by Redis and Memcached. Okay, so that’s the short summary wrapped up. So if you’re ready, let’s take the next step to become an AWS certified.</p>
<h1 id="2Amazon-Relational-Database-Service"><a href="#2Amazon-Relational-Database-Service" class="headerlink" title="2Amazon Relational Database Service"></a>2<strong>Amazon Relational Database Service</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">Instance Type Performance</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">Course: When to use RDS Multi-AZ &amp; Read Replicas</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Blog post: AWS Shared Responsibility Model</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Lab: Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Lab: Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="5RDS-Instance-Purchasing-Options"><a href="#5RDS-Instance-Purchasing-Options" class="headerlink" title="5RDS Instance Purchasing Options"></a>5<strong>RDS Instance Purchasing Options</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">Instance Type Performance</a></p>
<h1 id="8Backtrack-Storage-Pricing"><a href="#8Backtrack-Storage-Pricing" class="headerlink" title="8Backtrack Storage Pricing"></a>8<strong>Backtrack Storage Pricing</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/aws/amazon-aurora-backtrack-turn-back-time/">Backtrack Storage Blog Post</a></p>
<h1 id="9Snapshot-Export-Pricing"><a href="#9Snapshot-Export-Pricing" class="headerlink" title="9Snapshot Export Pricing"></a>9<strong>Snapshot Export Pricing</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/introduction/">Course: Understanding and Optimizing Costs with AWS Storage Services</a></p>
<h1 id="11Amazon-DynamoDB"><a href="#11Amazon-DynamoDB" class="headerlink" title="11Amazon DynamoDB"></a>11<strong>Amazon DynamoDB</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<h1 id="18Amazon-Redshift"><a href="#18Amazon-Redshift" class="headerlink" title="18Amazon Redshift"></a>18<strong>Amazon Redshift</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">Differences between Redshift and PostgreSQL</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">Selecting a Distribution Style</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Course: Overview of AWS IAM</a></p>
<h1 id="20Amazon-DocumentDB-With-MongoDB-Compatibility"><a href="#20Amazon-DocumentDB-With-MongoDB-Compatibility" class="headerlink" title="20Amazon DocumentDB (With MongoDB Compatibility)"></a>20<strong>Amazon DocumentDB (With MongoDB Compatibility)</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-the-aws-database-migration-service/dms-introduction/">Course: AWS Database Migration Service</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-the-aws-database-migration-service/dms-introduction/">Course: AWS Database Migration Service</a></p>
<h1 id="22Amazon-Keyspaces-for-Apache-Cassandra"><a href="#22Amazon-Keyspaces-for-Apache-Cassandra" class="headerlink" title="22Amazon Keyspaces (for Apache Cassandra)"></a>22<strong>Amazon Keyspaces (for Apache Cassandra)</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.cqlsh.html">Running queries on a CQLSH client</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/keyspaces/latest/devguide/programmatic.html">Running queries programmatically using an Apache 2 licensed Cassandra client driver</a></p>
<h1 id="24Amazon-Quantum-Ledger-Database-QLDB"><a href="#24Amazon-Quantum-Ledger-Database-QLDB" class="headerlink" title="24Amazon Quantum Ledger Database (QLDB)"></a>24<strong>Amazon Quantum Ledger Database (QLDB)</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/qldb/latest/developerguide/verification.html">QLDB Verification</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-2-of-2-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-2-of-2-21/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-2-of-2-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:29" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:29-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:08:18" itemprop="dateModified" datetime="2022-11-27T20:08:18-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-2-of-2-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-2-of-2-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Networking-SAA-C03-2-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-1-of-2-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-1-of-2-20/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-1-of-2-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:28" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:28-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:08:10" itemprop="dateModified" datetime="2022-11-27T20:08:10-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-1-of-2-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Networking-SAA-C03-1-of-2-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Networking-SAA-C03-1-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Amazon-CloudFront-Challenge-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Amazon-CloudFront-Challenge-19/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Amazon-CloudFront-Challenge-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:26" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:26-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:17:24" itemprop="dateModified" datetime="2022-11-27T20:17:24-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Amazon-CloudFront-Challenge-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Amazon-CloudFront-Challenge-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Securing-your-VPC-using-Public-and-Private-Subnets-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Securing-your-VPC-using-Public-and-Private-Subnets-18/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Securing-your-VPC-using-Public-and-Private-Subnets-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:25" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:25-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:11:04" itemprop="dateModified" datetime="2022-11-27T20:11:04-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Securing-your-VPC-using-Public-and-Private-Subnets-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Securing-your-VPC-using-Public-and-Private-Subnets-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-Virtual-Private-Cloud-VPC-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-Virtual-Private-Cloud-VPC-17/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Introduction-to-Virtual-Private-Cloud-VPC-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:24" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:24-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:12:18" itemprop="dateModified" datetime="2022-11-27T20:12:18-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-Virtual-Private-Cloud-VPC-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-Virtual-Private-Cloud-VPC-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Networking-SAA-C03-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Networking-SAA-C03-16/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Networking-SAA-C03-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:22" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:22-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 19:59:40" itemprop="dateModified" datetime="2022-11-27T19:59:40-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Networking-SAA-C03-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Networking-SAA-C03-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Networking-SAA-C03-Introduction"><a href="#Networking-SAA-C03-Introduction" class="headerlink" title="Networking (SAA-C03) Introduction"></a>Networking (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on networking in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the various networking services currently available in AWS that may be covered on the exam.</p>
<p>Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#112;&#x70;&#111;&#114;&#116;&#x40;&#x63;&#x6c;&#111;&#x75;&#x64;&#97;&#x63;&#x61;&#100;&#101;&#109;&#x79;&#46;&#99;&#111;&#109;">&#x73;&#117;&#112;&#x70;&#111;&#114;&#116;&#x40;&#x63;&#x6c;&#111;&#x75;&#x64;&#97;&#x63;&#x61;&#100;&#101;&#109;&#x79;&#46;&#99;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various networking services in AWS in preparation for the exam.</p>
<p>The objective of this course is to provide an introduction to networking services in AWS for solution architects, including:</p>
<ul>
<li>Amazon Virtual Private Cloud, or VPC;</li>
<li>Configuring security groups and Network Access Control Lists, or NACLs;</li>
<li>AWS Virtual Private Network, or VPN solutions; and</li>
<li>AWS Direct Connect.</li>
</ul>
<p>You’ll learn about AWS networking components that include:</p>
<ul>
<li>Elastic IP addresses;</li>
<li>Elastic Network Interfaces, or ENIs;</li>
<li>EC2 Enhanced Networking with the Elastic Network Adaptor, or ENA; and</li>
<li>VPC Endpoints.</li>
</ul>
<p>You’ll also learn about global networking services such as Amazon Route 53, Amazon CloudFront, and the AWS Global Accelerator, which leverages the AWS global infrastructure to reduce latency and improve the overall performance of your applications.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#x75;&#112;&#x70;&#111;&#114;&#x74;&#64;&#99;&#x6c;&#x6f;&#x75;&#x64;&#97;&#x63;&#x61;&#x64;&#101;&#x6d;&#121;&#46;&#99;&#x6f;&#109;">&#115;&#x75;&#112;&#x70;&#111;&#114;&#x74;&#64;&#99;&#x6c;&#x6f;&#x75;&#x64;&#97;&#x63;&#x61;&#x64;&#101;&#x6d;&#121;&#46;&#99;&#x6f;&#109;</a>. Thank you!</p>
<h1 id="What-is-a-VPC"><a href="#What-is-a-VPC" class="headerlink" title="What is a VPC?"></a>What is a VPC?</h1><p>Hello and welcome and I’m going to be talking to you about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>s. Virtual Private Clouds. Now to understand what a VPC is, let’s just take a look at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> infrastructure. </p>
<p>So, here is the AWS Cloud. Very simple. And a VPC resides inside of the AWS Cloud and it’s essentially your own isolated segment of the AWS Cloud itself, so here is your VPC sitting inside the AWS Cloud. </p>
<p>Now by default when you create your VPC, the only person that has access to this is your own AWS account, just you. It is totally isolated and no one else can gain access to your VPC other than your own AWS account. Now obviously there are millions upon millions of other VPCs within the AWS network created by other customers all across the world. So, there are millions of customer VPCs. However, they do not have access to your VPC and likewise, you do not have access to their VPC. </p>
<p>Now what do you use a VPC for? Well, essentially it allows you to start deploying resources within your VPC, for example, different compute resources or storage or database and other network infrastructure among others and this allows you to start building and deploying your solutions within the Cloud. </p>
<p>Now by default from a limitation perspective, you are allowed up to five VPCs per region per AWS account and it’s very simple to create a VPC. All you need to do is to give it a name, when you create your VPC and also define an IP address range that the VPC can use and this is done in the form of a CIDR block which stands for Classless Inter-Domain Routing. And I’ll talk more about that when I talk more about subnets in a few minutes. </p>
<p>So, just to recap at a high level, simply put, a VPC is an isolated segment of the AWS public cloud that allows you to provision and deploy resources in a safe and secure manner. I now want to dive deeper into the VPC architecture and start talking about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnets</a> and how you can segment your VPC out into different areas across multiple availability zones for resiliency and high availability, so let’s take a look.</p>
<h1 id="Subnets"><a href="#Subnets" class="headerlink" title="Subnets"></a>Subnets</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/vpc-cidr-blocks/">VPC TCP&#x2F;IP Addressing</a></p>
<p><strong>Transcript</strong></p>
<p>So <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-what-vpc/">now we know what a VPC is</a>, let’s take a look at subnets. Now, subnets reside inside your VPC, and they allow you to segment your VPC infrastructure into multiple different networks. Now, you might want to do this to create better management for your resources, or to isolate certain resources from others, or even to create high-availability and resiliency within your infrastructure. So let’s take a look at the subnets. </p>
<p>Firstly, then we’ll just draw our VPC quickly. So this is our VPC. And I mentioned when talking about VPCs that when you create your VPC, there’s two pieces of information that you need. You need to give it a name and also a CIDR block address. Now, the CIDR block address is a range of IP addresses and this CIDR block range can have a subnet mask between a range of IP addresses from a &#x2F;16 all the way through to a &#x2F;28. Now, if you’re not familiar with TCP&#x2F;IP addressing, now please take a look at the link on screen and check out the following course and this will dive into the CIDR block and TCP&#x2F;IP addressing in greater detail. </p>
<p>Now, for our example, let’s say we created our VPC with the following CIDR block. 10.0.0.0&#x2F;16. Now, this is important because any subnets that we create within our VPC need to reside within this CIDR block range. So let’s take a look at a couple of subnets. </p>
<p>Now, in this section, I want to talk to you about public subnets and also private subnets. So let’s just create a public subnet there and also a private subnet here. This yellow one can be our public subnet and the green one can be our private subnet. Now, similarly, when we create a VPC, we need to give it a CIDR block range. We need to do the same with our subnets as well. So let’s say for example this is 10.0.1.0&#x2F;24. Now, this range of addresses sits within this bigger CIDR block here, and then this private subnet can be 10.0.2.0&#x2F;24. And again, this CIDR block sits within the bigger VPC CIDR block. </p>
<p>Now, what makes a subnet public and what makes a subnet private? Well, essentially a public subnet is accessible from outside of your VPC. So essentially from the Internet. For any resources created within your public subnet, for example web servers, would be accessible from the Internet. Now, because we want these web service accessible from the Internet, I have two IP addresses. So they have their own internal IP address which will be within the range of the subnet, which, for this subnet, it’s 10.0.1.0&#x2F;24. And then also we’re going to assign them a public IP address as well, because to be accessible from the Internet, the instance itself has to have a public IP address. </p>
<p>Any resources created within your private subnet, for example your backend databases, would be considered private and inaccessible by default from the Internet. So how do you make a subnet public and how do you make one private? When you create a subnet, you create them both exactly the same. It’s what you configure afterwards that will dictate if a subnet is public or private. </p>
<p>There’s two changes you need to make to your infrastructure to make a subnet public. The first is to add an Internet gateway. Now, an Internet gateway is a managed component by AWS that is attached to your VPC and acts as a gateway between your VPC and the outside world. So essentially the Internet. So let’s just add in an Internet gateway here, IGW for Internet gateway. So now we have our Internet gateway attached to our VPC. And this Internet gateway then also connects out to the Internet. So we now have a bridge between our isolated VPC to the Internet by the Internet gateway which is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. </p>
<p>Now, you might think that a public subnet now has access to the Internet because there’s an Internet gateway. However, before the public subnet can access the Internet, we need to add a route to the public subnet’s route table. Now, associated with every subnet when it’s created will also be an associated route table. Now, you can have the same route table associated to multiple subnets. That’s not a problem. However, you can’t associate more than one route table to a single subnet. </p>
<p>Now, by default, when your subnet’s created, it will have a default route in it, and this is a local route. Let’s take a look. Now, your route table will contain a destination field and also a target field. Now, the destination field is the destination address that you’re trying to get to. The target essentially specifies the route to that destination. Now, within every route table that’s created, there will be this local route here. Now, what this enables your subnets to do is simply talk to each other. So any subnet within your VPC is able to communicate with each other without you having to configure any routes. It’s there by default. Every route table has this local route. It can’t be deleted, and it simply allows all subnets within your VPC to communicate with each other. </p>
<p>So what we need to do is we need to add a route to this route table that’s associated to the public subnet. Now, this new route here that’s been added to the route table has a destination of 0.0.0.0&#x2F;0. Now, that essentially means that for any IP address that’s not known within the route table already, send it to this target. Now, this target is the Internet gateway as shown by the IGW. This part here is simply the ID of the Internet gateway. So by adding this route to the route table, this public subnet now knows exactly how to get to the Internet by going by the Internet gateway as shown in the route table. Now, those two components are essentially what makes a subnet public, the fact that we have an Internet gateway attached to the VPC and this subnet has a route pointing to the Internet gateway for any traffic that it doesn’t know how to get to. </p>
<p>Now, if we compare the route table of the private subnet, we can see that it only has this local route, so it has no route to the Internet gateway. It’s not aware that an Internet gateway even exists, so it has no route out to the public Internet. So this is considered a private subnet. Now, if we go back to the public subnet, before we added this route here, so let’s just take that out. This subnet is effectively a private subnet, because it doesn’t have a route to the Internet gateway. So with that in mind, every time you create a subnet, it is a private subnet to begin with and that is until you attach an Internet gateway to your VPC and then add this additional route. </p>
<p>So now we’ve looked at public subnets and also private subnets. Let’s now look at architecting multiple subnets across your VPC for high availability and resilience. So let’s just clear the screen, give us a blank VPC to work with. So let’s consider we have three subnets this time. We’ll have a public subnet, and we’ll have two private subnets. This can be our public subnet, and these two will be our private subnets. This will be our web layer. This will be our application layer, and this will be our database layer. </p>
<p>So in our public subnet, we will have some web servers. In our application layer, we’ll just have some EC2 instances. And in our database layer, we’ll just have some databases. So there we have our three tiers of our deployment. So as we know with any subnet, we have a local route, so each of these will all have a local, as you can’t remove that local route. And this enables all of these subnets to communicate with each other. The public subnet, as we know, will also have a route to the Internet gateway. Now, when you create a subnet, you have to create it in one of the availability zones that are available within that region. Now, if you’re not too familiar with the AWS global infrastructure, then please take a quick read of the blog post below. </p>
<p>Let’s say for example when we created this subnet, we created it in availability zone one, and we’ve done the same for the remainder of our subnets as well, and placed them all in the same availability zone. And that’s all okay, we can deploy infrastructure all within the same availability zone and our solution would be operating fine. However, should AWS have an issue with availability zone within this region, for example they might experience a flood or a fire or a natural disaster, and it took out the services to availability zone one, what would happen to our resources? </p>
<p>Well, effectively, these would also be taken down because they’re all running in availability zone one. So that’s not ideal. It’s not best practice to deploy all of your resources within the same availability zone, within a single region, simply because it doesn’t offer high-availability and resilience. So what should you do in that situation? Well, the best thing to do to ensure high-availability is to add additional subnets to allow for resiliency. So we’ll add an additional web tier and also additional application, and also a database. </p>
<p>So now we have six subnets, and again, we’d replicate our resources, so it’d have our web infrastructure here, we’ll have our application service here and our databases here. Again, then we’ll have the routes and then Internet gateway route as well. This will have a local route, and as we know, this allows communication between all subnets. So now, every subnet can talk to every other subnet with a local route, and also this also has a route to the Internet gateway as well. So now let’s look at the availability zones that we’ll deploy our infrastructure in this time. </p>
<p>Let’s say for example we’ll deploy this in AZ-1, this application subnet in AZ-2, and this database subnet in AZ-3, and similarly, down here, we have this public subnet in AZ-3. This one in one, and this final one in two. So now let’s run through the scenario again. Let’s imagine AZ-1 experienced a failure. So what would happen here? This public subnet would be out of action. This application subnet and that is it. So in this situation, we still have one subnet available in each layer of our infrastructure. So should we experience a failure with availability zone one, our services will remain up and running. </p>
<p>So let’s do the same with availability zone two. What would happen in this situation? Well, both of our public subnets would be okay, because everyone’s in AZ-1 and three. This application subnet would be down, and this database subnet would also be down. So again, at least one subnet in each of our layers is operational and available. So again, our services would still be up and running. </p>
<p>Now, finally, just for clarity, if we take down availability zone three, this web layer would go and also this database layer. So again, we still have at least one subnet in each tier or each layer of our infrastructure operational. So this is a much better design. This allows you to ensure your resources stay up and running should a failure occur in one of the availability zones. Before we move onto some security features, let me just clear the screen because I want to talk to you just quickly about IP addressing, just a couple of points that I want to mention with regards to the subnets. So let’s just clear this quickly. </p>
<p>So I mentioned that when you create your subnet, you have to assign it a CIDR block range that fits within the VPC CIDR block. So say for example we created a subnet here, and we give the subnet the address of 10.0.1.0&#x2F;24. Now, with a slash 24 mask, this gives this subnet a total of 256 IP addresses. You can only actually use 251 IP addresses. And I’ll explain why. So the very first IP address in this subnet is 10.0.1.0 and this is known as the network address. Now, you’re not able to use this as an IP address to assign to your host addresses. This is reserved for networking. Now, the next available IP address after the network address is 10.0.1.1. And this is reserved for AWS routing. So again, not a network address. You can’t use this address as a host network in your subnet. Now, the next available IP address is 10.0.1.2. And again, this one is reserved by AWS, but this time for DNS. So you cannot use this IP address. Now, the fourth IP address that you won’t be able to use in this subnet is 10.0.1.3. And this is actually reserved by AWS for future use. Now, the fifth and final address that you can’t use in an AWS subnet is the last available address in the subnet. So in this case, it’d be 10.0.1.255, and the last address in any subnet is known as the broadcast address, and again, you cannot use this for host resources. </p>
<p>So when working with TCP&#x2F;IP addresses within your subnet, first four addresses in any subnet are reserved and you cannot use for host addresses and also the very last address is reserved. So that’s why use only 251 IP addresses available to you that you can use to assign to your host resources. So now we’ve covered what a VPC is. We’ve looked at subnets, both public and private, and also how it’s best to architect your subnets across multiple availability zones for high-availability. So now let’s look at some security features. I want to start with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-network-access-control-lists-nacls/">network access control lis</a>t. Let’s take a look.</p>
<h1 id="Network-Access-Control-Lists-NACLs"><a href="#Network-Access-Control-Lists-NACLs" class="headerlink" title="Network Access Control Lists (NACLs)"></a>Network Access Control Lists (NACLs)</h1><p>Security is a key part of any deployment within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, and managing security around your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">virtual private cloud</a> is no different. So I want to talk to you about a couple of different components here. </p>
<p>Firstly, I want to talk to you about NACLs which are network access control lists. Now these are essentially virtual network-level firewalls that are associated to each and every <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnet</a>, and they help to control both ingress and egress traffic moving in and out of your VPC and between your subnets. So let’s just quickly draw out our VPC. Very simple, and let’s just draw in a public subnet for example. So this is going to be public. Up here, we’ll have our internet gateway attached to our VPC, and obviously, we have a route out to the gateway, which then communicates with the internet. </p>
<p>So what we can do here to help maintain security is to configure the network access control list associated to this subnet. Now much like route tables, whenever you create a new subnet, it will also associate a network access control list. Now by default, this NACL will allow all traffic, both inbound and outbound, so it’s not very secure, so it’s a really good practice to configure your NACLs to only allow the traffic that you want to come in and out of your subnet. </p>
<p>Now with this being a public subnet, we’ll probably have some web servers in here talking over HTTP and HTTPS, so let’s look at the inbound network access control list that could be associated to this subnet. Now as you can see, there’s a number of different fields. We have the rule number, the type, the protocol, port range, source, and allow or deny. Now the rule numbers allow you to specify what order the rules will appear inside the NACL, and as soon as traffic hits one of these rule where it matches all of the type, protocol, port range, and source, et cetera, it will carry out the action at the end, whether that is allow or deny. </p>
<p>So let’s look at the requirements required to match this rule here. The type of traffic will need to be HTTP under port 80 using the TCP protocol, and again, the port range is 80 as that’s what used for HTTP. Now the source can be any IP address, so any IP address running HTTP coming into our subnet will be allowed. So as long as they’re running this protocol, then the traffic will be allowed inbound into our public subnet. </p>
<p>Now let’s look at the second rule. Now the second rule uses HTTPS using the TCP protocol using port 443, and again, any source, and the action will be allowed. Now the last rule here, now this is a default rule that’s applied at the end of every network access control list, and that’s why it doesn’t have a rule number, and it states that all traffic using any protocol in any port range from any IP address, then deny that access. So this rule is kind of a cover rule. So basically, what that allows you to do is ensure that any traffic that doesn’t meet the rules that you’ve entered is deleted and denied access to your subnet. </p>
<p>So with this in mind, the only traffic allowed in our public subnet is essentially HTTP and HTTPS, which is exactly what we want for our web servers here, and all other traffic will be denied. So that’s the inbound NACL. Let’s now take a look at the outbound. Now the field types are all exactly the same other than this one here. This has a destination whereas on the inbound, it has the source. So on the outbound, we restrict traffic against its destination. </p>
<p>So the first rule we have here says any traffic using any protocol in any port range going to any destination, then allow that traffic. Anything else should be denied, but in this case, there won’t be anything else because this outbound rule is essentially saying send any traffic you want to using any protocol out from this subnet to any destination. </p>
<p>Now an important point to make about NACLs is that they are stateless, and this means that any response traffic generated from a request will have to be explicitly allowed and configured in either the inbound or the outbound ruleset, depending on where the response is coming from. Now again, much like route tables, you can have the same NACL applied to a number of subnets, but only a single NACL can be associated to one subnet. So network access control lists are a great way to control traffic that comes into and out of a particular subnet. </p>
<p>Let me now talk about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-security-groups/">security groups</a>, and these are another method of controlling traffic, but this time, they work at the instance level rather than the network level like NACLs do.</p>
<h1 id="Security-Groups"><a href="#Security-Groups" class="headerlink" title="Security Groups"></a>Security Groups</h1><p>So, staying with security, I now want to talk to you about security groups. Now these are similar to network access controllers where they filter traffic both inbound and outbound but whereas <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-network-access-control-lists-nacls/">NACLs</a> worked at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnet</a> level, security groups work at the instance level and I’ll explain more about this as we go. </p>
<p>So let’s say we have three subnets, okay, so we just draw these out quickly and these would be three private subnets, for example. Each of them will have their IP addresses listed, first one being 10.0.1.0&#x2F;24. .2.0. And the last one, 3.0. Now this first subnet will have EC2 instances in them. The second subnet will have RDS instances running MySQL or Aurora and the last subnet here will also have EC2 instances in them. </p>
<p>Now each of these three subnets are associated to the same network access control list. So, this one is linked with this one and also this one. And this network access control list looks like this. And this is simply saying that any traffic that is running a TCP Protocol across any port range from any source, then allow it and deny all other traffic. So, between these subnets, any TCP Protocol on any port can be used and for simplicity, the same NACL rules are being used for both inbound and outbound. Now that’s not very secure, it’s not very restrictive but from a subnet network level, that is what it’s controlling. Now what we want to do is restrict access to which instances can actually talk to our RDS and Aurora databases here. </p>
<p>Now we only want to allow access from this subnet over here and deny access from this subnet here and we can use security groups to do just that. So, let’s take a look at the security group for this subnet here, from where our databases are. Now security groups have similar fields to NACLs but there’s just a couple less. So, there’s no rule number with the security group which means all the rules within the security group will be assessed before a decision is made on the action and you’ll also notice, there’s no allow or deny either. With security groups, if there’s a rule in there, then it’s considered allowed, if there’s no rule, then all traffic is dropped by default, so with this security group is stating that any MySQL or Aurora traffic using a TCP Protocol on the port 3306 from the source 10.0.1.0 which is this subnet here, then it’s considered allowed as we don’t have another rule in this security group for the source of 10.0.3.0&#x2F;24 which is this subnet here. Then it’s considered denied. It doesn’t exist, so it’s not allowed access, so how do both these NACLs and security groups work together? </p>
<p>Well, the NACL works at the subnet level, so let’s say the NACL is this purple line and as this NACL is associated to this subnet as an example, let’s just put that NACL around the edge of the subnet like so and let’s say this orange is our security group and that security group is associated to our databases inside this subnet. So, let’s assume that our EC2 instances here are looking to communicate with the RDS and Aurora databases over here, so let’s have a look how that traffic would flow through the NACL and also the security group. </p>
<p>So, the request would be sent, it would get to the NACL and the NACL say okay, is this traffic TCP traffic within this port range from any source? And it is. So, the traffic is allowed. So, that traffic is now allowed inside the subnet. It then hits the security group and the security group says is this a MySQL or Aurora traffic running the TCP Protocol using port range 3306 coming from 10.0.1.0? And it is as we’re trying to communicate with the databases, then access is allowed. Now if we look from this subnet here, the 10.0.3.0 and do the same thing where these two EC2 instances are trying to communicate with the RDS and Aurora instances using port 3306, let’s follow the same process. </p>
<p>So, the request is sent, it hits the NACL, the NACL says are you running TCP within this port range from any source? The answer is yes, so access is allowed. It then hits a security group and it says is this traffic MySQL or Aurora using TCP Protocol on port range 3306? At this point, everything is correct, yes. However, the source is different. We don’t have a source address of 10.0.3.0. It doesn’t exist in the security group. So, at this point, the traffic is dropped at the security group and access is not allowed. </p>
<p>So, you can see how NACLs and security groups can be used to filter traffic at different layers. The NACLs are used for the subnet and network layer and the security groups are used at the instance layer. Now one final thing I wanna say about security groups is that unlike NACLs which are stateless by design, security groups are stateful which means you don’t have to configure specific rules to allow return traffic from requests like you have to do with NACLs.</p>
<h1 id="NAT-Gateway"><a href="#NAT-Gateway" class="headerlink" title="NAT Gateway"></a>NAT Gateway</h1><p><strong>Resources referneced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">AWS Shared Responsibility Model</a></p>
<p><strong>Transcript</strong></p>
<p>I now want to talk to you about another VPC component, and that is the NAT gateway. To help explain what this does, let me just draw out our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a> quickly. So we have a very simple VPC, and we’re gonna have two subnets in this VPC, we’ll have a public subnet and also we’ll have a private subnet as well, and it’s the private subnet that we’re going to be focusing on. </p>
<p>So this will be our public, and the green one will be our private subnet. Now obviously we’ll have an Internet gateway attached to our VPC, which will then connect out to the Internet. Okay, so we have a public subnet, and a private subnet. Now in our private subnet we’ll have a number of EC2 instances running our applications, and in our public subnet we’re likely to have a number of web servers as well. As we know, each of these subnets also have a route table attached. Public route table will have access to the Internet gateway, and also to the other private subnet. </p>
<p>Now we need to start thinking about security again. Now, looking at our EC2 instances in the private subnet, we are responsible, as a part of the AWS Shared Responsibility Model, to update and patch the operating systems running on each of our EC2 instances. Now if you’re not familiar with the AWS Shared Responsibility Model, I suggest you take a look at it. It’s critical to all of your AWS deployments, and it essentially defines the boundaries of security as to what your roles and responsibilities are of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-security-groups/">implementing security within the cloud</a>, and what AWS’s responsibility is of maintaining security of the cloud. For more information, you can take a look at this blog post here. </p>
<p>Okay, so with that in mind, if we have the responsibility of maintaining the operating systems of our EC2 instances, then we need to be able to download updates as and when we need to. However, this subnet is private. Meaning it has no access to the Internet gateway, and therefore the Internet, so how can we download those updates? Well, what we can do, we can add a NAT gateway. </p>
<p>Now, a NAT gateway sits within the public subnet. Because it sits within the public subnet, it has to have a public IP address in the form of an EIP which is an Elastic IP address, and this is assigned to the instance itself. Now because it sits within the public subnet, it has a route out to the Internet gateway, and to the Internet. Now once we have our NAT gateway set up and configured, we need to update the route table of our private subnet. Now, by default our route table in our private subnet will just have the local route that all route tables have. But if we update that to provide a route to the NAT gateway, and we can see that I’ve added this additional route in here. Now this looks very familiar to the route we added to the public subnet to get access to the Internet via the Internet gateway, and it is essentially the same. So we’ll add the 0.0.0.0&#x2F;0 which is essentially a destination to any IP address unknown in the route table already. Then, send it to the target of the NAT gateway. And they can tell it’s a NAT gateway as this first part here, is prefixed with nat. And then this section along here, is essentially the ID of the NAT gateway within your VPC. </p>
<p>So what this route table is telling us, is that if any resource within this subnet needs to gain access to the Internet to perform an update, then it can do so via our NAT over here. This NAT gateway will then take the request, go via the Internet gateway, and download the appropriate software that’s required, and send it back to the EC2 instance requesting it. Now the important thing with a NAT gateway, is that it will not accept any inbound communication initiated from the Internet. It will only accept outbound communications originating from within your VPC. So it will deny all inbound traffic that’s been initiated from the Internet. </p>
<p>Now the NAT gateway itself is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, so you don’t have to provision the instance itself. It’s very easy to do, you simply create the NAT gateway, specify what subnet it should reside in, and associate an Elastic IP address, and AWS will manage all other configuration. Because it’s managed by default, AWS will set up multiple NAT gateways for resiliency, but you’ll only see the one NAT gateway within your account with the associated ID. </p>
<p>Now, earlier I mentioned about configuring your resources across Multi-Availability Zones. So if you have multiple public subnets in different Availability Zones, you will need to set up another NAT gateway within that subnet as well. AWS will not automatically deploy a NAT gateway within each of your public subnets. </p>
<p>So just as a quick summary, a NAT gateway allows instances within a private subnet access to the Internet, but the NAT gateway itself will block all incoming initiations from the Internet. So it protects the private subnet in that way. And this allows you to ensure that you maintain the security of your EC2 instances ensuring that their OS is kept up to date, and any patch management is taken care of as well. Now the next component I want to talk to you about is the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-bastion-hosts/">bastion host</a>. So let’s take a look.</p>
<h1 id="Bastion-Hosts"><a href="#Bastion-Hosts" class="headerlink" title="Bastion Hosts"></a>Bastion Hosts</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/security/securely-connect-to-linux-instances-running-in-a-private-amazon-vpc/">SSH Agent Forwarding</a></p>
<p><strong>Transcript</strong></p>
<p>In this section, I want to talk to you about bastion hosts. Now, consider a scenario where you might have EC2 instances sitting in a private subnet, but you want to be able to gain access to those instances from maybe your home office or from somewhere else on the internet. But because they’re sitting in the private subnet, how can you do that? Well, one of the ways you can do this is via a bastion host. </p>
<p>So let’s draw out our VPC configuration to allow me to explain how this works. So here, we have our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>. We’re going to have a public subnet and we’ll also have a private subnet as well. So this will be our public and this green one here will be our private. Now, obviously, we have an internet gateway attached as we have a public subnet, our IGW, and this connects out to the internet. We also have routes added to allow the subnets to talk to each other and also the public subnet to route out to the internet gateway as well. </p>
<p>Now, also, in the outside world, we have an engineer. Now, this engineer might be sitting at home in their home office in front of their laptop and what they need to do is to connect to resources sitting within the private subnet over here. Now, in this private subnet, we’re going to have a couple of EC2 instances. Now, we know it’s not possible to initiate an outside request to connect through to the internet down through to the internet gateway of our VPC and then across to our private subnet. It’s not possible. There aren’t routes to enable us to do that. Access isn’t allowed and it is private by design. </p>
<p>However, this engineer here needs to gain access to the EC2 instances to perform some maintenance or updates to those resources. Now, to enable you to do this, you need to use a bastion host. Now, this bastion host sits within the public subnet and this is just another EC2 instance. Now, this instance, to follow best practices, needs to be very secure. It needs to be hardened and very robust, but effectively, it needs to be tightened down to remove any kind of vulnerabilities and loose access controls. </p>
<p>Now, this EC2 instance is a part of a security group and this security group needs to be configured as shown. Now, what this security group shows is the inbound connectivity, and it allows SSH on port 22 from this IP address, which is from the engineer’s IP. So it’s being configured for this engineer over here. So this bastion host will essentially allow an SSH connection coming from our engineer over here. Now, that’s great because this engineer can then gain access to the bastion host here. And then, what that engineer can do is then use this as like a jump server and connect from the bastion host through to our EC2 instances here. </p>
<p>But before any of that can happen, we need to set up another security group for our EC2 instances here. So we’ll have another security group around our EC2 instances and this will be configured as shown. Again, this is the inbound rule set, and we can see that SSH is allowed on port 22 from this source here. Now, this source is actually a security group. It is prefixed with sg, which is security group, and this security group is actually this one here. This is associated with the bastion host. So what this is saying is any instances associated to this security group allow inbound SSH from any resource sitting within this security group, which as we know, is associated to our bastion host. So that will just allow the bastion host SSH access to these instances. </p>
<p>So now, we have our security groups set up and configured. However, let me just talk you through the connection process. So our engineer here will connect to our bastion host. Now, the engineer will be able to access the bastion host using the private key. So let’s just follow this process through. So the engineer will SSH to our bastion host, so it’ll connect via the internet. The connection will then come through the internet gateway. Let’s assume that any net calls that we have allow the access and we come to the security group here. Now, this security group says, allow connection if it’s an SSH connection from this IP address and this is the IP address of our engineer over here. So it allows access through. So now, this engineer has access to our bastion host. But now, our engineer needs to jump across to our private instances. Now, again, we’re going to need a private key to do that. </p>
<p>Now, one method would be to store the private keys on this bastion host and then run the command to SSH and access would be allowed, but that’s not best practice at all. We really don’t want to be installing private keys within the public subnet or on the bastion host because if this bastion host ever got compromised, then the malicious user will be able to use any private keys that are stored on the bastion host and connect to our private instances, which would be very bad. So how does this engineer SSH into our EC2 instances if he doesn’t have the private key? </p>
<p>Now, the best way to do this is to set up something called SSH agent forwarding. Now, what this allows us you to do is to store the private keys for the instances within the private subnet on your local client, so that when you connect through to the bastion host, you can then SSH, but using the private key to the EC2 instances that is stored on your client rather than storing it on the bastion host. Now, with that in mind, once you have connected to your bastion host, using the example that I just showed you, you can then SSH into your private instances, at which point, it will hit the private security group that allows any SSH access on port 22 from the security group associated with the bastion host and then there, you can gain access. </p>
<p>So just to summarize exactly what we’ve done here. We started off by creating an EC2 instance within the public subnet marked as our bastion host. We then hardened that instance to try and protect it against as many security threats as possible and to lock down access to that instance. We then associated a security group that only allowed SSH inbound access from a particular IP address or a particular range of IP addresses. We then added a rule to the security group associated to our private instances that allowed SSH inbound access from the bastion host security group. You then need to ensure that SSH agent forwarding is configured on your client and then this allows you to firstly connect to your bastion host using the private key of the bastion host and then using that as the jump server to jump into your private subnet from your bastion host using the private instances, private key, which is also stored on your client PC. </p>
<p>In the next section, I’m going to be coming away from the security aspects of VPC’s list and I’m going to be focusing more on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpn-direct-connect/">VPC connectivity</a>.</p>
<h1 id="VPN-amp-Direct-Connect"><a href="#VPN-amp-Direct-Connect" class="headerlink" title="VPN &amp; Direct Connect"></a>VPN &amp; Direct Connect</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-ipsec-vpns-understanding-building-and-configuring/">Amazon VPC IPSec VPNs- Understanding, Building and Configuring</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/">AWS Virtual Private Cloud: Subnets and Routing</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this section and I’m going to be talking about VPN connections, Virtual Private Networks. Now a Virtual Private Network is essentially a secure way of connecting two remote networks across the internet. So, let’s have a look how we can use VPNs within our <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>. </p>
<p>So, if we just create a VPC over here and we also have our remote data center over here. Just with a little door and this is our data center and this is our VPC. Now within our VPC, we’re going to have a single subnet. Now this is going to be a private subnet, so there’s no internet gateway and there’s no route to the internet gateway as well. It’s totally isolated. So, a bit more information relating to IP addressing, so our VPC will have a CIDR block of 10.0.0.0&#x2F;16 and for our data center, let’s say the IP address sits on a 192.168.0.0 address space. </p>
<p>Okay, so we have our VPC over here sitting in AWS and we have our remote data center over here, maybe sitting in London somewhere. Okay, now we have resources within our data center in London and we also have some resources over in our private subnet, for example, some EC2 instances. Now what we want to do is enable communications between our resources in our private subnet in our VPC in AWS and to resources that are held on premise within our data center. Now we want to do this via a secure connection. </p>
<p>So, one option is to create a VPN connection, a Virtual Private Network. Now let’s look at some of the components involved with that. Firstly, on your VPC side, you need to create something called a virtual gateway and this attaches directly to your VPC. Much in the same way an internet gateway does to enable public subnets access out to the internet and again, this is managed by AWS. So, here we have our virtual gateway. Now over in our data center, we also need another endpoint and this will be our customer gateway. And then this could be a piece of hardware or it can be a software virtual appliance, other way it need to be host within your data center. So, now we have an endpoint at our data center, the customer gateway and we also have an endpoint attached to our VPC, the virtual private gateway. </p>
<p>Now during the creation of our virtual private gateway, we’ll need to supply some additional information that’s going to be used in our customer gateway such as the customer gateway’s IP address and the type of routing to be used whether it’s dynamic or static. Now if you’re not familiar with dynamic or static routing, then please see our existing course shown on the screen and this dives into different types of routing across subnets and across site to site as well. So, that will give you a little bit in-depth information on how the routing would work. </p>
<p>Now once your virtual private gateway is attached to your VPC and configured and also your customer gateway’s installed, then what we can do is initiate a tunnel between the two endpoints. Now, this VPN tunnel can only be initiated from your customer gateway. It can’t be initiated from your virtual gateway. Now if there was some idle activity across this link for a period of 10 seconds or more, then this VPN tunnel connection would drop. So, to prevent that from dropping, you can set up network monitoring to set up continuous network pings from the customer gateway side to the virtual gateway to ensure that connection remains up and running. </p>
<p>So, now we have our VPN tunnel up and running created between our virtual private gateway and our customer gateway, we need to change the route table associated to this private subnet, so our EC2 instances know how to connect to the 192.168 network. So, let’s take a look at that. Now we can see here that we have the local route which we have with every route table as we know but we also have this additional route here. Now the destination is 192.168.0.0&#x2F;16 which points to our data center network and the target is this virtual gateway. Now we know that’s a virtual gateway ‘cause it’s prefixed with vgw and then this is the ID of the virtual gateway itself and this relates to our virtual gateway up here. So the instances within this subnet now have an additional route that points to this virtual gateway to get to the network of the data center. </p>
<p>What you can do is also enable route propagation within your route table as well. Now what this will do is once your VPN tunnel is up and running, then any routes that are represented across your VPN connection will be automatically added to your route table, so you might have other networks within your data center other than the 192.168 that are configured to use that VPN tunnel, so any traffic from another network received by your virtual gateway will allow these routes to be automatically propagated to the route tables that you’ve enabled route propagation on. Now depending on what sort of customer gateway you installed, will depend if it supports the BGP Protocol, which is the Border Gateway Protocol and if it does, then this supports dynamic routing, so this will populate all the routes for the VPN connection for you which means you won’t have to implement any static routing. Now it is recommended that if you can install a customer gateway that does support BGP, then it’s probably best to do so. </p>
<p>Now once our routes were in place, we also need to ensure we have our security groups configured for our instances as well to allow traffic to come from my resources over here and via the customer gateway across the VPN link to our virtual private gateway and then onto our instances but as we know, they are protected by a security group, so we need to ensure that the right protocols et cetera are allowed on the inbound rule set of our security group for our resources that are based over here. So, if we wanted to allow SSH access, for example, or RDP access, then the security group would look as shown. Now we can see that this security group allows both SSH and also RDP and it’s from the source 192.168.0.0 which is of course our network that we’re using on our data center. </p>
<p>So, to quickly recap. We have our virtual private gateway attached to our VPC and we have our customer gateway installed at our remote location. We then configure it with either dynamic or static routing and here we have a static route added for our subnet that points to the virtual private gateway within our VPC to get to our destination network which is of course our destination network of the remote data center. And then we also have our security group protecting our resources within our VPC allowing only specific ports and protocols which are inbound for my remote data center network. So, that’s just a simple example of a site-to-site connection using a VPN which is a secure connection across the internet. </p>
<p>I now want to talk to you about using another site-to-site connection called Direct Connect but this does not use the internet. This is totally isolated infrastructure. So, let me explain how this works. </p>
<p>Okay, so in this section, I’m going to be talking to you about Direct Connect. Now this is another method of connecting your remote location such as your data center or remote office to your AWS environment. Now whereas your VPN connection used the internet to get to your VPC, a Direct Connect connection doesn’t traverse the internet. Instead it uses private infrastructure and connects directly to your VPC. So, there’s no public network that the traffic traverses, so let’s look at the architecture of this to see how it works and how it’s different to a VPN. </p>
<p>Now I’m not going to go into fine configuration details on this, I just want to provide you a high-level overview of how the Direct Connect infrastructure is presented, so let’s take a look. So, let’s start with our on-premise data center that we’ll just over here. This would be our data center. And within our data center, we’ll have a router. Now with a Direct Connection, there’s a middle entity before you get to AWS infrastructure, now this is usually an AWS partner or an AWS customer that holds Direct Connect infrastructure and there’s two parts to this. </p>
<p>The first part is the partner’s infrastructure or the customer’s infrastructure and the other part will be managed by AWS. So, effectively we will have a customer side and also an AWS side as well. Now this is all held within a facility owned and managed by a partner of AWS. This is a separate building entirely to your remote data center. Now again in the customer side, there will also be a router and another router in the AWS side as well. Okay, let’s move on to the final section. So, here we have our AWS region because with AWS Direct Connect, it enables you to create a connection between your data center and an AWS region, not just a VPC, it’s actually connected to a defined region. So, this will be our region here. And within that region we also have our VPC as well. So, this is our VPC and again, within our VPC we’ll have a subnet with perhaps an EC2 instance in it, for example. </p>
<p>Now the reason it’s connected to a region and not a VPC is that a Direct Connect connection allows you to access public as as private resources, so an example of a public source could be Amazon S3 and that’s because Amazon S3 resources can be accessed over the internet via a public connection. Now attached to our VPC, we’ll also have a virtual gateway much like we did when we was talking about our VPN connection. </p>
<p>Okay, let me just recap the three elements that we have here before we go any further. So, we have our customer data center over here with a router. Now here in the middle we have our Direct Connect location, so this is our Direct Connect location. And this sits between our on-premise data center and our AWS infrastructure and this is separated into two cages effectively. We have our customer partner router and we also have the AWS router as well. And then finally we have our AWS infrastructure over here with our region and inside the region, a VPC and we have our public components over here. Let’s just nest that in the AWS cloud. So, that all sits within AWS. </p>
<p>Now I mentioned previously that you can have a private connection and also a public connection and as a part of that configuration, you can configure private virtual interfaces and also public virtual interfaces on your router. So, let’s take a look. So, there’ll be two virtual interfaces. So, one of them will be a private virtual interface and we’ll define this by using this gray line here. So, that connects from your on-premise router to the customer side of the Direct Connect location. Now from here there’ll be a cross connect from the customer router to the AWS router within the same Direct Connect location. And then from here, this virtual private interface will then connect to your virtual gateway. Then this will allow connectivity through to your resources within your VPC. </p>
<p>Now the second interface is a public virtual interface, so let’s use this reddish color for that. So again the connection comes from your on-premise router into the customer side of the Direct Connect location, then there a cross connect across to the AWS router and from here it connects to inside of your AWS region and from here you can access your public AWS resources such as Amazon S3 et cetera. So, now we’ve established a connection from our on-premise data center into a region within AWS where we can access both private resources and also public resources and it’s all done without having to traverse the public internet. Instead there’s dedicated and isolated infrastructure using the Direct Connect locations. Now to be able to use Direct Connect, the only path that you need to establish is from your on-premise data center to a Direct Connect location to enable you to establish a connection to this customer router here. So, as long as you have a dedicated network route to a co-location that provides a Direct Connect connection, then you can establish this dedicated network that we can see here. </p>
<p>Now the great thing when working with Direct Connect is that it’s private connection and also you get speeds from 1 through to 10 gigabits per second. Okay, the final section I want to talk to you about in this course is relating to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpc-peering/">VPC peering</a> and also the Transit Gateway, so let’s take a look at this final section.</p>
<h1 id="VPC-Peering"><a href="#VPC-Peering" class="headerlink" title="VPC Peering"></a>VPC Peering</h1><p>In this section, I want to talk to you about VPC peering. Now we’ve looked at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpn-direct-connect/">VPN connectivity</a> which looked at connecting your on-premise data center or remote office to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a> and also Direct Connect which done the same thing but over an isolated network. Now with VPC peering, it’s relating to connectivity again but what it allows you to do is connect two VPCs together. </p>
<p>So, we have one here and another VPC here. Now each of these VPCs will have resources in them. EC2 instances or databases, et cetera and what we want to allow to happen is for these two VPCs to be able to communicate with each other. Now these VPCs might be in the same region to they might be in different regions. Either way we can allow VPC peering to allow them to communicate with each other. Now the peering connection itself here that links the two VPCs is actually run and hosted on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> infrastructure. So, this is highly resilient, there’s no single point of failure and also there’s no bottlenecked bandwidth either. So, it’s a very good way of linking two VPCs together to allow you to exchange information and for each VPC to communicate with another. </p>
<p>Now you might have multiple VPCs for organization or management and there will be times when you want resources in one VPC to communicate with another. And a quick and simple solution is to implement VPC peering. Now it’s important to mention that this peering connection is a one-to-one connection only. So, if we had a third VPC down here, call this one VPC-3, again this had additional resources in as well and you had a VPC peering connection between two and three, resources in VPC-1 could not go via VPC-1 and then through VPC-2 to get through to VPC-3. That simply is not allowed as it’s a one-to-one connection only. If you wanted VPC-1 to connect to VPC-3, then you’d have to set up a separate VPC peering connection between one and three. So, that’s a very important point when mapping out your peering connections between your VPCs. </p>
<p>Now another important point relates to IP addressing, so for example, if VPC-1 had an address of 10.0.0.0&#x2F;16, VPC-2 was 172.31.0.0&#x2F;16 and then VPC-3 was also 10.0.0.0&#x2F;16, then this connection here would not be possible because when you create VPC peering connections, each VPC cannot have an IP address overlap between them and these two VPCs have the same IP addressing scheme, so this VPC connection would not be possible. So, that’s also something else to bear in mind when creating your VPC peers. So, let’s take that connection away. </p>
<p>Now I also mentioned that you can have VPC peering configured between the same region or between different regions. So, let’s say VPC-1 and VPC-2 was in one region and VPC-3 was in another region. Then this link here would be an inter-region VPC connection. Let me now run through the process of how this peering connection is initiated. </p>
<p>So, let me just get rid of what we have on the screen here and start again. So, we have two VPCs. Our first one and also our second one. VPC-1 and VPC-2. Now VPC-1 is going to be known as the requester and VPC-2 is going to be known as the accepter. Now the owner of VPC-1 needs to send a VPC peering request to the owner of VPC-2. And again, remember, we need to make sure that the CIDR blocks of these VPCs do not overlap, so that request comes across to the VPC accepter and that’s the first stage. If the VPC accepter is happy with that peering connection, then an acknowledgement and acceptance of that request is sent back to the requester and that’s the second stage and this creates the peering connection between the two. At this stage, each VPC needs to update their routing tables to allow the traffic from VPC-1 to get to the destination of VPC-2. </p>
<p>Now to do this, we need to know the CIDR blocks of these VPCs. So, let’s assume VPC-1 is 10.0.0.0&#x2F;16 and VPC-2 is 172.31.0.0&#x2F;16. So that are two CIDR blocks that we have for our VPCs and as we know, they’re not overlapping, so from an IP perspective, there’s no issues there. So, now let’s look at the route table for each of these. So, firstly, VPC-1. As we can see, we always have our local route and then we also have this additional route here. So, the destination 172.31.0.0&#x2F;16 which is VPC-2 to go via the target of this peering connection. And the pcx simply means that this target is a peering connection. And these digits here are the ID of that peering connection. Now this VPC knows how to get to the 172.31 network by going via the peering connection here. </p>
<p>So, let’s now look at the route table for VPC-2. Again, we have our local route which every route table has and then also this additional route that points to VPC-1 again across the same peering connection. So, this VPC can now access the network of VPC-1 again via the same peering connection. </p>
<p>Now the final part of the configuration would be to modify the security groups that are hosting any resources within your VPC. So, you might have a security group here and a security group here each with EC2 instances or databases and we’ll simply need to update the rules to allow the correct resources, ports and protocols to communicate with each other. </p>
<p>So that’s a high-level overview, that is VPC peering. Now what I want to talk to you about is the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-transit-gateway/">AWS Transit Gateway</a> and again, this looks at how to connect more than one VPC together but through a one-to-many connectivity method, so let’s take a look at that.</p>
<h1 id="Transit-Gateway"><a href="#Transit-Gateway" class="headerlink" title="Transit Gateway"></a>Transit Gateway</h1><p>So, the final element I want to talk to you about is the AWS Transit Gateway. And this is essentially a development on from the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpc-peering/">VPC peering</a>. In today’s world we’re using more and more <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPCs</a> to segment and manage different workloads and as our organization gets bigger and bigger, we’re creating more and more VPCs, we have more and more connections from our remote locations such as our data centers and offices, et cetera and creating VPC pairing connections to each one of these bearing in mind it’s a one-on-one connection can be very cumbersome and time consuming and just not very well to manage. </p>
<p>So, let’s say we had four VPCs represented by these circles here. And we also had a couple of remote offices as well. So, one there and one there. Now if we wanted to connect these VPCs into our office locations, now based on what we’ve already spoken about so far, we can use VPC pairing to link our VPCs together. But as we know, this is just a one-one-one connection, so we also need a connection across there and also a connection across there. So, we have one, two, three, four, five, six VPC pairing connections there. Now one of these remote locations might be using a VPN connection to get to that VPC, and also a VPN connection there and maybe even a third VPN connection to this VPC as well and this remote location might be used in Direct Connect to get to a couple of different VPCs in different regions. Now, that is a lot of connections and a lot of gateways to manage. We have customer gateways at the remote ends and also private gateways within our VPCs as well. </p>
<p>What AWS Transit Gateway allows you to do is to connect all of this infrastructure, so all of your VPCs, all of your remote locations, whether it’s over Direct Connect or VPN via a central hub. So, let’s take a look at how that looks. So, again we have our four VPCs and also we have our two data centers here at the bottom, our two remote locations. However, this time, we have the AWS Transit Gateway in the middle. Now, for each VPC or remote location that we want to allow to talk to each other, then all we need to do is to create a single connection to the Transit Gateway, so one from each of the VPCs and also one each from the remote locations as well. Again, these will be a VPN connection and maybe a Direct Connect connection. So, either way, VPN, Direct Connect or VPC, they all connect to this central hub, this AWS Transit Gateway. </p>
<p>As you can see between the two designs, this one over here has a lot more connections than this one over here. So, the AWS Transit Gateway simplifies your whole network connectivity. It allows all of your VPCs to easily communicate with one another and also communicate with your remote locations as well. All the routing is managed centrally within that hub and when any new remote locations or VPCs are created, for example, you might have another two VPCs created, all you’d need to do is to connect it to the AWS Transit Gateway and each of these new VPCs can then communicate with the entire rest of your infrastructure. </p>
<p>Now because the Transit Gateway goes through this central hub, it allows you to centralize all your monitoring as well for your network traffic and connectivity all through the one dashboard which is great. So, that was just a very quick high-level overview of AWS Transit Gateway and how it differs from the VPC pairing.</p>
<h1 id="Elastic-IP-Addresses-EIPs"><a href="#Elastic-IP-Addresses-EIPs" class="headerlink" title="Elastic IP Addresses (EIPs)"></a>Elastic IP Addresses (EIPs)</h1><p>Hello and welcome to this lecture covering an overview of Elastic IP addresses, known as EIPs.</p>
<p>When architecting your infrastructure from a network perspective, you might have both public and private IP addresses. Your public IP addresses will be reachable from the internet from within a public subnet, whereas your private IP addresses will be hidden in a private subnet.</p>
<p>When launching an EC2 instance, you can select the subnet that it will reside in, and if you want EC2 to auto-assign a Public IP address. If you select Enable, then your instance will be launched with one of AWS’ public IP addresses from their pool of available public addresses.</p>
<p>If this auto-assign option is selected, then that public IP address will remain with that instance until it is stopped or terminated, at which point it will be removed from your instance. However, there will be times when you need a persistent IPv4 public IP address that you need to have associated with your instance, which is exactly what an Elastic IP Address provides.</p>
<p>When you create a persistent elastic IP address, the IP address is associated with your account rather than an instance. This means you can attach an EIP address to an instance or an Elastic Network Interface, an ENI, and even if you stop the instance its associated with, the same EIP will remain in place. You can also detach the EIP from an instance and re-attach it to another instance. However, do bear in mind that when you detach an EIP and it’s not associated with a running instance, then you will incur a cost for it. If you no longer need the EIP, you must detach it from the associated instance and release it back to <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>.</p>
<p>If you associate an EIP to an instance that already has a pooled public IP address, then that pooled public address will be released and put back into the pool, and your instance will take on a new EIP address. It’s also worth mentioning that you can’t convert an existing pooled public IP address to an EIP.</p>
<p>I will now provide a quick demonstration, and I will start by creating a new EIP within my account. I will then associate this EIP to a running instance in my VPC that already has a public IP address. And I will then confirm that the instance has the newly associated public IP address. I will then detached this EIP and release it back to AWS.</p>
<p>Let’s take a look. So I’ve logged into my AWS management console, and I have an instance running here. Now if we look at the settings of this instance, we can see that it has currently a public IP address, 3.9.177.89. So this is sitting in a public subnet at the moment. And this IP address is just allocated out of AWS’ public IP address pool. It’s not an elastic IP address. If it was then you’d see an entry under this section here.</p>
<p>Firstly, what I want to demonstrate is that these public IP addresses that are just allocated from the pool are not persistent. So if I stop this instance and then restart it, we’ll see that it will have a different public IP address. So this currently has 3.9.177.89. So if I go ahead and stop this, that’ll just take a moment to stop. Okay, that’s now stopped, we can see that it’s released the public IP address. And if I start this instance again, and we’ll see what IP address it has this time.</p>
<p>Okay, that’s now back up and running. And we can see that it is a totally different public IP address. So it just goes to show when you stop and start your instances using one of the publicly assigned IP addresses from AWS, that is not an elastic IP address, then that IP address is going to change each time. Now let me go ahead and create an elastic IP address to show you how to do that. Now on the left hand side, under network and security, you will see elastic IPS. So if you select that, at the moment, I don’t have any elastic IP addresses, so all I need to do is select Allocate new address.</p>
<p>Now I can select an elastic IP address that is owned by Amazon or one that’s owned by myself. For this demonstration, I’m going to be using an Amazon-owned address, and select Allocate. Okay and we have our elastic IP address 3.11.45.130. So now we have that here, but at the moment, it’s not actually assigned to any instance. When you have an elastic IP address created and it’s not associated to an instance, it’s going to cost you money. So you want to make sure that you either release any unassociated elastic IP addresses back to AWS, or associate it with an instance. To associate it with an instance, select Actions, and then Associate address.</p>
<p>Now here, you can either associate it to an instance or a network interface. For this demonstration, I’m going to associate it to a running instance. Now the instance that we was looking at just a moment ago, was called public instance. And as we can see that is currently running. Now also have to associate a private IP address to associate to the IP as well, so it can communicate internally with the rest of the subnets on your VPC.</p>
<p>So this is the private IP address that was running on our instance, so we’ll select that. And now I just need to select Associate. So our EIP is now associated to a running instance. So let’s go and take a look. So if we go back to our instances, we can now see that the public IP address is our EIP address, which is 3.11.45.130. And we can also see here that it’s allocated the elastic IP address which is the same.</p>
<p>So to associate an EIP with an already running instance is very easy, it just replaces the existing public IP address that it had previously. Now if I stop this instance and restart it, we’ll see that it maintains this same elastic IP address, 3.11.45.130. So let me just demonstrate that now. So I’m gonna stop this instance.</p>
<p>Okay, that’s now stopped. And we can see here that it’s still maintaining this elastic IP address, whereas previously with the general pooled public IP address, this cleared, this entry cleared when we stopped the instance. So if we start this instance again, and we can see that it’s retained and persisted this public IP address. So it behaves very differently.</p>
<p>Okay, so now what I’m going to do is disassociate this elastic IP from this instance. So if I go back to my elastic IPs, we can see our elastic here and we can see that it’s associated to our instance, if I go to Actions, Disassociate address, and it just gives us information on the instance ID and the network interface that it’s going to disassociate it from.</p>
<p>So if I go ahead and disassociate that, we can now see that it’s not related to any instance at all. And if I go back to my instance, we can see that the elastic IP address is now gone, and instead it had pooled a general AWS public IP address instead. Which as we know, is not persistent when we restart and stop the EC2 instance. However, now I have an elastic IP address that isn’t being used.</p>
<p>So again, this is costing me money. So now I want to release it back to AWS ‘cause I no longer need it. So again, I select the EIP, select Actions, and Release address. Then we just have a confirmation message, and I select Release. And it’s as simple as that.</p>
<h1 id="Elastic-Network-Interfaces-ENIs"><a href="#Elastic-Network-Interfaces-ENIs" class="headerlink" title="Elastic Network Interfaces (ENIs)"></a>Elastic Network Interfaces (ENIs)</h1><p>Hello and welcome to this lecture, which is going to look at elastic network interfaces, which are commonly known as ENIs.</p>
<p>ENIs are logical virtual network cards within your virtual private cloud, your VPC, that you can create, configure, and attach to your EC2 instances. The configuration is bound to the ENI and not the instance that it is attached to. This means that you can also detach your ENI from one instance, and reconnect it to another instance and the configuration of that ENI would move with it. For example, a private IP address or an elastic IP address or it’s MAC address.</p>
<p>You may not have come across ENIs before, because when you create an instance your EC2 instance comes configured with a primary network interface that is already bound to your instance. And this can’t be removed or detached. If you look at your EC2 instances, you’ll see this primary interface labeled Eth0.</p>
<p>However, there will be occasions where you will need your instances to have multiple network interfaces. For example, if you wanted to create a management network, and in this instance, you can create and use an ENI to attach to your instance in addition to its primary interface of Eth0. This second interface can then be configured with a private IP address to handle any management traffic from within a different subnet.</p>
<p>Much like your Eth0 interface, all traffic originating from and being sent to an ENI can be captured using VPC flow logs. More information on VPC flow logs can be found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">here</a>.</p>
<p>When designing your solution and any requirements for multiple interfaces being attached to your instances, you’ll need to bear in mind that the quantity of interfaces is dependent on the EC2 instance type. To check how many interfaces can be attached to your instance, please check the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">AWS documentation</a>.</p>
<p>Let me now provide a quick demonstration on how to create and attach an ENI to existing EC2 instance.</p>
<p>In this demonstration, I have two private subnets within my VPC. These are labeled as production and management. I have an existing instance called Myinstance, within the production subnet. However, I want to implement a management network that will reside within the management subnet. As a result, I will create a new ENI and configure it for the management subnet, and then attach it to my instance. Let’s take a look. </p>
<p>Okay, so I’m logged into my <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Management Console, and I’m on my VPC dashboard. I’m just looking at my subnets, and I just want you to see the different subnets I’ve got for this cloud Academy VPC. The two subnets that I’m interested in are the production and the management. So for the production subnet, we have a 10.0.2.0 network. And for the management network, we have a 10.0.3.0 network.</p>
<p>Now I also have an EC2 instance. So if I just go across to EC2, I have an instance called Myinstance. And at the moment, this is residing in the 10.0.2.0 network, which is in the production subnet. Now what I want to do is to add a secondary network interface to this instance, with an IP address that sits on the management network. So at the minute, we can see that it only has this primary Eth0 interface. And we can see that this is the primary network interface, and it has a 10.0.2 address.</p>
<p>So let me go ahead and create a new network interface. So on the left hand side, if you scroll down to Network and Security, and select Network Interfaces, and then Create Network Interface, and add a description here, so I’m going to call it my Management_Interface. And the subnet that I want to associate this with is the management subnet. And I can either auto assign or add a custom IP address. For this demonstration, I’m just going to leave it as auto assigned.</p>
<p>An elastic fabric adapter is a network device that you can attach to your instances to reduce latency and increase throughput for distributed high performance computing and machine learning applications. So we don’t need to do that for this demonstration, but I just wanted to show you what the elastic fabric adapter is. And finally you can select any security groups as well. I’m just gonna select a default security group for this demonstration. Select Create, and we now have our new network interface created.</p>
<p>And we can see here under the Description, that it’s the Management_Interface that I named it. And currently the status is available is naturally in use. So I’ve literally just created a network interface. And if we look at the properties down here, and we can see that it resides on the 10.0.3 network, which is correct. So now what I want to do is attach this to our instance.</p>
<p>So once I’ve selected the interface, if I go to Actions, Attach, select the instance, which is Myinstance, which is running and then select Attach. That interface will then be attached to that EC2 instance. And we can see here that it’s now in use. So if we go back to Myinstance, and select it, so we can take a look at the properties, we can see down here that we now have a secondary network interface. So the Eth0 was the primary that sits on the 10.0.2 network. And that’s the primary network interface. And now if we look at this interface Eth1, this is our new management interface. And we can see that this sits on the 10.0.3 network. So this EC2 instance now has network interfaces connecting to two different subnets and one of those subnets is the management network.</p>
<p>Now to detach the interface is very easy. We can go back to our network interfaces section, select the Interface and select Detach. We get a confirmation message asking if we want to detach it, we say yes for the detachment. Once that interface is detached, we can then delete that interface as well if we no longer need it. And again, just to reiterate, we can see that the network interface itself retains its configuration. So it’s brought the IP address with it. So finally, let me just delete this interface by selecting Delete. Say yes, and that’s it.</p>
<h1 id="EC2-Enhanced-Networking-with-the-Elastic-Network-Adaptor-ENA"><a href="#EC2-Enhanced-Networking-with-the-Elastic-Network-Adaptor-ENA" class="headerlink" title="EC2 Enhanced Networking with the Elastic Network Adaptor (ENA)"></a>EC2 Enhanced Networking with the Elastic Network Adaptor (ENA)</h1><p>Hello and welcome to this lecture which will take a look at how to enable enhanced networking features on your EC2 instances with the Elastic Network Adapter (ENA), which is a custom interface used to optimize network performance.</p>
<p>If you are looking to enable enhanced networking features to reach speeds of up to 100 Gbps for your Linux compute instances, then you can do so using an ENA. However, ENAs are only supported on a limited number of instances as shown below, and by instances running kernel versions 2.6.32 and 3.2 and above.</p>
<p>For an up to date list of supported EC2 compute types please visit the following link to <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html#enabling_enhanced_networking">AWS documentation</a>.</p>
<p>In addition to 100 Gbps speeds, enhanced networking offers higher bandwidth with increased packet per second (PPS) performance, and a big bonus of enhanced networking is that it is offered at no extra cost. In fact, when launching an instance using Amazon Linux 2 or with the latest version of the Amazon Linux AMI, then the instance will have enhanced networking enabled by default, providing its provisioned with one of the supported instance types mentioned earlier.</p>
<p>Enhanced networking is enabled when the ena module is installed on your instance and that the enaSupport attribute is set. If you wanted to confirm that the ena module is installed on your instance then you can run modinfo ena from the terminal prompt. To check that the enaSupport attribute is also set you can use the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> CLI and run the following command, replacing the red text (“instance_id”) with the appropriate instance_id:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws ec2 describe-instances --instance-ids instance_id --query &quot;Reservations[].Instances[].EnaSupport&quot;</span><br></pre></td></tr></table></figure>

<h1 id="VPC-Endpoints"><a href="#VPC-Endpoints" class="headerlink" title="VPC Endpoints"></a>VPC Endpoints</h1><p>Hello and welcome to this lecture covering VPC Endpoints.</p>
<p>VPC Endpoints allow you to privately access <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services using the AWS internal network instead of connecting to such services via the internet using public DNS endpoints. This means that you can connect to the supported services without configuring an Internet Gateway, NAT Gateway, a Virtual Private Network or a Direct Connect connection.</p>
<p>There are 2 types of VPC Endpoints: Interface Endpoints and Gateway Endpoints.</p>
<p>Interface Endpoints are essentially ENIs that are placed within a subnet that act as a target for any traffic that is being sent to a supported services and operates through the use of PrivateLink. PrivateLink allows a private and secure connection between VPCs, AWS services, and on-premises applications, via the AWS internal network.</p>
<p>As an example of supported services, this list only shows a very small subset of what’s available via an Interface Endpoint.</p>
<p>One point to make is that when an interface endpoint is configured within your chosen subnet, the service that it is associated with is NOT able to initiate a connection through to your VPC, communication across this interface HAS to originate from within your VPC first before a response can be made by the service.</p>
<p>You might be wondering how you connect and make use of the endpoints, and the process is seamless to the end user when working with AWS services. When an interface endpoint is created for a service, a specific DNS hostname is created and is associated with a private hosted zone in your VPC. Within this hosted zone a record set for the default DNS name of the service is created resolving to the IP address of your interface endpoint. As a result, any applications using that service already does not need to be reconfigured, requests to that service using the default DNS name will now be resolved to the private IP address of the interface endpoint and will route through the internal AWS network instead of the internet.</p>
<p>A Gateway Endpoint is a target that is used within your route tables to allow you to reach supported services, currently the only supported services using a Gateway Endpoint are Amazon S3 and DynamoDB, but this like is likely to change over time to please ensure you check the latest supported services.</p>
<p>During the creation of your Gateway endpoint you will be asked which route tables within your VPC should be updated to add the new Target of the gateway endpoint. Any route table selected with then have a route automatically added to include the new Gateway Endpoint. The entry of the route will have a prefix list ID of the associated service (Amazon S3 or DynamoDB) and the target entry will be the VPC Endpoint ID, examples of these are shown on screen. You should also be aware that GateWay Endpoint only works with IPv4.</p>
<h1 id="Working-With-Amazon-CloudFront"><a href="#Working-With-Amazon-CloudFront" class="headerlink" title="Working With Amazon CloudFront"></a>Working With Amazon CloudFront</h1><p>In this section, we will review the purpose of Amazon CloudFront and its key features. The main role of CloudFront is caching of content. Caching allows us to store our content closer to the users that need it. If you have a website hosted in the EU-West-2 region, but a lot of your customers are in the US or Australia, they will have higher latency when compared to the users in the UK. But if we cache content closer to them, on 8-west edge locations in the US and Australia, their latency will be reduced. Amazon CloudFront allows customers to distribute content with low latency and high speed. Amazon CloudFront is a pay-as-you-use service. And when using CloudFront, files are delivered to end-users via a global network of edge locations. </p>
<p>CloudFront works with both static and dynamic content. For example, static content stored in Amazon S3 buckets. These static stores hold the definitive version of files. Dynamic content stored on Amazon EC2 or served up using Lambda functions. This content is generated on the compute resource and distributed through Amazon CloudFront. When working with CloudFront, you first create a CloudFront distribution. During this process, you identify one or more origins for the content that this distribution will serve the clients. You also configure options that control protocols that can be used such as HTTP or HTTPS; cache time to lives; custom headers; a price class, where its use all edge locations or a subset of locations; AWS WAF web ACL associations; alternate domain names; custom SSL certificates, and more. When creating a CloudFront distribution, you’re assigned a domain name.</p>
<p> For example, 1234.cloudfront.net. Although you can use this domain name, most customers of Amazon CloudFront will add an alternate domain name to their distribution. A name such as cloudacademy.com is the mat to the CloudFront assigned name using DNS. In the following demonstration, we already have an Internet application load balancer load balancing traffic to our website. We will create a CloudFront distribution to cache our website content globally. We are in the CloudFront distribution center dashboard. To create a distribution, we select Create distribution. We then select our origin domain. The origin can be an AWS origin or we just type in the domain name of the origin that we wish CloudFront into cache. If I click in the ‘Choose origin’ domain box, we can see a list of valid ADS origins, including S3 buckets and our application load balancer. </p>
<p>I’m going to select the application load balancer. With the load balancer selected, you then get to choose protocol information and port information that the distribution will use when connecting to the load balancer. If I scroll down a little bit, we get to choose an origin name. We can accept the default name for the origin or choose a name that’s more meaningful for us. There is a lot of optional information we can select. A lot of these settings are discussed later on in this course. If I scroll down a little bit, we can find settings that allow us to configure cache behavior. If I scroll down a little bit more, we find the pricing class. The pricing class allows us to choose groupings of edge locations that our distribution will use to cache content. We can choose to associate our WAF ACL with our distribution, and we can select an alternate domain name for our distribution. </p>
<p>Most deployments will use an alternate domain name, so that we can use our own nice friendly DNS names instead of having to rely on the CloudFront DNS name. If you choose to use an alternate domain name, you’ll also need to select a digital certificate. The digital certificate can be imported from your own certificate stars, or we can search certificate from Amazon certificate manager. If I scroll down a bit more, we can enable standard logging file distribution, so that we can log viewer requests into an S3 bucket. You can also turn on or off support for IPv6. If you’re happy with our choices, select Create distribution. Once you’ve selected distribution, you should see a message saying that your distribution is being deployed. </p>
<p>Although we often discuss CloudFront as a single cache, actually CloudFront has three cache in layers. Cloudfront distributions, these exist over 300 Amazon edge locations globally. Regional edge caches, and at the time of writing there are 13 regional edge caches. And AWS Origin shield, an additional cache in layer between your regional edge caches and the origins. Origin shield is not enabled by default. You must enable it for each origin in the distributions you create. By having multiple cache layers, you can cache more content for longer. Using regional caches and origin shield, you get better cache hit ratios. Because more of your content is cached, there is a much better chance that the content your customers need will be retrieved from cache. Reduced origin load: With more content being served from cache, less requests are sent to origins. </p>
<p>And when using origin shield, requests for the same object not in cache are consolidated, so only a single request is sent to the origin. Better network performance: Using multiple layers, content can stay on the AWS Lola into network for longer. CloudFront has a long list of security features. These include CloudFront use of SSDs, which are encrypted protecting your data at rest. We can use signed URLs and cookies to restrict access to content that is intended for specific users. We can use AWS WAF to create web ACLs to restrict access to content, and we can use geo restrictions to prevent users in certain regions from accessing content. For more information on AWS WAF, please refer to our existing class content here. Amazon CloudFront itself integrates with identity and access management, which we can use to control administrative access to CloudFront. And CloudFront can be monitored through integration with: Amazon CloudWatch alarms, AWS CloudTrail Logs, and CloudFront real-time IAM standard logs.</p>
<h1 id="Amazon-CloudFront-Patterns"><a href="#Amazon-CloudFront-Patterns" class="headerlink" title="Amazon CloudFront Patterns"></a>Amazon CloudFront Patterns</h1><p>In this section, we will discuss two examples of using CloudFront to cache and secure access to content.</p>
<p>Pattern 1 – Using CloudFront to cache and secure content when an Application Load Balancer is the Origin.</p>
<p>In this pattern we have an Application Load Balancer, load balancing HTTP port 80 and HTTPS port 443 traffic to a set of EC2 instances hosting a website. To integrate with CloudFront we would create a CloudFront distribution with the application load balancer as the origin. It is worth remembering that CloudFront can work with any public endpoint whether AWS or external. This Load Balancer will be an internet-facing load balancer.</p>
<p>For this pattern to work, we will need to do a couple of things.</p>
<ol>
<li>Work with Route 53</li>
<li>Secure connections to the application load balancer</li>
</ol>
<p>Originally the nice friendly name for your website will be mapped to the name of the application load balancer, but now we want the friendly name of your website to route people through CloudFront. For this to work, we need to edit the Alias record in Route53 to map the friendly name to the DNS name that CloudFront has provided you when you created your distribution. Remember we are using an internet-facing load balancer, so if someone knows the DNS name of the load balancer or its IP addresses then they might be able to connect to load balancer directly, bypassing your CloudFront distribution and the performance and security benefits it provides. To secure the connection between our CloudFront distribution and the Application Load Balancer we can do the following:</p>
<p>Firstly, Configure CloudFront to add a custom HTTP header to requests. When creating or editing your CloudFront distribution add a custom header. The custom header will be included in each request sent to the origin. Next. Configure an Application Load Balancer to only forward requests that contain a specific header. To do this we need to select the listeners configured on the load balancer and edit its rules. We then adjust the rules:</p>
<p>Rule 1: if the custom header is included in the request forward traffic to the target group. </p>
<p>Rule last: If the request does not include the custom header then return a 403 error.</p>
<p>With everything saved, if someone tries to access your website using the Application Load Balancers hostname or IP Addresses they should see a 403 error similar to the error below. By using HTTPS for origin requests we are configuring an encrypted connection between our distribution and the Load Balancer. This will help keep your custom header secret. This setting is configured on your CloudFront distribution and requires the appropriate secure listener to be configured on your Application Load Balancer. It is important you rotate the custom header in your distribution and on the load balancer. If you do not rotate the custom header then over time there is a greater chance that the header name and value will be discovered undermining your security.</p>
<p>To do this:</p>
<ol>
<li>Add a second custom header to the CloudFront distribution</li>
<li>Add a new rule to the load balancer listener that uses the new custom header as the condition and forwards traffic to the target group</li>
<li>Remove the old custom header and rule from the cloudfront distribution and the load balancer listener</li>
</ol>
<p>Pattern 2 – Using CloudFront to cache and secure content when an S3 bucket is the origin</p>
<p>Using S3 to deliver content is a common feature of many AWS Architectures, in S3 content is stored in buckets and buckets are in a single region. Because of this your applications can suffer from latency issues if you have customers globally who need to access content from a centralized bucket. CloudFront can help solve this problem by allowing content to be cached closer to the users who need to access it. We can also use S3 to host a website, this has lots of advantages, one of which is we no longer need compute resources such as EC2 to host our static content. When using S3 to host static content we:</p>
<ul>
<li>Create a bucket and enable it for static website hosting</li>
<li>Then Enable public read access on the bucket</li>
</ul>
<p>You will be given a URL that can be used to access your index document, S3 only supports HTTP access when using static website hosting. We have several problems here.</p>
<ol>
<li>What if you wish to use HTTPS instead of HTTP?</li>
<li>What if you don’t or can’t allow public access to content?</li>
<li>What about latency? Our website is still stored in a bucket in a single region</li>
</ol>
<p>CloudFront can solve these issues.</p>
<ul>
<li>CloudFront can distribute your content globally reducing latency for your customers.</li>
<li>When we create a CloudFront distribution we can add an alternate domain name and a certificate allowing us to use or even enforce HTTPS. </li>
<li>We can also remove public access to the S3 bucket and secure access so that only the CloudFront distribution can access content.</li>
</ul>
<p>To enable HTTPS you will need a digital certificate. AWS provides the AWS Certificate Manager service (ACM), through which we can request digital certificates for free. Using ACM we can request internal certificates or trusted public certificates. If you are planning on integrating your CloudFront distribution with AWS Certificate Manager (ACM) please keep in mind that CloudFront works with ACM in the North Virginia region. It is from here that you should create your certificate requests.</p>
<p>To make sure that only your distribution can request content from your S3 bucket and to allow you to remove the public access that static website hosting needs, we use Origin Access Identity (OAI) and S3 Bucket Policies. This process is important, without it users could still use the bucket’s URL to access your content, by passing the security and performance benefits of CloudFront.</p>
<p>Begin by creating an Origin Access Identity (OAI) in cloudfront and then associate it with your CloudFront distribution. Here you can see an OAI that has been created in the CloudFront console, notice the Name and ID. Next, you can see that we have associated the newly created OAI with our distribution</p>
<p>You can then select the option to automatically update the bucket policy of the S3 bucket or you can edit the bucket policy yourself. Here we can see the bucket policy that would be created on the S3 bucket that contains our content. We can see that the principal is the ARN of the OAI and that the effect is set to allow. You can adjust the actions as required, here we are allowing the S3.GetObject action on the bucket. </p>
<p>Finally, notice that public access is blocked. If someone was to obtain the bucket url and tried to access content using the url they would receive an access denied message. But if they use your friendly name mapped to the CloudFront distribution DNS name then access will be allowed. Here is an example of how that record might look in Route 53, we use an alias record mapped to the distribution DNS name.</p>
<h1 id="AWS-Global-Accelerator"><a href="#AWS-Global-Accelerator" class="headerlink" title="AWS Global Accelerator"></a>AWS Global Accelerator</h1><p>Hello and welcome to this lecture covering the AWS Global Accelerator, which is a Global <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service and therefore not tied to a specific region.</p>
<p>The ultimate aim of the AWS Global Accelerator is to get UDP and TCP traffic from your end user clients to your applications faster and quicker and more reliably, through the use of the AWS global infrastructure and specified endpoints, instead of having to traverse the public internet, which is not as reliable and carries a higher security risk.</p>
<p>Global Accelerator uses two static IP addresses associated with a DNS name which is used as a fixed source to gain access to your application which could be sitting behind a load balancer, such as a network or application load balancer, or directly connected to your EC2 instance or the Elastic IP address. These IP addresses can be mapped to multiple different endpoints, each operating in a different region if a multi-region application is deployed to enhance performance of routing choices.</p>
<p>Because the routing of your request is based across the AWS Global Infrastructure, Global Accelerator intelligently routes customers requests across the most optimized path using its global reach of edge locations, for the lowest latency and avoids any resources that are unhealthy. This helps to improve regional failover and high availability across your deployment.</p>
<p>To set up and configure AWS Global Accelerator there are effectively four steps to follow.</p>
<p>Firstly, you must create your accelerator and give it a name. You must also select if you want to use two IP addresses from AWS’ pool of IP addresses or use your own. For each accelerator created, you must select two IP addresses.</p>
<p>Next, you need to create a listener. The listener is used to receive and process incoming connections based upon both the protocol and ports specified, which can either be UDP or TCP based.</p>
<p>Once your listener is created you must associate it with an endpoint group. Each endpoint group is associated with a different region, and within each group there are multiple endpoints. You can also set a traffic dial for the endpoint group, and this is essentially a percentage of how much traffic you would like to go to that endpoint group. And this helps you with blue and green deployments of your application to control the amount of traffic to specific regions. At the stage of adding your endpoint groups you can also configure health checks to allow the global accelerator to understand what should be deemed as healthy and unhealthy. </p>
<p>Finally, you must associate and register your endpoints for your application. And this can either be an application load balancer, a network load balancer, an EC2 instance or an EIP. For each endpoint, you can also assign a weight to route the percentage of traffic to that endpoint in each of your endpoint groups.</p>
<p>Let me now provide a very quick demonstration to show you how this creation looks within the AWS Console.</p>
<p>Okay so I’m logged in to my AWS Management Console and I need to go to the Global Accelerator which is under the Network and Content Delivery category. So if I select the Global Accelerator, now at the moment I don’t have any Global Accelerators configured. So from here I’ll simply click Create Accelerator.</p>
<p>Now, to start with, I need to select a name for my accelerator. So let me just call this MyAccelerator. Now here we have the IP address type, which is IPv4, and then we have the IP address pool selection. And the default is to use Amazon’s pool of IP addresses but if you want to use your own pool of addresses, then this is where you could change it. And also you can add any tags to this service if you need to.</p>
<p>So onto the next stage, this is where we add our listeners. So we can add in a port, for example, port 80. Either TCP or UDP as the protocol, and then you also have Client affinity here. And we can see that if you have state full applications, Global Accelerator can direct all requests from a user at a specific client IP address to the same endpoint resource to maintain client affinity. The default for this option is None. We don’t need that for this demonstration, so I’m just gonna leave that as None. And if you want to add any more listeners, simply click on Add Listener, and fill in the relevant details.</p>
<p>For this demonstration, I’m just gonna leave it as the one listener. Once your listeners are configured as you need to, click on Next. And here we have our endpoint groups. Here you select your regions that you want your application to reside in. So, for example, I’ll select the London region. And also we have our traffic dial, which as I explained previously, is essentially the percentage of traffic to this region. We can add additional endpoints, so we can have multiple regions if we want to. And you can keep going, add in more more regions. So let’s go and remove those two, just leave it as the one region. If you select on the configure Health checks, then you can set your health check configuration as need be just so the AWS Global Accelerator knows what it deems as healthy.</p>
<p>Once you have set your health checks, then you can select Next. On the final stage we need to add our endpoints. So here we have our endpoint group, and we select add endpoint. Now we can either add an Application Load Balancer, Network Load Balancer, an EC2 instance or an Elastic IP address. For this example, I’m gonna select an EC2 instance. And I won’t need to select the specific instance. I have one here called MyApplication. And again we have weight information, which directs the amount of traffic to each of your endpoint in your groups. I only have the single endpoint in this group, so I’m just gonna change that to 255. It can be from zero all the way to 255.</p>
<p>To add additional endpoints, simply click on Add endpoint. Select the endpoint that you’d like and then the related resource. I’m just gonna have the one endpoint in this endpoint group. And then once you’ve done that, simply click Create accelerator. And this would take a few minutes to configure itself and become active. And as you can see the status is in progress.</p>
<p>Also, as I explained earlier, we can see that this Global Accelerator has been given a DNS name, which results to this two static IP addresses. And these are the two IP addresses from the AWS pool of addresses. So that means you can add or change your endpoint groups and related endpoints in any of the regions without having to change the DNS name or the static IP addresses that it results to. So it’s very easy to change and increase region availability and high availability for your Global Accelerator. And it’s as simple as that.</p>
<h1 id="Using-Amazon-Route-53-Introduction"><a href="#Using-Amazon-Route-53-Introduction" class="headerlink" title="Using Amazon Route 53 - Introduction"></a>Using Amazon Route 53 - Introduction</h1><p>Hello and welcome to this Cloud Academy presentation. This is Jorge Negrón and I’m part of the AWS Content development team here at Cloud Academy. </p>
<p>In this course, you will be introduced to Amazon Route 53 and learn how the service helps you register a domain name and manage it worldwide. Route 53 allows for Domain name registration and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-route-53-route-end-users-internet-applications-1902/amazon-route-53-and-dns-records/">Domain Name System</a> or DNS management. Route 53 also implements traffic management by routing internet traffic to the resources for your domain and even check the availability of your resources using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-route-53-route-end-users-internet-applications-1902/amazon-route-53-health-checks/">health checks</a> to verify they are working as expected. </p>
<p>Amazon Route 53 uses edge locations in the AWS global infrastructure and it’s a global service. You don’t have to specify a region in configuring Route 53 resources. AWS offers a 100% available SLA for Route 53. This is due to the distributed nature of the DNS system and the high redundancy of the AWS implementation.</p>
<p>Routing policies define how to route internet traffic to the resources in your domain. You can choose from a variety of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-route-53-route-end-users-internet-applications-1902/amazon-route-53-routing-policies/">routing policies</a> including failover routing, and latency routing among others. Route 53’s “<a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-route-53-route-end-users-internet-applications-1902/amazon-route-53-traffic-flow/">Traffic flow</a>” feature allows you to create complex routing configurations, combining one or more routing policies for your resources. </p>
<p>Route 53’s <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-route-53-route-end-users-internet-applications-1902/application-recovery-controller/">application recovery controller</a> feature allows you to manage failovers by using routing integrated with health checks and application component verification. </p>
<p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-route-53-route-end-users-internet-applications-1902/amazon-route-53-resolver/">Amazon Route 53 Resolver service</a> is for VPCs and integrates easily with DNS in your data center. You basically configure endpoints for DNS queries into and out of VPCs. </p>
<p>Finally, Route 53 Resolver DNS Firewall is a managed service for DNS queries that originate in your VPC. You can create rule groups that allow or block specific DNS queries.</p>
<p>In short, with Amazon Route 53 you get a domain name management service with features that go beyond registration and name resolution allowing you to control how traffic is directed globally.</p>
<p>If you have any questions about the material being discussed, please, feel free to contact me <a href="mailto:jorge.negron@cloudacademy.com">jorge.negron@cloudacademy.com</a> with any questions using the details shown on the screen, as an alternative, you can always get in touch with us here at Cloud Academy by sending an email to: <a href="mailto:&#x73;&#117;&#x70;&#112;&#x6f;&#114;&#116;&#64;&#99;&#x6c;&#111;&#x75;&#100;&#97;&#x63;&#x61;&#100;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;">&#x73;&#117;&#x70;&#112;&#x6f;&#114;&#116;&#64;&#99;&#x6c;&#111;&#x75;&#100;&#97;&#x63;&#x61;&#100;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;</a> where one of our Cloud experts will reply to your question.</p>
<p>This course is intended for architects, developers, system operators, and administrators looking for a way to manage domain name servers using AWS. This course also covers some of the objectives for both the solutions architect associate certification exam and SysOps administrator associate certification exam. </p>
<p>In this course, you will learn what Amazon Route 53 can do in terms of features and capabilities. You will also be able to understand the routing options and health checks performed by the service.</p>
<p>To get the most out of this course you will need to meet the requirements for the AWS cloud practitioner certification. </p>
<p>Feedback on our courses here at Cloud Academy is valuable to us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>Please note that, at the time of writing this content, all course information was accurate. AWS implements hundreds of updates every month as part of its ongoing drive to innovate and enhance its services. As a result, minor discrepancies may appear in the course content over time. Here at Cloud Academy, we strive to keep our content up to date and provide the best training available. </p>
<p>If you notice any information that is outdated, please contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. This will allow us to update the course during its next release cycle.</p>
<h1 id="Amazon-Route-53-and-DNS-Records"><a href="#Amazon-Route-53-and-DNS-Records" class="headerlink" title="Amazon Route 53 and DNS Records"></a>Amazon Route 53 and DNS Records</h1><p>Amazon Route 53 is the domain name management service provided by AWS.  The domain name management system or DNS is responsible for translating domain names to IP addresses every time we use the internet, similar to a phone book that translates from a person to an actual number to dial. As such, DNS is part of the essential fabric that holds together the internet. </p>
<p>When you use Amazon Route 53 to register a domain, the service becomes the authoritative DNS server for the domain and creates a public hosted zone. A Public zone defines how traffic is routed on the public internet. A Private zone defines how traffic is routed inside a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-what-vpc/">virtual private cloud</a> or VPC. VPCs intended to be used with Private Zones need to have DNS Hostname and DNS Support enabled in their configuration. </p>
<p>Private and Public Hosted Zones are made of records. There is a variety of record types. Two of the more important record types are the Name Server or NS record type and the Start of Authority or SOA record type. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/amazon-route-53/">Amazon Route 53</a> creates a set of 4 unique NS records and 1 SOA record in each hosted zone created. </p>
<p>The Name Server (NS) records are used to identify the DNS servers for a given hosted zone. </p>
<p>The Start of Authority (SOA) record is used to define the authoritative DNS servers for an individual DNS zone. </p>
<p>These two records are essential to integrating your domain to the existing DNS system.</p>
<p>Route 53 supports the common record types of DNS including:</p>
<p>The A record is used to map a hostname to an IP address. An A record is used for IPv4 address.</p>
<p>The AAAA record is also used to map a hostname to an IP address. The AAAA record is used for IPv6 addresses. </p>
<p>A Mail exchange (MX) record is used to identify email servers for a given domain. You can have more than one and set the priority using a number. For example, you may have a primary email server with a priority of 10 and a secondary email server with a priority of 20. The lowest number record is used first. </p>
<p>The text (TXT) record is used to provide information in a text format to systems outside of your domain. It has multiple use cases. </p>
<p>A canonical name or CNAME is used to map a hostname to another hostname. This can be used to map multiple names to the same host. For example, when a server needs to respond as webserver using the hostname WWW and mail server using the hostname MAIL at the same time. </p>
<p>Please note that DNS supports record types above and beyond those mentioned here. </p>
<p>One record type that is outside the scope of DNS is the Alias record type.</p>
<p>The Alias record type is unique to Amazon Route 53 and maps a custom hostname in your domain to an AWS Resource which is usually represented by an internal AWS name. For example, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/cloudfront/overview-2/">CloudFront</a> distributions, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon S3 buckets</a>, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-elastic-load-balancing-ec2-auto-scaling-support-aws-workloads/what-elastic-load-balancer-elb/">Elastic Load Balancers</a> provide you a domain name that is internal to AWS. You can use an alias record to define a custom name to that resource. You can also use Alias records to map to apex records which are the top nodes of a DNS namespace like on example.com or cloudacademy.com</p>
<p>When you create a record using Route 53 you specify the record name, the record type, the actual value, the Time-To-Live in seconds, and the Routing policy for this record. </p>
<p>The Time to Live specifies the amount of time the record is considered valid. The same record result obtained before is used in the future and DNS won’t be queried again until the TTL has expired. </p>
<p>The Routing policy for a record defines how to answer a DNS query. Each type of policy does something different including the possible use of health checks. Let’s talk about those health checks first. </p>
<h1 id="Amazon-Route-53-Health-Checks"><a href="#Amazon-Route-53-Health-Checks" class="headerlink" title="Amazon Route 53 Health Checks"></a>Amazon Route 53 Health Checks</h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/amazon-route-53/">Amazon Route 53</a> health checks are independent resources that can be used by most routing policies when defining a record. When you create a health check, Route 53 sends requests to the endpoint every 30 seconds, and based on the responses, Route 53 decides if the endpoint is Healthy or UnHealthy and uses that information to determine what value to provide as an answer to the query. </p>
<p>You can also configure a health check for other “health checks” allowing you to independently verify different tiers of your application before the actual total application is considered healthy. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> Route 53 adds up the number of health checks considered healthy and compares that number to the health threshold value you specify. </p>
<p>With Route 53 health checks you can also monitor the state of a cloud watch alarm. The health check status is healthy when the alarm is in the OK state. The health check status is unhealthy when the alarm status is in the ALARM state. You can also choose what the health check status is when the alarm is in the INSUFFICIENT state. The options are healthy, unhealthy or “last known status”. </p>
<p>When Route 53 receives a query it chooses a record based on the routing policy, it then determines the current health of the selected record by checking the status of the health check for that record and responds to the query with the value of a healthy record. Unhealthy records are not considered. If you do not associate a health check with a record, Route 53 treats those records as always healthy. </p>
<p>The health check is performed by a fleet of health checkers located worldwide. You can use the list of recommended health checkers by region or customize the list to the regions specific to your business. Health checks are performed every 30 seconds unless you specify every 10 seconds. </p>
<p>Endpoint health checks can be specified by IP address or by domain name. The health check protocol can be TCP, HTTP, or HTTPS. For the HTTP-related protocols, you can use an optional string matching where you indicate that Route 53 is to search the response body for the string specified. Route 53 considers the endpoint healthy only if the string specified appears entirely within the first 5120 bytes of the response body.  </p>
<p>Finally, for all health checks, you can choose to get notified when it fails. </p>
<h1 id="Amazon-Route-53-Routing-Policies"><a href="#Amazon-Route-53-Routing-Policies" class="headerlink" title="Amazon Route 53 Routing Policies"></a>Amazon Route 53 Routing Policies</h1><p>The Routing policy for a record defines how to answer a DNS query. Each type of policy does something different. </p>
<p>The Simple routing policy provides the IP address associated with a name. With Simple routing an A record is associated with one or more IP addresses. A random selection will choose which IP to use. It is important to note that Simple Routing policies do not support health checks. All other routing policies do. </p>
<p>The Weighted routing policy is similar to simple routing and you can define a weight per IP address. Basically, you create records that have the same name and type and assign each record a numerical value that favors one IP address over another. A value of 0 suggests a record is never returned. This is useful for simple load distribution or testing new software. Each record is returned based on the weight compared to the total weight of all records. If a chosen record is Unhealthy, the process is repeated until a healthy record is obtained. </p>
<p>The Geolocation routing policy tags records with a location that can be Default, Continent or Country. It allows you to distribute the IP of a resource that can cater to customers in different countries or different languages. It can also help you protect distribution or licensing rights. You can create a default record for IP addresses that do not map to a geographic location. With geolocation routing an IP check verifies the customer’s location and the corresponding record for that location is returned based on the Location Tag for country, continent, or default. </p>
<p>The Geo-proximity routing policy requires that you use Route 53’s traffic Flow feature and create a Traffic Policy. A traffic policy is a resource that combines one or more routing policies.  Geo-proximity records are tagged with an AWS Region or using latitude and longitude coordinates. Geo-proximity routing is based on distance and a defined bias. You can specify a Bias from -99 to 99. This is a value that you can use to route more traffic to an endpoint by using a positive value or Route less traffic to an endpoint by using a negative value. Use the bias of -99 to route the least amount of traffic to an endpoint. You can think of the bias as being able to increase or decrease a region size in terms of coverage. This allows you to shift traffic from one location to another and route traffic based on the location of your resources. </p>
<p>The Failover routing policy is able to route traffic to a primary resource and based on a health check re-direct traffic to a secondary resource. The re-direction happens if the health check fails. Using failover routing you define a record to be primary and a different record to be secondary. You are also required to have a health check pre-defined. The routing of the primary record is active when the health check result is healthy. Otherwise, the secondary record is used. </p>
<p>The Latency routing policy chooses the record with the lowest latency to the customer. You define multiple records with the same name and assign a region to each record. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> maintains a database of latency between the general location of users and the regions tagged in DNS records. The record used is the one with the lowest recorded latency and is healthy. This may not always be the closest resource, especially if the closest resource is saturated.</p>
<p>The Multi value Answer routing policy returns multiple IP addresses to a query. Up to 8 IP addresses corresponding to healthy records based on a health check are returned. If there are eight or less healthy hosts the response includes all healthy hosts. </p>
<h1 id="Amazon-Route-53-Traffic-Flow"><a href="#Amazon-Route-53-Traffic-Flow" class="headerlink" title="Amazon Route 53 Traffic Flow"></a>Amazon Route 53 Traffic Flow</h1><p>Traffic flow simplifies the process of creating and maintaining records in large and complex configurations. This is useful when you have a group of resources that perform the same operation, such as a fleet of web servers for the same domain. </p>
<p>The traffic flow visual editor lets you create complex sets of records and see the relationships among them. You can combine multiple routing policies and health checks in a single configuration. Each configuration is called a traffic policy and it’s automatically versioned so you don’t have to start all over again when your configuration changes. Old versions of traffic policies continue to be available until you delete them. </p>
<p>A traffic policy can define hundreds of records. Traffic flow creates all those records automatically when you create a policy record. You create policy records to associate traffic policies with a domain or subdomain name records. The traffic policy record appears in the list of records for the hosted zone. You can use the same traffic policy to create records in multiple public-hosted zones. This is useful when you’re using the same hosts for multiple domains. </p>
<p>Please note that the Geo-proximity routing policy is available only if you use traffic flow.</p>
<h1 id="Amazon-Route-53-Resolver"><a href="#Amazon-Route-53-Resolver" class="headerlink" title="Amazon Route 53 Resolver"></a>Amazon Route 53 Resolver</h1><p>The Route 53 resolver is the DNS service for VPCs that integrates with your data center. Connectivity needs to be established between your data center DNS and AWS using a Direct Connect (DX) or a Virtual Private Network (VPN) connection.  You configure endpoints for DNS queries into and out of VPCs. Endpoints are configured through IP address assignment in each subnet needing the Route 53 Resolver. </p>
<p>Inbound queries allow DNS queries that originate in your data center to resolve AWS-hosted domains. </p>
<p>Outbound DNS queries are enabled using conditional forwarding rules. Domains hosted in your data center can be configured as forwarding rules in Route 53 resolver. Rules trigger when a query is made to one of those domains and the request is forwarded to your data center. This recursive DNS for your VPCs controls how DNS queries are handled between your VPCs and your data center. </p>
<p>Finally, the Route 53 Resolver DNS firewall is a managed firewall service for DNS queries that start in your VPCs. You use a firewall rule group to define how Route 53 Resolver DNS firewall inspects and filters traffic coming from your VPC. Each rule consists of a domain list to inspect in DNS queries and an action to take when a query results in a match. You can allow a matching query to go through, allow it to go through with an alert or you can block it and respond with a default or a custom response. To begin the filtering you associate the rule group to the VPCs you want to protect. Route 53 resolver DNS firewall will apply your defined filtering rules to the outgoing VPC traffic.</p>
<h1 id="Application-Recovery-Controller"><a href="#Application-Recovery-Controller" class="headerlink" title="Application Recovery Controller"></a>Application Recovery Controller</h1><p>Route 53 application recovery controller is a set of capabilities that continuously monitors an application’s ability to recover from failures and controls application recovery across multiple availability zones, regions, and possibly your own data center environments. </p>
<p>You can define a readiness check to monitor AWS resource configurations, capacity, and network routing policies. They can check the configuration of Auto Scaling Groups, Amazon EC2 instances, Amazon EBS volumes, Elastic Load Balancers, RDS instances, and DynamoDB tables among others. </p>
<p>These readiness checks ensure that the recovery environment is scaled and configured to take over when needed. You can check AWS service limits to verify that enough capacity can be deployed.  You can also verify that capacity and scaling setups for applications are exactly the same across regions before a failover takes place. </p>
<p>Readiness Checks work with Routing Controls to give you a way to failover an entire application based on custom conditions like application metrics, partial failures, increased error rates, or latency. You can also failover manually.  With Routing Controls you can shift traffic for maintenance purposes or during a real failure scenario. You can also apply safety rules to routing controls as a way to prevent a failover to an unprepared replica. </p>
<p>A control panel is a group of routing controls for an application. As mentioned earlier, A routing control is used to turn traffic flow ON or OFF to individual cells in Regions or Availability Zones. </p>
<p>This will permit you to define custom traffic re-direction for your applications during failover or maintenance cycles. Amazon Route 53 Application recovery controller allows you to configure fine-grain failover and verification steps to implement applications requiring very high availability. </p>
<p>This concludes our introduction to Amazon Route 53. </p>
<h1 id="Using-Amazon-Route-53-Summary"><a href="#Using-Amazon-Route-53-Summary" class="headerlink" title="Using Amazon Route 53 - Summary"></a>Using Amazon Route 53 - Summary</h1><p>In this course, we introduced Amazon Route 53 and how the service helps you register a domain name and manage it worldwide. </p>
<p>For Cloud Academy, this is Jorge Negrón, Thanks for watching!</p>
<h1 id="Networking-Summary"><a href="#Networking-Summary" class="headerlink" title="Networking Summary"></a>Networking Summary</h1><p>So you’ve now finished the theory section of networking for the AWS SAA. And some people find networking confusing and complicated but hopefully, you now have a much better understanding of how AWS networking works. We covered everything that could potentially come up in the exam. But in this course, I want to reiterate some of the main things to remember to ensure that you feel more prepared for any networking questions that could make an appearance. Now, remember I’m here to make sure that you know what you need to know and that you feel confident to pass this certification. So please reach out to me on LinkedIn, Twitter or drop us an email and I’ll happily discuss any questions you have. </p>
<p>Anyway, let’s take a look starting at VPCs. So first and foremost, you have to have solid grasp of VPCs. You’ll definitely come across questions covering VPCs and then networking components and how they all fit together. So let’s break this down in its simplest form. So the VPC is your own networking space of AWS that resides within a single region. And within your VPC you can create both public and private subnets. And each subnet resides in a single availability zone. You can control network traffic between subnets using network access control lists or NACLs and to control access between resources such as EC2 instances we use security groups. And these both work at the port and protocol level. </p>
<p>If we need to connect our VPC to the outside world we must use an internet gateway. And when attached, we can add a route from a subnet to the internet gateway to make that subnet public. If we need private instances to initiate a connection to the internet, then we need to use a NAT gateway and this resides in the public subnet. So if you can grasp those basic principles of network connectivity it will put you in very good stead in breaking down many questions that come up relating to VPCs. Half of the battle is remembering which networking component is used for what purpose. If you get that right you can usually eliminate at least two wrong answers. I find that most people get confused between NACLs and security groups and also when to use internet gateways over NAT gateways.</p>
<p>Okay, so let’s now look at some of the connectivity options when working with VPCs, in particular VPN gateways and Direct Connect. Now, both options provide connectivity from your own corporate network to the AWS Cloud but it’s a difference differences between them that are important as to when you would use one over the other. So you’d use a VPN solution if you are looking for a solution to connect your corporate network to your VPC that was relatively easy to implement where security didn’t really require the use of a private network. And so it could be run across the internet instead. Now with minor configuration of a customer gateway in your network and a virtual private gateway in your VPC it would be set up and running fairly quickly. However, if you require this connection to be fast, stable and private, then a VPN wouldn’t be the right choice. Instead, you’d need to use Direct Connect which would provide a private connection between your data center and an AWS region not just your VPC. Now, this uses dedicated lease lines with an AWS partner and you could connect one interface to a virtual gateway in your VPC and another interface to connect to an AWS region allowing access to public AWS resources such as Amazon S3.</p>
<p>Now, we also covered VPC endpoints in this course. Again, this is connection related but it looks at connectivity between your VPC and other AWS services across a private network without exposing data to the internet using AWS private link. Now, this means that you can connect to the services without configuring an internet gateway or a NAT gateway. For the exam, be aware of interface endpoints and Gateway endpoints. Now interface endpoints are effectively ENIs with a private IP address within your subnet and this acts as an entry point to a supported AWS service. Whereas a Gateway endpoint is added as a target in your route table of your subnets which points to either Amazon S3 or DynamoDB.</p>
<p>So we’ve covered VPC and its components and also network connectivity, but let’s now look at some of a smaller networking components, these being ENIs, EIPs and ENAs. They all sound very similar but all perform very different functions. You don’t need to know the inner workings of each of them but you do need to know when there might be used and what they are. So you might get asked questions about network latency and how to resolve it or questions relating to persistent public IP addresses to help mask instance failures or the requirements to set up a management network between your EC2 instances, for each of these you would use either an ENA an EIP or ENI. So ENAs are used to provide enhanced networking features to high speeds for your Linux compute instances. So if you receive any questions on enhancing network performance for Linux instances and ENA is an option, it’s certainly worth taking note of. </p>
<p>EIPs provide persistent public IP addresses that you can associate with your instance which can be attached to an instance or an elastic network interface, an ENI. And these can be detached from one instance and reattached to another. And this can mask the failure of a publicly accessible instance. And ENIs are used to give your EC2 instances an additional network interface. And this allows the instance to connect to two different subnets at once, each interface configured with an IP address of each subnet. So this is great if you’re creating a management subnet you can then add management network interfaces to each EC2 instance you want to be apart of that management network.</p>
<p>Now we just spoke about networking performance with the ENA but you should also take note of the AWS Global Accelerator too which effectively allows you to get UDP and TCP traffic from your end user clients to your applications faster, quicker and more reliably by using the AWS global infrastructure. And it does this by intelligently routing customer requests across the most optimized path. </p>
<p>So the ENA provides high-speed performance for your instance, whereas the Global Accelerator provides high speed performance from an end client to your application using the AWS network.</p>
<p>The last two services I want to highlight are Route 53 and CloudFront. So the key points at the high level for Route 53 include, it’s a highly available and scalable DNS service that provides secure and reliable routing of requests. Now you have public hosted zones which determine how traffic is routed on the internet and then private hosted zones which determine how traffic is routed within a VPC. Now it uses different routing policies to route traffic and this is important. You need to be aware of those different routing policies. It also supports the most common resource record types as well. </p>
<p>An alias records act like a CNAME record allowing you to route your traffic to other AWS resources such as ELBs, VPC interface endpoints, et cetera. So make sure you’re aware of what an alias record is. So what sort of questions might you see relating to Route 53? Well, I expect your see something relating to routing policies, you will be expected to select the most appropriate routing policy given a particular scenario. So know the difference between the following policies. There’s simple, failover, geo-location, geoproximity, latency, multivalue answer and weighted.</p>
<p>Okay, so moving on to CloudFront. So CloudFront is used to speed up the distribution of your static and dynamic content by storing cache data through its global network of edge locations. Now it’s fault-tolerant and globally scalable by design and it’s AWS’s own content delivery service. </p>
<p>So normally when a user requests content from a web server that you’re hosting without a CDN the request is routed back to the source web server which could actually reside in a different country to the user initiating the request. However, if you use CloudFront, the request is routed to the closest edge location to the user’s location which would likely provide the lowest latency and therefore deliver the best performance using cached data. </p>
<p>So when you’re looking at questions that ask you about distributing traffic or enhancing the performance for your end users, perhaps to your website you need to think about the different network and solutions available to help you to do this. And CloudFront will usually be one or part of the answers. You should be familiar with the configuration of CloudFront distributions and the information they contain such as the origin information, what an Origin Access Identity is, known as OAI, also brush up on your caching behavior options as well which define how you want the data at the edge location to be cached using various methods and policies.</p>
<p>And that now brings me to the end of another section. So you should now have a solid understanding of AWS networking components and concepts. So let’s crack on and tackle the next steps. Again, if you have any questions about any of this please do reach out to me and I’ll be more than happy to explain any topic further with you.</p>
<h1 id="3Subnets"><a href="#3Subnets" class="headerlink" title="3Subnets"></a>3<strong>Subnets</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/vpc-cidr-blocks/">VPC TCP&#x2F;IP Addressing</a></p>
<h1 id="6NAT-Gateway"><a href="#6NAT-Gateway" class="headerlink" title="6NAT Gateway"></a>6<strong>NAT Gateway</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">AWS Shared Responsibility Model</a></p>
<h1 id="7Bastion-Hosts"><a href="#7Bastion-Hosts" class="headerlink" title="7Bastion Hosts"></a>7<strong>Bastion Hosts</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/security/securely-connect-to-linux-instances-running-in-a-private-amazon-vpc/">SSH Agent Forwarding</a></p>
<h1 id="8VPN-amp-Direct-Connect"><a href="#8VPN-amp-Direct-Connect" class="headerlink" title="8VPN &amp; Direct Connect"></a>8<strong>VPN &amp; Direct Connect</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-ipsec-vpns-understanding-building-and-configuring/">Amazon VPC IPSec VPNs- Understanding, Building and Configuring</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/">AWS Virtual Private Cloud: Subnets and Routing</a></p>
<h1 id="12Elastic-Network-Interfaces-ENIs"><a href="#12Elastic-Network-Interfaces-ENIs" class="headerlink" title="12Elastic Network Interfaces (ENIs)"></a>12<strong>Elastic Network Interfaces (ENIs)</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC Flow Logs</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">AWS Docs - Interfaces for One Instance</a></p>
<h1 id="13EC2-Enhanced-Networking-with-the-Elastic-Network-Adaptor-ENA"><a href="#13EC2-Enhanced-Networking-with-the-Elastic-Network-Adaptor-ENA" class="headerlink" title="13EC2 Enhanced Networking with the Elastic Network Adaptor (ENA)"></a>13<strong>EC2 Enhanced Networking with the Elastic Network Adaptor (ENA)</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html#enabling_enhanced_networking">List of Supported EC2 Compute Types</a></p>
<h1 id="15Working-With-Amazon-CloudFront"><a href="#15Working-With-Amazon-CloudFront" class="headerlink" title="15Working With Amazon CloudFront"></a>15<strong>Working With Amazon CloudFront</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/protecting-web-apps-common-exploits-using-aws-waf-1883/">AWS WAF</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-2-of-2-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-2-of-2-15/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-2-of-2-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:21" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:21-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:08:58" itemprop="dateModified" datetime="2022-11-27T20:08:58-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-2-of-2-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-2-of-2-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Storage-SAA-C03-2-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-1-of-2-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-1-of-2-14/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-1-of-2-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:19" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:19-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:08:50" itemprop="dateModified" datetime="2022-11-27T20:08:50-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-1-of-2-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Storage-SAA-C03-1-of-2-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Storage-SAA-C03-1-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/50/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/50/">50</a><span class="page-number current">51</span><a class="page-number" href="/page/52/">52</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/52/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
