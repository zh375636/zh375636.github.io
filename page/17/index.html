<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/17/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/17/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:37" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:37-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:23:42" itemprop="dateModified" datetime="2022-11-20T22:23:42-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Managing-Applications-with-Docker-Compose-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to managing applications with Docker Compose.</p>
<p>About Me<br>I’m Logan Rakai and I’ll be your instructor for this course. I’m a content researcher and developer here at Cloud Academy. I’ve mostly worked on developing Labs, but I’m excited to be your instructor for this course. I’ve put a lot of thought into it and I hope you enjoy it. I have over ten years of experience in software research and development including five years in the cloud. I’m an AWS Certified DevOps Engineer Professional and a Microsoft Certified Solutions Expert: Cloud Platform and Infrastructure. You can connect with me on LinkedIn or on Twitter.</p>
<p>Who this course is for<br>This course is for anyone who could find themselves working with Docker containers. Among the roles that might be in that situation are <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/devops/">DevOps</a> engineers, developers, cloud engineers, and test engineers.</p>
<p>Prerequisites<br>In order to get the most out of this course, you should have experience with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/course-intro-1/">Docker</a>. You probably have enough experience if you have ever written a Dockerfile or if you can answer questions like when should you use a volume? And when should you use a user-defined network? The course includes some development demos that are most beneficial if you have some software development experience. You can follow along and I’d encourage you to. You will need Docker version 1.13 or greater installed. I’ll be using a Mac with Docker for Mac installed but you can follow along in Linux or Windows. The code I’ll be using is all available on GitHub. A clickable link is available at the bottom of the transcript for this lesson. You’ll benefit from a good integrated development environment or IDE. I’ll use Visual Studio Code which is available for free on mac, Linux, and Windows.</p>
<p>What we’ll cover<br>In this Course, we’ll go over what <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/docker-compose-overview-1/">Docker Compose</a> is and why you would use it. Then we’ll explore the two parts of Docker Compose: Docker Compose files and the Docker Compose command-line interface. Next, we’ll get into demo-focused lessons beginning with running a web app with Compose. After that, we’ll see how to build images in a development scenario with Compose. Lastly, we’ll see how to use Compose to adapt an application to multiple different environments. In particular, we’ll see how to use Compose to manage an application in development and production. Wow, after seeing all those exciting topics I need a second to Compose myself.</p>
<p>Learning Objectives<br>After completing this course, you will be able to:<br>• Understand the anatomy of Docker Compose files<br>• Configure your application using Docker Compose files<br>• Use the Docker Compose CLI to manage the entire lifecycle of applications<br>• Build your own images from source code with Docker Compose<br>• Extend Docker Compose files to adapt applications to multiple environments</p>
<p>Feedback<br>I’m happy to hear from you. I make content for you and I want it to be as good as it can be. If you have any feedback, please get in touch with me by leaving a comment on the Comments tab below the video, by emailing <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, or by connecting with me on Twitter where my handle is @LoganRakai.</p>
<p>All right, that’s all for the introduction. In the next lesson, we’ll start to get a better idea of what Docker Compose is. Continue on to the next lesson whenever you are ready.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/docker-compose-training">https://github.com/cloudacademy/docker-compose-training</a></p>
<h1 id="Docker-Compose-Overview"><a href="#Docker-Compose-Overview" class="headerlink" title="Docker Compose Overview"></a>Docker Compose Overview</h1><p>Thanks for joining me. We will start to peel back the outer layers of Docker Compose in this overview lesson.</p>
<p>I will begin by looking at how you might accomplish a task without Docker Compose. This will highlight some of the issues that Docker Compose was made to solve and give motivation for this lesson and really the entire course on Docker Compose.</p>
<p>Then I will define Docker Compose at a high level in terms of what it can do and how it does it.</p>
<p>Lastly, I will introduce the two parts that make up Docker Compose.</p>
<p>By the end of this lesson, you will understand what Docker Compose is, why you would use it, and get to know a bit about the parts that make up Docker Compose.</p>
<p>Ok, to start off with, I want to share the motivation for Docker Compose and it will give an opportunity to review some core Docker concepts. Let’s say that you are working on developing an application with Docker. The application is relatively simple, consisting of two services that communicate with one another. Each service corresponds to a container in the diagram. One service, let’s call it service A, is entirely stateless and should be accessible from the host machine on a port. The other service, service B, is required to persist data. You have Dockerfiles for both services already.</p>
<p>The task at hand is to spin up a temporary environment with the application running to perform some tests and tear it down when you are finished. How do you create this environment?</p>
<p>In a world before Docker Compose, you might go about achieving that with the following series of Docker commands. You want to follow best practices in isolating your application’s containers from other containers that are running on the Docker host. To give you the most control over that you create a user-defined bridge network.</p>
<p>Using a user-defined network also gives you access to automatic DNS resolution from container names to IP addresses that your application might take advantage of.</p>
<p>For service B that persists data, you want to follow best practices again by choosing an option that is easy to back up and migrate, that can be managed using the Docker commands, can be safely shared among multiple containers, and have the flexibility to be stored on remote hosts or in the cloud. You naturally decide to create a volume to achieve this.</p>
<p>Now you build the Docker images using docker build with the –f option to specify the different Dockerfiles for each service.</p>
<p>Almost there. You only need a couple more commands to start running the containers using the images you built. The docker run command creates the containers and starts running them. You use the –network option so that both containers are in the app_network in order to communicate. You use the -p option so that Service A is accessible on a host port. You use the –mount option for service B to mount the volume into the container.</p>
<p>Now everything is up and running so you can perform some tests.</p>
<p>After your tests are completed you decide to tear down everything you created to keep the environment pristine.</p>
<p>Start by stopping the service A and service B containers. Once the containers are stopped, you are able to remove them. Next you can remove the images you built for service A and service B. After that, you’re free to delete the volume for service B’s persisted data. And finally, you can remove the network that enabled communication between the two containers. And that’s it.</p>
<p>Now let’s take a moment to discuss the solution in the grand scheme of things. Setting up and tearing down the environment required about ten Docker commands. Relatively speaking, it is not too bad compared to a solution using virtualization and even better when compared to what would be involved using bare metal. Docker has made great progress in creating environments quickly and without having to worry about nasty issues like configuration drift.</p>
<p>Now, the series of commands used isn’t the minimal number you could use to achieve the same result. For example, you might decide that it is acceptable to have Docker automatically create the volume for you as a side effect of the –mount option in the run command instead of explicitly creating the volume with docker volume. But even after some optimizing, the fact remains, there is a lot of typing involved to accomplish a fairly common task. Not to mention there could easily be more options involved for configuring each command. </p>
<p>However, it is natural to ask the question, can we do better? One option that could be useful when performing the commands more than once is to put all the commands in a script. That also allows you to check the script into version control to better manage changes and collaborate with other team members. But to write the script you still need to know all of the docker commands required and the sequence to put them in. You are in essence telling Docker how to do something with the commands in the script. This is sometimes referred to as the imperative paradigm in DevOps where you give explicit steps to perform.</p>
<p>As an alternative, wouldn’t it be nice to only have to declare what you want to make instead of the explicit steps to perform to create what you want? That is to take a declarative, as opposed to an imperative, approach and let some tooling figure out the steps to create what you want.</p>
<p>That is in essence what Docker Compose gives you with respect to defining and managing multi-container environments in Docker. You still get the benefits of being able to use source control, but the emphasis shifts to describing what you want instead of how to create it. By way of analogy, Docker Compose is similar to using Dockerfiles. You can run a container, attach to it, run some commands, and use Docker commit to create a new image from the container. But in most situations, you want the enhanced documentation and maintainability that a Dockerfile gives you for accomplishing the same task along with Docker build. Analogously, you usually want to use Docker Compose instead of running a series of Docker commands.</p>
<p>That gives you a high-level understanding of what Compose is and why you might use it. In the context of Docker, you can refer to Docker Compose simply as Compose. You will see a lot of examples of Compose in action throughout this course to develop a more robust understanding of Compose. </p>
<p>It is also worth noting that Docker Compose files can be used to manage multi-container applications that are distributed over a cluster of computing resources. To natively manage a cluster in Docker, you run Docker in swarm mode. Swarm mode is outside of the scope of this course. You can learn more about swarm mode in other excellent content on Cloud Academy. I just want you to know that the time you spend learning Docker Compose in a single host environment will pay dividends later on when you start running applications on a Docker swarm cluster.</p>
<p>Docker Compose consists of two parts: a specially formatted file called a Compose file, and a command-line interface.</p>
<p>A Compose file is where you declare services that comprise your application. You can do a lot inside a Compose file. The Docker commands you use for creating containers, volumes, and networks have equivalent declarative representations in Compose files. Knowing Docker commands makes writing Compose files quite easy given their close connection.</p>
<p>There is an entire lesson devoted to the details of Compose files in this course. To get a sneak peak of what’s to come, take a look at this example Compose file. The services section declares two services: web and redis. Each service has a set of options underneath it. For the web service, there’s an option for specifying the image, which ports to make expose on the host machine, and a volume to mount in the container created for the service. There is also the depends_on option which isn’t something that has an equivalent docker command option. It becomes necessary in the context of Compose because you need a way to specify the order services come up. You can no longer issue commands in a specific order as you would with docker commands. But I’m getting ahead of myself. We’ll cover a lot more details on Compose files in an upcoming lesson.</p>
<p>The other part of Compose is a command-line interface. It has a familiar feel to the docker command-line interface. Many of the commands you use with docker exist in docker-compose but generalized to multi-container applications. The name of the Compose binary is appropriately docker-compose. As an example of the power of docker-compose, the series of docker commands that was presented in the Motivation section can be performed with just two commands in Docker Compose. One for bringing the application up, and another for tearing everything down. The simplicity of creating isolated environments with Docker Compose makes automated testing one of its key use cases. As with Compose files, there is an entire lesson in this course devoted to the Compose command-line interface. You will also see a lot of both Compose files and command-line interface in all of the lessons that present examples showing you how to use Compose for different tasks.</p>
<p>It can be difficult to manage multiple container applications. In the example at the beginning of this lesson, many commands and options are required to start and stop a relatively simple multi-container application. The difficulty in managing multi-container applications grows with the number of containers involved.</p>
<p>Docker Compose lets you specify services in a multi-container application using a declarative paradigm. You declare what you want and Compose figures out how to create it.</p>
<p>There are two parts to Compose. Compose files are where you declare the services you want, and the docker-compose command-line interface is how you manage the multi-container application declared in a Compose file.</p>
<p>That’s all for this overview lesson on Docker Compose. We’ll dive deep into Compose files in the next lesson. Whenever you are ready, continue on and start to see your multi-container applications through the Docker Compose lens.</p>
<h1 id="How-to-Create-Docker-Compose-Files-Using-YAML"><a href="#How-to-Create-Docker-Compose-Files-Using-YAML" class="headerlink" title="How to Create Docker Compose Files Using YAML"></a>How to Create Docker Compose Files Using YAML</h1><p>It’s time to start digging into the details of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Docker Compose</a>. We’ll start by taking a close look at Compose files in this Lesson.</p>
<p>Agenda<br>I will start by giving you a brief introduction to the file format used for Compose files: YAML. If you haven’t used YAML before, you’ll learn enough to understand the Compose file examples used in this course.</p>
<p>Next, I will teach you about the root elements in a Compose file document. These are the top-level element of a Compose file and include: Compose file version, services in the application, volumes used by the services, and networks to be created. There is a lot of similarity between these sections of a Compose file and Docker commands that you are familiar with.</p>
<p>I will finish the lesson with a couple special topics in Compose files.<br>Let’s get started with YAML.</p>
<p>YAML</p>
<p>YAML is a data serialization language. Data serialization languages can be employed for a broad variety of programming scenarios including internet messaging, object persistence, or, in the case of compose, configuration files. Some of the design principles of YAML are that it should be human-friendly, and that it should work with any programming language. When YAML is stored in a file, the file can have a .yaml or .yml extension. Both are recognized as YAML. The capabilities outlined in the YAML specification are quite extensive. We’ll only really scratch the surface of what you can do with YAML.</p>
<p>Another, perhaps more common, data serialization language is JavaScript Object Notation (JSON). JSON formatted files are supported by Docker Compose. However, it is rare to see JSON Docker Compose files in practice. Comparing a YAML file to its JSON equivalent shows the cleanliness and fewer characters needed to represent the contents. In this example, Part of the reason why it can cut down on the line count is because it is whitespace sensitive. That means if you insert an extra space at the wrong place, the file will be corrupted. JSON on the other hand isn’t whitespace sensitive, meaning that you could squash all the whitespaces out and not harm the integrity of the file. However, readability would not fare so well. For that reason, JSON files tend to be formatted with abundant whitespace resulting in a less compact representation than YAML. It can take some getting used to working with a whitespace sensitive language, but IDEs tend to have support for formatting YAML files making it quite painless to work with. Some features of YAML, which is actually a superset of JSON, further enhance the compact representation of Compose files. You will see an example of this later in this lesson.</p>
<p>Data Types<br>Let’s take a look at a few basic data types in YAML. This list isn’t comprehensive but is enough to understand what usually goes into Compose files.</p>
<p>The first YAML data type we’ll consider are integers. Integers are whole numbers like zero or 1. You can also include a leading plus or minus sign to indicate positive or negative integers.</p>
<p>Strings are a sequence of characters that aren’t interpreted as a different data type. Strings can include spaces and the use of quotes to indicate the start and end of a string. Quotes are optional unless you use symbols that have a special meaning. Use single quotes around strings that use YAML syntactic characters like the pound symbol or colon. Use double quotes if you want to escape control characters like backslash n for newlines. If you want an integer to be interpreted as a string, you need to enclose it in quotes because it will be interpreted as an integer otherwise.</p>
<p>The null type is used to represent the absence of a value, or no value. It isn’t very common to see in Compose files but is a recognized data type in Compose. You use a tilde or the word null to represent it.</p>
<p>Booleans<br>Booleans are the last data type we’ll discuss. They indicate one of two values: true or false. Booleans can be represented with true, false, yes, no, on, off as well as the same words with the first letter capitalized or all the letters capitalized. In YAML any matches of a pattern including yes, and on get converted to true. This can cause unintended consequences if you have conditions testing Boolean values. For example, if you had a condition that was checking if a variable has a Boolean value of yes but it automatically got converted to true. Compose simply disallows the use of Booleans in contexts where such issues can arise to be on the defensive side.</p>
<p>You’ll see an error message similar to this if you use a Boolean value. In this particular case, I tried to use a Boolean as an environment variable. As the error message hints, only strings, number, or a null can be used. If you want to use true or false, yes or no, on or off as values, you need to use the string representation by wrapping them in quotes. If you ever encounter an error message involving true or false, this is probably what it relates to.</p>
<p>Collections<br>Collections are data structures that allow you to collect basic data type values in an organized manner. The first YAML collection we’ll consider is the mapping. Mappings are also known as dictionaries or hashes in different programming languages. Maps consist of keys mapped to values. The syntax for a mapping is a key followed by a colon, a space and then the value. The space is important. A mapping can have multiple key value pairs.</p>
<p>Mappings can also have mappings as values. Using a mapping as the value for a mapping is referred to as nested mappings. In Compose files the inner mapping is usually located on a new line with indentation.</p>
<p>There is also an inline syntax that lets you write a nested mapping on a single line. You use braces to wrap the inner mapping in this case. You may see this from time to time but I think it tends to hurt readability and should be avoided.</p>
<p>Sequences<br>The other kind of collection is a sequence. Sequences are also called lists or arrays in other languages. Sequences are simply lists of values.</p>
<p>You use dashes to indicate items in a sequence. Each item goes on its own line at the same level of indentation.</p>
<p>Sequences can also be nested. To represent an inner sequence, you use an indented dash on a new line below the outer sequence.</p>
<p>As with mappings, there is an inline syntax to represent a sequence on a single line. You use brackets to wrap a comma-separated list of items to use inline syntax. You see this inline syntax used for command options in Compose files.</p>
<p>Combos<br>You can also combine the sequence and mapping collections. For example, you can have a sequence as the value of a mapping. The dash in a sequence as a mapping value also counts as indentation, so you don’t have to use spaces for indentation when including a sequence in a mapping. That means both of these examples are valid YAML. You might prefer to indent for consistency, but they are not required and you will see both styles used in practice.</p>
<p>Similarly, you can use mappings in a sequence. There is again two ways to represent a mapping in a sequence. You can use a line with only a dash followed by the indented mapping, or you can include the first line of the mapping on the same line as the dash. Both styles get used and you should be aware of each.</p>
<p>The last thing I want to mention about YAML is that is supports inline comments. This makes it much more useful in terms of documenting Compose files than JSON which doesn’t support commenting. In YAML, a comment starts when a pound character is encountered and continues to the end of the line. The exception to a pound character starting a comment is if it’s within a quoted string.</p>
<p>That is enough YAML to get through the example Compose files used in this course. It’s also enough to understand most examples you might find online and enough for you to write your own Compose files. As I mentioned before, using an IDE that can automatically format YAML can save you some headaches. Compose also includes a command for verifying configuration files in case you need to verify your YAML syntax and configuration declared in a Compose file.</p>
<p>Compose files<br>Now that we’ve built up a foundation in YAML, we can focus on Compose file specifics. As mentioned earlier, YAML Compose files are the focus and make up the vast majority of Compose files in use. We will only consider version 3 Compose files.</p>
<p>From this compatibility table, you can see that version 3 Compose files require a Docker Engine of version 1.13 or higher. The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/docker-compose-cli-1/">Docker Compose command-line interface</a> has a different release schedule than the docker engine, and requires version 1.10 or high for version 3 Compose files. Docker recommends using version 3 Compose files. There are multiple minor version numbers for version 3, for example 3.0 and 3.4. Unless otherwise noted, examples used in this course follow the 3.0 Compose file format. This covers versions of Docker released since the beginning of 2017.</p>
<p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file">https://docs.docker.com/compose/compose-file</a> The Compose file reference is a great reference for understanding all the configuration options available to you in Compose files. This course covers many frequently used configuration options, but leaves out many that might be useful in certain situations. Let’s take a quick look at it now.</p>
<p>Here we are at the Compose file reference page. It defaults to showing reference material for the latest major version which is 3 at this time. There is a handy navigation bar on the right to see all the available configuration options</p>
<p>Just note that it can be confusing at times because some configuration options only apply to Docker swarm mode (deploy), some only apply when not running in swarm mode (security_opt), or specific Compose file minor versions (Extension fields), and sometimes different options are available for Docker running on Linux or windows based systems (isolation).</p>
<p>Version<br>A YAML Compose file is a mapping with several keys at the root or top level. The first one we’ll discuss is the version. The value for the version must be a string. The string can specify a major version number only, for example 3, or a major and minor version number, such as 3.1. If a only major version is included, the minor version is implied to be 0, or the earliest release. So if you want to use a feature that came out in the latest minor version release, you need to include it in the version string. The version string tells Compose how the contents of the file should be parsed.</p>
<p>Services<br>The services mapping is where you configure the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/containers/">containers</a> created for services in your application. Each service is configured in a nested mapping under the services key. You can assign an arbitrary name for each service. In the image, the service names are web and redis. Each service has a nested mapping that declares the configuration for containers started for the service. The configuration is the main piece of declaring services in a Compose file, so let’s focus in on that.</p>
<p>Configure the container for the service.<br>Inside of the service configuration mapping, you declare the configuration options for service containers in a way that is similar to how you would configure containers using docker run command parameters.</p>
<p>This table shows how you would configure a container using the docker run command and the corresponding configuration key in a Compose file. The only required argument for docker run is the image name, which you specify using the image key in a Compose file. There are a few ways to configure a volume with docker run, but they all map to the volumes key in Compose files. There are different syntaxes in Compose to support the different volume configurations that you use different parameters for with docker run. The -p parameter to publish ports on the host corresponds to the ports key in Compose files. The -e parameter for setting environment variables in a container maps to the environment key in Compose files. With docker run you have two parameters for setting up logging and both parameters go into a nested mapping under the logging key. The last one that I’ll mention is security-opt for setting security options which only differs in the use of an underscore in Compose files. There are many more, but aside from a using YAML and slightly different names your experience with docker run will make writing Compose files easy.</p>
<p>Caveats<br>There are some points to be aware of for the correspondence between docker run and Compose file service configuration. Some docker run parameters that you might expect to be able to use in Compose only work in swarm mode. This is the case for setting runtime constraints such as -m for memory limit, or –cpus for number of cpus. It’s possible to run Docker in swarm mode with a single machine so it isn’t a significant barrier. However, Swarm mode is outside of the scope of this course so that’s all I will say about it.</p>
<p>Other docker run parameters such as -d to run in detached mode or –rm to clean up the container when it exits are specified through the command-line interface and not in the Compose file configuration.</p>
<p>Dependencies<br>Because Compose supports multi-container applications, there are additional options for configuring services that don’t exist with docker run.</p>
<p>The depends_on key provides a way to list the services a service depends on. Docker Compose can use the dependency relationships to determine the order to start services. If you tell Compose to start a specific service instead of the entire application, Compose can also use the information to automatically start any dependencies of the service. However, it’s important to note that Compose won’t wait for the dependencies to be ready before starting a service. For example, Compose can start a database before a web service but it can’t account for the time it takes the database process to be ready to handle connections. Because of this, it’s best practice to write your applications in a way that can tolerate connection failures. If that isn’t an option, you can use scripts that poll the dependencies to wait until they are ready. One such script is called wait-for-it.sh.</p>
<p>The other key that expresses dependencies is links. Links correspond to the link parameter of docker run allowing you to grant access to a container to access an exposed port on a private interface and to provide aliases to reach containers. Links in Compose carry the same meaning but additionally determine startup order of services, the same way depends_on does. Generally, networks are a better way to express communication relationships. We’ll discuss more about networks in a bit.</p>
<p>Examples<br>To get a taste of service configuration in Docker Compose, let’s look at some examples. I’ll arbitrarily use the redis image for the examples. Starting off simple, this docker run command will start a container named app-cache using the redis image.</p>
<p>The equivalent service configuration in a Compose file would like this. In the first line we need to specify that we are using version 3 of compose files. The services mapping is a pretty simple conversion of the docker run command parameters. The container name used as the service key, and the redis image argument used as the image key-value pair.</p>
<p>Now, if you specify a tag to pull a specific version of the image,<br>you add the same tag to the image value. Note that although the colon is a special character in YAML, you don’t need quotes around the string because colon only takes on special meaning when followed by a space.</p>
<p>If you want to the redis server port of 6379 available on the Docker host, you include the -p argument like so.</p>
<p>The corresponding Compose file includes a ports key which has a sequence of port strings. Just like with Docker run you can specify host and container port, or just the container port to allow Docker to choose an available host port. When specifying host and container port like in the example, it’s a good idea to put quotes around the string because YAML will parse numbers separated by a colon as sexagesimal or base 60 numbers if the numbers are less than 60.</p>
<p>In this last example, a command with arguments is added to override the default command.</p>
<p>The same string can be used as the value of the command mapping in a Compose file.</p>
<p>Or you can use the same syntax you would use in a Dockerfile for setting the default command of an image. In this case you do need quotes around the last argument “yes” otherwise it gets treated as a Boolean value. It’s best to always quote as you would in a Dockerfile. You might also recognize that syntax as the inline syntax of a sequence.</p>
<p>That means you can also express the command in the normal sequence syntax. This form can make long commands more readable. You get the idea of how to work with service configuration in Compose files through these examples. You might need to consult the Compose file reference to get the correct key names but it’s usually a fairly straightforward exercise to write the configuration.</p>
<p>Volumes<br>The next root key in the Compose file mapping is volumes. It is an optional key. You use the volume mapping in a way that is similar to how you use docker volume create. Services can reference volumes in each service’s volumes configuration key.</p>
<p>It’s a good time to point out that you can use volumes in the service’s configuration even if you don’t have a volumes key in your Compose file. The use case for the root volumes key is to use named volumes and to share volumes across services.</p>
<p>You can also declare external volumes that have been created outside of the context of the Compose file. For example, a volume created by docker volume create, or a different compose file. In a volume’s nested configuration mapping, you can set the external key to true to declare an external volume. If the external volume doesn’t exist, an error will be reported.</p>
<p>Take a look at this example Compose file using the root volumes key. There are two named volumes declared on lines 13 and 14. The first, called named-volume, doesn’t have any nested configuration. This will create a volume using the default local volume driver. YAML sees the absence of any value and represents it as a null. You could equivalently write a tilde or the word null for the value on line 13. The other named volume is called external-volume and is configured as an external volume.</p>
<p>In the app-cache service’s volumes configuration starting on line 5, you can see a few ways to declare volumes in Compose. The first is using a named volume in the root volumes key and will be mounted at &#x2F;data in the container. The next example on line 9 uses a relative path to set the source of the mount. Relative paths are relative to the location of the Compose file. The last example on line 11 will have the Docker Engine create a volume automatically to mount at &#x2F;tmp&#x2F;stuff in the container.</p>
<p>Lastly for volumes, you can configure a custom volume driver using the driver and driver_opts keys. The named volume in the example called ebs-volume uses the convoy docker volume plugin by rancher. If you want to specify any driver-specific options, you can do so under the driver_opts key.</p>
<p>Networks<br>Networks are declared under the top-level networks key. By now it will come as no surprise, that network configuration in Compose files aligns closely with the docker network create command. However, there are a few new concepts to networking in Compose.</p>
<p>By default, Compose will automatically create a new network using the default bridge driver for an application in a Compose file. The name of the network is based on the name of the directory the Compose file is in with default appended on the end. All containers created for services in the Compose file join the default network and can be reached and discovered by the corresponding service name. This is slightly different from running containers with docker run and not specifying a network. In that case, the containers get added to the default network named bridge.</p>
<p>To review how you can use a bridge network, consider this example Compose file. There are two services, web, and cache. The Compose file doesn’t declare any networks so all service containers will join the default network created for the app declared in the Compose file.</p>
<p>In the default network, the cache container can reach the web container by using web as the hostname for the container.</p>
<p>Similarly, web can reach cache by resolving the cache hostname. Because web is inside the network, it uses the container port of 6379 to connect.</p>
<p>From the Docker host machine, cache can be reached at the host port of 36379. What about web? How can the host reach web?</p>
<p>It cannot reach web, because no ports are published making it accessible to the host.</p>
<p>Besides the default network, you can declare custom networks under the root networks key. This gives you more control and allows you to create more complex network topologies. Custom networks can be external to the application, similar to external volumes worked. You add the external: true mapping to tell Compose to verify the network already exists and join services to that external network.</p>
<p>This example Compose file illustrates how custom networks are used. The file declares two networks under the top-level networks key beginning at line 18. The frontend network uses the default network configuration, while the backend network refers to an external network created outside of the Compose file. The networks mapping for each service shows the proxy and app service are part of the frontend network, and the app and db service are in the backend network. With this configuration, the db and proxy services are isolated from one another. This approach of limiting communication between services to allow only what is necessary is a best practice. The db service further configures the alias database for itself on the backend network. The app container could resolve the hostname db or database as a result of the alias. The mapping syntax must be used for specifying aliases.</p>
<p>Special Topics: Variable Substitution<br>I will finish of the lesson by discussing a couple special topics in Compose files, starting with variable substitution. Variable substitution allows you to generalize your Compose files to create different environments without having to modify the Compose file. Docker Compose will substitute shell environment variables in place of variable placeholders in a Compose file. You indicate a variable by using a dollar sign before the variable name, and optionally surrounding the variable name in braces. If the variable is not defined in the environment where Docker Compose is running, an empty string is substituted for the variable. The example on the bottom half of the slide shows a snippet of a Compose file that uses a variable named REDIS_TAG for the image tag. In the shell environment, the REDIS_TAG environment variable is set to 4.0.5. When Docker Compose creates the application, the environment variable is substituted into the image value string.</p>
<p>Special Topics: Extension Fields<br>The other special topic I wanted to mention is extension fields. Extension fields let you reuse configuration blocks and move important configuration fragments to the root of the Compose file. Extension fields only work with version 3.4 or higher Compose files, so you will need to include a minor version number to get this to work in version 3. To use extension fields, add a root key that begins with x- and add the configuration to reuse under it. To insert the configuration somewhere else in the file, you use YAML anchors. Anchors allow you to create an alias for the configuration that effectively inserts the configuration fragment wherever you reference the anchor. The example on the right shows how an extension field called x-logging is used for configuring logging in two services. The anchor is indicated with an ampersand and is named default-logging. In the service definitions, the asterisk precedes the anchor name to indicate that an anchor is being used as the logging value. The extension field mapping including both options and driver are inserted at the proper indentation level where the default-logging anchor is referenced with an asterisk. The example illustrates how extension fields are useful for removing configuration clones.</p>
<p>Recap<br>This lesson began with a crash course in YAML. You learned about the string, integer, null, and Boolean data types. You also learned about the two collections in YAML: mappings, and sequences. This was just enough YAML to understand and write Compose files.</p>
<p>Next you understood the anatomy of a Compose file. The configuration in compose files falls under the top-level mapping keys of version, services, volumes, and networks. The configuration for services, volumes, and networks are similar to parameters you pass for docker run, volume create, and network create commands, but formatted in YAML syntax.</p>
<p>We finished the lesson by covering a couple special topics in Compose files. Variable substitutions allow you to generalize a Compose file using environment variables. Extension fields let you reuse configuration fragments to cut down on clones.</p>
<p>Up until now, you’ve only seen how to declare what’s in your multi-container applications. In the next lesson, you will see how to use the docker-compose command-line interface to run and manage the applications. When you are ready to learn how to start running your Compose file applications, continue on to the next lesson.</p>
<p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file">https://docs.docker.com/compose/compose-file</a></p>
<h1 id="Features-and-Commands-of-Compose-Command-Line-Interface"><a href="#Features-and-Commands-of-Compose-Command-Line-Interface" class="headerlink" title="Features and Commands of Compose Command-Line Interface"></a>Features and Commands of Compose Command-Line Interface</h1><p>In this lesson, we’ll see how to use the Docker Compose command-line interface to turn the multi-container applications described in Compose files into actual running environments in Docker.</p>
<p>Agenda<br>I’ll start by reviewing some of the high-level features of the Compose CLI.</p>
<p>Next, I will go through some of the installation options available for different platforms.</p>
<p>Lastly, I’ll finish the lesson by looking at how to use the docker-compose CLI by the reviewing common commands and parameters</p>
<p>Features<br>One of the features of the Compose CLI that I want to highlight is its ability to run multiple isolated environments on a single host. Some scenarios where this is extremely useful is on a continuous integration server where you need to run automated tests for each build version. The ability of Compose to run multiple isolated environments means you don’t have to sequentially iterate through each version. A development scenario where this comes in handy is when you may need to create multiple copies of an environment for different feature branches. You can use variable substitution in the Compose file to create the desired branch environment. We’ll talk more about development scenarios in later lessons in this course.</p>
<p>The Compose CLI uses a parallel execution model to perform tasks for creating and deleting an application environment. Not everything can run in parallel due to dependencies and limitations in Docker, but when possible parallel execution is used to reduce the time it takes to manage applications.</p>
<p>Another useful feature to be aware of is the change detection capabilities of Compose. Every time you start a container for a service in Compose, the configuration is cached. If you later restart a Compose application, Compose will reuse any <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/containers/">containers</a> that haven’t changed configuration. This is a bit like how layers are cached when building images from a Dockerfile. Just like with Dockerfiles, you can instruct Compose not to use the existing containers and instead force all containers to be rebuilt.</p>
<p>The last feature, using the term loosely, is that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Docker Compose</a> is an open-source project on Github with an active community. You can report issues and make feature requests there. If you are familiar with the Python programming language, you can fork the project and modify the source to better suit your needs. Maybe even make a pull request to have your improvements included into the project.</p>
<p>Installation<br>Before we get into using the Compose CLI, I want to say a few words about getting Compose installed on your system.</p>
<p>For mac users,<br>Compose comes installed with the Docker for Mac application and Docker Toolbox for older systems.</p>
<p>For Windows users,<br>If you obtained Docker through Docker for Windows, or Docker Toolbox<br>Compose came included with that.<br>If you are running the native Windows Docker Daemon, on Windows Server 2016 or Windows 10 with the Anniversary Update<br>You need to install Compose separately. You can choose the appropriate version of Compose and download an installer from the Compose Github releases page. For example, you could download version 1.17.0 to get the version of Compose I’m using for this course.</p>
<p>For Linux systems,<br>Docker Compose is included in many distribution repositories<br>For example, on CentOS or RedHat distributions you can use yum or dnf, and apt on Debian-based systems.<br>If Compose isn’t available through the distribution’s package repo or you want a specific version,<br>you can get Compose from the Github release page.</p>
<p>Usage<br>All right! With that out of the way, we can look at how to use the Compose CLI. I’ll show a couple slides to cover the Compose CLI basics and then hop over to my terminal to briefly illustrate using the Compose CLI.</p>
<p>docker-compose follows similar patterns to the docker CLI. You specify options to Compose, followed by a command, and add arguments for the command at the end. You can always use the –help argument to print a help page for any command.</p>
<p>Compose will use the Docker Daemon running on the host by default.</p>
<p>You can connect to a Docker Daemon running on a remote host using the -H option. Along with that, you can secure the connection to the remote host using transport level security options. This requires the remote host to have been configured to use tls for the Docker Daemon.</p>
<p>For commands that reference a Compose file, the default Compose files that the CLI tries to find in the current directory are named docker-compose.yml or docker-compose.yaml.</p>
<p>It can be restrictive using only the default Compose file, so the -f option is provided to allow you to specify a path to any file that you want Compose to use as the Compose file for a command.</p>
<p>Each isolated application is associated with a project in Compose. The project is given a name and that name appears in resources that get created by Compose. For example, the names of networks and containers created by Compose begin the project name followed by the arbitrary name key declared in the Compose file.</p>
<p>The default project name is the name of the directory containing the Compose file.</p>
<p>You can assign a custom project name by using the -p option.</p>
<p>Commands<br>As we have seen, Compose tries to make adoption easy for users already familiar with Docker. Most of the commands in the Compose CLI are familiar Docker commands that are generalized to work with multi-container applications.</p>
<p>This is the list of commands that exist in both Docker and Compose CLIs as of Compose version 1.17. As an example of how a command is generalized to multi-container applications, consider the stop command. In Docker, you use the stop command to stop one or more running containers passing the container names as arguments to the command. In Compose, the stop command will stop all containers declared in a Compose file, unless you provide the names of individual services to stop. Most commands generalize as you would expect. Some commands like config are not related to the Docker command. Config is useful for validating the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/anatomy-of-a-compose-file-1/">YAML</a> and configuration in a compose file. It’s important to note that you can still use the docker CLI to work with resources created by Docker Compose. For example, containers created by Compose are listed by docker ps since they are created by the Docker daemon. Compose is just wrapping commands to generalize them to how you would expect them to work with multi-container applications.</p>
<p>After removing the commands that exist in Docker, there are currently only two non-deprecated commands that are unique to Compose and they are big ones.</p>
<p>The first is up.</p>
<p>The up command performs the actions required to instantiate the application described in a Compose file. It starts by creating default and named networks as applicable, and any named volumes.</p>
<p>It then takes the actions required to bring up service containers. This includes building images if required, then creating and starting containers, and finally attaching to the containers to aggregate output and error streams from the containers. When the command exits, the containers are all stopped. However, you can use the -d option to let the containers run in detached mode.</p>
<p>The up command is also responsible for performing change detection when you bring up an application that has already been brought up. It will recreate containers with changed configuration and join them to the appropriate networks. Any connections that were established with the original container are closed. There are several options for configuring how Compose does this if the default behavior isn’t what you want.</p>
<p>The other command unique to Compose is down. Down is a partial opposite of up.</p>
<p>What I mean by partial opposite is that down will only remove containers, as well as any named and default networks by default.</p>
<p>It won’t delete volumes or images that up created, unless you pass arguments instructing the command to do so. Up and down make it easy to perform integration tests in a continuous integration pipeline. You can simply wrap a test script between up and down to have the tests run in the isolated environment. Ok, with that, we’ve covered enough of the Compose CLI to try it out and to use the help argument to find out more information when needed.</p>
<p>@Terminal docker-compose<br>Here we are at my terminal. I want to demonstrate using the Compose CLI just to give a first look at it. We will cover more in-depth examples showing Compose in action in the remaining lessons in this course.</p>
<p>To start with, you can always get the usage information by appending –help on any command or docker-compose itself. I’ll pipe it into more to page through the output. I won’t read through the output since we’ve discussed most of what is shown. The help output finishes with a list of all the available commands.</p>
<p>To get more information on the up command, I’ll enter docker-compose up –help. I’ll jump down to the options to see what’s available for configuring the behavior of the up command. Just as an example, –no-deps can be used to prevent starting dependent services. This doesn’t sound very useful when you first bring an application up, but if you later modify the configuration of one service, it can be useful to not restart the services that the one service depends on. As another example, adding the –remove-orphans option can clean up any services that are no longer declared in a compose file. This can happen if you delete a service outright from a Compose file or if you rename one.</p>
<p>To finish up, I’ll demonstrate how to use docker-compose’s config command to debug any YAML or configuration errors. You will also see how config shows you the effective configuration that is used by Compose after variable substitutions and extension field references.</p>
<p>If I switch over to VS Code, I have a Compose file open called 1-extension-fields.yml. It follows an example shown in the slides for using extension fields.</p>
<p>It’s using version 3.4 which is good because that’s the earliest version that supports extension fields. To see if everything is ok with the file, run the config command on it. At the terminal, I’ll use the -f option to specify that file as the Compose file to use. So, Compose reports an error about services.cache.command contains true which is not valid. If I jump back to Code, it seems strange at first because there is no instances of true in the file. But remember that multiple words get mapped to true in YAML. Yes is one of them. Code has even changed the color to indicate that it isn’t a string value. I’ll add quotes around it to correct the error.</p>
<p>Running config again reveals a different error. The cache service doesn’t set an image or build command so it can’t be created. I’ll set the image to redis, but I want to use variable substitution to set the tag, like so. Now when I run the config command again there are no errors and the effective configuration is displayed. Here you can see the default-logging YAML references have been replaced with the associated configuration under each services logging key. Compose reports a helpful warning at the top about the REDIS_VERSION variable not being set so an empty string is substituted. You can see that in the displayed configuration. That will need to be corrected. I’ll export the variable<br>Export REDIS_VERSION&#x3D;4.0.6<br>And the config command output confirms the variable is substituted into the configuration. I’ll leave it at that for now. We’ll see several more examples of Compose at the command-line in upcoming lessons.</p>
<p>Recap<br>This lesson started by outlining some of the features of the Compose CLI including, how it can create multiple isolated environments on the same host. This makes it appealing for continuous integration, testing, and development scenarios. It also has built-in Compose file change detection support to only do what is required to bring the application to the desired state described in a Compose file.</p>
<p>We saw that the docker-compose CLI follows the same pattern as the docker CLI. Most of the commands in docker-compose are analogous to ones you find in docker.</p>
<p>There are however, two important commands that are unique to compose. Up does everything required to bring an application described in a Compose file up, and down brings an application down, removing containers and networks, but leaving images and volumes untouched by default.</p>
<p>In the next lesson, we’ll go a step farther and bring up a web application with Compose using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/run-a-web-app-1/">pre-built images from Docker Hub</a>. When you are ready to see more of Compose in action, continue on with the next lesson.</p>
<p><a target="_blank" rel="noopener" href="https://github.com/docker/compose/releases">https://github.com/docker/compose/releases</a></p>
<h1 id="Deploying-and-Configuring-a-Web-Application-with-Compose"><a href="#Deploying-and-Configuring-a-Web-Application-with-Compose" class="headerlink" title="Deploying and Configuring a Web Application with Compose"></a>Deploying and Configuring a Web Application with Compose</h1><p>Welcome back. This lesson will go through deploying a web application with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Compose</a>. This lesson is more applied than the previous ones. We’ll spend most of our time at the command-line and making changes to a Compose file.</p>
<p>Agenda<br>I’ll start by briefly introducing the web application.</p>
<p>Then we’ll get right into the demo.</p>
<p>Wordpress<br>The web application that we’ll use is WordPress. WordPress is a popular content management system or CMS. You can create websites and blogs in WordPress. WordPress is written in PHP and uses MySQL as a database. The images for WordPress and MySQL are maintained by Docker. Both images have over 10 million pull on Docker Hub. This scenario relates to operating the application. This is where images have been created and you pull them from a registry, possibly Docker Hub, or your own corporate image registry. The following lesson gets into developing applications with Compose. Now the stage is set, so let’s hop over to Visual Studio Code to look at the Compose file I’ve prepped for the application.</p>
<p>@Terminal<br>Here is the Compose file, wordpress.yml. All of the contents just fit on the screen. Let’s take a moment to go through it since it ties together a lot of what we’ve seen in the course so far. There are two services, one for WordPress which is where the PHP application code exists and is served up by an apache web server, and one for the MySQL database. Both services use a specific tag on the image to have more control over the environment and to prevent unexpected changes from creeping in. Both services also have a restart key with the value of always. This is the same as the restart option for docker run. To make the application more production-worthy it’s a good idea to have the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/containers/">container</a> restarted automatically if it exits for some reason. You definitely want to persist data for a CMS, so the database service is using a named volume called db_data. This mounts into &#x2F;var&#x2F;lib&#x2F;mysql in the container and is where MySQL stores its database files. Each service has a set of environment variables configured. The db service uses a mapping for its environment variables to create users and a database called wordpress. The wordpress service uses a sequence of strings to configure the database user and host with equal signs separating the variable names from their values. Both syntax styles are allowed and equivalent. One variable to highlight is WORDPRESS_DB_HOST in the wordpress service. It configures the database hostname. The value that is assigned is db on port 3306, the default port for MySQL. There are no named networks in this file. How can the wordpress service connect to the db? Both services will be added to the default network that Compose will create. The last bit of configuration, is the publishing of the wordpress port. The app will be available on port 8000 on the Docker host. The string is enclosed in quotes as a best practice although not strictly necessary in this case because the container port, 80, wouldn’t mistakenly be interpreted as a base-60 number.</p>
<p>Switching over to my terminal, I’m in the webapp directory which contains the wordpress.yml compose file. I’m starting with a clean Docker environment. No containers, no volumes, and only the default Docker networks. I’ll bring the wordpress application up now, using the -f option to specify a custom compose file. The db’s mysql image gets pulled first followed by the wordpress image. I’ll speed this up while the image layers get pulled. Now you can see in the output that the container for the db service is created first followed by the container for the wordpress service. This is guaranteed because the db is in the wordpress services depends_on sequence. The webapp at the beginning of the container name is the project name and it defaults to the current directory name which is webapp. The up command then attaches to the containers and aggregates their output. Compose uses color to distinguish between the output from different containers. You can see WordPress attempting to connect to the database and failing. Recall that depends_on doesn’t wait until the database is ready, it only sequences the order containers are started in. Fortunately, WordPress follows the best practice of having the application handling failed connections and retrying until a connection is made. At this point, the db and wordpress services have finished initializing. I’ll stop the docker-compose command with ctrl+z instead of exiting with ctrl+c so the containers don’t get stopped. After clearing the screen, I’ll list the containers with docker ps and confirm that the containers made by compose are like any others. Checking on the volumes, we can see the db_data named volume created by Compose. The unnamed volume comes from the WordPress image. It declares a volume for the WordPress web assets that get served up by the Apache web server in the &#x2F;var&#x2F;www&#x2F;html directory. Next, we can see the default network Compose created in the networks list.</p>
<p>To verify the application is functioning correctly, I’ll jump over to a browser and navigate to port 8000 on localhost where the WordPress service published its web server container port. The first time you use wordpress, you need to configure the language, a site title, and some user information. I’ll set the title to Composing. The other details aren’t important. With those details set, I can log in and see the admin dashboard. Up in the upper right corner I can navigate to the public site that’s hosted by default. Here it is with the Composing title that I specified earlier. Everything is working as expected. We successfully ran a web application in Compose!</p>
<p>Let’s see how the Compose change detection works. Say we decide to accept the risks of using the latest tag for the wordpress image. I’ll change the tag, save the file, and go back to the terminal.</p>
<p>I’ll repeat the up command except using the -d argument to run the containers in detached mode so the shell prompt will be returned to me after the command finishes. It starts by pulling down the latest version of the wordpress image. After that, it checks and sees that the db service container already running matches the configuration in the Compose file. There is no need to restart it. It then detects that the wordpress container doesn’t match the configuration in the Compose file and recreates it using the updated configuration. You can change the behavior of up to suit your needs in different scenarios though. If you were uncertain if any other services had changed configuration and wanted to avoid recreating the db container at all costs, you can specify the –no-deps argument to up along with the service you want to bring up. In the output, notice that no check of the db service is made. If you want to recreate all containers even if their Compose configuration hasn’t changed, you can pass the –force-recreate argument. The output indicates each container is being recreated now. To be certain, check the output of docker ps and see the containers have just been created. Now I will demonstrate bringing the application down with the down command. The output describes the steps Compose is taking, stopping containers, then removing containers, and lastly the default network Compose created. Docker ps -a verifies there is no trace of any service containers. I can bring the application back up again very quickly having previously downloaded the images. Now, if I load WordPress in the browser, what do you think I will see?</p>
<p>We don’t see the first-time configuration page, we see the same composing site as before. That’s because docker-compose down leaves the volumes by default.</p>
<p>You can change that default behavior though. Add –rmi all to remove all the images used in the Compose file, –volumes to delete the named volumes declared in the Compose file as well as well as any anonymous volumes attached to service containers, and –remove-orphans to delete any project containers that are no longer defined in the Compose file. This time you can see the removal of volumes and images in the output. There is still the old 4.9.0 wordpress image kicking around however. I’ll use the image prune command to remove any images that are left over.</p>
<p>Closing<br>That was pretty awesome! Managing the multi-container web app with Compose was painless. In the next lesson, we’ll see how Compose works when we need to build the images using Dockerfiles.</p>
<h1 id="Using-Compose-Configurations-and-Commands-to-Build-Images"><a href="#Using-Compose-Configurations-and-Commands-to-Build-Images" class="headerlink" title="Using Compose Configurations and Commands to Build Images"></a>Using Compose Configurations and Commands to Build Images</h1><p>Up until now, we have seen <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Compose</a> working with images pulled from a Docker registry. Can you use Compose in development scenarios when the code isn’t ready to be sealed in an image? How do you build images with Compose? These are the questions I’ll answer in this lesson.</p>
<p>Agenda<br>I’ll begin by getting into the Compose file configuration and Compose commands needed for building images.</p>
<p>With that foundation in place, I’ll finish the lesson with a demo that illustrates how to use Compose to build an image in a development scenario. Compose will bring the application up and your code changes will be reflected in the running <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/containers/">container</a> without needing to rebuild or stop the container. This provides a similar experience to developing on your local machine without Docker, but the server and all of the application dependencies are running inside a container.</p>
<p>Building in Compose<br>When you build images in Compose, you make use of the same tried and true Dockerfiles that you use when building images with the docker build command. We won’t get into the details of Dockerfiles in this lesson, but I’ll quickly review one in the demo.</p>
<p>To instruct compose to build an image, add the build key in a service’s configuration. There can be more than one service in a Compose file with a build mapping.</p>
<p>The Docker-compose up and docker-compose build commands can be used to build images. We’ll take a closer at the build key and these commands in the next few slides.</p>
<p>Build Key<br>If a service has a build key present, Docker Compose will build the image for the service. There are two forms of build configurations in a Compose file. The short form sets the build value to the path of the build context which is where the Dockerfile is located.</p>
<p>The longer form uses a nested mapping. The context is a required key and it has the same meaning as the context for the short form. The Dockerfile key is optional. If specified, the value is the name of the Dockerfile to use. If it isn’t specified, Dockerfile will be used as the name for the file containing the image build instructions. Args are optional as well, and can be used to pass Arg values at build time. The Dockerfile should have corresponding ARG instructions.</p>
<p>The built image will be given a name that follows the pattern of Compose project name followed by the service name. If you want to use a different name, or want to specify a tag for the built image, beside the default latest tag, you can do so by using the image key. The image specified the image to pull from a Docker registry before, but if a build configuration is present for a service, the image is interpreted as the name of the built image.</p>
<p>Docker-compose up will build any image for services that don’t have one already built. Subsequent up commands won’t rebuild the image, unless you pass the –build option. This might not give enough control over built images, so there is another command for building.</p>
<p>Docker-compose build will build images or rebuild them if they already exist. Just like with the docker build command, there are a couple options to customize the behavior of docker-compose build. The –no-cache option will prevent using the layer cache causing all layers to be rebuilt. The –pull option will always attempt to pull a newer version of a base image described in the Dockerfile. That’s all there is to building in Compose.</p>
<p>Demo<br>Now, we’ll get into a demo to illustrate building in Compose. The demo will use a NodeJS project that uses MongoDB for persistence. The image shows the app. It simply accumulates whatever messages users enter. The goal of the demo is to build an image with Compose that will allow on the fly updates as you modify the source code. No rebuilds and no stopping the containers. This gives the instant feedback that developers crave. Let’s see how to do that.</p>
<p>Here in VS Code, I have a Dockerfile for the project open. It’s called dev.dockerfile. I’ll try to stay as language-agnostic as possible but the specific RUN instructions are specific to NodeJS development. At a high level, the instructions install the dependencies for developing and running the application. On line 5, nodemon is installed. nodemon is a tool that watches for changes to development files and automatically restarts the server to reflect the changes. On line 17, nodemon is set as the default command for running a container using the image. On lines 8 through 11, the src directory is created and set as the working directory. Then the application dependencies file, package.json, is added to the src directory in the image. The npm install command installs all of the dependencies in the src directory. Note that only the dependency file is added and not any source files. The image has everything the code needs to run but not the code itself. The development server port of 3000 is exposed on line 14. So how will this image be used to develop the code? The default command is expecting a file at &#x2F;src&#x2F;app&#x2F;bin&#x2F;www to start the server but it doesn’t exist in the image. How will that work? The answer to both questions is by mounting a volume. Specifically, the source will be mounted at &#x2F;src&#x2F;app. The default command will then start a server using the code in your development environment. Let’s take a look at the Compose file, that I’ve called dev.docker-compose.yml.</p>
<p>There are two services, app and app-db, that are in the backend network. App publishes the port of 3000 so that the host can access the development server. There are a couple environment variables to configure NodeJS for development and to pass the hostname of the database. What’s most important for this lesson is the build configuration. Because the dockerfile doesn’t have the default name, the mapping syntax is required. The context is ., representing the directory of the Compose file which is also where the dockerfile is. The volumes key also plays an important role. The src directory on the host is mounted at &#x2F;src&#x2F;app where the development server expects to find it. Let’s bring up the application using the Compose CLI. Thanks to the image configuration mapping, the built image will be named accumulator and will receive the default tag of latest.</p>
<p>I’ll use the up command which builds the image since there is no prior image to use. I’ll skip ahead to the build part. Each of the instructions in the Dockerfile are executed just like with docker build. I’ll jump ahead to when the image is ready. There are some harmless warnings because some optional dependencies are specific to macs but the image is Linux. The output reports that the accumulator:latest tag is used. Compose also gives a helpful warning telling you to use the build command or pass the –build option to rebuild the image. Let’s verify the app is up and running.</p>
<p>I’ll point my browser to localhost on port 3000 and voila, the accumulator app is up and running. I’ll enter some messages and refresh the page to ensure they are persisted in the database. Everything looks to be working.</p>
<p>I’ll hop back to VS Code, and edit one of the views by adding a colon after Enter messages to accumulate, to confirm that the change gets updated in the browser. I’ll save that change, and refresh the browser.</p>
<p>And there is the colon. That change was to a file that doesn’t require restarting the server. To confirm that nodemon is correctly watching for changes, I’ll modify a server-side JavaScript file.</p>
<p>I’ll add some exclamation marks at the end of the development environment notice that appears in the upper right corner. Going back to the browser and refreshing</p>
<p>We see the changes reflected. No stopping the container and no build command required.</p>
<p>To show that nodemon detected the change, let’s look at the app service’s logs. There it is in green, restarting due to changes. That’s pretty cool. You can use the development image to bundle up all the dependencies and all you need on your machine is the source files. You don’t need the dependencies installed locally. By relying on the image for dependencies, you are a step closer to having parity between development and production because the production image would be using the same dependencies. The chance of the code working on your machine but not in production is greatly reduced. We’ll look more at dev-prod parity in the next lesson.</p>
<p>I’ll take the application down now. And if I bring it back up, do you think the test messages I entered into the application will still be there?</p>
<p>Let’s refresh the page and see. In this case, the messages are gone. Recall that the db image isn’t using a volume, so once the container is removed, everything is gone. That give an easy way to start fresh when developing this app, but you could easily add a volume if you wanted to persist the messages.</p>
<p>Recap<br>This lesson illustrated how to build images and develop with Compose. The next lesson will further build upon what we’ve learned in this Lesson to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/extending-compose-1/">adapt Compose to multiple environments</a> so you can share common configuration between development and production. When you are ready, continue on to the next lesson to see how it’s done.</p>
<h1 id="How-Compose-Handles-and-Combines-Multiple-Files"><a href="#How-Compose-Handles-and-Combines-Multiple-Files" class="headerlink" title="How Compose Handles and Combines Multiple Files"></a>How Compose Handles and Combines Multiple Files</h1><p>We saw how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/building-in-compose-1/">use Compose to build an image</a> in a development scenario where the code is not ready to be sealed into the image. But what about once the code is ready? How can you use Compose to make the production image? Do you need to use two independent Compose files and Dockerfiles? This lesson will clear up these questions.</p>
<p>Agenda<br>I’ll start with a discussion of how Compose handles multiple Compose files.</p>
<p>Then I’ll mention a few considerations for using Compose for production environments.</p>
<p>I’ll finish by reviewing the concepts we discuss in a demo. The demo extends the app from the previous lesson to use Compose for development and production environments.</p>
<p>Multiple Compose Files<br>Although it’s an option to maintain completely separate compose files for each environment you maintain,</p>
<p>Compose has a useful feature that can combine Compose files.</p>
<p>The semantics of combining Compose files is to treat the first file as a base configuration and each additional file overrides configuration specified in the base configuration. The overrides can add configuration that isn’t present in the base configuration as well, not only strictly overriding existing values in the base configuration.</p>
<p>By default, Compose is set up to read two Compose files, the familiar docker-compose file, as well as an optional override file called docker-compose.override.yml.</p>
<p>The -f Compose option can be used multiple times to specify non-default override files. Each override file overriding the previous ones.</p>
<p>The Docker Compose config command is useful when writing and debugging multiple Compose files. It will display the effective Compose file after everything is combined.</p>
<p>The -H option of Compose allows you to manage the application on different Docker hosts, since the different environments are probably on different machines.</p>
<p>Multiple Compose Files Example<br>Let’s consider an example that uses Compose in two environments, one that is not intended for development, and another that is for development. The Dockerfile for the application is shown here. It’s follows the non-development image pattern of copying all of the source code into the image, and installing the dependencies. Note that the source files are in the &#x2F;src directory.</p>
<p>Here is the docker-compose.yml file, which plays the role of the base configuration in our example. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">Compose</a> is instructed to build the web image using the current directory as the context, along with publishing a port on the host, and running a redis container. During the build the source files in the current directory get added to the image. The image can then create containers that don’t have any dependency on source files outside of the container. This is what you want in a non-development scenario. Let’s see how an override Compose file can extend the application to work in development scenarios. Can you guess?</p>
<p>On the right, I’ve shown a development override Compose file. To use source files on your local machine instead of inside the image, you can use a volume. The example mounts the current directory at &#x2F;src in the container. Because of the way that layering works in images, the files that are in the container are effectively overwritten by the volume. With this override, you can modify the source files on your local machine and see the changes reflected in a running container. It isn’t always possible to use a common Dockerfile for each environment you intend to use the container. In that case, you can override the Dockerfile for each environment using the build’s dockerfile configuration in each Compose file.</p>
<p>Production Considerations<br>Moving to production environments is worthy of a course of its own. I will mention a few Compose file considerations for moving to a production environment, but know that there is more to it.</p>
<p>Remove any volumes for source code. You want the code to be frozen inside of a production image.</p>
<p>Consider using the always restart policy so services will automatically bring themselves back up if they exit.</p>
<p>Avoid host port conflicts that can prevent an application from coming up by letting Docker choose the host ports to use. You do this by only specifying the container port in the ports sequence.</p>
<p>Usually the runtime and the application have environment variables to configure production mode. This may reduce verbosity of logs and disable debug information.</p>
<p>The last one that I’ll mention, is consider additional services that may be useful in production. For example, monitoring and log aggregation services. Now let’s wrap up with a demo illustrating some of the concepts of using multiple compose files for development and production environments.</p>
<p>Demo<br>I have a project open that is similar to the example in the previous lesson, except I’ve modified it to use multiple Compose files. The development Dockerfile is the same as before. This is the file, dev.dockerfile, to refresh your memory. Because the application uses different ports and default commands for development and production, I’ve written a separate production Dockerfile.</p>
<p>This is it here, prod.dockerfile. It doesn’t install any development dependencies like nodemon and it copies all of the source files into the image, not just the dependency file. The exposed port has changed from 3000 to 8080 as well.</p>
<p>Now let’s get to the main subject, the Compose files. This is the base Compose file, docker-compose.yml, that has configuration that is common to both environments. Then each environment has its own override file to add its own unique configuration. Alternatively, you could use the base configuration for the production environment, and have a single override file for development that not only adds but actually overrides settings in the base configuration. The configuration in this file is similar to the Compose file in the previous lesson’s demo with the development specific configuration removed. One change is that the image has been given a registry URL. This allows you to later use docker-compose push to push the production image to a corporate registry, for example.</p>
<p>Looking at the development override Compose file now. This is the development specific configuration extracted from the single configuration in the previous lesson. When you tell Compose to use the base configuration plus this configuration as an override, it effectively reproduces the single development environment Compose file in the previous lesson. We’ll actually see how they combine with the docker-compose config command in awhile.</p>
<p>And over to the final file we’ll look at, the production override Compose file. I’ve use the name prod but the image could and probably should be used for automated testing in a continuous integration system and&#x2F;or staging before going into production. First, note the build configuration is set to use the prod dockerfile. We can also see several of the production considerations manifested in this override file. There is no volume for the app, both services are configured with the always restart policy, no specific host port is set to avoid port conflicts, and a production environment variable is set to configure the application for production. The last override is that the database is set to use a named volume to persist its data. In development it was considered optional, but we definitely want a volume in the production environment. Now let’s take a look at how to use multiple Compose files on the command-line.</p>
<p>I’ll focus on using the config command to show the effective configurations when you specify override files. I’ll start by validating the configuration for the development environment. The command just adds an extra -f for the development override file. The output shows the combined configuration that Docker Compose will use. The output of config is more explicit than what was in the Compose files, in terms of using absolute paths and not having implicit null values. But we can clearly see that both configuration’s options are there. For example, the base configuration’s image and the development override’s source code volume. I’ll clear that and have one more go at it, this time specifying the production override file. And here we can see the effective configuration for the production environment. For example, the always restart policy on both services.</p>
<p>Closing<br>In this Lesson, you saw how multiple Compose files and Docker Compose’s override feature makes it easy to manage your multi-container applications in multiple environments. We also discussed some considerations for when one of those environments is production. When you are ready, continue on to the next <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/summary-6/">lesson</a> where we’ll wrap up the course.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Congratulations! You made it to the end of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/course-introduction-18/">course</a>. I hope you enjoyed the course and learned a lot along the way. Let’s talk a walk down memory lane together.</p>
<p>Course Review<br>We began the course by introducing the problems that Compose aims to solve. Namely, simplifying the process of managing multi-container applications in Docker. Then we dove in to study the anatomy of Compose files. We covered a relatively small but powerful subset of YAML that I said was enough to understand all the examples in the course. Looking back, it was enough wasn’t it. I think you’ll find that it’s enough to understand almost any Compose file you come across. The YAML mapping used to declare multi-container applications in a Compose file has four top-level keys: version, services, volumes, and networks. We went into the details of each and drew upon experience with Docker commands to make it easier to write our own Compose files. We then covered the other part of Compose, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/managing-applications-with-docker-compose/docker-compose-cli-1/">Compose CLI</a>. In addition to many familiar commands that are generalized to work with multi-container applications, Compose introduces two new commands: up and down. We then demonstrated how to use Compose to manage WordPress, a popular content management system. The lessons learned applied to managing applications made up of pre-built images. We then looked at using Compose in development scenarios when you need to build the image yourself and still be able to modify the source code. This is possible with the build configuration that works with plain old Dockerfiles and strategically mounting volumes. Lastly, we saw how Compose can be used to manage applications that target multiple environments, such as development and production. Compose override files make it possible to do without duplicating common configuration shared between environments.</p>
<p>Learning Outcomes<br>By taking this course, you have achieved the following learning outcomes:<br>• Understand the anatomy of Docker Compose files<br>• Configure your application using Docker Compose files<br>• Use the Docker Compose CLI to manage the entire lifecycle of applications<br>• Build your own images from source code with Docker Compose<br>• Be able to extend Docker Compose files to adapt to multiple environments</p>
<p>Learning More<br>There are a few places I’d recommend for learning more. Try out some of the labs, quizzes or other courses on Cloud Academy. There is content on Compose as well as Docker swarm mode which lets you run multi-container applications on a cluster of computers instead of a single host.<br><a target="_blank" rel="noopener" href="https://cloudacademy.com/">https://cloudacademy.com</a></p>
<p>The Docker Compose docs are a great place to learn all the intricacies of Compose and to stay on top of release changes.<br><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/">https://docs.docker.com/compose/</a></p>
<p>The Docker Compose GitHub repository is also a good place to learn about what’s coming next for Compose and learn from discussions around reported issues. Of course, you can file your own issues and even contribute to the code base.<br><a target="_blank" rel="noopener" href="https://github.com/docker/compose">https://github.com/docker/compose</a></p>
<p>The broader GitHub community can also be a great place to learn more about using Compose. There are around a quarter of a million Compose files using the default docker-compose.yml or .yaml name on GitHub.<br><a target="_blank" rel="noopener" href="https://github.com/search?q=filename:docker-compose.yml+filename:docker-compose.yaml&amp;type=Code">https://github.com/search?q=filename:docker-compose.yml+filename:docker-compose.yaml&amp;type=Code</a></p>
<p>Feedback<br>I’m happy to hear from you. I make content for you. If you have any feedback, please get in touch with me by leaving a comment on the Comments tab below the video, by emailing <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, or by connecting with me on Twitter where my handle is @LoganRakai.</p>
<p>Thank you<br>That’s all for this course on managing applications with Docker Compose. I want to end by thanking you for going through the course with me. It’s been a blast! Now go on and put what you’ve learned here into action. Until next time, I’m Logan Rakai with Cloud Academy.</p>
<h1 id="1Course-Introduction"><a href="#1Course-Introduction" class="headerlink" title="1Course Introduction"></a>1<strong>Course Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/docker-compose-training">Course Github Repo</a></p>
<h1 id="8Summary"><a href="#8Summary" class="headerlink" title="8Summary"></a>8<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/">Docker Compose docs</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/docker/compose">Docker Compose GitHub</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/search?q=filename:docker-compose.yml+filename:docker-compose.yaml&type=Code">Docker Compose YAML files</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Docker-Swarm-Playground-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Docker-Swarm-Playground-7/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Docker-Swarm-Playground-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:36" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:36-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:54" itemprop="dateModified" datetime="2022-11-20T22:25:54-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Docker-Swarm-Playground-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Docker-Swarm-Playground-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:34" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:34-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:20" itemprop="dateModified" datetime="2022-11-20T22:25:20-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Manage-Your-Cluster-Using-Docker-Swarm-Mode-6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:33" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:33-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 03:02:06" itemprop="dateModified" datetime="2022-11-21T03:02:06-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Container-Orchestration-With-Docker-Swarm-Mode-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to Container Orchestration with Docker Swarm Mode.</p>
<p>About Me<br>I’m Logan Rakai and I’ll be your instructor for this Course. I’m a content researcher and developer here at Cloud Academy. I’ve been thinking a lot about how to maximize the return on your time invested in this course. I’m confident that you’ll be confident in your ability to orchestrate containers with Docker Swarm mode after completing the course. I have over ten years of experience in software research and development including five years in the cloud. I’m an AWS Certified DevOps Engineer Professional and a Microsoft Certified Solutions Expert: Cloud Platform and Infrastructure. You can connect with me on LinkedIn or on Twitter.</p>
<p>Who this course is for<br>This course is for anyone that is interested in orchestrating distributed systems at any scale.<br>DevOps Engineers<br>Site Reliability Engineers<br>Cloud Engineers<br>Software Engineers</p>
<p>Prerequisites<br>In order to get the most out of this course, you should have experience with Docker. You should have a solid understanding of images, networks, volumes, and have experience using Docker compose for managing multi-container applications. If you need to brush up on any of those topics, Cloud Academy has some great courses for that. “Introduction to Docker” by Ben Lambert covers fundamental Docker concepts and “Managing Applications with Docker Compose” by yours truly for working with multi-container applications using Docker Compose.</p>
<p>You can follow along with the course examples, and I’d encourage you to. You will need Docker version 1.13 or greater installed. I’ll be using a Mac with Docker for Mac version 17.12 installed but you can also follow along in Linux. You should have VirtualBox installed as well. Swarm mode in Windows has some additional limitations mainly around network encryption. I’ll mention the limitations when we cover the relevant topic. Almost everything we discuss will apply to Windows environments but I’ll be using Linux containers in the demos.<br>I’ve put resources that I use for the demos on GitHub. A clickable link is available at the bottom of the transcript for this lesson. Most of the work will happen at the command-line although we will work with some files near the end of the course. I’ll be using Visual Studio Code for working with the files but you could use whatever you are comfortable with.</p>
<p>Learning Objectives<br>After completing this course, you will be able to:<br>“ Describe what Docker swarm mode can accomplish<br>“ Explain the architecture of a swarm mode cluster<br>“ Use the Docker CLI to manage nodes in a swarm mode cluster<br>“ Use the Docker CLI to manage services in a swarm mode cluster<br>“ Deploy multi-service applications to a swarm using stacks</p>
<p>Feedback<br>I’m happy to hear from you. I make content for you and I want it to be as good as it can be. If you have any feedback, please get in touch with me by leaving a comment on the Comments tab below the video, by emailing <a href="mailto:&#x73;&#117;&#x70;&#112;&#111;&#114;&#116;&#x40;&#x63;&#108;&#x6f;&#117;&#100;&#x61;&#99;&#97;&#x64;&#101;&#109;&#121;&#46;&#99;&#x6f;&#x6d;">&#x73;&#117;&#x70;&#112;&#111;&#114;&#116;&#x40;&#x63;&#108;&#x6f;&#117;&#100;&#x61;&#99;&#97;&#x64;&#101;&#109;&#121;&#46;&#99;&#x6f;&#x6d;</a>, or by connecting with me on Twitter where my handle is @LoganRakai.</p>
<p><a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/docker-swarm-mode-training">Course resources on GitHub</a></p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>Welcome to this overview lesson on Docker swarm mode. We’ll get a conceptual understanding of swarm mode in this lesson before understanding its architecture and diving into the details and demos in following lessons.</p>
<p>Agenda<br>We’ll start the lesson by getting an understanding of why we need swarm mode. After that, we’ll highlight some features of Docker swarm mode to understand what swarm mode can do for you. Next, we’ll learn about the main concepts of Docker swarm mode. Lastly, I’ll touch on the universal control plane which is Docker’s enterprise product built on top of swarm mode.</p>
<p>Why Swarm?<br>Docker has made great strides in advancing development and operational agility, portability, and cost savings by leveraging containers. You can see a lot of benefits even when you use a single Docker host. But when container applications reach a certain level of complexity or scale you need to make use of several machines. Container orchestration products and tools allow you to manage multiple container hosts in concert. Docker swarm mode is one such tool.</p>
<p>Swarm Mode<br>Swarm mode is a feature built into the Docker Engine providing native container orchestration in Docker. Swarm mode is something you need to enable and when you do, the Docker Engine is said to be running in swarm mode. With swarm mode you can control a cluster of machines in a way that is similar to running and about as easy as running a single Docker Engine. Of course there are some differences and we’ll see them in this course.</p>
<p>Calling swarm mode a container orchestration feature doesn’t quite do it justice. It encompasses cluster management, container orchestration, and more. Some of the main features of swarm mode include:<br>“ Integrated cluster management within the Docker Engine without any additional software<br>“ A declarative service model that allows you to declare what you want and Docker can create it for you. There is no need for you to specify the sequence of commands to realize what you want.<br>“ Swarm mode is able to monitor the cluster state and reconcile any differences between the desired state and the actual state. (Desired state reconciliation)<br>“ Swarm mode uses certificates and cryptographic tokens to secure the cluster<br>“ As well as features you’d expect in a container orchestration offering such as service scaling, multi-host networking, resource-aware scheduling, load balancing, rolling updates, restart policies, and more.</p>
<p>Name disambiguation<br>Docker actually has two cluster management solutions. Both are open source and live on GitHub. Surprisingly, they are both called swarm. Docker Swarm, with a capital S, was the first container orchestration project by Docker. It uses the Docker API to turn a pool of Docker hosts into a single, virtual Docker host using a proxy system. To reduce confusion, Docker Swarm is now referred to as Docker Swarm standalone in documentation.</p>
<p>Although Docker Swarm standalone project is still maintained, the newer container orchestration tool is called Swarmkit. It is what is built into the Docker Engine since Docker version 1.12. You might see swarmkit mentioned from time to time, but this is the most commonly referred to as swarm mode. Docker recommends Swarm mode unless you have a specific reason to use Swarm standalone.</p>
<p>So now you know that there are two swarms, Swarm standalone and swarm mode. It’s useful to be aware of the distinction. You might search for Docker swarm online and stumble upon something related to Swarm standalone when you wanted swarm mode. To avoid any confusion, this course deals exclusively with swarm mode. In the remainder of the course, if I refer to swarm, I’m referring to Docker running in swarm mode. In practice, it’s pretty common to drop mode from the name although it can potentially lead to misunderstandings. In the remainder of the lesson, we’ll cover the architecture of swarm mode.</p>
<p>Swarm Mode Concepts<br>Before going too far, we’ll cover some of the main concepts and swarm mode terminology.</p>
<p>A swarm consists of one or more Docker Engines running in swarm mode. Each instance of the Docker Engine in the swarm is referred to as a node. It is possible to run multiple nodes on a single machine. For example, by using virtual machines. In production environments, you should use multiple machines to ensure availability of the swarm if a machine goes down.</p>
<p>Nodes can participate in a swarm by taking on specific roles: managers and workers. Every swarm requires at least one manager. Managers have several responsibilities, but we’ll start simple and consider one main responsibility. Managers accept specifications from users and drive the actual state of the swarm to the specified desired state. They do so by delegating units of work to workers in the swarm. Workers are primarily responsible for running the delegated units of work. Workers also run an agent which reports back to managers on the status of their work. A node can be either a manager, or a worker.</p>
<p>The specifications that users submit to managers are called services. This is the same concept as a service in Docker Compose. The service configuration declares its desired state, which includes the networks and volumes it uses, the number of replicas, resource constraints, and other details. A manager will ensure the actual state of the swarm matches the service configuration. if it is possible to realize in the swarm. There may not be enough available resources in the swarm which would prevent the desired state from being achieved. Docker will also make the changes necessary to reconcile the actual state with the desired state if you update a service.</p>
<p>There are two kinds of services: replicated and global. You specify the number of replicas for a replicated service based on the scale you desire. A global service allocates one unit of work for each node in the swarm. Global services can be useful for monitoring services, for example.</p>
<p>The units of work delegated by managers to realize a service configuration are referred to as tasks. The tasks correspond to running containers that are replicas of the service. Managers schedule the tasks across nodes in the swarm. If a node leaves the swarm, the tasks that the node was running will be scheduled onto the remaining nodes in the swarm.</p>
<p>By default, manager nodes also run tasks like workers. You can configure managers to participate exclusively in managing the cluster and that is probably a good idea in production. Allowing managers to run tasks by default enables easy to setup and functional single node swarms.</p>
<p>Universal Control Plane<br>The last topic I want to cover in giving an overview of swarm mode is the Universal Control Plane (UCP). UCP is only relevant for the enterprise edition of Docker so I will only briefly touch on it.</p>
<p>Working with swarm mode is similar to working with Docker. You interact with it through the Docker CLI. That is great, but sometimes it can be nice to have a web interface to manage and visualize the cluster and containers. UCP is Docker’s enterprise offering that is built on top of swarm mode to provide a web interface for cluster management and role-based access control. Because UCP is built on swarm, what you learn in this course applies to UCP as well.</p>
<p>Closing<br>All right, now we have a basic understanding of swarm mode. We will take closer look at how swarm mode works by understanding main components of its architecture in the next group of lessons.</p>
<h1 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h1><p>Thanks for joining me for this lesson on Docker swarm mode architecture. You heard about the great benefits swarm mode provides in the previous lesson. In these architecture lessons, we’ll understand more about the parts of swarm mode that enable it to accomplish all those great benefits, starting with networking. This lesson and the following architecture lessons build the foundations for using swarm mode. I promise we’ll be seeing swarm mode in action in the demos of the next lesson group in the course.</p>
<p>Agenda<br>This lesson will cover everything that is unique to swarm mode and networking:<br>“ (Overlay networks) Starting with a Docker network type exclusive to swarm mode, the overlay network.<br>“ (Service discovery) After that, we’ll discuss how services in a swarm can be discovered across multiple host swarm networks.<br>“ (Load balancing) On a related note, we’ll see how load is balanced across all the replicas of a service.<br>“ (External access) Then the mechanisms for accessing the swarm services from outside the swarm will be explored.</p>
<p>Networking<br>The networking requirements in a swarm are much more complex than using a single Docker host. Services need to communicate with one another and the replicas of the service can be spread across multiple nodes. Fortunately, Docker includes a network driver that makes multi-host networking reliable, secure, and a breeze to set up.</p>
<p>Overlay Networks<br>The driver I’m referring to is the overlay network driver. With the overlay driver a multi-host networking in a swarm is natively supported. There is no need to perform any external configuration. You can attach a service to one or more overlay networks, in the same way you would attach a container to one or more user-defined networks when not running in swarm mode.<br>Overlay networks only apply to swarm services and can’t be connected to by containers that aren’t part of a swarm service. Managers automatically extend overlay networks to nodes that run tasks requiring access to a given overlay network.</p>
<p>Network isolation and firewalls<br>It’s a good time to review Docker network isolation and firewall rules. These rules apply to overlay networks just as they do for bridge networks.<br>Containers within a Docker network are permitted access on all ports of containers in the same network.<br>Access is denied between containers that don’t share a common network.<br>Traffic originating inside of a Docker network and not destined for a Docker host is permitted. For example, access to the internet. However, any network infrastructure outside of Docker may still deny the traffic.<br>Ingress traffic, or traffic coming into a Docker network, is denied by default. Ports must be published in order to grant access form outside of Docker.</p>
<p>Service Discovery<br>With services distributed across multiple nodes, a service discovery mechanism is required in order to connect to the nodes running tasks for a service. Swarm mode has an integrated service discovery. It is based upon the domain name system (DNS). The DNS is internal to Docker and implemented in the Docker Engine. It is used for resolving names to IP addresses.</p>
<p>Actually, the same service discovery system is used when not running in swarm mode. Service discovery in Docker is scoped to a network. When you are in swarm mode, the network can be an overlay spanning multiple hosts. But the same internal DNS system is used. All nodes in a network store corresponding DNS records for the network. Only service replicas in the network can resolve other services and replicas in the network by name.</p>
<p>Internal Load balancing<br>There are some unique service discovery considerations for Swarm mode. Each individual task is discoverable with a name to IP mapping in the internal DNS. But because services can be replicated across multiple nodes, which IP address should a service name request resolve to? Docker assigns a service a single virtual IP (VIP) address, by default. Requests for the virtual IP address are automatically load balanced across all healthy tasks spread across the overlay network. By using a virtual IP, Docker can manage the load balancing allowing clients to interact with a single IP address without considering load balancing. It also makes the service more resilient since the service can scale and tasks can change the nodes that they are scheduled on but clients are sheltered from the changes.</p>
<p>Internal load balancing example<br>To illustrate how service discover and load balancing work in swarm mode, consider two services deployed in a swarm service A and service B. Service A has a single replica while service B has two replicas. When service A makes a request for service B by name, the virtual IP of service B is resolved by the DNS server. Service A uses the virtual IP to make a request for service B. Using support for ip virtual servers (IPVS) the request for the virtual IP address is routed to one of the two nodes running service B tasks.</p>
<p>DNS Round Robin<br>Besides the default virtual IP, you can configure load balancing using DNS round robin (DNS RR). You can configure the load balancing on a per service basis. When DNS round robin is used, the Docker Engine’s DNS server resolves a service name to individual task IP addresses by cycling through the list of IP addresses of node’s running a task in the service. If you need more control over load balancing than a virtual IP can give you, DNS round robin should be used for integrating your own external load balancer.</p>
<p>External Access<br>We’ve covered access to services within a Docker network, but what about accessing a service from the outside? With a single Docker host, you would publish a container port on the host to permit access to a container. Similar functionality is still available in swarm. But there are actually two modes for publishing ports in swarm.</p>
<p>Host mode<br>The first is the same as you would expect when publishing a port when not running in swarm mode. The container port is published on the host that is running the task for a service. This mode is referred to as host mode service publishing. You need to be careful with specifying a host port in host mode. If you have more tasks than available hosts, tasks will fail to run because the host port can only be bound to one task. You can omit a host port to allow Docker to assign an available port number in the default port range of 30000-32767. However, this can make it more difficult to work. Also, there isn’t load balancing unless you configure it externally. Obviously, that is useful when you don’t want load balancing, but what about when you do?</p>
<p>Ingress mode<br>Because services can be replicated and tasks can be rescheduled onto different nodes as the state of the swarm changes, it is useful to have the option to load balance a published port across all tasks of a service. This is referred to as ingress mode service publishing. For convenience, all nodes in the swarm publish the port. This is different from host mode where a port is only published if the node is running a task for the service. In ingress mode, requests are round robin load balanced across the healthy instances of the service’s tasks regardless of the node that receives the request.</p>
<p>Ingress mode is the default service publishing mode. It’s ideal when you have multiple replicas of a service and need to load balance between them. Host mode publishing is useful when you have an external service discovery service and potentially for global services where one task for a service runs on each node. For example, a global service that monitors each node’s health shouldn’t be load balanced since you want to get the status of a specific node.</p>
<p>Routing Mesh<br>At this point, you might be wondering how ingress mode publishing work. The magic happens in what is called the routing mesh. The routing mesh combines two of the swarm components that we discussed earlier: an overlay network, and a service virtual IP.</p>
<p>When you initialize a swarm, the manager creates an overlay network named ingress. Every node that joins the swarm is in the ingress network. The sole purpose of the ingress network is to transport traffic from external clients that is destined to published service ports to the service inside the swarm.</p>
<p>When a node receives an external request on the ingress network the node resolves the service name to a virtual IP address. This process is carried out using the same internal DNS server as we discussed in the internal load balancing. The IP virtual server then load balances the request to a service replica over the ingress network.</p>
<p>Because every node is in the ingress network, every node can resolve the external requests can handle the external requests. The nodes need to have a couple of ports open for all of this magic to work:<br>o Port 7946 for both TCP and UDP protocols to enable container network discovery.<br>o Port 4789 for the UDP protocol to enable the container ingress network.</p>
<p>It’s worth mentioning that you could add an external load balancer on top of the load balancing provided by the routing mesh. For example, if you have nodes running in the cloud, you can have the nodes in a private subnet so they aren’t directly accessible from the internet. You could provision a cloud load balancer to handle requests from the internet and load balance them across nodes in the swarm. The swarm nodes then load balance again across the nodes running tasks for the service.</p>
<p>As a final note on the routing mesh, if you are planning to use the routing mesh on Windows, you need to be running version 17.09 or greater.</p>
<p>docker_gwbridge<br>Besides the ingress network, Docker also creates a second network when running in swarm mode called docker_gwbridge. The docker_gwbridge is a virtual bridge that connects the overlay networks (including the ingress network) to an individual Docker daemon’s physical network. This interface provides default gateway functionality for all containers attached to the network. Docker creates it automatically when you initialize a swarm or join a Docker host to a swarm, but it is not a Docker device. It exists in the kernel of the Docker host. You can see it if you list the network interfaces on your host.</p>
<p>Recap<br>There was quite a few topics related to networking in swarm mode. Let’s recap the main points:<br>“ Swarm mode includes a new type of Docker network, the overlay network. Overlay networks make it easy to use multi-host networking in a swarm.<br>“ The same internal DNS service discovery mechanism used when not running in swarm mode is used in swarm mode. The internal DNS naturally extends to multi-host networks.<br>“ The services in a swarm can be load balanced by using a virtual IP address or by DNS round robin.<br>“ External access to the swarm is made possible by publishing ports. There are two modes for publishing in swarm mode: host and ingress.<br>o In host mode each service replica publishes it’s container port on the host. No load balancing is used.<br>o In ingress mode, every node in the swarm publishes the port and requests are load balanced across all the replicas of a service. Any node can handle requests for the service even if the node doesn’t have a replica of the service itself.<br>“ Ingress mode is made possible by the swarm routing mesh which uses two default swarm networks: the ingress overlay network and docker_gwbridge network</p>
<p>Closing<br>In the next lesson, we’ll look into swarm mode container orchestration features including rolling updates and scheduling constraints. When you’re ready continue on to the next lesson to see how swarm can orchestrate containers.</p>
<h1 id="Orchestration"><a href="#Orchestration" class="headerlink" title="Orchestration"></a>Orchestration</h1><p>Swarm mode is made to be familiar to single host Docker users. When you deploy a service, it is similar to running a container. You can specify an image, volumes, networks, published ports, After all, service tasks ultimately run containers. But there are container orchestration features of swarm mode that are unique to running services in a swarm.</p>
<p>Agenda<br>We’ll look at the following orchestration features of swarm mode:<br>“ (Service placement) Which nodes service tasks are placed on<br>“ (Update behavior) how service updates are rolled out, and<br>“ (Rollback behavior) how services can be rolled back to a previous version.</p>
<p>Service placement<br>As we’ve discussed, services can declare a set number of replicas as a replicated service or can be started on every worker node in a cluster as a global service. For replicated services, decisions need to be made by swarm managers for where service tasks will be scheduled, or where the service will be placed. A replicated service’s tasks will be spread across nodes by default. That is to promote high availability in case a node fails. But there are three ways that you can influence where a service is placed:<br>\1. CPU and Memory reservations<br>\2. Placement constraints<br>\3. Placement preferences<br>You can specify each at service creation time. Global services can also be restricted to a subset of nodes with these conditions. Although a node will never have more than one task for a global service. Let’s take a closer look at each.</p>
<p>CPU and Memory reservations<br>Similar to running individual containers, you can declare CPU and memory reservations for services. Each service task can only be scheduled on a node that has enough available CPU and memory to meet the given reservations. Any tasks that remain stay in a pending state until a node with sufficient resources becomes available. Global services will only run on nodes that meet a given resource reservation.</p>
<p>Setting sufficient memory reservations for services is important when there isn’t an abundance of CPU and memory available for the applications you are running. If services attempt to use more memory than is available, the container or Docker daemon could get killed by the out of memory or OOM killer.</p>
<p>Placement constraints<br>Placement constraints allow you to restrict the placement of tasks by providing equality and inequality conditions. The conditions compare node attributes to a string value. There are a few built-in attributes for each node<br>\1. node.id matches the ID of a node<br>\2. node.hostname matches a node’s hostname<br>\3. node.role matches a node’s role, either manager or worker</p>
<p>You can also define your own labels. You can configure labels on a Docker engine or on a node. Engine labels are usually used to indicate things like operating system, system architecture, available drivers. An example is engine.labels.operatingsystem and values could be Ubuntu 14.04 or Windows Server 2016. Node labels are added by Swarm administrators for operational purposes. Node labels can indicate they type of application a node is intended to run, the datacenter location a node is in, the server rack a node is in, et cetera. An example is node.labels.datacenter and values could be north, south, east, or west.</p>
<p>When you provide multiple placement constraints for a service, all constraints must be satisfied by a node in order to be scheduled a service task. If resource reservations are also provided, all constraints and resource reservations must be met. This is true for replicated and global services.</p>
<p>Placement Preference<br>Placement preference is not required as was the case for resource reservations and placement constraints. Instead, placement preferences influence how tasks are distributed across appropriate nodes. Currently the only distribution option is spread which will evenly spread tasks.<br>Labels are again used as the attribute for spreading tasks. For example, assume every node in a swarm has a datacenter label with either east or west as the value. Using the datacenter label and the spread placement preference, half of the tasks will be scheduled on east datacenter nodes and the other half on west datacenter nodes.</p>
<p>Multiple placement preferences can be specified. In this case a hierarchy of preferences is created. For example, if the first preference is datacenter and the second Is server-rack, tasks will be evenly spread across nodes in each datacenter, and within each datacenter tasks are spread evenly across racks.</p>
<p>Nodes that are missing a placement preference label are included in the spread and receive tasks in proportion equal to all other label values. They are treated as the group having the null value for the label. Placement preferences are ignored by global services.</p>
<p>That’s all that there is to influencing service placement in swarm.</p>
<p>Update Behavior<br>You can also configure the way that swarm applies updates to services. Swarm supports rolling updates where a fixed number of replicas are updated at a time until all service replicas have been updated.</p>
<p>You can configure several update parameters:<br>\1. Update parallelism, which sets the number of tasks the scheduler updates at a time<br>\2. Update delay, which sets the amount of time between updating sets of tasks, and<br>\3. Update failure action, which can be set to pause, continue or automatically rollback if an update fails. The default is to pause.<br>These are the three main settings. There are also settings to configure what qualifies as failure. You can set a ratio for the number of failed task updates to tolerate before failing a service update, and set the frequency for monitoring for a failure.</p>
<p>These parameters give you some flexibility in how aggressively or conservatively you roll out an update to the swarm.</p>
<p>Rolling Back Updates<br>Docker swarm keeps track of the previous configuration for services. This allows you to rollback manually at any time or automatically when an update fails, as we discussed.</p>
<p>The same options available for configuring update behavior are available separately for configuring rollbacks. For example, rollback parallelism sets how many nodes to roll back at a time.</p>
<p>Recap<br>In this lesson, we saw how you can influence the nodes that swarm schedules services on by using resource reservations, placement constraints, and placement preferences. Resource reservations and placement constraints must be satisfied, while placement preferences won’t prevent a task from being scheduled. We also discussed how rolling updates and rollbacks can be configured in Swarm. Updates and rollbacks share the same available configuration options.</p>
<p>Closing<br>In the next lesson, we’ll see how swarm mode keeps a consistent view of the swarm. An important topic for any distributed system. When you are ready, continue on to the next lesson to learn about swarm mode consistency.</p>
<h1 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h1><p>Consistency is an important consideration for any distributed system. In this lesson, we’ll look at the consistency model of swarm mode and how it can impact how you operate a swarm.</p>
<p>Agenda<br>To kick things off we’ll discuss:<br>“ (Consistency) the consistency problem and<br>“ (Raft) how swarm mode goes about solving it, in particular the Raft Consensus algorithm.<br>“ (Tradeoffs) We’ll cover just what you need to know of Raft to understand key tradeoffs that you should consider when deciding on the composition of your swarm.<br>“ (Raft Logs) Lastly, we’ll talk about the raft logs where the cluster state is stored.</p>
<p>Consistency<br>We have seen that swarm mode can include several manager and worker nodes in a swarm. This provides fault tolerance if a node were to go down and ensures services are highly available. But with multiple managers, how does swarm make decisions regarding the state of the cluster? Do nodes in the swarm share a consistent view of the cluster or could one node have a different view than the other? And if so, for how long? These questions all touch on the issue of consistency.</p>
<p>In swarm mode, managers all share a consistent internal state of the entire swarm. This avoids any potential issues that could arise if managers were allowed to eventually converge to a shared state. Workers, on the other hand, do not share a view of the entire swarm. That is exclusively a manager responsibility.</p>
<p>The managers maintain a consistent view of the state of the cluster by using a consensus algorithm. There are several consensus algorithms to choose from and the implementation details are outside the scope of this course. But the consensus algorithm has an impact on how the swarm operates. We’ll look at the basics of swarm modes consensus algorithm so we can understand the implications in operating a swarm.</p>
<p>Raft Consensus<br>The consensus algorithm used by managers to maintain a consistent view of the state of the cluster is called Raft. Raft achieves consensus by electing one manager as the leader. The elected leader makes all of the decisions for changing the state of the cluster to bring it to the desired state. For example, the leader accepts new service requests and service updates and also decides how to schedule tasks.</p>
<p>In order to maintain a consistent view across the managers, the decisions aren’t acted upon until a majority of managers agree on the proposed changes to the cluster. A manager “agrees” simply by receiving a proposed change and acknowledging they received it. When the leader is certain a majority of managers have received the proposed change, the change can be implemented. In this context, the majority of managers are referred to as a quorum.</p>
<p>The reason why a quorum is enough to proceed is because Raft limits how many managers failures it can tolerate. If you have N managers in a swarm, Raft allows for (N-1)&#x2F;2 failures. In the case of a three manager swarm, that means 3 minus 1 divided by two is one, so one manager can fail and the swarm can continue to operate as usual. If two managers were to fail, the cluster state would freeze until a quorum of managers again became available. In the absence of a quorum, currently running services will continue to run but no new scheduling decisions take place.</p>
<p>Regarding leader elections, when a swarm is initialized the first manager is automatically the leader. If the currently elected leader fails or voluntarily steps down, say to perform system updates, an election between remaining manager nodes takes place. Until a newly elected leader is chosen, the cluster state is frozen.</p>
<p>Manager Tradeoffs<br>After that overview of Raft consensus, you might be tempted to add a lot of managers to your swarm. The more managers, the more failures your swarm can tolerate and remain fully operational. Although, that is true, the more managers that are in the swarm also increases the amount of managerial traffic required for maintaining a consistent view of the cluster and the amount of time it takes to achieve reach consensus with every state change. Although increasing managers does increase fault-tolerance it generally decreases performance and scalability.</p>
<p>There are some general rules for setting the number of managers:<br>You should usually have an odd number of managers. Having an even number of managers doesn’t improve the fault tolerance compared to having one less manager and increases communication overhead.<br>A single manager swarm is acceptable for development and test swarms. Because a single manager swarm can’t tolerate any failures, it is not something you should use in production.<br>A three manager swarm can tolerate one failure, while a five manager swarm can tolerate two.<br>Docker recommends a maximum of seven managers which can tolerate three manager failures. Above seven has too much of an impact on performance to be beneficial.</p>
<p>However many managers you settle on, you will want to distribute them across availability zones to maintain a fully operational swarm in the event of a datacenter outage. Docker recommends distributing across at least three availability zones in production.</p>
<p>Working Manager<br>There is another tradeoff when considering managers in a swarm. Have you heard the bad joke that goes “Don’t stand around doing nothing. People will think you’re the boss.” In swarm mode, you need to consider whether or not you let the boss, or the managers, do work. By default managers perform worker responsibilities, namely running tasks. But that has more to do with enabling single node swarms than anything.</p>
<p>Because managers participate in the Raft consensus process, it can be detrimental to the performance of the swarm if managers are overly utilized. You can use conservative resource reservations to make sure that managers won’t become starved for resources. To be on the safe side, you can also prevent any work from being scheduled on manager nodes by draining them. Draining essentially removes any tasks currently on a node and preventing new tasks from being scheduled to it.</p>
<p>Worker Node Tradeoffs?<br>You might be wondering if there are any tradeoffs to consider when adding worker nodes to a swarm. There really isn’t much to worry about in the case of adding more worker nodes. More workers give you more capacity for running services and improves service fault tolerance. More workers don’t affect the manager’s raft consensus process so the swarm performance isn’t harmed.</p>
<p>Workers actually do participate in a consensus process. To exchange overlay network information nodes participate in a weakly-consistent, highly scalable gossip protocol called SWIM. The details are outside the scope of this course and the performance implications are negligible. The protocol is an example of an eventually consistent model where the network state is allowed to differ between nodes but eventually they converge on a consistent view.</p>
<p>Raft logs<br>The last topic we’ll discuss in this lesson is Raft logs. If you arrived at this lesson from a search for log rafts, I’m afraid you’ll need to continue your search. The logs we’re talking about are where the leader manager records the Raft consensus state changes, such as creating a new service or adding a new worker. These logs are what get shared with other managers to establish a quorum.</p>
<p>The Raft logs are persisted to disk. The logs are stored in the raft subdirectory of your Docker swarm data directory. This is &#x2F;var&#x2F;lib&#x2F;docker&#x2F;swarm on Linux by default. As part of a disaster recovery strategy, you can back up a swarm cluster by backing up the entire swarm directory which includes certificates and other files in addition to the raft change logs in the raft subdirectory. You can restore a new swarm from a backup by replacing the directory swarm directory with the backed-up copy.</p>
<p>Recap<br>That’s everything for this lesson. We started by understanding how swarm mode solves consistency challenges by electing a leader manager and ensuring a majority of managers acknowledge swarm changes. This strategy comes from the Raft consensus algorithm. We understood the tradeoffs between fault tolerance and performance when choosing the number of managers in a swarm, as well as whether or not managers should do work. We finished by discussing the raft logs which are persisted on disk and record all the changes the leader makes to a swarm.</p>
<p>Closing<br>In the next lesson, we will cover the security measures included in swarm mode. Continue on to the next lesson when you are ready to learn about security in Docker swarm mode.</p>
<h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><p>Docker takes security seriously. All the security features that you can use when not running in swarm mode can be used in swarm mode. This includes using trusted images, encrypted communication with docker engines, and kernel security features leveraged by Docker. This lesson covers the security provisions in Docker swarm mode.</p>
<p>Agenda<br>We’ll begin by covering the<br>“ (Cluster Management) cluster management aspects of swarm security<br>“ (Data Plane) Next, we will discuss security of data communicated between services<br>“ (Secrets) After that, we’ll see how swarm secrets are kept secure.<br>“ (Locking a Swarm) Lastly, the locking functionality of a swarm will be described.</p>
<p>Cluster Management<br>Docker swarm mode uses public key infrastructure (PKI) to secure swarm communication and state. Swarm nodes encrypt all control plane communication using mutual transport level security (TLS). When you initialize a swarm, Docker assigns the node that executed the command as a manager. The manager automatically creates several resources for security:<br>“ A root Certificate Authority (CA): This plays the standard role of a CA in PKI by being a trusted entity that issues certificates verifying the identity of certificate holders.<br>“ A key pair: This is the public and private key used for secure communication between nodes in the swarm.<br>“ A worker token: This token is used to by nodes to join the swarm as a worker node. The token is a digest of the root CA and a secret.<br>“ A manager token: This is similar to the worker token but used to join nodes as managers in the swarm.<br>Whenever a new node joins the swarm, the manager issues a new certificate identifying the node. The node uses the certificate for communicating in the swarm. New manager nodes also get a copy of the root CA certificate so that they can take over leadership in the event of an election.</p>
<p>You can use an alternate CA instead of allowing Docker to automatically handle their creation for you. The CA can also be rotated out whenever your security policies require it. Rotating the CA will automatically rotate the TLS certificates of all swarm nodes in the cluster.</p>
<p>Data Plane<br>As for the data plane, you can enable encryption of overlay networks at the time of creation. Any time traffic leaves a host an IPSec encrypted channel is used to communicate with a destination host where the traffic is decrypted. The swarm leader periodically regenerates and distributes the key used for encrypting IPSec data plane traffic. Overlay network encryption is not supported for Windows as of Docker version 17.12.</p>
<p>Raft Logs&#x2F;Secrets<br>The Raft logs are encrypted at rest on the manger nodes. This protects against intruders that gain access to the raft logs on disk. The encryption is particularly important because swarm secrets are stored in the raft logs. Secrets are a feature of swarm that allows you to securely store secrets that can be used by services. This could include passwords, API keys, or any other information you wouldn’t want to be exposed over the network or in a Dockerfile. In Windows, secrets are supported in version 17.06 an above.</p>
<p>Locking a Swarm<br>A challenge with encrypting the raft logs is that the keys used to encrypt the logs needs to be stored somewhere a manager has access to. By default, the keys are stored on disk along with the raft logs. If an attacker gains access to the raft logs, there is a good chance they could gain access to the keys used to encrypt them. They could then decrypt the logs and expose any secrets therein.</p>
<p>For an extra layer of security, a swarm allows you to take control of the key used for encrypting the logs. This allows you to implement strategies where the key is never persisted to disk. This works with a swarm feature called autolock. When a swarm is autolocked, you must provide the key when starting a docker daemon. For greatly improved security, you have to pay that price of requiring manual intervention when a manager is restarted. You can rotate the key at any point or disable autolock so managers can be restarted without intervention.</p>
<p>Recap<br>In this lesson, we saw the security measures that are in place in out of the box when using swarm mode. This included several security layers with regards to managing a cluster. We also saw how overlay network communication can optionally be encrypted to secure communication between services. Swarm supports sharing secrets and uses encryption to protect the secrets on disk in the manager Raft logs. The keys for decrypting the logs are stored on disk by default, but you can use autolocking to take control of the keys and improve the defense of your swarm.</p>
<p>Closing<br>We have now covered all of the architecture topics related to swarm mode. In the following lesson group, we will get hands-on with a swarm and see how to operate and use a swarm from the command line and by describing applications in stack files.</p>
<h1 id="Setting-Up-a-Swarm"><a href="#Setting-Up-a-Swarm" class="headerlink" title="Setting Up a Swarm"></a>Setting Up a Swarm</h1><p>All right, this lesson and remaining lessons focus more on getting hands-on with Docker swarm mode. You will see a lot of the concept knowledge that you’ve built up in the previous lessons in action. These lessons will focus more on the commands you need to use to accomplish tasks related to swarm mode, starting with setting up a swarm.</p>
<p>We will begin by laying out the options available to you for setting up a swarm mode cluster. After that we’ll show two ways to set up a swarm locally on your machine: as a single node swarm and as a multi-node swarm using virtual machines.</p>
<p>There are several options for creating a swarm mode cluster. You should consider factors such as the workloads you want to deploy on the swarm, the management complexity, and cost when determining which option to choose.</p>
<p>The simplest option is creating a single-node swarm. Recall that swarm mode managers can also perform work by default meaning that you can run swarm workloads with a single node. We will see later in this lesson how easy it is to set up. This may be appropriate in development and test scenarios. With no fault tolerance, it is not something to do in production.</p>
<p>The other options are for multi-node clusters. There are unmanaged options that you put you in charge of maintaining the infrastructure and applying patches, and there are more managed options where you can use the swarm as a service without worrying about hardware or software patches.</p>
<p>For the unmanaged option, you would likely have your own compute cluster or private cloud. You would need to ensure Docker is installed on the bare metal servers or on virtual machines running on top. The network firewall would need to allow traffic on the ports swarm mode requires (TCP port 7946 and UDP ports 7946 and 4789). The Universal Control Plane that is available through Docker Enterprise edition can set up an on-prem swarm using a graphical interface.</p>
<p>Here is a screenshot of the UCP web interface in action showing a three node swarm.</p>
<p>We will setup a multi-node cluster using VMs on a single physical host later in this lesson. You could run VMs in a public cloud and make a swarm out of the VMs. However, there may be a better option if you are going to leverage the public cloud.</p>
<p>For the more managed options, you could use cloud provider templates that allow you to set a few parameters and have swarm created for you. This is true for Microsoft Azure, Amazon Web Services, and IBM Cloud. You can also leverage Docker’s Docker Cloud offering to create swarms on Azure and AWS through the Docker Cloud graphical interface. Each option is explained in Docker’s own documentation.</p>
<p>Now, it’s time to demo setting up some swarms. I’ll first setup a single node swarm and then a multi-node swarm using virtual machines and the help of the docker-machine command.</p>
<p>I’m here at my terminal on my mac. I have Docker for Mac installed<br>$ docker version<br>To see the current status of the Docker daemon’s swarm mode, you can use the docker info command and look for the Swarm key:</p>
<p>$ docker info | grep Swarm<br>The inactive value means the daemon is not running in swarm mode.</p>
<p>Now we’ll see how easy it is to start running in swarm mode. The commands relevant to managing a swarm are under the swarm subcommand of the Docker CLI.</p>
<p>$ docker swarm –help<br>In the commands list you see everything from rotating the root certificate authority for a swarm to unlocking a locked swarm. The only command needed to start a new swarm is </p>
<p>$ docker swarm init<br>And that’s all that it takes to start running a single-host swarm. The output tells you that the current node is running as a swarm manager and provides a command for joining workers to the swarm. The value of the token argument is the worker join token. A similar looking token is used for joining manager’s to a swarm as seen from the join-token manager output</p>
<p>$ docker swarm join-token manager</p>
<p>Let’s probe around to see some of the changes that occur when you start running in swarm mode. First, let’s revisit the docker info output<br>$ docker info<br>The state has changed to active to indicate that the daemon is indeed running in swarm mode. There is also a bunch of useful tidbits related to the swarm’s configuration: Number of managers, number of nodes, right down to internals of the Raft consensus algorithm. You can even eek out additional information including TLS certificate info by using the format flag and specifying the Swarm field</p>
<p>$ docker info –format ‘‘<br>I’ll clear that because it is quite unsightly and there is no pretty print option.</p>
<p>We can also verify that the networks we learned in the swarm architecture lessons have been created<br>$ docker network ls<br>Here we see the docker_gwbridge local bridge network for connecting overlay networks to the hosts network and the ingress network used for handling external ingress traffic to the swarm.</p>
<p>That’s all there is to the single-node swarm. You could start using it for development and test scenarios as is. For demonstration purposes, I want to use a multi-node cluster so I will tear down our current swarm. To do that, you force leave the swarm<br>docker swarm leave –force<br>The force flag is required because when the last manager in a swarm leaves all the swarm state goes with it. This is what we want to happen in this case.</p>
<p>I’ll set up a multi-node swarm with two workers and one manager for demonstrating various swarm concepts. Remember that one manager is not a good idea in production, but it is going to be enough to illustrate working with a swarm mode cluster. To quickly create Docker-enabled VMs, I’m going to use docker-machine.<br>$docker-machine<br>docker-machine comes installed with Docker for Mac and Docker for Windows. Only a few docker-machine commands are needed so I’ll explain them as they are required. But know that there is a lot more to docker-machine than what I’ll explain in this lesson.</p>
<p>The first command is create, which does exactly what you’d expect. </p>
<p>$ docker-machine create vm1<br>By default it will create a VM in VirtualBox using an image with docker installed. Virtualbox was installed previously on my mac so everything went off without a hitch. I’m using the names vm1, vm2, and vm3 instead of more descriptive names like manger1 because it’s possible for nodes to change their role in a swarm. However, vm1 will be used as the manager in this lesson. I’ll speed this up until it finishes…<br>Now I’ll create vm2 in the same way</p>
<p>$ docker-machine create vm2<br>And finally vm3</p>
<p>$ docker-machine create vm3<br>Now I’ll use the ls command to list the vms and their IP addresses</p>
<p>$ docker-machine ls<br>The machines are at 192.168.99.100, 101, and 102. The VMs are running the 18.01 edge release which doesn’t have any significant changes in swarm mode compared to the 17.12 stable release I have running on my Mac. I’ll connect to vm1 using docker-machine’s ssh command</p>
<p>$ docker-machine ssh vm1<br>And I’ll show docker info to confirm that docker is installed but swarm mode is inactive</p>
<p>$ docker info<br>To initialize a swarm, I’ll use the same init command as with a single-node setup but with an advertise address:</p>
<p>$ docker swarm init –help<br>The advertise address is the IP address other nodes will use to join the swarm.</p>
<p>$ docker swarm init –advertise-addr 192.168.99.100<br>I’ll give the IP address but you could alternatively provide the network interface name. I’ll copy the prepared join command for joining workers. You can always retrieve the join token later using the join-token swarm subcommand.<br>I’ll drop out of vm1 and ssh into vm2 to join the swarm</p>
<p>$ exit</p>
<p>$ docker-machine ssh vm2</p>
<p>$ docker swarm join …<br>The output acknowledges that the node joined the swarm as a worker node. Now I’ll repeat the process for vm3.<br>To confirm the swarm has one manager and 3 nodes in total, I need to run the docker info command on the manager node, which is vm1. </p>
<p>$ docker info<br>There we have it, a 3-node swarm with one manager setup with the help of docker-machine.</p>
<p>In this lesson, we learned some of the options available for setting up a swarm mode cluster. These included some high touch options putting you in charge of the hardware and software patching to fully automated solutions like the one provided by Docker Cloud allowing you to spin up a swarm on Amazon Web Services or Azure from the comfort of a graphical interface.<br>We then saw how to set up a single node swarm using the swarm init command<br>After we set up a multi-node swarm with the help of docker-machine and Virtualbox. The same init command was used with an advertise address for other nodes to use to join the swarm using the join command.</p>
<p>This is a depiction of the swarm that we currently have set up. vm1, vm2, and vm3 are all in the swarm, while my mac is not participating in the swarm. vm1 is the swarm manager, as indicated by the orange tie. We’ll use this multi-node swarm for the remainder of the lessons.</p>
<h1 id="Managing-Nodes"><a href="#Managing-Nodes" class="headerlink" title="Managing Nodes"></a>Managing Nodes</h1><p>We now have a 3-node swarm with one manager. This lesson will demonstrate how to perform swarm node management tasks. For example, promoting a worker to a manager, organizing nodes with labels, and preventing manager’s from doing work.</p>
<p>Agenda<br>I’ll give a brief overview of the swarm node management tasks we’ll be going through. We’ll spend most of the time at the command-line where we’ll execute the tasks on the swarm we stood up.</p>
<p>Node Management<br>Promoting<br>The first task that we’ll go through is promoting a worker node to a manager. You may want to do this to increase your fault-tolerance or in order to take an existing manager out of service without impacting the number of managers available. Remember that if you are going for an increase in fault-tolerance that you should increase the manager count up to the next odd number. For example, going from one manager to three.</p>
<p>Demoting<br>Demoting is the opposite of promoting. It takes a node that is currently in a manager role and demotes the node to a worker role.</p>
<p>Availability<br>The availability of a node refers to the ability to schedule tasks to the node. It isn’t whether a node is up or down which one might logically guess. The availability of a node is configured by managers. The allowed availability states are: active, pause, and drain. Active means that work can be scheduled on a node, pause means no new work can be scheduled but existing work scheduled on the node won’t be canceled, and drain means nothing can be scheduled and any running work is terminated. Setting availability to drain is useful for gracefully taking a node offline to perform maintenance. Draining managers is also useful to prevent them from having work scheduled to them, which is the default behavior.</p>
<p>Labeling<br>The final node management task that we’ll demonstrate is labeling. Recall that labels are useful for influencing where services are placed in a swarm. For example, labels can be used to ensure that tasks are scheduled in different availability zones to provide service availability SLAs.</p>
<p>Demo<br>Now I’ll hop over to the command-line and start with the demo</p>
<p>I’m connected into the vm1 swarm node which is currently the manager of the swarm. In docker, node management tasks are accomplished by using the docker node management command.</p>
<p>$ docker node –help<br>There are some standard commands that are available for most docker management commands: namely ls for listing nodes, ps for listing tasks scheduled to nodes, and rm for removing nodes from a swarm. Inspect is also a familiar docker command that lists detailed information about a node. To get a view of the swarm, I’ll run the ls command</p>
<p>$ docker node ls<br>And we see the three nodes, that all three are available, and that only vm1 is a manager, and is therefore the leader.</p>
<p>To promote vm2 to the manager role in the swarm, I’ll use the promote command:<br>$ docker node promote vm2<br>And easy as that vm2 is now a manager in the cluster</p>
<p>$ docker node ls<br>The manager status of reachable means the node is a manager and is participating in the Raft consensus quorum. The other possible manager status is unavailable, which indicates the manager has a problem communicating with the other managers.</p>
<p>To change vm2 back to the worker role, I’ll use the demote command:<br>$ docker node demote vm2</p>
<p>$ docker node ls</p>
<p>Next up is modifying the availability of a node. You can use the update command for that<br>$ docker node update –help<br>The availability option does what we want. You can also see the label-add and label-rm options which add and remove labels from nodes. There’s also the role option which promote and demote are a short form of updating a node to the role of either worker or manager. Let’s say we don’t want the manager to have any tasks scheduled to it, so I’ll set the availability of vm1 to drain</p>
<p>$ docker node update –availability drain vm1</p>
<p>$ docker node ls<br>and the availability in the ls table reflects the change.</p>
<p>I actually want the manager to be able run tasks so I’ll undo that by setting availability to active<br>$ docker node update –availability active vm1</p>
<p>To finish up I’ll add a fictitious availability zone labels to each node using the label-add update option. I’ll say vm1 is in zone 1, vm2 is in zone 2, and vm3 is in zone 3:<br>$ docker node update –label-add zone&#x3D;1 vm1</p>
<p>$ docker node update –label-add zone&#x3D;2 vm2</p>
<p>$ docker node update –label-add zone&#x3D;3 vm3<br>To see node labels, you need to use the inspect command</p>
<p>$ docker node inspect vm3<br>Here is the labels property in the Spec. You can also filter out everything but the labels by using the format option with a Go template</p>
<p>$ docker node inspect -f ‘&amp;#123;&amp;#123;.Spec.Labels&amp;#125;&amp;#125;’ vm3<br>Here again is the zone label key-value pair in the Labels map.</p>
<p>Recap<br>In this lesson we learned about the node management tasks that are part of managing a swarm. We understood the concepts and demonstrated how to promote and demote a node, set a node’s availability, and label swarm nodes.</p>
<p>This slide shows the current state of our swarm. Each node is now labeled with a zone compared to where we began the lesson.</p>
<p>Closing<br>In the next lesson, we’ll see how to schedule tasks onto the swarm by using services. If you are ready to see swarm mode in action, continue on to the next Lesson.</p>
<h1 id="Managing-Services"><a href="#Managing-Services" class="headerlink" title="Managing Services"></a>Managing Services</h1><p>Services are what make up distributed applications running on a swarm. In this lesson, we’ll get experience running and managing services in our swarm.</p>
<p>Agenda<br>To begin, I’ll give a quick rundown of what services we’ll be running, and then we’ll get into the demo.</p>
<p>The Plan<br>I’ll use two images for demonstrating how to work with services.<br>The first is a swarm visualizer provided by Docker. It allows you to visualize the state of nodes in a swarm and see where service tasks have been scheduled. It requires information that only manager nodes have access to. We’ll constrain the placement of the service to make sure it gets what it needs.<br>The second is a web service that serves a simple web page that displays the name of the node running the task. This will give us a way to verify that requests are load balanced across multiple nodes when using the ingress network in swarm.</p>
<p>Demo<br>Ok, now let’s get to the demo.</p>
<p>When working with services, all of the commands are conveniently located under the docker service management command<br>$ docker service –help<br>There are some familiar commands: inspect, logs, ls, ps, and rm. They do what you would expect given your knowledge of the Docker CLI. We’ll use them as we work through this demo. We’ll give the rest more attention, starting with create.</p>
<p>$ docker service create –help | more<br>This is the equivalent of docker run for swarm services. There are too many options to go through. Several match docker run options and several others are unique to services. We’ll go through some of the unique ones in this lesson and save some for the next.</p>
<p>Let’s start by creating the swarm visualizer. The visualizer must run on managers, so we can use a constraint on the node role to handle that. For demonstration purposes, I’ll make the service global so that every manager will run one task for the service. The service could be load-balanced since the swarm state that the service visualizes is the same regardless of which manager you use. But I will publish the port using host mode so we can compare that to ingress mode. Ingress mode is the default, so the mode&#x3D;host part of the string must be provided. The mount option is required so that the service containers can access the manager node’s docker daemon socket. That is where it pulls the swarm state information from. Finally, we’ll give the service the name viz and specify the latest version of the dockersamples&#x2F;visualizer image.</p>
<p>$ docker service create <br>–constraint&#x3D;node.role&#x3D;&#x3D;manager <br>–mode global <br>–publish mode&#x3D;host,target&#x3D;8080,published&#x3D;8080 <br>–mount&#x3D;type&#x3D;bind,src&#x3D;&#x2F;var&#x2F;run&#x2F;docker.sock,dst&#x3D;&#x2F;var&#x2F;run&#x2F;docker.sock <br>–name&#x3D;viz <br>dockersamples&#x2F;visualizer</p>
<p>The commands can get pretty long and we’ll see how to bettern manage them in the next lesson. I’ll speed things up until it is finished. The service converged message lets us know that the actual state has converged to the desired state in the service spec. We can see the service spec using inspect<br>$ docker service inspect viz –pretty | more<br>This output shows some of the default values that were used, such as the update and rollback config. It also shows that the service mode is global and that the port has been published in host mode. We can use the ps command to confirm the actual state matches the desired state</p>
<p>$ docker service ps viz</p>
<p>Now let’s switch over to a web browser to see the swarm visualizer. The manager is running on vm1 which has an IP address of 192.168.99.100. The visualizer displays a column for each node. The node’s name, role, memory, operating system, and the hard to read text is the node labels we applied earlier. Tasks are shown with squares under each node’s heading. Currently there is only the one visualizer task. If we didn’t have any role constraint there would be one on every node, but because the service was constrained to managers, there is only one. Task borders are color-coded according to their service. Because the service published its port in host mode, I have to use the manager’s IP. If I try vm2’s IP, it won’t be able to reach the service. So I’ll go back to vm1’s IP address.</p>
<p>I’ll promote vm2 to be a manager and that will cause a viz replica to be started on vm2 because the viz service is global. I’ll use the ls command to see the change. Notice the replicas has jumped up to 2. After awhile there will be 2 of 2 tasks running and I can try again to access the visualizer on vm2 in the browser. There it is. Both vm1 and vm2 are manager’s and there are two replicas of the viz service shown. I’ll go back to vm1’s visualizer and demote vm2 back to a worker. Now we’re back to just one viz replica.</p>
<p>Let’s focus in on the 2nd service now. I’ll switch over to VS Code and quickly go through the source. This file, index.php, is doing going to echo back the node name which it gets from an environment variable. That’s all there is to it. Taking a look at the Dockerfile, the base image is an php image with the apache web server installed. The index.php source file is copied into the image and the web server serves it on port 80. There is also a healthcheck embedded in the image. You can of course override the image healthcheck or create a healthcheck if the image doesn’t have one when you create the service. This is the same behavior as with docker run.</p>
<p>Back to the command line. We’ll create the service with a constraint to not schedule any tasks in zone 1. The exclamation mark followed by equals means not equal to. We’ll declare 2 replicas. This implies the service mode is replicated and not global. replicated is also the default. The manager will try to spread the tasks over available nodes by default, but we will specify a placement preference to take control of how it spreads. Next, I’ll add an environment variable for the NODE_NAME and use a Go template to get the hostname of the node. Port 80 will be published in ingress mode by default making the service reachable from any node’s IP address regardless of if a task is running on the node. I’ll give the service the name nodenamer and specify version 1.0.0.</p>
<p>$ docker service create <br>–constraint node.labels.zone!&#x3D;1 <br>–replicas 2 <br>–placement-pref ‘spread&#x3D;node.labels.zone’ <br>-e NODE_NAME&#x3D;’&amp;#123;&amp;#123;.Node.Hostname&amp;#125;&amp;#125;’ <br>–publish 80:80 <br>–name nodenamer <br>lrakai&#x2F;nodenamer:1.0.0</p>
<p>Now the two tasks move through the preparing, and starting states to reach the running state before the service converges to the desired state. We can check on the swarm state in the visualizer. There are two tasks for the nodenamer service and they have a dark grey border. They are deployed evenly across all the zones except zone 1 as we constrained the placement. We can also test the ingress routing capabilities. I’ll send a request to port 80 on vm1 which isn’t running a task for the service. But the page loads thanks to the ingress routing mesh. if I reload a few times, you can see the node name changing. This illustrates the virtual IP load balancing of the service. If I send my requests to vm2, the behavior is the same.</p>
<p>Back at the command-line I can also access the service at localhost on vm1 through the ingress network.<br>$ curl localhost<br>The output doesn’t have new line so the command prompt gets tacked onto the end. Version 1.0.1 of nodenamer fixes this issue. We’ll do a rolling update to version 1.0.1. Before we do, we can inspect the service</p>
<p>$ docker service inspect nodenamer<br>and observe the setting for updates. Parallelism is one by default so only one task will be upgraded at a time. We won’t look at failed updates, so the only other relevant setting is update order which is only available in Docker 17.05 and above. It defaults to stopping existing tasks and then starting a new task. The other value is start first, which starts a new task first and then stop the old one when the new task is running. There’s also update delay which sets the delay between rolling updates. It defaults to zero which is not a problem in this case, we’ll still be able to see the update roll through because it takes some time for new tasks to get to running. Let’s do the update to version 1.0.1</p>
<p>$ docker service update –image lrakai&#x2F;nodenamer:1.0.1 nodenamer<br>And switch over the the visualizer to watch the update roll through. The old task on vm2 is taken down first, then a new task using the 1.0.1 image comes in. As soon as it reaches the running state, the task on vm3 is stopped and a new 1.0.1 image task starts.<br>Now if I curl localhost from vm1, the new line is added to the output.</p>
<p>We can scale up the service to, say 6 replicas using the scale command<br>$ docker service scale replicas&#x3D;6<br>and we will see them come up equally spread across the eligible nodes. Even though there are more than one task on each node there are no port conflicts. This wouldn’t be the case if host mode was used for publishing the service port.</p>
<p>Let’s set the parallelism for rollbacks to 2 so we can rollback faster than we update.<br>$ docker service update –rollback-parallelism 2<br>Then we intentionally update the service back to version 1.0.0 so we can finish the lesson with a rollback to version 1.0.1.</p>
<p>$ docker service update –image lrakai&#x2F;nodenamer:1.0.0 nodenamer<br>And we can see it roll through 1 task at a time.<br>Now we can rollback</p>
<p>$ docker service rollback nodenamer<br>and we should see two tasks at a time being rolled back. There we see it rolling back two at a time.</p>
<p>That’s it for this lesson. You now know how to put the theory we studied earlier into practice by using the docker service management command for managing services.</p>
<p>Closing<br>In the next lesson, we’ll cut down on the lengthy commands and improve the repeatability and maintainability of swarm service deployments by using stacks. Whenever you are ready, continue on to the next lesson.</p>
<h1 id="Working-with-Stacks"><a href="#Working-with-Stacks" class="headerlink" title="Working with Stacks"></a>Working with Stacks</h1><p>Stacks help you manage applications distributed across a swarm. In this lesson, we’ll get some practice working with stacks. I think you’ll find that it’s easy to get the hang of and better than writing sprawling, multi-line docker service create commands.</p>
<p>Agenda<br>We’ll start the lesson by introducing stacks and stack files. It won’t take very long because your prior experience with Docker Compose will serve you well here. Then we’ll get started with a demo. Our last demo of the course.</p>
<p>Stacks<br>In swarm mode, stacks are a group of related services that can be orchestrated and scaled together. It carries the same meaning as a software stack or technology stack in application development.</p>
<p>Stacks are declared using a Compose file. That makes it very easy to start using stacks given your Docker Compose experience. You can use stacks to manage services, networks, and volumes as you would with Docker Compose. For a compose file to work as a stack, you have to be using Compose file version 3 or greater. Although they are the same type of file, when you deploy an application declared in a compose file to a swarm, it’s referred to as a stack. By convention, the file name that is used to indicate a stack is docker-stack.yml.</p>
<p>A lot of the Docker Compose features you know and love work with stacks. These include the declarative configuration, an active community on Github, and the benefits of source-controlled configuration. Similar to Compose, when you deploy a stack a network is created by default to isolate the stack from other services.</p>
<p>However, there are some differences between using Docker Compose and stacks that you should be aware of. Currently, there are several configuration options that are ignored with stacks that you could use in Compose. Some of the most notable ones are build and depends_on. The Compose file reference documentation should be consulted as support is added&#x2F;removed for options over time. A link to the documentation is at the bottom of the transcript for this video.</p>
<p>On the flip side, what’s stacks can use that Compose can’t are mostly gathered under the deploy key. That is where you can specify options for node labels, replication mode, resource reservations, update configuration, and others. Currently, there isn’t support for configuration rollbacks in a stack file, and placement preferences and endpoint_mode for setting virtual IP or DNS round robin service discovery are only available in version 3.3 Compose files or above. Stack files also support swarm secrets by a top-level secrets key. We won’t get into the details but know that they are supported.</p>
<p>With previous experience in Compose, that’s all that we need to go over to start using stacks in swarm mode. We can get started with the demo which will reproduce what we did in the last lesson’s demo except using stacks.</p>
<p>Demo<br>I’ll start by removing the currently running services we deployed with docker service create, since we’re going to recreate them using a stack.<br>$ docker service rm viz nodenamer</p>
<p>Now let’s take a look at the stack file. I won’t dwell on it since it is a one-to-one mapping of the commands we entered to create the services before except with more structure and slightly different option names. The stack specific options are under the deploy keys. Because we have a placement preference, we need to use version 3.3 or higher. It is more pleasant to work with stacks compared to entering all the options at the command-line. We’ll also see next that commands for stacks can limit output to services declared in a stack without having to do any filtering.</p>
<p>The commands for working with stacks are organized under the docker stack management command<br>$ docker stack –help<br>ls lists the current stacks, services lists the services in a stack and ps lists all the service tasks for a given stacks. The deploy and rm commands are similar to docker-compose up and down. In fact, up and down are aliases for deploy and rm so you could use up and down with stacks if you prefer.</p>
<p>Let’s look at the deploy options<br>$ docker stack deploy –help<br>You have options for removing services that are no longer in a stack, controlling when images are resolved, and for passing along credentials if you are using images in a private Docker registry. The one required option is -c to specify a Compose file for the stack.</p>
<p>$ docker stack deploy -c docker-stack.yml demo<br>I’ll call the stack demo. We can see the stack has a network created automatically for the services in the stack. Of course, you can, and often should, exercise more control over the networks you want, just as you would in Docker Compose. The stack has finished deploying so we should see the visualizer on port 8080 of vm1, and there it is.</p>
<p>To do an update you use the same deploy command you used with the original stack deploy. Let’s make an update to the stack so there are 6 replicas, and let’s also put a resource reservation of half a cpu for each nodenamer replica. Each vm only has one cpu so there won’t enough cpu available for all 6 replicas. We’ll use this to verify that swarm respects the resource constraints. Re-run the deploy command<br>$ docker stack deploy -c docker-stack.yml demo<br>The configuration changes will be detected, and the swarm leader will bring the swarm to the new desired state. Let’s watch the visualizer to see what happens. We can see 2 new tasks starting on vm2 and then another on vm3. After all the tasks on vm2 are running one is stopped to respect the resource reservations. We can also see from listing the services in the stack that only 4 of 6 tasks for nodenamer are running while two are left pending.</p>
<p>That’s all for this demo and this lesson. We saw that working with stacks is a natural extension of Compose files and docker commands.</p>
<p>Closing<br>We’ve almost reached the finish line now! Join me for the final lesson when you’re ready to wrap up the course.</p>
<p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file">Compose File Reference</a></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Here we are the of this course about container orchestration with Docker swarm mode. I hope you it has been as fun for you as it was for me. We covered a lot of ground in this course. Let’s take a look back at what we learned.</p>
<p>Course Review<br>We started with a high-level overview of swarm mode and then dove into various components of the swarm mode architecture. The first was networking.<br>We learned about the overlay network type that makes networking swarm services distributed across multiple hosts extremely easy.<br>We saw how swarm accomplishes load balancing by virtual IP or by DNS round robin.<br>Finally, we saw swarm services can be accessed via any node when using ingress port publishing. That is possible thanks to the swarm mode routing mesh.</p>
<p>After that, we looked at the headline architecture component: container orchestration. In that lesson we saw how to influence the placement of services on the swarm,<br>and we also considered rolling updates and rollbacks.</p>
<p>The next architecture component was consistency. Swarm mode uses the raft consensus algorithm for guaranteeing a consistent state of the swarm. Managers participate in elections and elect a single leader responsible for making changes to bring the actual swarm state to the desired state.<br>We examined the fault-tolerance and performance tradeoff related to the number of managers in a swarm. You should consider using 3, 5, or 7 for production workloads.<br>Lastly, we discussed the raft logs where cluster state is persisted.</p>
<p>Security was the last architecture component we looked at.<br>Swarm mode has many security features enabled by default including public key infrastructure and token semantics for joining members to a swarm.<br>Data plane communication on overlay networks is optionally encrypted using IPSec.<br>We also learned how you can improve swarm security by taking control of keys used for decrypting raft logs through a feature called autolocking.</p>
<p>Then we started getting hands-on and learned about options for setting up swarms from single-node, to mutli-node, and from on-prem to in the cloud. We then created a single-node swarm and a multi-node swarm with the help of docker machine.</p>
<p>Following that, we discussed node management in swarm. We demonstrated routine tasks such as promoting, demoting, labeling, and setting a node’s availability to schedule jobs.</p>
<p>The next demo focused lesson was about service management. We saw how to create and update services using various configurations such as host and ingress routing, global and replicated services, resource and placement constraints, and updates and rollbacks.</p>
<p>In the last demo lesson, we explored using stacks for managing distributed applications in Docker swarm mode. Stacks are represented using Compose files and have swarm specific configuration under the deploy key.</p>
<p>Learning Outcomes<br>By taking this course, you have achieved the following learning outcomes. You are able to:<br>“ Describe what Docker swarm mode can accomplish<br>“ Explain the architecture of a swarm mode cluster<br>“ Use the Docker CLI to manage nodes in a swarm mode cluster<br>“ Use the Docker CLI to manage services in a swarm mode cluster<br>“ Deploy multi-service applications to a swarm using stacks</p>
<p>Learning More<br>There are a few places I can recommend for learning more. Try out some of the labs, quizzes or other courses on Cloud Academy. There is more content on swarm as well as other container orchestration tools in case you want to understand the entire ecosystem.<br><a target="_blank" rel="noopener" href="https://cloudacademy.com/">https://cloudacademy.com</a></p>
<p>The Docker Swarm Mode docs are a great place to learn all the intricacies of swarm mode and to stay on top of release changes.<br><a target="_blank" rel="noopener" href="https://cloudacademy.com/admin/clouda/videos/video/2103/change/https:/docs.docker.com/compose/">https://docs.docker.com/compose/</a></p>
<p>The Docker swarmkit GitHub repository is the ultimate place to learn about swarm mode from its source. See what features are coming next for swarm mode and learn from discussions around reported issues. Maybe even report your own or contribute to the code base.<br><a target="_blank" rel="noopener" href="https://github.com/docker/swarmkit">https://github.com/docker/swarmkit</a></p>
<p>Feedback<br>I’m happy to hear from you. I make content for you. If you have any feedback, please get in touch with me by leaving a comment on the Comments tab below the video, by emailing <a href="mailto:&#x73;&#x75;&#x70;&#x70;&#111;&#x72;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#117;&#x64;&#x61;&#99;&#x61;&#100;&#101;&#109;&#x79;&#46;&#x63;&#111;&#109;">&#x73;&#x75;&#x70;&#x70;&#111;&#x72;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#117;&#x64;&#x61;&#99;&#x61;&#100;&#101;&#109;&#x79;&#46;&#x63;&#111;&#109;</a>, or by connecting with me on Twitter where my handle is @LoganRakai.</p>
<p>Thank you<br>That does it for another course here on Cloud Academy. You have developed some serious Docker swarm mode skills and should be proud of your accomplishment. Now go and do me the ultimate service by putting what you’ve learned here into practice. Until next time, I’m Logan Rakai with Cloud Academy.</p>
<p>What’s that? All right, swarm twister, take us away!</p>
<h1 id="1Course-Introduction"><a href="#1Course-Introduction" class="headerlink" title="1Course Introduction"></a>1<strong>Course Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/docker-swarm-mode-training">Course resources on GitHub</a></p>
<h1 id="7Setting-Up-a-Swarm"><a href="#7Setting-Up-a-Swarm" class="headerlink" title="7Setting Up a Swarm"></a>7<strong>Setting Up a Swarm</strong></h1><p><a target="_blank" rel="noopener" href="https://gitlab.com/gitlab-org/ci-cd/docker-machine">Gitlab Fork</a></p>
<h1 id="10Working-with-Stacks"><a href="#10Working-with-Stacks" class="headerlink" title="10Working with Stacks"></a>10<strong>Working with Stacks</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file">Compose File Reference</a></p>
<h1 id="11Summary"><a href="#11Summary" class="headerlink" title="11Summary"></a>11<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.docker.com/compose/">Docker Swarm Mode docs</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/docker/swarmkit">Docker Swarmkit GitHub repository</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Docker-Basics-Challenge-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Docker-Basics-Challenge-4/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Docker-Basics-Challenge-4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:31" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:31-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:42" itemprop="dateModified" datetime="2022-11-20T22:25:42-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Docker-Basics-Challenge-4/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Docker-Basics-Challenge-4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:30" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:30-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:08" itemprop="dateModified" datetime="2022-11-20T22:25:08-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Getting-Started-with-Docker-on-Linux-for-Azure-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Docker-Playground-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Docker-Playground-2/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Docker-Playground-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:28" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:28-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:26:04" itemprop="dateModified" datetime="2022-11-20T22:26:04-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Docker-Playground-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Docker-Playground-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Introduction-to-Docker-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Docker-1/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Introduction-to-Docker-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:27" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:27-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:23:20" itemprop="dateModified" datetime="2022-11-20T22:23:20-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Docker-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Introduction-to-Docker-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Intro"><a href="#Course-Intro" class="headerlink" title="Course Intro"></a>Course Intro</h1><p>I remember in my early development career, I was working on a basic CMS-backed website. I worked on the backend code and the front-end developer handle, the HTML, CSS, and JavaScript. We were able to get the site up and running rather quickly, well ahead of schedule. Everything worked well on the development laptops.</p>
<p>It all worked well on our development server, and it even worked great on our staging server. You know where it didn’t work well? In production. We handed it over to the client, whose IT staff insisted on getting it up and running in their environment for themselves. They ran into issues getting everything up and running.</p>
<p>Eventually, they had us configure everything, which turned out to be nontrivial, and a time-consuming task, because there were issues with the versions of some of the libraries that the app was using. Now, this was a long time ago in the dark ages before DevOps was a word, before AWS had a service for everything imaginable, and before Docker existed.</p>
<p>Now, sure, there are things that we could have done to make the handoff easier and hindsight is 20&#x2F;20, after all. If we could have bundled all of our code, along with all of the dependencies into one self-contained executable entity, then that really would’ve saved hours of work. Now this scenario is something that Docker can help with, and it’s just one of the reasons why Docker has become so popular.</p>
<p>Welcome to Introduction to Docker, where we’re going to go through some of the concept and features of Docker. A bit about me. I’m a DevOps engineer with a passion for security, for containers and automation, among other tech topics. I’m looking forward to this course because Docker is really more than just a buzzword, it’s also a really useful technology for both developers and operations.</p>
<p>So, who is this course for? If you are a developer, a DevOps engineer, a site reliability engineer, an ops engineer, or some sort of similar role, then this course is going to be valuable for you. As always, I’ve made some assumptions about what you should know before taking this course. First, you’ll want to be familiar with containers as a concept.</p>
<p>Now if you’re not yet, I recommend checking out the course called Introduction to Containers. Second, you’re going to want to be comfortable using the command line. And third, an understanding of Linux is going to be incredibly helpful to contextualize some of the concepts. Finally, if you have some development experience, it’s really going to make it a lot easier to test the stuff out for yourself.</p>
<p>However, if you’re on the operations side of things, you can use the code that I’ve created for these demos. I’ll include a link to the source code in the course description, so you can find it there. When creating this course, I had some learning objectives in mind. Now by the end of the course, you should know what Docker is, you should understand how to create Docker images.</p>
<p>You should understand how to map ports between the Docker, container, and the Docker Host OS. You should understand the basics of Docker networking. You should understand how to use volumes for persistent storage, and you should be able to tag images. So, at a minimum, that’s what I hope you’ll get out of this course.</p>
<p>To meet these learning objectives, here are the lessons. First, we’ll cover what Docker is, followed by an overview of the Docker architecture. Then we’ll walk through installing Docker on a CentOS VM. After that, we’ll create a container. Then, we’re going to walk through the differences between images and containers.</p>
<p>And then we’re going to create images based on Docker files. Then, we’ll create images based on changes made to an existing container. After that, we’ll cover port mapping, then we’re going to talk about networking options. And then we’ll cover volumes followed by tagging, and finally we’ll wrap up with a summary of what we’ve covered.</p>
<p>Okay, before we get started, you’ll want to adjust the speed of the video in the player to find the right setting for you. I like to watch through on two-X, but you may have your own preference. So, feel free to play around with the settings. Also, I have added several links to the course description for reference.</p>
<p>So, when I mention them in the course, you know where to find them. Also, I love hearing from you all. So, if you want to reach out, you can reach me at <a href="mailto:&#x73;&#117;&#x70;&#x70;&#x6f;&#114;&#116;&#64;&#x63;&#x6c;&#x6f;&#x75;&#100;&#x61;&#99;&#97;&#100;&#x65;&#109;&#x79;&#x2e;&#99;&#111;&#x6d;">&#x73;&#117;&#x70;&#x70;&#x6f;&#114;&#116;&#64;&#x63;&#x6c;&#x6f;&#x75;&#100;&#x61;&#99;&#97;&#100;&#x65;&#109;&#x79;&#x2e;&#99;&#111;&#x6d;</a>, or you can find me, I’m @sowhelmed on Twitter. Good or bad, all of the feedback helps me to create better courses for you, so I hope to hear from you.</p>
<p>Alright, if you’re ready to start learning, then I’ll see you in the first lesson.</p>
<h1 id="What-is-Docker"><a href="#What-is-Docker" class="headerlink" title="What is Docker?"></a>What is Docker?</h1><p>Welcome back. Before we dive in to talking about how we use Docker, we need to cover the high-level concepts. So in this lesson I’ll answer the question: What is Docker? So here’s my attempt at a simple explanation. Docker is a container platform that allows you to separate your application from the underlying infrastructure by bundling up your code and all of its dependencies into a self-contained entity that will run the same on any supported system.</p>
<p>Picture an actual physical shipping container. You can use a shipping container to transport anything that will fit inside. Since shipping containers come in standard sizes, they’re easier to manage because they’re consistent. It really doesn’t matter what’s inside the container because the infrastructure used to transport them is the same.</p>
<p>The ships are able to transport them, cranes are able to move them on and off ships, trucks are able to transport them, and none of that infrastructure is concerned with what’s inside the container. When it comes to software development, it’s not the initial development that’s challenging; it’s everything that comes after that.</p>
<p>Deploying code is a challenge. Managing dependencies is a challenge. Handling rollbacks is a challenge. All of these things are made more challenging because the development environments are seldom identical to production, and this is where Docker containers can help. Docker containers are similar to their physical namesake.</p>
<p>They allow you to take whatever software you need to run, bundle it up into a consistent format, and run it on any infrastructure that knows how to handle a container. The end result is kind of like having a single executable for all of your code. It’s similar to an app on your phone. All of the code is bundled up into a single unit, and it’s going to run the same way on my phone and yours.</p>
<p>With Docker, regardless of the programming language you use or the Linux distribution your code runs on, you can wrap it all up into one unit called a container; and the container knows how to run your app. If your app relies on a specific version of ImageMagick, then you can include it in the container.</p>
<p>Then any time you run that container, you know that you have the correct version. If later you need to update the version of ImageMagick, then you create new container with whatever version you need and any time you run that container, it’s going to run correctly because it has everything it needs inside.</p>
<p>Having your code run inside of a container means that it’s isolated from other processes and other containers. So each container can have its own process space, its own network stack, resource allocation, et cetera. Docker containers are often compared to virtual machines because VMs are familiar to most IT people.</p>
<p>However, they are pretty different. When you run a virtual machine, the OS that you’re using in the VM is running on virtualized hardware. That means that you’re using software to emulate hardware and then running a full OS on top. Now virtual machines are great; however, they can be a bit slow to start up, they can consume a lot of resources that you might not need, and they require patching.</p>
<p>Also, they can take up a lot of space on disk, especially considering they contain thousands of files that your app really doesn’t need. Here’s a diagram from Docker’s documentation showing three applications, each running in a VM. So you could imagine that the first one is CentOS, the second is Ubuntu, and the third is Debian.</p>
<p>Each OS is running on the hypervisor and consuming a lot of system resources. Now here’s a look at how Docker is different. Notice the host OS and then the Docker engine here in place of the guest OS. So then you have your binaries and libraries that your application needs followed by your application code.</p>
<p>So what Docker is doing is it’s cutting out the guest OS, allowing you to bundle up the system libraries, any binaries that your app needs, as well as your code, all into one unit that can be executed via the Docker engine. Where VMs run the entire OS, Docker shares the kernel with the host OS. This means that containers can start up about as quickly as any other process since they really don’t contain all of the files required to run the entire OS, and they don’t consume all of the resources that a guest OS does.</p>
<p>So it allows you to run more of them on the same hardware. In Linux, everything is just a file, which means the difference between Ubuntu and CentOS is really just a set of files. By files, I mean things like binaries for package management, configuration files, different services, et cetera. With Docker, you use the kernel of the host OS.</p>
<p>However, because the difference between distributions is really just different files, you can create Docker containers based on the different distributions. So if your application requires some libraries that run on Ubuntu, then you base your container on Ubuntu. This means you can bundle up your code, as well as just the system libraries that you need, into one container.</p>
<p>Once all of your code and dependencies are in a container, they’re going to run the same way anywhere because everything required to have the code run is inside the container. So if you use Docker containers for all of your applications, then that’s going to allow you to standardize on how you deploy all of your applications.</p>
<p>All right, admittedly, this is an intentionally overly simplistic explanation of Docker. However, it should serve as a solid enough intro that we can build on that in the following lessons. So if you’re ready to dive a little bit deeper, let’s cover the architecture of Docker in the next lesson. So if you’re ready, I’ll see you in the next lesson.</p>
<h1 id="The-Docker-Architecture"><a href="#The-Docker-Architecture" class="headerlink" title="The Docker Architecture"></a>The Docker Architecture</h1><p>Welcome back. In this lesson we’re going to take a look at some of the different aspects of the Docker architecture. Now there are a lot of things to explain with Docker and it’s all very interconnected, so don’t feel bad if this doesn’t make sense right away. Stick with it and by the end of this course things should start to make sense.</p>
<p>Okay, at a high level, Docker uses a client-server architecture. The server part is that there’s a Docker daemon, which is responsible for managing Docker objects. By objects I mean things such as images, containers and networks, which we’ll cover later in the course. The daemon exposes a REST API that the client consumes over UNIX sockets or a network interface.</p>
<p>The client is the Docker binary, so whenever you use the docker command, that’s the client. This diagram here gives a glimpse of how the client and daemon interact together. Here you can see that the subcommands issued by the client are sent over to the daemon, for example, the docker pull command here instructs the daemon to get an image from the registry.</p>
<p>The Docker daemon has a lot of different configuration options that you can pass in when you run the daemon. There are different options that let you change how the daemon operates, for example, if you want to use a remote daemon you could adjust the socket option, if you want to have some debugging capabilities you can pass in the -D flag, so if you want to make changes to the runtime, you could do that too.</p>
<p>So the Docker daemon is in charge of managing Docker objects, and the client is the primary way that you’ll interact with the Docker API. Being new to Docker, you don’t need to change anything about the daemon, the defaults will get you by just fine. So while you might not be customizing the daemon settings, knowing about that separation of the client and the daemon will help if you run into an error such as this one when you’re using the Docker binary.</p>
<p>In this example, the Docker binary was used to try and list off the running containers. Since the client relies on that daemon to be running to get that information and the daemon isn’t running, the client is kind of useless and it throws this error, and the solution to this is just to make sure that the daemon is started on the OS that you’re running on.</p>
<p>Containers aren’t a new concept, they’ve existed in some form for years. If you haven’t heard about them before, you probably didn’t have to deal with the types of problems that containers solved. However, the cloud has really shifted things and hyperscale is the new defacto standard. There are a couple of reasons Docker has taken off so well.</p>
<p>First is because Docker took a lot of the current container technologies and combined them into one product. The second reason is that Docker came along at a time when people were looking for better ways to build, deploy and run highly scalable, secure apps. So the combination of market demand and well established technologies made it a prime candidate to become synonymous with the term container.</p>
<p>Docker is actually built on some very well established Linux kernel features, that when combined together, allow processes to be run in isolated environments. Let’s go through the technologies to understand how they combine into one product. The first is called namespaces, which allow you to isolate system resources.</p>
<p>Docker uses a few different namespaces. The namespaces supported by Docker are the pid namespace, the net namespace, the ipc and mnt namespaces, as well as the uts namespace. It uses pid to handle process isolation, this means that each namespace has its own set of process IDs. The net namespace, which is short for networking, is used to isolate network stacks.</p>
<p>Each net namespace has a private set of IP addresses, its own firewall, routing table, et cetera. Linux has several mechanisms for interprocess communication. One of those mechanisms is called System V, using the Roman numeral V for five, or SysV for short. The ipc namespace allows processes to be isolated from SysV-style interprocess communication.</p>
<p>The mnt namespace handles mount points, which allows for isolation of file system mounts. And finally, the uts namespace, which stands for Unix Timesharing System, allows for isolation of the hostname. So namespaces allow for isolated aspects of a system’s resources. Now there are additional Linux kernel namespaces, however the ones that I’ve shown here are the namespaces being used by Docker.</p>
<p>So, when these are all used together they allow for a very high level of process isolation. The next feature that Docker uses is called control groups, or Cgroups for short. Control groups are used to limit resource allocation. Since containers allow processes to be run in isolation, you can’t have any single process consuming all of the system resources.</p>
<p>So as an example, imagine you need to run five containers and one of those consumes all of the system’s CPU. The end result is a sort of denial of service attack, because the other four containers are not able to get the CPU that they need to function. So control groups allow for sort of limits to be set on different subsystems, which ensures that the processes aren’t going to be taking more than they should be allowed.</p>
<p>The next bit of functionality that makes up Docker is the union file system. Now this is an important part of Docker, and it’s functionality that really helps to keep down the overall size of Docker containers as well. A union file system starts with a base image and then can merge in any changes. When you create a Docker container, you have a starting image, which is a set of files that make up the base image for your container.</p>
<p>As you start making customizations by adding or removing packages, files, directories, et cetera, those create different layers. Each layer is a set of file changes that the union file system can merge into the previous layer. Because of this layered design you don’t end up with duplicate files because each layer just needs to know about the changes.</p>
<p>Okay, so none of this was really deep dive, this is after all an intro course, so I want to stop here and let’s summarize what we’ve covered. First, Docker is a client-server application, the client is the Docker binary, the server is the Docker daemon which runs a REST API. Second, Docker is comprised of several well established Linux technologies, including namespaces, control groups and a union file system.</p>
<p>It’s the combination of these technologies that makes Docker so useful. Each of these is great on their own, but together they allow Docker a better way to run your code in isolation. Alright, let’s wrap up here, and in the next lesson we’re going to cover actually installing Docker. So if you’re ready to keep going, then I’ll see you in the next lesson.</p>
<h1 id="Installing-Docker"><a href="#Installing-Docker" class="headerlink" title="Installing Docker"></a>Installing Docker</h1><p>Welcome back! In this lesson we’ll be installing Docker on CentOS Linux. I’ll be using Vagrant to start up a CentOS VM. However, Docker has good documentation covering all of the different installation options available, and there are several operating systems that you can use. So if you wannna run Docker on Windows, macOS or Linux, there are instructions for that, and I’m going to leave the link to that documentation in the description of this course.</p>
<p>Now, I’m currently connected to the CentOS VM via SSH. The Docker documentation recommends that you start with a clean slate and remove any existing version of Docker. Now, this is a new VM. There isn’t anything installed, so this shouldn’t have anything that needs to be removed, but I’m going to run it anyway.</p>
<p>Great. Next up we need to install some prerequisites. The yum-utils package includes the yum-config-manager, which allows us to add and enable yum repositories. The device-mapper and lvm2 packages are used for the devicemapper storage driver. Now, this is only going to take a few moments. And there it is.</p>
<p>So with these installed, now we can use the yum-config-manager that allows us to add the Docker repo. Okay, with the Docker repo added, we can install Docker. And you can see here there is a summary of what’s going to be installed. This is going to install nine dependencies and the total size will be 81 megabytes.</p>
<p>So let’s say yes to this part. And there we go. Now it’s going to ask to verify a couple of GPG keys, and this can be done by typing the letter Y. And once again. Okay, perfect. The next part is going to take a minute or so, so I’m going to speed this up. And we’re back. Okay, everything went well, so the container engine should be all set.</p>
<p>Next let’s start up the Docker daemon. And there it is. And it’s returned with no error message. Now let’s verify that the daemon is running with the status subcommand. So you can see here that the daemon is running. Now, in theory, everything should be working and ready to go. However, let’s verify that, and to do that, we’re going to actually start up a very simplistic hello-world container.</p>
<p>Now, I don’t want you to worry. We haven’t really covered containers yet. If this doesn’t make sense, that’s okay. We’re going to cover that in the next lesson. So let’s test this out by running Docker’s hello-world image. Okay, there’s a lot happening here in this output and we’ll go through that later.</p>
<p>For now, I want you to focus on this “Hello from Docker” message. Now, seeing this means that Docker is working correctly because it was able to grab the image and then run a container based off of that and display the message. Okay, let’s wrap up here. Docker is up and running on CentOS and we’ve verified that it is working.</p>
<p>So if you ran into any issues, what I want you to do is use Docker’s documentation to kind of help work through them. With Docker running, you’ll be ready to start using it for the rest of the course. So if you have it up and running and wanna start actually using it, then I’ll see you in the next lesson.</p>
<h1 id="Creating-and-Executing-Your-First-Container-Using-Docker"><a href="#Creating-and-Executing-Your-First-Container-Using-Docker" class="headerlink" title="Creating and Executing Your First Container Using Docker"></a>Creating and Executing Your First Container Using Docker</h1><p>Welcome back. While we did run the hello-world container in the previous lesson, in this lesson, I’m going to actually explain what’s happening. I want to start by running another container. The command will be similar to the hello-world example, though it’s not exactly the same thing.</p>
<p>I’m going to run this command, and then we’ll break it down so that you can see how it all works in a moment. The command is sudo docker run -it ubuntu &#x2F;bin&#x2F;bash and this is going to take a moment to complete. While this is running, I want to describe what’s happening at a high level. The command that I just ran instructs Docker to run a container based on the official Ubuntu image.</p>
<p>I want you to focus on the Ubuntu part of the command for now. If you recall, the VM that we’re running on here is CentOS. So this is often the point where students ask where is Ubuntu coming from? We’ve run two containers now, we ran hello-world and Ubuntu, and from an outsider’s perspective, it’s not really clear where those names come from and what they actually mean.</p>
<p>When we reviewed the Docker architecture I very casually mentioned that Docker downloads images from a registry. Now that registry’s where hello-world and Ubuntu come from. They’re official images stored on either Docker Hub or the newer Docker Store. Both the Docker Hub and Docker Store serve as centralized locations for Docker images to be downloaded.</p>
<p>Here’s a look at the store. Notice the landing page has a search box, at the top and then there’s this explore link. While this UI is likely to change over time, as of now, the explore page offers a faceted search. And if you notice here, you can select the registry of either Store or Docker Hub. And there are some additional filters on the side here, and I recommend that you check them out for yourself to see what kind of images already exist.</p>
<p>For now, let’s just search for Ubuntu and see what we get. Okay, so we get a few options. Clicking on the one named Ubuntu brings us to a landing page for this particular image. On the right-hand side you can see that there’s a command provided that we can use to download this image from the registry. Running this command as it is will download the latest version of Ubuntu.</p>
<p>The ability to pull the latest version is really useful, especially when developing. However, in production you’ll need to be precise about which versions to use. And that’s where the ability to specify a tag will allow you to download the exact version. That’s something we’ll cover later in the course.</p>
<p>However, I mention it here because depending on when you watch this, the version for latest, may not be the same for you as it is for me. So, let’s go back to the terminal and look at the output. So, notice here at the top, it says unable to find image Ubuntu:latest locally. Docker looks locally on this VM, it can’t find the image, so was it does it goes and downloads it from the Docker Hub.</p>
<p>When it downloads the image, it stores it in a subdirectory that lives inside of the &#x2F;var&#x2F;lib&#x2F;docker directory. The next time it runs, it’s going to see if an image has changed since it was last downloaded. If it hasn’t, then it’s just going to use that local version because it already exists locally.</p>
<p>So, the image is now downloaded locally, and if you notice the prompt has changed. Look up here and notice it says vagrant@localhost. That’s the bash prompt for our CentOS and down here it says root@ followed by the first several characters of the container ID. This is the bash terminal inside of our Ubuntu container.</p>
<p>Let’s look back at the command and see how we got here. Behind the scenes this ran docker pull to pull down the image because it didn’t exist locally. Then the Docker run command allows you to execute a command from inside of the container. The command that we’re running is the bash binary. The way we are able to get an interactive shell is that we’re using the I and T flags.</p>
<p>The I flag makes it interactive by redirecting standard IO. The T flag implements a pseudo TTY, which basically makes the terminal behave like a standard terminal. Because we told Docker to run a bash shell as the container process, if we exit out of this, then the container is going to stop. Okay, let’s wrap up here and let’s summarize what we’ve covered.</p>
<p>First, both the Docker Hub and the new Store serve as a registry of existing images that you can use as is, or to form the base for your own images. Second, when using the Docker run command, behind the scenes it’s going to download the image if it doesn’t exist locally. Then it will run whatever command that you specify.</p>
<p>Using the lowercase I and lowercase T flags will provide an interactive terminal session on the container. And finally, generally speaking, when the process that you run exits the container will too. Alright, in the next lesson we’ll dig into the difference between an image and a container. So if you’re ready to keep learning, then let’s get started in the next lesson.</p>
<h1 id="Images-vs-Containers"><a href="#Images-vs-Containers" class="headerlink" title="Images vs Containers"></a>Images vs Containers</h1><p>Welcome back. Throughout the course I’ve mentioned images and containers. Though, the difference between the two may not be clear. So, that’s what we’ll do in this lesson. We’re going to take a look at the difference between images and containers. At high level, the difference between the two is similar to the difference between an executable and a running application.</p>
<p>Each running application is its own instance and independent of the others. The running application is also independent from the executable in that changes to the app won’t impact the executable. In this analogy, the executable is like an image, and the running app is like a container. An image is a template that defines how the container will look once it creates an instance.</p>
<p>Images are built on the concept of layers. There is always a base layer, and then there is some number of additional layers that represent file changes. Each layer is stacked on top of the others, consisting of the differences between it and the previous layer. The union file system that we talked about in a previous lesson is used to merge the changes.</p>
<p>Because Docker builds images from layers, and the different layers are file diffs. The layers usually don’t take up too much space on disk. Let’s demonstrate the difference between images and containers in a terminal. To start, I’ll switch to the root user that way I don’t have to type sudo so much. Okay so, so far we’ve used two images.</p>
<p>One was the hello-world image, the other was for Ubuntu. We can see those images with the docker images command. Notice here there’s a table that displays the image name, the tag, which are both tagged as latest, and there’s the image ID which is the first few characters of the SHA256 hash for the image.</p>
<p>And take a look at the image sizes. The hello-world image is very small because it contains just a single binary. The Ubuntu image however is a bit larger because it contains all of the software that makes Ubuntu, Ubuntu. That means software such as the apt package manager is included in that. So these are the two images that I have locally.</p>
<p>Using the docker ps command will show all of the running containers. And as you can see there aren’t any here. However, by adding the minus a flag you can see that this lists off all of the previously run containers. Let’s explore the &#x2F;var&#x2F;lib&#x2F;docker directory a bit, because this is where all of the images and containers are actually stored.</p>
<p>Looking at the contents of &#x2F;var&#x2F;lib&#x2F;docker you can see we have a directory here for containers and one for images. Let’s drill into the one for images, and into the image&#x2F;overlay directory. Here you can see some directories for images and layers. From here let’s drill into imagedb&#x2F;content&#x2F;sha256 and notice there are two files here with names consisting of sha256 strings.</p>
<p>If I run the docker images command again, notice that the image ID is a truncated version of these hashes. So this is the file here that is actually the hello-world image, and this one is the Ubuntu image. Now these files are just JSON files. Let’s take a look, let’s show that by printing out the output of the Ubuntu image.</p>
<p>I’m going to pipe it through the Python JSON module, that’s going to make it a bit easier to read. Okay, there’s a lot of info here, and honestly, you don’t need to know any of this to successfully use Docker. The reason I’m showing this is to part one, I wanna demystify Docker just a bit, by pulling back the curtain.</p>
<p>And two, I want to kind of show you how the image is comprised of different layers. By scrolling down to the history array you can see that there are these objects here with the created_by property. These are the commands that need to be executed to turn the default layer named scratch into an Ubuntu container.</p>
<p>Each command produces a new image layer. So behind the scenes the Ubuntu image starts with the default starting image which is called scratch, it’s just a lightweight image. And then it adds all of the files it needs and then executes a series of command on those files, and the final result is an Ubuntu image consisting of several layers.</p>
<p>Now I wanna emphasize again, understanding this directory structure or these file structures is not required for understanding Docker. It’s just going to help you understand just a bit better how things are working behind the scenes. Now, as a Docker user, you won’t be using these files directly, rather, you’re going to use abstractions such as the Dockerfile which will be covered in a later lesson.</p>
<p>And when you want to view information about an image, you don’t even need to browse to the &#x2F;var&#x2F;lib&#x2F;docker directory, you can use things like the Docker inspect command, and it’s going to give you most of the same info. Though inspect isn’t going to show you the command you use for each layer like this.</p>
<p>So, we’ve talked about images a fair amount. They’re the template used for creating containers. Let’s highlight the difference by starting up a container, making some changes to it, and then starting another container to show how they’re separate. Let’s use the Ubuntu image and let’s run the bash process.</p>
<p>So using the I and t flags, it’s going to allow us an interactive terminal session. Okay, so we’re currently at the bash prompt for this container. We’re currently in the root directory. And there’s nothing in the home directory. Okay, I wanna add a new directory for all of my awesome files in the home directory.</p>
<p>Now let’s CD into that. And now let’s create a file by echoing some text. Let’s echo this only exists in this container. And we’ll direct that output to container. txt. Okay, let’s verify that it exists. And there’s our text. Remember when we exit a bash process, the container will stop. That’s because we told Docker to create a container based on Ubuntu, run the bash command.</p>
<p>So once that process terminates, Docker has done everything we’ve asked of it. Now running the Docker ps shows us that there are no running containers. Let’s run another bash process with the Ubuntu image. And there it is. We have our bash prompt. Let’s look in the home directory. And notice that nothing is here.</p>
<p>This is exactly what you’d expect. Because each container is independent of the others. Okay, we can see the two containers that we just created by using the Docker ps command with the minus a flag. And I wanna shrink this text down just a bit so that this table is easier to read. Okay, the container we just ran is the one at the top.</p>
<p>And the one before that is the one with the text file. The important part of this table at the moment is the list of names. Because I didn’t supply any name for these containers, so, Docker provided these names. So you can optionally add your own name, and if not, Docker will name them. I’m going to copy the name of this container here.</p>
<p>This is the one with the text file. The container is currently stopped, so we’re gonna learn a new command. And we’re going to use the Docker start command. Followed by the name of the container. Okay, let’s see if the container is running with the Docker ps command. And it is, so, perfect. You may have noticed that the bash prompt on the screen is not the one for our container.</p>
<p>That’s because we started it and it’s running in the background now. So now in order to interact with it, we need to use the Docker attach command and passing the container name to attach to it. Great. So, here we are at the bash terminal for our container. And this should have our text file. And let’s verify.</p>
<p>And there it is. So, what did all of this prove? We created a container based on the Ubuntu image. And then we added a text file to that container. Then we started up a separate container, also based on Ubuntu, and the text file wasn’t there. It didn’t exist in that container. That’s because each time you create a container, it’s based on how the image looks at the moment the container is created.</p>
<p>Alright, let’s wrap up here and summarize what we’ve covered. Images in Docker are self-contained packages that are used to create containers. Making containers an instance of an image. Alright. In the next lesson, we’re going to look at how to create Docker images using the Dockerfile. So, if you’re ready to keep learning, then I’ll see you in the next lesson.</p>
<h1 id="Images-From-The-Dockerfile"><a href="#Images-From-The-Dockerfile" class="headerlink" title="Images From The Dockerfile"></a>Images From The Dockerfile</h1><p>Welcome back, so far we’ve used existing images that come from Docker registries. In this lesson, we’re going to cover how to create your own image, using a Dockerfile. The Dockerfile is text file with commands that you can use to create your own image. This allows you to specify the starting place for you image, and then you can specify the changes you’d like to have made to that image.</p>
<p>Let’s start off with a very basic example, first, I want to start by removing all of the containers that I’ve created. So I’m going to use the Docker container prune command, now, I feel like there should be some flashing lights and loud sirens going off, because this is going to delete all of the stocked containers.</p>
<p>If you don’t want to remove all of your containers, then don’t use this command. Instead, use the Docker rm command, and specify the containers that you want to remove. Okay, so I’ll say yes to the prompt, great. Next, let’s look at the images, with a command that is probably becoming familiar to you now, which is the Docker images command.</p>
<p>So you can see there are two images, we have Ubuntu and hello world. We’re going to create a new image, here’s what I want for this demo, I want a lightweight container, that runs a binary of my own creation, and by lightweight, what I mean is, I’m looking for a container that only has as much as it needs to run my binary.</p>
<p>I don’t want a bunch of additional, files, such as what you’d see in an Ubuntu image. Here’s the code for the binary, this is a very basic, hello world app, written in Go, and if you want to run this for yourself, the bottom comment here is how you would compile it, so that it will run in the container.</p>
<p>Though, I will mean you’ll need to have Go installed and configured. Now I’ve already compiled the app, and I have the binary ready, so let’s look at how to create an image with Dockerfile that can run this binary. A Dockerfile is actually called Dockerfile, it’s one word, no extension. Here’s the Dockerfile that we’ll use for this demo.</p>
<p>It’s starts out with a from instruction, which is used to specify the starting image. This allows you to set the image that you want to build on top of. This makes Docker rather flexible, because you can use any of the community images, an image from the store, or you own base image, as the start. This demo uses a special image called scratch, the reason it’s special is that it’s not something that you can just run, the same way the we ran the hello world or Ubuntu images.</p>
<p>This is meant to be used a minimalist base. So, from scratch, tells Docker, to start our image using the scratch image. The next instruction is the copy instruction, which allows you to copy files from your host, into a layer of the image. In this example, I’m telling Docker, to copy the file named hello, into the root directory of the image.</p>
<p>After the command is processed, a new layer will be created, containing this binary. Any commands that we run afterward, will be able to interact with that hello binary. The copy instruction can copy files, and directories, and it also supports wild cards. And I’m not going to go into each of the aspects of all of these instructions.</p>
<p>So I’ll leave a link in the course description, for the documentation so you check it out for yourself. Finally, down at the bottom, we have the command instruction, which is the default command to run, when the container starts up, unless one is specified on the command line. So there should be one command instruction per Dockerfile.</p>
<p>If you have multiple, then it’s always going to use the bottom most. This syntax here, has the command to run as an array, where the first element is the binary to execute. And then additional elements in the array, are arguments for the executable. So this is telling Docker to run the hello world binary when the container starts.</p>
<p>Okay, in theory, this Dockerfile should meet the requirements for the demo is set. I said, it should be lightweight, and allow me to run a binary. Because we’re using the default scratch image, it’s going to be lightweight. And using the copy and command instructions, allow us to run the hello binary. Let’s turn this Dockerfile into an image now.</p>
<p>I’m here at the bash prompt, for our CENTOS7 VM, and I’m in a directory containing the Dockerfile, and hello binary. If I run this binary here, it’s going to show us what the results will look like when we run it inside of a Docker container. So there you go. Now I’m currently logged in as root, so I don’t need to type sudo in front of everything, and if I list off the existing images, you can see that they’re currently are the two, that we’ve used throughout the course so far.</p>
<p>And we don’t have any containers at the moment. So, the command to build an image from a Dockerfile, is Docker build, followed by the directory, where the Dockerfile is located. Now you can specify a Dockerfile manually, if you want to have a name other than Dockerfile, we’re not going to covert that, but it’s minus f, for file.</p>
<p>Check that out in the documentation, if that’s something you want to use. In this case, I’m going to use the current directory, because the Dockerfile is located here. And notice it goes through our instructions from the Dockerfile. And it ends with a success message. So, now if we list the images again, it’s going to show our new image.</p>
<p>And there it is. So what I want you notice is that, it doesn’t actually have a repository or a tag, it does have an ID, though, using an ID to reference an image is, one of the most unintuitive ways you can interact with an image. So I built it this way intentionally, to show you what happens when you build this, without providing a repo name.</p>
<p>Let’s remove this image. I want to remove this, and build it again with a repo name. To remove it, we can use the Docker rmi, as in, remove image, and, let’s pass in the ID. Okay, now to verify that it’s gone, there we go. So, let’s do this again, only this time, I’m going to use the minus t flag, to specify the repo and tag.</p>
<p>Let’s call this greeting. And there we have it. Notice here at the bottom, it says, it was successfully tagged with greeting, and a colon, and then the word latest. Listing the images again, notice the repository is named greeting, and then there’s a tag that says latest. Tags in Docker have their own structure, and they allow you to supply a repo name, and a tag, at the same time.</p>
<p>Because I only provided the repository portion of the Docker tag, it’s going to automatically use a tag of latest. In a later lesson, we’re going to get into tagging a bit, so for now, the important part of this, is that we have an image, and it has a name that we can use to reference it. And then, if you notice here, it has a size of around 1.</p>
<p>3 megabytes. Now that we have an image, we can actually create a container based on it. So let’s give that a try. And we’ll do that with the Docker run command, and passing in the repository name of the image. And there it is, there’s the output that we expected from the hello binary. So we can use the Docker ps command, with a minus a flag, to show that the container ran, and then it exited successfully, based on this status code of zero here.</p>
<p>I’m going to remove this with a prune command, just to keep things clean for later lessons, as well as to keep showing the common commands. Okay, let’s dive into things a bit more in depth now. So let’s do a little bit of exploration, again, to help demystify Docker, it’s not essential to mastering Docker, but hopefully this will help to, kind of make sense of things a bit.</p>
<p>Earlier in the course, I showed where the image file was stored, and that it’s just a JSON file. I want to show how the Dockerfile we just used, maps to that image file, that Docker creates behind the scenes. So here’s a listing of the images. If I list the images, you can see the ID for the image we want, the one we just created, it starts with 934.</p>
<p>And it matches up to this image file here. So let’s print the contents, of that, then we’ll pipe it through JSON beautifier, okay. Here in the history section, notice two objects, the first one makes reference to copying a file, and over here in our Dockerfile, this maps to this copy command. Then the second object, which references the command, maps to the cmd instruction in the Dockerfile.</p>
<p>So Docker turns our simple Dockerfile, into a format that it knows how to work with, making it easier for us, as end users, to use relatively simple extractions. Alright, let’s wrap up here. There’s still a lot about the Dockerfile that hasn’t been covered. And later in the course, we’ll cover some more instructions as we go on.</p>
<p>For now, the takeaway is that, you can create your own images using the Dockerfile. It allows you to build your image, based on an existing image, and include any files you may need, and then set the default command to execute, when the container starts up. In the next lesson, we’re going to look at another way to create an image, which is, to use an existing container, as a base, make some changes, and then commit that.</p>
<p>So if you’re ready to learn more, then I’ll see you in the next lesson.</p>
<h1 id="Images-From-Containers"><a href="#Images-From-Containers" class="headerlink" title="Images From Containers"></a>Images From Containers</h1><p>Welcome back. We’ve seen how to create a basic image using a docker file. However, the docker file isn’t the only way to create an image. You can also use the docker commit command to commit changes that you’ve made to an existing container. Now, my recommendation is that you should use the docker file when creating images.</p>
<p>When you use the docker file you get to treat that image as code in a sense. That means you can commit it to source control, you can easily share that docker file with other developers, and you can use it in automated testing to build the image, et cetera. So, while I recommend using the docker file, it’s not the only option.</p>
<p>So I’m going to demonstrate how to use the docker commit command. Using the commit command allows you to save whatever changes you’ve made to a container. So I’m here at the bash prompt for the CentOS host and I’m currently the root user. If you’re trying this along with me you’ll need to be root or you’ll need to use Sudu or maybe you’ll add your user to the docker group, but inevitably you need to have root privileges.</p>
<p>So, there are no running containers as you can see here with the docker ps command and there aren’t any stopped either, which shows because we’re using the -a flag. There are a few images here. We have the greeting image, we have the hello world image, and the Ubuntu image. I’m going to use the Ubuntu as the base for this new image.</p>
<p>I want to create an image that has Python installed. Now, if this wasn’t a demo I’d recommend that you use the official Python image. However, for the sake of this demo I’m going to install it in an Ubuntu container. Let’s fire up the container based on the Ubuntu image and then run the ash binary. Okay great, we’re at the bash prompt for the container and I want to show that Python is not installed.</p>
<p>So, I’m going to use the command which python and it returns nothing. So at a minimum the Python binary can’t be found in any of the directories in the path environment variable. So, before we can install Python I need to update the apt cache and this is going to take a little while. So what I’ll do is speed up the video and I’ll meet ya back.</p>
<p>Okay, welcome back. Now let’s install Python with apt and I will agree to the install. And again, this is gonna take a little while so I’m gonna speed this up too. Alright, with this done let’s verify that it’s installed. It seems that the binary exists and running Python with the version flag. Perfect, it shows that we have Python 2.</p>
<p>\7. 12. Okay, we’re now left with a container that has Python installed. The next step is to turn this into an image. So I’m going to exit out of the bash prompt, which will stop the container. If I list off the images, you can see that we still don’t have our image. We do, however, have a container. The way that you turn a container into an image is with the commit command.</p>
<p>So let’s run docker commit and then paste in the container ID and we’ll pass along the tag for this image, which I’ll call ubuntu_python. Notice how the image is now in the list of images. Here’s the problem with this image. We didn’t specify a new command for this image. So it’s just going to use the default for the Ubuntu image, which in this case happens to be the bash command.</p>
<p>It’s not because we ran the bash command, that’s because whoever created the Ubuntu image used that as a default. So if we run a container based on this new image, since it’s using bash as the default and I’m not using the interactive and a TTY for the pseudoterminal, it’s just going to run the executable and then exit out and so what we’re left with is a stopped container.</p>
<p>So, if the default command that you wanted to use was bash then you’d be all set. However, in this example what I’d rather do is use Python since we took the time to install it. So I’m going to remove this container that I just created. Okay, and I’m going to remove the image that we created so that we can commit a new one with a new command.</p>
<p>Perfect, okay now it’s roughly the same command that we used before only this time we’re going to use the change flag to change the command instruction. You can also change other instructions, however, for this demo. I’m only going to change the command. So the new command is going to instruct the container to use a python binary by default and that’s going to pass in this c flag, which allows some arbitrary Python code to be executed and then the final element in this array here is the Python code that I want to execute.</p>
<p>And what this will do is just print out some text to standard out. Okay, now I need the container ID that this image will be based on and finally a tag for this image. Great, notice the image is in the list. Now if we create a container based on this image the default command should run that python binary, it’s going to pass on the c flag, it’s going to execute the code that I specified, which is import this.</p>
<p>So let’s try that out. Perfect, there’s our output. So everything’s working. If we use the docker ps with the -a flag you can see that here it lists off a truncated version of the command that was executed. So, what have we learned in this lesson? We learned that docker allows you to create an image based on an existing container by using the commit command.</p>
<p>It also allows you to change instructions using the change flag. Alright, let’s wrap up here. In the next lesson we’re going to cover port mapping. So if you’re ready to keep learning then let’s get started in the next lesson.</p>
<h1 id="Port-Mapping"><a href="#Port-Mapping" class="headerlink" title="Port Mapping"></a>Port Mapping</h1><p>Welcome back. So far the code that we’ve run inside of containers has been writing some text to standard output. Now it’s time to run something a bit more complex. So let’s run a basic web application. Here’s the web application we’re going to run. It’s an application written in Go, and it has two URL endpoints.</p>
<p>The first is the root URL, and then there’s this other endpoint at slash host which will display the host name and some environment variables. I’m not going to go through all of this code. The important part to know is that this is a basic web app. It’s going to run on port 8080 and serve up just a bit of markup.</p>
<p>I have the web application compiled into a binary named web app, and it’s in the same directory as the Docker file. If I print off the Docker file to the terminal, you can see that it uses the scratch base image. It copies the web app binary into the container at the root directory, and then it exposes port 8080 from the container.</p>
<p>And we’ll talk about this in a bit. And then it sets the default command to be the web app. So it’s going to actually execute that binary. Okay, let’s build this image, and then we can start up the container based on the image. Great, so the image built successfully. And we can see it here in the list of images.</p>
<p>Let’s run this container with the docker run command. So notice it’s just hanging here. This is expected, because the code is running inside of the container in an infinite loop. It’s just running forever, serving up this website until we stop it. So the web application is running indefinitely and waiting for connections.</p>
<p>And while it’s running, we’re unable to interact with our bash prompt. Now really this is no different than if we ran this outside of Docker. It’s not specific to Docker. However, we can solve this. Docker allows us to run the process in the background with the minus D flag. So for this flag, D stands for detach, as in Docker detaches from standard I&#x2F;O and instead just prints the container ID, and then it’s going to return us to our prompt.</p>
<p>So this allows us to run containers in the background. Let’s try it out. Okay, there, notice the container ID is printed to the screen. And then we have access to our prompt again. So if I list off the containers, you can see based on the status here that that container is up and running. At this point, the container is running.</p>
<p>However, it’s really not clear how we interact with this web app. By default, Docker containers run inside of their own network, called the bridge network. We’re going to dive into that in the coming lesson on networking. So for now, the thing to know is that this web app is running inside of the container, and it’s accessible via the container’s IP address on port 8080.</p>
<p>It’s possible to fetch the IP address using the docker inspect command and then passing in the container ID or name. Notice here, the IP address field, you can see that it’s set with the container’s IP. And if we curl this and we use port 8080, you can see that we get back some HTML. So this is the home page of the app.</p>
<p>Because each container has its own IP address, all of them can run on port 8080 inside of their own container, and then we can interact with them via their IP address. However, there is another way to interact with a web application inside of a container, and that would be to bind the port in the container to a port on the host.</p>
<p>Docker allows you to do this dynamically or explicitly. The Docker file specifies that the container exposes port 8080. By default, the expose instruction really doesn’t do anything to the host. Docker lets you bind the container port to a host port with the publish or publish all flags. Publish all will dynamically map the exposed ports of the container to open ports on the host.</p>
<p>The minus capital P flag is the short form of publish all. So this is going to run the container in detached mode, which means it’s gonna run in the background. It will also dynamically bind the exposed port 8080 to a port on the host. So we can use the docker ps command to see which port. In this case, the port is 32768.</p>
<p>So if we curl localhost on that port, we should get back some HTML. And we do. If I use the terminal-based browser Lynx, you can see that the site is up and running. It’s usable, and everything works. So now if we start up another container, again using the publish all flag to dynamically map the ports, it’s going to choose a different port automatically.</p>
<p>And notice the first container in the list here is 32769, and the second one is 32768. So it’s going to map that for us automatically. Okay, let’s stop these containers. And notice that the stop command allows you to pass in multiple container IDs. So now that they’re stopped, let’s prune them. And that’s going to remove them both.</p>
<p>All right, perfect. So using publish all, you can dynamically map the ports that are exposed from the container to the host. Now, you can also use the publish flag, which will allow you to map specific ports. So in this case, we’re going to use a lowercase P flag, which is the short form of publish, and it allows you to specify the port on the host that you want to use, and then a colon, and then the port on the container.</p>
<p>So what I wanna do is map port 3000 on the host to port 8080 inside of the container. And now if I list off the containers, you can see port 3000 maps to 8080. And if I curl localhost on port 3000, there we go, we get our HTML back. So since I’ve explicitly told Docker to use port 3000 on the host, if I run the same command, it’s going to throw an error.</p>
<p>And there it is. When we did this dynamically, the ports were selected for us to ensure that there wasn’t any conflict. If I run docker ps, you’ll notice only the previous container is running. And if I run it again with the minus A flag, you can see that the other container tried to start and failed, so it’s just left here in a created state.</p>
<p>So as expected, you can’t bind two processes at the same time to the same port. So if you wanted to switch, you would have to stop the first container and then start the other. And there it is. Notice the first container is stopped and no longer using port 3000. And the new container is in a running state, and 3000 is bound to it.</p>
<p>Okay, let’s summarize what we’re covered so far in this lesson. Docker allows you to map ports from containers to the hosts running the containers. The mapping can be done dynamically with the publish all flag or explicitly with the publish flag. And by default, the expose instruction in the Docker file doesn’t actually perform any port mapping.</p>
<p>It’s up to you to determine how you want to publish the ports. All right, in the next lesson, we’re going to be covering the Docker networking just a bit more in depth. So if you’re ready to keep learning, then I’ll see you in the next lesson.</p>
<h1 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h1><p>Welcome back. In the previous lesson, we touched on networking just a bit when I mentioned bridged networks. Networking with Docker can get as intricate as networking anywhere else. So we’re not going to be covering everything there is to know in this lesson. However, in this lesson we will cover some of the basics that will help get you started.</p>
<p>For this lesson, we’re going to use an image that’s based on Ubuntu 16. 04 and then add some networking tools on top to help kind of show the networking concepts just a bit. Here’s a look at the file. It starts with the From instruction, specifying that the base image is Ubuntu, and then it specifies the tag of 16.</p>
<p>\04. The next instruction is the Run instruction, which is used to install some packages with the Apt package manager. These packages contain some networking binaries, things such as ping, arp-scan, IP, and others. The Copy instruction, we’re using that to copy the same web application binary that we used previously.</p>
<p>And then finally, the Command sets the default command to run bash. So this image will have the tools needed to test out networking with Docker. Okay, so now that you’ve seen the image we’ll be using, I want to introduce a new command. This is the docker network command, and we’re going to us the ls subcommand to show the existing networks.</p>
<p>The three that are listed here are the three that are pre-configured by Docker. The info in this table is pretty basic. There’s an ID, a name, a driver, and a scope. All of these have a local scope, which means they all exist as networks only on this local host. We’re only going to cover these three local options in this course, though future courses will cover additional types.</p>
<p>Let’s go through these three types. The bridge network is the default. Whenever you start up a new container and you don’t specify a network, it’s going to use the default bridge network. Okay, so I’ve created a new directory called networking and I’m currently in that directory. There are no containers running, and there are four images here, though none of these are the image based on that Dockerfile that I showed a minute ago.</p>
<p>So let’s build that image. So here’s a refresher of the contents of that Dockerfile. By listing off the contents here in this directory, you can see that we have the web app binary here alongside the Dockerfile. So using docker build, we can build this, and we can tag it with a repo name of ubuntu_networking.</p>
<p>So this will take a minute. I’m going to speed things along, and I’ll see you in just a second. Okay, with this done, let’s check the images. And there it is, perfect. Now we can start up a container based on this image. And so we’re here at the bash prompt. Let’s list off the interfaces here for this container.</p>
<p>And there we have the loopback interface, and we have the eth0 with an address of 172. 17. 0. 2. This container is using the bridge driver, which is connected to the interfaces on the host. So it has full connectivity. If I ping google.com, there you go. We have some results that come back. Okay, I want to detach from this container without stopping it.</p>
<p>That’s something I haven’t shown previously, but you can do that by pressing CTRL-p and then CTRL-q. And if I run docker ps, notice that the container is still running. From here on the host, you can interact with the container via its IP address. So if I ping it, there’s our response. Let’s start up a few additional containers so that we can see how these containers see each other on the same network.</p>
<p>So this one has an IP address here ending in three. So now we have two containers running inside of the network. Let’s start up one more. Okay, if we try and ping the others, you can see that we get some results back. I want to show all of the containers on this network, so for that I’m going to use the arp-scan tool.</p>
<p>I have the command copied to my clipboard, so I’m just going to paste it in. Okay, take a look at the results. We have the default gateway at the IP address ending in one. Then the other two are the two containers that I started before this one. So all of these containers that I started up are in this bridge network.</p>
<p>They have access to interact with each other as well as being able to reach the outside world. Now there’s not much you can do to customize the default network. However, you can create your own networks if you need some sort of customization. All right, let’s check out the next network on the list, which is our host network.</p>
<p>The host network is an interesting one. Host networks add the container to the host’s networking. This means if you have an application inside of the container running on, say, port 8080, it’s going to be bound to port 8080 on the host. A host network has no isolation between the host and the containers.</p>
<p>Let’s test this out by running the web application, which binds to port 8080. Because this isn’t the default network, we need to specify the host network in the command prompt here using the network flag. So this command will run the container in the background using the host network, and it’s going to run that web application.</p>
<p>Now listing off the containers, you can see that it’s running, and if we inspect the container, you’ll see that it doesn’t get its own IP address. Check out here under the networks section, the IP Address property is empty, and that’s because everything is exposed on this current host. So now if we curl localhost on port 8080, then we get our HTML back.</p>
<p>Now recall that in the Dockerfile, we didn’t actually expose port 8080, and we didn’t publish any of those ports manually. When using host networking, whatever ports you open up in the container are going to be bound to the host. Okay, let’s check out the third networking option, which is the none network.</p>
<p>This, as the name suggests, means there’s no network at all. Let’s start up a container using the none network. Okay, so here we’re at our bash prompt. If I list off the network interfaces, you can see that there’s only the loopback interface. And if I try and ping google.com, it’s going to fail. And there it is.</p>
<p>It’s failed. So the none network is just what it says. There’s no networking interface. It’s completely isolated. All right, let’s wrap up this intro to networking here. Now there’s a lot more that could be covered regarding networking, though it should be covered once you have a more solid grasp of Docker.</p>
<p>So we’re going to save that for a future course. In the next lesson, we’re going to cover persistent storage options when we cover volumes. So if you’re ready to keep learning, then I’m going to see you in the next lesson.</p>
<h1 id="Introduction-to-Persistent-Storage-Options-in-Docker"><a href="#Introduction-to-Persistent-Storage-Options-in-Docker" class="headerlink" title="Introduction to Persistent Storage Options in Docker"></a>Introduction to Persistent Storage Options in Docker</h1><p>Welcome back! Now, if all of your applications were stateless and never needed data storage, life would be much simpler. However, in the real world we need data storage. We’ve talked about the different layers that make up a Docker image. The last layer of the image is a writable layer. Which means applications that need write access will work just fine.</p>
<p>However, as soon as you stop the container, whatever you’ve written there is gone. Which means using the writable layer is not an effective way to handle persistent storage. Luckily, Docker provides three options. Which are bind mounts, volumes and in memory option called tmpfs, as in temporary file system.</p>
<p>Bind mounts have been around for a while. They work by mounting a file or directory that resides on the host, inside the container. This remains an effective mechanism that allows you to access files from the host inside the container. And once the container stops, the data remains because it lives on the host.</p>
<p>The downside here is that bind mounts aren’t as decoupled from the host as you might like. You need to know the exact path on the host that you want to mount in the container. The upside is that this could work well for development, because you don’t need to rebuild the image to access the new source code, so you make changes to your source and it reflects immediately.</p>
<p>Docker still supports bind mounts because they work well. However, the preferred way to handle persistent file storage is with volumes. Volumes are basically just bind mounts, except that Docker manages the storage on the host. So you don’t need to know the fully qualified path to a file or directory. This makes it easier when working cross platform, because Docker handles the volume.</p>
<p>Volumes aren’t limited to the local host file system either, they allow you to use different drivers. The drivers support the use of external storage mechanisms such as Amazon’s S3, Google Cloud Storage, and more. When you stop a container using volumes or bind mounts, the data remains on the host. In contrast, the third storage option, tmpfs, is different.</p>
<p>Temp FS is an in temporary file system is an in memory file system. From inside the container you still interact with the files the same way you would any other file. The difference is that tmpfs is not persistent. Because tmpfs allows file system access for the life of the running container, it’s commonly used to hold sensitive information, that includes things like access tokens.</p>
<p>Let’s try these out. To test out bind mounts and volumes, I’ve created an app in Go that will loop 50 times and it will write the host name and the loop counter to a file. The file is specified as a command line argument so you can pass in any file you want. So this will allow us to write some data to the volumes from multiple containers.</p>
<p>I’ve already compiled this, and the binary is in the same directory as the Dockerfile. The Dockerfile for this demo is very basic. It’s based on the scratch image, it copies the binary to the root directory, and finally it sets the default command to run the binary, and pass in the path to where we want to write the data to be written to.</p>
<p>So that’s our volume. The directory of &#x2F;logs is going to be the mounted directory, and the name myapp is just an arbitrary name I’ve made up, so it has no real meaning. Here in the terminal, you can see there are no containers, and here’s the list of existing images. In this directory you can see the Dockerfile and binary.</p>
<p>Let’s test out bind mounts first, and for that we need a directory to mount. So I’ve created a directory under &#x2F;var&#x2F;demo&#x2F;logs, and you can see here that it’s empty. Okay, let’s build the image that we’ll use for the demo. Let’s call it scratch_volume. Okay, that didn’t take long, and there it is in the list.</p>
<p>So now, I want to paste in a command that I have copied to my clipboard. Most of the parameters used here have been used throughout the course. With the exception of the mount flag. This is the most current way to specify different storage options, since it’s the most flexible. Notice that the type is set to bind.</p>
<p>Then you need to specify the directory source and the destination. The source is the directory on the host you want to mount, and the destination is the path inside the container where it will be mounted. In this example the &#x2F;var&#x2F;demo&#x2F;logs directory will be available inside the container at &#x2F;logs. Recall that this image is going to write some data to the a file inside of the mounted &#x2F;logs directory.</p>
<p>So let’s start up a few containers because I want to show how multiple containers will have access to the same directories and files. Okay, now, that this has been running for a moment, there should be some data that has been written from the container to the bind mount. Here you can see the last 30 lines written to the myapp file that the application created.</p>
<p>If I print out the unique host IDs in the file, here we go, you can see there are four entries. If I compare them against the container IDs you can see that each container was able to write to the same shared file. Remember that when using a bind mount, the directories and files are managed by you and not Docker.</p>
<p>This means that the listing here for volumes will show zero results. However if you list off the files under the &#x2F;var&#x2F;demo&#x2F;logs you’re going to see the myapp file is still available to use on the host. Okay, let’s prune these containers so we can clean things up a bit, and there we go. Now let’s try the volume.</p>
<p>Here’s roughly the same command as before except the type is now set to volume. Also with a bind mount the source is a fully qualified path, and now it’s not. Since Docker manages it, you can just use a name for the volume. Docker allows you to create the volume with the docker volume command. However, if you use the mount flag with a type set to volume, and the volume doesn’t exist, it’s going to automatically be created for you.</p>
<p>Notice how it shows up under the volumes as a local volume and it’s named logs. Docker manages the location of the volume, which you can find by using the docker volume ls command. Notice here that the mount point is under &#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes. Let’s start up a few more containers, okay great! Now we can use tail to show the results streaming in.</p>
<p>Let’s use the tail command and refresh every second. Notice all of the results from the different hosts are being appended. Let’s once again display all of the unique IDs to see if all of the containers successfully were able to write their data. Let’s cat the file, and then let’s cut based on a space, and we’ll grab the second field, and we’ll pipe through sort, and we’ll cap it off piping through the unique binary.</p>
<p>Okay, and you can see we have 4 IDs. Listing off the existing containers you can see it’s the same IDs. With a bit more bash scripting we can sort just the container IDs and there you can see the IDs are in the same order now. Just makes it easier to compare. So what does all of this actually show? Both bind mounts and volumes are roughly the same.</p>
<p>However, volumes are managed by Docker. Both allow multiple containers to access the same mounts. Both keep the data on the host after the containers are stopped or removed. Let’s check out the final storage type, which is the temp filesystem, tmpfs&#x2F; Tmpfs is similar to the others in that you can create it with the mount flag.</p>
<p>However since it’s only creating an in memory construct you don’t need to specify a source directory, because there’s nothing on the host that’s gonna persist that info. For this demo we’ll use the ubuntu image so we can create an interactive bash session. The type here is tmpfs and the destination is set to &#x2F;logs.</p>
<p>So inside the container we’ll have a directory under &#x2F;logs that behaves just like the other storage options; except that it only exists while the container is running. There’s nothing here under the &#x2F;logs. So let’s add a file. We can just echo some text out to standard out, and we’ll call the file demo.</p>
<p>Now, I’m going to exit the container and keep it running with CTRL+p followed by CTRL+q. You can see that based on the listed containers that it’s still running. If I list off the volumes, you can see that it doesn’t show any new volumes. I show this to demonstrate that all three storage types use the same mount flag, however, they’re all implemented in a different way.</p>
<p>If we start a new Ubuntu container with a tmpfs mount we can see that unlike bind mounts and volumes, tmpfs is container specific. Let’s list off the files under &#x2F;logs, and it’s empty. Let’s reattach to this container start with ID starting with 4d. Alright, if we look at the logs directory and print the demo file to screen, you can see that the demo file text is there and the file still exists.</p>
<p>That’s because the container hasn’t been stopped, so while it’s running that information is available. So now if we stop the container by existing out. Okay let’s list off you can see that it’s stopped, and there it is. Now let’s start the container again, okay. And now let’s attach to the container. And here we are, back at the bash prompt.</p>
<p>And if we list off the contents of the logs directory you can see that it’s now empty. Tmpfs is a great option when you need file system access that’s isolated to the container. So this is really great for things like sensitive information, access tokens, or any sort of sensitive information that your app might need while the container’s running is useful for this sort of mount.</p>
<p>Storage is a crucial part of most applications, and that’s why Docker provides you options. Each of these is a viable option, depending on the use case. However, if you’re not sure which type to use, then volumes are probably the safest bet. Alright, let’s wrap up here. In the next lesson, we’re going to learn more about tagging.</p>
<p>So, if you’re ready to keep learning, then I’ll see you in the next lesson!</p>
<h1 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h1><p>Welcome back. Throughout the course, we’ve used tags. In the networking lesson, we used the tag 16. 04 to specify the version of Ubuntu. And in other lessons, the tag latest was implied because we didn’t specify a tag for ourselves. In this lesson, we’re going to cover tagging a bit more.</p>
<p>Tags provide you with a way of identifying specific versions of an image. Tags make it easy to deploy a specific version, and then if something goes wrong, you can easily roll back to the previous version. One of the useful features of tags is that an image can have more than one tag. This is useful if you want to have version numbers as well as named versions.</p>
<p>An example of this is the Ubuntu image, which has tags for the version, the OS code name, and optionally a tag to indicate the development image, et cetera. So tags are rather versatile and are an important mechanism for a solid continuous delivery process. However, we’re not going to cover the use cases here.</p>
<p>Rather, since this is an intro course, I want to cover how to actually tag an image. To do that, we have this basic Dockerfile. Starting at the top, you can see the use of the tagged image version. This is going to use the Ubuntu image that’s tagged 16. 04. Then it will create a directory and create a file with the contents of Version 1.</p>
<p>The Docker command will simply print the message to standard out by using the concatenate command. Okay, let’s build this Dockerfile into an image. With the T flag, we’re actually specifying the tag. Thus far, we’ve used the tag as just the local image name. However, the tag structure allows you to define a Docker registry where the image will reside.</p>
<p>If you don’t include a registry name, Docker assumes that you want to use its registry by default. And then you can specify your registry, your username and the image name. And finally you can end it with a tag. Now, this may sound confusing, so let’s use an example. Let’s give this an image name of tag_demo.</p>
<p>And it takes a second to build, and here at the end, notice it says successfully tagged as tag_demo:latest. Up here, we only provided the image name, so Docker provided the tag of latest automatically. If you don’t provide a tag, then Docker automatically uses the special tag called latest. If we list off the images, there it is, and the tag says latest.</p>
<p>Now, imagine you want this image to be Version 1. So, let’s use the command that we haven’t used before called docker tag, and we’ll specify the tag_demo image with a tag of latest. And now we need to set our new tag. It doesn’t require the same name. However, I’m going to keep it the same name for consistency.</p>
<p>So we’ll call it tag_demo:v1. OK, and listing it off, you can see that it’s listed as v1. Notice that both images have the same ID. That’s because it’s the same image with just a different way to reference it. If I was to run a container based on the tag_demo image, without specifying a tag, it’s going to use the image with a tag of latest.</p>
<p>Anytime you reference an image and you don’t provide a specific tag, it’s always going to look for a tag named latest. So, running this returns the message in the file of v1. And if we run another and specify the tag of v1, again we get the same message because these two containers are using the same image, which you can tell from the IDs.</p>
<p>Now, let’s edit the Dockerfile to write out a different message, let’s say, “Version 2,” this time, and let’s save this, great. Now, let’s build this image. This time, I’m going to specify a tag of v2. Okay, check this out. It says it was tagged with v2. Recall that previously when we built the images, we didn’t specify the tag, so it automatically used the tag of latest.</p>
<p>Now, if I list off these images, notice that the v2 image has a different ID than the v1 or latest. Let’s create a container based on the image with a tag of latest. Before we do, what do you expect is gonna be printed to the screen here? Which text will print out for the latest image? Well, if you said, “Version 1,” then you’re correct.</p>
<p>If you said, “Version 2,” maybe running the container tagged v2, it will help clarify things. Notice it prints out, “Version 2”. The reason is that we built the image after changing the message, and we explicitly set the tag to v2, which means that the latest tag is still set to v1. Let’s update the latest tag to point to v2.</p>
<p>So we specify v2, and then we set the latest tag, perfect. Now, listing off the images again, you can see that the IDs match between v2 and latest. If we run a container based on the latest tag, it shows the message, “Version 2”. If we run again without specifying a tag, it automatically tries the one with a tag of latest and again, Version 2.</p>
<p>Let’s make another change to the Dockerfile and we’ll change the message again to Version 3, and let’s save this, perfect. Now, let’s build it again, and tag it with a name but without providing a tag explicitly, so that it’s automatically going to be tagged as latest. Listing off the images, the three named tag_demo each have a different tag.</p>
<p>If we run a container based on the image named tag_demo, it going to print off the message, “Version 3”. So now let’s tag the latest image as v3. And listing off the images, you can see that v3 and latest are both the same image. Running either the image named latest or v3 will produce the same message.</p>
<p>Throughout the course so far, I’ve used some of the public images in the Dockerfile as our base image. Let’s use one of our own images as a base. First, let’s create a new directory and CD into it. And now let’s use nano to create a new Dockerfile. And for this, the only instruction we’re going to use is the FROM instruction, and we’ll specify tag_name and we’ll use v2, and now let’s build this.</p>
<p>And we’ll tag it as different_tag_demo, and we’re not going to provide a tag so that it will automatically be set to latest. And if we run a container based on this new image, it prints off Version 2 because, in the Dockerfile, we explicitly used the v2 tag. Outside of the development environments, it’s considered a best practice to use explicit tags for the image that you want to use in your Dockerfile.</p>
<p>Do you recall earlier in the course I mentioned Docker Hub? Well, before we wrap up this lesson, I want to show how to push an image to Docker Hub. Let’s push the latest version of tag_demo. This is done with a docker push command. If I try this out now, it’s going to throw an error. Okay, notice here it’s throwing an error because I’m not logged in yet.</p>
<p>So, before you can push an image, you need to authenticate with the docker login command. Before that, you need to have an account set up on Docker Hub. Now, that’s a separate thing. You’ll need to go create an account for yourself. Notice it has my username set here as a default, so I can press enter to accept that.</p>
<p>And I’ll paste in my password, and there we go. We’re all logged in. Let’s run the docker push command again and specify the image name, and we get another error message. Don’t worry, this is expected. Notice here in the URL it’s trying to push the image to docker.io&#x2F;library, followed by the name of the image.</p>
<p>The reason that this failed is that you can’t just upload an image of whatever name you want. That would be potentially chaotic. That’s why Docker makes you specify the name of your Docker Hub account in the tag. If you recall, I mentioned that the tag structure is a bit more complex than just an image name.</p>
<p>It also allows you to specify a registry, a username, et cetera. In order to push to my Docker Hub account, I need to tag this with my username. So, if I switch over to the Docker Hub UI, you can see it here. The username is separated with a forward slash, then the image name, and then the specific tag.</p>
<p>Okay, perfect, listing off the images, you can see that it has the same ID has the image named tag_demo. Okay, now we are actually ready to push this image to Docker Hub. This time, we just need to provide the new tag of sowhelmed&#x2F;tag_demo. And if you’re following along, you’ll need to use your own account name.</p>
<p>Now, back in the Docker UI, there we go. There’s our image. It even lists off the command to run so that anybody else can download this image. So, there you have it! Tagging in Docker allows you to give images specific identifiers. An image can have more than one tag. And Docker has a special tag called latest that will be automatically used if you don’t specify a tag.</p>
<p>However, the latest tag only represents the last image that you built that you didn’t provide a tag, unless you tag it as latest for yourself. So, don’t think that by typing latest you’re going to automatically get the newest image. That’s not how it works. And finally, when you upload a image to the Docker Registry, you’ll need to update the tag to include some additional information, such as the account name.</p>
<p>All right, let’s wrap up this lesson here. In the next lesson, we’re going to summarize what we’ve covered throughout the course and wrap things up. So, if you’re ready to wrap up this course, then I’ll see you in the next lesson.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Welcome back. We’ve covered a lot throughout the course, so let’s wrap up the course by summarizing what we’ve learned. In the first two lessons, we talked about Docker at a pretty high level. We covered what it is, and we talked about its architecture. Some topics are a bit of a chicken and egg type of scenario, as in it’s difficult to explain one without first explaining the other, in a circular logic sort of endless loop.</p>
<p>Now that you’ve seen Docker a bit, and you have some familiarity with it, the architecture should make a bit more sense. The gist of that lesson was to explain that Docker isn’t like a virtual machine; rather, it allows you to run processes in an isolated way, using several Linux kernel features. Now, it may be worth going back and re-watching the architecture lesson again so that it kind of ALT clicks now that you have some familiarity with Docker.</p>
<p>After that, we installed Docker on CentOS, and created a container. Creating a container is just a simple command, however, behind the scenes there’s a lot going on. Docker tries to find the image locally. If it can’t, it attempts to download it from a Docker registry, and once the image is on the local host, then it can create containers based off of that image.</p>
<p>And Docker also allows you to use the I and T flags so that you can interact with the processes running inside of the container. After creating the container, we talked about the difference between a container and an image. The difference being that a container is a running instance of an image; similar to how an executable becomes a running process.</p>
<p>Then we went on to cover creating images via the Dockerfile, and via Docker Commit. Creating your own images allows you to bundle up just what you need for your app. Now, there are a lot of best practices for creating images, and we didn’t really get into them. However, practices such as keeping the images as small as possible, using official public images as base images, and ensuring images have one purpose, as well as reducing the number of layers, are just a few best practices.</p>
<p>After creating some basic images that run simple binaries, printing out text to standard output, we tested a web application. With the webapp we get to see how ports work, and how to dynamically and explicitly set them. And we tested out three different networking options. Tested out bridge networks, which are the default if you don’t specify one.</p>
<p>Bridge networks allow containers to start in their own network that’s connected to the interfaces of the host OS. The default bridge network isn’t very customizable, however, you can create your own if you need to make some changes. We covered volumes after that. Docker provides three different storage options.</p>
<p>Volumes are the newest and most versatile because they also can allow you to specify your own driver to use. The drivers allow you to have different storage options, such as S3, Google Cloud Storage, or others, in place of local disc storage. We also covered the original option called, bind mounts, The difference between bind mounts and volumes is that volumes are managed by the Docker engine.</p>
<p>Bind mounts aren’t, and that requires you to know where on disc that mount is located. The final option was the temporary file system, named TempFS, which is an in memory option that lasts while the container is running. The final lesson covered tagging a bit more in depth. Tagging is important, and in production it matters how you tag things.</p>
<p>Following a strict procedure for tagging ensures that you’re going to be able to deploy the correct version, and then roll back if needed. And while it’s important, there’s no single strategy that’s going to work for everyone. So it’s important to find a strategy that works for your team. At the start of the course, I listed off some learning objectives.</p>
<p>I said, by the end of the course, you should understand what Docker is. You should understand how to create Docker images. You should understand how to map ports between the Docker host, and the Docker container. You should understand the networking basics. And you should understand how to use volumes for persistent storage.</p>
<p>And finally I said that you should be able to tag images. Now having made it this far, you should feel comfortable with each of these objectives by now. So, what’s next? What comes after all of this? Next, I recommend that you try this stuff out for yourself. Hands on experience is the best teacher. So, I recommend that you try out our Docker Labs, and that you try installing this locally, and follow some of the examples.</p>
<p>When you’ve gone through the examples, try and create some of your own. If you have some source code that you want to try and bundle up, I think that’s the best way to learn, is seeing how things that you already are familiar with will work inside of a container. Docker is a technology that continues to evolve, so depending on when you see this course, things may have changed.</p>
<p>If part of the changes introduce errors, then I want you to try and work through them. These are the sorts of errors that you’re going to run into in the real world. So, take the opportunity to understand why the error is happening, and it’s going to give you a better depth of knowledge into Docker. All right, let’s wrap up the course here.</p>
<p>As I mentioned at the start, I enjoy hearing from you guys. If you don’t provide me with the feedback about what you dislike, and what you like, I can’t improve upon the content. So, feel free to reach out to me via <a href="mailto:&#115;&#117;&#x70;&#x70;&#111;&#x72;&#116;&#x40;&#99;&#x6c;&#x6f;&#x75;&#x64;&#x61;&#99;&#x61;&#x64;&#x65;&#109;&#121;&#x2e;&#x63;&#x6f;&#109;">&#115;&#117;&#x70;&#x70;&#111;&#x72;&#116;&#x40;&#99;&#x6c;&#x6f;&#x75;&#x64;&#x61;&#99;&#x61;&#x64;&#x65;&#109;&#121;&#x2e;&#x63;&#x6f;&#109;</a>, or on Twitter, I’m @sowhelmed. That’s gonna do it for this course.</p>
<p>Thank you so much for watching. I hope that this has been useful to you. I had a lot of fun making it, and I hope that you’ve enjoyed it. I’m Ben Lambert. From Cloud Academy and myself, thanks for watching.</p>
<h1 id="7Images-From-The-Dockerfile"><a href="#7Images-From-The-Dockerfile" class="headerlink" title="7Images From The Dockerfile"></a>7<strong>Images From The Dockerfile</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">Docker Documentation</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/CKA-Cert-Prep-Certified-Kubernetes-Administrator-CKA-Exam-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/CKA-Cert-Prep-Certified-Kubernetes-Administrator-CKA-Exam-16/" class="post-title-link" itemprop="url">CKA-Cert-Prep-Certified-Kubernetes-Administrator-CKA-Exam-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:39:48" itemprop="dateCreated datePublished" datetime="2022-11-19T00:39:48-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 23:50:18" itemprop="dateModified" datetime="2022-11-20T23:50:18-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CKA-Certificate/" itemprop="url" rel="index"><span itemprop="name">CKA-Certificate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/CKA-Cert-Prep-Certified-Kubernetes-Administrator-CKA-Exam-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/CKA-Cert-Prep-Certified-Kubernetes-Administrator-CKA-Exam-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p><object data="Cert-Prep-Certified-Kubernetes-Administrator-CKA-Exam.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/CKA-Certified-Kubernetes-Administrator-CKA-Challenge-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/CKA-Certified-Kubernetes-Administrator-CKA-Challenge-15/" class="post-title-link" itemprop="url">CKA-Certified-Kubernetes-Administrator-CKA-Challenge-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:39:46" itemprop="dateCreated datePublished" datetime="2022-11-19T00:39:46-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 23:51:26" itemprop="dateModified" datetime="2022-11-20T23:51:26-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CKA-Certificate/" itemprop="url" rel="index"><span itemprop="name">CKA-Certificate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/CKA-Certified-Kubernetes-Administrator-CKA-Challenge-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/CKA-Certified-Kubernetes-Administrator-CKA-Challenge-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/16/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><span class="page-number current">17</span><a class="page-number" href="/page/18/">18</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/18/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
