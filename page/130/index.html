<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/130/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/130/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:55:52" itemprop="dateCreated datePublished" datetime="2022-11-14T12:55:52-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:41:42" itemprop="dateModified" datetime="2022-11-21T02:41:42-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Required-Reading-for-Google-Cloud-Digital-Leader-Exam"><a href="#Required-Reading-for-Google-Cloud-Digital-Leader-Exam" class="headerlink" title="Required Reading for Google Cloud Digital Leader Exam"></a>Required Reading for Google Cloud Digital Leader Exam</h1><p>Before taking the Google Cloud Digital Leader exam, you should review the following documentation:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/architecture/migration-to-gcp-assessing-and-discovering-your-workloads#calculating_total_cost_of_ownership">Calculating total cost of ownership</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/products/gcp/improving-data-quality-for-machine-learning-and-analytics-with-cloud-dataprep">Data quality in machine learning</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/products/ai">GCP AI and ML products</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/storage">GCP Cloud Storage</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/security">GCP Security</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/apigee/docs/api-platform/get-started/what-apigee">Introduction to Apigee</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/looker">Introduction to Looker</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs">Optimizing cloud costs</a></li>
<li><a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">OWASP Top Ten</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/solutions/unlocking-legacy-applications">Unlocking legacy applications using APIs</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/products/data-analytics/how-7-eleven-japan-built-its-new-data-platform">Updating legacy systems with GCP</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/learn/what-is-a-data-lake">What is a data lake?</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/learn/what-is-a-data-warehouse">What is a data warehouse?</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/learn/what-is-machine-learning">What is machine learning?</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:54:45" itemprop="dateCreated datePublished" datetime="2022-11-14T12:54:45-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:40:34" itemprop="dateModified" datetime="2022-11-21T02:40:34-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to Site Reliability Engineering Principles on Google Cloud Platform. My name is Daniel Mease and I’ll be taking you through this course. I’m a GCP instructor here at cloud Academy with over five years of cloud experience and 15 years of software development experience. If you have any questions or concerns about the content of this course, please connect with me on LinkedIn or send an email to <a href="mailto:daniel.mease@cloudacademy.com">daniel.mease@cloudacademy.com</a>. If you experience any problems or technical issues make sure to email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. And one of our cloud experts will get back to you as soon as possible.</p>
<p>There are a few different types of people who will benefit from taking this course. First, anyone who wants a basic introduction to Site Reliability Engineering. Second, anyone currently implementing DevOps practices who want to understand how Site Reliability Engineering can help play a role. Finally, anyone taking the Google Professional Cloud DevOps Engineer certification exam.</p>
<p>Now this course is going to provide a general overview of Site Reliability Engineering, including its history, core vocabulary, practices, and how it can complement DevOps. The content of this course is mostly high-level concepts. And so the prerequisites are gonna be pretty light. It would be useful to have a basic understanding of DevOps as well as having a basic understanding of the software development life cycle.</p>
<p>So if you’re ready to learn about Site Reliability Engineering, let’s get started.</p>
<h1 id="History-and-Goals"><a href="#History-and-Goals" class="headerlink" title="History and Goals"></a>History and Goals</h1><p>In order to better understand site reliability engineering or SRE, you first need to understand a little about its history as well as its relationship to DevOps. Historically, building a production system involved two different teams, developers and operators. Developers were responsible for updating and writing new software. Operators were responsible for deploying that software to production and monitoring it.</p>
<p>Now, operators were not programmers. They did not touch the code. Typically they didn’t even look at it, but they did understand how to assemble all the software components and make them work together to produce a surface. They also knew how to scale and maintain everything. If anything went wrong, it was the operators who sprung into action to resolve the problem. Each role had a different set of skills, and focused on different priorities. These differences often led to conflict.</p>
<p>Developers would spend their days fixing bugs, adding new features and constantly evolving the code. They were focused on agility and always wanted to make bigger and more frequent software updates. Operators spent their time fixing production issues and generally trying to keep everything running smoothly. They were focused on stability and always wanted smaller and less frequent updates. It was pretty common for operators to try to implement procedures, to slow down the rate of change while developers would try to bypass or ignore them. It was from this conflict that the idea for DevOps was born.</p>
<p>DevOps is a set of practices, guidelines and culture that was designed to reduce the gap between software development and software operations. The basic idea being, if we can get these two groups to work together, we can increase the company’s overall throughput and growth. In order to achieve this, DevOps established five goals. Number one, reduce organizational silos. The separation of development and operations meant that there was very little collaboration or cross training. At times these two groups were even working at cross purposes. Number two, accept failure as normal. Neither people nor systems are perfect. People who are scared of failure also tend to fear change, and a company that does not change will not grow. Number three, implement gradual changes. Big changes are harder, riskier and take longer to recover from in the event of failure. Smaller incremental changes are just the opposite. Easier, safer, and quicker to recover from. Number four, leverage tooling and automation. Manual work simply does not scale, and too much of it can be very costly. The right tooling and automation frees people up to do more interesting and valuable work. Number five, measure everything. Growth requires making the right changes. You need to constantly measure your performance to make sure you’re making the right decisions.</p>
<p>Now, the problem with DevOps as I previously mentioned was that it is very broad and it does not explicitly define how to implement these goals. That is where site reliability engineering or SRE comes in. Site reliability engineering evolved at Google independently of the DevOps movement, but it happens to embody many of the same goals. It also has a much more prescriptive way of achieving those goals. You can think of DevOps as a philosophy and site reliability engineering as one specific implementation of that philosophy. So, if you want to implement DevOps practices on Google Cloud Platform, it is valuable to understand SRE.</p>
<p>Site reliability engineering implements that DevOps goals with the following practices. Number one, creating the site reliability engineer role. This is a new role that replaces operators, and it focuses on sharing the responsibility of production with developers. Number two, holding blameless postmortems. These are retrospective meetings held after incidents to learn what failed and how to prevent those failures from happening again. Number three, defining and enforcing an error budget. Budgeting your money results in less spending and ensuring that the right bills get paid. In much the same way, an error budget encourages smaller changes and ensures the right balance of growth and stability is maintained. Number four, identify and work to reduce toil. Site reliability engineering defines medial operational tasks as toil, and it provides mechanisms for measuring and reducing that. Number five, track service level metrics and goals. These include SLIs, SLOs, and SLAs.</p>
<p>The following sections will cover each of these five concepts in more detail.</p>
<h1 id="SRE-as-a-Role"><a href="#SRE-as-a-Role" class="headerlink" title="SRE as a Role"></a>SRE as a Role</h1><p>So let’s talk about what a Site Reliability Engineer is and what their responsibilities include. DevOps and SRE have a lot in common, but there are some differences. DevOps is a company-wide culture and it is supposed to be everyone’s job. In contrast, Site Reliability Engineering is a specific job role. That means the old role of operator is replaced with another, the Site Reliability Engineer.</p>
<p>A Site Reliability Engineer is basically the result of asking a software engineer to design an operations team. This means that the new role requires experience in both software development as well as a strong knowledge of operations. Someone who acts as an SRE will spend about half of their time doing ops-related work, such as monitoring and responding to production issues, being on-call, or performing manual interventions. But the other half of their time will be spent on development tasks, such as building new features, scaling systems, or writing automation.</p>
<p>A Site Reliability Engineer views operations as a software problem, and uses software engineering approaches to solve issues. One key difference from the old operator role is that both SREs and developers share the responsibility of maintaining production. Developers are no longer allowed to write some code and then throw it over the wall expecting operations to figure it out and make it work. Instead, SREs build the tools that developers use to compile, test, and deploy their code. If something breaks, the tools written by the SRE team will detect and alert everyone. And when it comes time to fix the issue, developers and SREs work together to come up with a solution. So when an incident is detected, SREs help coordinate the response.</p>
<p>The person who declares the incident takes on the role of Incident Commander or IC. It is the IC’s job to direct the high-level state of the incident. To help assist, the IC will assign two other roles: first, an Operations Lead or OL and second, a Communications Lead or CL. The OL and the CL both report back to the IC. The Operations Lead role exists to lead the team who will be investigating and ultimately resolving the issue. The engineers doing the actual work report their progress back to the OL. And while the IC and OL are working to mitigate and resolve the incident, the Communications Lead or CL is busy keeping everyone informed and answering questions.</p>
<p>Clear communication is critical, and it needs to be made a high priority. Do not wait until after the incident has ended. The CL should immediately establish clear channels of communication, provide regular updates to the response team as well as stakeholders, and handle any inquiries. Site Reliability Engineers don’t just react to problems; they are proactive as well. SREs are heavily involved in the development process. In the design phase, SREs help establish best practices, identify architectural mistakes, and even help co-design parts of the service.</p>
<p>During development, SREs should be building the tools that everyone will use to eventually manage and maintain it, including monitoring and alerting. After deployment, SREs verify that the service is stable and performs as planned. And finally, once a service has been deprecated, SREs will help transition users from the old service to a new one, if available, as well as help clean up configurations and documentation.</p>
<p>Now, remember when I said that the first goal of DevOps is to reduce organizational silos? Site Reliability Engineering accomplishes this by involving SREs in development work and developers in operations work.</p>
<h1 id="Blameless-Postmortems"><a href="#Blameless-Postmortems" class="headerlink" title="Blameless Postmortems"></a>Blameless Postmortems</h1><p>In this section, I want to explain what a blameless postmortem is, including why they’re important and how they should be conducted. As stated previously, the second goal of DevOps is to accept failure as normal. Now, this is an important concept to understand. No matter how hard you try, failures will happen. Change and growth requires risk. Faster growth requires higher risk. In order to reduce risk to near zero, you would need to reduce your rate of change to near zero.</p>
<p>Now, ask yourself, will my users be satisfied with a stable but quickly out-of-date system? Instead of trying to avoid failure at any cost, you can view it as an opportunity to grow. When things break, we learn how to fix them. When you break and fix something enough times you gain a better understanding of how it works. This understanding helps minimize future issues and expedites the resolution process.</p>
<p>In site reliability engineering, this is accomplished through holding retrospectives or blameless postmortems. A retrospective or post-mortem is a meeting whose goal is to recap and analyze a significant service failure. It provides an open forum where everyone can ask questions, share their experience, and gain a clear understanding of exactly what happened. The goals of a post-mortem are to identify the contributing factors, determine how those factors could have been mitigated and to come up with a list of action items that will prevent the same failure from happening again.</p>
<p>A blameless post-mortem is one that focuses on dealing with the incident without trying to single out an individual or team for bad behavior. It assumes that everyone involved had good intentions and made the best choices they could with the information at hand. A post-mortem should never become a witch hunt, looking for someone or something to blame. This will inevitably create a culture in which issues are swept under the rug, leading to greater risk for the organization.</p>
<p>So, when conducting a post-mortem, you will want to ask the following questions:</p>
<ol>
<li>When did the incident begin?</li>
<li>When did the incident end?</li>
<li>How were we notified that there was a problem? It is important to understand exactly how long the problem lasted and how long it took you to notice that there was a problem. You may need to adjust your monitoring and alert system if there is a significant gap between the two. </li>
<li>Who was involved in responding?</li>
<li>When did we begin to respond?</li>
<li>What was our response? Carefully review what your response was. Was it quick enough? Did it follow established procedures? Was it sufficient?</li>
<li>What was affected? It is important to document everything that was affected, which systems were down, which customers noticed, was there any revenue lost.</li>
<li>Is there anything else that still needs to be done to recover? You might’ve made some temporary fixes that need to be replaced with a long-term solution, or there may be other after effects that need to be dealt with in the future.</li>
<li>What were all the things that contributed to this failure? Remember that in complicated systems there are usually many contributing factors to a failure, and not a single root cause. Identify all factors and document them. This should include writing up new bugs in your tracking system. </li>
<li>How can we avoid similar problems in the future? It is critical to establish detailed action items. What specific changes are going to be made? What is the deadline for making those changes? Who specifically is going to be responsible for making them? Remember you wanna focus on solutions. Don’t assign blame for past mistakes, but do assign responsibility for future improvement. Common types of changes will include things such as fixing bugs, adding new infrastructure, updating or building new tools, revising current policies and creating new documentation or training. </li>
<li>What went right? </li>
<li>What went wrong?</li>
<li>Where did we get lucky? You not only want to identify what went wrong, but also the things that help mitigate and recover from the issue. And sometimes you just get really lucky and it’s important to call that out as well.</li>
</ol>
<p>You want an accurate understanding of all risks so that you can properly prioritize your fixes. Blameless postmortems encourage open and honest communication after a service failure. They acknowledge that system design is complicated and that human beings make mistakes. They also ensure that your team will learn from those mistakes and avoid repeating them in the future.</p>
<h1 id="Toil"><a href="#Toil" class="headerlink" title="Toil"></a>Toil</h1><p>In this next section, I will define what toil is and explain how it can be mitigated using automation. Earlier, I had mentioned that the third goal of DevOps is to leverage tooling and automation. A lot of traditional operations work was manual, repetitive and labor intensive. Common examples would include things like resetting passwords, responding to alerts, rolling out patches, restarting servers, and copying and pasting commands from a playbook. S.R.E calls this type of work toil.</p>
<p>Toil occurs every time an operator needs to manually touch a system during normal operations. Keep in mind that toil is not a synonym for boring or frustrating. Filling out an expense report may not be fun but that does not make it toil. Instead, toil is work that is tied to running a production service and tends to be manual repetitive, automatable, tactical, and devoid of long-term value. The amount of toil increases linearly as the service grows and if ignored can grow out of control until your entire team is consumed by it. Too much toil leads to career stagnation, boredom and burnout.</p>
<p>You can group S.R.E activities into four main categories: software engineering, systems engineering, overhead, and toil. Software engineer includes things like writing automation scripts, creating tools, or modifying infrastructure code to make it more robust. Systems engineering includes things like installing updates, server configuration, or load balancer setup. Overhead is administrative work that’s not directly tied to running a service and includes things like conducting interviews, attending meetings and completing peer reviews. Toil is the work directly tied to running a service and it’s repetitive, manual, et cetera.</p>
<p>The S.R.E discipline aims to reduce toil through automation. S.R.E is try to identify repeatable tasks and write programs to reproduce the work. This means creating things like scheduled jobs instead of manually running scripts, automated monitoring tools instead of manually monitoring and rebooting unresponsive servers, continuous integration and continuous deployment pipelines instead of manually testing and deploying new code. And auto-scaling infrastructure instead of manually provisioning new hardware.</p>
<p>While automation is extremely helpful, not every task is worth automating. But if a repetitive task can be automated, it probably should be. And once it has, you’ve successfully freed up more resources for future development efforts. Ideally, S.R.Es will spend about half their time or less on toil and the other half on reducing toil for themselves and others.</p>
<h1 id="Error-Budgets"><a href="#Error-Budgets" class="headerlink" title="Error Budgets"></a>Error Budgets</h1><p>Next, I will talk about what an error budget is, what it is used for and how it can help provide direction for a team’s future activities. Recall that the fourth goal of DevOps is to implement gradual change. The number one source of outages is change. Whether that is adding new features, applying security patches or deploying new hardware. Any of these things can potentially impact your uptime.</p>
<p>So the question is how do I balance the proper amounts of change and stability? Extremely high stability is expensive and can drive your innovation down and prices up, maybe beyond what your customers are willing to bear. But extremely high rates of change can drive your failure rates up and your customers elsewhere.</p>
<p>So what then is the right target of reliability for your system? This is an important question, but it isn’t technical. It’s really a question for the business. You need to understand things such as how much can the service fail before it begins to have a significant negative impact? How quickly do we need to be able to release new features? And what type and how many resources are available? These answers will most likely need to come from your product team as it will require an understanding of your users behavior, your business needs, and your product roadmap.</p>
<p>Once you’ve determined the right target for reliability, you can enforce it by using an error budget. An error budget works in a similar way to a monetary budget. A certain amount of errors or downtime is allocated to each service. As long as the number of errors or downtime of the service does not exceed the error budget, then the service is considered to be reliable enough.</p>
<p>So how exactly do you spend your error budget? Well, as failures increase, the error budget is consumed. If a service goes down and it’s unresponsive that downtime is subtracted from the budget. Also, if the performance of a service drops beyond an acceptable threshold, the time it takes to restore performance is subtracted from the budget as well.</p>
<p>The longer your service is down or degraded, the more is subtracted from the error budget. And as your error budget shrinks, your team should respond by shifting resources away from adding new features and onto making more reliability improvements. This could include changing feature priority, reallocating developers to different projects or even delaying certain releases to a later time.</p>
<p>If failures continue to increase, your error budget might be in danger of being completely depleted. In this case, your team should hold all new deployments and focus completely on restoring service back to an acceptable range. At this point, you may end up with all of your S.R.S.Es and developers working a hundred percent on stability fixes and improvements.</p>
<p>Finally, once failures begin to decrease, your uptime will stabilize and your error budget will begin to replenish. It is at this point that your team can begin to shift resources back towards new development. So as you can see with an error budget, your team can commit to releasing features as quickly as a safe, where safe means staying within your budget.</p>
<p>Just like a monetary budget, picking the right amount is critical. Even small changes to an error budget can have a significant effect. Let’s say that you wanted to ensure that a service was highly available and set the error budget at 0.01%. That would mean that your service could only be down for four and a half minutes per month. Now with a budget the small, your team is going to be mostly focused on reliability, changes will be limited. One small problem could consume the entire budget. However, if you’re willing to accept a larger 1% error budget, your service can now be down for up to seven hours per month. This would allow for more frequent and riskier change.</p>
<p>Just like a monetary budget, the team is likely to spend the entire budget. So setting the budget too large could result in unnecessary downtime. Smaller budgets allow greater stability at the risk of slowing down the rollout of new features, while larger budgets allow the quicker release of features at the risk of longer and more frequent outages. No matter the size error budgets push teams towards making smaller, more gradual changes. A small deployment gone bad can be much more easily mitigated. A large deployment gone bad can exceed the budget, freeze development, and break schedules.</p>
<h1 id="Service-Level-Objectives-SLOs"><a href="#Service-Level-Objectives-SLOs" class="headerlink" title="Service Level Objectives (SLOs)"></a>Service Level Objectives (SLOs)</h1><p>The fifth and final goal of DevOps is to measure everything. In order to be successful, you need to set goals. And you need a way to measure your progress towards meeting those goals. To help with this, Site Reliability Engineering has three defined metrics: SLOs, SLIs, and SLAs. First, I will talk about SLOs or Service Level Objectives and how you can use them to define success.</p>
<p>A Service Level Objective is a goal that your business aspires to meet and intends to take action to defend. The error budget for a service is directly related to the Service Level Objective. Your error budget represents the percentage of time that your service can be down, while your SLO represents the percentage of time that your service should be up. Here is a simple formula to help explain: Your error budget plus your SLO will equal 100%. An error budget of 2% implies an SLO of 98%.</p>
<p>SLOs provide a clear signal that your service is performing successfully. There is always some small amount of error in any system. Without clearly defined limits, you won’t know if your current error rate is high enough to constitute a serious issue or not. You also won’t be able to accurately prioritize improvements.</p>
<p>Services usually have multiple SLOs associated with them. And typical SLOs include things like availability, response time, and latency. For example, you might have an SLO that requires 99% of all web server responses to be non-500 errors. Or you might have an SLO that requires 95% of your home page requests to be served in under 200 milliseconds.</p>
<p>SLOs are not intended to define ideal, best-case performance. A good rule of thumb is that your SLOs should represent the lowest level of reliability that you can get away with. You can pick all kinds of different objectives, but not every objective is useful. SLOs need to be meaningful. Meeting your SLOs should result in happy users. Missing an SLO should result in unhappy users. Failing to meet an SLO can potentially have serious consequences: damaged reputations, drops in revenue, or even a loss of customers.</p>
<p>SLOs also need to be attainable, measurable, and repeatable. Picking an objective you cannot possibly achieve or reproduce is useless and just causes needless frustration. Also, SLOs need to be understandable and controllable. You need to know how to achieve your objectives, and have the ability to make the changes necessary to do so. Correctly setting and measuring service level objectives is a key aspect of the SRE role.</p>
<p>SLOs not only assist in measuring your success, but they can also be used to create powerful feedback loops. They show you which parts of your system needs improvement, and by how much. Thus, allowing you to easily identify trouble spots and prioritize work. By tracking your current performance versus your SLOs, you will get instant feedback on any changes and will be able to know with confidence what your team should be working on next.</p>
<h1 id="Service-Level-Indicators-SLIs"><a href="#Service-Level-Indicators-SLIs" class="headerlink" title="Service Level Indicators (SLIs)"></a>Service Level Indicators (SLIs)</h1><p>An SLO by itself is not very useful. You need to compare your objectives against your current performance. This is what a service level indicator, or SLI, is used for. SLIs are the metrics of your system tracked over time. Similar to SLOs, service level indicators are reported as percentages. SLIs range from zero to 100%, where zero means nothing works and 100% means everything is working perfectly.</p>
<p>The basic formula for calculating an SLI is the total number of good events divided by the total number of events multiplied by 100. So let’s say you have an SLO that requires 95% of your homepage requests to be served in under 200 milliseconds. If your current SLI was only 94%, that would mean your service is performing below minimum expectations and that this problem needs to be fixed. If the SLI was actually 96%, then the service would be working as expected.</p>
<p>SLOs and SLIs allow you to quickly understand which services are performing well and which are experiencing problems. They also let you know how severe any detected problems are. SREs typically use monitoring tools or services to track and monitor SLIs. Just like your SLOs, your service level indicators should be focused on measuring the customer experience.</p>
<p>A good SLI should rise when customers are happy and fall when they are unhappy. If a metric can change and not significantly impact the customer experience, then it probably isn’t worth tracking via SLI. Now there are four golden signals of monitoring, latency, traffic, errors, and saturation. If you can only measure a few metrics, focus on these four.</p>
<p>Latency tells you how quickly a certain percentage of requests can be fulfilled. Traffic tells you how much demand is being placed on your system. Errors tell you the rate of requests that fail, and saturation tells you how full your service is. There are many other types of SLIs and different systems use different types. Here are some examples. Typical SLIs for serving systems include availability, quality, and latency. Typical SLIs for data processing includes coverage, correctness, freshness, and throughput. And typical SLIs for storage systems include durability, throughput, and latency.</p>
<p>By setting the right SLOs and tracking the right SLIs, you create a clear path forward for success.</p>
<h1 id="Service-Level-Agreement-SLAs"><a href="#Service-Level-Agreement-SLAs" class="headerlink" title="Service Level Agreement (SLAs)"></a>Service Level Agreement (SLAs)</h1><p>A Service Level Agreement or SLA is a guarantee you make to your customers. It is a contract with consequences of failing to meet the SLOs they contain. SLOs and SLAs are similar. However, your SLAs should not be the same as your SLOs. Both are objectives, but an SLO is an internal objective, only used within the team. If the team fails to meet the SLO, then they may slow down deployments or hold a blameless postmortem.</p>
<p>SLAs violations are shared with your customers and usually require some sort of recompensation, such as a credit or refund. Also, they should not be the same because SLOs are supposed to be stricter than SLAs. You want to be notified of any problems and have a chance to address them well before they affect your customers. Ideally, the three metrics should exist on a spectrum. You want your SLIs to be higher than both your SLOs and SLAs. This means you are meeting your objectives and your service is performing as expected. You also want your SLOs to be higher than your SLAs. If your SLI drops below your SLO, you are in violation and need to take steps to resolve the issue. If your SLI drops below your SLA, you need to notify your customers and offer them compensation. It’s better to break an internal objective than one that is visible to your customers.</p>
<p>SLAs need to be carefully set. Making them too high means you’re more likely to violate them. Making them too low means your customers may feel less confident in your ability to deliver a quality service. SLAs that are too close to SLOs mean that you’re less likely to catch a problem in time to prevent it. However, if your SLAs are too far away from your SLOs, that too can be a problem.</p>
<p>The SLO you run at tends to become the SLA everyone expects. So let’s say you offered a 95% SLA, but were consistently delivering 99.99% for a long period of time. If your SLI then dropped to 98%, you might get customer complaints. You see, even though you only promised 95%, you proved that you can actually deliver higher. And now your customers have begun to depend upon this higher level of service. To avoid this issue, Google recommends adding extra downtime to services to prevent them from being overly available. As you can probably tell, picking the right SLOs and SLAs can be tricky. You may need to start tracking SLIs for a while, and then use the average to help define realistic SLOs and SLAs. In any case, Site Reliability Engineering recommends making sure they are all a part of your system requirements. And if you already have a production system but don’t have them clearly defined, then that should be your highest priority.</p>
<h1 id="Review-and-Resources"><a href="#Review-and-Resources" class="headerlink" title="Review and Resources"></a>Review and Resources</h1><p>As we have seen, SRE and DevOps complement each other quite nicely. Although they are different, they both share the same underlying goals. DevOps, which is the broader of the two, seeks to one, reduce organizational silos. Two, accept failure as normal. Three, implement gradual changes. Four, leverage tooling and automation. And five, measure everything.</p>
<p>Site reliability engineering provides a specific implementation for achieving these same goals by one, the SRE role, which shares responsibility of production with developers. Two, blameless postmortems to learn from mistakes and to avoid a culture of fear and blame. Three, error budgets to balance growth with stability. Four, to identify and reduce toil via automation. And five, tracking SLIs against defined SLOs and SLAs.</p>
<p>Remember, an SLI is a measurement of how your system is performing, an SLO is an internal goal, and an SLA is a guarantee to customers. At this point, you should now have a basic understanding of site reliability engineering principles. If you are interested in learning more, I encourage you to check out the following resources.</p>
<p>First, we offer a whole learning path called <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/site-reliability-engineering-sre-foundation-1759/">Site Reliability Engineering Foundation</a> Learning Path. It provides more in-depth knowledge about SRE and covers a wider range of topics. This is a great resource for deeper understanding. Also, there are a number of excellent SRE resources available from google at <a target="_blank" rel="noopener" href="https://sre.google/">sre.google</a>. And if you’re planning to take the Professional Cloud DevOps Engineer Exam, I highly recommend three resources in particular. First is the “<a target="_blank" rel="noopener" href="https://sre.google/sre-book/table-of-contents/">Site Reliability Engineering</a>“ book. Second is “<a target="_blank" rel="noopener" href="https://sre.google/workbook/table-of-contents/">The Site Reliability Workbook</a>“, and finally, the <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLIivdWyY5sqJrKl7D2u-gmis8h9K66qoj">playlist</a> of short videos that helps explain core SRE concepts.</p>
<p>Well, that’s all I have for you today. Remember to give this course a rating and if you have any questions or comments, please let us know. Thanks for watching and make sure to check out our many other courses on Cloud Academy.</p>
<h1 id="4Blameless-Postmortems"><a href="#4Blameless-Postmortems" class="headerlink" title="4Blameless Postmortems"></a>4<strong>Blameless Postmortems</strong></h1><p><a target="_blank" rel="noopener" href="https://sre.google/sre-book/postmortem-culture/">Postmortem Culture: Learning from Failure</a></p>
<h1 id="10Review-and-Resources"><a href="#10Review-and-Resources" class="headerlink" title="10Review and Resources"></a>10<strong>Review and Resources</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/site-reliability-engineering-sre-foundation-1759/">Site Reliability Engineering Foundation Learning Path</a></p>
<p><a target="_blank" rel="noopener" href="https://sre.google/">Google SRE resources</a></p>
<p><a target="_blank" rel="noopener" href="https://sre.google/sre-book/table-of-contents/">Site Reliability Engineering book</a></p>
<p><a target="_blank" rel="noopener" href="https://sre.google/workbook/table-of-contents/">The Site Reliability Workbook</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLIivdWyY5sqJrKl7D2u-gmis8h9K66qoj">SRE videos playlist</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:44:33" itemprop="dateCreated datePublished" datetime="2022-11-14T12:44:33-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:40:48" itemprop="dateModified" datetime="2022-11-21T02:40:48-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to our “Introduction to Google Cloud Operations Suite”. I’m Guy Hummel, and I’ll be showing you how to monitor your GCP systems.</p>
<p>To get the most from this course, you should know the fundamentals of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, such as how to create virtual machine instances and use App Engine. If you need a refresher, then you can take our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/google-cloud-platform-fundamentals/">Google Cloud Platform: Fundamentals</a> course.</p>
<p>You should also have experience with performing operations tasks, especially working with Linux. It would also be helpful to have some programming experience, although just knowing the basics of a typical programming language should be enough.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>We’ll be going through the components of the Cloud Operations suite to monitor, log, report errors, debug, and trace your applications.</p>
<p>Now, if you’re ready to learn how to monitor your Google Cloud infrastructure, then let’s get started.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h1><p>After you’ve implemented your infrastructure in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud platform</a>, the first thing you’ll want to do is set up a monitoring system that will alert you when there are major problems. The easiest way to do this is to use Cloud Operations, Google’s powerful monitoring, logging, and debugging tool.</p>
<p>To get to it, select “Monitoring” from the console menu. The first time you bring up Monitoring in a project, it’ll take a while to set up. I’ll fast-forward.</p>
<p>Here’s where you’ll find the instructions for installing the Monitoring Agent (and the Logging Agent). You don’t need to install the agent to be able to use Monitoring, but if you do install it, you can get more information about an instance, such as about the third-party software running on it. We don’t need to install it yet, so we’ll leave that until later.</p>
<p>Suppose you want to monitor a web server and get notified if it goes down. First, you need to create an Uptime Check. </p>
<p>For the title, let’s call it “Example”. Click “Next”. Since we want to check if a web server is up, leave the Protocol as HTTP and the Resource Type as URL.</p>
<p>For the hostname, I’m going to put in the IP address of an instance I have that’s running a web server. Leave “Check Frequency” set to 1 minute. And click “Next”. We can leave the Response Validation with the defaults, so click “Next” again.</p>
<p>This is where we set up an alert for when the uptime check fails. First, we specify how long a failure has to last before it’ll trigger an alert. Let’s leave it set to 1 minute. We should also tell it how to send alert notifications. Click the dropdown menu, and then click “Manage Notification Channels”. You can be alerted by email, text message, or a variety of other options, such as Slack. We’ll get it to send an email when the web server is down. Click “Add New”, and enter your email address and your name.</p>
<p>Okay, now close this tab in your browser, and go back to the previous one. Click Refresh and select your email. Now click the “Test” button. Since the web server at that address is up, it came back right away.</p>
<p>Now I’m going to stop Apache on the instance that’s running the web server and test it again. This time, the connection failed, as expected. Click the Create button.</p>
<p>To see the results of the uptime check, go to the menu on the left and select Uptime Checks. It’ll take a while before it runs for the first time, so don’t worry if you don’t see anything in the dashboard right away. I’ll skip ahead to when the uptime check has run. OK, now you can see it’s showing that the web server’s down. After a little while, it’ll send a notification email. Here’s what it looks like.</p>
<p>Now I’ll start Apache up again and see if the alert policy sees it. I’ll just skip ahead a few minutes. Yes, it sees that the web server is up now.</p>
<p>If you want to see data graphically, then click on Dashboards. It provides default dashboards for many Google services. To create your own, click Create Dashboard. I’ll call it “Example Dashboard”.</p>
<p>Now click the “Add Chart” button. In this search field, type “URL”. There’s the resource type we need. It’s called “Uptime Check URL”. It gives us a few different metrics to choose from. The obvious one to choose is “Check passed”, but to make things more interesting, let’s choose “Request latency”. Click Save and the graph will be added to your dashboard.</p>
<p>This graph shows the network latency between each region and the web server. You can see when the web server was down, but it gives us more information than that. This network latency data from different locations around the world can be quite helpful, especially if some of your users are reporting slow performance.</p>
<p>Note that you’ll need to refresh this page to see the latest data. You can turn on auto-refresh if you want.</p>
<p>Suppose you’d like to get more information about the instance where the web server is running, such as the CPU load. Let’s create another chart.In the search field, type “cpu load”. Let’s select the 1-minute version. For the resource type, select “VM instance”. Notice that you can even monitor Amazon EC2 instances.</p>
<p>You’ll notice that the chart is blank. That’s because we need to install the Monitoring Agent to get CPU data.</p>
<p>Go back to the Overview, and then click this link to bring up the installation instructions. The instructions are different depending on which Linux distribution you’re running on your instance. I’m running Debian, so I need to follow these instructions. I’ll fast-forward to the point after I’ve run all of the install commands.</p>
<p>While we’re here, let’s install the logging agent too. You can find a link to the documentation in the GitHub repository I created for this course. The link to the GitHub repository is at the bottom of the Overview tab below this video.</p>
<p>Installing the logging agent will prepare this instance for the next lesson when we use Cloud Logging. I’ll fast-forward again.</p>
<p>Now let’s go back and see what happened to our chart. Okay, now there’s a line showing the load average, so it worked.</p>
<p>If you’ve been following along using your own account, then you should go back and delete the monitoring you set up. First, go to the Alerting page from the menu, and then delete the policy. You have to do that before it will let you delete the uptime check. Okay, now go to “Uptime Checks” and delete the one you created. Finally, go to Dashboards, click on the one you created, and delete it.</p>
<p>That’s it for this lesson.</p>
<h3 id="Lectures-1"><a href="#Lectures-1" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h1><p>Looking at real-time monitoring is great, but there will be many times when you’ll want to look at what happened in the past. In other words, you need logs.</p>
<p>For example, suppose I wanted to see when a VM instance was shut down. Compute Engine, like almost every other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a> service, writes to the Cloud Audit Logs. These logs keep track of who did what, where, and when.</p>
<p>There are three types of audit logs. Admin Activity logs track any actions that modify a resource. This includes everything from shutting down VMs to modifying permissions.</p>
<p>System Event logs track Google’s actions on Compute Engine resources. Some examples are maintenance of the underlying host and reclaiming a preemptible instance.</p>
<p>Data Access logs are pretty self-explanatory. They track data requests. Note that this also includes read requests on configurations and metadata. Since these logs can grow very quickly, they’re disabled by default. One exception is BigQuery Data Access logs, which are not only enabled by default, but it’s not even possible to disable them. Fortunately, you won’t get charged for them, though.</p>
<p>In the console, select “Logging”.</p>
<p>There are lots of options for filtering what you see here. You can look at the logs for your VM instances, firewall rules, projects, and many other components. You can even send logs from other cloud platforms like AWS to here. You just need to install the logging agent on any system that you want to get logs from.</p>
<p>This is a great way to centralize all of your logs. Not only does centralizing your logs make it easier to search for issues, but it can also help with security and compliance, because the logs aren’t easy to edit from a compromised node.</p>
<p>In this case, we need to look at the VM instance logs. You can choose a specific instance or all instances. I only have one instance right now, called instance-1. Since we installed the logging agent on instance-1 in the last lesson, there are already some log entries for it. </p>
<p>Here you can choose which logs you want from the instance, such as the Apache access and error logs. I could set it to “syslog” since that’s where the shutdown message will be, but I’ll just leave it at “All logs” because sometimes you might not know which log to look in.</p>
<p>You can also filter by log level, and for example, only look at critical entries. I’ll leave it at “Any log level”.</p>
<p>Finally, you can change how far back it will look for log entries. I’ll change it to the last 24 hours.</p>
<p>OK, now I’ll search for any entries that contain the word “shutdown” so I can see if this instance was shut down in the last 24 hours.</p>
<p>If you need to do really serious log analysis, then you can export the logs to BigQuery, which is Google’s data warehouse and analytics service. Before you can do that, you need to have the right permissions to export the logs. If you are the project owner then, of course, you have permission. If you’re not, then the “Create Sink” button will be greyed out, and you’ll have to ask a project owner to give you the Logs Configuration Writer role.</p>
<p>First, click the “Create Sink” button. A sink is a place where you want to send your data. Give your sink a name, such as “example-sink”. Under “Sink Service”, you have quite a few options, such as BigQuery, Cloud Storage, or a Custom destination. We’ll choose BigQuery. </p>
<p>Under “Sink Destination”, you have to choose a BigQuery dataset to receive the logs. If you don’t have one already, then click “Create new BigQuery dataset”. Give it a name, such as “example_dataset”. Note that I used an underscore instead of a dash because dashes are not allowed in BigQuery dataset names. Now click the “Create Sink” button.</p>
<p>It says the sink was created, so let’s jump over to BigQuery and see what’s there. Hmmm. It created our example dataset, but it doesn’t contain any tables, which means it doesn’t have any data. That’s weird, right? Well, it’s because when you set up a sink, it only starts exporting log entries that were made after the sink was created.</p>
<p>OK, then let’s generate some more log entries and see if they get exported. I’ll restart the VM, which will generate lots of log entries. Okay, I’ve restarted it. Now if we go back to the Logging page, do we see the new messages? Yes, we do.</p>
<p>Now let’s go back to BigQuery and see if the data’s there. Yes, there are two tables there now. Click on the syslog table. Now click the “Query Table” button. To do a search in BigQuery, you need to use SQL statements, so let’s write a simple one just to verify that the log entries are there.</p>
<p>Thankfully, it already gave me the skeleton of a SQL statement. I just need to fill in what I’m selecting. I’ll put in an asterisk to select everything, but I’ll restrict it by using a WHERE clause with the column name “textPayload” (which is the column that contains the text in the log entry)…”LIKE ‘%shutdown%’”. The percent signs are wildcards, so this SQL statement says to find any log entries that have the word “shutdown” in them somewhere.</p>
<p>Now we click the “Run” button…and it returns the matching log entries. If we scroll to the right, then we can see the textPayload field and it does indeed contain the word “shutdown” in each of the entries.</p>
<p>Of course, we did exactly the same search on the Logging page and it was way easier, so why would we want to go through all of this hassle of exporting to BigQuery and writing SQL statements? Well, because sometimes you may need to search through a huge number of log entries and need to do complicated queries. BigQuery is lightning fast when searching through big data, and if you build a complex infrastructure in Google Cloud Platform, then the volume of log data it will generate will easily qualify as big data.</p>
<p>Since we don’t want our example sink to keep exporting logs to BigQuery and incurring storage charges, let’s delete what we’ve created. On the Logging page, click on “Logs Router” in the left-hand menu, then select the sink and delete it.</p>
<p>We should also delete the BigQuery dataset, so go back to the BigQuery page, select the dataset, and click “Delete Dataset”. It wants you to be sure that you actually want to delete the dataset, so you have to type the dataset name before it will delete it.</p>
<p>One concern that you or your company may have is how to ensure the integrity of your logs. Many hackers try to cover their tracks by modifying or deleting log entries. There are a number of steps you can take to make it more difficult to do that.</p>
<p>First, apply the principle of least privilege. That is, give users the lowest level of privilege they need to perform their tasks. In this case, only give the owner role for projects and log buckets to people who absolutely need it.</p>
<p>Second, track changes by implementing object versioning on the log buckets. The Cloud Storage service automatically encrypts all data before it is written to the log buckets, but you can increase security by forcing a new version to be saved whenever an object in a log bucket is changed. Unfortunately, this won’t prevent an owner from deleting an incriminating object, which is why you need to keep tight control on which users are given the owner role.</p>
<p>Third, you could add more protection by requiring two people to inspect the logs. You could copy the logs to another project with a different owner using either a cron job or the Cloud Storage Transfer Service. Of course, this still won’t prevent an owner in the first project from deleting the original bucket before the copy occurs or from disabling the original logging.</p>
<p>So the bottom line is that a person with the owner role can get around just about anything you put in place, but you can make it nearly impossible for someone without the owner role to change the logs without you knowing about it.</p>
<p>And that’s it for this lesson.</p>
<h3 id="Lectures-2"><a href="#Lectures-2" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Error-Reporting-and-Debugging"><a href="#Error-Reporting-and-Debugging" class="headerlink" title="Error Reporting and Debugging"></a>Error Reporting and Debugging</h1><p>So far, we’ve been looking at alerts and log messages from system software. Now it’s time to look at how to get error information from your applications. That’s where the Cloud Error Reporting service comes in. To show you how this works, I’m going to install Google’s Hello World application in App Engine and then get it to generate an error.</p>
<p>Normally, you’d use your own workstation for the development environment, but to simplify this demo, I’m going to use Cloud Shell. The nice thing about Cloud Shell is that it already has all of the packages installed that you need. When you do want to write and test Java code on your own workstation, remember that you need to install the Google Cloud SDK, the Java SE 11 Development Kit, Git, and Maven 3.5 or greater on your system.</p>
<p>First, open Cloud Shell. Next, get a copy of the Hello World application with this “git clone” command. Then go into the directory where the app is.</p>
<p>Now use the local development server to make sure the app works. To see if it’s working, click the “Web Preview” icon, and select “Preview on port 8080”. You should see a “Hello world!” message. OK, it’s working, so let’s stop the development server and upload the application to App Engine. You can stop the development server with a Ctrl-C.</p>
<p>Now use the “gcloud app deploy” command to upload it to App Engine. If you’re doing this yourself, then it may look slightly different than mine because I’ve already configured App Engine. If this is your first time deploying to App Engine, then it will likely ask you to choose a region. OK, it’s done deploying.</p>
<p>There are a couple of ways to test it. If you’re not using Cloud Shell, then you could do a “gcloud app browse”, which is pretty handy. Since we are using Cloud Shell, we’ll have to go to this URL. There’s “Hello World!” again.</p>
<p>Now, in order to see an error on the Error Reporting page, we need to generate an error. Let’s edit the code and mess something up. I’m going to add a line that I know will cause a problem. This’ll throw an exception because you can’t divide a number by zero.</p>
<p>Now run “gcloud app deploy” again. When it’s done, bring the app up in your browser again. This time it gives you a big error message, which is actually what we want, for once.</p>
<p>Let’s see if Error Reporting picked it up. The <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud</a> Console shows errors on the main dashboard, so you don’t have to go to the Error Reporting page to see them. I’ll refresh the page. There it is. Click on “Go to Error Reporting” to see what it shows. If you click on the error, you’ll see more details, including the stack trace. You can even click on the line where the error occurred and it will take you into your source code in the Debugger. However, in this case, the line it’s showing at the top of the stack trace is not in our code. We can click on this line, though, which is in our code, and it should take us there.</p>
<p>The Debugger is a great tool that you can use whether an error occurred or not. Let’s put a more subtle problem in the code and see how we can use the Debugger to figure out what’s wrong.</p>
<p>Suppose we want to check the operating system running our app, and if it’s Ubuntu, then we’ll print “Ubuntu rocks!”</p>
<p>First, we have to fix the bug that we introduced previously, so we’ll remove that line. We have to go back to the editor to do that. Now I’ll add the new code. Even if you’re not familiar with Java, this is pretty straightforward. It gets the name of the operating system, then it checks to see whether it’s equal to Ubuntu or not, and if it is, it says, “Ubuntu rocks!”, and if it isn’t, it says, “Hello world!”.</p>
<p>Now we’ll upload it to App Engine again. OK, now we’ll refresh the webpage. And it says “Hello world!” again, not “Ubuntu rocks!” That might be because the underlying operating system isn’t Ubuntu, but let’s go back to the Debugger and see if that’s the reason.</p>
<p>You’ll notice that this is still the old version of the file. First, refresh the browser. It’s still showing the old version. To get to the new version, you have to click on this drop-down menu and select the right one. The latest version should say 100% at the end. Sometimes you have to tell it where the source code is. Find “App Engine” in the list, and click the “Select source” button.</p>
<p>Now find the file. You’ll see some text on the right-hand side that says to click a line number to take a snapshot of the variables and call stack. It also points out that taking a snapshot does not stop the running application, which is good to know.</p>
<p>Click in the left-hand gutter on the line just after the “osname” variable is set. Now that the snapshot point is set, we can refresh the webpage and trigger the snapshot. If we go back to the Debugger tab, you’ll see that it’s showing the variables and call stack on the right-hand side. There’s “osname”. It’s set to “Linux”, not anything more specific. I guess it doesn’t know the specific distribution of Linux that’s running, so let’s change our code to check for Linux instead.</p>
<p>And deploy the new version. Now refresh the webpage. It worked!</p>
<p>Let’s go back to the main Error Reporting page and I’ll show you a couple of other things. First, if you’re sitting on this page watching for errors in real-time, then you should click the “AUTO RELOAD” button, which will refresh the page every 5 seconds. If you don’t want to hang around here and just want to get an email when an error occurs, then click the “Turn on notifications” button.</p>
<p>Alright, that’s it for this lesson.</p>
<h3 id="Lectures-3"><a href="#Lectures-3" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Tracing"><a href="#Tracing" class="headerlink" title="Tracing"></a>Tracing</h1><p>In the last lesson, we looked at how to debug errors in your application, but what do you do if your application is working properly but performing too slowly? That’s what Cloud Profiler and Cloud Trace are used for. In this lesson, I’m only going to talk about Cloud Trace.</p>
<p>The Trace List is probably where you will spend most of your time. It shows you all of the traces over a specific period of time in this cool graph. It is set to “1 hour” right now, but we can change that to give a longer view. Each one of these dots is a trace of an individual request to the application. If you click on one of the dots, it brings up two more panes underneath. The Waterfall View shows what happened during the request. The first bar shows the total end-to-end time, which was 215 milliseconds in this case. The bars underneath show the time it took to complete calls performed when handling the request. In this case, we have one bar for an HTTP GET request.</p>
<p>Of course, this timeline would be a lot more useful if we were running a more complex application with multiple calls so you could see which ones were taking the most time. Each of those calls would have a bar on this chart. The Hello World application is about the simplest application possible, so you’ll just have to use your imagination here.</p>
<p>Analysis reports show you the latency distribution for your application and also attempt to identify performance bottlenecks, which is a great feature. You have to have at least 100 traces before you can run a report, though.</p>
<p>If you’re running your applications in App Engine, then it’ll automatically capture and submit traces, but if you want to trace code that’s running outside of App Engine, then you’ll have to either add instrumentation code to your applications using the Trace SDK or submit traces through the API.</p>
<p>Before we go, you might want to delete your application, so it doesn’t incur charges. Go to App Engine and then go to Settings. Click “Disable application”. It will ask you to type the app’s ID before you can click “DISABLE”. This doesn’t delete the application, but it does stop it from serving requests. To start the application up again, you can just click “Enable application”.</p>
<p>If you want to permanently delete the application, then you’ll have to delete the project it’s associated with, which you can do in the “IAM &amp; Admin” page. Be aware that if you delete a project, you will never be able to use that project ID again. That is, you won’t be able to create a new project with the same ID.</p>
<p>That’s it for this lesson.</p>
<h3 id="Lectures-4"><a href="#Lectures-4" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I hope you enjoyed learning about the Cloud Operations Suite. Let’s do a quick review of what you learned.</p>
<p>You can use the Cloud Operations suite (formerly known as Stackdriver) for monitoring, logging, error reporting, debugging, profiling, and tracing your applications.</p>
<p>You don’t need to install the Monitoring Agent to be able to use Monitoring, but if you do install it, you can get more information about an instance, such as about the third-party software running on it.</p>
<p>To monitor a server, first you need to create an Uptime Check. Then you need to set up an Alert Policy. To see data graphically, create a Dashboard. </p>
<p>Cloud Audit Logs keep track of who did what, where, and when. There are three types of audit logs. Admin Activity logs track any actions that modify a resource. This includes everything from shutting down VMs to modifying permissions. System Event logs track Google’s actions on Compute Engine resources. Some examples are maintenance of the underlying host and reclaiming a preemptible instance. Data Access logs track data requests, including read requests on configurations and metadata.</p>
<p>Cloud Logging can even collect logs from other cloud platforms like AWS. You just need to install the Logging Agent on any system that you want to get logs from.</p>
<p>If you need to do serious log analysis, then you can export the logs to BigQuery. In this case, BigQuery acts as a sink for log data. To do a search in BigQuery, you need to use SQL statements.</p>
<p>The Error Reporting service alerts you to errors in your applications. You can use the Debugger to figure out what’s causing an error.</p>
<p>Cloud Profiler and Cloud Trace are used to analyze performance problems in applications. A trace is an individual request to an application. Cloud Trace shows you how much time was taken by each of the calls generated by an application request.</p>
<p>If you’re running your applications in App Engine, then it’ll automatically capture and submit traces. If you want to trace code that’s running outside of App Engine, then you’ll have to add instrumentation code to your applications. The recommended way is to use OpenTelemetry and the associated Cloud Trace client library.</p>
<p>To learn more about the Cloud Operations suite, you can read Google’s documentation. Also watch for new <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a> courses on Cloud Academy, because we’re always publishing new courses. If you have any questions or comments, please let us know in the Cloud Academy Community or send an email to <a href="mailto:&#x73;&#117;&#112;&#112;&#x6f;&#x72;&#116;&#64;&#x63;&#x6c;&#111;&#117;&#100;&#x61;&#99;&#97;&#x64;&#x65;&#x6d;&#x79;&#x2e;&#x63;&#x6f;&#109;">&#x73;&#117;&#112;&#112;&#x6f;&#x72;&#116;&#64;&#x63;&#x6c;&#111;&#117;&#100;&#x61;&#99;&#97;&#x64;&#x65;&#x6d;&#x79;&#x2e;&#x63;&#x6f;&#109;</a>. Before you go, please give this course a rating. Thanks!</p>
<h3 id="Lectures-5"><a href="#Lectures-5" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:43:43" itemprop="dateCreated datePublished" datetime="2022-11-14T12:43:43-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:42:54" itemprop="dateModified" datetime="2022-11-21T02:42:54-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:38:04" itemprop="dateCreated datePublished" datetime="2022-11-14T12:38:04-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:42:10" itemprop="dateModified" datetime="2022-11-21T02:42:10-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p><object data="Knowledge-Check-Designing-a-Google-Cloud-Infrastructure.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:36:54" itemprop="dateCreated datePublished" datetime="2022-11-14T12:36:54-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:38:34" itemprop="dateModified" datetime="2022-11-21T02:38:34-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to designing a Google Cloud infrastructure. I’m Guy Hummel, and I’ll be showing you how to build an enterprise IT solution in Google platform.</p>
<p>To get the most from this course, unless you already have a lot of experience using Google Cloud, you should take the Google Cloud Platform Fundamentals and Systems Operations courses to get a solid understanding of the different components of Google Cloud. In this course, I’ll be showing you how to use these building blocks to construct an enterprise class application architecture.</p>
<p>We’re going to use a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/case-study-1/">case study</a> as an example of how to apply enterprise principles to a design. I’ll start by explaining how you would take an organization’s requirements, and translate them into the appropriate <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/compute-1/">compute</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/storage-2/">storage</a>, and network components in Google Cloud. I’ll also show you how to make it a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> design.</p>
<p>Then I’ll cover how to secure the environment, including how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/authentication-2/">authenticate</a> and give permissions to people as well as to applications using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/service-accounts-1/">service accounts</a>, how to encrypt your data, and how to comply with a rigorous security standard like PCI DSS.</p>
<p>Finally, we’ll wrap up with how to design a solution that can recover from <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disasters</a>.</p>
<p>All right, if you’re ready to learn how to create an enterprise class architecture for your Google Cloud infrastructure, then let’s get started.</p>
<h1 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h1><p>Suppose you’ve been hired to help a company called Great Inside, which offers interior design software as a service.</p>
<p>Great Inside makes its money by selling subscriptions to its web-based interior design application. It also has a free version that’s supported by advertising. Their customers are primarily in North America, but they hope to expand in Europe and Asia at some point in the future.</p>
<p>The company has grown slowly for five years, but recently closed a venture capital round, brought in experienced executives, and is now growing more quickly. The company’s existing infrastructure is not capable of scaling up quickly enough, so they would like to move to the cloud.</p>
<p>Great Inside started off with a Microsoft-centric infrastructure and then migrated to a LAMP stack. The only Microsoft infrastructure left is the payment processing system and an Active Directory server. They would like to retire their Microsoft servers in the future, other than Active Directory. But that isn’t a priority right now, and the company would like to move both types of servers to the cloud. They’ve also started a pilot project using a NoSQL database.</p>
<p>Since they accept credit cards, they need to be PCI DSS compliant. Since their volume is increasing, they need to ensure that their payment processing environment meets a higher level of compliance. Note that Great Inside passes the validation and processing of credit card information to a certified payment processor.</p>
<p>They would like to improve their disaster recovery solution. At the moment, they’re backing their data up to a cloud service, but it would take them a long time to recover from a disaster.</p>
<p>Their existing technical environment is all in a single data center.</p>
<p>They have three types of databases. MySQL for the interior design application, Microsoft SQL Server for payment processing, and a NoSQL database in the development environment.</p>
<p>They have two types of web and application servers. Apache and Tomcat are running on six servers, each with 2 dual-core CPUs, 24GB of RAM, and two mirrored 200GB disks. These servers are for their interior design application. IIS is running on four servers- two customer-facing and two internal, each with a dual-core CPU, 16GB of RAM and two mirrored 250GB disks. These servers are for payment processing.</p>
<p>They have a variety of infrastructure servers, including Active Directory and a file server for internal documents, etc.</p>
<p>Here are their business requirements. Scale easily to handle rapid growth, move as much of the development, test, and production infrastructure as possible to the cloud, and increase performance, reliability, and security while reducing management overhead.</p>
<p>And their technical requirements are: connect the data center’s network with the cloud environment’s network, encrypt all data, design <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> into all tiers, and create a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disaster recovery</a> solution that will reduce recovery time to a few hours, rather than a day.</p>
<p>I should mention up front that some aspects of this case study may not be completely realistic. It’s simplified so we can go through it in a reasonable amount of time, but it has just enough complexity to allow us to cover the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">key topics</a>.</p>
<h1 id="Compute"><a href="#Compute" class="headerlink" title="Compute"></a>Compute</h1><p>Although most Google Cloud designs include virtual machine instances, that doesn’t mean VMs are your only option for compute resources. Before you start designing a solution using only Compute Engine instances, you should consider App Engine and Kubernetes Engine.</p>
<p>App Engine is designed for people who don’t want to manage an application’s underlying infrastructure. App Engine provisions and scales all of the resources your application needs behind the scenes, without any human intervention required. That sounds great, doesn’t it? So why wouldn’t you use App Engine?</p>
<p>The main reason is that it is much easier to develop a new application on App Engine than it is to migrate an existing one to it. So if you’re developing an application from scratch, then App Engine may be a good choice. If you have an existing application, then you’ll need to check if App Engine supports the programming languages your app is written in and if your app has any operating system dependencies (such as only being able to run on Windows, which isn’t supported by App Engine).</p>
<p>You’ll also need to look at your application’s architecture to see if it would be able to run on App Engine without having to re-architect it. App Engine is designed for microservices-based apps, so if your existing application has a monolithic architecture, then it might require some work to migrate it.</p>
<p>For all of these reasons, it’s usually advisable to use App Engine only for new applications rather than existing ones.</p>
<p>The next option is Kubernetes Engine. It provides many of the benefits of App Engine, in that you don’t have to worry about the underlying operating system running your application. It also handles scaling, although you have to configure that yourself first. It does require more management than App Engine, but it doesn’t require as much management as Compute Engine.</p>
<p>The ideal case for using Kubernetes Engine is, of course, if your application already runs in containers, especially Docker containers, since that’s what Kubernetes Engine supports. On the other hand, if your application will only run on certain operating systems, especially Windows, then it won’t run in Kubernetes Engine.</p>
<p>If you have an existing app that does not currently run in containers, then you might want to see if it’s possible to containerize it so you can take advantage of Kubernetes Engine.</p>
<p>If your existing application runs on virtual machines, then the easiest way to migrate it to Google Cloud is to use Compute Engine instances. If it doesn’t run on virtual machines, then you’ll have to virtualize it before you can run it on Google Cloud.</p>
<p>Although Compute Engine requires more management than App Engine or Kubernetes Engine, it does give you ultimate flexibility. For example, you could run an application that requires Windows, a specific network driver, and high-performance GPUs.</p>
<p>Since our case study involves an existing application that doesn’t currently run in containers, we’re going to choose Compute Engine for our design.</p>
<p>The case study company, GreatInside, currently has 6 machines running Apache and Tomcat, and 4 machines running IIS. Let’s have a look at Google’s predefined machine types . We need to decide how many vCPUs and how much memory to use. Memory is pretty straightforward. Our existing machines have 24GB for the Tomcat servers and 16GB for the IIS servers. VCPUs are more complicated, though.</p>
<p>The existing Tomcat servers have two dual-core CPUs and the IIS servers have one dual-core CPU. How does that translate into vCPUs? Some people say that cores and vCPUs are equivalent, but that’s not quite true. A vCPU on a Compute Engine instance is implemented as a single hyper-thread on an Intel Xeon processor. Since each Xeon processor has 2 hyperthreads, that means you need to multiply the number of cores by 2 to get the number of threads, and thus the number of vCPUs.</p>
<p>So our Tomcat servers have the equivalent of 8 vCPUs (4 cores times 2) and our IIS servers have the equivalent of 4 vCPUs (2 cores times 2). Of course, if we really wanted to be accurate, we’d need to take into account things like the clock speed of the CPUs, but we’re not going to go that far.</p>
<p>So, we need 8 vCPUs and 24GB of RAM for the Tomcat servers and 4 vCPUs and 16GB of RAM for the IIS servers. Do any of the predefined machine types match these requirements? Well, the n1-standard-4 is almost identical to the IIS server requirements. It has 4 vCPUs and 15GB of RAM. Having one less gig of RAM is probably fine, but you can monitor it in production to make sure it’s sufficient.</p>
<p>The Tomcat servers are another story, though. The closest match is the n1-standard-8, which has 8 vCPUs and 30GB of memory. That’s 6GB more than we need, so we should consider a custom machine type. We can select the exact size we need. With this custom configuration, it says it will cost $190.54 per month. Let’s see how that compares to the n1-standard-8. That costs $194.58 per month, which is more expensive, but only 2% more.</p>
<p>I should mention that there are a couple of ways to reduce those costs: sustained-use discounts and committed-use discounts. If you know that you’re going to be running an instance continuously for a long period of time, then you can pay much less by purchasing either a one-year or three-year contract, which is called a committed-use contract. This will typically reduce the cost by up to 57%. However, that’s a pretty big commitment, so Google provides a way to reduce costs without signing a long-term contract. You start getting an automatic discount after an instance runs for more than 25% of a month, and the discount increases the longer the instance runs during that month. For most machine types, you’ll receive a sustained-use discount of 30% if you run the instance for the entire month.</p>
<p>Okay, let’s get back to our case study. Since IIS and SQL Server run on Windows, we’ll need to figure out how to license them. Let’s start with IIS. For Windows Server itself, you can either use Google’s pay-as-you-go Windows licensing or you can bring your own license. </p>
<p>There are two ways to use Google’s pay-as-you-go Windows licensing. The first way is to create a new instance with one of the pre-configured Windows Server boot disks . The second way is to import a Windows VM. There are two options for importing a VM. The first option is to import a virtual disk and turn it into an image that you can use to create a Compute Engine instance. That’s quite simple to do, but it’s not meant for migrating mission-critical applications or migrating a large number of VMs in an automated fashion.</p>
<p>A more sophisticated option is to use Cloud Migrate for Compute Engine. This service makes replicas of existing VMs you have running on-premises or on another cloud platform. It will take care of the many steps that are needed to migrate important applications. </p>
<p>If you want to bring your own Windows licenses, then you can run your Windows VMs on sole-tenant nodes, which are dedicated physical servers that are not shared with other customers.</p>
<p>If you need to run any Microsoft applications, then you’ll need licenses for those too, of course, but Microsoft is more flexible with its application licensing than with Windows licensing. If your organization has active Software Assurance contracts for its Microsoft applications, then you can move those licenses to either Compute Engine instances or sole-tenant nodes.</p>
<p>Now let’s move on to SQL Server. You can use any of the options I just mentioned, but fortunately, there are also easier options for SQL Server. One option is to create instances with pre-configured SQL Server boot disks . These include pay-as-you-go licenses for both Windows Server and SQL Server. The second option is to use Cloud SQL, which is a managed service. I’ll tell you more about it in the next video.</p>
<p>For premium Linux OSs (such as Red Hat or SUSE), licensing is much simpler. You can either create an instance with a pre-configured boot disk or you can import your Linux VM. In both cases, you can either use a Google pay-as-you-go license or bring your own license.</p>
<p>I should mention one other Compute Engine option – preemptible VMs. They’re up to 80% cheaper than regular instances, but since Google can remove them with only 30 seconds’ notice, you would usually only use them as disposable instances for things like big data batch jobs. That doesn’t fit our use case, so we’ll stick with regular instances.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><p>Each of the instances for the Tomcat and IIS servers will come with a standard persistent boot disk by default, but we might need something different. There are many options for instance storage, including Standard Persistent Disk, SSD Persistent Disk, Local SSD, RAM Disk, and Cloud Storage.</p>
<p>Standard Persistent Disks are magnetic drives. Their main advantage is low cost. SSD Persistent Disks (or solid state disks) have up to 4 times the throughput and up to 40 times the I&#x2F;O operations per second of a Standard Persistent Disk, so if you need high performance, SSDs are a must.</p>
<p>But SSD Persistent Disks aren’t even your fastest option. Local SSDs are up to 600 times as fast as Standard Persistent Disks in IOPS and up to 15 times as fast in throughput.</p>
<p>Why are Local SSDs so much faster than SSD Persistent Disks, which are obviously both using SSD technology? Well, it’s because Local SSDs are not redundant and are directly attached to an instance. That gives them major speed advantages, but with high risk because if they suffer a hardware failure, then your data will be gone. Furthermore, Local SSDs disappear when you stop or delete an instance, so you should only use them for temporary data that you can afford to lose, such as a cache.</p>
<p>There are a couple more disadvantages of Local SSDs too. First, they are only available in one size – 375GB, which is kind of an awkward number. Second, they can’t be used as boot disks.</p>
<p>If you need even faster storage, then you can use a RAM disk, which essentially makes a chunk of memory look like a filesystem. Although RAM disks are the fastest option, they’re even less durable than Local SSDs, so they’re only suitable for temporary data. It’s also an expensive option because RAM is much more expensive than SSDs.</p>
<p>One more option is Cloud Storage. This is kind of a weird way to add storage to an instance because a bucket is object storage rather than block storage. That means it can’t be used as a root disk and it may be unreliable as a mounted filesystem. So why would you ever use it? The first advantage of using Cloud Storage is that multiple instances can write to a bucket at the same time. You can’t do that with persistent disks, which can only be shared between instances in read-only mode. The danger is that one instance could overwrite changes made by another instance, so your application would have to take that into account.</p>
<p>The second advantage is that an instance can access a bucket in a different zone or region, which is great for sharing data globally, especially if it’s read-only data, which would avoid the overwriting problem.</p>
<p>However, Cloud Storage usually isn’t a good option for instance storage. It is good for general-purpose file serving, though, so it would be a potential choice for replacing GreatInside’s internal file server if they want to move it to the cloud. To do this, you’d need to use Cloud Storage FUSE, which is open source software that translates object storage names into a file and directory system. Essentially, it makes Cloud Storage buckets look like network file systems. A better choice, though, would be Cloud Filestore, which is a fully-supported file sharing service that’s designed specifically for this purpose. It’s compatible with NFS version 3.</p>
<p>So, which instance storage option should we use for our instances? Since performance is important, we should use something faster than Standard Persistent Disks. SSD Persistent Disks are many times faster than standard ones, so they’d be a good choice. Should we consider Local SSDs or RAM disks? Well, neither of those can be boot disks, so we would have to use them in addition to a persistent boot disk. The higher performance wouldn’t outweigh the extra cost and complexity of using one of these options, though, so we should just stick with SSD Persistent Disks. Furthermore, since persistent disks are redundant, we don’t need to have two mirrored disks on each instance like GreatInside does in its existing data center. We can just have a single persistent boot disk on each instance.</p>
<p>As for the size, we can specify the exact amount we need, so for the Tomcat servers, we should use one 200GB disk on each instance, and for the IIS servers, we should use one 250GB disk on each.</p>
<p>Next, we need to look at our database options. Google Cloud has 5 different database services: Cloud SQL, Cloud Datastore, Bigtable, BigQuery, and Cloud Spanner.</p>
<p>Cloud SQL is a relational database. It’s a managed service for MySQL, PostgreSQL, or Microsoft SQL Server. It’s suitable for everything from blogs to ERP and CRM to ecommerce.</p>
<p>Cloud Datastore is a NoSQL database service. Unlike a relational database, such as Cloud SQL, it is horizontally scalable. A relational database can scale vertically, meaning that you can run it on a more powerful VM to handle more transactions, but there are obviously limits to the size of a VM. You can also scale a relational database horizontally for reads by using read replicas, but most relational databases can’t scale horizontally for writes. That is a major problem that is solved by NoSQL databases.</p>
<p>Because of this and because it’s an eventually consistent database, Cloud Datastore is faster than Cloud SQL. It’s best suited to relatively simple data and queries, especially key-value pairs. Typical examples include user profiles, product catalogs, and game state. For complex queries, Cloud SQL is a better choice.</p>
<p>Cloud Bigtable is also a NoSQL database. It’s designed to scale into the petabyte range with high throughput and low latency. It does not support ACID transactions, so it shouldn’t be used for transaction processing. It’s best suited for storing huge amounts of single-keyed data. If you have less than one terabyte of data, then Bigtable is not the best solution. It can handle big data in real-time or in batch processing. Typical examples are Internet of Things applications and product recommendations.</p>
<p>BigQuery also handles huge amounts of data, but it’s more of a data warehouse. It’s something you use after data is collected, rather than being a transactional system. It’s best suited to aggregating data from many sources and letting you search it using SQL queries. In other words, it’s good for OLAP (that is, Online Analytical Processing) and business intelligence reporting.</p>
<p>Google’s newest database service is Cloud Spanner, which seems to combine the best of all worlds. It’s a relational database that also scales horizontally. That is, it combines the best features of traditional databases like Cloud SQL and the best features of NoSQL databases like Cloud Datastore. So why wouldn’t you use it for all of your database needs? Well, mostly because it’s more expensive than the other options. Also, if your application is written specifically for a particular database, such as MySQL, then Cloud SQL would be a better choice, unless you can rewrite it to work with Cloud Spanner. </p>
<p>So use Cloud Spanner when you need a relational database that is massively scalable. Typical uses are financial services and global supply chain applications.</p>
<p>Now, which database services should GreatInside use? It currently has two production databases – MySQL for the interior design application and SQL Server for payment processing. There are two ways you could migrate the MySQL database to Google Cloud. You could use Cloud SQL or run MySQL on a regular instance. Considering that GreatInside wants to reduce system management tasks, Cloud SQL would be the best choice since it’s a fully managed MySQL service, with automatic replication and backups.</p>
<p>For SQL Server, you have the same two options. You could use Cloud SQL, or you could run it on a regular instance. Again, Cloud SQL is the best choice.</p>
<p>GreatInside does have one more database – their experimental NoSQL datastore. Since the development team is still evaluating this technology, you should talk to them about trying Cloud Datastore. They should also try App Engine because Cloud Datastore works best when used with App Engine.</p>
<p>And that’s it for storage and databases.</p>
<h1 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h1><p>Before we talk about networks, we need to talk about how we can make our applications highly available.</p>
<p>If you have an application that’s running on only one VM instance, then, of course, it’s a single point of failure, and if it goes down, your application goes down. So, at a minimum, you should always have at least two VMs for every component of your solution. But where should those instances be located?</p>
<p>When you create a VM instance, it gets created in a particular zone, such as us-central1-a. A zone is an isolated location. You can think of a zone as a data center or an isolated portion of a data center.</p>
<p>If you put both instances in the same zone, then both of them could potentially go down if there’s a problem in that zone. So, you should put the instances in different zones. For performance reasons, you may need to put them in zones that are in the same region, such as us-central1. Notice that the zone name is just the region name with a dash and a letter at the end. All of the zones in a region have high-bandwidth, low-latency network connections between them, so if instances that are spread across a region need to mirror data with each other, then they can do this quickly.</p>
<p>Although “region” sounds like a geographic area, it’s just a data center campus in one location. For example, all of the zones in the us-central1 region are in Council Bluffs, Iowa. So, for maximum availability, you may also want to distribute your instances across different regions.</p>
<p>For a higher level of availability, you can use autoscaling instance groups. This was covered extensively in the “Google Cloud Platform: Systems Operations” course, so I’ll just go over the highlights.</p>
<p>An instance group consists of identical instances that perform processing for your application. If one of the instances fails, then a health check will notice this and replace the instance with a new one. If the load on the instance group gets too high, then the autoscaler will add more instances to maintain good application performance.</p>
<p>To ensure availability even if an entire zone fails, you should distribute the instances across multiple zones. Luckily, this is very easy to do. You just have to select “Multizone” when you’re creating the instance group.</p>
<p>If you want to make sure you’ll still have enough instances to handle the load if an entire zone goes down, then you should overprovision by 50%. For example, if your instances are spread across 3 zones and you need 6 instances to handle your normal traffic load, then you should provision 9 instances. That way if one of the zones goes down (which would take out 3 of the instances), you’ll still have 6 instances left in the two remaining zones.</p>
<p>You can either overprovision by 50% at all times or you could save money by just setting the upper limit on your autoscaler to at least 50% more than the normal number of instances. If you decide to depend on the autoscaler during a zone failure, then the instances in your remaining two zones will be very heavily loaded until the autoscaler provisions additional instances, so only choose this option if you can tolerate this temporary performance degradation.</p>
<p>Since GreatInside has 6 web tier instances for its main application, this is how it should be set up. For the 2 customer-facing IIS instances in the payment processing system, you’d set an upper limit of 3 instances, which is 50% more than the 2 instances that it normally needs.</p>
<p>To make the instance group work as a high availability solution, you’ll need a couple of other components. First, the instance group has to be behind a load balancer that will distribute incoming requests to different instances. Second, the instances cannot have any stateful data. Otherwise, the same instance would have to handle all requests from a given user. Although you can enable the “session affinity” option in this situation, it will ruin your high availability since a failed instance will impact all of the users on it.</p>
<p>Since most applications do have stateful data, you have to put it on other components, such as a database or Cloud Storage. Unfortunately, that just moves the availability issue to a different layer, but fortunately, Google Cloud has good ways to handle storage availability.</p>
<p>If Cloud Storage is sufficient for your stateful data needs, then you’re covered because Cloud Storage is automatically replicated either across zones in a region (for the Regional type) or across regions (for the Multi-Region type).</p>
<p>If you need a database for your stateful data, then there are different availability solutions depending on the data service.</p>
<p>With Cloud SQL, you can simply check the “High availability” box when you create a Cloud SQL instance. This will create a failover replica in another zone. In the event of a failure, Cloud SQL will automatically fail over to the replica. This option is available for MySQL, PostgreSQL, and SQL Server.</p>
<p>Since Cloud Datastore is a NoSQL database, it scales horizontally, which makes high availability easier than with Cloud SQL. Cloud Datastore automatically replicates data across zones in a region. When you create a Datastore instance, you specify which region and it does the rest.</p>
<p>Bigtable is also a NoSQL database that scales horizontally, but if you want it to replicate across multiple zones, then you’ll have to configure it to do that. You can even configure it to support replication across regions if you need that. But in its simplest configuration, it only stores data in a single zone, which gives it higher performance. It’s still stored redundantly in that configuration but within the same zone.</p>
<p>BigQuery automatically replicates data within a region, but it’s a data warehouse, so it’s not suitable for real-time stateful data storage.</p>
<p>Cloud Spanner also automatically replicates data within a region, so it’s highly available out of the box, and unlike Cloud SQL, it doesn’t need a failover replica, which is a less available solution.</p>
<p>In summary, if a NoSQL database is sufficient for your application, then Cloud Datastore is your best choice for storing stateful data. If you need to use a relational database, then either use Cloud SQL and enable high availability or use Cloud Spanner for even higher availability if you’re willing to pay a higher price.</p>
<p>Since GreatInside is going to use Cloud SQL for both MySQL and SQL Server, then we just need to enable the high availability option when we create those databases.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Networks"><a href="#Networks" class="headerlink" title="Networks"></a>Networks</h1><p>Now we know all of the components we want to use and we just need to connect them together with networks. Google provides what are called Virtual Private Clouds, or VPCs, but I’m just going to call them networks.</p>
<p>There are 5 layers in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">Google Cloud</a> you can use to isolate and manage resources: organizations, folders, projects, networks, and subnetworks.</p>
<p>You aren’t required to have organizations or folders, but they can be useful, especially for large companies.</p>
<p>Projects are required, though. You use them to provide a level of separation between resources. Not only are resources in different projects unable to communicate with each other, but they’re even in different billing accounts. Projects also have separate security controls, so for example, you could give Bob in QA the highest level of access in the Test Environment project, but a lower level of access in the Production Environment project.</p>
<p>Each project has one default network that comes with preset configurations and firewall rules to make it easier to get started, but you can customize it, or you can create up to 4 additional networks (for a total of 5). If 5 networks per project isn’t enough for you, then you can request a quota increase to support up to 15 networks in each project.</p>
<p>A network belongs to only one project, a subnet belongs to only one network, and an instance belongs to only one subnet.</p>
<p>Instances in the same subnet or even different subnets within the same network can communicate with each other. Subnetworks are used to group and manage resources.</p>
<p>A network spans all regions, but each subnet can only be in one region. A subnet allows you to define an IP address range and a default gateway for the instances you put in it. The IP address ranges of the different subnets must be non-public (such as 10.0.0.0) and must not overlap, but other than that, there are no restrictions on them. For example, they can be different sizes. They must be IPv4 addresses, though, because Compute Engine doesn’t support IPv6 yet.</p>
<p>A network can have either automatic or custom subnets. With automatic, the subnets are created for you, one in each region. With custom, you create them yourself. If you discover that you need to customize a network with automatic subnets, then you can convert it to custom mode, but once you do, you cannot convert it back to automatic mode.</p>
<p>On the default network, instances within the same subnet can communicate with each other over any TCP or UDP port, as well as with ICMP. Instances in the same network can communicate with each other, regardless of which subnets they’re in, because Google Cloud creates routes between all of the subnets in a network. However, the default network’s firewall rules only allow ssh, rdp, and icmp traffic between subnets.</p>
<p>If you don’t want instances in different subnets to be able to reach each other, then you can change the firewall rules to deny traffic between them.</p>
<p>Note that only the default network comes with predefined firewall rules. When you create a new network, it doesn’t have any firewall rules. However, the instances in that network will still be able to communicate with the Internet, assuming they have external IP addresses, because all outgoing traffic from instances is allowed. Only incoming traffic is blocked. And when an instance sends a request over the Internet, the incoming response is allowed, so two-way traffic is enabled at that point.</p>
<p>Each network includes a local DNS server so VM instances can refer to each other by name. The fully qualified domain name for an instance is [HOSTNAME].c.[PROJECT_ID].internal. This name is tied to the internal IP address of the instance. An Instance does not know its external IP address and name. That translation is handled elsewhere in the network.</p>
<p>To reach Internet resources, each VM needs an external IP address. An ephemeral external IP address is created for each VM by default, but an ephemeral address gets replaced with another one if you stop and restart the instance, so if you want an instance to always have the same IP address, then you need to assign a static IP address to it.</p>
<p>Since IPv4 addresses are a scarce resource, Google doesn’t want customers to waste them. So you’re not charged for having static IP addresses as long as you’re using them. But if a static IP address is not associated with a VM instance or if it’s associated with an instance that’s not running, then you’ll be charged for it.</p>
<p>Normally, if a VM needs to send requests to other Google services, such as Cloud Storage, then by default, it has to do so using a public IP address rather than an internal one. This is problematic if you don’t want any of your internal network communications to go over the Internet. However, if you enable the Private Google Access option in a subnet, then VMs in that subnet can connect to Google services using internal IP addresses, so their requests will go over Google’s network rather than the Internet.</p>
<p>If you want instances in different projects to communicate with each other, then you have three options: the Internet, VPC Network Peering, or a Shared VPC. Connecting over the Internet is slower, less secure, and more expensive than the other two options, so it’s not usually the best choice.</p>
<p>The simplest alternative is VPC Network Peering. This allows two VPCs to connect over a private RFC 1918 space, that is, using non-routable internal IP addresses, such as 10.x.y.z. In other words, they don’t need public IP addresses, and they communicate over Google’s network. Not only can you do this for VPCs in different projects, but you can even use it to connect VPCs in different organizations. To make this work, both sides have to set up a peering association. If only one side sets up a peering association with the other VPC, then the networks won’t be able to communicate with each other. Also bear in mind that there can’t be any overlapping IP ranges in the two networks. You’ll notice in this example that the two ranges are not overlapping.</p>
<p>A more complicated option is to use a Shared VPC. The idea is that instances in different projects can share the same network. This is kind of a weird idea. If you’ve put resources in different projects, you probably want them to be managed separately, so why would you get them to use the same network? In most cases, it’s to enforce security standards. For example, if you want to use the same firewall rules across all of your projects, then this is a good way to do that.</p>
<p>To set up a Shared VPC, you need to designate one of the projects as the host project and the others as service projects. The host project is the one that contains the Shared VPC. Instances in the service projects can use subnets in the Shared VPC. This is made possible by giving Service Project Admins the authority to create and manage instances in the Shared VPC but nothing more. Meanwhile, the Shared VPC Admins have full control over the network. Note that all of the projects in this arrangement have to be part of the same organization.</p>
<p>OK, we’ve gone over a lot of networking topics. Now how should we apply these concepts to GreatInside?</p>
<p>At a minimum, we should create separate projects for the Development, Test, and Production environments. Inside each project, we should stick with the default network. There’s no need to add any additional ones. We should also stick with automatic subnetworks. The only subnetwork we need right now is one in the US, such as us-central1, since we don’t currently have any plans to expand into other parts of the world. When GreatInside decides to add instances overseas, then they can be added to the other regional subnets.</p>
<p>The default firewall rules should also be fine, since they only allow internal traffic plus ssh, icmp, http, and https. We should remove the rule that allows rdp traffic in the Production network, though, since we don’t have any Windows instances in it.</p>
<p>We don’t want the Production, Development, and Test environments to be part of the same network, so we don’t need a Shared VPC. In fact, we don’t want them to communicate with each other at all, so we don’t need to use VPC Network Peering either.</p>
<p>By the way, you probably noticed that everything I’ve shown so far is only for the interior design application. I’m going to get into the details of how to set up the payment processing environment in the Legislation and Compliance lesson.</p>
<p>One last item is that we have to decide which components need external IP addresses. That’s easy in this case because the load balancer is the only one that needs an external IP address (and ideally it should be a static address). Users will connect to the web instances through the load balancer, so the web instances only need internal IP addresses, and for security reasons, that’s all they should have.</p>
<p>That does raise the question of how a system administrator could connect to them for troubleshooting, though. One way is to give your administrators access to the internal network by interconnecting it with the company’s on-premises network. That’s something that GreatInside has already requested, so let’s see how to do that. There are three ways: Cloud VPN, Cloud Interconnect, and Direct Peering.</p>
<p>Cloud VPN lets you set up a virtual private network connection between your own network and Google Cloud. To do this, you need to have a peer VPN gateway in your own network and it needs to use IPsec to connect to the Cloud VPN Gateway and encrypt traffic. You can have multiple tunnels to a single VPN gateway.</p>
<p>By itself, Cloud VPN requires you to make changes to static routes on your tunnels manually. But if you use Google Cloud Router, then the routes will be updated dynamically using BGP (that is, Border Gateway Protocol). Network topology changes are propagated automatically.</p>
<p>The second way to connect is called Cloud Interconnect. Instead of connecting over the Internet, you can use an enterprise-grade connection to Google’s network edge. There are two ways to do this: Dedicated Interconnect and Partner Interconnect. If your internal network extends into a colocation facility where Google has a point of presence, then you can connect your network to Google’s. This is called Dedicated Interconnect. It’s a great solution that provides higher bandwidth and lower latency than a connection over the public internet. It’s a bit expensive, though, because the minimum bandwidth is 10 Gbps.</p>
<p>If you don’t have a presence in a supported colocation facility or you want to pay for a connection that’s smaller than 10 Gbps, you can use Partner Interconnect. With this option, you connect to a service provider that has a presence in a supported colocation facility. You can purchase a monthly contract for connections as small as 50 Mbps and as large as 10 Gbps. </p>
<p>The third way is to use Peering. This is similar to Cloud Interconnect because you connect your network to Google’s network at a point of presence either directly (which is called Direct Peering) or through a service provider (which is called Carrier Peering). One big difference with peering is that it doesn’t cost anything. So why would anyone pay for Cloud Interconnect when they could peer with Google for free? Well, because with Cloud Interconnect you get a direct connection between your on-premises network and one of your VPCs in Google Cloud. You have full control over the routing between your networks. If you want to change a route, you can change it on your on-premises router, and it will be picked up by BGP. Although the peering option uses BGP, too, it’s done at the most basic level. It doesn’t create any custom routes in your VPC network.</p>
<p>Since we don’t have requirements for low latency and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> between the company network and Google Cloud, we should go with Cloud VPN to connect. We should also use Cloud Router so network routes will be updated dynamically.</p>
<p>And that’s it for networks.</p>
<h1 id="Authentication"><a href="#Authentication" class="headerlink" title="Authentication"></a>Authentication</h1><p>The first step in giving secure access to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">Google Cloud infrastructure</a> is to decide how to authenticate your users. By default, Google Cloud Platform requires users to have a Google account to access it. But if you have more than a handful of users, then you’ll want to find a centralized way to manage your user accounts. The solution is to use the G Suite Global Directory. You don’t have to use G Suite products like Google Docs, you can just use G Suite for user management.</p>
<p>Most organizations already have a user directory, so the best policy is usually to manage users in your existing directory, and then synchronize the account information in G Suite. There are three ways to do this: Google Cloud Directory Sync or GCDS, the Google Apps Admin SDK, or a third party connector.</p>
<p>Google Cloud Directory Sync is the easiest solution if you have either Active Directory or an LDAP server. It synchronizes users, groups, and other data from your existing directory to your Google Cloud Domain Directory. GCDS runs inside your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/networks-1/">network</a> on a machine that you control.</p>
<p>It’s a one-way synchronization, so GCDS doesn’t modify your existing directory. Of course the synchronization can’t be a one-time event. It has to happen on a regular basis to keep your Google Directory up-to-date.</p>
<p>To make authentication even easier for your users, you can implement single sign-on or SSO. Google Cloud Platform supports SAML 2.0-based SSO. If your system doesn’t support SAML 2.0, then you can use a third party plugin.</p>
<p>Once you’ve implemented SSO, then when a user would normally have to login, Google will redirect your authentication system. If the user is already authenticated in your system, then they don’t have to login to Google Cloud separately. If they aren’t already logged in, then they’re prompted to login.</p>
<p>In order for this to work, your users must have a matching account in Google’s Directory. So you still need to use GCDS or one of the other synchronization options.</p>
<p>In our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/case-study-1/">case study</a>, since we have an active directory server, we’ll use GCDS for synchronization and also implement single sign-on.</p>
<p>And that’s it for authentication.</p>
<h1 id="Roles"><a href="#Roles" class="headerlink" title="Roles"></a>Roles</h1><p>To give a user permission to access particular Google Cloud resources, you assign a role to them. Basic roles act at the project level. There are 3 basic roles available: Owner, Editor, and Viewer. There are also fine-grained roles for individual resources. These are called predefined roles. (They were previously known as curated roles.) For example, the Cloud SQL Viewer role gives read-only access to Cloud SQL resources.</p>
<p>You can assign roles at different levels of the hierarchy, that is, at the organization, folder, project, and resource levels. If you assign roles to the same user at different levels, then their effective permissions are the union of the permissions at the different levels.</p>
<p>For example, if you granted Marie the Viewer role at the organization level and the Editor role at the project level, then she would have Editor permissions for all of the resources in that project. The Viewer role at the organization level would not override the Editor role at the project level. Similarly, if you assigned them in the opposite way, with the Editor role at the organization level and the Viewer role at the project level, Marie would still have the Editor role for all of the resources in that project because the project-level permissions would not override the organization-level permissions.</p>
<p>There are a few principles you should apply when setting roles and permissions. </p>
<p>First, use the principle of least privilege when granting roles. That is, assign roles with the least permissions required for people to do what they need.</p>
<p>Second, whenever possible, assign roles to groups instead of to individuals. Then, when you need to grant a role to a user, you can just add them to the group. Not only is this easier to manage, but it also ensures consistent privileges among members of a particular group. You can also use a descriptive group name that makes it clear why group members need those permissions.</p>
<p>Third, keep tight control of who can add members to groups and change policies. If you don’t, then people could give themselves or others more privileges than they should have.</p>
<p>Fourth, to make sure that inappropriate policy changes aren’t made, audit all policy changes by checking the Cloud Audit Logs, which record project-level permission changes.</p>
<p>Now let’s apply these principles to GreatInside. First, you would grant the project owner role to a few key system administrators. Owners are the only ones who can change policies (unless you grant users the Organization Administrator role). You should always have more than one owner. Otherwise, if that person is unavailable or leaves the organization, it would be difficult to for someone else to take their place as owner. So avoid that situation by giving the owner role to several people, but choose wisely because owners can do just about anything. </p>
<p>Similarly, you would have a small number of G Suite administrators who could add users to groups.</p>
<p>Obviously, there would be a large number of users who would need permissions, so I’m not going to talk about every type of user, but I’ll give a couple of examples. One example would be a network administration group that you would grant the Compute Network Admin role to.</p>
<p>Another example would be a QA team. You could grant their group the editor role on the Test Environment project and the viewer role on the Production Environment project. Alternatively, if the QA people don’t need full access to the Test Environment, then you could grant them several predefined roles, such as Compute Instance Admin, Cloud SQL Admin, and Compute Storage Admin.</p>
<p>Regarding audit logs, someone would need to take on the responsibility of checking for policy changes. The Admin Activity audit logs are viewable by all project members, so you wouldn’t need to grant access to the person who does the checking.</p>
<p>And that’s it for roles.</p>
<h1 id="Service-Accounts"><a href="#Service-Accounts" class="headerlink" title="Service Accounts"></a>Service Accounts</h1><p>Now that you have user authentication and permissions figured out, it’s time to plan how your applications will access the Cloud Platform services it needs to use. To avoid embedding credentials in an application, you need to use service accounts. For example, if an application uses Cloud Datastore as a database, then it needs to have authorization to use the Datastore API.</p>
<p>You would accomplish this by enabling Datastore API access on any VM instances that will be involved in the part of the application that uses the database. By default, all VM instances run as the Compute Engine default service account. If you want something different, then you can create your own.</p>
<p>A service account has an email address and a public&#x2F;private key pair that it uses to prove its identity. Your instances use that identity when communicating with other Cloud Platform services. However, by default, an instance running as the Compute Engine default service account has limited scope in how it can interact with other services. For example, by default an instance can only read from Cloud Storage and can’t write to it.</p>
<p>To give an instance more permissions, you need to set the scope when you’re creating the VM. So, in the case of interacting with Datastore, you have to enable access to the Datastore API. You also have to enable the Datastore API at the project level, but you only have to do that once.</p>
<p>Then your application code has to obtain credentials from the service account whenever it uses the Datastore API. Google Cloud Platform uses OAuth 2.0 for API authentication and authorization. There are two ways to do it: Application Default Credentials and access tokens.</p>
<p>The easiest way is to use Google Cloud Client Libraries. They use Application Default Credentials (or ADC) to authenticate with Google APIs and send requests to those APIs. One great feature of ADC is that you can test your application locally and then deploy it to Google Cloud without changing the application code. </p>
<p>Here’s how it works. To run your code outside Google Cloud Platform, such as in your on-premise data center or on another cloud platform, create a service account and download its credentials file to the servers where the code will be running. Then set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the credentials file.</p>
<p>So while you’re developing locally, the application can authenticate using the credentials file and when you run it on a production instance, it will authenticate using the instance’s service account. This works because ADC allows applications to get credentials from multiple sources.</p>
<p>The second way is to use OAuth2 access tokens to directly connect to the API without going through a client library. One reason you’d have to use this method is if your application needs to request access to user data.</p>
<p>The way it works is the application requests an access token from the metadata server and then uses the token to make an API request. Tokens are short-lived, so your application needs to request new ones regularly.</p>
<p>If you need to write shell scripts that access other Cloud Platform services, then you can use gcloud and gsutil commands to make API calls. These two tools are included by default in most Compute Engine images and they automatically use the instance’s service account to authenticate with APIs.</p>
<p>So what service accounts would you need to create for GreatInside? The load balancer and the web instances communicate over HTTPS, so you don’t a service account for that. Since the Tomcat instances communicate with the MySQL database in Cloud SQL, you would need a service account for that. Similarly, the IIS instances communicate with SQL Server in Cloud SQL, so you’d need a service account for that, too. There may be a need for other service accounts when we add more features to our architecture, such as disaster recovery, but we’ll cover that later.</p>
<p>And that’s it for service accounts.</p>
<h1 id="Data-Protection-and-Encryption"><a href="#Data-Protection-and-Encryption" class="headerlink" title="Data Protection and Encryption"></a>Data Protection and Encryption</h1><p>Protecting data is critical in any organization. Google Cloud Platform is very strong in this area because of its default encryption policies. Before we get into encryption, though, let’s look at Access Control Lists (or ACLs).</p>
<p>ACLs specify who has access to Cloud Storage buckets and objects in buckets. I’m not going to cover this topic in depth, but there are a few things to keep in mind when you’re deciding what ACLs to apply to your Cloud Storage.</p>
<p>First, there are actually five different mechanisms for controlling access to Cloud Storage: IAM permissions, ACLs, Signed URLs, Signed Policy Documents, and Firebase Security Rules. With so many different ways to control access, you have to be careful not to create conflicting permissions. Start with the first two: IAM permissions and ACLs.</p>
<p>IAM permissions work at the project level. For example, you can specify that a user has full control of all the objects in all of the buckets in your project, but cannot create, modify, or delete the buckets themselves. So they’re a nice way to grant broad access to buckets and objects, but if you want to set fine-grained access, such as which buckets or objects a particular group can read, then you need to use ACLs.</p>
<p>The confusing thing about using these two mechanisms is that you have to look at both of them to get a complete picture of access permissions. For example, you could list the ACLs for a bucket and see that only Bob has been granted write access, but it wouldn’t show that Jill has also been granted write access to all buckets by IAM. For this reason, whenever possible, you should try to use either IAM or ACLs, but not both.</p>
<p>Another potential source of confusion is that bucket and object ACLs are independent of each other. The ACLs on a bucket do not affect the ACLs on objects inside that bucket. For example, you might think that Jane doesn’t have access to the objects in a bucket because she hasn’t been granted access to the bucket itself, but she could have been granted access to any of the objects in the bucket.</p>
<p>So you should keep a couple of principles in mind. First, apply the principle of least privilege. Grant users and groups only as much access as they need. Second, keep your access control as simple as possible. Try to use as few control mechanisms as you can.</p>
<p>If GreatInside decides to replace its internal file server using Cloud Storage, then the best way to secure the files would be to use ACLs. You would create groups to match the teams in the company and create ACLs that give those groups access to the appropriate resources. For example, you could create a bucket for each group. Then for each bucket, you would make the associated group a writer of the bucket. Finally, you would set the object default permissions so that any new objects uploaded to the bucket would get the same permissions and everyone in the group would have full access. If the company’s needs aren’t that simple, then you would set more complex ACLs.</p>
<p>Now let’s move on to encryption. To ensure that your data is encrypted at all times, it needs to be encrypted when it’s in storage (also known as “at rest”) and when it is being sent over a network (also known as “in flight”). Google Cloud Platform takes care of both of these situations.</p>
<p>Encryption in flight is handled very simply. All of the Cloud Platform services are accessible only by API (even when you’re using other methods, such as the Cloud Console or the gcloud command, they’re making API calls under the hood). And all API communication is encrypted using SSL&#x2F;TLS channels. Furthermore, every request has to include a time-limited authentication token, so the token can’t be used by an attacker after it expires. Of course, for any communications between your Google Cloud infrastructure and outside parties, such as website visitors, you have to use SSL&#x2F;TLS yourself to encrypt the traffic.</p>
<p>Encryption at rest is just as simple if you’re willing to leave it to Google because Cloud Platform encrypts all customer data at rest by default.</p>
<p>So without you having to do anything, all of your data will be encrypted both at rest and in flight. Then why isn’t this the end of this lesson? Well, because your organization might want to take on some of the encryption responsibilities itself.</p>
<p>There are actually two layers of encryption for data at rest. First, the data is broken into subfile chunks, and each chunk is encrypted with an individual data encryption key (or DEK). These keys are stored near the data to ensure low latency and high availability. The DEKs are then encrypted with a key encryption key (or KEK). The keys are AES-256 symmetric encryption keys.</p>
<p>Google always manages the data encryption keys, but your organization can manage the key encryption keys if that’s your preference. There are two options for doing this: Customer-managed encryption keys or Customer-supplied encryption keys.</p>
<p>With the customer-managed option, you use the Cloud KMS service to create, rotate (or automatically rotate), and destroy your encryption keys. The keys are hosted on Google Cloud. You can have as many keys as you want, even millions of them if you actually need that many. You can set user-level permissions on individual keys using IAM and monitor their use with Cloud Audit Logging.</p>
<p>Cloud KMS is a nice service, but why wouldn’t you just let Google manage your key encryption keys and not have to deal with it yourself? The biggest reason is compliance with standards or regulatory requirements, such as HIPAA (for health information) or PCI (for credit card information).</p>
<p>If your organization requires that you generate your own keys and&#x2F;or that they’re managed on-premises, then you have to use Customer-supplied encryption keys. Be aware that this option is only available for Cloud Storage and Compute Engine.</p>
<p>With CSEK, Google doesn’t store your key. You have to provide your key for each operation, and your key is purged from Google Cloud after the operation is complete. Here’s how to do it from the command line with each of the two supported services. To encrypt the disk on a Compute Engine instance, you add the csek-key-file flag and point it to a file that contains the key. To encrypt data you’re uploading to Cloud Storage, you have to do it a bit differently. Rather than adding an encryption flag to the gsutil command, you need to add the encryption key to your .boto file, which is the configuration file for the gsutil command. Then all of your gsutil commands will use that key.</p>
<p>It only stores an SHA256 hash of the key as a way to uniquely identify the key that was used to encrypt the data. When you make a request to read or write the data in the future, your key can be validated against the hash. The hash cannot be used to decrypt your data.</p>
<p>There’s a big risk in using this method, though. If you lose your keys, you won’t ever be able to read your data again, and you’ll end up deleting it so you won’t be paying storage charges for unreadable data.</p>
<p>So far all of the encryption methods we’ve covered, including default encryption, Cloud KMS, and CSEK have been examples of server-side encryption. This is where your data is encrypted after Google Cloud receives your data. The only major difference between the 3 methods is where the key comes from. But there is another way. It’s client-side encryption. This means that you encrypt the data before you send it to Google Cloud. Google won’t even know that it’s already encrypted and it will encrypt it again. When you read your data back, Google Cloud will decrypt it on the server side first and then you’ll decrypt your own layer of encryption on the client side. The same warning applies - if you lose your keys, your data will effectively be gone.</p>
<p>Since our case study includes credit card information, we’ll need to be PCI DSS compliant, so we should use Cloud KMS to manage our keys. I’ll talk more about PCI compliance in the next lesson.</p>
<h1 id="Legislation-and-Compliance"><a href="#Legislation-and-Compliance" class="headerlink" title="Legislation and Compliance"></a>Legislation and Compliance</h1><p>Google Cloud Platform has passed annual audits for some of the most important security standards, including SOC 1, 2, and 3, ISO 27001, and PCI DSS. It also complies with HIPAA, CSA STAR, the EU-US Privacy Shield Framework, and MPAA controls, none of which require annual audits.</p>
<p>So if your organization is required to comply with any of these standards, then you know that Google has done its part. But this is a shared responsibility because <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">your security processes and applications</a> running on top of Google’s infrastructure also need to comply.</p>
<p>You’ll notice that Network is listed for both Google and the customer. That’s because Google takes care of some parts of networking and the customer takes care of the rest. One of the most interesting areas of shared responsibility for network security is protecting against distributed denial of service (or DDoS) attacks.</p>
<p>Google provides many features to help deal with DDoS attacks, but it’s up to the customer to use them properly. Here are some of the techniques.</p>
<p>Reduce the attack surface by</p>
<ul>
<li>Isolating and securing your deployment with firewall rules</li>
<li>Google also provides anti-spoofing protection by default</li>
</ul>
<p>Isolate your internal traffic from the external world by</p>
<ul>
<li>Deploying instances without public IPs unless necessary</li>
</ul>
<p>Use Load Balancing</p>
<ul>
<li>Because a load balancer acts as a proxy that hides your internal instances</li>
</ul>
<p>Use Cloud Armor</p>
<ul>
<li>This service is specifically designed to provide DDoS defense</li>
<li>And it works with Load Balancing</li>
<li>It protects against layer 3 and layer 4 DDoS attacks</li>
</ul>
<p>Google Cloud also enforces API rate limits and resource quotas to prevent a spike in one customer’s activity from affecting other Cloud Platform customers.</p>
<p>Now it’s time to get back to the PCI DSS standard and how to comply with it. If your organization accepts credit card payments, then you need to comply with this standard or you could be fined. More importantly, if you have security flaws that allow hackers to steal credit card information from your systems, then it would be very damaging to both your customers and your reputation.</p>
<p>In the case study, GreatInside provides an interface for collecting credit card information, but it passes the validation and processing of the information to a Certified Payment Processor. This makes the company an SAQ A-EP merchant in PCI lingo. I’ll go over Google’s recommendations for how this type of merchant could comply with PCI DSS.</p>
<p>First, you have to check that the other parties involved (that is, Google Cloud and the payment processor) are certified for your volume of transactions (since there are different PCI DSS merchant levels based on the number of transactions). Google Cloud Platform has the highest level of PCI DSS certification, so that’s not a concern, but you’ll have to check your payment processor’s certification level because your volume might exceed their certification level.</p>
<p>Here’s a suggested architecture to handle the company’s credit card processing. Here’s how it works. A customer enters their credit card information in a form on your website. Then your payment-processing application sends the information to the external payment processor. Now the payment processor tells your application whether the card was accepted or declined. After that your payment processing application sends some or all of the response data to your core application, so it knows how to proceed with this customer.</p>
<p>You also need to log and monitor all of these interactions. Every instance involved in payment processing sends its logs to Stackdriver Logging and its alerts to Stackdriver Monitoring.</p>
<p>Now let’s move on to how you would set this up. To reduce the number of systems that need to be PCI-compliant, you have to fully isolate your payment-processing environment from the rest of your production environment. The best way to do this is to use a separate Google Cloud account, rather than just a separate project within your main account.</p>
<p>Then use IAM to grant access only to people who absolutely need to work on the payment-processing environment, such as people who will be deploying new versions of the application or managing the systems. These people must also pass a background check first.</p>
<p>To create the instances, you should first create your own Linux image that’s based on one of the preconfigured boot disk images and that contains the bare minimum of additional software needed to run your application. Then use this custom image when creating all of your VM instances.</p>
<p>To secure the network, create firewall rules that only allow three types of inbound traffic:</p>
<ul>
<li>HTTPS traffic from the load balancer to the payment form servers, so that customers can reach your payments page</li>
<li>Credit card authorization responses from the external payment processor to your internal payment authorization servers, and</li>
<li>VPN traffic from your internal office network to the VPN Gateway, so your authorized people can manage and audit the application and systems.</li>
</ul>
<p>Then create firewall rules for outbound traffic. There’s only one type of outbound traffic you need to allow – HTTPS traffic from the payment form servers to the external payment processor, so they can send credit card authorization requests.</p>
<p>Now all of the traffic in and out of the network is locked down, but you’ll also have to open up internal traffic, such as:</p>
<ul>
<li>From all of the instances to Google’s NTP servers for time synchronization, and</li>
<li>SSH traffic from the VPN Gateway to all of the instances, so authorized people can access the systems for maintenance</li>
</ul>
<p>OK, let’s move on to deploying your application. To be compliant, you have to make sure you’re deploying the correct application every time, that it’s deployed securely, and that no other software packages are installed during the deployment. If you don’t already have an automated deployment tool, then you might want to use Cloud Deployment Manager, which could automate the creation of everything in your payment-processing environment, even the firewall rules. It could also help you create an audit trail of deployments.</p>
<p>Since you’ve used the same custom Linux image for all of your instances, you’ll need to install additional software on each instance. For example, some instances may need a web server, while others don’t, and each instance should only have the software it needs, which will reduce your security risks. To make this process consistent and reliable, it should be automated as well. The easiest way to automate software installation and configuration is to use a configuration management tool such as Chef, Puppet, or Ansible. Cloud Academy has courses on all three of these tools, so check one out if you’re not familiar with how to use any of them. </p>
<p>There are a few packages that you’ll want to install on all instances. First there’s iptables. You can set it up to log all network activity to and from each instance. This data is required for PCI DSS compliance audits.</p>
<p>Second, each instance needs the Stackdriver Monitoring and Logging agents so it can send logs and alerts.</p>
<p>Third, each instance should run an Intrusion Detection System (or IDS) to alert you to suspicious activity.</p>
<p>Finally, your configuration management tool needs to securely retrieve and launch the latest version of your application. </p>
<p>Even with an automated deployment, you’d still need to verify the integrity of the software being deployed. You could do this by running an automated checksum comparison against each package as it’s installed. You could also run an automated code analysis tool to check for security flaws.</p>
<p>Now let’s move on to logging. To be compliant, every step in the payment-processing environment has to be monitored and recorded. All instance activity and all user activity must be logged. Stackdriver Logging is a great service for collecting logs. You can record network traffic to and from your instances by enabling VPC Flow Logs on each subnet in your VPC.</p>
<p>By the way, you might think that we need to assign a service account to the instances so they can write logs to Stackdriver, but the default service account for VM instances already grants write access to Stackdriver, so you don’t need to configure that yourself.</p>
<p>I mentioned that user activity needs to be logged, but you also need to log the activity of people who have administrative access to the environment. The easiest way is to log all shell commands.</p>
<p>The amount of log information generated by all of this is likely to be very large, so you might want to export your Stackdriver logs to BigQuery if you need to do some complex analysis.</p>
<p>In addition to logging, you also need to set up real-time monitoring alerts, such as when your IDS detects any intrusion attempts.</p>
<p>After your environment is implemented, but before any production traffic flows through it, you have to validate the environment, either by contracting a Qualified Security Assessor if you’re a Level 1 merchant or by filling out the Self Assessment Questionnaire if you’re not a Level 1 Merchant.</p>
<p>Wow, that was a lot of work, wasn’t it? Well, if you’re going to be handling credit card information, you’ll be happy when your rigorous security design prevents damaging incidents.</p>
<h1 id="Disaster-Recovery"><a href="#Disaster-Recovery" class="headerlink" title="Disaster Recovery"></a>Disaster Recovery</h1><p>In an earlier lesson, we covered how to design a highly available architecture that will keep running even if an instance fails, by using load balancers, instance groups, and redundant databases. However, there are more catastrophic events that might occur. I’m not talking about an entire city getting destroyed or anything like that (although it would be good to have an architecture that could handle that). But much smaller incidents can be disastrous too. For example, one of your databases could become corrupt. This is actually worse than the database server going down because it may take a while before you realize there’s a problem, and in the meantime, the corruption problem could get worse.</p>
<p>To recover from this sort of disaster, you need backups along with transactional log files from the corrupted database. That way you can roll back to a known-good state. Each type of database has its own method for doing this.</p>
<p>If you’re using Cloud SQL to run a MySQL database (which we are for the interior design application), then you should enable automated backups and point-in-time recovery. Then if your database becomes corrupt, you can restore it from a backup or use point-in-time recovery to bring the database back to a specific point in time. </p>
<p>If you’re using Cloud SQL for SQL Server, then you should enable automated backups. At this time, Cloud SQL does not support point-in-time recovery for SQL Server, so you can only restore a database to the point when a specific backup was taken. For both types of databases, Cloud SQL retains up to 7 automated backups for each instance.</p>
<p>If you’re hosting a database on Compute Engine instances directly, then you’ll have to configure backups and transaction logging yourself. For example, suppose that instead of using Cloud SQL for SQL Server, we ran SQL Server on a Compute Engine instance. Then we’d need to set up our own disaster recovery solution for it. Luckily, Google has a very detailed white paper on this topic. I’ll give you the highlights.</p>
<p>First, set up an automated task that copies the SQL Server database backups to Google Cloud Storage. This is where we would finally need a service account because instances can’t write to Cloud Storage by default. The SQL Server instances need to have a service account with the Storage Object Creator role. Another way to do it would be to set a Cloud Storage access scope for the instance, but service accounts are more flexible.</p>
<p>Once the database is being backed up, then if disaster strikes, you would spin up a new SQL Server instance. Either use one of Google’s preconfigured SQL Server images or your own custom disk image. It doesn’t mention this in the whitepaper, but it’s the sensible thing to do and I’ll talk about it more in a minute. Next, you can use an open-source script to restore the database and re-execute the events in the log files up to the point in time desired.</p>
<p>When you’re designing a disaster recovery solution, you need to consider RPO and RTO. RPO stands for Recovery Point Objective. This is the maximum length of time when data can be lost. It affects your backup and recovery strategy because, for example, if it’s acceptable to lose an entire day’s worth of work, then you can just recover using the previous night’s backups. If you have a short RPO, which is usually the case, then you need to make sure you are constantly backing up your data, and when recovering from database corruption, you have to carefully consider which point in time to recover to.</p>
<p>RTO stands for Recovery Time Objective. This is the maximum length of time that your application can be offline and still meet the service levels your customers expect (usually in a service level agreement).</p>
<p>In the SQL Server example, I suggested using either one of Google’s preconfigured SQL Server images or your own custom disk image that has SQL Server installed and configured. The advantage of having a custom disk image is that it helps you meet your recovery time objective because it reduces the amount of time it takes to get a new SQL Server instance running. If you have to configure SQL Server manually, that could significantly impact how long it takes to recover from a disaster.</p>
<p>As with everything, though, there are tradeoffs. If your SQL Server implementation is customized, then you’ll have to weigh the benefits of fast recovery time against the maintenance effort required to keep your custom image up-to-date . If you have a very short RTO, then you may have no choice but to maintain a custom disk image. You might be able to ease the maintenance required, though, by using a startup script to perform some of the customization. Since the startup script resides on either the metadata server or Cloud Storage, you can change it without having to create a new disk image.</p>
<p>In some cases, you may want to run an application from your own data center or from another cloud platform and use Google Cloud as a disaster recovery solution. There are many ways you could do this, but I’ll go over a couple of common designs.</p>
<p>The first way is to continuously replicate your database to an instance on Google Cloud. Then you would set up a monitoring service that would watch for failures. In the event of a disaster, the monitoring service would trigger a spin-up of an instance group and load balancer for the web tier of the application. The only part you would need to do manually is to change the DNS record to point to the load balancer’s IP address. You could use Cloud DNS or another DNS service for this.</p>
<p>This is already a low-cost solution because the only Google Cloud resource that needs to run all the time is the database instance. But you can reduce the costs even further by running the database on the smallest machine type capable of running the database service. Then if there’s a disaster, you would delete the instance, but with the option to keep the persistent disk, and spin up a bigger instance with the saved disk attached. Of course, this solution would require more manual intervention and would lengthen your downtime, so you wouldn’t want to do this if you have a short RTO.</p>
<p>If you want to reduce your downtime as much as possible, or even keep running in the event of hardware failures, you could serve your application from both your on-premises environment and your Google Cloud environment at all times. That way if you have an on-premise failure, the Google Cloud environment would already be running and serving customers. It would just need to scale up to handle the extra load, which would be automatic if you use an autoscaling instance group.</p>
<p>To make this hybrid solution work, you would need to use a DNS service that supports weighted routing, so it could split incoming traffic between the two environments. In the event of a failure, you would need to disable DNS routing to the failed environment.</p>
<p>And that’s it for disaster recovery.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">design a Google Cloud infrastructure</a>. Now you know how to map <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/compute-1/">compute</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/storage-2/">storage</a>, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/networks-1/">network</a> requirements into Google Cloud, and secure your infrastructure with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/authentication-2/">authentication</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/roles-1/">roles</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/service-accounts-1/">service accounts</a>, ACLs, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/data-protection-and-encryption-1/">encryption</a>. You also know some of the ways to design <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disaster recovery</a>, and PCIDSS compliance into your solution.</p>
<p>To learn more about Google Cloud platform, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know on the Cloud Academy community forums. Thanks and keep on learning.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:36:04" itemprop="dateCreated datePublished" datetime="2022-11-14T12:36:04-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:43:06" itemprop="dateModified" datetime="2022-11-21T02:43:06-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Setting-up-a-Google-Cloud-Platform-Environment-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Setting-up-a-Google-Cloud-Platform-Environment-6/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Knowledge-Check-Setting-up-a-Google-Cloud-Platform-Environment-6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:34:33" itemprop="dateCreated datePublished" datetime="2022-11-14T12:34:33-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:41:56" itemprop="dateModified" datetime="2022-11-21T02:41:56-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Setting-up-a-Google-Cloud-Platform-Environment-6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Setting-up-a-Google-Cloud-Platform-Environment-6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p><object data="Knowledge-Check-Setting-up-a-Google-Cloud-Platform-Environment.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Setting-up-a-Google-Cloud-Platform-Environment-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Setting-up-a-Google-Cloud-Platform-Environment-5/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Setting-up-a-Google-Cloud-Platform-Environment-5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:32:57" itemprop="dateCreated datePublished" datetime="2022-11-14T12:32:57-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:41:16" itemprop="dateModified" datetime="2022-11-21T02:41:16-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Setting-up-a-Google-Cloud-Platform-Environment-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Setting-up-a-Google-Cloud-Platform-Environment-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to “Setting up a Google Cloud Platform Environment”. My name is Daniel Mease and I’ll be taking you through this course. I am a Google Cloud Platform instructor at Cloud Academy with over 20 years of software and web development experience.</p>
<p>In this course, I am going to show you how to organize your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> account using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/creating-projects/">projects</a> and prepare it for development.</p>
<p>This course is intended for Google Cloud Platform Administrators or people who want to get GCP certified (eg: Associate Cloud Engineer).</p>
<p>After completing this course, you will be able to create and manage cloud projects, create and add users to projects, assign permissions to users, enable APIs for a project, and set up budgets and monitor spending.</p>
<p>The following prerequisites will be helpful for this course. Access to a GCP admin account is not strictly required, but it will be useful if you want to practice the tasks I will be demonstrating. If you didn’t already know, you can sign up for a free trial.</p>
<p>Feedback on our courses is valuable to both us as trainers and our future students. If you have any criticisms or suggestions for improvement, we would greatly appreciate it if you would share those with us.</p>
<p>Please note, when this video was recorded, all course information was accurate. Google is constantly updating its products and services as part of its ongoing drive to innovate. As a result, over time, minor discrepancies may appear in the course content. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. </p>
<p>So, if you notice any information that is outdated, please contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. This will allow us to make all necessary fixes to the course during its next release cycle.</p>
<h1 id="Creating-Projects"><a href="#Creating-Projects" class="headerlink" title="Creating Projects"></a>Creating Projects</h1><p>Before you can start building anything on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, you first need a project. Projects allow you to easily organize all the resources in your account. Every time you create a compute, storage or network resource, you will need to assign it to a project. Projects also contain their own unique set of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/adding-users/">users</a>, APIs, and billing account information.</p>
<p>So let’s say you wanted to build a blog for sharing photos. You would start out by creating a new project. Let’s say you named it “Photo blog”. You then would start adding some resources to it. First, you might add a Compute Engine instance for running WordPress. Then a Cloud SQL database for storing your posts. And finally a Cloud Storage bucket for hosting the photos. All three of these resources would be linked to the project “Photo blog”. Other projects would be set up in a similar fashion. In this way, you can easily tell which of your resources belong to “Photo blog” and which belong to other projects.</p>
<p>If you are just starting out, creating projects might feel like an unnecessary step. However, the value quickly becomes apparent as you start building more and more. Organizing your resources into projects means that you won’t have to scroll through a thousand Virtual Machines to find the one you are looking for. Also, it means that you won’t accidentally get a production database confused with a development database. Finally, it will make your life easier when you need to shut down a project. You won’t have to track down each individual resource and manually delete it. When you delete a project, all associated resources are deleted as well.</p>
<p>And that is just a few. There are many other advantages. For example, security is greatly enhanced. Just because one project has access to an API, does not mean all your projects need access. The same is true for users. Not every user account needs the same level of permissions to every project. If you are going to be building anything substantial, you really need to get comfortable with creating and organizing Google Cloud Projects.</p>
<p>So now that you understand the importance of projects, let me demonstrate how to work with them in the GCP console. It’s actually pretty simple. If you want to try to follow along, you will need admin access to a GCP account. Remember, you can always sign up for a free trial at cloud.google.com.</p>
<p>Once you first log in, your screen should look similar to this. If you just created your account, then a default project has already been set up for you. It’s called “My First Project”. You can use this project, but eventually, you will want more. So let’s go through the process of creating a new project.</p>
<p>The console usually has multiple ways of accomplishing the same goal. For example, here you could click on the project selector drop-down menu at the top of the screen. And then you can click on the “New Project” button here. Or you can also go to the Navigation menu, click “IAM &amp; Admin”, and then on “Manage Resources”. From here you can see all of your projects. You might see more or less here, depending on if this is a new account or if not. I just created this account and have not defined any organizations, so it’s mostly empty except for the one default project. </p>
<p>The obvious next step is to click the “Create Project” button. Now I need to pick a project name. This can be whatever you want. Usually, I try to pick a name that describes what I am planning to build. For this example, let’s go with “Photo Blog”.</p>
<p>When you create a new project, a Project ID is automatically generated for you based upon the Project Name. Usually, it will contain the name of the project and have a random number appended to the end. The Project Name is purely for your benefit. It can be whatever you want. You can change it at any time. And it does not need to be unique. </p>
<p>The Project ID is for uniquely identifying your project. This allows GCP to distinguish your project from all others. That also means your Project ID needs to be globally unique. The ID cannot be the same as any other projects in any other account. You can see that it is possible to try to pick your own ID, but generally, I would recommend against it. It’s going to be hard to pick something clever that has not already been used by someone else. Instead, pick a good name and let Google generate the ID for you.</p>
<p>You are also asked to pick a location. If you have a more complicated setup with folders or multiple organizations, you will need to pick the correct one. But since this is a new, personal account I don’t have any of that. I will just leave it set to the default.</p>
<p>Once I click “Create”, the project will be created.</p>
<p>After creating a project, you can view the details on the Project info card on the Dashboard. You also can switch between projects by clicking on the project selector drop-down at the top of the page. Now that I have switched to the “Photo blog” project, anything I build will be assigned to that project.</p>
<p>When you no longer have use for a project, you can delete it. A project is marked for deletion for 30 days before it is actually deleted. This gives you some time to recover a project if you accidentally deleted it, or if you change your mind later. While the project is marked for deletion, it is unusable; however, its resources still count against your project quota until it is deleted. If a billing account is assigned to the project, the project may not be deleted until the current billing cycle ends.</p>
<p>To shut down a project, open the Nav menu, click on “IAM and Admin”, then click “Manage Resources”. Then simply select the Project and click the “Delete” button at the top of the page. Then you will get a confirmation warning about deleting a project. To confirm deleting, enter the Project ID. And finally, click the “Shut down” button. Be aware that deleting a project will not allow you to reuse the same Project ID. Once you use a project ID, that ID can never be used again by anyone.</p>
<p>A deleted project can be restored, as long as you do it before the end of the 30-day shutdown period. To restore a project, simply click on the “Resources Pending Deletion” button. Find the deleted project or projects that you want to restore, and select them. Then click the “Restore” link at the top of the page. And finally, confirm that you wish to restore the project by clicking the “Restore” button. So if we go back, we can confirm that the project is no longer marked for deletion.</p>
<p>So now you know how to create, delete and restore cloud projects. If you have access to a GCP account, you may want to try creating and deleting a few projects yourself to get familiar with the process.</p>
<h1 id="Adding-Users"><a href="#Adding-Users" class="headerlink" title="Adding Users"></a>Adding Users</h1><p>Typically, companies will have many different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/creating-projects/">projects</a> with many different users working on them. And those users will often have different needs. For example, some users might need access to every project, while others might only need to access one or two. Some users will just need the ability to just look at things, while others might also need to make changes. That is why every project in your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a> account has its own set of users. And each user account can have its own unique set of permissions.</p>
<p>When it comes to creating new user accounts, you have a couple of options. First, you can manually create them yourself. This works fine if you have a small number of users. However, for large companies with hundreds or thousands of users, you may want to consider automating the process.  If you already have something like Active Directory or LDAP, you can use Google Cloud Directory Sync to automatically replicate them to Google Cloud Identity. Cloud Identity allows you to manage and authorize your user accounts across multiple applications and projects. It also supports SAML 2.0 (Security Assertion Markup Language) for single sign-on (SSO), as well as two-factor authentication (2FA).</p>
<p>Whichever option you choose, you have full control over which permissions are granted to your users. This is accomplished by assigning roles. Roles are groups of permissions. You could try to directly assign individual permissions to users, but this would take a lot of time. Users typically need many, many different types of permissions to do their jobs. So to save time, you instead assign permissions to roles. And then you assign roles to users. </p>
<p>Now there are three main types of roles: basic, predefined, and custom. Basic and predefined roles are already created for you by Google. Custom roles are roles that you can create yourself.</p>
<p>Basic roles (formerly called primitive roles) represent a very simple and broad set of permissions. The Browser role allows a user to view what resources already exist, but not get any detailed information. The Viewer role allows a user to get more detailed information about resources, but not modify them. The Editor role allows a user to modify resources, but not other user accounts. And the Owner role gives a user full control. Owners can modify any resource or user account, and can also do things like set up <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/manage-billing/">billing</a>. It is generally recommended that you avoid using basic roles, especially in production <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/introduction/">environments</a>. Basic roles typically either grant too many or not enough permissions.</p>
<p>Predefined roles provide smaller sets of permissions for specific resources. And because of this, there are many, many more predefined roles than basic ones. For example, if a user needed to be able to inventory all the compute engine resources, you could grant them the “Compute Viewer” predefined role. This would only allow them to view the current Compute Resources. If a user needed to modify firewall rules, you could grant them the “Compute Security Admin” predefined role. And if a user needed full control over all Compute Engine resources including VMs, security, and storage, you could grant them the “Compute Admin” predefined role.</p>
<p>The third type of role is the custom role. Custom roles are groups of permissions that can be as wide or as restricted as you wish. Predefined roles cover many typical scenarios, but can still end up granting unnecessary permissions. By defining your own Custom roles, you can assign exactly the permissions each user needs and no more.</p>
<p>Alright. Enough talk. Time to demonstrate how to create user accounts and assign roles. First, go to the Navigation menu and select “IAM &amp; Admin” and then “IAM”. Right now I just have a single user account for myself. This is the account I am using right now. Let’s add a few more.</p>
<p>Before creating a new user, always double-check that you have the right project selected. You don’t want to accidentally add a user to the wrong project. </p>
<p>Next, click the “Add” button at the top of the page. First, you will need to enter the user’s email address. So let me do that. Second, you need to pick at least one role to assign. There are the three main types of roles I talked about before. You can find Basic roles here. For this first user I will pick “Browser”. And once you click on “Save” the user will be created. So now the person with the email address of newuser1@gmail can log in and browse the resources inside of my “Photo blog” project.</p>
<p>Next, let’s add another user with some predefined roles. I will enter another email address. You can find the predefined roles down here under “All roles”. You can see that there are a lot more predefined roles than basic. There are so many, that it is often helpful to use the filter to search for the roles you are interested in. I can search for “App Engine” and get a list of App Engine related roles. So let me assign the App Engine Viewer role. You can also add more roles. You aren’t just limited to one. So let me add another predefined role. I’ll pick “BigQuery Data Viewer”. And then click “Save”.</p>
<p>So I have assigned basic and predefined roles. Let’s create a third user and assign custom roles. I have already created a few custom roles before I started filming so you could see how this work. If you want to create a new role, you can click on the “Manage Roles” link below. I’m going to assign Custom Role 1.  And also Custom Role 2. So now you know how to add new user accounts to your project. </p>
<p>Really quickly, I want to also show you how to create your own custom roles. If you select “Roles” from the side menu, you will see a list of the predefined and custom roles that exist. From here you just need to click on “Create Role” at the top of the page and you can pick a title (which is the name) and then add permissions. There are a lot of permissions to choose from, so you can also search and filter by name. I am just going to pick a few permissions at random.</p>
<p>So this new custom role is called “Custom Role 3”. It is going to have 4 assigned permissions. And once I click “Create” it will be available to use.</p>
<p>There are going to be times when you want to delete a user.  Luckily, deleting is pretty simple. Let’s go back to the IAM page. You just select the users, and then click the “Remove” button at the top of the page. Confirm that, yes, you wish to remove the users from the project. And that is all there is to it. The user accounts have been deleted.</p>
<p>As you can see, creating users and assigning them roles is fairly straightforward. The real trick is understanding what permissions your users will need. It depends on what you are building and what your requirements are. Remember, you can always change permissions later. So it is recommended to start out assigning the minimum amount of permissions at first. Then you can add more later as required. It is safer to give out too few, than too many.</p>
<p>And that wraps up the lesson on users and roles. If you can, I recommend creating a few users with different levels of permissions. Then try to log in as each and take note of the limitations on what you can see and do. </p>
<h1 id="Enabling-APIs"><a href="#Enabling-APIs" class="headerlink" title="Enabling APIs"></a>Enabling APIs</h1><p>In order to access certain Google Cloud resources, you may need to enable the corresponding API. Google Cloud APIs are application programming interfaces that give you access to certain Google Cloud services. APIs for common tasks are enabled by default. But many APIs are disabled and need to be explicitly enabled for your project before you can start using them.</p>
<p>Suppose you are writing an application that interacts with a user’s Google Calendar. In order to add or modify events, you would need to enable the Google Calendar API. You can find a complete list of all APIs at <a target="_blank" rel="noopener" href="https://cloud.google.com/apis">https://cloud.google.com/apis</a>. When you enable an API, the corresponding monitoring pages and billing properties are also added to your project.</p>
<p>Let me demonstrate how to enable an API for a project. First, verify that you have the correct project selected in the drop down. Then go to the navigation menu and select “APIs &amp; Services” and “Dashboard”. This will show you all your currently enabled APIs. If you want to add a new one, click the “Enable APIs and Services” button at the top of the page. You can browse the list or search for the name. I’ll just pick the “Google Calendar API”. And then click the enable button. </p>
<p>Now some APIs require more than just enabling them. You might need to do some extra steps. For example, some will ask you to accept their terms of service. Others may require additional signup. Like here, I need to create some credentials to get it fully working. But we can verify that the API was enabled by going back to the Dashboard. And here we can see that the Google Calendar API has been enabled. </p>
<p>When you no longer have use for an API, you can disable it as well. To disable an API for your project, select the API. Then click the “Disable API” link at the top of the page. We can confirm it was disabled by double checking the dashboard. And there you go. The Google Calendar API is no longer listed.</p>
<p>Remember that you only want to enable the APIs you actually need in the projects that will use them. When an API is disabled, you can’t accidentally use it and avoid wasting money.</p>
<h1 id="Manage-Billing"><a href="#Manage-Billing" class="headerlink" title="Manage Billing"></a>Manage Billing</h1><p>Using Google Cloud Platform costs money. At the time of this recording, signing up for a free trial gets you $300 worth of credit for up to 90 days. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a> also has a free tier which gives you limited access to some GCP services as well. This free trial and free tier should provide enough to experiment with and learn, but eventually, you will need to start paying. This becomes especially true once you begin building and deploying production-ready workloads.</p>
<p>Obviously, it is important to understand how to set up billing, monitor your spending, and get detailed information on any charges. I will cover all of that in this lesson.</p>
<p>First, let me demonstrate how to add a new billing account to a project. Now if you signed up for an account, you already set up a billing by providing a credit card. But sometimes you will want to have your projects use different billing accounts.</p>
<p>Let’s start by clicking on “Billing” in the Nav menu. Then open the drop down and click on ”Manage billing accounts”. You can see my current billing account. I want to create a new one, so I will click on “Create Account”. You need to assign a name. Then select a country. This determines the currency you will be charged in. Now review the information in payments profile. It suggests using the credit card I previously entered. But I want to enter a different one, It already has my name and billing address, so I don’t need to add those. Finally, click “Submit and Enable Billing”.  And you can see that I now have two billing accounts, instead of just one.</p>
<p>Next, I want to link my new billing account to one of my projects. Click on “Account Management” to view the projects that are currently linked to your billing account. Since this is the new account, no projects are listed. I need to select the old account, to see my current projects. Now I can modify my “Photo blog” project to use the new billing account. Let me jump back to the new billing account. And we confirm that “Photo blog” is linked to it.</p>
<p>You can also disable billing. To disable billing for a project, click the Actions button to the right of the project you want to change. Then select “Disable billing”. And confirm by clicking the “Disable Billing” button. When you disable billing it simply stops automatic payments. You are still responsible for all outstanding charges for the project. So if you want to make sure you do not receive any more charges, you should also delete the project.</p>
<p>So now that you understand how to manage billing accounts, let’s cover setting a budget. Setting a budget does not create a hard cap on spending. You can still spend more than what the budget allows. Think about it. You would NOT want a production system to suddenly shut down because it exceeded it’s budget. Budgets are used for generating alerts when costs begin to rise. If you want to actually stop spending, you will need to delete the project.</p>
<p>To create a budget for a project, from the Billing page, click on “Budgets and alerts”. Then click on “Create Budget”. You will need to give your new budget a name. Then select the scope for your budget to cover. In this case, I will leave it set to “All services”. You can also tell it to filter out any discounts or promotions if you wish.</p>
<p>Next, you specify the amount of your budget. For this example, I will pick $100 because it is a nice round number.</p>
<p>Under actions, you define how and when you will be notified. By default, administrators and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/adding-users/">users</a> with billing account roles will receive email alerts when the amount spent exceeds 50%, 90%, and 100% of the budget. You can change or add more thresholds if you wish. Let me add a new notification at 25%. Remember, the budget won’t actually cap your spending. Once you receive a notification, it is up to you to decide what to do.</p>
<p>You also can choose how you want to be notified. You can send emails or you can publish a message to a Pub&#x2F;Sub topic.</p>
<p>Click the Finish button and your budget will be set. Now I will get my first notification email when my spend exceeds $25. If you are using a free trial, you may want to create an alert so you do not end up getting an unexpected bill at the end of the month.</p>
<p>Next, I’ll show you how to generate a billing export for a project. Billing exports are detailed copies of your charges that live inside a BigQuery database. Once you have your charges in a database, you can easily filter and sort the details. You can also use DataStudio to create charts and graphs to generate reports.</p>
<p>To create a billing export, select “Billing export” from the Billing page. From here you can choose between the different types. I can pick either to export all my costs, or I can export any price changes. For this example, I want a full list of all my costs, so I’ll pick the first option.</p>
<p>Next, I’ll confirm the project I want to perform the export on. And then I have to pick a BigQuery table to export to. I do not have any BigQuery tables set up, so I will choose “Create New Dataset”. Now I have to pick a name for the new table. This must only include letters, numbers and underscores. Then I can pick where the data will reside. If you want, you can set an expiration date for the table. And finally, you can pick an encryption method. I do not want anything fancy, so I am going to stick with the defaults. Click the “Create Dataset” to confirm.</p>
<p>I can confirm that the new table has been created by clicking on the Dataset name. Now if I want, I can use BigQuery to view my bill details. I’m not going to actually demonstrate that, since using BigQuery is out of scope. If you understand how to write SQL queries, you shouldn’t have too much trouble accessing this information yourself.</p>
<p>So that’s it. You know the basics for managing billing on Google Cloud Platform.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I hope you enjoyed learning about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/introduction/">how to set up a Google Cloud Platform environment</a>. Let’s do a quick review. </p>
<p>First, I showed you how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/creating-projects/">create and delete projects</a>. You will use projects to manage and organize your GCP account.</p>
<p>Second, I showed you how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/adding-users/">create user accounts</a> and to assign them permissions via roles. You should try to remember the three types of roles: Basic, Predefined, and Custom. Basic roles include the Browser, Viewer, Editor, and Owner roles. Basic roles are simple to use and understand, but not very secure to use in any sort of real environment. Predefined roles are more granular and are broken up into smaller groups of permissions. If a predefined role matches your need, use that. Otherwise, you can create your own custom roles for when you need something special. It is highly recommended that you avoid using basic roles, and choose predefined or custom roles instead.</p>
<p>Third, we covered enabling APIs to give you access to different Google services. These services include all kinds of things, including compute, storage, networking, and many more. You can control which resources a project has access to by enabling or disabling the associated API.</p>
<p>Fourth, I showed you how to add a billing account. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/setting-up-google-cloud-platform-environment-1697/manage-billing/">Billing accounts</a> allow you to specify how you will pay for the services consumed in each project. Projects can share the same billing account, or can have different accounts, depending upon your needs. You can set up spending alerts for a project by setting a budget. And you analyze your spending habits by performing a billing export.</p>
<p>If you are interested in learning more, I highly recommend reading the GCP Resource Manager documentation and trying out some of the “How-to guides” at <a target="_blank" rel="noopener" href="https://cloud.google.com/resource-manager">https://cloud.google.com/resource-manager</a>.</p>
<p>Well, that’s all I have for you today. Remember to give this course a rating, and if you have any questions or comments, please let us know. Thanks for watching, and make sure to check out our many other courses at Cloud Academy!</p>
<h1 id="4Enabling-APIs"><a href="#4Enabling-APIs" class="headerlink" title="4Enabling APIs"></a>4<strong>Enabling APIs</strong></h1><p><a target="_blank" rel="noopener" href="https://cloud.google.com/apis">API List</a></p>
<h1 id="6Summary"><a href="#6Summary" class="headerlink" title="6Summary"></a>6<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://cloud.google.com/resource-manager">How To Guide</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Tour-of-Google-Cloud-Platform-Services-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Tour-of-Google-Cloud-Platform-Services-4/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Tour-of-Google-Cloud-Platform-Services-4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:32:23" itemprop="dateCreated datePublished" datetime="2022-11-14T12:32:23-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:41:02" itemprop="dateModified" datetime="2022-11-21T02:41:02-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Tour-of-Google-Cloud-Platform-Services-4/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Tour-of-Google-Cloud-Platform-Services-4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to a “Tour of Google Cloud Platform Services”. My name is Daniel Mease and I’ll be taking you through this course. I am a trainer at Cloud Academy with over 20 years of software and web development experience. This course is intended for:</p>
<ul>
<li>Anyone who wants to learn about the main services available on Google Cloud Platform</li>
</ul>
<p>After completing this course, you will be familiar with the primary Google offerings for the following categories:</p>
<ul>
<li>Compute</li>
<li>Storage</li>
<li>Networking</li>
<li>Artificial Intelligence &amp; Machine Learning</li>
<li>Security &amp; Operations</li>
</ul>
<p>The following prerequisites will helpful:</p>
<ul>
<li>Basic understanding of computers, servers and data centers</li>
<li>Basic understanding of cloud principles</li>
</ul>
<p>Feedback on our courses is valuable, both to us as trainers and our future students. If you have any criticisms or suggestions for improvement, we would greatly appreciate it if you would share those with us.</p>
<p>Please note that, when this video was recorded, all course information was accurate. Google is constantly updating its products and services as part of its ongoing drive to innovate. As a result, over time, minor discrepancies may appear in the course content. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. </p>
<p>So, if you notice any information that is outdated, please contact <a href="mailto:&#x73;&#x75;&#112;&#112;&#111;&#x72;&#116;&#x40;&#x63;&#108;&#x6f;&#x75;&#100;&#x61;&#99;&#x61;&#100;&#101;&#x6d;&#x79;&#x2e;&#x63;&#111;&#109;">&#x73;&#x75;&#112;&#112;&#111;&#x72;&#116;&#x40;&#x63;&#108;&#x6f;&#x75;&#100;&#x61;&#99;&#x61;&#100;&#101;&#x6d;&#x79;&#x2e;&#x63;&#111;&#109;</a>. This will allow us to make all necessary fixes to the course during its next release cycle.</p>
<h1 id="What-is-GCP"><a href="#What-is-GCP" class="headerlink" title="What is GCP?"></a>What is GCP?</h1><p>Before I start talking about its services, I first want to explain exactly what Google Cloud Platform is. Google Cloud Platform (often abbreviated as GCP) is a public cloud vendor. This means it is a collection of virtual, on-demand services that anyone can use to build, host, and deliver applications. The main advantage of using GCP is that it gives you access to the same hardware and software that Google uses to host all of its services. This includes Google Search, Gmail, Google Calendar, and Google Docs. In addition to the global network, you also get to leverage Google’s vast experience with serving applications to billions of people all around the world.</p>
<p>With Google Cloud Platform, you are spared the hassle and cost of building and maintaining your own data centers. Everything is on-demand. Any resources you need are available wherever you need them to be, and they can be accessed at any time. Plus, GCP has hundreds of different services to choose from: Artificial Intelligence, Machine Learning, Big Data, Internet of Things, Healthcare, Gaming. The list goes on and on.</p>
<p>The number of options available is almost overwhelming. That is why in this course, I am going to be giving you a virtual tour of the main offerings. I have divided everything into five main categories: </p>
<ol>
<li>Compute</li>
<li>Storage</li>
<li>Networking</li>
<li>AI &amp; ML</li>
<li>Security &amp; Operations</li>
</ol>
<p>In the next few lessons, we will go through each category in more detail. </p>
<h1 id="Compute"><a href="#Compute" class="headerlink" title="Compute"></a>Compute</h1><p>In this lesson, I am going to cover the main “Compute” offerings available on Google Cloud Platform. Compute services are services used for running code. So, maybe you have purchased some software and you need the hardware to run it. Or maybe you are developing your own custom software and need machines to build and test it. Any project that requires processing power and memory can take advantage of a Compute service.</p>
<p>When most people think about Cloud computing, they immediately think of virtual machines. You can configure virtual machines to do pretty much anything you want. Google’s main service for creating your own VMs is called Compute Engine. Compute Engine VMs can either run Linux or Windows. And there are many different predefined machine types to choose from, each with different amounts of processing power and memory. General-purpose machines offer the best price-performance ratio. While optimized machines offer higher levels of performance for either compute, memory or graphics. You can also create custom machine types as well.</p>
<p>Compute Engine is “Infrastructure-as-a-Service”. That means you can use it to build pretty much whatever you want. You can spin up a powerful Windows instance and install SQL Server. Or you can spin up a smaller Linux instance and install Apache web server. The possibilities are near endless.</p>
<p>So that’s great if you want to build everything from scratch, but what if you want to move over some existing infrastructure? Luckily, Google has made it easy to migrate. If you are currently using VMware, then you can use Google Cloud VMware Engine to run any existing VMs. Now if you are running bare metal servers or have another type of VM, then you can migrate those over to Compute Engine using Migrate for Compute Engine. This service makes it really fast and easy to move your workloads over. It creates a new instance on GCP while copying the data from your original VM. Once the data transfer is finished, the GCP instance reboots, and then your migration is complete. In addition to VMware and physical machines, Migrate for Compute Engine can also support VMs running on Amazon Web Services and Microsoft Azure.</p>
<p>Virtual machines are not the only compute resource. These days, containers have become increasingly popular. Containers are somewhat like virtual machines, except they don’t include the operating system. This means that they lose a little bit of flexibility, but the upside is that containers are much more lightweight. They take up less disk space and they are faster to start up and shut down.</p>
<p>Google Kubernetes Engine (or GKE) is used for running containers on Google. It makes it easy to deploy, maintain and scale your containerized applications. It also includes features for logging, monitoring, and health management. Now, some people want to spread out their containers to run across multiple cloud providers. And Google has created a product just for this. This is called Anthos. With Anthos, you can run many different containers across multiple locations. So you could have some running on-premises, some on GCP, and some on AWS.  Anthos provides a single command interface that controls and monitors everything. So if you are building a hybrid environment, you might want to check Anthos out. </p>
<p>If you are interested in converting some of your VMs to containers. Google makes this easy with Migrate for Anthos. Just pick a VM and it will be transformed into a GKE container. You can also combine Migrate for Compute Engine with Migrate for Anthos to convert a physical server to a container, if you wish.</p>
<p>While Compute Engine (CE) and Google Kubernetes Engine (GKE) give you a lot of power, they do require quite a lot of work. You have to provision instances. You have to be able to pick the right machine types. You have to apply the latest patches and security updates. If you already know how to do all this, then it is probably not a big deal. But a lot of times customers don’t know and don’t care about this level of detail. This is where Google’s “serverless” options come in. </p>
<p>Google has a “Platform-as-a-Service” offering called App Engine. App Engine lets you run web and mobile applications without having to worry about the underlying infrastructure. You just upload your code and let GCP take care of the rest. It will provision and scale the underlying resources up and down automatically. When there is little to no traffic, App Engine will scale down so that you are charged less. When your app starts receiving lots of traffic, it will scale up to handle the load. Compute Engine gives you far more control and flexibility. But App Engine provides simplicity and ease-of-use.</p>
<p>Similarly, Google has a serverless offering for containers, called Cloud Run. If you have a complex application that involves many different containers, then you’ll probably want to stick with using GKE. But if your application runs inside a single container, then Cloud Run is much easier to use.  Just upload your container and let Cloud Run deploy it as a stateless, auto scaling service.</p>
<p>So App Engine is for running applications and Cloud Run is for running containers. But what if you just want to call a simple function? Well, there is a third serverless option for that as well. Cloud Functions can create small, single-purpose functions that respond to events. For example, you could configure a Cloud Function to be triggered whenever a file is uploaded or whenever a database is changed. Cloud Functions are great for adding a little extra functionality without having to create something far more complicated.</p>
<p>So, to summarize:</p>
<ul>
<li>If you want to run a Windows application, you should pick Compute Engine.</li>
<li>If you want to run a Java or Python app, then App Engine would probably be the best choice.</li>
<li>For running a single container, I would choose Cloud Run.</li>
<li>For a multiple container application, then you probably want to choose GKE.</li>
<li>Unless you also want to distribute your containers across multiple cloud vendors, then you would pick Anthos.</li>
<li>And if you just want to run a simple function whenever an event occurs, Cloud Functions would be the right tool.</li>
</ul>
<p>Ok, that should cover all the main Compute offerings on GCP.</p>
<h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><p>In this lesson, I am going to cover the main “Storage” services available on Google Cloud Platform. </p>
<p>Storage services are all about storing data. Different types of data require different types of services. For example, data can be divided into two main types: structured and unstructured. Structured data is composed of clearly defined data types and patterns. Think of stuff like names, dates and credit card numbers. Structured data is easy to search and usually is kept in a relational database. Unstructured data is just the opposite. It is more freeform and has no obvious organization. So think about things like music, photos and video.</p>
<p>When most people think of storage, they probably think of files in a filesystem. This would be an excellent example of unstructured data. If you want to read and write files to the cloud, then you can use Google Cloud Storage. Cloud Storage is perfect for serving web pages, archiving log files, or you can even use it as a data lake. A data lake is basically a repository of raw data that can be used for later analysis or machine learning. Essentially, it is just a really big collection of unorganized data.</p>
<p>Cloud Storage is fast, secure, durable, and has almost unlimited capacity. Because there are different types of unstructured data, Cloud Storage offers several different storage classes:</p>
<ol>
<li>Standard storage is best for short-lived or frequently accessed data. This class costs the most, but there is no minimum for length of storage.</li>
<li>Nearline storage is best when you need to store things for a longer period of time (at least 30-days) and need to access it less frequently (eg: once per month or less). Files stored in nearline will cost less than standard.</li>
<li>Coldline storage is even cheaper, but it has a longer storage minimum (90 days) and should only be used for files you need to access four times a year or less.</li>
<li>Archive storage is the cheapest. This is the best choice when you need to keep files for a long time due to compliance. Archive storage must be kept for at least 365 days, and it should only be accessed less than once a year.</li>
</ol>
<p>All four storage classes give you immediate access to your files. This is different from some other cloud providers, where the lowest cost storage can take hours to access.</p>
<p>Now while it stores files, Cloud Storage does not work exactly like a traditional file system. It is considered “object storage”. So there are no actual directories, and everything is kept inside “buckets”. You also cannot edit or update part of a file. You can only delete and recreate it. If you need “block level” storage access with a true hierarchical structure, you can use Filestore instead.  Filestore can be used to create NFS-compatible file shares that can be mounted on your virtual machines and containers. If you just want to store and share files, Cloud Storage is perfect. But if you need lower level block access or say you need to constantly append data to the end of a file, Firestore is a much better match.</p>
<p>Now these options are fine if you just want to store your data in files. But what about databases? Well Google has several services for those too. If you want to store structured data inside organized tables, columns and rows (much like a traditional SQL database) then Cloud SQL could be a good choice. It is fully-managed by Google, so it is very easy to get started. And it supports the most common database engines: MySQL, PostgreSQL, and Microsoft SQL Server. So if you are already using one of those, you can easily migrate to Cloud SQL and then let Google manage the details for you. Cloud SQL is perfect for storing data you need to sort, transform, and search.</p>
<p>If for some reason Cloud SQL isn’t powerful enough, then Google offers something even better: Cloud Spanner. Cloud Spanner is unique because it is a relational database that’s massively scalable. You can get much higher performance than Cloud SQL, however it comes with a higher cost. Cloud Spanner is for companies that need an extremely powerful, multi-regional database that can handle a heavy amount of I&#x2F;O.</p>
<p>Finally, if you need to store and work with structured “Big Data” (which is extremely large and complex data sets) then BigQuery might be your best bet. BigQuery is Google’s data warehouse service. Now, a data warehouse stores data from multiple sources, including data lakes and databases, and can be used to analyze huge, multidimensional datasets. Databases are optimized for transactions. But data warehouses are optimized for analytics. BigQuery can process petabytes of data and it integrates with many other services. For example, Looker (a business intelligence solution) works with BigQuery to help you explore your data and share insights in real time.</p>
<p>Of course, Google also supports non-relational or “NoSQL” databases as well. NoSQL databases do not use tables, columns, rows, or schemas to organize data and are particularly useful for storing unstructured data. NoSQL supports flexible data models, it scales horizontally, and it has incredibly fast queries. However, searching, sorting and joining data is much more limited.</p>
<p>There are three main options here, and they include:</p>
<ol>
<li>Firestore, which is ideal for building client-side mobile and web applications</li>
<li>Firebase, which is best for syncing data between users in real-time, such as in collaboration apps</li>
<li>Bigtable, which is best for running large analytical workloads</li>
</ol>
<p>Time to summarize the main storage options:</p>
<ol>
<li><p>If you need to store files for sharing, then Cloud Storage will probably be your best option.</p>
</li>
<li><p>If you need to edit and update files, then you might want to look at using Filestore. </p>
</li>
<li><p>For SQL databases you have three main choices:</p>
</li>
<li><ol>
<li>Cloud SQL is great for when you need a standard MySQL, PostgreSQL, or Microsoft SQL Server</li>
<li>Cloud Spanner will handle larger workloads that Cloud SQL can’t</li>
<li>Big Query is great for creating a data warehouse for analytics</li>
</ol>
</li>
<li><p>You also have three choices when it comes to noSQL databases as well:</p>
</li>
<li><ol>
<li>Firestore is great for smaller applications (such as web and mobile)</li>
<li>Firebase is better for larger datasets</li>
<li>Bigtable is best for running very large analytical workloads</li>
</ol>
</li>
</ol>
<p>That should cover the main storage offerings on GCP. </p>
<h1 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h1><p>In this lesson, I am going to cover the main “Networking” offerings on Google Cloud Platform.</p>
<p>Google networking services are focused on allowing your other resources to connect and communicate. Your Compute and Storage resources can be as accessible or isolated as you need. You can reroute traffic when a VMs fails. Or throttle high traffic so that a database is not overwhelmed. Also, networking services include setting policies and rules that ensure that only authorized connections are allowed.</p>
<p>Virtual Private Clouds (or VPC for short) allow you to organize and share your resources. VPCs are networks that are logically isolated from each other. VPCs can be used to group or separate VMs and containers. You can also divide a VPC into sub-networks, define routes, and specify how traffic should flow between them.</p>
<p>By default, all incoming traffic to a VPC is blocked, and all outgoing traffic is allowed. This protects your VPC from outside access, but allows your resources to still connect to the internet. By creating firewall rules, you have the ability to override this default behavior. So you can block outbound traffic in case you want to prevent a resource from accessing the internet. Or you can allow external access to a public web server. You can also control which VPCs can connect to each other using a service called VPC Network Peering.</p>
<p>Now, what if you want to connect a VPC to an external network outside of Google? For example, maybe you wish to route traffic between your own private data centers. There are several ways of accomplishing this. First, you can create a secure connection to a VPC using Cloud VPN. VPN stands for Virtual Private Network. A VPN uses the public internet to send encrypted traffic back and forth. This works, but it does mean that any slowdown or disruption of the internet can affect your internal connections as well. So, if you want even greater security and reliability, you can use Cloud Interconnect instead. Cloud Interconnect is much more expensive than a VPN, but it provides a direct, dedicated connection to Google which results in higher speed and reliability. Now as a third option, you can also choose Direct Peering by coordinating with your local internet service provider. Peering is free, but it depends on your ISP. It’s not really well-integrated with GCP and requires a lot more setup.</p>
<p>Google has many other networking services as well. For example, Load Balancers help distribute network traffic among groups of resources, so that no individual part of your infrastructure gets overwhelmed. Cloud Armor works with your load balancers to provide built-in defenses against application and Denial of Service attacks. Cloud DNS allows you to create and manage millions of DNS records, for both public and private domains. And Cloud CDN can accelerate your web and application content by using Google’s globally distributed caches.</p>
<p>Ok, so let’s do a quick review of everything that was covered:</p>
<ul>
<li>You group your VMs into VPCs and then create firewall rules to isolate or connect them to the internet</li>
<li>Cloud VPN, Interconnect, and Peering all can be used to connect your corporate network to GCP</li>
<li>Load balancers are used for distributing traffic across multiple resources</li>
<li>Cloud Armor helps block internet attacks</li>
<li>Cloud DNS manages domain names</li>
<li>Cloud CDN uses caching to accelerate content delivery</li>
</ul>
<p>So those are the most commonly used Networking services on GCP.</p>
<h1 id="Artificial-Intelligence-and-Machine-Learning"><a href="#Artificial-Intelligence-and-Machine-Learning" class="headerlink" title="Artificial Intelligence and Machine Learning"></a>Artificial Intelligence and Machine Learning</h1><p>In this lesson, I want to talk about some of the most exciting services in GCP: that is Artificial Intelligence and Machine Learning. Google is known for being on the cutting edge of this space. The world was shocked when its AlphaGo software defeated one of the best human players of the board game, Go.</p>
<p>For those who don’t already know, Artificial intelligence (usually abbreviated as AI) is a field of study that attempts to give machines the same level of intelligence that humans possess. Essentially, the goal of AI is to create machines that are capable of acting on their own, without direct human intervention. Now AI is sort of a wider field of study, where Machine Learning (or ML) is a specific branch in that field. Machine learning is mostly focused on developing systems that can learn. Now by “learn”, I mean that they use large amounts of data to slowly improve their responses.</p>
<p>Currently, Google divides its AI and ML services into four categories: Sight, Language, Conversation, and Structured Data.</p>
<p>Under Sight, they have services like the Vision API, which can detect objects, faces, and text in images. The Video Intelligence API recognizes objects, places, and actions in video. Document AI uses machine learning to help you parse and identify data inside your documents, such as company names, addresses, and phone numbers.</p>
<p>Under Language, they have the Translation API, which can convert text from one language to another. It currently supports over one hundred different languages. They also have the Natural Language API, which performs tasks like sentiment analysis. For example, it can classify a message as being either positive, negative, or neutral.</p>
<p>The Conversation category includes the Text-to-Speech and Speech-to-Text APIs, which do exactly what you would expect. It also includes a service called DialogFlow, which can generate realistic sounding dialogue. Using DialogFlow, you can build chatbots and voicebots to handle things like customer support requests.</p>
<p>The last category is Structured Data. These services allow you to feed in structured data and get back insights. For example, the Recommendations AI can look at a customer’s previous purchases and recommend other products that they might be interested in buying. Also, Cloud Talent Solutions can automatically detect and understand job content and applicant intent. Candidates can quickly find the most appropriate jobs, while companies can identify, attract and hire quality candidates.</p>
<p>All of these prebuilt AI services have been trained based on general sets of data, so they might not be able to handle your specific needs. Now for these scenarios, Google provides its AutoML suite of services. For example, you might need to identify product defects in your factory assembly lines. By feeding your own data into AutoML Vision, you can train a custom model without having to know machine learning.</p>
<p>If you need to build custom models that are outside the scope of the AutoML suite, then you can use Vertex AI (which was previously named the AI Platform suite). It includes many different services, but the most important ones are AI Platform Training and AI Platform Prediction. The Training service lets you train custom models using popular Python-based frameworks. Or you can also access a repository of plug-and-play AI components (including end-to-end AI pipelines and out-of-the-box algorithms) by using AI Hub.</p>
<p>Alright, so let’s review all the AI and ML services I just talked about. Remember, you got the four main categories:</p>
<ul>
<li>Sight services, including the Vision API, Document API, and Video Intelligence API</li>
<li>Language services, including the Translation API and Natural Language API</li>
<li>Conversation services, include Text-to-Speech, Speech-to-Text and DialogFlow</li>
<li>Structured Data services, including Recommendations AI and Talent Solutions</li>
</ul>
<p>Now, remember, if the standard APIs are not enough, you can also train your own models using AutoML. And you can access AI Hub for plug-and-play AI components as well. And that should cover the main AI &amp; ML services on GCP.</p>
<h1 id="Security-and-Operations"><a href="#Security-and-Operations" class="headerlink" title="Security and Operations"></a>Security and Operations</h1><p>In this lesson, I am going to talk about some of the Security and Operations services available on GCP.</p>
<p>First, let’s talk about security. Security services exist to help you protect your data. One of the main concerns with using the cloud is privacy. You want to be able to control exactly who can access your data. So privacy is the goal, and security is the method. Google security services allow you to define policies, procedures, and controls that everyone must follow. In order to protect your customers and your company, you need to properly implement security. The policies you define will often be shaped by compliance requirements. Compliance refers to a third party set of standards, usually by some sort of regulatory authority. For example, different countries have different laws about storing and encrypting data. Digital security is complicated, so there are many different GCP services to assist you.</p>
<p>Security Command Center provides a centralized control panel to help you discover vulnerabilities, detect threats, and generate reports. Secret Manager gives you centralized storage for things like passwords, API keys, and certificates. The Data Loss Prevention service helps you identify and scrub sensitive data. For example, if your user records contain credit card numbers, you could configure DLP to remove them before responding to a database query.</p>
<p>Next, I want to talk about operation services. Now operations is focused on monitoring and maintaining your existing infrastructure. </p>
<p>So you have things like Google Cloud Operations Suite which was previously called Stackdriver. Now this suite includes:</p>
<ol>
<li>Cloud Logging: This acts as a centralized repository for all your logs. It makes it easy to find your logs and to search the entries for any messages you are interested in. </li>
<li>Cloud Monitoring: This gives you an overview of everything that is happening. You can view metrics like CPU utilization, response latency, and network traffic. Plus, you can use it to set up alerts to notify you where there are any problems. If a metric is too low or too high, you will know about it right away.</li>
<li>Cloud Debugger: This helps you easily track down software bugs by allowing you to inspect the state of a running application.</li>
<li>Cloud Profiler: This can be used to identify latency issues inside of an application.</li>
<li>Cloud Trace: This helps you track down latency issues that exist between two or more applications.</li>
</ol>
<p>Now there are other operation tools as well. These include things like:</p>
<ul>
<li>Cloud Deployment Manager which allows you to write templates and then use those to provision resources. You can have a template for creating a VM, and then run it 20 times to get 20 VMs. This is extremely useful for when you need to create a lot of the same resources. Now you no longer have to create everything by hand. </li>
<li>Cloud Build is a tool that can help automate building, testing and deploying your code. </li>
<li>Apigee helps you design secure and scale application programming interfaces (or APIs). This way your web services can easily communicate with each other.</li>
<li>Cloud Composer can be used to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers.</li>
</ul>
<p>Ok, so let’s quickly review the Security and Operations services I just talked about:</p>
<p>Under Security, you have:</p>
<ul>
<li>Security Command Center provides a centralized overview of security issues</li>
<li>Secret Manager for storing passwords and keys</li>
<li>Data Loss Prevention for identifying and hiding sensitive data</li>
</ul>
<p>Under Operations, we talked about:</p>
<ul>
<li><p>Cloud Operations Suite</p>
</li>
<li><ul>
<li>Includes Logging &amp; Monitoring</li>
<li>Debugging for finding bugs in an app</li>
<li>Profiling for identifying slowdowns in an app</li>
<li>Trace for identifying bottlenecks between applications</li>
</ul>
</li>
<li><p>Deployment Manager for creating templates and automating infrastructure changes</p>
</li>
<li><p>Cloud Build for automating software deployments</p>
</li>
<li><p>Apigee for building APIs</p>
</li>
<li><p>Cloud Composer for managing workflows</p>
</li>
</ul>
<p>And that just about covers the main Security and Operations services available on GCP.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I hope you enjoyed learning about the main services available on Google Cloud Platform. This lesson will be a final recap of everything that was covered.</p>
<p>In the first lesson, you learned about Google’s main Compute offerings:</p>
<ul>
<li>Compute Engine is used for directly managing virtual machines</li>
<li>Kubernetes Engine is used for directly managing containers</li>
<li>Anthos is used for managing containers running in hybrid environments</li>
</ul>
<p>Now, if you don’t want to directly manage the services yourself, you can choose one of the managed, serverless options:</p>
<ul>
<li>App Engine is for running web or mobile applications</li>
<li>Cloud Run is for running single containers</li>
<li>Cloud Functions is used for triggering functions based off of certain events</li>
</ul>
<p>In the second lesson, we covered the main Storage offerings:</p>
<ul>
<li>Cloud Storage is used for storing files</li>
<li>Cloud SQL, Cloud Spanner and Big Query are used for SQL databases</li>
<li>Firestore, Firebase and Bigtable are used for noSQL databases</li>
</ul>
<p>In the third lesson, we talked about Networking services:</p>
<ul>
<li>VPCs are used to isolate or connect your VMs</li>
<li>Cloud VPN, Interconnect, and Peering can connect your company network to GCP VMs</li>
<li>Load balancers can distribute traffic across multiple VMs</li>
<li>Cloud Armor can help protect your VMs from internet attack</li>
</ul>
<p>In the fourth lesson, we covered Google’s AI and ML services:</p>
<ul>
<li>Sight, Language, Conversation, and Structured Data Services</li>
<li>AutoML for training your own models</li>
<li>AI Hub for finding plug-and-play components</li>
</ul>
<p>Finally, we covered Security and Operations:</p>
<ul>
<li>Secret Manager provides secure storage for passwords and keys</li>
<li>Cloud Debugger allows you to inspect the state of a running application</li>
<li>Cloud Profiler can help you identify latency issues inside an application</li>
<li>Cloud Trace helps you track down latency issues in between applications</li>
<li>Apigee is for building scalable and secure APIs</li>
</ul>
<p>Well, that’s all I have for you today. Remember to give this course a rating, and if you have any questions or comments, please let us know. Thanks for watching, and make sure to check out our many other courses on Cloud Academy!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/129/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/129/">129</a><span class="page-number current">130</span><a class="page-number" href="/page/131/">131</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/131/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
