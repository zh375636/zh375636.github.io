<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/138/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/138/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-720-Troubleshoot-name-resolution-issues-in-Microsoft-Azure-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-720-Troubleshoot-name-resolution-issues-in-Microsoft-Azure-2/" class="post-title-link" itemprop="url">AZ-720-Troubleshoot-name-resolution-issues-in-Microsoft-Azure-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:34:37" itemprop="dateCreated datePublished" datetime="2022-11-14T13:34:37-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 21:37:00" itemprop="dateModified" datetime="2022-11-27T21:37:00-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-720/" itemprop="url" rel="index"><span itemprop="name">AZ-720</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-720-Troubleshoot-name-resolution-issues-in-Microsoft-Azure-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-720-Troubleshoot-name-resolution-issues-in-Microsoft-Azure-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Completed100 XP</p>
<ul>
<li>3 minutes</li>
</ul>
<p>As an Azure network support engineer, you are responsible for ensuring that computers and devices on your network stay connected. Name resolution is the process of matching the name of a computer to its network address. If a name cannot be resolved, the computer will not be visible on the network, causing a breakdown in connectivity. Name resolution may be:</p>
<ul>
<li>Public, using public Domain Name System (DNS) servers.</li>
<li>Internal to Azure, using Azure infrastructure.</li>
<li>On-premises, using your own DNS server.</li>
</ul>
<p>You might also adopt a hybrid approach, using a combination of either private and public DNS, or on-premises and Azure DNS servers.</p>
<p>In this unit you will learn how to troubleshoot internal Azure name resolution issues and public DNS issues.</p>
<h2 id="Learning-objectives"><a href="#Learning-objectives" class="headerlink" title="Learning objectives"></a>Learning objectives</h2><p>By completing this module, you’ll learn how to:</p>
<ul>
<li>Troubleshoot built-in Azure name resolution issues</li>
<li>Troubleshoot custom Azure name resolutions issues</li>
<li>Troubleshoot Azure Private Zone DNS</li>
<li>Troubleshoot Public DNS issues</li>
<li>Review the Azure DNS logs</li>
</ul>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><ul>
<li>Demonstrate an understanding of the OSI model</li>
<li>Demonstrate an understanding of Azure CLI</li>
<li>Demonstrate an understanding of PowerShell</li>
<li>Know how to run Cloud Shell to run commands</li>
</ul>
<h1 id="Azure-name-resolution-issues"><a href="#Azure-name-resolution-issues" class="headerlink" title="Azure name resolution issues"></a>Azure name resolution issues</h1><p>Completed100 XP</p>
<ul>
<li>3 minutes</li>
</ul>
<p> Important</p>
<p>You need your own <a target="_blank" rel="noopener" href="https://azure.microsoft.com/free/">Azure subscription</a> to complete the exercises in this module. If you don’t have an Azure subscription, you can still view the demonstration video in the exercise unit.</p>
<ol>
<li>Select <strong>Sign in</strong> to activate sandbox and sign in using your own credentials.</li>
<li>Select <strong>Review permissions</strong>, read and accept to continue.</li>
<li>Azure Cloud Shell will authenticate and build and you are ready to start the exercise.</li>
</ol>
<p>You’ll troubleshoot a name resolution problem in a later exercise. Use the following instructions and the Azure Cloud Shell on the right of the screen to create the environment for this exercise.</p>
<p> Note</p>
<p>The resource group <strong>sandbox-rg</strong> has been created automatically. Use this as your resource group name. You won’t have permission to create additional resource groups in the sandbox.</p>
<p>In the sandbox to the right, follow these steps:</p>
<h2 id="Create-the-exercise-environment"><a href="#Create-the-exercise-environment" class="headerlink" title="Create the exercise environment"></a>Create the exercise environment</h2><p>Using the Cloud Shell on the right, run these commands to create the example topology. The environment will take approximately 13 minutes to build.</p>
<ol>
<li><p>Clone the setup script from GitHub.</p>
<p>ConsoleCopy</p>
<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/MicrosoftDocs/mslearn-name-resolution-issues networking</span><br></pre></td></tr></table></figure>
</li>
<li><p>Run the setup script.</p>
<p>ConsoleCopy</p>
<figure class="highlight console"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">networking/setup.ps1</span><br></pre></td></tr></table></figure>
</li>
<li><p>The script will create all the resources. Wait until it completes, you should see a Lab Environment Created message.</p>
</li>
<li><p>You can continue to the next unit as this command runs in the background.</p>
</li>
</ol>
<h1 id="Troubleshoot-public-DNS-in-Microsoft-Azure"><a href="#Troubleshoot-public-DNS-in-Microsoft-Azure" class="headerlink" title="Troubleshoot public DNS in Microsoft Azure"></a>Troubleshoot public DNS in Microsoft Azure</h1><p>Completed100 XP</p>
<ul>
<li>7 minutes</li>
</ul>
<h2 id="Troubleshoot-issues-with-Domain-Name-System-records-at-public-providers"><a href="#Troubleshoot-issues-with-Domain-Name-System-records-at-public-providers" class="headerlink" title="Troubleshoot issues with Domain Name System records at public providers"></a>Troubleshoot issues with Domain Name System records at public providers</h2><p>Public DNS servers convert domain names to IP addresses to allow people to find web sites and other addresses on the internet. If your VM has internet connectivity, it will need to resolve names from public DNS servers. If you have problems with resolving names from public DNS servers, try the following:</p>
<ol>
<li><p>Check your VM has internet connectivity.</p>
</li>
<li><p>From a command prompt, ping the IP address of the website you are trying to reach. If ping does not get a response, this tells you that the website is not available.</p>
</li>
<li><p>Check whether the DNS server is the primary server for the zone, or a secondary copied DNS server. Updates are made to the primary DNS server, so this will always include the latest changes and be the most up to date. The primary DNS server is also known as the Authoritative DNS. The secondary DNS server is a read-only copy of the primary DNS server.</p>
</li>
<li><p>From a command prompt, run nslookup:</p>
<p>nslookup <em>name IP address of the DNS server</em>;</p>
<p>For example:</p>
<p>nslookup app1 10.0.0.1</p>
<p>If you get a failure or time-out response, check for recursion problems. Check the server that was used in your original query forwards queries by examining the Forwarders tab in the Server properties in the DNS console. If the Enable forwarders check box is selected, and one or more servers are listed, this server forwards queries.</p>
<p>With recursion, all DNS servers that are used in the path of a recursive query must be able to respond and forward correct data. If they can’t, a recursive query can fail because:</p>
<ul>
<li>The query times out before it can be completed.</li>
<li>A server failed to respond.</li>
<li>A server provides incorrect data.</li>
</ul>
<p>Nslookup will also tell you whether the name has been resolved by an authoritative or non-authoritative server.</p>
</li>
<li><p>Flush the resolver cache by running the dnscmd &#x2F;clearcache command in an Elevated Command Prompt window:</p>
<ul>
<li>Right select Command Prompt and choose ‘Run as administrator’</li>
</ul>
<p>Type dnscmd <em>servername</em> &#x2F;clearcache</p>
<p>The <em>servername</em> is its IP address, FQDN, or hostname. If omitted, the local server is used.</p>
<p>You can also use <strong>ipconfig -flushdns</strong> to clear the DNS cache.</p>
<p>Alternatively, in an Elevated PowerShell window, run the following cmdlet:</p>
<p>Clear-DnsServerCache</p>
<p>Once you have cleared the cache, try again.</p>
</li>
<li><p>Check the DNS server address. If necessary, correct the address.</p>
</li>
<li><p>The <a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/linux-dig-command-examples">dig command</a> (domain information groper) gives DNS information which is helpful in troubleshooting problems. It is a free utility which needs to be installed on Windows machines. To get dig information, run the following command:</p>
<p>dig <em>domain name or IP</em></p>
<p>DIG displays the <strong>status</strong> of whether a query was successful.</p>
<p>The <strong>ANSWER SECTION</strong> shows a response to a request sent in the <strong>QUESTION SECTION</strong>.</p>
<p>The <strong>SERVER</strong> displays the address for the public DNS server.</p>
<p>By default, dig looks up the <strong><a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/dns-record-types">A record</a></strong> for a domain and shows which IP address the domain points to when resolving the name.</p>
<p>Use <strong>dig</strong> with the <strong>+trace</strong> tag to see the full path to the destination:</p>
<p>dig contoso.com +trace</p>
<p>To check the delegated name servers, use the ns option:</p>
<p>dig contoso.com ns</p>
</li>
<li><p>If a computer cannot be reached, try using the commands IPCONFIG &#x2F; RELEASE and IPCONFIG &#x2F;RENEW from the computer’s command prompt to refresh cached information.</p>
<p> Important</p>
<p>IPCONFIG &#x2F;RELEASE will disconnect the computer from the internet. Run IPCONFIG &#x2F;ALL to check the current information.</p>
</li>
<li><p>To check the IP configuration of a computer, run ipconfig &#x2F;all at a command prompt. Scroll down to verify the IP address, subnet mask, and default gateway. The default gateway will be a router or firewall.</p>
</li>
<li><p>The tracert tools displays the route from source to destination. When troubleshooting DNS issues, it helps to identify where packets stopped on the network. Install the traceroute tool using the <a target="_blank" rel="noopener" href="https://phoenixnap.com/kb/how-to-manage-packages-ubuntu-debian-apt-get">apt package manager</a>.</p>
<p>If running on a Windows machine:</p>
<p>tracert <em>domain name or IP</em></p>
</li>
<li><p>If the computer uses the ISP’s DNS, and you cannot resolve the issue, contact the ISP.</p>
</li>
</ol>
<h2 id="Troubleshoot-domain-configuration-issues"><a href="#Troubleshoot-domain-configuration-issues" class="headerlink" title="Troubleshoot domain configuration issues"></a>Troubleshoot domain configuration issues</h2><p>To use a domain name with Microsoft 365 products, you must set up your custom domain names in the Microsoft 365 admin center:</p>
<ol>
<li>From the Microsoft 365 admin center, select <a target="_blank" rel="noopener" href="https://go.microsoft.com/fwlink/p/?linkid=2171997">Setup</a>.</li>
<li>Under Get your custom domain set up, select View &gt; Manage &gt; Add domain.</li>
<li>Enter the domain name, and then select Next.</li>
<li>If your domain registrar is available, sign in to your domain registrar, and then select Next.</li>
<li>Choose the services for your new domain.</li>
<li>Select Next &gt; Authorize &gt; Next, and then Finish. Your new domain has been added.</li>
</ol>
<p>You can check for problems with your domain by checking its status. Go to Setup &gt; Domains and view the notifications in the Status column. If you see an issue, select the three dots (more actions), and then select Check health to display any issues occurring with your domain.</p>
<p>There are a couple of common reasons that domain verification doesn’t work as it should:</p>
<ul>
<li>The verification record value is incorrect. Check the value in the TXT verification record at your DNS host. This should include the “MS&#x3D;” part of the record.</li>
<li>The record hasn’t been saved. Some DNS hosts require you to save the zone file where the DNS record is stored. If this has not been saved, it will not be visible by Microsoft 365.</li>
<li>The record hasn’t updated across the Internet. It can take a few minutes or a few hours for a new record to be visible.</li>
</ul>
<h2 id="Troubleshoot-delegation-issues"><a href="#Troubleshoot-delegation-issues" class="headerlink" title="Troubleshoot delegation issues"></a>Troubleshoot delegation issues</h2><p>Azure DNS zones allow you to manage the DNS records for a domain. For domain queries to reach Azure DNS, the parent domain must be delegated to Azure DNS. There are two types of DNS servers:</p>
<ul>
<li>An <strong>authoritative</strong> DNS server hosts DNS zones. It answers DNS queries for records in those zones only.</li>
<li>A <strong>recursive</strong> DNS server doesn’t host DNS zones. It answers all DNS queries by calling authoritative DNS servers to gather the data it needs.</li>
</ul>
<p>Azure DNS provides an authoritative DNS service.</p>
<p>First, you need to know the name servers for your zone. Azure DNS allocates name servers from a pool each time a zone is created.</p>
<p>In the Azure portal, select <strong>All resources</strong>, and then select your DNS zone. Alternatively, enter your domain name in the Filter by name box. Retrieve the name servers from the DNS zone page.</p>
<p>For example, the zone contoso.net might be assigned name servers:</p>
<ul>
<li>ns1-01.azure-dns.com</li>
<li>ns2-01.azure-dns.net</li>
<li>ns3-01.azure-dns.org</li>
<li>ns4-01.azure-dns.info</li>
</ul>
<p>Azure DNS automatically creates authoritative NS records in your zone for the assigned name servers.</p>
<p>Now you have the name servers, you can update the parent domain. Each registrar has its own DNS management tools. From the registrar’s DNS management page, edit the NS records and replace the NS records with the Azure DNS name servers.</p>
<p>The image shows an example DNS query, with contoso.net and partners.contoso.net in Azure DNS zones:</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/3-zones.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/3-zones.png" alt="Diagram of a DNS query in the Azure DN zones."></a></p>
<p>Each delegation has two copies of the NS records - one in the parent zone pointing to the child, and another in the child zone itself. The ‘contoso.net’ zone contains the NS records for ‘contoso.net’, in addition to the NS records in ‘net’. These records are called authoritative NS records and they sit at the apex of the child zone.</p>
<p>To troubleshoot domain delegation, try the following:</p>
<ol>
<li>Check you have updated the domain with the name servers that Azure DNS provides. Use all four name servers, regardless of the name of your domain. Domain delegation doesn’t require a name server to use the same top-level domain as your domain.</li>
<li>Include the trailing period at the end of the name server address. The trailing period indicates the end of a fully qualified domain name. Some registrars append the period if the NS name doesn’t have it. To be compliant with the DNS RFC, always include the trailing period.</li>
<li>Delegations that use name servers in your own zone, sometimes called vanity name servers, are not supported in Azure DNS.</li>
<li>Verify that the delegation is working. Use a tool such as nslookup to query the Start of Authority (SOA) record. The SOA record is automatically created with the zone. Wait at least 10 minutes after delegation has been set up, as it can take a while for changes to propagate through the DNS system.</li>
</ol>
<p>From a command prompt, enter a nslookup command:</p>
<p>nslookup -type&#x3D;SOA contoso.net</p>
<p>The response should look similar to this:</p>
<p>Server: ns1-04.azure-dns.com</p>
<p>Address: 208.76.47.4</p>
<p>contoso.net</p>
<p>primary name server &#x3D; ns1-04.azure-dns.com</p>
<p>responsible mail addr &#x3D; msnhst.microsoft.com</p>
<p>serial &#x3D; 1</p>
<p>refresh &#x3D; 900 (15 mins)</p>
<p>retry &#x3D; 300 (5 mins)</p>
<p>expire &#x3D; 604800 (7 days)</p>
<p>default TTL &#x3D; 300 (5 mins)</p>
<h2 id="Review-Domain-Name-System-audit-logs"><a href="#Review-Domain-Name-System-audit-logs" class="headerlink" title="Review Domain Name System audit logs"></a>Review Domain Name System audit logs</h2><p>Use DNS Analytics to help you to identify:</p>
<ul>
<li>Clients that are trying to resolve malicious domain names.</li>
<li>Identify stale resource records.</li>
<li>Identify frequently queried domain names and talkative DNS clients.</li>
<li>View request load on DNS servers.</li>
<li>View dynamic DNS registration failures.</li>
</ul>
<p>DNS Analytics (Preview) collects, analyzes, and correlates Windows DNS analytic and audit logs and other related data from your DNS servers.</p>
<ul>
<li>You must install the Log Analytics agent on each Windows DNS server you want to monitor.</li>
<li>If you are running Operations Manager, you will need to connect it to Azure Monitor.</li>
</ul>
<p>The solution starts collecting data without the need of further configuration.</p>
<p>You can download DNS Analytics (Preview) from Azure Marketplace. For DNS related insights, the DNS server should be Windows Server 2012 R2 or later.</p>
<p>You can also access DNS Analytics via Azure Monitor and Log Analytics workspaces. In the Azure portal, go to Azure Monitor. On the Insights menu, select Insights Hub, and then select Log Analytics. The Overview page in Azure Monitor displays a tile for each solution installed in a Log Analytics workspace.</p>
<h1 id="Troubleshoot-name-resolution-issues"><a href="#Troubleshoot-name-resolution-issues" class="headerlink" title="Troubleshoot name resolution issues"></a>Troubleshoot name resolution issues</h1><p>Completed100 XP</p>
<ul>
<li>5 minutes</li>
</ul>
<p>Name resolution in Azure is done by one of three methods:</p>
<ul>
<li>Azure built-in name resolution</li>
<li>Azure custom name resolution</li>
<li>Azure DNS private zones</li>
</ul>
<p>In this unit you will learn how to troubleshoot each of these methods.</p>
<h2 id="2-1-Troubleshoot-built-in-Azure-name-resolution"><a href="#2-1-Troubleshoot-built-in-Azure-name-resolution" class="headerlink" title="2.1 Troubleshoot built-in Azure name resolution"></a>2.1 Troubleshoot built-in Azure name resolution</h2><p>Built-in Azure name resolution provides basic authoritative Domain Name System (DNS) capabilities. DNS names and records are managed by Azure. Built-in Azure name resolution does not allow you to control the DNS names or the life cycle of DNS records.</p>
<p>Azure built-in name resolution works with public DNS names and provides internal name resolution for VMs and role instances within the same virtual network or cloud service.</p>
<p>Built-in Azure name resolution has some limitations:</p>
<ul>
<li>The Azure-created DNS suffix cannot be modified.</li>
<li>DNS lookup is scoped to a virtual network. DNS names created for one virtual network can’t be resolved from other virtual networks.</li>
<li>You cannot manually register your own records.</li>
<li>WINS and NetBIOS are not supported. You cannot see your VMs in Windows Explorer.</li>
<li>Host names must be DNS-compatible. Names must use only 0-9, a-z, and ‘-‘, and cannot start or end with a ‘-‘.</li>
<li>DNS query traffic is throttled for each virtual machine (VM). Throttling shouldn’t impact most applications. If request throttling is observed, ensure that client-side caching is enabled.</li>
<li>Only VMs in the first 180 cloud services are registered for each virtual network in a classic deployment model. This limit does not apply to virtual networks in Azure Resource Manager.</li>
<li>The Azure DNS IP address is 168.63.129.16. This is a static IP address, is used in all regions and all national clouds and will not change.</li>
<li>Azure Dynamic Host Configuration Protocol (DHCP) provides an internal DNS suffix (.internal.cloudapp.net) to each VM. This suffix enables host name resolution because the host name records are in the internal.cloudapp.net zone.</li>
</ul>
<p> </p>
<h2 id="2-2-Troubleshoot-Domain-Name-System-private-zones"><a href="#2-2-Troubleshoot-Domain-Name-System-private-zones" class="headerlink" title="2.2 Troubleshoot Domain Name System private zones"></a>2.2 Troubleshoot Domain Name System private zones</h2><p>Azure Private DNS allows you to manage and resolve domain names in a virtual network without the need to add a custom DNS solution. You can use custom domain names, rather than the Azure-provided names.</p>
<p>DNS resolution using a private DNS zone works only from virtual networks that are linked to it. These private DNS zones records can be resolved from the internet, but the private IP address is not routable via the Internet.</p>
<p>You can link a private DNS zone to one or more virtual networks by creating <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/private-dns-virtual-network-links">virtual network links</a>. You can also enable the <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/private-dns-autoregistration">autoregistration</a> feature to automatically manage the life cycle of the DNS records for the virtual machines that get deployed in a virtual network. With autoregistration enabled, Azure DNS will update the zone record whenever a virtual machine gets created, changes its’ IP address, or gets deleted.</p>
<p> Note</p>
<p>A Virtual Network can only have autoregistration enabled on one Private DNS Zone link. If you try to link two private DNS zones to the same VNet, only one of the links will be enabled for autoregistration.</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/4-link-zones.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/4-link-zones.png" alt="Diagram of private DNS zones on the same Vnet."></a></p>
<p>There are limits on how many private DNS zones you can create, how many records sets, and records per record set.</p>
<p>Single-labelled private DNS zones aren’t supported. Your private DNS zone must have two or more labels. For example, contoso.com has two labels separated by a dot. A private DNS zone can have a maximum of 34 labels.</p>
<p>You can’t create zone delegations (NS records) in a private DNS zone. If you intend to use a child domain, you can directly create the domain as a private DNS zone. Then you can link it to the virtual network without setting up a nameserver delegation from the parent zone.</p>
<p>To create a Private DNS zone:</p>
<ol>
<li>In the Azure portal, <strong>type private dns zones</strong> in the search text box and press <strong>Enter</strong>. The <strong>Private DNS zones</strong> blade is displayed.</li>
<li>Select <strong>Create</strong>. The <strong>Create Private DNS</strong> zone blade is displayed.</li>
<li>Type or select the following:<ol>
<li>Subscription</li>
<li>Resource group</li>
<li>Name – this must be unique within the Resource group.</li>
<li>Resource group location</li>
</ol>
</li>
<li>Select <strong>Review + Create</strong>.</li>
<li>Select <strong>Create</strong>.</li>
</ol>
<p>To troubleshoot issues relating to DNS zones:</p>
<ol>
<li>Review the Azure DNS audit logs.</li>
<li>Check that each DNS zone name is unique within its resource group.</li>
<li>Do not create zone names that could affect the DNS resolution of Microsoft services, such as azure.com, and the like.</li>
<li>Do not use a .local domain for your private DNS zone. Not all operating systems support this.</li>
<li>Check you have not reached the maximum number of zones for your subscription. If so, you will see the error message “You have reached or exceeded the maximum number of zones in subscription {subscription ID}.” Either use a different Azure subscription, delete some zones, or contact Azure Support to raise your subscription limit.</li>
<li>“The zone ‘{zone name}’ is not available” indicates that Azure DNS is unable to allocate name servers for this DNS zone. Rename the zone or contact Azure support to allocate name servers for you.</li>
</ol>
<p>To troubleshoot issues related to DNS records:</p>
<ol>
<li>The record set already exists. Record set names must be unique within the zone.</li>
<li>CNAME records must not be created at the apex.</li>
<li>CNAME record sets cannot have the same name as other record sets.</li>
<li>Apex records consist of the ‘@’ character.</li>
<li>The maximum number of records that can be created is shown in the Azure portal, under the ‘Properties’ for the zone. If you’ve reached this limit, then either delete some record sets or contact Azure Support to raise your record limit for this zone.</li>
</ol>
<p> Note</p>
<p>Apex records are records added at the root of the zone.</p>
<p>To troubleshoot resolving DNS records:</p>
<ol>
<li>Check that the fully qualified name, zone name, and record type are correct.</li>
<li>Check that no DNS records have the same name, even if they are of different types.</li>
<li>Check that the DNS records resolve correctly on the Azure DNS name servers.</li>
<li>Check name resolution with a service such as <strong><a target="_blank" rel="noopener" href="https://digwebinterface.com/">digwebinterface</a></strong>. This tests the current state of the name servers by removing proxy servers and cached results.</li>
<li>Check that the name servers are correct for your DNS zone, as shown in the Azure portal.</li>
<li>Check that the DNS domain name has been correctly <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/dns-domain-delegation">delegated to Azure DNS</a>. Use nslookup to validate that the zone is delegated to in-built Azure DNS.</li>
</ol>
<p> Note</p>
<p>If your environment uses a hybrid approach and uses both private zone DNS and public zone, records in private zone DNS will be resolved first.</p>
<p>To troubleshoot resolving DNS records:</p>
<ol>
<li>Check that the fully qualified name, zone name, and record type is correct.</li>
<li>Check that no DNS records have the same name, even if they are of different types.</li>
<li>Check that the DNS records resolve correctly on the Azure DNS name servers.</li>
<li>Check name resolution with a service such as <a target="_blank" rel="noopener" href="https://digwebinterface.com/">digwebinterface</a>. This removes proxy servers and cached results, and only tests the current state of the name servers.</li>
<li>Check that the name servers are correct for your DNS zone, as shown in the Azure portal.</li>
</ol>
<p>Azure Private DNS zones have the following limitations:</p>
<ul>
<li>If automatic registration of VM DNS is enabled, only one private zone can be linked to a virtual network. You can however link multiple virtual networks to a single DNS zone.</li>
<li>Reverse DNS works only for private IP space in the linked virtual network.</li>
<li>Reverse DNS for a private IP address in linked virtual network will return internal.cloudapp.net as the default suffix for the virtual machine. For virtual networks that are linked to a private zone with autoregistration enabled, reverse DNS for a private IP address returns two fully qualified domain names (FQDNs): one with default the suffix internal.cloudapp.net and another with the private zone suffix.</li>
<li>Conditional forwarding isn’t currently natively supported. To enable resolution between Azure and on-premises networks, see <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/virtual-network/virtual-networks-name-resolution-for-vms-and-role-instances">Name resolution for VMs and role instances.</a></li>
</ul>
<h2 id="2-3-Troubleshoot-custom-Domain-Name-System-configuration-issues"><a href="#2-3-Troubleshoot-custom-Domain-Name-System-configuration-issues" class="headerlink" title="2.3 Troubleshoot custom Domain Name System configuration issues"></a>2.3 Troubleshoot custom Domain Name System configuration issues</h2><p>As well as Azure in-built DNS, you also have the option to configure a custom DNS server. For example, you might want to integrate with on-premises Active Directory or resolve names between VNets.</p>
<p>To use Azure custom DNS, you must add a list of IP addresses that point to DNS servers. This list will be distributed to any devices in the virtual network that were using the Azure DNS server.</p>
<p>There are some limitations with Azure custom DNS:</p>
<ul>
<li>You cannot register a public domain name using Azure custom DNS.</li>
<li>DNSSEC is not enabled.</li>
<li>You cannot do zone transfers.</li>
</ul>
<p> Note</p>
<p>Updating from in-built DNS (Inherit from virtual network) to custom DNS will restart all affected VMs.</p>
<p>In DNS settings, you can choose whether to inherit DNS settings from the virtual network or use custom DNS. Check that you have saved any change from Azure in-built DNS to custom DNS, otherwise, the changes will not persist.</p>
<p>To test whether custom DNS is working, use the PowerShell command:</p>
<p>Copy</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test-netconnection -computername -port  </span><br></pre></td></tr></table></figure>

<p>To check your custom DNS is using the correct DNS server, from a command prompt type:</p>
<p>Copy</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ipconfig /all</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Exercise-Name-resolution-issues"><a href="#Exercise-Name-resolution-issues" class="headerlink" title="Exercise: Name resolution issues"></a>Exercise: Name resolution issues</h1><p>Completed100 XP</p>
<ul>
<li>10 minutes</li>
</ul>
<p> Important</p>
<p>You need your own <a target="_blank" rel="noopener" href="https://azure.microsoft.com/free/">Azure subscription</a> to complete the exercises in this module. If you don’t have an Azure subscription, you can still view the demonstration video at the bottom of this page.</p>
<p>Sign in to the <a target="_blank" rel="noopener" href="https://portal.azure.com/learn.docs.microsoft.com">Azure portal</a> using the same account you used to activate the sandbox.</p>
<p>If you have not already run the script in unit 2, please do so now so you can follow the exercise below.</p>
<p>You work for Contoso as a network engineer, and users are complaining that they cannot access VM1 or VM2. You have configured two Azure virtual networks: VNet1 and VNet2. They are connected with peering.</p>
<table>
<thead>
<tr>
<th align="left"><strong>Virtual network</strong></th>
<th align="left"><strong>IPv4 network address</strong></th>
<th align="left"><strong>Subnet</strong></th>
<th align="left"><strong>IPv4 network address</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">VNet1</td>
<td align="left">10.1.0.0&#x2F;16</td>
<td align="left">Subnet1</td>
<td align="left">10.1.1.0&#x2F;24</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left">Subnet2</td>
<td align="left">10.1.2.0&#x2F;24</td>
</tr>
<tr>
<td align="left">VNet2</td>
<td align="left">10.2.0.0&#x2F;16</td>
<td align="left">Default</td>
<td align="left">10.2.0.0&#x2F;24</td>
</tr>
</tbody></table>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-network-1.png" alt="Screenshot showing the topology of v net 1."></p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-network-2.png" alt="Screenshot showing the topology of v net 2."></p>
<table>
<thead>
<tr>
<th align="left"><strong>Virtual machine</strong></th>
<th align="left"><strong>Operating system</strong></th>
<th align="left"><strong>VNet and subnet</strong></th>
<th align="left"><strong>DNS domain</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="left">VM1</td>
<td align="left">Windows Server 2019</td>
<td align="left">VNet1, Subnet1</td>
<td align="left">contoso.com</td>
</tr>
<tr>
<td align="left">VM2</td>
<td align="left">Windows Server 2019</td>
<td align="left">VNet1, Subnet2</td>
<td align="left">contoso.com</td>
</tr>
<tr>
<td align="left">VM3</td>
<td align="left">Windows Server 2019</td>
<td align="left">VNet2, default</td>
<td align="left">contoso.com</td>
</tr>
</tbody></table>
<h2 id="Diagnosis"><a href="#Diagnosis" class="headerlink" title="Diagnosis"></a>Diagnosis</h2><p>Use Nslookup on VM1 and VM2 and check you get the following results:</p>
<ul>
<li>vm1.contoso.com – success</li>
<li>vm2.contoso.com – success</li>
<li>vm3.contoso.com – can’t find</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-nslookup.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-nslookup.png" alt="Screenshot showing the results of n s lookup in the command prompt."></a></p>
<p>Nslookup on VM3 gives these results:</p>
<ul>
<li>vm1.contoso.com – can’t find</li>
<li>vm2.contoso.com – can’t find</li>
<li>vm3.contoso.com – can’t find</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-nslookup-3.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-nslookup-3.png" alt="Screenshot of results of running n s lookup in the command prompt on vm3."></a></p>
<h2 id="Diagnosis-1"><a href="#Diagnosis-1" class="headerlink" title="Diagnosis"></a>Diagnosis</h2><h3 id="Examine-the-Internet-Protocol-configuration-of-the-Virtual-Machines"><a href="#Examine-the-Internet-Protocol-configuration-of-the-Virtual-Machines" class="headerlink" title="Examine the Internet Protocol configuration of the Virtual Machines"></a>Examine the Internet Protocol configuration of the Virtual Machines</h3><p>Connect to each VM using Remote Desktop. Open a command prompt window and type: ipconfig &#x2F;all</p>
<p>The IP addresses are:</p>
<ul>
<li>VM1 &#x3D; 10.1.1.4</li>
<li>VM2 &#x3D; 10.1.2.4</li>
<li>VM3 &#x3D; 10.2.0.4</li>
</ul>
<p>The DNS server address is 168.63.129.16, which is the wire server.</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-network-configure.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-network-configure.png" alt="Screenshot of command prompt results after running i p config / all."></a></p>
<h3 id="Test-network-connectivity"><a href="#Test-network-connectivity" class="headerlink" title="Test network connectivity"></a>Test network connectivity</h3><p>Use <strong>ping</strong> to test network connectivity between the three virtual machines.</p>
<p>All three VMs are able to ping each other, so network connectivity is good at the IP level (OSI Layer 3).</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-ping-connectivity.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-ping-connectivity.png" alt="Screenshot of command screen after pinging for network connectivity."></a></p>
<h3 id="Examine-the-Azure-resource-group"><a href="#Examine-the-Azure-resource-group" class="headerlink" title="Examine the Azure resource group"></a>Examine the Azure resource group</h3><p>There are two virtual networks (VNets) called VNet1 and VNet2.</p>
<p>There is a private DNS zone, which is contoso.com.</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-resource-group.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-resource-group.png" alt="Screenshot showing the resources within the resource group."></a></p>
<p>The private DNS zone has vm1 and vm2 automatically registered, but vm3 does not appear.</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-no-machine-3.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-no-machine-3.png" alt="Screenshot showing that that vm 3 is not appearing."></a></p>
<p>Go to <strong>Settings</strong> &gt; <strong>Virtual network links</strong>. We see that the private DNS zone is linked to VNet1, but not to VNet2.</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-virtual-network.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-virtual-network.png" alt="Screenshot showing the virtual links."></a></p>
<h2 id="Resolution"><a href="#Resolution" class="headerlink" title="Resolution"></a>Resolution</h2><h3 id="Link-the-private-Domain-Name-System-zone-to-Virtual-Network-2"><a href="#Link-the-private-Domain-Name-System-zone-to-Virtual-Network-2" class="headerlink" title="Link the private Domain Name System zone to Virtual Network 2"></a>Link the private Domain Name System zone to Virtual Network 2</h3><p>Navigate to the private DNS zone (contoso.com) and select the Virtual network links page. Add a new link.</p>
<ul>
<li>Link name: vnet2_dns</li>
<li><input disabled type="checkbox"> I know the resource ID of virtual network – leave unchecked</li>
<li>Subscription: <the name of your subscription></the></li>
<li>Virtual network: VNet2</li>
<li>Configuration: [X] Enable auto registration</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-add-virtual-network.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-add-virtual-network.png" alt="Screenshot showing the add virtual network link screen."></a></p>
<p>After you select OK, it may take a few minutes for the link to be created. Select Refresh occasionally to see the latest status. Wait until the link status says Completed.</p>
<h3 id="Inspect-the-Domain-Name-System-name-table"><a href="#Inspect-the-Domain-Name-System-name-table" class="headerlink" title="Inspect the Domain Name System name table"></a>Inspect the Domain Name System name table</h3><p>Navigate to the Overview page and inspect the DNS name table.</p>
<p>VM1, VM2, and VM3 should appear. You may need to wait a short while for VM3 to appear. Select Refresh if necessary.</p>
<p>Nslookup on VM1 and VM2 should resolve vm3.contoso.com.</p>
<p> Tip</p>
<p>If VM3 does not appear after several minutes, try restarting the VM.</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-table.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-table.png" alt="Screenshot showing the d n s table."></a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-command-prompt-machine.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-command-prompt-machine.png" alt="Screenshot showing the results of running the n s lookup commands."></a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-command-prompt-lookup.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/name-resolution-issues/media/5-command-prompt-lookup.png" alt="Screenshot of the command prompt showing the results of n s lookup."></a></p>
<p>Optionally, you can test pinging the VMs, using their DNS names.</p>
<ul>
<li>vm1.contoso.com</li>
<li>vm2.contoso.com</li>
<li>vm3.contoso.com</li>
</ul>
<p>In this demonstration you will see how to proactively troubleshoot Conditional Access policies using the What if tool in the Azure portal:</p>
<iframe src="https://www.microsoft.com/en-us/videoplayer/embed/RE4TYML?postJsllMsg=true" frameborder="0" allowfullscreen="true" data-linktype="external" title="Video Player" style="box-sizing: inherit; outline-color: inherit; margin: 0px; padding: 0px; border: 0px; width: 640px; height: 360px; position: absolute; inset: 0px;"></iframe>



















<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Completed100 XP</p>
<ul>
<li>3 minutes</li>
</ul>
<p>In this module you have learned how to troubleshoot Azure in-built name resolution, custom name resolution, and Azure private zone. You have learned how to troubleshoot public DNS, and domain delegation.</p>
<p>Now that you have completed this module you know how to troubleshoot:</p>
<ul>
<li>Troubleshoot built-in Azure name resolution issues</li>
<li>Troubleshoot custom Azure name resolutions issues</li>
<li>Troubleshoot Azure Private Zone DNS</li>
<li>Public DNS issues</li>
<li>Reviewing the Azure DNS logs</li>
</ul>
<h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><p>For more information about the articles discussed in this module, see:</p>
<h3 id="Azure-Built-in-Name-resolution"><a href="#Azure-Built-in-Name-resolution" class="headerlink" title="Azure Built-in Name resolution"></a>Azure Built-in Name resolution</h3><p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/dns-troubleshoot">Azure DNS troubleshooting guide</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/virtual-network/virtual-networks-name-resolution-for-vms-and-role-instances">Name resolution for resources in Azure virtual networks</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/dns-delegate-domain-azure-dns">Tutorial: Host your domain in Azure DNS</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/dns-getstarted-powershell">Create a DNS zone</a></p>
<h3 id="Azure-DNS-Private-Zones"><a href="#Azure-DNS-Private-Zones" class="headerlink" title="Azure DNS Private Zones"></a>Azure DNS Private Zones</h3><p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/azure-monitor/insights/dns-analytics">DNS Analytics solution in Azure Monitor</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/private-dns-getstarted-portal">Quickstart: Create an Azure private DNS zone using the Azure portal</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/dns-getstarted-powershell">Quickstart: Create an Azure DNS zone and record using Azure PowerShell</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/windows-server/networking/dns/troubleshoot/troubleshoot-dns-server">Troubleshooting DNS Servers</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/dns-domain-delegation">Azure DNS delegation overview</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits">Azure subscription limits and quotas</a></p>
<h3 id="Custom-DNS"><a href="#Custom-DNS" class="headerlink" title="Custom DNS"></a>Custom DNS</h3><p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/dns/dns-web-sites-custom-domain">Tutorial - Create custom Azure DNS records for a web app</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/virtual-network/virtual-networks-name-resolution-ddns">Using dynamic DNS to register hostnames in Azure</a></p>
<h3 id="Log-Analytics"><a href="#Log-Analytics" class="headerlink" title="Log Analytics"></a>Log Analytics</h3><p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/azure-monitor/agents/agent-windows">Install Log Analytics agent on Windows computers</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/azure-monitor/agents/om-agents">Connect Operations Manager to Azure Monitor</a></p>
<p><a target="_blank" rel="noopener" href="https://azuremarketplace.microsoft.com/marketplace/apps/Microsoft.DnsAnalyticsOMS?tab=Overview">Microsoft Azure Marketplace</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/azure-monitor/insights/solutions?tabs=portal">Monitoring solutions in Azure Monitor</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/azure-monitor/insights/dns-analytics">Gather insights about your DNS infrastructure with the DNS Analytics Preview solution</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-720-Troubleshoot-business-continuity-with-Microsoft-Azure-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-720-Troubleshoot-business-continuity-with-Microsoft-Azure-1/" class="post-title-link" itemprop="url">AZ-720-Troubleshoot-business-continuity-with-Microsoft-Azure-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:02:09" itemprop="dateCreated datePublished" datetime="2022-11-14T13:02:09-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 21:37:08" itemprop="dateModified" datetime="2022-11-27T21:37:08-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AZ-720/" itemprop="url" rel="index"><span itemprop="name">AZ-720</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-720-Troubleshoot-business-continuity-with-Microsoft-Azure-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-720-Troubleshoot-business-continuity-with-Microsoft-Azure-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Completed100 XP</p>
<ul>
<li>3 minutes</li>
</ul>
<p>As an Azure network support engineer, you understand that business data is valuable, and needs to be protected from internal and external risks. When you encounter problems with backing up or recovery, you need to know how to approach the problem. In this module, you will learn how to monitor the status of backups, review logs, and troubleshoot common issues that occur when backing up and recovering data.</p>
<h2 id="Learning-objectives"><a href="#Learning-objectives" class="headerlink" title="Learning objectives"></a>Learning objectives</h2><p>By the end of this module, you will be able to:</p>
<ul>
<li>Review the status of a backup job, manage alerts, and review logs.</li>
<li>Troubleshoot backing and restoring a virtual machine (VM).</li>
<li>Troubleshoot Microsoft Azure Recovery Services backups.</li>
<li>Troubleshoot backing up and restoring servers.</li>
<li>Troubleshoot Azure to Azure Site Recovery.</li>
<li>Troubleshoot site recovery with Hyper-V, VMM, and VMware.</li>
</ul>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><ul>
<li>Demonstrate an understanding of the OSI model</li>
<li>Demonstrate an understanding of Azure CLI</li>
<li>Demonstrate an understanding of PowerShell</li>
<li>Know how to run Cloud Shell to run commands</li>
</ul>
<h1 id="Troubleshoot-backup-issues-with-Microsoft-Azure"><a href="#Troubleshoot-backup-issues-with-Microsoft-Azure" class="headerlink" title="Troubleshoot backup issues with Microsoft Azure"></a>Troubleshoot backup issues with Microsoft Azure</h1><p>Completed100 XP</p>
<ul>
<li>8 minutes</li>
</ul>
<h2 id="Review-backup-status-in-the-Azure-portal"><a href="#Review-backup-status-in-the-Azure-portal" class="headerlink" title="Review backup status in the Azure portal"></a>Review backup status in the Azure portal</h2><p>The Azure Backup Center is a one-stop location to manage Azure backups including:</p>
<ul>
<li>Azure VM backup</li>
<li>SQL in Azure VM backup</li>
<li>SAP HANA in Azure VM backup</li>
<li>Azure Files backup</li>
<li>Azure Blobs backup</li>
<li>Azure Managed Disks backup</li>
<li>Azure Database for PostgreSQL Server backup</li>
</ul>
<p>For information about supported scenarios, see the <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-center-support-matrix">Support matrix for Backup center</a>.</p>
<p>In the Azure portal, search for Backup center in the Search bar. You can then pin the Backup center to your dashboard. Use the filter options to view the specific jobs you are interested in. You can filter on the following parameters:</p>
<ul>
<li>Datasource subscription, resource group, location, tag, or type</li>
<li>Vault</li>
<li>Protection status</li>
</ul>
<p>Use the Backup jobs menu to review the job status. You can review the time the backup started, the duration, the job operation, and its status. You can also display the backup instance associated with the job, the subscription, resource group, and location.</p>
<p>Select an item in the grid to get more details. Right-click an item to go to the resource and take any necessary actions.</p>
<h3 id="Review-and-respond-to-backup-alerts"><a href="#Review-and-respond-to-backup-alerts" class="headerlink" title="Review and respond to backup alerts"></a>Review and respond to backup alerts</h3><p>From the Backup center left menu, under Monitoring + reporting, select Alerts (preview).</p>
<p>By default, the summary displays open alerts in the last 24 hours. You can also filter by a range of parameters:</p>
<ul>
<li><p>Datasource subscription</p>
</li>
<li><p>Datasource resource group</p>
</li>
<li><p>Datasource location</p>
</li>
<li><p>Datasource type</p>
</li>
<li><p>Vault</p>
</li>
<li><p>Severity (0 &#x3D; critical, 1 &#x3D; error, 2 &#x3D; warning, 3 &#x3D; informational, and 4 &#x3D; verbose)</p>
</li>
<li><p>State (New, acknowledged, closed)</p>
</li>
<li><p>Type (Security alert, configured alert)</p>
</li>
<li><p>Signal type (metric or log)</p>
</li>
<li><p>Time range (24 hours, week, two weeks, 30 days, or custom)</p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/2-backup-reports.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/2-backup-reports.png" alt="Screenshot that shows Backup Report."></a></p>
</li>
</ul>
<h3 id="Alerts-by-email-notification"><a href="#Alerts-by-email-notification" class="headerlink" title="Alerts by email notification"></a>Alerts by email notification</h3><p>You can get email notifications when alerts are triggered by creating an alert processing rule.</p>
<ul>
<li>From the Azure portal, go to the Backup center.</li>
<li>From the left menu, select Alerts (Preview).</li>
<li>From the top menu, select Alert processing rule (preview).</li>
<li>From the top menu, select Create.</li>
<li>In the Scope section, select Select scope to display the Select Scope pane.</li>
<li>Select the resource you want the alert processing rule applied to by either selecting from the drop-down list, or by typing to filter the resources. The matching resources are then displayed. Use the check boxes to select the resources you want the alert processing rule to apply to.</li>
<li>Alternatively, you can apply the rule for all resources within a subscription. When the Select Scope pane is displayed, select the check box next to the correct subscription.</li>
<li>In the Filter section, you can apply one or more filters. For example, select Severity to generate notifications for alerts of a certain severity.</li>
<li>Under Rule Settings, create an action group (or use an existing one). An action group is the destination to which the alert notification should be sent, such as an email address.</li>
<li>On the Basics tab, select the name of the action group, the subscription, and resource group under which it should be created.</li>
<li>On the Notifications tab, select Email&#x2F;SMS message&#x2F;Push&#x2F;Voice and enter the recipient’s details.</li>
<li>Test the action group. A test email is sent to the specified email addresses.</li>
<li>Select Review + Create, and then Create to save the action group.</li>
<li>Select Create to save the action rule.</li>
</ul>
<h3 id="Review-and-interpret-backup-logs"><a href="#Review-and-interpret-backup-logs" class="headerlink" title="Review and interpret backup logs"></a>Review and interpret backup logs</h3><p>To review and interpret backup logs, select the Backup Reports menu in the Backup center. When configured, Azure Backup Reports allow you to audit your backups and restores, analyze trends, and track and forecast usage.</p>
<p>Configure diagnostics settings for a Recovery Services vault by going to the vault and selecting Diagnostics settings. Select + Add Diagnostic Setting for the metrics you want to collect. Under Destination details, select the destinations that you want to stream them to. This includes Log Analytics workspace as an option.</p>
<p>The following views are available from the tabs:</p>
<ul>
<li><p>Overview – select specific subscriptions and workspaces, and links to more information.</p>
</li>
<li><p>Summary – a high-level overview of your backup estate.</p>
</li>
<li><p>Backup Items – cloud storage consumed at a backup item level.</p>
</li>
<li><p>Usage – key billing parameters for your backups.</p>
</li>
<li><p>Jobs – view long-running trends on jobs, such as the number of failed jobs per day, and the top causes of job failure.</p>
</li>
<li><p>Policies – information on active policies, such as the number of associated items, and the total cloud storage consumed by items backed up under a given policy.</p>
</li>
<li><p>Optimize – potential cost optimization opportunities for your backups.</p>
</li>
<li><p>Policy adherence – success of backups per day for every backup instance.</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/2-backup-center.png" alt="Screenshot of Backup Center."></p>
</li>
</ul>
<h2 id="Troubleshooting-backups-with-Microsoft-Azure-Recovery-Services-MARS"><a href="#Troubleshooting-backups-with-Microsoft-Azure-Recovery-Services-MARS" class="headerlink" title="Troubleshooting backups with Microsoft Azure Recovery Services (MARS)"></a>Troubleshooting backups with Microsoft Azure Recovery Services (MARS)</h2><p> Note</p>
<p>Microsoft documentation refers to both the backup agent and MARS interchangeably: both refer to the same service name (cbengine). In this module, it is referred to as Microsoft Azure Recovery Services (MARS).</p>
<p>Microsoft Azure Recovery Services is also known as the backup agent. This is the Azure service that is used to back up data. The MARS agent can run on:</p>
<ul>
<li>On-premises Windows machines.</li>
<li>Azure VMs running Windows. The MARS agent runs side by side with the Azure VM backup extension. The agent backs up specific files and folders on the VM.</li>
<li>A Microsoft Azure Backup Server (MABS) or a System Center Data Protection Manager (DPM) server.</li>
</ul>
<p>Use the MARS agent to back up files, folders, and Windows machines either on-premises or in the cloud. Data is backed up to an Azure Recovery Services Vault.</p>
<p>To troubleshoot with the MARS agent, first check basic issues, and then more advanced issues.</p>
<h3 id="Step-1-Basic-troubleshooting"><a href="#Step-1-Basic-troubleshooting" class="headerlink" title="Step 1: Basic troubleshooting"></a>Step 1: Basic troubleshooting</h3><p>Check that:</p>
<ul>
<li>You have network connectivity between the MARS backup agent and Azure.</li>
<li>The MARS agent is running. You might need to restart it and ensure the MARS agent is ready.</li>
<li>There is 5%-10% free space in the scratch folder.</li>
<li>Antivirus software or any other process is not interfering with the backup.</li>
<li>Any warning messages have been reviewed.</li>
</ul>
<h3 id="Step-2-Troubleshoot-versions-and-other-steps"><a href="#Step-2-Troubleshoot-versions-and-other-steps" class="headerlink" title="Step 2: Troubleshoot versions and other steps"></a>Step 2: Troubleshoot versions and other steps</h3><ul>
<li>For offline backups, ensure Azure PowerShell 3.7.0 is installed on both computers.</li>
<li>The operating system and MARS agent are up-to-date.</li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-support-matrix-mars-agent">Unsupported drives, and files with unsupported attributes, are excluded from backup</a>.</li>
<li>Are manual backups working, but scheduled backups are not?</li>
<li>Are all relevant clocks set to the correct time zone?</li>
</ul>
<h2 id="Troubleshoot-backing-up-Azure-VMs"><a href="#Troubleshoot-backing-up-Azure-VMs" class="headerlink" title="Troubleshoot backing up Azure VMs"></a>Troubleshoot backing up Azure VMs</h2><p>When configuring backups for an Azure VM, follow best practice guidelines. If you encounter problems with backing up an Azure VM, try the following:</p>
<h3 id="Step-1-Basic-troubleshooting-1"><a href="#Step-1-Basic-troubleshooting-1" class="headerlink" title="Step 1: Basic troubleshooting"></a>Step 1: Basic troubleshooting</h3><ul>
<li>Is the Azure VM provisioning state ‘Running’? The backup will not run if it is in a Stopped&#x2F;Deallocated&#x2F;Updating state. You might see the error message: VM is not in a state that allows backups.</li>
<li>Are there any pending operating system updates or does the VM need to reboot? Install system updates and reboot before retrying the backup.</li>
<li>Does the Azure VM have internet connectivity?</li>
<li>Is another backup service running? Two backup services cannot run at the same time. If you suspect another backup has not left a snapshot extension, <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-troubleshoot-vm-backup-fails-snapshot-timeout">uninstall extensions</a> to force a reload, then try the backup again.</li>
<li>Is antivirus software preventing the backup from running? If so, you might see the error message: antivirus configured in the VM is restricting the execution of backup extension. To resolve this issue, in the antivirus configuration, exclude the directories below:</li>
</ul>
<p>C:\Packages\Plugins\Microsoft.Azure.RecoveryServices.VMSnapshot</p>
<p>C:\WindowsAzure\Logs\Plugins\Microsoft.Azure.RecoveryServices.VMSnapshot</p>
<h3 id="Step-2-Check-the-Azure-VM-Guest-Agent"><a href="#Step-2-Check-the-Azure-VM-Guest-Agent" class="headerlink" title="Step 2: Check the Azure VM Guest Agent"></a>Step 2: Check the Azure VM Guest Agent</h3><ul>
<li>Is the Azure VM Guest Agent service installed and started? Go to services.msc to check it is running then try rebooting the VM.</li>
<li>Is the Microsoft Azure VM Guest Agent the latest version?</li>
</ul>
<h3 id="Step-3-Check-Azure-VM-extension-health"><a href="#Step-3-Check-Azure-VM-extension-health" class="headerlink" title="Step 3: Check Azure VM extension health"></a>Step 3: Check Azure VM extension health</h3><ul>
<li>Are all Azure VM extensions in the ‘provisioning succeeded’ state? From the Azure portal, go to the VM, then Settings. From the Extensions menu, select Extensions status.</li>
<li>Have all <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/virtual-machines/extensions/overview">extension issues</a> been resolved?</li>
</ul>
<h3 id="Step-4-Check-the-Azure-backup-agent"><a href="#Step-4-Check-the-Azure-backup-agent" class="headerlink" title="Step 4: Check the Azure backup agent"></a>Step 4: Check the Azure backup agent</h3><ul>
<li>Is the Windows or Linux VM operating system supported? Refer to the <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-support-matrix-iaas">IaaS VM Backup Support Matrix</a> for supported versions.</li>
<li>Is the VM agent up-to-date? Azure VMs are backed up by installing a backup agent. Check that the backup agent is installed and it’s a recent version.</li>
<li>Is the Azure Backup option grayed out? Hover over the grayed-out option to find out the reason.</li>
<li>Is antivirus software blocking the extension? If there are log entries in Event Viewer Application logs with the faulting application name IaaSBcdrExtension.exe then it could be the antivirus software.</li>
<li>Are there entries in the Event Log? The Event Log may show backup failures from products other than Azure Backup. If Azure Backup is failing, find the corresponding error code in <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-troubleshoot-vm-backup-fails-snapshot-timeout">Common Issues</a> to discover the solution.</li>
</ul>
<p>For an up-to-date list of common error messages, see <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-vms-troubleshoot">Troubleshoot backup errors with Azure VMs - Azure Backup | Microsoft Docs</a>.</p>
<h2 id="Troubleshoot-Azure-Backup-Server-issues"><a href="#Troubleshoot-Azure-Backup-Server-issues" class="headerlink" title="Troubleshoot Azure Backup Server issues"></a>Troubleshoot Azure Backup Server issues</h2><p>Microsoft Azure Backup Server (MABS), also known as Azure Backup Server, is the software used to back up a range of servers and workloads. For a complete list of what can be protected with MABS, see:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-mabs-protection-matrix">MABS (Azure Backup Server) V3 UR1 (and later) protection matrix</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/microsoft-azure-backup-server-protection-v3">Azure Backup Server V3 RTM protection matrix</a></li>
</ul>
<p>MABS must be installed on a dedicated, single-purpose server. The computer cannot be a domain controller, a node of a cluster, have Exchange Server running, have System Center Operations Manager running, or have the Application Server role installed. It must be a dedicated backup server.</p>
<p>Microsoft Azure Backup Server isn’t supported on Windows Server Core or Microsoft Hyper-V Server.</p>
<p>Microsoft Azure Backup Server must be part of a domain. You cannot move an existing MABS machine to a new domain after deployment.</p>
<p>Microsoft Azure Backup Server must be registered with a Recovery Services vault, whether backup data is stored locally or in Azure.</p>
<p>Run through the following questions to troubleshoot Azure Backup Server issues:</p>
<ul>
<li>Is there network connectivity between the MARS agent and the Azure server?</li>
<li>Is the MARS service running? In the Service console, start or restart the service and try the backup again.</li>
<li>Is the SQL Agent service running and set to automatic in the MABS server?</li>
<li>Is the MARS agent up-to-date?</li>
<li>Ensure no other process or antivirus software is interfering with Azure backup.</li>
<li>If Push install fails, check if DPM agent is already present. If yes, then uninstall the agent and install it again.</li>
<li>Ensure .NET Framework 4.5.2 or later is installed on the server.</li>
<li>Ensure the server on which you’re trying to install Azure Backup Server isn’t already registered with another vault.</li>
<li>Configure antivirus software for MABS server.</li>
<li>Check where MABs is installed, and what else is installed on that server.</li>
<li>Check the backup vault credentials.</li>
</ul>
<h2 id="Troubleshoot-scheduled-backups"><a href="#Troubleshoot-scheduled-backups" class="headerlink" title="Troubleshoot scheduled backups"></a>Troubleshoot scheduled backups</h2><p>There are two types of policies for scheduled backups:</p>
<ul>
<li>Default backup policy – the default policy backs up the VM once a day, retaining the backups for 30 days.</li>
<li>Custom policy – you define whether backups run daily or weekly, and specify how long to retain the backup.</li>
</ul>
<p>If you have problems with scheduled backups, try the following:</p>
<ul>
<li>The VMs must be in the same region and subscription as the recovery vault.</li>
<li>If you get a message saying the recovery point is crash-consistent, check whether the VM is switched on. If the VM is not on, it will be backed up as an offline VM and the recovery point will be crash-consistent. This means that Azure gives no guarantee about the consistency of the data on the storage medium.</li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/troubleshoot/system-center/dpm/troubleshoot-scheduled-backup-job-failures">Troubleshoot scheduled backup job failures - Data Protection Manager | Microsoft Docs</a></li>
</ul>
<h1 id="Troubleshoot-recovery-issues-with-Microsoft-Azure"><a href="#Troubleshoot-recovery-issues-with-Microsoft-Azure" class="headerlink" title="Troubleshoot recovery issues with Microsoft Azure"></a>Troubleshoot recovery issues with Microsoft Azure</h1><p>Completed100 XP</p>
<ul>
<li>9 minutes</li>
</ul>
<p>When you have created a backup, you may need to recover the data as part of a planned migration, or in case of data loss. In this unit, you will learn how to troubleshoot data recovery for different scenarios.</p>
<h2 id="Troubleshoot-file-recovery-for-an-Azure-VM-backup"><a href="#Troubleshoot-file-recovery-for-an-Azure-VM-backup" class="headerlink" title="Troubleshoot file recovery for an Azure VM backup"></a>Troubleshoot file recovery for an Azure VM backup</h2><p>When you recover data from an Azure VM backup you have three options to restore the data:</p>
<ul>
<li>File Recovery</li>
<li>Restore VM</li>
<li>Disk restore. Only the disks are restored, which can then be used to create a new VM or replace a disk on an existing VM.</li>
</ul>
<p>To recover files, select the <strong>File Recovery</strong> option. It is then a three-step process to recover the files:</p>
<ol>
<li>Select a recovery point. This is the date when your files were available.</li>
<li>Download a script. The script will mount the drives storing your files. The drives will remain mounted for 12 hours for you to restore the files you need.</li>
<li>Unmount the drives and close the connection.</li>
</ol>
<p>The following sections provide troubleshooting information when common error messages appear:</p>
<h3 id="Error-message-Exception-caught-while-connecting-to-target"><a href="#Error-message-Exception-caught-while-connecting-to-target" class="headerlink" title="Error message: Exception caught while connecting to target"></a>Error message: Exception caught while connecting to target</h3><p>This may appear if the script is unable to access the recovery point. To resolve this issue:</p>
<ol>
<li>Check that the machine running the script has access to the recovery vault.</li>
<li>Verify the connection to the Azure target IP addresses. To check, run the following from an elevated command prompt: nslookup download.microsoft.com or ping download.microsoft.com</li>
<li>Ensure access to iSCSI outbound port 3260.</li>
<li>Check for a firewall or NSG blocking traffic to Azure target IPs or recovery service URLs.</li>
<li>Check that antivirus software isn’t preventing the script from running.</li>
</ol>
<p>Also check the following common error messages:</p>
<h3 id="Error-message-The-target-has-already-been-logged-in-via-an-iSCSI-session"><a href="#Error-message-The-target-has-already-been-logged-in-via-an-iSCSI-session" class="headerlink" title="Error message: The target has already been logged in via an iSCSI session"></a>Error message: The target has already been logged in via an iSCSI session</h3><p>This might be caused by the script already running on the same machine, and the drives are already attached. Browse the available volumes using File Explorer to find the mounted drives.</p>
<h3 id="Error-message-This-script-is-invalid-because-the-disks-have-been-dismounted-via-portal-x2F-exceeded-the-12-hr-limit-Download-a-new-script-from-the-portal"><a href="#Error-message-This-script-is-invalid-because-the-disks-have-been-dismounted-via-portal-x2F-exceeded-the-12-hr-limit-Download-a-new-script-from-the-portal" class="headerlink" title="Error message: This script is invalid because the disks have been dismounted via portal&#x2F;exceeded the 12-hr limit. Download a new script from the portal"></a>Error message: This script is invalid because the disks have been dismounted via portal&#x2F;exceeded the 12-hr limit. Download a new script from the portal</h3><p>This message appears if you attempt to run the script more than 12 hours after you downloaded it. You need to download another script from the portal.</p>
<h3 id="Error-message-ExtensionSnapshotFailedCOM"><a href="#Error-message-ExtensionSnapshotFailedCOM" class="headerlink" title="Error message: ExtensionSnapshotFailedCOM"></a>Error message: ExtensionSnapshotFailedCOM</h3><p>The backup operation failed due to an issue with the Windows service COM+ System application. To resolve this issue, follow these steps:</p>
<ol>
<li>Try starting&#x2F;restarting Windows service COM+ System Application (from an elevated command prompt – net start COMSysApp).</li>
<li>Ensure Distributed Transaction Coordinator service is running as Network Service account. If not, change it to run as Network Service account and restart COM+ System Application.</li>
<li>If unable to restart the service, then reinstall Distributed Transaction Coordinator service by following the steps below:</li>
<li>Stop the MSDTC service</li>
<li>Open a command prompt (cmd)</li>
<li>Run the command msdtc -uninstall</li>
<li>Run the command msdtc -install</li>
<li>Start the MSDTC service</li>
<li>Start the Windows service COM+ System Application. After the COM+ System Application starts, trigger a backup job from the Azure portal.</li>
</ol>
<p>For an up-to-date list of common error messages and recommended action, see <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-vm-file-recovery-troubleshoot">Troubleshoot Azure VM file recovery - Azure Backup | Microsoft Docs</a>.</p>
<h2 id="Troubleshoot-restoring-using-Microsoft-Azure-Recovery-Services-MARS-backup-agent"><a href="#Troubleshoot-restoring-using-Microsoft-Azure-Recovery-Services-MARS-backup-agent" class="headerlink" title="Troubleshoot restoring using Microsoft Azure Recovery Services (MARS) backup agent"></a>Troubleshoot restoring using Microsoft Azure Recovery Services (MARS) backup agent</h2><p>To restore data that has been backed up using Microsoft Azure Recovery Services (MARS):</p>
<ol>
<li>From the MARS agent home screen, select Recover Data. The Recover Data Wizard is displayed.</li>
<li>Specify where you want to restore the data.</li>
<li>Select the data to restore, and the date and time it was backed up. These selections determine the data recovery point.</li>
<li>Select Mount to mount the disk containing the recovery point then select the location where you want to recover the data.</li>
<li>Confirm your selections before starting the data recovery.</li>
</ol>
<p>When you installed the MARS agent, an encryption passphrase was generated. This is stored in a text file and should be kept somewhere safe. If the passphrase is lost, you cannot recover any data backed up by the MARS backup agent.</p>
<p>When the MARS agent is configured, you specify how long the backup data should be retained. Data will not be kept after this period of time.</p>
<h2 id="Troubleshoot-restore-issues-when-using-Azure-backup-agent"><a href="#Troubleshoot-restore-issues-when-using-Azure-backup-agent" class="headerlink" title="Troubleshoot restore issues when using Azure backup agent"></a>Troubleshoot restore issues when using Azure backup agent</h2><p>When you recover an Azure Virtual Machine from a backup, a process server is used to handle the data. If you encounter problems with restoring data, use a step-by-step methodology:</p>
<h3 id="Step-1-Monitor-process-server-health"><a href="#Step-1-Monitor-process-server-health" class="headerlink" title="Step 1: Monitor process server health"></a>Step 1: Monitor process server health</h3><p>It is good practice to proactively monitor the process server when restoring data. This allows you to monitor alerts, and handle issues as they occur.</p>
<p>The following graphic summarizes the steps to troubleshoot a backup recovery:</p>
<ol>
<li>Are all services running?</li>
<li>Is the CPU state OK?</li>
<li>Is the memory state OK?</li>
<li>Is cache free space OK?</li>
<li>Does the process server have a heartbeat?</li>
<li>Troubleshoot connection&#x2F;replication issues.</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/3-steps-backup-recovery.png#lightbox"><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/3-steps-backup-recovery.png" alt="Screenshot of steps to troubleshoot a backup recovery."></a></p>
<h2 id="Troubleshoot-restore-issues-from-Microsoft-Azure-Backup-Server-MABS"><a href="#Troubleshoot-restore-issues-from-Microsoft-Azure-Backup-Server-MABS" class="headerlink" title="Troubleshoot restore issues from Microsoft Azure Backup Server (MABS)"></a>Troubleshoot restore issues from Microsoft Azure Backup Server (MABS)</h2><p>Azure Backup Server is designed to back up and restore workloads such as Hyper-V VMs, Microsoft SQL Server, SharePoint Server, Microsoft Exchange, and Windows clients.</p>
<p>If you have problems restoring from MABS, try the following troubleshooting checklist:</p>
<h3 id="Check-the-installation-folder"><a href="#Check-the-installation-folder" class="headerlink" title="Check the installation folder"></a>Check the installation folder</h3><p>The default installation folders for DPM are as follows:</p>
<p>C:\Program Files\Microsoft Azure Backup Server\DPM\DPM</p>
<p>You can also run the following command to find the install folder path:</p>
<p>Reg query “HKLM\SOFTWARE\Microsoft\Microsoft Data Protection Manager\Setup”</p>
<h3 id="Invalid-vault-credentials"><a href="#Invalid-vault-credentials" class="headerlink" title="Invalid vault credentials"></a>Invalid vault credentials</h3><p>When you register to a vault, if you get an error message saying invalid vault credentials have been provided, either you have a corrupt file, or you provided incorrect credentials. Try the following:</p>
<ol>
<li>Download the latest credentials file from the vault and try again.</li>
<li>Try downloading the credentials to a different local directory or create a new vault.</li>
<li>Try updating the date and time settings as described in <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-mars-troubleshoot">this article</a>.</li>
<li>Check to see if c:\windows\temp has more than 65000 files. Move stale files to another location or delete the items in the Temp folder.</li>
<li>Check the status of certificates.</li>
<li>In Control Panel, open <strong>Manage Computer Certificates</strong>.</li>
<li>Expand the <strong>Personal</strong> node and its child node <strong>Certificates</strong>.</li>
<li>Remove the certificate <strong>Windows Azure Tools</strong>.</li>
<li>Retry the registration in the Azure Backup client.</li>
<li>Check to see if a group policy is in place.</li>
</ol>
<h3 id="Replica-is-inconsistent"><a href="#Replica-is-inconsistent" class="headerlink" title="Replica is inconsistent"></a>Replica is inconsistent</h3><p>In the Protection Group Wizard, verify that the automatic consistency check option is turned on.</p>
<p>In the case of System State&#x2F;BMR backup, verify that Windows Server Backup is installed on the protected server.</p>
<ul>
<li>Check for space-related issues in the DPM storage pool on the DPM&#x2F;Microsoft Azure Backup Server and allocate storage as required.</li>
<li>Check the state of the Volume Shadow Copy Service on the protected server. If it’s in a disabled state, set it to start manually. Start the service on the server then go back to the DPM&#x2F;Microsoft Azure Backup Server console, and start the sync with the consistency check job.</li>
</ul>
<h3 id="Online-recovery-point-creation-failed"><a href="#Online-recovery-point-creation-failed" class="headerlink" title="Online recovery point creation failed"></a>Online recovery point creation failed</h3><p>If you get an error message saying “the Windows Azure Backup Agent was unable to create a snapshot of the selected volume”, try increasing the available space in the recovery point volume.</p>
<p>If you get an error message saying “the Windows Azure Backup Agent cannot connect to the OBEngine service”, try verifying that the OBEngine exists in the list of running services on the computer. If the OBEngine service is not running, use the “net start OBEngine” command to start it.</p>
<p>If you get an error message saying that “the encryption passphrase for this server is not set. Please configure an encryption passphrase”, try configuring an encryption passphrase. If that fails, take the following steps:</p>
<ol>
<li>Verify that the scratch location exists. This is the location that’s mentioned in the registry <strong>HKEY_LOCAL_MACHINE\Software\Microsoft\Windows Azure Backup\Config</strong>, where the name <strong>ScratchLocation</strong> should exist.</li>
<li>If the scratch location exists, try re-registering by using the old passphrase.</li>
</ol>
<p> Note</p>
<p>When you configure an encryption passphrase, always save it in a secure location.</p>
<h2 id="Troubleshoot-hybrid-scenarios"><a href="#Troubleshoot-hybrid-scenarios" class="headerlink" title="Troubleshoot hybrid scenarios"></a>Troubleshoot hybrid scenarios</h2><h3 id="Troubleshoot-site-recovery-with-Hyper-V"><a href="#Troubleshoot-site-recovery-with-Hyper-V" class="headerlink" title="Troubleshoot site recovery with Hyper-V"></a>Troubleshoot site recovery with Hyper-V</h3><p>When recovering an on-premises Hyper-V VM to Azure, use Azure Site Recovery. If you experience issues, use the following checklist to troubleshoot the problem:</p>
<ul>
<li>Do all your Hyper-V hosts and VMs meet the requirements and prerequisites for recovery?</li>
<li>Check the “Support for disaster recovery of on-premises Hyper-V VMs to Azure” link at the end of this module.</li>
<li>If Hyper-V hosts are managed by Virtual Machine Manager (VMM), the VMM server must have at least one cloud and one or more host groups. The Hyper-V hosts running the VMs should be in the cloud with network mapping between on-premises VMM VM networks, and Azure virtual networks. See links in the Summary of this module.</li>
<li>Check the log located in <strong>Applications and Services Logs</strong> &gt; <strong>Microsoft</strong> &gt; <strong>Windows</strong>.</li>
<li>On the guest VM, verify that WMI is enabled and accessible.<ul>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/windows/win32/wmisdk/wmi-troubleshooting">Troubleshoot</a> WMI.</li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/previous-versions/tn-archive/ff406382(v=msdn.10)">Troubleshoot</a> problems with WMI scripts and services.</li>
</ul>
</li>
<li>On the guest VM, ensure that you have the latest version of Integration Services and it is running. Microsoft recommends keeping integration services up-to-date.</li>
</ul>
<p>Also, check common error messages:</p>
<h3 id="Cannot-enable-protection-as-the-virtual-machine-is-not-highly-available"><a href="#Cannot-enable-protection-as-the-virtual-machine-is-not-highly-available" class="headerlink" title="Cannot enable protection as the virtual machine is not highly available"></a>Cannot enable protection as the virtual machine is not highly available</h3><p>Try restarting the VMM service on the VMM machine. If that doesn’t work, try removing the virtual machine from the cluster and adding it again.</p>
<h3 id="The-VSS-writer-NTDS-failed-with-status-11-and-writer-specific-failure-code-0x800423F4"><a href="#The-VSS-writer-NTDS-failed-with-status-11-and-writer-specific-failure-code-0x800423F4" class="headerlink" title="The VSS writer NTDS failed with status 11 and writer specific failure code 0x800423F4"></a>The VSS writer NTDS failed with status 11 and writer specific failure code 0x800423F4</h3><p>To resolve this issue, upgrade to Windows Server R2 with 4072650 applied. Also, check that the Hyper-V host has Windows 2016 or later installed.</p>
<h3 id="Troubleshoot-site-recovery-with-VMware"><a href="#Troubleshoot-site-recovery-with-VMware" class="headerlink" title="Troubleshoot site recovery with VMware"></a>Troubleshoot site recovery with VMware</h3><p>Before you restore a VMware VM, ensure it meets <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/vmware-physical-azure-support-matrix">Azure requirements</a>.</p>
<p>Verify properties as follows:</p>
<ol>
<li>In <strong>Protected Items</strong>, select <strong>Replicated Items</strong>, and then select the VM you want to verify.</li>
<li>In the <strong>Replicated item</strong> pane, there’s a summary of VM information, health status, and the latest available recovery points. Select <strong>Properties</strong>.</li>
<li>In <strong>Compute and Network</strong>, you can modify these properties as needed:<ul>
<li>Azure name</li>
<li>Resource group</li>
<li>Target size</li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/virtual-machines/windows/tutorial-availability-sets">Availability set</a></li>
<li>Managed disk settings</li>
</ul>
</li>
<li>You can view and modify network settings, including:<ul>
<li>The network and subnet in which the Azure VM will be located after failover.</li>
<li>The IP address that will be assigned to it.</li>
</ul>
</li>
<li>In <strong>Disks</strong>, review operating system information, and data disks on the VM.</li>
</ol>
<p>To run a failover to Azure, in <strong>Settings</strong>, select <strong>Replicated items</strong>, select the VM to fail over, and then select <strong>Failover</strong>.</p>
<p>In <strong>Failover</strong>, select a <strong>Recovery Point</strong> to fail over to. You can use one of the following options:</p>
<ul>
<li><strong>Latest</strong>: This option first processes all the data sent to Site Recovery. It provides the lowest Recovery Point Objective (RPO) because the Azure VM that’s created after failover has all the data that was replicated to Site Recovery when the failover was triggered.</li>
<li><strong>Latest processed</strong>: This option fails the VM over to the latest recovery point processed by Site Recovery. This option provides a low RTO (Recovery Time Objective) because no time is spent processing unprocessed data.</li>
<li><strong>Latest app-consistent</strong>: This option fails the VM over to the latest app-consistent recovery point processed by Site Recovery.</li>
<li><strong>Custom</strong>: This option lets you specify a recovery point.</li>
</ul>
<p>You then select <strong>Shut down machine before beginning failover</strong> to attempt to shut down source VMs before triggering the failover. Failover continues even if the shutdown fails. You can follow the failover progress on the Jobs page.</p>
<p>In some scenarios, failover requires additional processing that takes around 8 to 10 minutes to complete. You might notice longer test failover times for:</p>
<ul>
<li>VMware VMs running a Mobility service version older than 9.8.</li>
<li>Physical servers.</li>
<li>VMware Linux VMs.</li>
<li>Hyper-V VMs protected as physical servers.</li>
<li>VMware VMs that don’t have the DHCP service enabled.</li>
<li>VMware VMs that don’t have the following boot drivers: storvsc, vmbus, storflt, intelide, atapi.</li>
</ul>
<p> Note</p>
<p>Don’t cancel a failover in progress. If you cancel a failover in progress, the VM won’t replicate again.</p>
<p>If you have problems, try the following troubleshooting steps:</p>
<ol>
<li>Monitor process server health. In the Azure portal, monitor the process servers to verify they are connected and working.</li>
<li>Check connectivity between the source server and the process server, and between the process server and Azure.</li>
<li>Check that the source machine is available for replication, specifically:<ul>
<li>Check that two VMs don’t have the same UUID. Refer to <a target="_blank" rel="noopener" href="https://social.technet.microsoft.com/wiki/contents/articles/32026.asr-vmware-to-azure-how-to-cleanup-duplicatestale-entries.aspx">Azure Site Recovery VMware-to-Azure: How to clean up duplicate or stale entries</a>.</li>
</ul>
</li>
<li>Ensure the <strong>vCenter credentials</strong> are correct when you set up the configuration server, by using the OVF template or unified setup. To verify the credentials, see <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/vmware-azure-manage-configuration-server#modify-credentials-for-automatic-discovery">Modify credentials for automatic discovery</a>.</li>
<li>If insufficient permissions are provided to access vCenter, failure to discover virtual machines might occur. Ensure that the permissions described in <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/vmware-azure-tutorial-prepare-on-premises">Prepare an account for automatic discovery</a> are added to the vCenter user account.</li>
<li>Management servers cannot be replicated. If the VM is used as one or more of the following roles—Configuration server &#x2F; scale-out process server &#x2F; Master target server—then you will not be able to choose the virtual machine from the portal.</li>
<li>If the virtual machine is already protected or failed over through Site Recovery, it will not be available to select for protection in the portal. Ensure that the virtual machine isn’t already protected by any other user, or under a different subscription.</li>
<li>Check if vCenter is in connected state. To verify, go to <strong>Recovery Services vault</strong> &gt; <strong>Site Recovery Infrastructure</strong> &gt; <strong>Configuration Servers</strong> &gt; <strong>Click on respective configuration server</strong>. A pane opens on the right with details of associ&#x2F;rvers. Check if vCenter is connected. If it’s in a “Not Connected” state, resolve the issue and then refresh the configuration server on the portal. After this, virtual machine will be listed on the portal.</li>
<li>If the ESXi host under which the VM resides is in powered off state, then virtual machine will not be listed or will not be selectable on the Azure portal. Power on the ESXi host, and refresh the configuration server on the portal. After this, virtual machine will be listed on the portal.</li>
<li>If there is a pending reboot, you will not be able to select the VM on the Azure portal. Complete the pending reboot and refresh the configuration server. They should then be listed on the portal.</li>
<li>If the virtual machine doesn’t have a valid IP address associated with it, you will not be able to select the machine on the Azure portal. Assign a valid IP address to the virtual machine, and refresh the configuration server. This could also be caused by the machine not having a valid IP address associated with one of its NICs. Either assign a valid IP address to all NICs or remove the NIC that’s missing the IP.</li>
</ol>
<h3 id="Troubleshoot-site-recovery-with-SCCM"><a href="#Troubleshoot-site-recovery-with-SCCM" class="headerlink" title="Troubleshoot site recovery with SCCM"></a>Troubleshoot site recovery with SCCM</h3><p>A System Center Configuration Manager (SCCM) site recovery is needed if a site fails, or you lose data in the site database. Site recovery includes repairing and resynchronizing the data.</p>
<p>If you experience problems, try the following troubleshooting issues:</p>
<ol>
<li>Check that previous configurations are not on the site server, as this can cause conflicts. Remove previous configurations before restoring Configuration Manager by using one of the following methods:<ul>
<li>Restoring to a new server.</li>
<li>Formatting the disks and reinstalling the operating system.</li>
<li>Cleaning an existing server, including deleting registry entries starting with SMS from HKLM\System\CurrentControlSet\Services.</li>
</ul>
</li>
<li>For site database recovery only before restoring Configuration Manager:<ul>
<li>Back up the site database, including supporting databases such as WSUS.</li>
<li>Note the SQL Server name and instance name.</li>
<li>Delete the site database from the SQL Server.</li>
<li>Restart the SQL Server.</li>
</ul>
</li>
<li>Clean an existing server for full recovery before restoring Configuration Manager:<ul>
<li>Back up the site database, including supporting databases such as WSUS.</li>
<li>Make a copy of the content library.</li>
<li>Uninstall the Configuration Manager site.</li>
<li>Manually delete the site database from the SQL Server.</li>
<li>Manually delete the Configuration Manager installation folder, and any other Configuration Manager folders.</li>
<li>Restart the server.</li>
<li>Restore the content library and other databases like WSUS.</li>
</ul>
</li>
<li>Use a supported version and the same edition of SQL Server:<ul>
<li>Check that the version of SQL Server is both supported, and the same edition as the original. Restoring to a newer version of SQL Server is supported, providing you don’t change the edition—you can restore Standard to Standard and Enterprise to Enterprise.</li>
<li>Ensure SQL Server is not set to <strong>single-user mode</strong>.</li>
<li>Make sure the MDF and LDF files are valid. When you recover a site, there’s no check for the state of the files.</li>
</ul>
</li>
<li>SQL Server Always On availability groups:&#x2F;<ul>
<li>If you use SQL Server Always On availability groups to host the site database, modify your recovery plans as described in <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/mem/configmgr/core/servers/deploy/configure/sql-server-alwayson-for-a-highly-available-site-database">Prepare to use SQL Server Always On</a>.</li>
</ul>
</li>
<li>Database replicas:<ul>
<li>After you restore a site database that you configured for database replicas, reconfigure each replica. Before you can use the database replicas, recreate both the publications and subscriptions.</li>
</ul>
</li>
</ol>
<h2 id="Troubleshoot-site-to-site-recovery"><a href="#Troubleshoot-site-to-site-recovery" class="headerlink" title="Troubleshoot site-to-site recovery"></a>Troubleshoot site-to-site recovery</h2><p>Azure to Azure Site Recovery allows you to replicate Azure virtual machines (VMs) from one region to another. In case of problems, check the following:</p>
<h3 id="High-data-change-rate-on-the-source-virtual-machine"><a href="#High-data-change-rate-on-the-source-virtual-machine" class="headerlink" title="High data change rate on the source virtual machine"></a>High data change rate on the source virtual machine</h3><p>Azure Site Recovery creates an event if the data change rate on the source virtual machine is higher than the supported limits. Go to <strong>Replicated items</strong> &gt; <strong>VM</strong> &gt; <strong>Events</strong> - <strong>last 72 hours</strong>. You should see the event <strong>Data change rate beyond supported limits</strong>:</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/3-backup-center-data-change.png" alt="Screenshot that shows Data change rate beyond supported limits."></p>
<p>Select the event to display disk information:</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/3-disk-information.png" alt="Screenshot that shows disk information."></p>
<h3 id="Azure-Site-Recovery-limits"><a href="#Azure-Site-Recovery-limits" class="headerlink" title="Azure Site Recovery limits"></a>Azure Site Recovery limits</h3><p>Azure Site Recovery limits are data churn per disk and data churn per virtual machine. The actual limits vary according to specific configurations. For example, a single VM Site Recovery can handle 5MB&#x2F;s of churn per disk and a maximum of five disks. Site Recovery has a limit of 54MB&#x2F;s of total churn per VM.</p>
<p>To find out whether this is a recurring problem, check the data change rate of the relevant virtual machine under <strong>Monitoring</strong>. You will need to add the metrics shown in the following screenshot:</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/3-data-change-rate.png" alt="Screenshot that shows O S Disk Writes Bytes per Sec."></p>
<h3 id="Network-connectivity-problems"><a href="#Network-connectivity-problems" class="headerlink" title="Network connectivity problems"></a>Network connectivity problems</h3><p>Site Recovery sends replicated data to the cache storage account. You might experience network errors if uploading the data from a virtual machine to the cache storage account is slower than 4MB in three seconds.</p>
<p>To check for latency problems, use the command-line utility <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10">AzCopy</a>. This uploads data from the virtual machine to the cache storage account. If the latency is high, check whether you’re using a network virtual appliance (NVA) to control outbound network traffic from VMs. The appliance might get throttled if all the replication traffic passes through the NVA.</p>
<p>Microsoft recommends creating a network service endpoint in your virtual network for “Storage” so that the replication traffic doesn’t go to the NVA.</p>
<h3 id="Network-connectivity"><a href="#Network-connectivity" class="headerlink" title="Network connectivity"></a>Network connectivity</h3><p>Site Recovery needs the VM to provide outbound connectivity to specific URLs or IP ranges. You might have your VM behind a firewall or use network security group (NSG) rules to&#x2F;l outbound connectivity. If so, you might experience issues. Make sure all the URLs are connected. For more information, see <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/azure-to-azure-about-networking">Outbound connectivity for URLs</a>.</p>
<p>For an up-to-date list of common issues, see: <a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/azure-to-azure-troubleshoot-replication">Troubleshoot replication of Azure VMs with Azure Site Recovery - Azure Site Recovery | Microsoft Docs</a>.</p>
<h1 id="Exercise-Troubleshoot-business-continuity-issues"><a href="#Exercise-Troubleshoot-business-continuity-issues" class="headerlink" title="Exercise: Troubleshoot business continuity issues"></a>Exercise: Troubleshoot business continuity issues</h1><p>Completed100 XP</p>
<ul>
<li>7 minutes</li>
</ul>
<p>You’ve been asked to make a backup of a virtual machine. The backup must be application consistent.</p>
<p> Important</p>
<p>You need your own <a target="_blank" rel="noopener" href="https://azure.microsoft.com/free/">Azure subscription</a> to complete the exercises in this module. If you don’t have an Azure subscription, you can still view the demonstration video at the bottom of this page.</p>
<h2 id="Create-the-exercise-environment"><a href="#Create-the-exercise-environment" class="headerlink" title="Create the exercise environment"></a>Create the exercise environment</h2><p>Using the Cloud Shell on the right, run these commands to create the example topology. The environment will take approximately 5 minutes to build.</p>
<ol>
<li><p>Clone the setup script from GitHub.</p>
<p>PowerShellCopy</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/MicrosoftDocs/mslearn<span class="literal">-business-continuity</span> networking</span><br></pre></td></tr></table></figure>
</li>
<li><p>Run the setup script.</p>
<p>PowerShellCopy</p>
<figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">networking/setup.ps1</span><br></pre></td></tr></table></figure>
</li>
<li><p>The script will create all the resources. Wait until it completes, you should see a <strong>Lab Environment Created</strong> message.</p>
</li>
</ol>
<h2 id="Create-a-backup"><a href="#Create-a-backup" class="headerlink" title="Create a backup"></a>Create a backup</h2><ol>
<li><p>Sign in to the <a target="_blank" rel="noopener" href="https://portal.azure.com/learn.docs.microsoft.com">Azure portal</a> using your own subscription.</p>
</li>
<li><p>Select the portal menu on the top left, select <strong>Virtual machines</strong>, and then select <strong>labvm</strong>.</p>
</li>
<li><p>If the VM is running, select <strong>Stop</strong>, and wait for the <strong>Status</strong> to show <strong>Stopped (deallocated)</strong>.</p>
</li>
<li><p>In the left navigation pane, under <strong>Operations</strong>, select <strong>Backup</strong>.</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/4-azure-backup-welcome.png" alt="Screenshot showing the back up welcome screen."></p>
</li>
<li><p>Enter the following settings, and then select <strong>Enable Backup</strong>.</p>
<ul>
<li>Recovery Services vault: <strong>Create new</strong></li>
<li>Backup vault: <strong>labdemo</strong></li>
<li>Choose backup policy: <strong>DefaultPolicy-labdemo</strong></li>
</ul>
</li>
<li><p>This takes you back to the <strong>Virtual machines</strong> page, select <strong>labvm</strong>.</p>
</li>
<li><p>In the left navigation pane, under <strong>Operations</strong>, select <strong>Backup</strong>.</p>
</li>
<li><p>Select <strong>Backup now</strong>, and then select <strong>OK</strong>.</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/4-machine-backup-screen.png" alt="Screenshot showing the backup screen."></p>
</li>
<li><p>The backup starts, and runs in two phases.</p>
<ul>
<li>In the first phase, it takes a snapshot of the VM. This takes about 10 minutes.</li>
<li>In the second phase, it moves the data into the recovery services vault. This takes about 50 minutes.</li>
</ul>
</li>
</ol>
<h2 id="Monitor-progress-of-the-first-phase"><a href="#Monitor-progress-of-the-first-phase" class="headerlink" title="Monitor progress of the first phase"></a>Monitor progress of the first phase</h2><p>You won’t see anything in the Backup page of the VM until the backup has completed. However, you can monitor it in the Backup center.</p>
<ol>
<li><p>In the Azure portal, search for <strong>backup center</strong>, and then under <strong>Services</strong>, select <strong>Backup center</strong>.</p>
</li>
<li><p>In the left navigation pane, under <strong>Monitoring + reporting</strong>, select <strong>Backup jobs</strong>. You should see your job running.</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/4-backup-instances.png" alt="Screenshot showing the backup job running."></p>
</li>
<li><p>Select the backup job to see more info. You can see in the screen picture that the first phase has completed.</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/4-single-backup.png" alt="Screenshot showing a single backup instance."></p>
</li>
</ol>
<h2 id="View-results-of-first-phase"><a href="#View-results-of-first-phase" class="headerlink" title="View results of first phase"></a>View results of first phase</h2><p>When the first phase has completed, you can see that it is <strong>Crash Consistent</strong>.</p>
<ol>
<li><p>In the breadcrumb trail, select <strong>Backup center</strong>.</p>
</li>
<li><p>In the left navigation pane, under <strong>Manage</strong>, select <strong>Backup instances</strong>.</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/4-multiple-backups.png" alt="Screenshot showing multiple backup instances."></p>
</li>
<li><p>Scroll down until you see <strong>labvm</strong> and select <strong>labvm</strong>. This shows that the backup is <strong>Crash Consistent</strong>. This is incorrect, the backup should be <strong>Application Consistent</strong>. You need to find out why and fix the problem.</p>
<p> Note</p>
<p>Only the first phase has completed, and the data will now be transferring to the vault. You can continue with the lab; you don’t have to wait for this to complete.</p>
</li>
</ol>
<h2 id="Resolution"><a href="#Resolution" class="headerlink" title="Resolution"></a>Resolution</h2><p>The VM was in a <strong>Stopped</strong> state. You need to start the VM and wait for it to be in a running state before making another backup.</p>
<ol>
<li><p>Select the portal menu on the top left, select <strong>Virtual machines</strong>, and then select <strong>labvm</strong>.</p>
</li>
<li><p>Select <strong>Start</strong>, and wait for the <strong>Status</strong> to show <strong>Running</strong>.</p>
</li>
<li><p>In the left navigation pane, under <strong>Operations</strong>, select <strong>Backup</strong>.</p>
</li>
<li><p>On the <strong>labvm</strong> page, you should see that there is one <strong>Crash Consistent</strong> restore point.</p>
</li>
<li><p>Select <strong>Backup now</strong>.</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/4-backup-screen.png" alt="Screenshot of the backup screen."></p>
</li>
<li><p>On the <strong>Backup now</strong> page, select <strong>OK</strong>. The second backup will be an incremental backup and should take less time than the first backup.</p>
</li>
<li><p>You can monitor progress in the <strong>Backup center</strong>.</p>
</li>
<li><p>When the first phase is complete, navigate to the <strong>Backup</strong> page of the VM. The send backup is <strong>Application Consistent</strong>.</p>
<p><img src="https://learn.microsoft.com/en-us/training/wwl-azure/business-continuity/media/4-crash-instance-restore-point.png" alt="Screenshot showing the crash consistent restore point."></p>
</li>
</ol>
<p>In this demonstration you will see how to proactively troubleshoot Conditional Access policies using the What if tool in the Azure portal:</p>
<iframe src="https://www.microsoft.com/en-us/videoplayer/embed/RE4Ubfe?postJsllMsg=true" frameborder="0" allowfullscreen="true" data-linktype="external" title="Video Player" style="box-sizing: inherit; outline-color: inherit; margin: 0px; padding: 0px; border: 0px; width: 640px; height: 360px; position: absolute; inset: 0px;"></iframe>



















<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Completed100 XP</p>
<ul>
<li>3 minutes</li>
</ul>
<p>In this module, you have learned how to troubleshoot common backup and restore issues when working in Azure.</p>
<p>Now that you have completed this module, you’ll be able to:</p>
<ul>
<li>Review the status of a backup job, manage alerts, and review logs.</li>
<li>Troubleshoot backing up and restoring a virtual machine (VM).</li>
<li>Troubleshoot Microsoft Azure Recovery Services backups.</li>
<li>Troubleshoot backing up and restoring servers.</li>
<li>Troubleshoot Azure to Azure Site Recovery.</li>
<li>Troubleshoot site recovery with Hyper-V, SCCM, and VMware.</li>
</ul>
<h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><p>Use these resources to discover more.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-center-support-matrix">Support matrix for Backup center</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-center-monitor-operate">Monitor and operate backups using Backup Center - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/monitoring-and-alerts-overview">Monitoring and reporting solutions for Azure Backup - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-monitoring-use-azuremonitor">Monitor Azure Backup with Azure Monitor - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-monitoring-built-in-monitor">Monitor Azure Backup protected workloads - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/azure-policy-configure-diagnostics">Configure Vault Diagnostics settings at scale - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-monitoring-built-in-monitor">Monitoring Azure Backup workloads</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-action-rules?tabs=portal">Alert processing rules for Azure Monitor alerts - Azure Monitor | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-mabs-troubleshoot">Troubleshoot Azure Backup Server - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-troubleshoot-vm-backup-fails-snapshot-timeout">Troubleshoot Agent and extension issues - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-mars-troubleshoot">Troubleshoot the Azure Backup agent - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/hyper-v-azure-troubleshoot">Troubleshoot Hyper-V disaster recovery with Azure Site Recovery - Azure Site Recovery | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/azure-to-azure-troubleshoot-replication">Troubleshoot replication of Azure VMs with Azure Site Recovery - Azure Site Recovery | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/vmware-azure-troubleshoot-replication">Troubleshoot replication issues for disaster recovery of VMware VMs and physical servers to Azure by using Azure Site Recovery - Azure Site Recovery | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-vm-file-recovery-troubleshoot">Troubleshoot Azure VM file recovery - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-vms-troubleshoot">Troubleshoot backup errors with Azure VMs - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-troubleshoot-slow-backup-performance-issue">Troubleshoot slow backup of files and folders - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/mem/configmgr/core/servers/manage/recover-sites">Site recovery - Configuration Manager | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/mem/configmgr/core/servers/deploy/configure/sql-server-alwayson-for-a-highly-available-site-database">Prepare to use an availability group - Configuration Manager | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-support-matrix-iaas">Support matrix for Azure VM backup - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/microsoft-azure-backup-server-protection-v3">Azure Backup Server V3 RTM protection matrix</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-mabs-protection-matrix">MABS (Azure Backup Server) V3 UR1 (and later) protection matrix</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/site-recovery-overview">About Azure Site Recovery - Azure Site Recovery | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-vms-introduction">About Azure VM backup - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-file-folder-backup-faq">Microsoft Azure Recovery Services (MARS) Agent – FAQ - Azure Backup | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/vmware-azure-tutorial">Set up VMware VM disaster recovery to Azure with Azure Site Recovery - Classic - Azure Site Recovery | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/hyper-v-azure-support-matrix">Support for disaster recovery of Hyper-V VMs to Azure with Azure Site Recovery - Azure Site Recovery | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/hyper-v-prepare-on-premises-tutorial">Prepare for disaster recovery of Hyper-V VMs to Azure with Azure Site Recovery - Azure Site Recovery | Microsoft Docs</a></li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/site-recovery/azure-to-azure-about-networking">Network virtual appliance configuration</a>.</li>
<li><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/backup/backup-azure-arm-vms-prepare">Back up Azure VMs in a Recovery Services vault - Azure Backup | Microsoft Docs</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Cert-Prep-Google-Cloud-Digital-Leader-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Cert-Prep-Google-Cloud-Digital-Leader-14/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Cert-Prep-Google-Cloud-Digital-Leader-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:56:56" itemprop="dateCreated datePublished" datetime="2022-11-14T12:56:56-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:42:24" itemprop="dateModified" datetime="2022-11-21T02:42:24-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Cert-Prep-Google-Cloud-Digital-Leader-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Cert-Prep-Google-Cloud-Digital-Leader-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p><object data="Cert-Prep-Google-Cloud-Digital-Leader.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:55:52" itemprop="dateCreated datePublished" datetime="2022-11-14T12:55:52-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:41:42" itemprop="dateModified" datetime="2022-11-21T02:41:42-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Resource-Required-Reading-for-Google-Cloud-Digital-Leader-Exam-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Required-Reading-for-Google-Cloud-Digital-Leader-Exam"><a href="#Required-Reading-for-Google-Cloud-Digital-Leader-Exam" class="headerlink" title="Required Reading for Google Cloud Digital Leader Exam"></a>Required Reading for Google Cloud Digital Leader Exam</h1><p>Before taking the Google Cloud Digital Leader exam, you should review the following documentation:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/architecture/migration-to-gcp-assessing-and-discovering-your-workloads#calculating_total_cost_of_ownership">Calculating total cost of ownership</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/products/gcp/improving-data-quality-for-machine-learning-and-analytics-with-cloud-dataprep">Data quality in machine learning</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/products/ai">GCP AI and ML products</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/storage">GCP Cloud Storage</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/security">GCP Security</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/apigee/docs/api-platform/get-started/what-apigee">Introduction to Apigee</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/looker">Introduction to Looker</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/products/it-ops/best-practices-for-optimizing-your-cloud-costs">Optimizing cloud costs</a></li>
<li><a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">OWASP Top Ten</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/solutions/unlocking-legacy-applications">Unlocking legacy applications using APIs</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/products/data-analytics/how-7-eleven-japan-built-its-new-data-platform">Updating legacy systems with GCP</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/learn/what-is-a-data-lake">What is a data lake?</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/learn/what-is-a-data-warehouse">What is a data warehouse?</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/learn/what-is-machine-learning">What is machine learning?</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:54:45" itemprop="dateCreated datePublished" datetime="2022-11-14T12:54:45-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:40:34" itemprop="dateModified" datetime="2022-11-21T02:40:34-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Site-Reliability-Engineering-Principles-on-GCP-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to Site Reliability Engineering Principles on Google Cloud Platform. My name is Daniel Mease and I’ll be taking you through this course. I’m a GCP instructor here at cloud Academy with over five years of cloud experience and 15 years of software development experience. If you have any questions or concerns about the content of this course, please connect with me on LinkedIn or send an email to <a href="mailto:daniel.mease@cloudacademy.com">daniel.mease@cloudacademy.com</a>. If you experience any problems or technical issues make sure to email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. And one of our cloud experts will get back to you as soon as possible.</p>
<p>There are a few different types of people who will benefit from taking this course. First, anyone who wants a basic introduction to Site Reliability Engineering. Second, anyone currently implementing DevOps practices who want to understand how Site Reliability Engineering can help play a role. Finally, anyone taking the Google Professional Cloud DevOps Engineer certification exam.</p>
<p>Now this course is going to provide a general overview of Site Reliability Engineering, including its history, core vocabulary, practices, and how it can complement DevOps. The content of this course is mostly high-level concepts. And so the prerequisites are gonna be pretty light. It would be useful to have a basic understanding of DevOps as well as having a basic understanding of the software development life cycle.</p>
<p>So if you’re ready to learn about Site Reliability Engineering, let’s get started.</p>
<h1 id="History-and-Goals"><a href="#History-and-Goals" class="headerlink" title="History and Goals"></a>History and Goals</h1><p>In order to better understand site reliability engineering or SRE, you first need to understand a little about its history as well as its relationship to DevOps. Historically, building a production system involved two different teams, developers and operators. Developers were responsible for updating and writing new software. Operators were responsible for deploying that software to production and monitoring it.</p>
<p>Now, operators were not programmers. They did not touch the code. Typically they didn’t even look at it, but they did understand how to assemble all the software components and make them work together to produce a surface. They also knew how to scale and maintain everything. If anything went wrong, it was the operators who sprung into action to resolve the problem. Each role had a different set of skills, and focused on different priorities. These differences often led to conflict.</p>
<p>Developers would spend their days fixing bugs, adding new features and constantly evolving the code. They were focused on agility and always wanted to make bigger and more frequent software updates. Operators spent their time fixing production issues and generally trying to keep everything running smoothly. They were focused on stability and always wanted smaller and less frequent updates. It was pretty common for operators to try to implement procedures, to slow down the rate of change while developers would try to bypass or ignore them. It was from this conflict that the idea for DevOps was born.</p>
<p>DevOps is a set of practices, guidelines and culture that was designed to reduce the gap between software development and software operations. The basic idea being, if we can get these two groups to work together, we can increase the company’s overall throughput and growth. In order to achieve this, DevOps established five goals. Number one, reduce organizational silos. The separation of development and operations meant that there was very little collaboration or cross training. At times these two groups were even working at cross purposes. Number two, accept failure as normal. Neither people nor systems are perfect. People who are scared of failure also tend to fear change, and a company that does not change will not grow. Number three, implement gradual changes. Big changes are harder, riskier and take longer to recover from in the event of failure. Smaller incremental changes are just the opposite. Easier, safer, and quicker to recover from. Number four, leverage tooling and automation. Manual work simply does not scale, and too much of it can be very costly. The right tooling and automation frees people up to do more interesting and valuable work. Number five, measure everything. Growth requires making the right changes. You need to constantly measure your performance to make sure you’re making the right decisions.</p>
<p>Now, the problem with DevOps as I previously mentioned was that it is very broad and it does not explicitly define how to implement these goals. That is where site reliability engineering or SRE comes in. Site reliability engineering evolved at Google independently of the DevOps movement, but it happens to embody many of the same goals. It also has a much more prescriptive way of achieving those goals. You can think of DevOps as a philosophy and site reliability engineering as one specific implementation of that philosophy. So, if you want to implement DevOps practices on Google Cloud Platform, it is valuable to understand SRE.</p>
<p>Site reliability engineering implements that DevOps goals with the following practices. Number one, creating the site reliability engineer role. This is a new role that replaces operators, and it focuses on sharing the responsibility of production with developers. Number two, holding blameless postmortems. These are retrospective meetings held after incidents to learn what failed and how to prevent those failures from happening again. Number three, defining and enforcing an error budget. Budgeting your money results in less spending and ensuring that the right bills get paid. In much the same way, an error budget encourages smaller changes and ensures the right balance of growth and stability is maintained. Number four, identify and work to reduce toil. Site reliability engineering defines medial operational tasks as toil, and it provides mechanisms for measuring and reducing that. Number five, track service level metrics and goals. These include SLIs, SLOs, and SLAs.</p>
<p>The following sections will cover each of these five concepts in more detail.</p>
<h1 id="SRE-as-a-Role"><a href="#SRE-as-a-Role" class="headerlink" title="SRE as a Role"></a>SRE as a Role</h1><p>So let’s talk about what a Site Reliability Engineer is and what their responsibilities include. DevOps and SRE have a lot in common, but there are some differences. DevOps is a company-wide culture and it is supposed to be everyone’s job. In contrast, Site Reliability Engineering is a specific job role. That means the old role of operator is replaced with another, the Site Reliability Engineer.</p>
<p>A Site Reliability Engineer is basically the result of asking a software engineer to design an operations team. This means that the new role requires experience in both software development as well as a strong knowledge of operations. Someone who acts as an SRE will spend about half of their time doing ops-related work, such as monitoring and responding to production issues, being on-call, or performing manual interventions. But the other half of their time will be spent on development tasks, such as building new features, scaling systems, or writing automation.</p>
<p>A Site Reliability Engineer views operations as a software problem, and uses software engineering approaches to solve issues. One key difference from the old operator role is that both SREs and developers share the responsibility of maintaining production. Developers are no longer allowed to write some code and then throw it over the wall expecting operations to figure it out and make it work. Instead, SREs build the tools that developers use to compile, test, and deploy their code. If something breaks, the tools written by the SRE team will detect and alert everyone. And when it comes time to fix the issue, developers and SREs work together to come up with a solution. So when an incident is detected, SREs help coordinate the response.</p>
<p>The person who declares the incident takes on the role of Incident Commander or IC. It is the IC’s job to direct the high-level state of the incident. To help assist, the IC will assign two other roles: first, an Operations Lead or OL and second, a Communications Lead or CL. The OL and the CL both report back to the IC. The Operations Lead role exists to lead the team who will be investigating and ultimately resolving the issue. The engineers doing the actual work report their progress back to the OL. And while the IC and OL are working to mitigate and resolve the incident, the Communications Lead or CL is busy keeping everyone informed and answering questions.</p>
<p>Clear communication is critical, and it needs to be made a high priority. Do not wait until after the incident has ended. The CL should immediately establish clear channels of communication, provide regular updates to the response team as well as stakeholders, and handle any inquiries. Site Reliability Engineers don’t just react to problems; they are proactive as well. SREs are heavily involved in the development process. In the design phase, SREs help establish best practices, identify architectural mistakes, and even help co-design parts of the service.</p>
<p>During development, SREs should be building the tools that everyone will use to eventually manage and maintain it, including monitoring and alerting. After deployment, SREs verify that the service is stable and performs as planned. And finally, once a service has been deprecated, SREs will help transition users from the old service to a new one, if available, as well as help clean up configurations and documentation.</p>
<p>Now, remember when I said that the first goal of DevOps is to reduce organizational silos? Site Reliability Engineering accomplishes this by involving SREs in development work and developers in operations work.</p>
<h1 id="Blameless-Postmortems"><a href="#Blameless-Postmortems" class="headerlink" title="Blameless Postmortems"></a>Blameless Postmortems</h1><p>In this section, I want to explain what a blameless postmortem is, including why they’re important and how they should be conducted. As stated previously, the second goal of DevOps is to accept failure as normal. Now, this is an important concept to understand. No matter how hard you try, failures will happen. Change and growth requires risk. Faster growth requires higher risk. In order to reduce risk to near zero, you would need to reduce your rate of change to near zero.</p>
<p>Now, ask yourself, will my users be satisfied with a stable but quickly out-of-date system? Instead of trying to avoid failure at any cost, you can view it as an opportunity to grow. When things break, we learn how to fix them. When you break and fix something enough times you gain a better understanding of how it works. This understanding helps minimize future issues and expedites the resolution process.</p>
<p>In site reliability engineering, this is accomplished through holding retrospectives or blameless postmortems. A retrospective or post-mortem is a meeting whose goal is to recap and analyze a significant service failure. It provides an open forum where everyone can ask questions, share their experience, and gain a clear understanding of exactly what happened. The goals of a post-mortem are to identify the contributing factors, determine how those factors could have been mitigated and to come up with a list of action items that will prevent the same failure from happening again.</p>
<p>A blameless post-mortem is one that focuses on dealing with the incident without trying to single out an individual or team for bad behavior. It assumes that everyone involved had good intentions and made the best choices they could with the information at hand. A post-mortem should never become a witch hunt, looking for someone or something to blame. This will inevitably create a culture in which issues are swept under the rug, leading to greater risk for the organization.</p>
<p>So, when conducting a post-mortem, you will want to ask the following questions:</p>
<ol>
<li>When did the incident begin?</li>
<li>When did the incident end?</li>
<li>How were we notified that there was a problem? It is important to understand exactly how long the problem lasted and how long it took you to notice that there was a problem. You may need to adjust your monitoring and alert system if there is a significant gap between the two. </li>
<li>Who was involved in responding?</li>
<li>When did we begin to respond?</li>
<li>What was our response? Carefully review what your response was. Was it quick enough? Did it follow established procedures? Was it sufficient?</li>
<li>What was affected? It is important to document everything that was affected, which systems were down, which customers noticed, was there any revenue lost.</li>
<li>Is there anything else that still needs to be done to recover? You might’ve made some temporary fixes that need to be replaced with a long-term solution, or there may be other after effects that need to be dealt with in the future.</li>
<li>What were all the things that contributed to this failure? Remember that in complicated systems there are usually many contributing factors to a failure, and not a single root cause. Identify all factors and document them. This should include writing up new bugs in your tracking system. </li>
<li>How can we avoid similar problems in the future? It is critical to establish detailed action items. What specific changes are going to be made? What is the deadline for making those changes? Who specifically is going to be responsible for making them? Remember you wanna focus on solutions. Don’t assign blame for past mistakes, but do assign responsibility for future improvement. Common types of changes will include things such as fixing bugs, adding new infrastructure, updating or building new tools, revising current policies and creating new documentation or training. </li>
<li>What went right? </li>
<li>What went wrong?</li>
<li>Where did we get lucky? You not only want to identify what went wrong, but also the things that help mitigate and recover from the issue. And sometimes you just get really lucky and it’s important to call that out as well.</li>
</ol>
<p>You want an accurate understanding of all risks so that you can properly prioritize your fixes. Blameless postmortems encourage open and honest communication after a service failure. They acknowledge that system design is complicated and that human beings make mistakes. They also ensure that your team will learn from those mistakes and avoid repeating them in the future.</p>
<h1 id="Toil"><a href="#Toil" class="headerlink" title="Toil"></a>Toil</h1><p>In this next section, I will define what toil is and explain how it can be mitigated using automation. Earlier, I had mentioned that the third goal of DevOps is to leverage tooling and automation. A lot of traditional operations work was manual, repetitive and labor intensive. Common examples would include things like resetting passwords, responding to alerts, rolling out patches, restarting servers, and copying and pasting commands from a playbook. S.R.E calls this type of work toil.</p>
<p>Toil occurs every time an operator needs to manually touch a system during normal operations. Keep in mind that toil is not a synonym for boring or frustrating. Filling out an expense report may not be fun but that does not make it toil. Instead, toil is work that is tied to running a production service and tends to be manual repetitive, automatable, tactical, and devoid of long-term value. The amount of toil increases linearly as the service grows and if ignored can grow out of control until your entire team is consumed by it. Too much toil leads to career stagnation, boredom and burnout.</p>
<p>You can group S.R.E activities into four main categories: software engineering, systems engineering, overhead, and toil. Software engineer includes things like writing automation scripts, creating tools, or modifying infrastructure code to make it more robust. Systems engineering includes things like installing updates, server configuration, or load balancer setup. Overhead is administrative work that’s not directly tied to running a service and includes things like conducting interviews, attending meetings and completing peer reviews. Toil is the work directly tied to running a service and it’s repetitive, manual, et cetera.</p>
<p>The S.R.E discipline aims to reduce toil through automation. S.R.E is try to identify repeatable tasks and write programs to reproduce the work. This means creating things like scheduled jobs instead of manually running scripts, automated monitoring tools instead of manually monitoring and rebooting unresponsive servers, continuous integration and continuous deployment pipelines instead of manually testing and deploying new code. And auto-scaling infrastructure instead of manually provisioning new hardware.</p>
<p>While automation is extremely helpful, not every task is worth automating. But if a repetitive task can be automated, it probably should be. And once it has, you’ve successfully freed up more resources for future development efforts. Ideally, S.R.Es will spend about half their time or less on toil and the other half on reducing toil for themselves and others.</p>
<h1 id="Error-Budgets"><a href="#Error-Budgets" class="headerlink" title="Error Budgets"></a>Error Budgets</h1><p>Next, I will talk about what an error budget is, what it is used for and how it can help provide direction for a team’s future activities. Recall that the fourth goal of DevOps is to implement gradual change. The number one source of outages is change. Whether that is adding new features, applying security patches or deploying new hardware. Any of these things can potentially impact your uptime.</p>
<p>So the question is how do I balance the proper amounts of change and stability? Extremely high stability is expensive and can drive your innovation down and prices up, maybe beyond what your customers are willing to bear. But extremely high rates of change can drive your failure rates up and your customers elsewhere.</p>
<p>So what then is the right target of reliability for your system? This is an important question, but it isn’t technical. It’s really a question for the business. You need to understand things such as how much can the service fail before it begins to have a significant negative impact? How quickly do we need to be able to release new features? And what type and how many resources are available? These answers will most likely need to come from your product team as it will require an understanding of your users behavior, your business needs, and your product roadmap.</p>
<p>Once you’ve determined the right target for reliability, you can enforce it by using an error budget. An error budget works in a similar way to a monetary budget. A certain amount of errors or downtime is allocated to each service. As long as the number of errors or downtime of the service does not exceed the error budget, then the service is considered to be reliable enough.</p>
<p>So how exactly do you spend your error budget? Well, as failures increase, the error budget is consumed. If a service goes down and it’s unresponsive that downtime is subtracted from the budget. Also, if the performance of a service drops beyond an acceptable threshold, the time it takes to restore performance is subtracted from the budget as well.</p>
<p>The longer your service is down or degraded, the more is subtracted from the error budget. And as your error budget shrinks, your team should respond by shifting resources away from adding new features and onto making more reliability improvements. This could include changing feature priority, reallocating developers to different projects or even delaying certain releases to a later time.</p>
<p>If failures continue to increase, your error budget might be in danger of being completely depleted. In this case, your team should hold all new deployments and focus completely on restoring service back to an acceptable range. At this point, you may end up with all of your S.R.S.Es and developers working a hundred percent on stability fixes and improvements.</p>
<p>Finally, once failures begin to decrease, your uptime will stabilize and your error budget will begin to replenish. It is at this point that your team can begin to shift resources back towards new development. So as you can see with an error budget, your team can commit to releasing features as quickly as a safe, where safe means staying within your budget.</p>
<p>Just like a monetary budget, picking the right amount is critical. Even small changes to an error budget can have a significant effect. Let’s say that you wanted to ensure that a service was highly available and set the error budget at 0.01%. That would mean that your service could only be down for four and a half minutes per month. Now with a budget the small, your team is going to be mostly focused on reliability, changes will be limited. One small problem could consume the entire budget. However, if you’re willing to accept a larger 1% error budget, your service can now be down for up to seven hours per month. This would allow for more frequent and riskier change.</p>
<p>Just like a monetary budget, the team is likely to spend the entire budget. So setting the budget too large could result in unnecessary downtime. Smaller budgets allow greater stability at the risk of slowing down the rollout of new features, while larger budgets allow the quicker release of features at the risk of longer and more frequent outages. No matter the size error budgets push teams towards making smaller, more gradual changes. A small deployment gone bad can be much more easily mitigated. A large deployment gone bad can exceed the budget, freeze development, and break schedules.</p>
<h1 id="Service-Level-Objectives-SLOs"><a href="#Service-Level-Objectives-SLOs" class="headerlink" title="Service Level Objectives (SLOs)"></a>Service Level Objectives (SLOs)</h1><p>The fifth and final goal of DevOps is to measure everything. In order to be successful, you need to set goals. And you need a way to measure your progress towards meeting those goals. To help with this, Site Reliability Engineering has three defined metrics: SLOs, SLIs, and SLAs. First, I will talk about SLOs or Service Level Objectives and how you can use them to define success.</p>
<p>A Service Level Objective is a goal that your business aspires to meet and intends to take action to defend. The error budget for a service is directly related to the Service Level Objective. Your error budget represents the percentage of time that your service can be down, while your SLO represents the percentage of time that your service should be up. Here is a simple formula to help explain: Your error budget plus your SLO will equal 100%. An error budget of 2% implies an SLO of 98%.</p>
<p>SLOs provide a clear signal that your service is performing successfully. There is always some small amount of error in any system. Without clearly defined limits, you won’t know if your current error rate is high enough to constitute a serious issue or not. You also won’t be able to accurately prioritize improvements.</p>
<p>Services usually have multiple SLOs associated with them. And typical SLOs include things like availability, response time, and latency. For example, you might have an SLO that requires 99% of all web server responses to be non-500 errors. Or you might have an SLO that requires 95% of your home page requests to be served in under 200 milliseconds.</p>
<p>SLOs are not intended to define ideal, best-case performance. A good rule of thumb is that your SLOs should represent the lowest level of reliability that you can get away with. You can pick all kinds of different objectives, but not every objective is useful. SLOs need to be meaningful. Meeting your SLOs should result in happy users. Missing an SLO should result in unhappy users. Failing to meet an SLO can potentially have serious consequences: damaged reputations, drops in revenue, or even a loss of customers.</p>
<p>SLOs also need to be attainable, measurable, and repeatable. Picking an objective you cannot possibly achieve or reproduce is useless and just causes needless frustration. Also, SLOs need to be understandable and controllable. You need to know how to achieve your objectives, and have the ability to make the changes necessary to do so. Correctly setting and measuring service level objectives is a key aspect of the SRE role.</p>
<p>SLOs not only assist in measuring your success, but they can also be used to create powerful feedback loops. They show you which parts of your system needs improvement, and by how much. Thus, allowing you to easily identify trouble spots and prioritize work. By tracking your current performance versus your SLOs, you will get instant feedback on any changes and will be able to know with confidence what your team should be working on next.</p>
<h1 id="Service-Level-Indicators-SLIs"><a href="#Service-Level-Indicators-SLIs" class="headerlink" title="Service Level Indicators (SLIs)"></a>Service Level Indicators (SLIs)</h1><p>An SLO by itself is not very useful. You need to compare your objectives against your current performance. This is what a service level indicator, or SLI, is used for. SLIs are the metrics of your system tracked over time. Similar to SLOs, service level indicators are reported as percentages. SLIs range from zero to 100%, where zero means nothing works and 100% means everything is working perfectly.</p>
<p>The basic formula for calculating an SLI is the total number of good events divided by the total number of events multiplied by 100. So let’s say you have an SLO that requires 95% of your homepage requests to be served in under 200 milliseconds. If your current SLI was only 94%, that would mean your service is performing below minimum expectations and that this problem needs to be fixed. If the SLI was actually 96%, then the service would be working as expected.</p>
<p>SLOs and SLIs allow you to quickly understand which services are performing well and which are experiencing problems. They also let you know how severe any detected problems are. SREs typically use monitoring tools or services to track and monitor SLIs. Just like your SLOs, your service level indicators should be focused on measuring the customer experience.</p>
<p>A good SLI should rise when customers are happy and fall when they are unhappy. If a metric can change and not significantly impact the customer experience, then it probably isn’t worth tracking via SLI. Now there are four golden signals of monitoring, latency, traffic, errors, and saturation. If you can only measure a few metrics, focus on these four.</p>
<p>Latency tells you how quickly a certain percentage of requests can be fulfilled. Traffic tells you how much demand is being placed on your system. Errors tell you the rate of requests that fail, and saturation tells you how full your service is. There are many other types of SLIs and different systems use different types. Here are some examples. Typical SLIs for serving systems include availability, quality, and latency. Typical SLIs for data processing includes coverage, correctness, freshness, and throughput. And typical SLIs for storage systems include durability, throughput, and latency.</p>
<p>By setting the right SLOs and tracking the right SLIs, you create a clear path forward for success.</p>
<h1 id="Service-Level-Agreement-SLAs"><a href="#Service-Level-Agreement-SLAs" class="headerlink" title="Service Level Agreement (SLAs)"></a>Service Level Agreement (SLAs)</h1><p>A Service Level Agreement or SLA is a guarantee you make to your customers. It is a contract with consequences of failing to meet the SLOs they contain. SLOs and SLAs are similar. However, your SLAs should not be the same as your SLOs. Both are objectives, but an SLO is an internal objective, only used within the team. If the team fails to meet the SLO, then they may slow down deployments or hold a blameless postmortem.</p>
<p>SLAs violations are shared with your customers and usually require some sort of recompensation, such as a credit or refund. Also, they should not be the same because SLOs are supposed to be stricter than SLAs. You want to be notified of any problems and have a chance to address them well before they affect your customers. Ideally, the three metrics should exist on a spectrum. You want your SLIs to be higher than both your SLOs and SLAs. This means you are meeting your objectives and your service is performing as expected. You also want your SLOs to be higher than your SLAs. If your SLI drops below your SLO, you are in violation and need to take steps to resolve the issue. If your SLI drops below your SLA, you need to notify your customers and offer them compensation. It’s better to break an internal objective than one that is visible to your customers.</p>
<p>SLAs need to be carefully set. Making them too high means you’re more likely to violate them. Making them too low means your customers may feel less confident in your ability to deliver a quality service. SLAs that are too close to SLOs mean that you’re less likely to catch a problem in time to prevent it. However, if your SLAs are too far away from your SLOs, that too can be a problem.</p>
<p>The SLO you run at tends to become the SLA everyone expects. So let’s say you offered a 95% SLA, but were consistently delivering 99.99% for a long period of time. If your SLI then dropped to 98%, you might get customer complaints. You see, even though you only promised 95%, you proved that you can actually deliver higher. And now your customers have begun to depend upon this higher level of service. To avoid this issue, Google recommends adding extra downtime to services to prevent them from being overly available. As you can probably tell, picking the right SLOs and SLAs can be tricky. You may need to start tracking SLIs for a while, and then use the average to help define realistic SLOs and SLAs. In any case, Site Reliability Engineering recommends making sure they are all a part of your system requirements. And if you already have a production system but don’t have them clearly defined, then that should be your highest priority.</p>
<h1 id="Review-and-Resources"><a href="#Review-and-Resources" class="headerlink" title="Review and Resources"></a>Review and Resources</h1><p>As we have seen, SRE and DevOps complement each other quite nicely. Although they are different, they both share the same underlying goals. DevOps, which is the broader of the two, seeks to one, reduce organizational silos. Two, accept failure as normal. Three, implement gradual changes. Four, leverage tooling and automation. And five, measure everything.</p>
<p>Site reliability engineering provides a specific implementation for achieving these same goals by one, the SRE role, which shares responsibility of production with developers. Two, blameless postmortems to learn from mistakes and to avoid a culture of fear and blame. Three, error budgets to balance growth with stability. Four, to identify and reduce toil via automation. And five, tracking SLIs against defined SLOs and SLAs.</p>
<p>Remember, an SLI is a measurement of how your system is performing, an SLO is an internal goal, and an SLA is a guarantee to customers. At this point, you should now have a basic understanding of site reliability engineering principles. If you are interested in learning more, I encourage you to check out the following resources.</p>
<p>First, we offer a whole learning path called <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/site-reliability-engineering-sre-foundation-1759/">Site Reliability Engineering Foundation</a> Learning Path. It provides more in-depth knowledge about SRE and covers a wider range of topics. This is a great resource for deeper understanding. Also, there are a number of excellent SRE resources available from google at <a target="_blank" rel="noopener" href="https://sre.google/">sre.google</a>. And if you’re planning to take the Professional Cloud DevOps Engineer Exam, I highly recommend three resources in particular. First is the “<a target="_blank" rel="noopener" href="https://sre.google/sre-book/table-of-contents/">Site Reliability Engineering</a>“ book. Second is “<a target="_blank" rel="noopener" href="https://sre.google/workbook/table-of-contents/">The Site Reliability Workbook</a>“, and finally, the <a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLIivdWyY5sqJrKl7D2u-gmis8h9K66qoj">playlist</a> of short videos that helps explain core SRE concepts.</p>
<p>Well, that’s all I have for you today. Remember to give this course a rating and if you have any questions or comments, please let us know. Thanks for watching and make sure to check out our many other courses on Cloud Academy.</p>
<h1 id="4Blameless-Postmortems"><a href="#4Blameless-Postmortems" class="headerlink" title="4Blameless Postmortems"></a>4<strong>Blameless Postmortems</strong></h1><p><a target="_blank" rel="noopener" href="https://sre.google/sre-book/postmortem-culture/">Postmortem Culture: Learning from Failure</a></p>
<h1 id="10Review-and-Resources"><a href="#10Review-and-Resources" class="headerlink" title="10Review and Resources"></a>10<strong>Review and Resources</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/site-reliability-engineering-sre-foundation-1759/">Site Reliability Engineering Foundation Learning Path</a></p>
<p><a target="_blank" rel="noopener" href="https://sre.google/">Google SRE resources</a></p>
<p><a target="_blank" rel="noopener" href="https://sre.google/sre-book/table-of-contents/">Site Reliability Engineering book</a></p>
<p><a target="_blank" rel="noopener" href="https://sre.google/workbook/table-of-contents/">The Site Reliability Workbook</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLIivdWyY5sqJrKl7D2u-gmis8h9K66qoj">SRE videos playlist</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:44:33" itemprop="dateCreated datePublished" datetime="2022-11-14T12:44:33-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:40:48" itemprop="dateModified" datetime="2022-11-21T02:40:48-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Introduction-to-Google-Cloud-Operations-Suite-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to our “Introduction to Google Cloud Operations Suite”. I’m Guy Hummel, and I’ll be showing you how to monitor your GCP systems.</p>
<p>To get the most from this course, you should know the fundamentals of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, such as how to create virtual machine instances and use App Engine. If you need a refresher, then you can take our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/google-cloud-platform-fundamentals/">Google Cloud Platform: Fundamentals</a> course.</p>
<p>You should also have experience with performing operations tasks, especially working with Linux. It would also be helpful to have some programming experience, although just knowing the basics of a typical programming language should be enough.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>We’ll be going through the components of the Cloud Operations suite to monitor, log, report errors, debug, and trace your applications.</p>
<p>Now, if you’re ready to learn how to monitor your Google Cloud infrastructure, then let’s get started.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h1><p>After you’ve implemented your infrastructure in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud platform</a>, the first thing you’ll want to do is set up a monitoring system that will alert you when there are major problems. The easiest way to do this is to use Cloud Operations, Google’s powerful monitoring, logging, and debugging tool.</p>
<p>To get to it, select “Monitoring” from the console menu. The first time you bring up Monitoring in a project, it’ll take a while to set up. I’ll fast-forward.</p>
<p>Here’s where you’ll find the instructions for installing the Monitoring Agent (and the Logging Agent). You don’t need to install the agent to be able to use Monitoring, but if you do install it, you can get more information about an instance, such as about the third-party software running on it. We don’t need to install it yet, so we’ll leave that until later.</p>
<p>Suppose you want to monitor a web server and get notified if it goes down. First, you need to create an Uptime Check. </p>
<p>For the title, let’s call it “Example”. Click “Next”. Since we want to check if a web server is up, leave the Protocol as HTTP and the Resource Type as URL.</p>
<p>For the hostname, I’m going to put in the IP address of an instance I have that’s running a web server. Leave “Check Frequency” set to 1 minute. And click “Next”. We can leave the Response Validation with the defaults, so click “Next” again.</p>
<p>This is where we set up an alert for when the uptime check fails. First, we specify how long a failure has to last before it’ll trigger an alert. Let’s leave it set to 1 minute. We should also tell it how to send alert notifications. Click the dropdown menu, and then click “Manage Notification Channels”. You can be alerted by email, text message, or a variety of other options, such as Slack. We’ll get it to send an email when the web server is down. Click “Add New”, and enter your email address and your name.</p>
<p>Okay, now close this tab in your browser, and go back to the previous one. Click Refresh and select your email. Now click the “Test” button. Since the web server at that address is up, it came back right away.</p>
<p>Now I’m going to stop Apache on the instance that’s running the web server and test it again. This time, the connection failed, as expected. Click the Create button.</p>
<p>To see the results of the uptime check, go to the menu on the left and select Uptime Checks. It’ll take a while before it runs for the first time, so don’t worry if you don’t see anything in the dashboard right away. I’ll skip ahead to when the uptime check has run. OK, now you can see it’s showing that the web server’s down. After a little while, it’ll send a notification email. Here’s what it looks like.</p>
<p>Now I’ll start Apache up again and see if the alert policy sees it. I’ll just skip ahead a few minutes. Yes, it sees that the web server is up now.</p>
<p>If you want to see data graphically, then click on Dashboards. It provides default dashboards for many Google services. To create your own, click Create Dashboard. I’ll call it “Example Dashboard”.</p>
<p>Now click the “Add Chart” button. In this search field, type “URL”. There’s the resource type we need. It’s called “Uptime Check URL”. It gives us a few different metrics to choose from. The obvious one to choose is “Check passed”, but to make things more interesting, let’s choose “Request latency”. Click Save and the graph will be added to your dashboard.</p>
<p>This graph shows the network latency between each region and the web server. You can see when the web server was down, but it gives us more information than that. This network latency data from different locations around the world can be quite helpful, especially if some of your users are reporting slow performance.</p>
<p>Note that you’ll need to refresh this page to see the latest data. You can turn on auto-refresh if you want.</p>
<p>Suppose you’d like to get more information about the instance where the web server is running, such as the CPU load. Let’s create another chart.In the search field, type “cpu load”. Let’s select the 1-minute version. For the resource type, select “VM instance”. Notice that you can even monitor Amazon EC2 instances.</p>
<p>You’ll notice that the chart is blank. That’s because we need to install the Monitoring Agent to get CPU data.</p>
<p>Go back to the Overview, and then click this link to bring up the installation instructions. The instructions are different depending on which Linux distribution you’re running on your instance. I’m running Debian, so I need to follow these instructions. I’ll fast-forward to the point after I’ve run all of the install commands.</p>
<p>While we’re here, let’s install the logging agent too. You can find a link to the documentation in the GitHub repository I created for this course. The link to the GitHub repository is at the bottom of the Overview tab below this video.</p>
<p>Installing the logging agent will prepare this instance for the next lesson when we use Cloud Logging. I’ll fast-forward again.</p>
<p>Now let’s go back and see what happened to our chart. Okay, now there’s a line showing the load average, so it worked.</p>
<p>If you’ve been following along using your own account, then you should go back and delete the monitoring you set up. First, go to the Alerting page from the menu, and then delete the policy. You have to do that before it will let you delete the uptime check. Okay, now go to “Uptime Checks” and delete the one you created. Finally, go to Dashboards, click on the one you created, and delete it.</p>
<p>That’s it for this lesson.</p>
<h3 id="Lectures-1"><a href="#Lectures-1" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h1><p>Looking at real-time monitoring is great, but there will be many times when you’ll want to look at what happened in the past. In other words, you need logs.</p>
<p>For example, suppose I wanted to see when a VM instance was shut down. Compute Engine, like almost every other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a> service, writes to the Cloud Audit Logs. These logs keep track of who did what, where, and when.</p>
<p>There are three types of audit logs. Admin Activity logs track any actions that modify a resource. This includes everything from shutting down VMs to modifying permissions.</p>
<p>System Event logs track Google’s actions on Compute Engine resources. Some examples are maintenance of the underlying host and reclaiming a preemptible instance.</p>
<p>Data Access logs are pretty self-explanatory. They track data requests. Note that this also includes read requests on configurations and metadata. Since these logs can grow very quickly, they’re disabled by default. One exception is BigQuery Data Access logs, which are not only enabled by default, but it’s not even possible to disable them. Fortunately, you won’t get charged for them, though.</p>
<p>In the console, select “Logging”.</p>
<p>There are lots of options for filtering what you see here. You can look at the logs for your VM instances, firewall rules, projects, and many other components. You can even send logs from other cloud platforms like AWS to here. You just need to install the logging agent on any system that you want to get logs from.</p>
<p>This is a great way to centralize all of your logs. Not only does centralizing your logs make it easier to search for issues, but it can also help with security and compliance, because the logs aren’t easy to edit from a compromised node.</p>
<p>In this case, we need to look at the VM instance logs. You can choose a specific instance or all instances. I only have one instance right now, called instance-1. Since we installed the logging agent on instance-1 in the last lesson, there are already some log entries for it. </p>
<p>Here you can choose which logs you want from the instance, such as the Apache access and error logs. I could set it to “syslog” since that’s where the shutdown message will be, but I’ll just leave it at “All logs” because sometimes you might not know which log to look in.</p>
<p>You can also filter by log level, and for example, only look at critical entries. I’ll leave it at “Any log level”.</p>
<p>Finally, you can change how far back it will look for log entries. I’ll change it to the last 24 hours.</p>
<p>OK, now I’ll search for any entries that contain the word “shutdown” so I can see if this instance was shut down in the last 24 hours.</p>
<p>If you need to do really serious log analysis, then you can export the logs to BigQuery, which is Google’s data warehouse and analytics service. Before you can do that, you need to have the right permissions to export the logs. If you are the project owner then, of course, you have permission. If you’re not, then the “Create Sink” button will be greyed out, and you’ll have to ask a project owner to give you the Logs Configuration Writer role.</p>
<p>First, click the “Create Sink” button. A sink is a place where you want to send your data. Give your sink a name, such as “example-sink”. Under “Sink Service”, you have quite a few options, such as BigQuery, Cloud Storage, or a Custom destination. We’ll choose BigQuery. </p>
<p>Under “Sink Destination”, you have to choose a BigQuery dataset to receive the logs. If you don’t have one already, then click “Create new BigQuery dataset”. Give it a name, such as “example_dataset”. Note that I used an underscore instead of a dash because dashes are not allowed in BigQuery dataset names. Now click the “Create Sink” button.</p>
<p>It says the sink was created, so let’s jump over to BigQuery and see what’s there. Hmmm. It created our example dataset, but it doesn’t contain any tables, which means it doesn’t have any data. That’s weird, right? Well, it’s because when you set up a sink, it only starts exporting log entries that were made after the sink was created.</p>
<p>OK, then let’s generate some more log entries and see if they get exported. I’ll restart the VM, which will generate lots of log entries. Okay, I’ve restarted it. Now if we go back to the Logging page, do we see the new messages? Yes, we do.</p>
<p>Now let’s go back to BigQuery and see if the data’s there. Yes, there are two tables there now. Click on the syslog table. Now click the “Query Table” button. To do a search in BigQuery, you need to use SQL statements, so let’s write a simple one just to verify that the log entries are there.</p>
<p>Thankfully, it already gave me the skeleton of a SQL statement. I just need to fill in what I’m selecting. I’ll put in an asterisk to select everything, but I’ll restrict it by using a WHERE clause with the column name “textPayload” (which is the column that contains the text in the log entry)…”LIKE ‘%shutdown%’”. The percent signs are wildcards, so this SQL statement says to find any log entries that have the word “shutdown” in them somewhere.</p>
<p>Now we click the “Run” button…and it returns the matching log entries. If we scroll to the right, then we can see the textPayload field and it does indeed contain the word “shutdown” in each of the entries.</p>
<p>Of course, we did exactly the same search on the Logging page and it was way easier, so why would we want to go through all of this hassle of exporting to BigQuery and writing SQL statements? Well, because sometimes you may need to search through a huge number of log entries and need to do complicated queries. BigQuery is lightning fast when searching through big data, and if you build a complex infrastructure in Google Cloud Platform, then the volume of log data it will generate will easily qualify as big data.</p>
<p>Since we don’t want our example sink to keep exporting logs to BigQuery and incurring storage charges, let’s delete what we’ve created. On the Logging page, click on “Logs Router” in the left-hand menu, then select the sink and delete it.</p>
<p>We should also delete the BigQuery dataset, so go back to the BigQuery page, select the dataset, and click “Delete Dataset”. It wants you to be sure that you actually want to delete the dataset, so you have to type the dataset name before it will delete it.</p>
<p>One concern that you or your company may have is how to ensure the integrity of your logs. Many hackers try to cover their tracks by modifying or deleting log entries. There are a number of steps you can take to make it more difficult to do that.</p>
<p>First, apply the principle of least privilege. That is, give users the lowest level of privilege they need to perform their tasks. In this case, only give the owner role for projects and log buckets to people who absolutely need it.</p>
<p>Second, track changes by implementing object versioning on the log buckets. The Cloud Storage service automatically encrypts all data before it is written to the log buckets, but you can increase security by forcing a new version to be saved whenever an object in a log bucket is changed. Unfortunately, this won’t prevent an owner from deleting an incriminating object, which is why you need to keep tight control on which users are given the owner role.</p>
<p>Third, you could add more protection by requiring two people to inspect the logs. You could copy the logs to another project with a different owner using either a cron job or the Cloud Storage Transfer Service. Of course, this still won’t prevent an owner in the first project from deleting the original bucket before the copy occurs or from disabling the original logging.</p>
<p>So the bottom line is that a person with the owner role can get around just about anything you put in place, but you can make it nearly impossible for someone without the owner role to change the logs without you knowing about it.</p>
<p>And that’s it for this lesson.</p>
<h3 id="Lectures-2"><a href="#Lectures-2" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Error-Reporting-and-Debugging"><a href="#Error-Reporting-and-Debugging" class="headerlink" title="Error Reporting and Debugging"></a>Error Reporting and Debugging</h1><p>So far, we’ve been looking at alerts and log messages from system software. Now it’s time to look at how to get error information from your applications. That’s where the Cloud Error Reporting service comes in. To show you how this works, I’m going to install Google’s Hello World application in App Engine and then get it to generate an error.</p>
<p>Normally, you’d use your own workstation for the development environment, but to simplify this demo, I’m going to use Cloud Shell. The nice thing about Cloud Shell is that it already has all of the packages installed that you need. When you do want to write and test Java code on your own workstation, remember that you need to install the Google Cloud SDK, the Java SE 11 Development Kit, Git, and Maven 3.5 or greater on your system.</p>
<p>First, open Cloud Shell. Next, get a copy of the Hello World application with this “git clone” command. Then go into the directory where the app is.</p>
<p>Now use the local development server to make sure the app works. To see if it’s working, click the “Web Preview” icon, and select “Preview on port 8080”. You should see a “Hello world!” message. OK, it’s working, so let’s stop the development server and upload the application to App Engine. You can stop the development server with a Ctrl-C.</p>
<p>Now use the “gcloud app deploy” command to upload it to App Engine. If you’re doing this yourself, then it may look slightly different than mine because I’ve already configured App Engine. If this is your first time deploying to App Engine, then it will likely ask you to choose a region. OK, it’s done deploying.</p>
<p>There are a couple of ways to test it. If you’re not using Cloud Shell, then you could do a “gcloud app browse”, which is pretty handy. Since we are using Cloud Shell, we’ll have to go to this URL. There’s “Hello World!” again.</p>
<p>Now, in order to see an error on the Error Reporting page, we need to generate an error. Let’s edit the code and mess something up. I’m going to add a line that I know will cause a problem. This’ll throw an exception because you can’t divide a number by zero.</p>
<p>Now run “gcloud app deploy” again. When it’s done, bring the app up in your browser again. This time it gives you a big error message, which is actually what we want, for once.</p>
<p>Let’s see if Error Reporting picked it up. The <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud</a> Console shows errors on the main dashboard, so you don’t have to go to the Error Reporting page to see them. I’ll refresh the page. There it is. Click on “Go to Error Reporting” to see what it shows. If you click on the error, you’ll see more details, including the stack trace. You can even click on the line where the error occurred and it will take you into your source code in the Debugger. However, in this case, the line it’s showing at the top of the stack trace is not in our code. We can click on this line, though, which is in our code, and it should take us there.</p>
<p>The Debugger is a great tool that you can use whether an error occurred or not. Let’s put a more subtle problem in the code and see how we can use the Debugger to figure out what’s wrong.</p>
<p>Suppose we want to check the operating system running our app, and if it’s Ubuntu, then we’ll print “Ubuntu rocks!”</p>
<p>First, we have to fix the bug that we introduced previously, so we’ll remove that line. We have to go back to the editor to do that. Now I’ll add the new code. Even if you’re not familiar with Java, this is pretty straightforward. It gets the name of the operating system, then it checks to see whether it’s equal to Ubuntu or not, and if it is, it says, “Ubuntu rocks!”, and if it isn’t, it says, “Hello world!”.</p>
<p>Now we’ll upload it to App Engine again. OK, now we’ll refresh the webpage. And it says “Hello world!” again, not “Ubuntu rocks!” That might be because the underlying operating system isn’t Ubuntu, but let’s go back to the Debugger and see if that’s the reason.</p>
<p>You’ll notice that this is still the old version of the file. First, refresh the browser. It’s still showing the old version. To get to the new version, you have to click on this drop-down menu and select the right one. The latest version should say 100% at the end. Sometimes you have to tell it where the source code is. Find “App Engine” in the list, and click the “Select source” button.</p>
<p>Now find the file. You’ll see some text on the right-hand side that says to click a line number to take a snapshot of the variables and call stack. It also points out that taking a snapshot does not stop the running application, which is good to know.</p>
<p>Click in the left-hand gutter on the line just after the “osname” variable is set. Now that the snapshot point is set, we can refresh the webpage and trigger the snapshot. If we go back to the Debugger tab, you’ll see that it’s showing the variables and call stack on the right-hand side. There’s “osname”. It’s set to “Linux”, not anything more specific. I guess it doesn’t know the specific distribution of Linux that’s running, so let’s change our code to check for Linux instead.</p>
<p>And deploy the new version. Now refresh the webpage. It worked!</p>
<p>Let’s go back to the main Error Reporting page and I’ll show you a couple of other things. First, if you’re sitting on this page watching for errors in real-time, then you should click the “AUTO RELOAD” button, which will refresh the page every 5 seconds. If you don’t want to hang around here and just want to get an email when an error occurs, then click the “Turn on notifications” button.</p>
<p>Alright, that’s it for this lesson.</p>
<h3 id="Lectures-3"><a href="#Lectures-3" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Tracing"><a href="#Tracing" class="headerlink" title="Tracing"></a>Tracing</h1><p>In the last lesson, we looked at how to debug errors in your application, but what do you do if your application is working properly but performing too slowly? That’s what Cloud Profiler and Cloud Trace are used for. In this lesson, I’m only going to talk about Cloud Trace.</p>
<p>The Trace List is probably where you will spend most of your time. It shows you all of the traces over a specific period of time in this cool graph. It is set to “1 hour” right now, but we can change that to give a longer view. Each one of these dots is a trace of an individual request to the application. If you click on one of the dots, it brings up two more panes underneath. The Waterfall View shows what happened during the request. The first bar shows the total end-to-end time, which was 215 milliseconds in this case. The bars underneath show the time it took to complete calls performed when handling the request. In this case, we have one bar for an HTTP GET request.</p>
<p>Of course, this timeline would be a lot more useful if we were running a more complex application with multiple calls so you could see which ones were taking the most time. Each of those calls would have a bar on this chart. The Hello World application is about the simplest application possible, so you’ll just have to use your imagination here.</p>
<p>Analysis reports show you the latency distribution for your application and also attempt to identify performance bottlenecks, which is a great feature. You have to have at least 100 traces before you can run a report, though.</p>
<p>If you’re running your applications in App Engine, then it’ll automatically capture and submit traces, but if you want to trace code that’s running outside of App Engine, then you’ll have to either add instrumentation code to your applications using the Trace SDK or submit traces through the API.</p>
<p>Before we go, you might want to delete your application, so it doesn’t incur charges. Go to App Engine and then go to Settings. Click “Disable application”. It will ask you to type the app’s ID before you can click “DISABLE”. This doesn’t delete the application, but it does stop it from serving requests. To start the application up again, you can just click “Enable application”.</p>
<p>If you want to permanently delete the application, then you’ll have to delete the project it’s associated with, which you can do in the “IAM &amp; Admin” page. Be aware that if you delete a project, you will never be able to use that project ID again. That is, you won’t be able to create a new project with the same ID.</p>
<p>That’s it for this lesson.</p>
<h3 id="Lectures-4"><a href="#Lectures-4" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/summary/">Summary</a></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I hope you enjoyed learning about the Cloud Operations Suite. Let’s do a quick review of what you learned.</p>
<p>You can use the Cloud Operations suite (formerly known as Stackdriver) for monitoring, logging, error reporting, debugging, profiling, and tracing your applications.</p>
<p>You don’t need to install the Monitoring Agent to be able to use Monitoring, but if you do install it, you can get more information about an instance, such as about the third-party software running on it.</p>
<p>To monitor a server, first you need to create an Uptime Check. Then you need to set up an Alert Policy. To see data graphically, create a Dashboard. </p>
<p>Cloud Audit Logs keep track of who did what, where, and when. There are three types of audit logs. Admin Activity logs track any actions that modify a resource. This includes everything from shutting down VMs to modifying permissions. System Event logs track Google’s actions on Compute Engine resources. Some examples are maintenance of the underlying host and reclaiming a preemptible instance. Data Access logs track data requests, including read requests on configurations and metadata.</p>
<p>Cloud Logging can even collect logs from other cloud platforms like AWS. You just need to install the Logging Agent on any system that you want to get logs from.</p>
<p>If you need to do serious log analysis, then you can export the logs to BigQuery. In this case, BigQuery acts as a sink for log data. To do a search in BigQuery, you need to use SQL statements.</p>
<p>The Error Reporting service alerts you to errors in your applications. You can use the Debugger to figure out what’s causing an error.</p>
<p>Cloud Profiler and Cloud Trace are used to analyze performance problems in applications. A trace is an individual request to an application. Cloud Trace shows you how much time was taken by each of the calls generated by an application request.</p>
<p>If you’re running your applications in App Engine, then it’ll automatically capture and submit traces. If you want to trace code that’s running outside of App Engine, then you’ll have to add instrumentation code to your applications. The recommended way is to use OpenTelemetry and the associated Cloud Trace client library.</p>
<p>To learn more about the Cloud Operations suite, you can read Google’s documentation. Also watch for new <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a> courses on Cloud Academy, because we’re always publishing new courses. If you have any questions or comments, please let us know in the Cloud Academy Community or send an email to <a href="mailto:&#x73;&#x75;&#x70;&#112;&#111;&#114;&#116;&#64;&#99;&#108;&#x6f;&#x75;&#x64;&#97;&#x63;&#97;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#109;">&#x73;&#x75;&#x70;&#112;&#111;&#114;&#116;&#64;&#99;&#108;&#x6f;&#x75;&#x64;&#97;&#x63;&#97;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#109;</a>. Before you go, please give this course a rating. Thanks!</p>
<h3 id="Lectures-5"><a href="#Lectures-5" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/monitoring-gcs/">Monitoring</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/logging-gcs/">Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/error-reporting-and-debugging-gcs/">Error Reporting and Debugging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-google-cloud-operations-suite/tracing-gcs/">Tracing</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:43:43" itemprop="dateCreated datePublished" datetime="2022-11-14T12:43:43-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:42:54" itemprop="dateModified" datetime="2022-11-21T02:42:54-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Lab-Build-and-Deploy-a-Container-Application-with-Google-Cloud-Run-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:38:04" itemprop="dateCreated datePublished" datetime="2022-11-14T12:38:04-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:42:10" itemprop="dateModified" datetime="2022-11-21T02:42:10-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Knowledge-Check-Designing-a-Google-Cloud-Infrastructure-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p><object data="Knowledge-Check-Designing-a-Google-Cloud-Infrastructure.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:36:54" itemprop="dateCreated datePublished" datetime="2022-11-14T12:36:54-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:38:34" itemprop="dateModified" datetime="2022-11-21T02:38:34-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Designing-a-Google-Cloud-Infrastructure-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to designing a Google Cloud infrastructure. I’m Guy Hummel, and I’ll be showing you how to build an enterprise IT solution in Google platform.</p>
<p>To get the most from this course, unless you already have a lot of experience using Google Cloud, you should take the Google Cloud Platform Fundamentals and Systems Operations courses to get a solid understanding of the different components of Google Cloud. In this course, I’ll be showing you how to use these building blocks to construct an enterprise class application architecture.</p>
<p>We’re going to use a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/case-study-1/">case study</a> as an example of how to apply enterprise principles to a design. I’ll start by explaining how you would take an organization’s requirements, and translate them into the appropriate <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/compute-1/">compute</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/storage-2/">storage</a>, and network components in Google Cloud. I’ll also show you how to make it a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> design.</p>
<p>Then I’ll cover how to secure the environment, including how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/authentication-2/">authenticate</a> and give permissions to people as well as to applications using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/service-accounts-1/">service accounts</a>, how to encrypt your data, and how to comply with a rigorous security standard like PCI DSS.</p>
<p>Finally, we’ll wrap up with how to design a solution that can recover from <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disasters</a>.</p>
<p>All right, if you’re ready to learn how to create an enterprise class architecture for your Google Cloud infrastructure, then let’s get started.</p>
<h1 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h1><p>Suppose you’ve been hired to help a company called Great Inside, which offers interior design software as a service.</p>
<p>Great Inside makes its money by selling subscriptions to its web-based interior design application. It also has a free version that’s supported by advertising. Their customers are primarily in North America, but they hope to expand in Europe and Asia at some point in the future.</p>
<p>The company has grown slowly for five years, but recently closed a venture capital round, brought in experienced executives, and is now growing more quickly. The company’s existing infrastructure is not capable of scaling up quickly enough, so they would like to move to the cloud.</p>
<p>Great Inside started off with a Microsoft-centric infrastructure and then migrated to a LAMP stack. The only Microsoft infrastructure left is the payment processing system and an Active Directory server. They would like to retire their Microsoft servers in the future, other than Active Directory. But that isn’t a priority right now, and the company would like to move both types of servers to the cloud. They’ve also started a pilot project using a NoSQL database.</p>
<p>Since they accept credit cards, they need to be PCI DSS compliant. Since their volume is increasing, they need to ensure that their payment processing environment meets a higher level of compliance. Note that Great Inside passes the validation and processing of credit card information to a certified payment processor.</p>
<p>They would like to improve their disaster recovery solution. At the moment, they’re backing their data up to a cloud service, but it would take them a long time to recover from a disaster.</p>
<p>Their existing technical environment is all in a single data center.</p>
<p>They have three types of databases. MySQL for the interior design application, Microsoft SQL Server for payment processing, and a NoSQL database in the development environment.</p>
<p>They have two types of web and application servers. Apache and Tomcat are running on six servers, each with 2 dual-core CPUs, 24GB of RAM, and two mirrored 200GB disks. These servers are for their interior design application. IIS is running on four servers- two customer-facing and two internal, each with a dual-core CPU, 16GB of RAM and two mirrored 250GB disks. These servers are for payment processing.</p>
<p>They have a variety of infrastructure servers, including Active Directory and a file server for internal documents, etc.</p>
<p>Here are their business requirements. Scale easily to handle rapid growth, move as much of the development, test, and production infrastructure as possible to the cloud, and increase performance, reliability, and security while reducing management overhead.</p>
<p>And their technical requirements are: connect the data center’s network with the cloud environment’s network, encrypt all data, design <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> into all tiers, and create a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disaster recovery</a> solution that will reduce recovery time to a few hours, rather than a day.</p>
<p>I should mention up front that some aspects of this case study may not be completely realistic. It’s simplified so we can go through it in a reasonable amount of time, but it has just enough complexity to allow us to cover the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">key topics</a>.</p>
<h1 id="Compute"><a href="#Compute" class="headerlink" title="Compute"></a>Compute</h1><p>Although most Google Cloud designs include virtual machine instances, that doesn’t mean VMs are your only option for compute resources. Before you start designing a solution using only Compute Engine instances, you should consider App Engine and Kubernetes Engine.</p>
<p>App Engine is designed for people who don’t want to manage an application’s underlying infrastructure. App Engine provisions and scales all of the resources your application needs behind the scenes, without any human intervention required. That sounds great, doesn’t it? So why wouldn’t you use App Engine?</p>
<p>The main reason is that it is much easier to develop a new application on App Engine than it is to migrate an existing one to it. So if you’re developing an application from scratch, then App Engine may be a good choice. If you have an existing application, then you’ll need to check if App Engine supports the programming languages your app is written in and if your app has any operating system dependencies (such as only being able to run on Windows, which isn’t supported by App Engine).</p>
<p>You’ll also need to look at your application’s architecture to see if it would be able to run on App Engine without having to re-architect it. App Engine is designed for microservices-based apps, so if your existing application has a monolithic architecture, then it might require some work to migrate it.</p>
<p>For all of these reasons, it’s usually advisable to use App Engine only for new applications rather than existing ones.</p>
<p>The next option is Kubernetes Engine. It provides many of the benefits of App Engine, in that you don’t have to worry about the underlying operating system running your application. It also handles scaling, although you have to configure that yourself first. It does require more management than App Engine, but it doesn’t require as much management as Compute Engine.</p>
<p>The ideal case for using Kubernetes Engine is, of course, if your application already runs in containers, especially Docker containers, since that’s what Kubernetes Engine supports. On the other hand, if your application will only run on certain operating systems, especially Windows, then it won’t run in Kubernetes Engine.</p>
<p>If you have an existing app that does not currently run in containers, then you might want to see if it’s possible to containerize it so you can take advantage of Kubernetes Engine.</p>
<p>If your existing application runs on virtual machines, then the easiest way to migrate it to Google Cloud is to use Compute Engine instances. If it doesn’t run on virtual machines, then you’ll have to virtualize it before you can run it on Google Cloud.</p>
<p>Although Compute Engine requires more management than App Engine or Kubernetes Engine, it does give you ultimate flexibility. For example, you could run an application that requires Windows, a specific network driver, and high-performance GPUs.</p>
<p>Since our case study involves an existing application that doesn’t currently run in containers, we’re going to choose Compute Engine for our design.</p>
<p>The case study company, GreatInside, currently has 6 machines running Apache and Tomcat, and 4 machines running IIS. Let’s have a look at Google’s predefined machine types . We need to decide how many vCPUs and how much memory to use. Memory is pretty straightforward. Our existing machines have 24GB for the Tomcat servers and 16GB for the IIS servers. VCPUs are more complicated, though.</p>
<p>The existing Tomcat servers have two dual-core CPUs and the IIS servers have one dual-core CPU. How does that translate into vCPUs? Some people say that cores and vCPUs are equivalent, but that’s not quite true. A vCPU on a Compute Engine instance is implemented as a single hyper-thread on an Intel Xeon processor. Since each Xeon processor has 2 hyperthreads, that means you need to multiply the number of cores by 2 to get the number of threads, and thus the number of vCPUs.</p>
<p>So our Tomcat servers have the equivalent of 8 vCPUs (4 cores times 2) and our IIS servers have the equivalent of 4 vCPUs (2 cores times 2). Of course, if we really wanted to be accurate, we’d need to take into account things like the clock speed of the CPUs, but we’re not going to go that far.</p>
<p>So, we need 8 vCPUs and 24GB of RAM for the Tomcat servers and 4 vCPUs and 16GB of RAM for the IIS servers. Do any of the predefined machine types match these requirements? Well, the n1-standard-4 is almost identical to the IIS server requirements. It has 4 vCPUs and 15GB of RAM. Having one less gig of RAM is probably fine, but you can monitor it in production to make sure it’s sufficient.</p>
<p>The Tomcat servers are another story, though. The closest match is the n1-standard-8, which has 8 vCPUs and 30GB of memory. That’s 6GB more than we need, so we should consider a custom machine type. We can select the exact size we need. With this custom configuration, it says it will cost $190.54 per month. Let’s see how that compares to the n1-standard-8. That costs $194.58 per month, which is more expensive, but only 2% more.</p>
<p>I should mention that there are a couple of ways to reduce those costs: sustained-use discounts and committed-use discounts. If you know that you’re going to be running an instance continuously for a long period of time, then you can pay much less by purchasing either a one-year or three-year contract, which is called a committed-use contract. This will typically reduce the cost by up to 57%. However, that’s a pretty big commitment, so Google provides a way to reduce costs without signing a long-term contract. You start getting an automatic discount after an instance runs for more than 25% of a month, and the discount increases the longer the instance runs during that month. For most machine types, you’ll receive a sustained-use discount of 30% if you run the instance for the entire month.</p>
<p>Okay, let’s get back to our case study. Since IIS and SQL Server run on Windows, we’ll need to figure out how to license them. Let’s start with IIS. For Windows Server itself, you can either use Google’s pay-as-you-go Windows licensing or you can bring your own license. </p>
<p>There are two ways to use Google’s pay-as-you-go Windows licensing. The first way is to create a new instance with one of the pre-configured Windows Server boot disks . The second way is to import a Windows VM. There are two options for importing a VM. The first option is to import a virtual disk and turn it into an image that you can use to create a Compute Engine instance. That’s quite simple to do, but it’s not meant for migrating mission-critical applications or migrating a large number of VMs in an automated fashion.</p>
<p>A more sophisticated option is to use Cloud Migrate for Compute Engine. This service makes replicas of existing VMs you have running on-premises or on another cloud platform. It will take care of the many steps that are needed to migrate important applications. </p>
<p>If you want to bring your own Windows licenses, then you can run your Windows VMs on sole-tenant nodes, which are dedicated physical servers that are not shared with other customers.</p>
<p>If you need to run any Microsoft applications, then you’ll need licenses for those too, of course, but Microsoft is more flexible with its application licensing than with Windows licensing. If your organization has active Software Assurance contracts for its Microsoft applications, then you can move those licenses to either Compute Engine instances or sole-tenant nodes.</p>
<p>Now let’s move on to SQL Server. You can use any of the options I just mentioned, but fortunately, there are also easier options for SQL Server. One option is to create instances with pre-configured SQL Server boot disks . These include pay-as-you-go licenses for both Windows Server and SQL Server. The second option is to use Cloud SQL, which is a managed service. I’ll tell you more about it in the next video.</p>
<p>For premium Linux OSs (such as Red Hat or SUSE), licensing is much simpler. You can either create an instance with a pre-configured boot disk or you can import your Linux VM. In both cases, you can either use a Google pay-as-you-go license or bring your own license.</p>
<p>I should mention one other Compute Engine option – preemptible VMs. They’re up to 80% cheaper than regular instances, but since Google can remove them with only 30 seconds’ notice, you would usually only use them as disposable instances for things like big data batch jobs. That doesn’t fit our use case, so we’ll stick with regular instances.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><p>Each of the instances for the Tomcat and IIS servers will come with a standard persistent boot disk by default, but we might need something different. There are many options for instance storage, including Standard Persistent Disk, SSD Persistent Disk, Local SSD, RAM Disk, and Cloud Storage.</p>
<p>Standard Persistent Disks are magnetic drives. Their main advantage is low cost. SSD Persistent Disks (or solid state disks) have up to 4 times the throughput and up to 40 times the I&#x2F;O operations per second of a Standard Persistent Disk, so if you need high performance, SSDs are a must.</p>
<p>But SSD Persistent Disks aren’t even your fastest option. Local SSDs are up to 600 times as fast as Standard Persistent Disks in IOPS and up to 15 times as fast in throughput.</p>
<p>Why are Local SSDs so much faster than SSD Persistent Disks, which are obviously both using SSD technology? Well, it’s because Local SSDs are not redundant and are directly attached to an instance. That gives them major speed advantages, but with high risk because if they suffer a hardware failure, then your data will be gone. Furthermore, Local SSDs disappear when you stop or delete an instance, so you should only use them for temporary data that you can afford to lose, such as a cache.</p>
<p>There are a couple more disadvantages of Local SSDs too. First, they are only available in one size – 375GB, which is kind of an awkward number. Second, they can’t be used as boot disks.</p>
<p>If you need even faster storage, then you can use a RAM disk, which essentially makes a chunk of memory look like a filesystem. Although RAM disks are the fastest option, they’re even less durable than Local SSDs, so they’re only suitable for temporary data. It’s also an expensive option because RAM is much more expensive than SSDs.</p>
<p>One more option is Cloud Storage. This is kind of a weird way to add storage to an instance because a bucket is object storage rather than block storage. That means it can’t be used as a root disk and it may be unreliable as a mounted filesystem. So why would you ever use it? The first advantage of using Cloud Storage is that multiple instances can write to a bucket at the same time. You can’t do that with persistent disks, which can only be shared between instances in read-only mode. The danger is that one instance could overwrite changes made by another instance, so your application would have to take that into account.</p>
<p>The second advantage is that an instance can access a bucket in a different zone or region, which is great for sharing data globally, especially if it’s read-only data, which would avoid the overwriting problem.</p>
<p>However, Cloud Storage usually isn’t a good option for instance storage. It is good for general-purpose file serving, though, so it would be a potential choice for replacing GreatInside’s internal file server if they want to move it to the cloud. To do this, you’d need to use Cloud Storage FUSE, which is open source software that translates object storage names into a file and directory system. Essentially, it makes Cloud Storage buckets look like network file systems. A better choice, though, would be Cloud Filestore, which is a fully-supported file sharing service that’s designed specifically for this purpose. It’s compatible with NFS version 3.</p>
<p>So, which instance storage option should we use for our instances? Since performance is important, we should use something faster than Standard Persistent Disks. SSD Persistent Disks are many times faster than standard ones, so they’d be a good choice. Should we consider Local SSDs or RAM disks? Well, neither of those can be boot disks, so we would have to use them in addition to a persistent boot disk. The higher performance wouldn’t outweigh the extra cost and complexity of using one of these options, though, so we should just stick with SSD Persistent Disks. Furthermore, since persistent disks are redundant, we don’t need to have two mirrored disks on each instance like GreatInside does in its existing data center. We can just have a single persistent boot disk on each instance.</p>
<p>As for the size, we can specify the exact amount we need, so for the Tomcat servers, we should use one 200GB disk on each instance, and for the IIS servers, we should use one 250GB disk on each.</p>
<p>Next, we need to look at our database options. Google Cloud has 5 different database services: Cloud SQL, Cloud Datastore, Bigtable, BigQuery, and Cloud Spanner.</p>
<p>Cloud SQL is a relational database. It’s a managed service for MySQL, PostgreSQL, or Microsoft SQL Server. It’s suitable for everything from blogs to ERP and CRM to ecommerce.</p>
<p>Cloud Datastore is a NoSQL database service. Unlike a relational database, such as Cloud SQL, it is horizontally scalable. A relational database can scale vertically, meaning that you can run it on a more powerful VM to handle more transactions, but there are obviously limits to the size of a VM. You can also scale a relational database horizontally for reads by using read replicas, but most relational databases can’t scale horizontally for writes. That is a major problem that is solved by NoSQL databases.</p>
<p>Because of this and because it’s an eventually consistent database, Cloud Datastore is faster than Cloud SQL. It’s best suited to relatively simple data and queries, especially key-value pairs. Typical examples include user profiles, product catalogs, and game state. For complex queries, Cloud SQL is a better choice.</p>
<p>Cloud Bigtable is also a NoSQL database. It’s designed to scale into the petabyte range with high throughput and low latency. It does not support ACID transactions, so it shouldn’t be used for transaction processing. It’s best suited for storing huge amounts of single-keyed data. If you have less than one terabyte of data, then Bigtable is not the best solution. It can handle big data in real-time or in batch processing. Typical examples are Internet of Things applications and product recommendations.</p>
<p>BigQuery also handles huge amounts of data, but it’s more of a data warehouse. It’s something you use after data is collected, rather than being a transactional system. It’s best suited to aggregating data from many sources and letting you search it using SQL queries. In other words, it’s good for OLAP (that is, Online Analytical Processing) and business intelligence reporting.</p>
<p>Google’s newest database service is Cloud Spanner, which seems to combine the best of all worlds. It’s a relational database that also scales horizontally. That is, it combines the best features of traditional databases like Cloud SQL and the best features of NoSQL databases like Cloud Datastore. So why wouldn’t you use it for all of your database needs? Well, mostly because it’s more expensive than the other options. Also, if your application is written specifically for a particular database, such as MySQL, then Cloud SQL would be a better choice, unless you can rewrite it to work with Cloud Spanner. </p>
<p>So use Cloud Spanner when you need a relational database that is massively scalable. Typical uses are financial services and global supply chain applications.</p>
<p>Now, which database services should GreatInside use? It currently has two production databases – MySQL for the interior design application and SQL Server for payment processing. There are two ways you could migrate the MySQL database to Google Cloud. You could use Cloud SQL or run MySQL on a regular instance. Considering that GreatInside wants to reduce system management tasks, Cloud SQL would be the best choice since it’s a fully managed MySQL service, with automatic replication and backups.</p>
<p>For SQL Server, you have the same two options. You could use Cloud SQL, or you could run it on a regular instance. Again, Cloud SQL is the best choice.</p>
<p>GreatInside does have one more database – their experimental NoSQL datastore. Since the development team is still evaluating this technology, you should talk to them about trying Cloud Datastore. They should also try App Engine because Cloud Datastore works best when used with App Engine.</p>
<p>And that’s it for storage and databases.</p>
<h1 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h1><p>Before we talk about networks, we need to talk about how we can make our applications highly available.</p>
<p>If you have an application that’s running on only one VM instance, then, of course, it’s a single point of failure, and if it goes down, your application goes down. So, at a minimum, you should always have at least two VMs for every component of your solution. But where should those instances be located?</p>
<p>When you create a VM instance, it gets created in a particular zone, such as us-central1-a. A zone is an isolated location. You can think of a zone as a data center or an isolated portion of a data center.</p>
<p>If you put both instances in the same zone, then both of them could potentially go down if there’s a problem in that zone. So, you should put the instances in different zones. For performance reasons, you may need to put them in zones that are in the same region, such as us-central1. Notice that the zone name is just the region name with a dash and a letter at the end. All of the zones in a region have high-bandwidth, low-latency network connections between them, so if instances that are spread across a region need to mirror data with each other, then they can do this quickly.</p>
<p>Although “region” sounds like a geographic area, it’s just a data center campus in one location. For example, all of the zones in the us-central1 region are in Council Bluffs, Iowa. So, for maximum availability, you may also want to distribute your instances across different regions.</p>
<p>For a higher level of availability, you can use autoscaling instance groups. This was covered extensively in the “Google Cloud Platform: Systems Operations” course, so I’ll just go over the highlights.</p>
<p>An instance group consists of identical instances that perform processing for your application. If one of the instances fails, then a health check will notice this and replace the instance with a new one. If the load on the instance group gets too high, then the autoscaler will add more instances to maintain good application performance.</p>
<p>To ensure availability even if an entire zone fails, you should distribute the instances across multiple zones. Luckily, this is very easy to do. You just have to select “Multizone” when you’re creating the instance group.</p>
<p>If you want to make sure you’ll still have enough instances to handle the load if an entire zone goes down, then you should overprovision by 50%. For example, if your instances are spread across 3 zones and you need 6 instances to handle your normal traffic load, then you should provision 9 instances. That way if one of the zones goes down (which would take out 3 of the instances), you’ll still have 6 instances left in the two remaining zones.</p>
<p>You can either overprovision by 50% at all times or you could save money by just setting the upper limit on your autoscaler to at least 50% more than the normal number of instances. If you decide to depend on the autoscaler during a zone failure, then the instances in your remaining two zones will be very heavily loaded until the autoscaler provisions additional instances, so only choose this option if you can tolerate this temporary performance degradation.</p>
<p>Since GreatInside has 6 web tier instances for its main application, this is how it should be set up. For the 2 customer-facing IIS instances in the payment processing system, you’d set an upper limit of 3 instances, which is 50% more than the 2 instances that it normally needs.</p>
<p>To make the instance group work as a high availability solution, you’ll need a couple of other components. First, the instance group has to be behind a load balancer that will distribute incoming requests to different instances. Second, the instances cannot have any stateful data. Otherwise, the same instance would have to handle all requests from a given user. Although you can enable the “session affinity” option in this situation, it will ruin your high availability since a failed instance will impact all of the users on it.</p>
<p>Since most applications do have stateful data, you have to put it on other components, such as a database or Cloud Storage. Unfortunately, that just moves the availability issue to a different layer, but fortunately, Google Cloud has good ways to handle storage availability.</p>
<p>If Cloud Storage is sufficient for your stateful data needs, then you’re covered because Cloud Storage is automatically replicated either across zones in a region (for the Regional type) or across regions (for the Multi-Region type).</p>
<p>If you need a database for your stateful data, then there are different availability solutions depending on the data service.</p>
<p>With Cloud SQL, you can simply check the “High availability” box when you create a Cloud SQL instance. This will create a failover replica in another zone. In the event of a failure, Cloud SQL will automatically fail over to the replica. This option is available for MySQL, PostgreSQL, and SQL Server.</p>
<p>Since Cloud Datastore is a NoSQL database, it scales horizontally, which makes high availability easier than with Cloud SQL. Cloud Datastore automatically replicates data across zones in a region. When you create a Datastore instance, you specify which region and it does the rest.</p>
<p>Bigtable is also a NoSQL database that scales horizontally, but if you want it to replicate across multiple zones, then you’ll have to configure it to do that. You can even configure it to support replication across regions if you need that. But in its simplest configuration, it only stores data in a single zone, which gives it higher performance. It’s still stored redundantly in that configuration but within the same zone.</p>
<p>BigQuery automatically replicates data within a region, but it’s a data warehouse, so it’s not suitable for real-time stateful data storage.</p>
<p>Cloud Spanner also automatically replicates data within a region, so it’s highly available out of the box, and unlike Cloud SQL, it doesn’t need a failover replica, which is a less available solution.</p>
<p>In summary, if a NoSQL database is sufficient for your application, then Cloud Datastore is your best choice for storing stateful data. If you need to use a relational database, then either use Cloud SQL and enable high availability or use Cloud Spanner for even higher availability if you’re willing to pay a higher price.</p>
<p>Since GreatInside is going to use Cloud SQL for both MySQL and SQL Server, then we just need to enable the high availability option when we create those databases.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Networks"><a href="#Networks" class="headerlink" title="Networks"></a>Networks</h1><p>Now we know all of the components we want to use and we just need to connect them together with networks. Google provides what are called Virtual Private Clouds, or VPCs, but I’m just going to call them networks.</p>
<p>There are 5 layers in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">Google Cloud</a> you can use to isolate and manage resources: organizations, folders, projects, networks, and subnetworks.</p>
<p>You aren’t required to have organizations or folders, but they can be useful, especially for large companies.</p>
<p>Projects are required, though. You use them to provide a level of separation between resources. Not only are resources in different projects unable to communicate with each other, but they’re even in different billing accounts. Projects also have separate security controls, so for example, you could give Bob in QA the highest level of access in the Test Environment project, but a lower level of access in the Production Environment project.</p>
<p>Each project has one default network that comes with preset configurations and firewall rules to make it easier to get started, but you can customize it, or you can create up to 4 additional networks (for a total of 5). If 5 networks per project isn’t enough for you, then you can request a quota increase to support up to 15 networks in each project.</p>
<p>A network belongs to only one project, a subnet belongs to only one network, and an instance belongs to only one subnet.</p>
<p>Instances in the same subnet or even different subnets within the same network can communicate with each other. Subnetworks are used to group and manage resources.</p>
<p>A network spans all regions, but each subnet can only be in one region. A subnet allows you to define an IP address range and a default gateway for the instances you put in it. The IP address ranges of the different subnets must be non-public (such as 10.0.0.0) and must not overlap, but other than that, there are no restrictions on them. For example, they can be different sizes. They must be IPv4 addresses, though, because Compute Engine doesn’t support IPv6 yet.</p>
<p>A network can have either automatic or custom subnets. With automatic, the subnets are created for you, one in each region. With custom, you create them yourself. If you discover that you need to customize a network with automatic subnets, then you can convert it to custom mode, but once you do, you cannot convert it back to automatic mode.</p>
<p>On the default network, instances within the same subnet can communicate with each other over any TCP or UDP port, as well as with ICMP. Instances in the same network can communicate with each other, regardless of which subnets they’re in, because Google Cloud creates routes between all of the subnets in a network. However, the default network’s firewall rules only allow ssh, rdp, and icmp traffic between subnets.</p>
<p>If you don’t want instances in different subnets to be able to reach each other, then you can change the firewall rules to deny traffic between them.</p>
<p>Note that only the default network comes with predefined firewall rules. When you create a new network, it doesn’t have any firewall rules. However, the instances in that network will still be able to communicate with the Internet, assuming they have external IP addresses, because all outgoing traffic from instances is allowed. Only incoming traffic is blocked. And when an instance sends a request over the Internet, the incoming response is allowed, so two-way traffic is enabled at that point.</p>
<p>Each network includes a local DNS server so VM instances can refer to each other by name. The fully qualified domain name for an instance is [HOSTNAME].c.[PROJECT_ID].internal. This name is tied to the internal IP address of the instance. An Instance does not know its external IP address and name. That translation is handled elsewhere in the network.</p>
<p>To reach Internet resources, each VM needs an external IP address. An ephemeral external IP address is created for each VM by default, but an ephemeral address gets replaced with another one if you stop and restart the instance, so if you want an instance to always have the same IP address, then you need to assign a static IP address to it.</p>
<p>Since IPv4 addresses are a scarce resource, Google doesn’t want customers to waste them. So you’re not charged for having static IP addresses as long as you’re using them. But if a static IP address is not associated with a VM instance or if it’s associated with an instance that’s not running, then you’ll be charged for it.</p>
<p>Normally, if a VM needs to send requests to other Google services, such as Cloud Storage, then by default, it has to do so using a public IP address rather than an internal one. This is problematic if you don’t want any of your internal network communications to go over the Internet. However, if you enable the Private Google Access option in a subnet, then VMs in that subnet can connect to Google services using internal IP addresses, so their requests will go over Google’s network rather than the Internet.</p>
<p>If you want instances in different projects to communicate with each other, then you have three options: the Internet, VPC Network Peering, or a Shared VPC. Connecting over the Internet is slower, less secure, and more expensive than the other two options, so it’s not usually the best choice.</p>
<p>The simplest alternative is VPC Network Peering. This allows two VPCs to connect over a private RFC 1918 space, that is, using non-routable internal IP addresses, such as 10.x.y.z. In other words, they don’t need public IP addresses, and they communicate over Google’s network. Not only can you do this for VPCs in different projects, but you can even use it to connect VPCs in different organizations. To make this work, both sides have to set up a peering association. If only one side sets up a peering association with the other VPC, then the networks won’t be able to communicate with each other. Also bear in mind that there can’t be any overlapping IP ranges in the two networks. You’ll notice in this example that the two ranges are not overlapping.</p>
<p>A more complicated option is to use a Shared VPC. The idea is that instances in different projects can share the same network. This is kind of a weird idea. If you’ve put resources in different projects, you probably want them to be managed separately, so why would you get them to use the same network? In most cases, it’s to enforce security standards. For example, if you want to use the same firewall rules across all of your projects, then this is a good way to do that.</p>
<p>To set up a Shared VPC, you need to designate one of the projects as the host project and the others as service projects. The host project is the one that contains the Shared VPC. Instances in the service projects can use subnets in the Shared VPC. This is made possible by giving Service Project Admins the authority to create and manage instances in the Shared VPC but nothing more. Meanwhile, the Shared VPC Admins have full control over the network. Note that all of the projects in this arrangement have to be part of the same organization.</p>
<p>OK, we’ve gone over a lot of networking topics. Now how should we apply these concepts to GreatInside?</p>
<p>At a minimum, we should create separate projects for the Development, Test, and Production environments. Inside each project, we should stick with the default network. There’s no need to add any additional ones. We should also stick with automatic subnetworks. The only subnetwork we need right now is one in the US, such as us-central1, since we don’t currently have any plans to expand into other parts of the world. When GreatInside decides to add instances overseas, then they can be added to the other regional subnets.</p>
<p>The default firewall rules should also be fine, since they only allow internal traffic plus ssh, icmp, http, and https. We should remove the rule that allows rdp traffic in the Production network, though, since we don’t have any Windows instances in it.</p>
<p>We don’t want the Production, Development, and Test environments to be part of the same network, so we don’t need a Shared VPC. In fact, we don’t want them to communicate with each other at all, so we don’t need to use VPC Network Peering either.</p>
<p>By the way, you probably noticed that everything I’ve shown so far is only for the interior design application. I’m going to get into the details of how to set up the payment processing environment in the Legislation and Compliance lesson.</p>
<p>One last item is that we have to decide which components need external IP addresses. That’s easy in this case because the load balancer is the only one that needs an external IP address (and ideally it should be a static address). Users will connect to the web instances through the load balancer, so the web instances only need internal IP addresses, and for security reasons, that’s all they should have.</p>
<p>That does raise the question of how a system administrator could connect to them for troubleshooting, though. One way is to give your administrators access to the internal network by interconnecting it with the company’s on-premises network. That’s something that GreatInside has already requested, so let’s see how to do that. There are three ways: Cloud VPN, Cloud Interconnect, and Direct Peering.</p>
<p>Cloud VPN lets you set up a virtual private network connection between your own network and Google Cloud. To do this, you need to have a peer VPN gateway in your own network and it needs to use IPsec to connect to the Cloud VPN Gateway and encrypt traffic. You can have multiple tunnels to a single VPN gateway.</p>
<p>By itself, Cloud VPN requires you to make changes to static routes on your tunnels manually. But if you use Google Cloud Router, then the routes will be updated dynamically using BGP (that is, Border Gateway Protocol). Network topology changes are propagated automatically.</p>
<p>The second way to connect is called Cloud Interconnect. Instead of connecting over the Internet, you can use an enterprise-grade connection to Google’s network edge. There are two ways to do this: Dedicated Interconnect and Partner Interconnect. If your internal network extends into a colocation facility where Google has a point of presence, then you can connect your network to Google’s. This is called Dedicated Interconnect. It’s a great solution that provides higher bandwidth and lower latency than a connection over the public internet. It’s a bit expensive, though, because the minimum bandwidth is 10 Gbps.</p>
<p>If you don’t have a presence in a supported colocation facility or you want to pay for a connection that’s smaller than 10 Gbps, you can use Partner Interconnect. With this option, you connect to a service provider that has a presence in a supported colocation facility. You can purchase a monthly contract for connections as small as 50 Mbps and as large as 10 Gbps. </p>
<p>The third way is to use Peering. This is similar to Cloud Interconnect because you connect your network to Google’s network at a point of presence either directly (which is called Direct Peering) or through a service provider (which is called Carrier Peering). One big difference with peering is that it doesn’t cost anything. So why would anyone pay for Cloud Interconnect when they could peer with Google for free? Well, because with Cloud Interconnect you get a direct connection between your on-premises network and one of your VPCs in Google Cloud. You have full control over the routing between your networks. If you want to change a route, you can change it on your on-premises router, and it will be picked up by BGP. Although the peering option uses BGP, too, it’s done at the most basic level. It doesn’t create any custom routes in your VPC network.</p>
<p>Since we don’t have requirements for low latency and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> between the company network and Google Cloud, we should go with Cloud VPN to connect. We should also use Cloud Router so network routes will be updated dynamically.</p>
<p>And that’s it for networks.</p>
<h1 id="Authentication"><a href="#Authentication" class="headerlink" title="Authentication"></a>Authentication</h1><p>The first step in giving secure access to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">Google Cloud infrastructure</a> is to decide how to authenticate your users. By default, Google Cloud Platform requires users to have a Google account to access it. But if you have more than a handful of users, then you’ll want to find a centralized way to manage your user accounts. The solution is to use the G Suite Global Directory. You don’t have to use G Suite products like Google Docs, you can just use G Suite for user management.</p>
<p>Most organizations already have a user directory, so the best policy is usually to manage users in your existing directory, and then synchronize the account information in G Suite. There are three ways to do this: Google Cloud Directory Sync or GCDS, the Google Apps Admin SDK, or a third party connector.</p>
<p>Google Cloud Directory Sync is the easiest solution if you have either Active Directory or an LDAP server. It synchronizes users, groups, and other data from your existing directory to your Google Cloud Domain Directory. GCDS runs inside your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/networks-1/">network</a> on a machine that you control.</p>
<p>It’s a one-way synchronization, so GCDS doesn’t modify your existing directory. Of course the synchronization can’t be a one-time event. It has to happen on a regular basis to keep your Google Directory up-to-date.</p>
<p>To make authentication even easier for your users, you can implement single sign-on or SSO. Google Cloud Platform supports SAML 2.0-based SSO. If your system doesn’t support SAML 2.0, then you can use a third party plugin.</p>
<p>Once you’ve implemented SSO, then when a user would normally have to login, Google will redirect your authentication system. If the user is already authenticated in your system, then they don’t have to login to Google Cloud separately. If they aren’t already logged in, then they’re prompted to login.</p>
<p>In order for this to work, your users must have a matching account in Google’s Directory. So you still need to use GCDS or one of the other synchronization options.</p>
<p>In our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/case-study-1/">case study</a>, since we have an active directory server, we’ll use GCDS for synchronization and also implement single sign-on.</p>
<p>And that’s it for authentication.</p>
<h1 id="Roles"><a href="#Roles" class="headerlink" title="Roles"></a>Roles</h1><p>To give a user permission to access particular Google Cloud resources, you assign a role to them. Basic roles act at the project level. There are 3 basic roles available: Owner, Editor, and Viewer. There are also fine-grained roles for individual resources. These are called predefined roles. (They were previously known as curated roles.) For example, the Cloud SQL Viewer role gives read-only access to Cloud SQL resources.</p>
<p>You can assign roles at different levels of the hierarchy, that is, at the organization, folder, project, and resource levels. If you assign roles to the same user at different levels, then their effective permissions are the union of the permissions at the different levels.</p>
<p>For example, if you granted Marie the Viewer role at the organization level and the Editor role at the project level, then she would have Editor permissions for all of the resources in that project. The Viewer role at the organization level would not override the Editor role at the project level. Similarly, if you assigned them in the opposite way, with the Editor role at the organization level and the Viewer role at the project level, Marie would still have the Editor role for all of the resources in that project because the project-level permissions would not override the organization-level permissions.</p>
<p>There are a few principles you should apply when setting roles and permissions. </p>
<p>First, use the principle of least privilege when granting roles. That is, assign roles with the least permissions required for people to do what they need.</p>
<p>Second, whenever possible, assign roles to groups instead of to individuals. Then, when you need to grant a role to a user, you can just add them to the group. Not only is this easier to manage, but it also ensures consistent privileges among members of a particular group. You can also use a descriptive group name that makes it clear why group members need those permissions.</p>
<p>Third, keep tight control of who can add members to groups and change policies. If you don’t, then people could give themselves or others more privileges than they should have.</p>
<p>Fourth, to make sure that inappropriate policy changes aren’t made, audit all policy changes by checking the Cloud Audit Logs, which record project-level permission changes.</p>
<p>Now let’s apply these principles to GreatInside. First, you would grant the project owner role to a few key system administrators. Owners are the only ones who can change policies (unless you grant users the Organization Administrator role). You should always have more than one owner. Otherwise, if that person is unavailable or leaves the organization, it would be difficult to for someone else to take their place as owner. So avoid that situation by giving the owner role to several people, but choose wisely because owners can do just about anything. </p>
<p>Similarly, you would have a small number of G Suite administrators who could add users to groups.</p>
<p>Obviously, there would be a large number of users who would need permissions, so I’m not going to talk about every type of user, but I’ll give a couple of examples. One example would be a network administration group that you would grant the Compute Network Admin role to.</p>
<p>Another example would be a QA team. You could grant their group the editor role on the Test Environment project and the viewer role on the Production Environment project. Alternatively, if the QA people don’t need full access to the Test Environment, then you could grant them several predefined roles, such as Compute Instance Admin, Cloud SQL Admin, and Compute Storage Admin.</p>
<p>Regarding audit logs, someone would need to take on the responsibility of checking for policy changes. The Admin Activity audit logs are viewable by all project members, so you wouldn’t need to grant access to the person who does the checking.</p>
<p>And that’s it for roles.</p>
<h1 id="Service-Accounts"><a href="#Service-Accounts" class="headerlink" title="Service Accounts"></a>Service Accounts</h1><p>Now that you have user authentication and permissions figured out, it’s time to plan how your applications will access the Cloud Platform services it needs to use. To avoid embedding credentials in an application, you need to use service accounts. For example, if an application uses Cloud Datastore as a database, then it needs to have authorization to use the Datastore API.</p>
<p>You would accomplish this by enabling Datastore API access on any VM instances that will be involved in the part of the application that uses the database. By default, all VM instances run as the Compute Engine default service account. If you want something different, then you can create your own.</p>
<p>A service account has an email address and a public&#x2F;private key pair that it uses to prove its identity. Your instances use that identity when communicating with other Cloud Platform services. However, by default, an instance running as the Compute Engine default service account has limited scope in how it can interact with other services. For example, by default an instance can only read from Cloud Storage and can’t write to it.</p>
<p>To give an instance more permissions, you need to set the scope when you’re creating the VM. So, in the case of interacting with Datastore, you have to enable access to the Datastore API. You also have to enable the Datastore API at the project level, but you only have to do that once.</p>
<p>Then your application code has to obtain credentials from the service account whenever it uses the Datastore API. Google Cloud Platform uses OAuth 2.0 for API authentication and authorization. There are two ways to do it: Application Default Credentials and access tokens.</p>
<p>The easiest way is to use Google Cloud Client Libraries. They use Application Default Credentials (or ADC) to authenticate with Google APIs and send requests to those APIs. One great feature of ADC is that you can test your application locally and then deploy it to Google Cloud without changing the application code. </p>
<p>Here’s how it works. To run your code outside Google Cloud Platform, such as in your on-premise data center or on another cloud platform, create a service account and download its credentials file to the servers where the code will be running. Then set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the credentials file.</p>
<p>So while you’re developing locally, the application can authenticate using the credentials file and when you run it on a production instance, it will authenticate using the instance’s service account. This works because ADC allows applications to get credentials from multiple sources.</p>
<p>The second way is to use OAuth2 access tokens to directly connect to the API without going through a client library. One reason you’d have to use this method is if your application needs to request access to user data.</p>
<p>The way it works is the application requests an access token from the metadata server and then uses the token to make an API request. Tokens are short-lived, so your application needs to request new ones regularly.</p>
<p>If you need to write shell scripts that access other Cloud Platform services, then you can use gcloud and gsutil commands to make API calls. These two tools are included by default in most Compute Engine images and they automatically use the instance’s service account to authenticate with APIs.</p>
<p>So what service accounts would you need to create for GreatInside? The load balancer and the web instances communicate over HTTPS, so you don’t a service account for that. Since the Tomcat instances communicate with the MySQL database in Cloud SQL, you would need a service account for that. Similarly, the IIS instances communicate with SQL Server in Cloud SQL, so you’d need a service account for that, too. There may be a need for other service accounts when we add more features to our architecture, such as disaster recovery, but we’ll cover that later.</p>
<p>And that’s it for service accounts.</p>
<h1 id="Data-Protection-and-Encryption"><a href="#Data-Protection-and-Encryption" class="headerlink" title="Data Protection and Encryption"></a>Data Protection and Encryption</h1><p>Protecting data is critical in any organization. Google Cloud Platform is very strong in this area because of its default encryption policies. Before we get into encryption, though, let’s look at Access Control Lists (or ACLs).</p>
<p>ACLs specify who has access to Cloud Storage buckets and objects in buckets. I’m not going to cover this topic in depth, but there are a few things to keep in mind when you’re deciding what ACLs to apply to your Cloud Storage.</p>
<p>First, there are actually five different mechanisms for controlling access to Cloud Storage: IAM permissions, ACLs, Signed URLs, Signed Policy Documents, and Firebase Security Rules. With so many different ways to control access, you have to be careful not to create conflicting permissions. Start with the first two: IAM permissions and ACLs.</p>
<p>IAM permissions work at the project level. For example, you can specify that a user has full control of all the objects in all of the buckets in your project, but cannot create, modify, or delete the buckets themselves. So they’re a nice way to grant broad access to buckets and objects, but if you want to set fine-grained access, such as which buckets or objects a particular group can read, then you need to use ACLs.</p>
<p>The confusing thing about using these two mechanisms is that you have to look at both of them to get a complete picture of access permissions. For example, you could list the ACLs for a bucket and see that only Bob has been granted write access, but it wouldn’t show that Jill has also been granted write access to all buckets by IAM. For this reason, whenever possible, you should try to use either IAM or ACLs, but not both.</p>
<p>Another potential source of confusion is that bucket and object ACLs are independent of each other. The ACLs on a bucket do not affect the ACLs on objects inside that bucket. For example, you might think that Jane doesn’t have access to the objects in a bucket because she hasn’t been granted access to the bucket itself, but she could have been granted access to any of the objects in the bucket.</p>
<p>So you should keep a couple of principles in mind. First, apply the principle of least privilege. Grant users and groups only as much access as they need. Second, keep your access control as simple as possible. Try to use as few control mechanisms as you can.</p>
<p>If GreatInside decides to replace its internal file server using Cloud Storage, then the best way to secure the files would be to use ACLs. You would create groups to match the teams in the company and create ACLs that give those groups access to the appropriate resources. For example, you could create a bucket for each group. Then for each bucket, you would make the associated group a writer of the bucket. Finally, you would set the object default permissions so that any new objects uploaded to the bucket would get the same permissions and everyone in the group would have full access. If the company’s needs aren’t that simple, then you would set more complex ACLs.</p>
<p>Now let’s move on to encryption. To ensure that your data is encrypted at all times, it needs to be encrypted when it’s in storage (also known as “at rest”) and when it is being sent over a network (also known as “in flight”). Google Cloud Platform takes care of both of these situations.</p>
<p>Encryption in flight is handled very simply. All of the Cloud Platform services are accessible only by API (even when you’re using other methods, such as the Cloud Console or the gcloud command, they’re making API calls under the hood). And all API communication is encrypted using SSL&#x2F;TLS channels. Furthermore, every request has to include a time-limited authentication token, so the token can’t be used by an attacker after it expires. Of course, for any communications between your Google Cloud infrastructure and outside parties, such as website visitors, you have to use SSL&#x2F;TLS yourself to encrypt the traffic.</p>
<p>Encryption at rest is just as simple if you’re willing to leave it to Google because Cloud Platform encrypts all customer data at rest by default.</p>
<p>So without you having to do anything, all of your data will be encrypted both at rest and in flight. Then why isn’t this the end of this lesson? Well, because your organization might want to take on some of the encryption responsibilities itself.</p>
<p>There are actually two layers of encryption for data at rest. First, the data is broken into subfile chunks, and each chunk is encrypted with an individual data encryption key (or DEK). These keys are stored near the data to ensure low latency and high availability. The DEKs are then encrypted with a key encryption key (or KEK). The keys are AES-256 symmetric encryption keys.</p>
<p>Google always manages the data encryption keys, but your organization can manage the key encryption keys if that’s your preference. There are two options for doing this: Customer-managed encryption keys or Customer-supplied encryption keys.</p>
<p>With the customer-managed option, you use the Cloud KMS service to create, rotate (or automatically rotate), and destroy your encryption keys. The keys are hosted on Google Cloud. You can have as many keys as you want, even millions of them if you actually need that many. You can set user-level permissions on individual keys using IAM and monitor their use with Cloud Audit Logging.</p>
<p>Cloud KMS is a nice service, but why wouldn’t you just let Google manage your key encryption keys and not have to deal with it yourself? The biggest reason is compliance with standards or regulatory requirements, such as HIPAA (for health information) or PCI (for credit card information).</p>
<p>If your organization requires that you generate your own keys and&#x2F;or that they’re managed on-premises, then you have to use Customer-supplied encryption keys. Be aware that this option is only available for Cloud Storage and Compute Engine.</p>
<p>With CSEK, Google doesn’t store your key. You have to provide your key for each operation, and your key is purged from Google Cloud after the operation is complete. Here’s how to do it from the command line with each of the two supported services. To encrypt the disk on a Compute Engine instance, you add the csek-key-file flag and point it to a file that contains the key. To encrypt data you’re uploading to Cloud Storage, you have to do it a bit differently. Rather than adding an encryption flag to the gsutil command, you need to add the encryption key to your .boto file, which is the configuration file for the gsutil command. Then all of your gsutil commands will use that key.</p>
<p>It only stores an SHA256 hash of the key as a way to uniquely identify the key that was used to encrypt the data. When you make a request to read or write the data in the future, your key can be validated against the hash. The hash cannot be used to decrypt your data.</p>
<p>There’s a big risk in using this method, though. If you lose your keys, you won’t ever be able to read your data again, and you’ll end up deleting it so you won’t be paying storage charges for unreadable data.</p>
<p>So far all of the encryption methods we’ve covered, including default encryption, Cloud KMS, and CSEK have been examples of server-side encryption. This is where your data is encrypted after Google Cloud receives your data. The only major difference between the 3 methods is where the key comes from. But there is another way. It’s client-side encryption. This means that you encrypt the data before you send it to Google Cloud. Google won’t even know that it’s already encrypted and it will encrypt it again. When you read your data back, Google Cloud will decrypt it on the server side first and then you’ll decrypt your own layer of encryption on the client side. The same warning applies - if you lose your keys, your data will effectively be gone.</p>
<p>Since our case study includes credit card information, we’ll need to be PCI DSS compliant, so we should use Cloud KMS to manage our keys. I’ll talk more about PCI compliance in the next lesson.</p>
<h1 id="Legislation-and-Compliance"><a href="#Legislation-and-Compliance" class="headerlink" title="Legislation and Compliance"></a>Legislation and Compliance</h1><p>Google Cloud Platform has passed annual audits for some of the most important security standards, including SOC 1, 2, and 3, ISO 27001, and PCI DSS. It also complies with HIPAA, CSA STAR, the EU-US Privacy Shield Framework, and MPAA controls, none of which require annual audits.</p>
<p>So if your organization is required to comply with any of these standards, then you know that Google has done its part. But this is a shared responsibility because <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">your security processes and applications</a> running on top of Google’s infrastructure also need to comply.</p>
<p>You’ll notice that Network is listed for both Google and the customer. That’s because Google takes care of some parts of networking and the customer takes care of the rest. One of the most interesting areas of shared responsibility for network security is protecting against distributed denial of service (or DDoS) attacks.</p>
<p>Google provides many features to help deal with DDoS attacks, but it’s up to the customer to use them properly. Here are some of the techniques.</p>
<p>Reduce the attack surface by</p>
<ul>
<li>Isolating and securing your deployment with firewall rules</li>
<li>Google also provides anti-spoofing protection by default</li>
</ul>
<p>Isolate your internal traffic from the external world by</p>
<ul>
<li>Deploying instances without public IPs unless necessary</li>
</ul>
<p>Use Load Balancing</p>
<ul>
<li>Because a load balancer acts as a proxy that hides your internal instances</li>
</ul>
<p>Use Cloud Armor</p>
<ul>
<li>This service is specifically designed to provide DDoS defense</li>
<li>And it works with Load Balancing</li>
<li>It protects against layer 3 and layer 4 DDoS attacks</li>
</ul>
<p>Google Cloud also enforces API rate limits and resource quotas to prevent a spike in one customer’s activity from affecting other Cloud Platform customers.</p>
<p>Now it’s time to get back to the PCI DSS standard and how to comply with it. If your organization accepts credit card payments, then you need to comply with this standard or you could be fined. More importantly, if you have security flaws that allow hackers to steal credit card information from your systems, then it would be very damaging to both your customers and your reputation.</p>
<p>In the case study, GreatInside provides an interface for collecting credit card information, but it passes the validation and processing of the information to a Certified Payment Processor. This makes the company an SAQ A-EP merchant in PCI lingo. I’ll go over Google’s recommendations for how this type of merchant could comply with PCI DSS.</p>
<p>First, you have to check that the other parties involved (that is, Google Cloud and the payment processor) are certified for your volume of transactions (since there are different PCI DSS merchant levels based on the number of transactions). Google Cloud Platform has the highest level of PCI DSS certification, so that’s not a concern, but you’ll have to check your payment processor’s certification level because your volume might exceed their certification level.</p>
<p>Here’s a suggested architecture to handle the company’s credit card processing. Here’s how it works. A customer enters their credit card information in a form on your website. Then your payment-processing application sends the information to the external payment processor. Now the payment processor tells your application whether the card was accepted or declined. After that your payment processing application sends some or all of the response data to your core application, so it knows how to proceed with this customer.</p>
<p>You also need to log and monitor all of these interactions. Every instance involved in payment processing sends its logs to Stackdriver Logging and its alerts to Stackdriver Monitoring.</p>
<p>Now let’s move on to how you would set this up. To reduce the number of systems that need to be PCI-compliant, you have to fully isolate your payment-processing environment from the rest of your production environment. The best way to do this is to use a separate Google Cloud account, rather than just a separate project within your main account.</p>
<p>Then use IAM to grant access only to people who absolutely need to work on the payment-processing environment, such as people who will be deploying new versions of the application or managing the systems. These people must also pass a background check first.</p>
<p>To create the instances, you should first create your own Linux image that’s based on one of the preconfigured boot disk images and that contains the bare minimum of additional software needed to run your application. Then use this custom image when creating all of your VM instances.</p>
<p>To secure the network, create firewall rules that only allow three types of inbound traffic:</p>
<ul>
<li>HTTPS traffic from the load balancer to the payment form servers, so that customers can reach your payments page</li>
<li>Credit card authorization responses from the external payment processor to your internal payment authorization servers, and</li>
<li>VPN traffic from your internal office network to the VPN Gateway, so your authorized people can manage and audit the application and systems.</li>
</ul>
<p>Then create firewall rules for outbound traffic. There’s only one type of outbound traffic you need to allow – HTTPS traffic from the payment form servers to the external payment processor, so they can send credit card authorization requests.</p>
<p>Now all of the traffic in and out of the network is locked down, but you’ll also have to open up internal traffic, such as:</p>
<ul>
<li>From all of the instances to Google’s NTP servers for time synchronization, and</li>
<li>SSH traffic from the VPN Gateway to all of the instances, so authorized people can access the systems for maintenance</li>
</ul>
<p>OK, let’s move on to deploying your application. To be compliant, you have to make sure you’re deploying the correct application every time, that it’s deployed securely, and that no other software packages are installed during the deployment. If you don’t already have an automated deployment tool, then you might want to use Cloud Deployment Manager, which could automate the creation of everything in your payment-processing environment, even the firewall rules. It could also help you create an audit trail of deployments.</p>
<p>Since you’ve used the same custom Linux image for all of your instances, you’ll need to install additional software on each instance. For example, some instances may need a web server, while others don’t, and each instance should only have the software it needs, which will reduce your security risks. To make this process consistent and reliable, it should be automated as well. The easiest way to automate software installation and configuration is to use a configuration management tool such as Chef, Puppet, or Ansible. Cloud Academy has courses on all three of these tools, so check one out if you’re not familiar with how to use any of them. </p>
<p>There are a few packages that you’ll want to install on all instances. First there’s iptables. You can set it up to log all network activity to and from each instance. This data is required for PCI DSS compliance audits.</p>
<p>Second, each instance needs the Stackdriver Monitoring and Logging agents so it can send logs and alerts.</p>
<p>Third, each instance should run an Intrusion Detection System (or IDS) to alert you to suspicious activity.</p>
<p>Finally, your configuration management tool needs to securely retrieve and launch the latest version of your application. </p>
<p>Even with an automated deployment, you’d still need to verify the integrity of the software being deployed. You could do this by running an automated checksum comparison against each package as it’s installed. You could also run an automated code analysis tool to check for security flaws.</p>
<p>Now let’s move on to logging. To be compliant, every step in the payment-processing environment has to be monitored and recorded. All instance activity and all user activity must be logged. Stackdriver Logging is a great service for collecting logs. You can record network traffic to and from your instances by enabling VPC Flow Logs on each subnet in your VPC.</p>
<p>By the way, you might think that we need to assign a service account to the instances so they can write logs to Stackdriver, but the default service account for VM instances already grants write access to Stackdriver, so you don’t need to configure that yourself.</p>
<p>I mentioned that user activity needs to be logged, but you also need to log the activity of people who have administrative access to the environment. The easiest way is to log all shell commands.</p>
<p>The amount of log information generated by all of this is likely to be very large, so you might want to export your Stackdriver logs to BigQuery if you need to do some complex analysis.</p>
<p>In addition to logging, you also need to set up real-time monitoring alerts, such as when your IDS detects any intrusion attempts.</p>
<p>After your environment is implemented, but before any production traffic flows through it, you have to validate the environment, either by contracting a Qualified Security Assessor if you’re a Level 1 merchant or by filling out the Self Assessment Questionnaire if you’re not a Level 1 Merchant.</p>
<p>Wow, that was a lot of work, wasn’t it? Well, if you’re going to be handling credit card information, you’ll be happy when your rigorous security design prevents damaging incidents.</p>
<h1 id="Disaster-Recovery"><a href="#Disaster-Recovery" class="headerlink" title="Disaster Recovery"></a>Disaster Recovery</h1><p>In an earlier lesson, we covered how to design a highly available architecture that will keep running even if an instance fails, by using load balancers, instance groups, and redundant databases. However, there are more catastrophic events that might occur. I’m not talking about an entire city getting destroyed or anything like that (although it would be good to have an architecture that could handle that). But much smaller incidents can be disastrous too. For example, one of your databases could become corrupt. This is actually worse than the database server going down because it may take a while before you realize there’s a problem, and in the meantime, the corruption problem could get worse.</p>
<p>To recover from this sort of disaster, you need backups along with transactional log files from the corrupted database. That way you can roll back to a known-good state. Each type of database has its own method for doing this.</p>
<p>If you’re using Cloud SQL to run a MySQL database (which we are for the interior design application), then you should enable automated backups and point-in-time recovery. Then if your database becomes corrupt, you can restore it from a backup or use point-in-time recovery to bring the database back to a specific point in time. </p>
<p>If you’re using Cloud SQL for SQL Server, then you should enable automated backups. At this time, Cloud SQL does not support point-in-time recovery for SQL Server, so you can only restore a database to the point when a specific backup was taken. For both types of databases, Cloud SQL retains up to 7 automated backups for each instance.</p>
<p>If you’re hosting a database on Compute Engine instances directly, then you’ll have to configure backups and transaction logging yourself. For example, suppose that instead of using Cloud SQL for SQL Server, we ran SQL Server on a Compute Engine instance. Then we’d need to set up our own disaster recovery solution for it. Luckily, Google has a very detailed white paper on this topic. I’ll give you the highlights.</p>
<p>First, set up an automated task that copies the SQL Server database backups to Google Cloud Storage. This is where we would finally need a service account because instances can’t write to Cloud Storage by default. The SQL Server instances need to have a service account with the Storage Object Creator role. Another way to do it would be to set a Cloud Storage access scope for the instance, but service accounts are more flexible.</p>
<p>Once the database is being backed up, then if disaster strikes, you would spin up a new SQL Server instance. Either use one of Google’s preconfigured SQL Server images or your own custom disk image. It doesn’t mention this in the whitepaper, but it’s the sensible thing to do and I’ll talk about it more in a minute. Next, you can use an open-source script to restore the database and re-execute the events in the log files up to the point in time desired.</p>
<p>When you’re designing a disaster recovery solution, you need to consider RPO and RTO. RPO stands for Recovery Point Objective. This is the maximum length of time when data can be lost. It affects your backup and recovery strategy because, for example, if it’s acceptable to lose an entire day’s worth of work, then you can just recover using the previous night’s backups. If you have a short RPO, which is usually the case, then you need to make sure you are constantly backing up your data, and when recovering from database corruption, you have to carefully consider which point in time to recover to.</p>
<p>RTO stands for Recovery Time Objective. This is the maximum length of time that your application can be offline and still meet the service levels your customers expect (usually in a service level agreement).</p>
<p>In the SQL Server example, I suggested using either one of Google’s preconfigured SQL Server images or your own custom disk image that has SQL Server installed and configured. The advantage of having a custom disk image is that it helps you meet your recovery time objective because it reduces the amount of time it takes to get a new SQL Server instance running. If you have to configure SQL Server manually, that could significantly impact how long it takes to recover from a disaster.</p>
<p>As with everything, though, there are tradeoffs. If your SQL Server implementation is customized, then you’ll have to weigh the benefits of fast recovery time against the maintenance effort required to keep your custom image up-to-date . If you have a very short RTO, then you may have no choice but to maintain a custom disk image. You might be able to ease the maintenance required, though, by using a startup script to perform some of the customization. Since the startup script resides on either the metadata server or Cloud Storage, you can change it without having to create a new disk image.</p>
<p>In some cases, you may want to run an application from your own data center or from another cloud platform and use Google Cloud as a disaster recovery solution. There are many ways you could do this, but I’ll go over a couple of common designs.</p>
<p>The first way is to continuously replicate your database to an instance on Google Cloud. Then you would set up a monitoring service that would watch for failures. In the event of a disaster, the monitoring service would trigger a spin-up of an instance group and load balancer for the web tier of the application. The only part you would need to do manually is to change the DNS record to point to the load balancer’s IP address. You could use Cloud DNS or another DNS service for this.</p>
<p>This is already a low-cost solution because the only Google Cloud resource that needs to run all the time is the database instance. But you can reduce the costs even further by running the database on the smallest machine type capable of running the database service. Then if there’s a disaster, you would delete the instance, but with the option to keep the persistent disk, and spin up a bigger instance with the saved disk attached. Of course, this solution would require more manual intervention and would lengthen your downtime, so you wouldn’t want to do this if you have a short RTO.</p>
<p>If you want to reduce your downtime as much as possible, or even keep running in the event of hardware failures, you could serve your application from both your on-premises environment and your Google Cloud environment at all times. That way if you have an on-premise failure, the Google Cloud environment would already be running and serving customers. It would just need to scale up to handle the extra load, which would be automatic if you use an autoscaling instance group.</p>
<p>To make this hybrid solution work, you would need to use a DNS service that supports weighted routing, so it could split incoming traffic between the two environments. In the event of a failure, you would need to disable DNS routing to the failed environment.</p>
<p>And that’s it for disaster recovery.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">design a Google Cloud infrastructure</a>. Now you know how to map <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/compute-1/">compute</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/storage-2/">storage</a>, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/networks-1/">network</a> requirements into Google Cloud, and secure your infrastructure with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/authentication-2/">authentication</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/roles-1/">roles</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/service-accounts-1/">service accounts</a>, ACLs, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/data-protection-and-encryption-1/">encryption</a>. You also know some of the ways to design <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disaster recovery</a>, and PCIDSS compliance into your solution.</p>
<p>To learn more about Google Cloud platform, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know on the Cloud Academy community forums. Thanks and keep on learning.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/" class="post-title-link" itemprop="url">gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 12:36:04" itemprop="dateCreated datePublished" datetime="2022-11-14T12:36:04-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-21 02:43:06" itemprop="dateModified" datetime="2022-11-21T02:43:06-04:00">2022-11-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Digital-Leader/" itemprop="url" rel="index"><span itemprop="name">GCP-Digital-Leader</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/gcp-digital-leader-exam-Lab-Starting-a-Linux-Virtual-Machine-on-Google-Compute-Engine-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/137/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/137/">137</a><span class="page-number current">138</span><a class="page-number" href="/page/139/">139</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/139/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
