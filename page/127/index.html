<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/127/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/127/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:14" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:14-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:02:46" itemprop="dateModified" datetime="2022-11-19T23:02:46-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:12" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:12-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:55:58" itemprop="dateModified" datetime="2022-11-19T22:55:58-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to this course which will look at how you can use AWS Trusted Advisor to help you follow and implement some best practices and recommendations across your AWS environment with your organization.</p>
<p>Before we start I’d like to introduce myself, my name is Stuart Scott, and I am the AWS content and security lead here at Cloud Academy. Feel free to connect with me to ask any questions using the details shown on the screen, alternatively you can always get in touch with us here at Cloud Academy by sending an e-mail to <a href="mailto:&#x73;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#x40;&#x63;&#108;&#x6f;&#x75;&#x64;&#x61;&#x63;&#x61;&#100;&#101;&#x6d;&#121;&#x2e;&#x63;&#x6f;&#109;">&#x73;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#x40;&#x63;&#108;&#x6f;&#x75;&#x64;&#x61;&#x63;&#x61;&#100;&#101;&#x6d;&#121;&#x2e;&#x63;&#x6f;&#109;</a> where one of our Cloud experts will reply to your question.</p>
<p>This course has been designed for those who might be in one of the following roles:</p>
<ul>
<li>Security Professionals &amp; Security Auditors</li>
<li>Systems Engineers and Administrators</li>
<li>Compliance Managers</li>
<li>Anyone looking to learn to become AWS certified</li>
</ul>
<p>There are a number of learning objectives to this course, you will understand:</p>
<ul>
<li>The purpose and benefits of Trusted Advisor</li>
<li>How to navigate the Trusted Advisor Console</li>
<li>How to use Trusted Advisor to optimize your AWS resources and account</li>
<li>How to take actionable steps with Trusted Advisor to improve your AWS infrastructure</li>
</ul>
<p>As a prerequisite to this course, I recommend that you have a basic understanding of some of the core services, such as IAM. This is because AWS Trusted Advisor provides recommendations and best practices across a range of other AWS services.</p>
<h1 id="What-is-AWS-Trusted-Advisor"><a href="#What-is-AWS-Trusted-Advisor" class="headerlink" title="What is AWS Trusted Advisor?"></a>What is AWS Trusted Advisor?</h1><p>Hello and welcome to this lecture where I am going to be looking at AWS Trusted Advisor, explaining what it is and the different components that make up this service. </p>
<p>Trusted Advisor plays an integral part in helping you to optimize your infrastructure across a number of key areas, allowing you to make decisions upon recommendations made by the service which follow and best practices that have been honed over the years by AWS.</p>
<p>The service itself can be found within the AWS Management Console under the Management &amp; Governance category, alongside services such as Amazon CloudWatch, Control Tower and Systems Manager. </p>
<p>The main function of Trusted Advisor is to recommend improvements across your AWS account to help optimize and streamline your environment based on these AWS best practices. These recommendations cover 5 distinct categories:</p>
<ol>
<li><strong>Cost optimization</strong> - Helps to identify ways in which you could optimize your resources to help you reduce costs by implementing features such as reserved capacity and removing unused capacity</li>
<li><strong>Performance</strong> - This reviews your resources to highlight any potential performance issues across your infrastructure, determining if you could take benefits from performance-enhancing capabilities such as provisioned throughput</li>
<li><strong>Security</strong> - This analyses your environment for any potential security weaknesses or vulnerabilities that could potentially lead to a breach.</li>
<li><strong>Fault Tolerance</strong> - This helps to suggest best practices to maintain service operations by increasing resiliency, should a fault or incident occur across your resources.</li>
<li><strong>Service Limit</strong> - This identifies and warns you when your resources reach 80% capacity of their service limit quota.</li>
</ol>
<p>Within each of these 5 categories, Trusted Advisor has a list of control points and checks to see how your account, resources and architecture is implemented to determine if you’re aligned with best practice. So it essentially acts as an automatic auditor across your account, which can save you money, increase the efficiency of your resources, maintain a tighter and more secure environment, help to ensure your resources remain operational should a failure occur and that you remain in line with your service limitations, allowing you to request an increase where possible.</p>
<p>Between the 5 different categories and at the time of writing this course, there are over 115 different checks. Please note, that the number of these checks are constantly changing, so for the most up to date figures, please review the following link: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>
<p>Although there are a lot of these checks that Trusted Advisor can perform, not all of them are freely available to anyone with an AWS account. The list of checks that you have access to is very dependent on the support agreement with hold with AWS.</p>
<p>The full power and potential of AWS Trusted Advisor is only available if you have a Business or Enterprise Support Plan with AWS. Without either of these plans then you will only have access to 6 core checks in the security category and all the Service Limits </p>
<p>The 6 checks within security are as follows:</p>
<ul>
<li>S3 Bucket permissions</li>
<li>Security Groups - Specific Ports Unrestricted</li>
<li>EBS Public Snapshots</li>
<li>RDS Public Snapshots</li>
<li>IAM Use</li>
<li>MFA on root account</li>
</ul>
<p>At the time of writing this course, here are the available service limit checks.</p>
<p>Now if you compare this to the full list of checks here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>
<p>….that are included with Business and Enterprise support plans, you will see that the full checklist can provide a huge wealth of valuable information to help you optimise your infrastructure. </p>
<p>In addition to these extra checks that these support plans offer, you will also get the additional benefit of being able to administer certain functions of Trusted Advisor, such as:</p>
<ul>
<li>being able to track the most recent changes to your AWS account by bringing them to the top of your AWS Trusted Advisor dashboard.</li>
<li>using the AWS Support API to retrieve and refresh trusted advisor results. </li>
<li>Also you’ll have the added advantage of having Amazon CloudWatch integration to detect and react to changes made to your Trusted Advisor checks</li>
</ul>
<p>There are also a number of features that everyone has access to, including those outside of the Enterprise and Business support plans, these being:</p>
<ul>
<li><p><strong>Trusted Advisor Notifications</strong> - This is an opt-in or opt-out feature which is completely free to everyone and can be configured within the preferences pane of the Trusted Advisor console. It tracks your resource check changes and cost saving estimates over the course of a week and it will then email up to 3 recipients, for billing, operations and security notifications with a report.</p>
</li>
<li><p><strong>Exclude Items</strong> - This allows you to select specific resources to be excluded from appearing in the console within a specific check. You may want to do this if you are not interested in the reporting for that particular resource and so you decide to exclude it. You can decide to include it again at any point if you do change your mind. This feature can make viewing and managing your checks easier by eliminating some resources within the console.</p>
</li>
<li><p><strong>Action Links</strong> - Many of the items identified within the Checks against resources have hyperlinks associated, these are known as Action Links which allow quick access to the resource in question allowing you to remediate the issue identified. For example, if you reached 80% of the number of VPC’s within a Region, the <strong>‘VPC’</strong> Service Limit Check would highlight this as an issue. The Action Link against the resource would lead you to an AWS Support Center page to create a case to increase the quantity of VPCs you’re allowed within a single region.</p>
</li>
<li><p><strong>Access Management</strong> - AWS Trusted Advisor is tightly integrated within Identity &amp; Access Management. You can grant different levels of access to Trusted Advisor, including Full Access, Read Only, or even restrict access down to specific Categories, Checks and Actions. For example, the following IAM policy allows access to AWS Trusted Advisor, but denies the user from performing a refresh and updating notification preferences.</p>
</li>
</ul>
<p>For a full list of IAM permissions using the trustedadvisor namespace please see the following AWS reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html">https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html</a></p>
<ul>
<li><strong>Refresh</strong> - The data within Trusted Advisor is automatically refreshed if the data is more than 24 hours old when you view it within the console. However, after any refresh, you can perform a manual refresh 5 minutes after the previous refresh. You can either choose to perform a refresh against individual checks or against all checks.</li>
</ul>
<p>Before I finish this lecture I just want to give a high level overview of how Trusted Advisor works in a few simple steps:</p>
<ul>
<li>Once you connect to AWS Trusted Advisor, the service will scan your infrastructure </li>
<li>It will then compare the state of your infrastructure against best practices defined within the 5 categories of Cost Optimization, Security, Performance, Fault Tolerance and service limits</li>
<li>The output of this scan will generate a number of recommendations of how your infrastructure could be optimised with a priority factor</li>
<li>This then allows you to optimize your resources based on the recommendations</li>
</ul>
<p>AWS Trusted Advisor uses a service-linked IAM role to access you resources, named <strong>AWSTrustedAdvisorServiceRolePolicy.</strong> This is a predefined role created by AWS and allows the services to call other services on your behalf. The policy summary of this role is as shown here and helps to define which AWS services that Trusted Advisor communicates with.</p>
<p>Please be aware that this list will change over time, so for an updated list please refer to the role within IAM to determine which services <strong>AWSTrustedAdvisorServiceRolePolicy</strong> has access to.</p>
<h1 id="Reviewing-Checks-amp-Taking-Action"><a href="#Reviewing-Checks-amp-Taking-Action" class="headerlink" title="Reviewing Checks &amp; Taking Action"></a>Reviewing Checks &amp; Taking Action</h1><p>- Hello, and welcome to this lecture, while we shall be looking at how to review the status of your checks and take action against any findings identified by Trusted Advisor. Firstly, let me explain the dashboard, when you first go into Trusted Advisor, you’ll be presented with the five different categories. Beneath each of these categories, there are three icons, these being a tick, a triangle with an exclamation mark, and a circle with an exclamation mark. If any of these icons are Grey, it means there are no checks within that category that I’ve met an alert criteria to activate the icon. In this screenshot, the reason that so many of the icons are Grey, will be that my personal AWS account, is not covered by a business or enterprise support plan, and so those checks are locked as explained in the previous lecture. If a check does meet the criteria within the category, then one of the following will be presented. The tick box may become green, meaning no action is necessary for the check that has been reviewed, the triangle may become yellow meaning investigation required, and the circle may become red, identifying an item requires immediate action and attention. Next to each of these icons when active will be a number, and that number represents the amount of checks within that category, with a specific status. So as we can see from this image, I’ve one check within the service limits category, that should have been investigated as a priority. Five checks within the security category when no action is required, but I do have one check within this category that requires investigation. So that’s a very quick glance of the dashboard, I can see that I have a potential security threat that I need to look into, in addition to being very close to a service limitation, that I need to check immediately, as it could potentially cause a production issue. As mentioned previously, if you don’t have an enterprise or business support plan with AWS for your account, like in my example, then you will not be able to take full advantage of what Trusted Advisor has to offer. However, for this lecture I will review the six core checks that are freely available to anyone with an AWS account. Plus, I shall take a look at some of the service limit checks that are available. For every check that Trusted Advisor provides, it will provide four pieces of information. A description of the check and why it is used, the alert criteria, and this shows the conditions under which a check has given a green, no action necessary status, a yellow, investigation required status, or red, immediate action required status. Recommended action. This gives a high-level suggestion on the steps and actions that you could take to remediate any findings, based upon a yellow or red alert criteria. And additional resources, this highlights additional reading material for you to learn more about the topics being discussed. And this usually refers to AWS documentation or white papers. Let me now take a look at the free checks under the security category in more detail. Security groups, specific ports unrestricted. So this particular check, assesses your security groups that you have configured, and checks to see if you have any rules that allow an unrestricted source or destination, such as 0.0.0.0&#x2F;0. Having an unrestricted rules such as this, is not considered a best practice, as it is considered a security risk. And so you should aim to implement a tighter and more restrictive IP address range. However, some ports and protocols might be required to have an unrestricted setting, such as HTTP protective for web traffic on a web server. If you do have any secrets group roles that are exposed and fall within a yellow or red alert criteria, then it could lead to a security breach, allowing the intrusion of malicious activity, within your network and against your resources. When organizations are implementing security at the instance level using security groups, unrestricted access is often given to test or to help resolve incidents, to help identify where a problem might exist, and as a result, the correct and original source and destinations, are sometimes left exposed without intention. Trusted Advisor can help you identify these security groups, to make the necessary changes. By selecting this check, I can drill down into the different security groups, that have triggered the yellow or red status. As you can see from the screenshot, I’ve 13 security groups, that should be investigated. You may also notice that, the security groups listed have hyperlinks associated, and this will allow me to quickly select the security group which will take me directly to the configuration page, to change or modify the security group to make any alterations. If upon investigation I consider this cage group is configured correctly for the use intended, then I can choose to exclude the security group from any further reviews relating to this check. I just simply need to mark the security groups, and then select exclude and refresh. And this will remove the selected security group from any further reviews, carried out by this particular check. To then view the excluded items, I can just change the view from the included items to excluded items. I can then move these security groups back to the included group at any time. IAM use. The IAM use check simply ascertains if you’re using the identity and access management service. It recommends that you should have at least one user created to log in with, instead of operating and managing your AWS account using your route administrator account, which has the highest level of security privileges available. MFA on root account. Your root account has administrative level access to your AWS account. As a result, it is a very powerful account to use. And that such, logging in as the root account, should have additional levels of authentication, in addition to a password to verify the account. Added multi-factor authentication, MFA, to your root account, helps you protect your AWS account, so this check simply looks to see if you have activated MFA on your root account. MFA uses a random six digit number that is only available for a very short period of time before the number changes again, which is generated by an MFI device. There is no additional charge for this level of authentication, however, you will need your own MFA device, which can be a physical token or a virtual device. AWS provides a summary of all supported devices found here. Personally, I use Google Authenticator on my phone, because it is simple and easy to set up and configure. Before a user can authenticate with MFA, it must be configured and associated to the user from within IAM. Amazon EBS Public Snapshots. This check identifies if any of your elastic block store snapshots have been marked as public. When EBS snapshot is public, it is then accessible to all other AWS accounts and users within those accounts. With access to these snapshots, users can then access the data held within the snapshot. There may be circumstances where you need to allow other users or AWS accounts, access to specific snapshots. If this is the case, then he should mark the snapshot as private, and explicitly allow access on a per AWS account&#x2F;user level, rather than exposing all of the data to all accounts by marking it as public. For information on the elastic block store service, please see our existing course here. Amazon RDS Public Snapshots. This check performs exactly the same function as the Amazon EBS Public Snapshots but for your RDS snapshots instead of EBS. For more information on Amazon RDS, please see your existing course here. Service limit category checks. The checks within this category are used to assess when a service limit reaches 80% or more. Unfortunately, this doesn’t perform checks on all AWS services. As a recap from our previous lecture, it does support the following services and limits. It’s important to bear in mind that this list is changing all the time. AWS are constantly evolving and updating their services, and so over time, this list will change. For the most accurate and up-to-date list, please visit the link shown. If I look at a service from this list, such as Amazon Virtual Private Cloud, We can see that it will monitor the service limits of three different features relating to VPCs. These being elastic IP addresses, internet gateways and VPCs. So for each of these services or service features, you are allowed five per region. As a result, the service limit check will highlight if any of these thresholds get to four, which is 80%. The advantage of having this check, gives you enough time to either request an increased limit with AWS if possible or allowed, or you choose to simply reduce the number of AIPs, internet gateways or VPCs that you have. This may also force you to undertake some much needed housekeeping of your environment. I now want to perform a demonstration while I should provide an overview of the Trusted Advisor dashboard, and how to drill down into the issues or identified area. Within this demonstration, I will perform the following steps, navigate to AWS Trusted Advisor, provide an overview of the dashboard, drill down into the Trusted Advisor checks, identify and rectify the issues that are displayed, and refreshed Trusted Advisor to ensure the issues have been resolved. And then I’ll download the status of the checks as an Excel file for offline review. Okay, so I’m in my AWS Management Console, and if I scroll down to the management and governance section here, and I’ll be able to see Trusted Advisor just here. So if I select Trust Advisor, and that will take me to the Trusted Advisor dashboard which we can see here. Now we have our five categories our cost optimization, performance, security, full tolerance, and service limits. Now as I explained previously, I don’t have the enterprise or business support plans, so I’ll just have access to the free checks that are available. So there is six in security, and then all of the service limit checks. Now down here, the recommended actions, this will show all of the checks that I have access to. So it will be the six on the security and the 49 service limit checks here, and it will list them here. Now I’ll list them in a priority. So at the top here it is listed the one for action recommended, and then we have our investigation recommended here, and then we have lots of checks that are all absolutely fine. So there’s two items that I really need to check. Now here we can see, we have the orange triangle, with one investigation recommended, and over here and more importantly, we have one where the action is recommended. So we have an issue with a service limits and also a security issue as well. So let’s take a look at each of these to see if we can rectify them or the actions that we should take to try and rectify them. So firstly, the VPC. So let’s break this out a little bit to take a look. So we can see here that this check, checks for usage that is more than 80% of the VPC limit. Now on our alert criteria, we can see that if it’s yellow, then 80% of it has been reached, it turns red if a hundred percent of the limit is reached. So that tells me straight away that I’m unable to add any more VPCs in a specific region because I’m at a red alert and I’ve reached the maximum limit. So if I scroll down, I’ll be able to see exactly what region this is in, and we can see at the top here. So we have a red alert for the VPC service in EU West one. The limit amount was five and my current usage is five. So I can do two things, I can request a limit increase, and we can see here under recommended action, that if I click on request a limit increase, then it will take me straight to the support center page, where I can then request a service limit increase for the VPC. Or alternatively, I can go to my VPC configuration, in the EU West one Region and delete one of my VPCs. So if I swap over to VPC, and I’m in the region of EU West one, which has we know is where we have the issue, and I can see here that I have my five VPCs. So if I wanted to reduce these, I can simply select any of the VPCs that I want, and then go to actions and delete VPC, and that will take me down to four, which would be 80%, and I would come off the Radler, but I would go into the yellow because I would bet 80%. So like I say, you can either delete VPCs to reduce the current usage, down to a four, three, two whatever you need to, or request a limit increase if you do need all of these VPCs and would likely be needing more in that region. So that’s how we’d resolve the VPC check. Next up, we have our security groups, specific ports are restricted at a investigation required level. So let’s take a look at this. So this as we know, checks for any rules that allow unrestrictive access to specific ports. And if we look at the alert criteria, we’re at a yellow, so we have a security group that provides access to any other port that is unrestricted, that doesn’t fall into either the red or green categories. So let’s take a look at some of the VPCs that it’s detected. So it can see here, that 10 of the 44 security groups that I have, are meeting the investigation required category. So let’s go ahead and take a look at one of these security groups. Let me just pick this one for example, so I can select on the security group name, and it will take me straight to the configuration of that security group. Now, if we look at the details if we look at the inbound rules, we can see there that this is allowing SSH on port 22 from anywhere inbound, and the outbound rules are very open as well. So what we could do, we could edit these inbound rules, and change this from anywhere to just allow my IP address, and then save rules. Then if we go to the outbound, and say edit outbound rules, and again set the destination to only my IP address. If we save that. Now, if we go back to our Trusted Advisor, so as the launch, was it a security group name? So let me just highlight that check, now what I want to do is just refresh this check, to see if that is still an issue or to see if it has been resolved. So I can select the security group that in question, and go up to the check, and then say, refresh this check. So let’s do that now. Now we want to see if this is still an issue or if it’s been resolved. So as we can see it’s done a current refresh at the moment. Okay, so that’s refreshed, and we can see that that security group is no longer in the list, so we resolved the issue. So basically, I just went into the security group and removed the unrestricted access that it was granting from both the source and destination. Now I could do the same for each of the rest of these security groups, I can go into them, all the settings if need be, or leave them as they were, if that is how they were intended to be. And if they were supposed to have the open access and it’s not an issue, then I can simply select the checks and then select exclude and refresh again. No no, I can’t do this straight away because you have to wait five minutes before each refresh, but in another four minutes time, I will be able to simply exclude these from this check and they would move into the excluded items list. Okay, now the last thing I want to show you, is how to download the status of all the checks to an Excel file that you can then view offline. So at the top right-hand of the screen here, if you click on this download arrow, this will then download an Excel spreadsheet, and if we open that up, we can see here there’s a tab for every check, and it will print out the details for each of those checks, to allow you to view all of these issues offline outside of the management console. So here we have the security groups specific ports some restricted, and that we looked at just a moment ago, and we can see that we have all of the security groups that still have problems that security group IDs, and also the protocol status, et cetera. So that’s just another way of viewing your results of Trusted Advisor offline out of the management console. So that’s a very quick overview of the Trusted Advisor dashboard, the different categories and the checks, and how to look into some of those issues if you do have any alerts or investigation that’s required and how to resolve them.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello and welcome to this short lecture where I want to summarise some of the key elements of what we have covered throughout this introductory course.</p>
<p>I started off by explaining what the service is and what it does, here I explained that: </p>
<ul>
<li><p>The main function of Trusted Advisor is to recommend improvements across your AWS account to help optimize your environment based on AWS best practices</p>
</li>
<li><p>Trusted Advisor focuses on 5 categories with a list of best practice checks in each:</p>
</li>
<li><ul>
<li>Cost optimization </li>
<li>Performance </li>
<li>Security </li>
<li>Fault Tolerance </li>
<li>Service Limits</li>
</ul>
</li>
<li><p>The list of available checks to your account is dependant on your AWS Support Plan</p>
</li>
<li><p>Business and Enterprise support can take full advantage of all the checks available</p>
</li>
<li><p>All other AWS accounts only have access to 6 free core checks in the Security category, plus the Service Limit checks.</p>
</li>
</ul>
<p>The Security checks are:</p>
<ul>
<li>Security Groups - Specific Ports Unrestricted</li>
<li>Amazon EBS Public Snapshots</li>
<li>Amazon RDS Public Snapshots</li>
<li>IAM Use</li>
<li>MFA on root account</li>
</ul>
<p>There are a number of useful features within Trusted Advisor, these being:</p>
<ul>
<li>Trusted Advisor Notifications - This tracks your resource check changes and cost-saving estimates over the course of a week and e-mail you a report</li>
<li>Exclude Items - This allows you to select specific resources to be excluded from appearing in the console within a specific check. </li>
<li>Action Links - Action Links lead you on to remediate any issue identified</li>
<li>Access Management - Using IAM you can grant different levels of access to Trusted Advisor</li>
<li>Refresh - You can perform a manual refresh 5 minutes after the previous refresh against either individual checks or against all checks</li>
</ul>
<p>Following this, I focused on reviewing checks and taking appropriate action. </p>
<ul>
<li><p>A summary is provided for each category displaying how many checks require no action, how many need investigation, and how many should be looked at immediately</p>
</li>
<li><p>For every check that Trusted Advisor provides you will see:</p>
</li>
<li><ul>
<li>A description </li>
<li>Alert Criteria</li>
<li>Recommended Action</li>
<li>Additional Resources</li>
</ul>
</li>
</ul>
<p>This lecture also included a demonstration where I provided an overview of the dashboard and performed the following steps:</p>
<ul>
<li>Provided an overview of the dashboard</li>
<li>Drilled down into the Trusted Advisor Checks</li>
<li>Identified and rectified the issues that were displayed </li>
<li>Refreshed Trusted Advisor to ensure the issues had been resolved</li>
<li>Downloaded the status of the checks as an Excel file for offline review</li>
</ul>
<p>I also provided an overview of each of the core checks available to all AWS accounts.</p>
<p>You should now have a greater understanding of what AWS Trusted Advisor is and does and how you can use it within your environment to optimize your infrastructure. It is a powerful tool especially if you do have a Business or Enterprise support plan to fully maximize its potential.</p>
<h1 id="2What-is-AWS-Trusted-Advisor"><a href="#2What-is-AWS-Trusted-Advisor" class="headerlink" title="2What is AWS Trusted Advisor?"></a>2<strong>What is AWS Trusted Advisor?</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">Best Practice Checklist</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html">Full list of IAM permissions</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Amazon-Inspector-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Amazon-Inspector-18/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Amazon-Inspector-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:10" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:10-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:55:02" itemprop="dateModified" datetime="2022-11-19T22:55:02-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Amazon-Inspector-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Amazon-Inspector-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course, where I shall be explaining what Amazon Inspector is and does, and how can you use this to help play a key part within your organization’s security policies and assessments.</p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I am one of the trainers here at Cloud Academy specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the detail sheet on the screen. Alternatively, you can always get in touch with us here at Cloud Academy using the community form where one of our Cloud experts will reply to your question.</p>
<p>With Amazon Inspector being a security focus service, this course is beneficial to those who are responsible for managing and supporting AWS security, such as Security Architects, Security Assessment Managers, Operations Managers and Security Compliance Managers.</p>
<p>Understanding Amazon Inspector would also be of benefit for Application Developers and those working on Application Delivery. Finally, anyone looking to learn more about AWS Security in general, then this course would also be recommended.</p>
<p>This course will look at Amazon Inspector from the ground up, which cover the following lectures. What is Amazon Inspector? Here, I shall explain at a high level what Amazon Inspector is and why you may want to use it.</p>
<p>Components of Amazon Inspector. Within this lecture, I shall explain the main components of the service and how these fit together.</p>
<p>Then, I’ll provide a demonstration on how to configure Amazon Inspector. In this demonstration, I will cover many of the components that I would of discussed in the previous lecture, showing how to get started and how to configure the service.</p>
<p>I’ll then provide another demonstration on how to work with your findings. This lecture looks at how to view the different Amazon Inspector findings following an assessment.</p>
<p>Integration with CloudWatch and CloudTrail. This lecture explains how Amazon Inspector can be monitored with CloudWatch and CloudTrail.</p>
<p>Service Limitations and Costs. This lecture explains the limitations of the service in addition to how costings are calculated.</p>
<p>And then finally, a summary, and this lecture will summarize the key points learnt from the previous lectures within the course.</p>
<p>Once you have completed this course, you will have gained the knowledge to understand what Amazon Inspector is and does. You will know why you should implement Amazon Inspector within your environment, and you’ll understand how to configure Amazon Inspector. You’ll also know how to view and manage findings that Amazon Inspector detects, and understand the limitations, restrictions and costs of the service.</p>
<p>The only prerequisite of this course is that you are familiar with EC2 instances. Knowledge of the following services would be beneficial, but not essential. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">AWS Identity and Access Management</a>, IAM, Simple Notification Service, SNS, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/">Amazon CloudWatch</a> and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail</a>.</p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could use the comment section found on the landing page of this course.</p>
<p>That brings us to the end of this lecture. Coming up next, we start off by looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/what-is-amazon-inspector/">what Amazon Inspector is and does</a>.</p>
<h1 id="What-is-Amazon-Inspector"><a href="#What-is-Amazon-Inspector" class="headerlink" title="What is Amazon Inspector?"></a>What is Amazon Inspector?</h1><p>Hello and welcome to this lecture, where I answer the question of what <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector</a> is and does, and why you may want to use it. Amazon Inspector is a managed service that is used to help you find security vulnerabilities within your EC2 instances and any applications running on them during any stage of development and deployment.</p>
<p>This is automatically achieved for a series of assessments against specified resources, based on hundreds of best practices and known security weaknesses. Covering common vulnerabilities and exposures; The CVE is a publicly known reference list of security threats that are well documented.</p>
<p>Center for Internet SecurityBenchmarks. These benchmarks are continuously refined, and are used as global standards for best practices for protecting data and IT resources.</p>
<p>Security best practices, which look for weaknesses in common security best practices, and Runtime Behavior Analysis, which looks at the behavior of your EC2 instances during an assessment.</p>
<p>On assessment completion, a detailed assessment report can be produced which will highlight all of the findings, including any threats allowing you to make the necessary changes to resolve any security and compliance issues.</p>
<p>The Amazon Inspector service is agent based, meaning it requires software agents to be installed on any EC2 instances you want to assess. This makes it an easy service to be configured and added at any point to existing resources already running within your AWS infrastructure. This helps Amazon Inspector to become a seamless integration with any of your existing security processes and procedures as another level of security.</p>
<p>Through a level of customization of the vast knowledge base of best practices and vulnerabilities that is constantly updated that Amazon Inspector can call upon, you are able to select which packages are best for your use case, fitting into your own standards that your resources must adhere to. This allows you to customize the security for your environment, and ensures that any specific security loopholes are identified and addressed immediately.</p>
<p>Amazon Inspector records it’s assessments, which makes this a great service to present findings to auditors who may require to see evidence of security compliance and adherence to specific government controls.</p>
<p>Maintaining these records and reports helps you to maintain compliance that you may need for certifications such as PCI. So now we know at high level what the service is used for, why would we use the service? In the industry today we hear more and more about how the level of attacks and sheer quantity of hacking into small and large enterprise infrastructure in the attempt to steal and manipulate data is rising. New methods of cyber security attacks are being devised and as a result, new methods of prevention have to follow suit.</p>
<p>In a traditional data center deployment, most organizations have a level of intrusion detection and prevention plus monitoring systems in place at different levels within their infrastructure. However, not everyone has the same within the cloud. Security as a topic within the cloud is still the number one reason that prevents businesses from adopting the cloud. Much of this can be identified due to the lack of understanding, the correct skillset, and compliance.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> invests a huge amount of capital into security, and as a result, more and more security services and tools are being made available to us as customers, which is what spawned the creation of Amazon Inspector.</p>
<p>By using the Amazon Inspector service, we gain confidence in the level of security built into our applications and services due to the configurable assessments that we can run. The level of confidence not only benefits your organization, but your customers too. Having your service cross-checked for security compliance, threats, and vulnerabilities, ensures a reduction of attacks that your customer may be exposed to.</p>
<p>As you can see, this service offers some amazing benefits when looking at security compliance and reduction of exposure attack points within your infrastructure. Traditionally, to implement, manage, operate, and analyze your infrastructure resources and applications for these threats and best practices, would be difficult and take a very particular security focused skillset. This skillset, along with the systems and applications to implement such a service would come at a high cost to your business. The talent and budget may not be there for many organizations for this to happen. Thankfully, Amazon Inspector offers a solution that is lower in cost than that of a traditional solution.</p>
<p>As your organization grows, Amazon Inspector scales with it through the use of it’s agents. This allows repeatable and automated assessments to take place. With easy to understand assessment reports, it removes the highly skilled resource, that may have been required traditionally, to dissect and implement the necessary fix to resolve any findings.</p>
<p>That now brings us to the end of this lecture. Coming up next, I’m going to look at the service in greater detail, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/components-of-amazon-inspector/">identifying the components</a> used.</p>
<h1 id="Components-of-Amazon-Inspector"><a href="#Components-of-Amazon-Inspector" class="headerlink" title="Components of Amazon Inspector"></a>Components of Amazon Inspector</h1><h3 id="Resoures-amp-Links-used-within-this-Lecture"><a href="#Resoures-amp-Links-used-within-this-Lecture" class="headerlink" title="Resoures &amp; Links used within this Lecture"></a><strong>Resoures &amp; Links used within this Lecture</strong></h3><p><strong>IAM Course:</strong> <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/</a></p>
<p><strong>AWS Agent Installation:</strong> </p>
<p>Linux Install:</p>
<ul>
<li>wget <a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install">https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install</a></li>
<li>curl -O <a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install">https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install</a></li>
<li>sudo bash install</li>
</ul>
<p>Windows Install:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/windows/installer/latest/AWSAgentInstall.exe">https://d1wk0tztpsntt1.cloudfront.net/windows/installer/latest/AWSAgentInstall.exe </a></li>
</ul>
<p><strong>CVE Rules:</strong> <a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/rules-engine/CVEList.txt">https://s3-us-west-2.amazonaws.com/rules-engine/CVEList.txt</a></p>
<p><strong>Amazon Inspectors CIS Certificates:</strong> <a target="_blank" rel="noopener" href="https://www.cisecurity.org/partner/amazon-web-services/">https://www.cisecurity.org/partner/amazon-web-services/</a></p>
<p><strong>CIS Benchmarks:</strong> <a target="_blank" rel="noopener" href="https://www.cisecurity.org/cis-benchmarks/">https://www.cisecurity.org/cis-benchmarks/</a></p>
<p><strong>Security Best Practices:</strong> <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_security-best-practices.html">http://docs.aws.amazon.com/inspector/latest/userguide/inspector_security-best-practices.html</a></p>
<p><strong>Runtime Behavior Analysis:</strong> <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_runtime-behavior-analysis.html">http://docs.aws.amazon.com/inspector/latest/userguide/inspector_runtime-behavior-analysis.html</a></p>
<p>-—————————————————————————————————————————————————–</p>
<h3 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a><strong>Transcript</strong></h3><p>Hello, and welcome to this lecture covering the different components that make up the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector service</a>. I should look at each component and how they link together to make the service function.</p>
<p>There are essentially nine different components and elements of the service. These are: the Amazon Inspector role, assessment targets, AWS agents, assessment templates, rule packages, assessment runs, telemetry, assessment reports, and findings.</p>
<p>Let me break down each of these to see which part they play within the service, starting with the Amazon Inspector role. When you first start using Amazon Inspector, you are required to create or select a role to allow Amazon Inspector to have read-only access to all of your EC2 instances. Without this role, the service would not have the relevant permissions to be able to gather the telemetry data of the instance during any assessment runs.</p>
<p>If you allow Amazon Inspector to create a role, then it will have a policy attached as detailed here. As you can see, this simply allows the role to have read-only access to all EC2 instances within your AWS account. For more information on IAM and IAM roles, please see our existing course here. Assessment Targets.</p>
<p>An Assessment Target is a grouping of AWS EC2 instances that you want to run an assessment against. This grouping of EC2 instances are managed and defined by the tags that are associated to your EC2 instance. Tagging is simply a way of adding metadata to your instances to help with management and organization, consisting of a key value pair.</p>
<p>For example, you might have a key of ‘project,’ with a value of ‘new app. ‘ Or a key of ‘department,’ with a value of ‘finance. ‘ The tags help you to create custom metadata of your infrastructure. When creating an assessment target, you are asked to select which keys from your tags that you would like to include within your Assessment Target. You can also refine your selection even further by providing the values for each of those keys, too. So you could have an Assessment Target configured as follows. This Assessment Target would then be associated to any instances which had either the project or department key with the corresponding values.</p>
<p>The EC2 instances are not required to contain both keys to be included within this Assessment Target. Only a match of one key is necessary. AWS Agents.</p>
<p>AWS Agents are essential to Amazon Inspector. These are software agents that must be installed on EC2 instances that you with to monitor and run the assessments on. Without this agent, Amazon Inspector would not be able to perform the analysis that it needs to. Once installed, the agent will be able to track and monitor data across the network file system, and any process activity of the instance. This data is then recorded as telemetry data, and is fed back to the Amazon Inspector service via the public endpoint of the service over a Transport Layer Securityprotected channel.</p>
<p>A regular heartbeat is sent from the agent to Inspector, which the Inspector service will respond to with instructions, such as to perform an assessment at a particular time. As the Agent is software-based, it is necessary from time to time to update the agent with the latest version. These new updates are managed and automatically installed by AWS, and so you don’t need to worry about the latest Agent software version.</p>
<p>The auto update of these agents are scheduled outside of any assessments that are due to take place. As this is a key component of Amazon Inspector, I now want to provide a demonstration on how to install these agents for both Linux and Windows operating systems. I will start by installing the agent on a Linux instance, and then a Windows instance.</p>
<p>I shall add all of these commands and links that I use to this lecture’s transcript, allowing you to copy and repeat if required.</p>
<p>Okay, so I’m within the AWS Management Console within the EC2 dashboard. And as you can see, I have two instances up and running. One of them is a Linux box, and another is a Windows box.</p>
<p>Now what I want to do is install the Amazon Inspector agents on each of these. So I’ll start with the Linux box, and it’s a very simple process. So to do this, what we need to do is download an agent installation script using either of these two commands, and then install the agent with this command, here.</p>
<p>So let’s go ahead and download and install the installation script. So if I go across to my instance, which I’m already connected to here. If I run the command. As you can see it’s a very quick install, and now it’s downloaded. I actually need to install the agent. So if I just type in this command here, sudo bash install, that will go ahead and install the Amazon Inspector agent for us.</p>
<p>And that’s the installation completed for a Linux box. So now if we swap over to our Windows box and install the agent in there. So for the Windows install, we need to go to this URL here to download the agent. So if I’m now to swap over to my Windows box, open up Internet Explorer and paste in that URL.</p>
<p>Click on okay. You may get a security warning about trusted sites, so just add that. . . And then try again. Click on okay. Save the download. Now the download is completed, we can then run the . exe file. And that will go ahead and install the agent. So if we agree to the license terms and conditions, and click on install, you can see the usual progress bar of the installation taking place.</p>
<p>That’s now successfully completed. And the agent is now installed. So that’s how you install the Amazon Inspector agents for both a Linux and Windows box.</p>
<p>An assessment template defines a specific configuration as to how an assessment is run on your EC2 instances. These configurable items within the template include the following.</p>
<p>Rules packages to be used during the assessment, and I’ll explain more on this later in this lecture. The duration of the assessment run, which can be 15 minutes, one hour, which is the recommendation by AWS, eight hours, 12 hours, or 24 hours. Specific SNS topics that Amazon Inspector should be used for any notifications, and these notifications can be when an assessment run starts, when it finishes, when it changes state, or when findings are reported.</p>
<p>Do be aware that you’ll need to allow access for Amazon Inspector to publish to the topic to enable these notifications within the topic policy. And any Amazon Inspector-specific attributes that can be assigned to findings generated by the assessment. So essentially, your findings can be tagged with keys and values of your choice. For example, with a key of ‘Assign To’ with a value of ‘Windows Security Team. ‘ Once your assessment template is created, you are not able to modify it again to make changes. You can, however, tag your assessment templates to help with the organization and management of your assessment runs. Rules Packages.</p>
<p>When Amazon Inspector gathers telemetry during an assessment run, it will then compare this data against specific security rules to ascertain its compliance. And these rules are grouped together in what is known as a rules package. So a rules package contains a number of individual rules that are each checked against the telemetry data back from the EC2 instance.</p>
<p>Each rule will also have an associated severity which will be one of the following. High. Should there be a finding with a rule with a severity of high, then this should be looked at immediately and rectified. This is the highest severity level, and as a result would likely compromise the integrity, confidentiality, and availability of your data and instants if left.</p>
<p>Medium. This is the next level down from high, and so any findings with a medium rule should be attended to in a timely manner. This severity also poses a risk to confidentiality and integrity. Low. Any rules highlighted with a low severity need to be rectified, but the urgency is not as severe as medium or high.</p>
<p>If any findings are returned with a high or medium rule, then these should be resolved as a matter of priority over any with a low severity. Informational. These would describe a particular security configuration within your assessment target, which you could then use to make changes to your environment to improve its effectiveness, or simply make a note for future reference.</p>
<p>The rule packages themselves are split across four different categories, these being: Common Vulnerabilities and Exposures, Center for Internet SecurityBenchmarks, Security Best Practices, and Runtime Behavior Analysis. Let me explain each of these in a bit more detail to understand what each of these rule packages look for during an assessment run.</p>
<p>Common Vulnerabilities and Exposures. The CVE is a publicly-known reference list of security threats that are well-documented. The rules used within this package will check the Assessment Target for exposure to any known security holes that would compromise the integrity, confidentiality, and availability of your EC2 instance.</p>
<p>Should any findings from an assessment be found against a CVE, it’s recommended you visit this site and search for specific vulnerability ID to gather additional detailed information to help you resolve and mitigate the issue. To check which CVEs the rules within the rules package are performing an assessment against, you can visit the following <a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/rules-engine/CVEList.txt">link</a>.</p>
<p>As new CVEs are found, they are added to this list by AWS, and the corresponding rules added to the rules package, preventing the need for you to stay up-to-date with the latest known security issues. Center for Internet Security Benchmarks. These benchmarks are continuously refined and used as global standards for best practices for protecting data and IT resources.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> is a CIS Benchmarks member company, and Amazon Inspector’s associated certifications can be found here. The rules within this rule package help to assess security for the following operating systems as per the certification <a target="_blank" rel="noopener" href="https://www.cisecurity.org/partner/amazon-web-services/">link</a>. If any findings are made against this rules package, then similarly to the CVE list, you can visit the following <a target="_blank" rel="noopener" href="https://www.cisecurity.org/cis-benchmarks/">link</a> to download the detailed description, explanation, and advice on how to mitigate the security issue found.</p>
<p>Security Best Practices. This rules package looks for weaknesses in common security best practices. However, this only applies to Assessment Targets that are running the Linux operating system. At this stage, it’s not possible to run this rules package on any target that has the marks of Windows OS. The following security checks are covered within this rules package.</p>
<p>Disable root login over SSH. This checks to see if the SSH daemon is configured to allow login into your instances root. Support SSH version two only. This checks to see if the instance is configured to run SSH protocol version one. Disable password authentication over SSH. This rule checks to see if password authentication over SSH is configured.</p>
<p>Configure password maximum age. This simply checks to see if a maximum age for a password has been configured on the instance. Configure password minimum length. Similarly to the previous point, this checks to see if a minimum password length has been configured on the instance.</p>
<p>Configure password complexity. This checks to see if the instance is using a password complexity mechanism for passwords.</p>
<p>Enable ASLR. This simply checks to see if address space layout randomization is enabled.</p>
<p>Enable DEP. This rule checks to see if data execution prevention is enabled on the instance.</p>
<p>And finally, configuration permissions for systems directories. This rule checks to ensure that only the root user has right access to system directories. For detailed information on each of these rules, see the AWS documentation found <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_security-best-practices.html">here</a>. </p>
<p>During the creation of your assessments, you are able to select more than one rules package. And so if supported by your target assessments, you can run all four rules packages for deep security analysis of your resources. However, do be aware that not all the rules packages are available on all operating systems.</p>
<p>For example, and as I have already mentioned, the security best practice rules package is only available for the Linux OS. The table here shows a quick reference guide as to which rules packages are available for which operating systems. So do be aware of this when planning your security targets and templates.</p>
<p>Assessment Run. As assessment run can happen once you have configured your Amazon Inspector role, installed the agents and configured your Assessment Target and Assessment Templates. Once these components are in place, you are then able to run the configured assessment on your assessment targets. This process is known as the assessment run.</p>
<p>During this time, telemetry data will be sent back to Amazon Inspector and S3 to assess the data against the specified rules packages defined within the assessment template. Multiple assessment runs can be run at the same time, but only if the assessment targets do not have any duplicated EC2 instances within them.</p>
<p>During the assessment run, it is possible from within the management console to view the progress of the run in addition to stopping, starting, and deleting the run. Telemetry. I’ve mentioned telemetry a number of times already, and you are probably now aware of what this is. However, telemetry is a data that is collected from an instance, detailing its configuration, behavior and processes during an assessment run.</p>
<p>Once collected, the data is then sent back to Amazon Inspector in near-real-time over TLS where it is then stored and encrypted on S3 via an ephemeral KMS key. Amazon Inspector then accesses the S3 Bucket, decrypts the data in memory, and analyzes it against any rules packages used for that assessment to generate the findings.</p>
<p>After 30 days, this telemetry data is then deleted using a lifecycle policy attached to the dedicated Amazon Inspector S3 Bucket. Assessment Reports. On completion of an assessment run, it is possible to generate an assessment report which provides details on what was assessed, and the results of that assessment.</p>
<p>As this feature was only released at the end of April 2017, It’s only possible to generate these reports for any assessment runs that were completed on or after the 25th of April, 2017. There are two different types of reports that you can generate: a findings report, and a full report. The findings report will only contain a subset of the full report.</p>
<p>This will include a summary of the assessment that took place, a list of the EC2 instances that were within the assessment targets, the rules packages that were used from within the assessment template, and finally a detailed report on any findings that occurred. The full report, and as expected, contains all of the information from the findings report, in addition to a list of rules that were passed successfully for all the instances within the assessment target.</p>
<p>Findings. Findings are generated from the results of an assessment run. A finding is a potential security issue or risk against one of your EC2 instances within the assessment target. For each finding, an explanation of the issue is given, along with guidance on how to remediate the problem. More on findings will be given in a later lecture within this course.</p>
<p>That now brings us to the end of this lecture. And so to quickly recap, we looked at the following components: the Amazon Inspector role and the permissions required, assessment targets and how they use tagging of EC2 instances, AWS Agents and how they are installed on both Linux and Windows instances, assessment templates including configuration and the different rules packages available, when an assessment run can be performed, an understanding of telemetry data, the different types of assessment reports that are available, and lastly, an overview of what findings are.</p>
<p>Coming up in the next lecture, I’m going to demonstrate <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/how-to-configure-amazon-inspector/">how to configure Amazon Inspector</a>, which will include a lot of the components that I have mentioned in this lecture.</p>
<h1 id="How-to-Configure-Amazon-Inspector"><a href="#How-to-Configure-Amazon-Inspector" class="headerlink" title="How to Configure Amazon Inspector"></a>How to Configure Amazon Inspector</h1><p>Hello, and welcome to this lecture where I’m going to demonstrate how to configure <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector</a>.</p>
<p>For the purpose of this demonstration, I have a single Linux instance, and a single Windows instance running within a public subnet of a VPC. The instances already have the agents installed as per a previous demonstration.</p>
<p>I have also tagged the instances with the key of OperatingSystem, and value of Windows and Linux respectively. In this demonstration, I’m going to complete the following steps. I shall create and confirm the Amazon Inspector rule. Create two assessment targets. Define two assessment templates. Run two assessment runs simultaneously. Generate an assessment report for the assessment run. And automatically schedule future assessment runs via an AWS lambda function. So let’s get started.</p>
<p>Okay so as I just explained, I’ve already created two EC2 instances. A Linux box, and a Windows box. And they both have the agents installed already. And I’ve set up another tag called OperatingSystem with Linux and Windows tagged respectively.</p>
<p>So now I want to go across to Amazon Inspector to start configuring the service. So I go up to services, I’ve got a shortcut here already to get us in Inspector. But it’s also under the security identity and compliance section here. Okay so I’m now at the Amazon Inspector dashboard. And as you can see, I don’t have any findings or any assessments kind of running or completed at the moment.</p>
<p>If we go down to the account settings here, to manage the Amazon Inspector service rule, then we can see here that we can choose or create a new Amazon Inspector rule with the required permissions. So if we click on choose or create rule, it then takes us to a screen where it states that Amazon Inspector is requesting permission to use resources in your account. And all we need to do here is click allow in the bottom right-hand corner. But before we do, let’s take a look at the details.</p>
<p>So it gives it a rule description. Gives it a rule name of Inspector. And it will create a new policy when we click on allow. So let’s take a look at the policy document. As you can see, it allows the action of EC2 describe instances to all resources.</p>
<p>So effectively, read-only access to your EC2 instances. Okay, so that’s the policy document that’ll be associated with the rule. Then we click on allow. And that will go ahead and validate the IAM rule for Amazon Inspector. Okay so now we have the rule configured. If we click on cancel. Okay so the first thing to do is to set up some assessment targets.</p>
<p>Now I want to set up two assessment targets. One for the Windows boxes, and one for the Linux boxes. So if you click on create, and give it a name. Call this one Linux Assessment Target. And then we have the tags here. So we want all EC2 instances with the OperatingSystem key and a value of Linux to be included in this assessment target.</p>
<p>Then click on save. And we want to set up another one for the Windows box as well. So Windows Assessment Target. Again, OperatingSystem, but this time the value of Windows. So that will search all EC2 instances within my AWS account with the tag of OperatingSystem with a value of Windows. Click on save. So now we have our two assessment targets.</p>
<p>Now we’ll need to create our assessment templates. So if we click on assessment templates up here, go to create. Give this a name. So we’ll just call this one Linux. And here we can select our targets that we just created. So I want to select the Linux assessment target. Now here we can select which rule’s packages we want to run against our targets. So I’m going to select all of them here, all four. And then we can select the duration of the assessment run. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> recommends that it’s run for an hour. The longer the better. But you can run it for up to 24 hours. For the sake of this demonstration, I’m just going to leave it at 15 minutes. We should pick up a few <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/working-with-findings/">findings</a> with that.</p>
<p>And like I mentioned earlier as well, you can associate an SNS topic to be notified of any events. So let me associate this topic I created earlier. And I want to see events for when the assessment run is started. When it’s finished, when the state has changed, and any findings that are reported. And then down here we have attributes added to findings. So I’m going to say here assign to Linux Security Team. So if any findings are found, I want a tag to be associated with them with the key of assign to and a value of the Linux Security Team. So let’s create that.</p>
<p>And then we have our template with the target names, and those packages, and the SNS topics, et cetera.</p>
<p>Now I want to create another one for Windows. So I’ll call this one Windows. Set a target of the Windows assessment target. So now I’ll select the rules packages, and we know security best practices doesn’t run on Windows, so I’ll select all the other packages other than that one. Again, duration for 15 minutes.</p>
<p>And again, the SNS topic. And here I’m going to say assign to Windows Security Team. And create. Okay, so we now have two assessment templates. One for Windows, and one for Linux. Each with different rules packages assigned.</p>
<p>So now I want to run the assessment, and I want to run them both at the same time just because I know the EC2 instances do not overlap.</p>
<p>So I’m able to run them both. So I’ve selected the assessment templates. And then I click on run. And I have a message there saying assessment run started. And as you can see, we have information here saying it is now collecting data. And the number of runs, so it’s the first time this is running. So what you should do now is actively use both of those boxes for your applications, for loggin in, just as you would kinda stress test those boxes to try and uncover and unearth as many security flaws as possible.</p>
<p>So use them as much as you can within the assessment run. And that should uncover as many security flaws as possible. So what I’ll do now, I’ll pause the video, and I’ll come back to this when the assessment has finished.</p>
<p>Okay so the assessment run is now completed. And we can see that there are four findings for our Linux box and our Windows box has 230. So if we click on the four findings for the Linux boxes, that will take us over to our Findings section here. And we can see that we have one with a severity of medium, one at low, and two informational. And similarly if we go back to our Windows box, we can see all of our findings here and like I said, we have 230 of them.</p>
<p>So we have quite a lot of severity high which should be rectified immediately. I won’t go through on how to analyze the findings just yet. I’ll be doing that in a later lecture. So I just wanted to show you at this stage that the assessment has complete. And it’s found a number of findings for both assessment targets.</p>
<p>But what I will show you quickly is the reports. So as you can see on the far side here, we have reports, and we have an icon for each. So if we click on the one for Linux, here we can see, we can have two different reports. A findings report, or full report. The findings report just shows information about the findings.</p>
<p>Whereas the full report will contain all that same information plus all the rules that were passed as well. So let’s just have a look at the findings report as a PDF. So if we generate this report. We can see here that it’s formatted with a nice front cover. Gives you the date of which the report was generated and what assessment template was run, and the start and finish times as well. And it will just give you a quick summary of what happened during the assessment, and which rules packages we used. And also what was tested as well within those rules packages. So it’s a nice report to present to any auditors or internal security teams to nicely demonstrate what was found, what was run, and all the detailed information in between.</p>
<p>So let’s just go back to Amazon Inspector. So that’s how you configure Amazon Inspector, and how you create the assessment targets, the assessment templates, and generate an assessment run as well and view the reports.</p>
<p>Okay so what I want to show you now is how to automatically schedule your assessment runs based off your assessment templates.</p>
<p>And to do this, we need to use a different service. We need to use AWS Lambda. So let’s go across to the Lambda service. We will click on Dashboard. Then we need to create a Lambda function. From here, we need to select a blueprint. And if we just type in inspector, then it will filter out all the blueprints that AWS has already created for us.</p>
<p>And here we’ve got the Inspector scheduled run. Which schedules a recurring Amazon Inspector assessment run. So that’s what we need. We’ll need to create a rule name. So create a new rule. And we’ll call this Amazon Inspector demo. Description, daily run. And then we need to enter a scheduled expression. So I want this to run everyday. And there’s a couple of examples under here on how to represent days and times, et cetera. And if we enable the trigger. Then go to next.</p>
<p>Give this Lambda function a name. We’ll call this AI demo. We can leave the description as is. Leave the runtime as NodeJS. And this is all the code within the Lambda function based off of the template that we selected.</p>
<p>But we do need to add an environment variable here. So we need to look for our assessment template ARN. So if we go back to Amazon Inspector. Select our Windows assessment template. And we can see here, we have an ARN for this template. So if we copy and paste that into the variable. If we then go down to the handler, we’ll leave the handler as index-handler as the default. We’ll then need to create a rule for permissions to run this function. So if we create a new rule from template, then Lambda will automatically create an SO permissions for us. We’ll give this rule a name. Lambda demo. And then all we need to do at this point now, is click on next.</p>
<p>This is just a review of the options. We create the function. And that’s it. It’s created. Congratulations, your Lambda function has been successfully created. So now everyday, my Windows assessment target will be automatically run for me.</p>
<h1 id="Working-with-Findings"><a href="#Working-with-Findings" class="headerlink" title="Working with Findings"></a>Working with Findings</h1><p>Hello and welcome to this lecture. What I want to show you, how to work with Findings from your Assessment Runs, that you may find. I thought that this lecture might be easier if we continue on from the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/how-to-configure-amazon-inspector/">previous demonstration</a>, so let’s get back to the environment.</p>
<p>Okay, so we’re back in the console, looking at our two Assessment Runs, that we had and just as a quick reminder, for the Linux template, we had four Findings and for the Windows, we had 230, so I just kind of want to run through how you can look at the Findings in a bit more detail and the information that they provide.</p>
<p>If we go across to the left-hand side here under Findings, there’s a number of Severity Filters, High, Medium, Low and Informational, so if we take a look at the High Severity Findings first. Now, we can see that there’s 227 of these, that the Assessment has found and we can see that the majority of them are against the Windows template, so let’s have a look at one of these Findings, just to see what information we have.</p>
<p>So, if we expand the Finding and get a bit more detail, this will give us the ARN of the Finding, the Run Name, the Target Assessment, that the Finding was run against and which template was used within the Assessment as well and the Start and End Time of the Assessment Run and the Current Status. As we scroll down, we can also see which Rules Package that this Finding came from, so this came from the CIS Benchmarks and it also gives the ID of the Instance as well, I mean, if we was to click on this, for example, it’ll take us straight to that Instance, as we can see there, it’s the Windows box, so there’s a number of hyperlinks, that you can kind of access the Target etc and the template, if you needed to to get more information.</p>
<p>Right, in this section here, the Finding, this is actually the issue that it found and then it raised, so here you can see, it explains that this Instance is not compliant with a specific rule within the Rules Package and it explains that you need to ensure there’s a minimum password age is set to one or more days and this is a requirement against the CIS Benchmark for this Windows server, 2012, so that’s been highlighted as Severity of High.</p>
<p>If we go down to the Descriptions section, it gives us further details again and again, this talks about the minimum password age, it needs to be set to more than one day, it says you can go up to a maximum of 999 days and it also gives a rationale behind the reasoning for this Finding, so there’s a quite a lot of information there to get an understanding of why the Finding has been found and why there should be Recommendations to rectify it and then finally with regards to Recommendations, it does give a Recommendation on what you should do to resolve the issue and here it says you need to establish the recommended configuration, you need to set the following UI path to one or more days and then here, it’s also given us the path as well, so it’s very easy for us to get an understanding of what the issue is, why it’s an issue and what we need to do to remediate the problem.</p>
<p>So, it’s quite a lot of detail within the Findings. Let’s close that one up and again, if we go through any other Finding, it’ll be similar kind of information, again a Description, a rationale and a Recommendation as well.</p>
<p>So, if we look at some of the Medium Severities, so it’s only found one Medium Severity issue here against the Linux box, so if we take a look at that, again we have the ARN, the Run Name, Target and template name and again, what’s important is to know which Rules Package this has come from, so it’s come from the Security Best Practices and against which Agent as well and the Finding here explains that it’s configured to allow users to log in with root credentials over SSH, which increases the likelihood of a brute force attack and again it’s given a Recommendation as to what to do to resolve the issue and it explains here a couple of commands to disable SSH root logins, so again, very good information, very useful and a Recommendation on how to resolve the problem.</p>
<p>So, let’s take a look at if we’ve got any Low Findings and yes, we have a couple, we have one for Windows and one for Linux against Behavior Analysis, Runtime Behavior Analysis, so let’s take a quick look. Again, all very similar information, we have the Rules Package there, the Agent and the Finding as well, it’s saying that there are insecure protocols used to connect to the remote host and again, a Recommendation of replacing those insecure protocols with encrypted versions and if we look at the Linux Finding, we can also see here that it’s the same issue, that insecure protocols were used to connect to the remote host.</p>
<p>So that’s just a couple of Low Findings there, but if it was in a production environment, then you would definitely look to resolve all the High Severities first and then the Mediums and then work on the Lows afterwards and then finally we have Informational and we have a couple of items here for both boxes, Windows and Linux, so if we take a look at one of the Windows, let’s see what it says here. It just explains in the Finding, that this agent was listening on TCP ports, but no connections were using those ports during the Assessment Run, so the Recommendation is to disable any network services, that we don’t use, so we don’t expose those ports and we can reduce the attack surface area of our deployment.</p>
<p>So again, these are just Informational, we can take action upon that, if we need to, but it’s not a great risk to us.</p>
<p>What you can also do as well is download this information by clicking on this button here and it will export the data as a CSV file, so let’s export all columns and take a look. So this opens the CSV file of a lot of the Findings information stood, Severity, Date, the actual Finding itself, which is the important part, which Targets these were found on and which template and against which Rules Package and again, we have the ARN Rule, agent ID, etc. So you might want to export some of those Findings into a CSV file to kind of work against them and maybe track them a little easier and pass this document around to other members within the team.</p>
<p>So let’s go back to the console. So that’s essentially it for Findings, they’ll all be found on this left-hand side in this menu under Findings and then you can Filter them as required against specific Targets or templates or even Rules Packages as well, if you’re just interested in a particular Rules Package, then you can just break those down individually.</p>
<p>Okay, then that’s the end of the demonstration.</p>
<h1 id="Integration-with-CloudWatch-amp-CloudTrail"><a href="#Integration-with-CloudWatch-amp-CloudTrail" class="headerlink" title="Integration with CloudWatch &amp; CloudTrail"></a>Integration with CloudWatch &amp; CloudTrail</h1><p><strong>Resources &amp; Links used in this Lecture</strong></p>
<p><strong>Amazon CloudWatch Course:</strong> <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/">https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/</a></p>
<p><strong>AWS CloudTrail Course:</strong> <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/</a>**<br>**</p>
<p><strong>———————————————————————————————————————————————</strong></p>
<p><strong>Transcript:</strong></p>
<p>Hello and welcome to this very short lecture where I want to show you how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector</a> integrates with Amazon CloudWatch and AWS CloudTrail.</p>
<p>For more information on both of these services, please see our existing courses <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">here</a> on both services.</p>
<p>Being able to monitor and track the performance of a service and trace its actions from an audit perspective is invaluable. And Amazon Inspector integrates seamlessly with both CloudWatch and CloudTrail which performs these functions.</p>
<p>You can use Amazon CloudWatch to monitor and inspect agents against specific metrics for both assessment targets and assessment templates as shown. There is also an aggregate metric which will count the number of assessment runs within your account. These metrics allow you to monitor the performance and activity of Amazon Inspector, providing a greater insight into specific statistics.</p>
<p>From an AWS CloudTrail perspective, all API calls that are performed by Inspector are logged with CloudTrail. This allows you to understand by who, what, and when specific actions were carried out, which are all saved and recorded within the CloudTrail log files that are stored in S3.</p>
<p>These integration elements allow for additional auditing and monitoring to take place throughout your security assessments within your environment which is always an added benefit from a governance and security audit perspective. That now brings us to the end of this lecture.</p>
<p>Coming up next, I want to highlight the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/service-limitations-and-costs/">limitations of the Amazon Inspector service</a>.</p>
<h1 id="Service-Limitations-and-Costs"><a href="#Service-Limitations-and-Costs" class="headerlink" title="Service Limitations and Costs"></a>Service Limitations and Costs</h1><p>Hello and welcome to this lecture. We’re going to explain the different service limitations of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector</a> as well as how much it’s going to cost to use and implement within your own environment.</p>
<p>Across the service there are a number of limitations that you’ll need to be aware of.</p>
<p>Agents per assessment. When creating your target assessments the maximum number of agents that can be included is 500. Be aware that this limit can’t be increased. The number of assessment runs.</p>
<p>There is a default limit of 50,000 assessment runs that you can have per account. If you need this limit to be increased then you need to contact the AWS Customer Support to do so.</p>
<p>The number of assessment templates. Again, by default, there is a limit of 500 assessment templates that can assist in an AWS account. If this limit needs to be increased, then again, you can contact AWS Customer Support.</p>
<p>The number of assessment targets. This has a default limit set to 50, which can also be increased if required to do so.</p>
<p>Although I mentioned this earlier in a previous lecture, it’s worth noticing the limitation of rule packages against specific operating systems.</p>
<p>So how much will using Amazon Inspector cost you to gain all of this additional security information within your environment? Which is always an important factor for any service. For the benefit it brings to you and your environment, I think it’s very cost-effective.</p>
<p>Essentially, Amazon Inspector is priced at per-agent, per-assessment run, which is an agent assessment per month. For example, if you were to run one agent against 20 assessments that would be 20 agent assessments, or two agents against five assessments, that would be 10 agent assessments. There are no other costs associated within Amazon Inspector so there are no up front or on-going maintenance costs.</p>
<p>The pricing for agent assessments per month starts at 30 cents but there is capacity for a discount with the more agent assessments run per month.</p>
<p>That now brings us to the end of this lecture. Coming up in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/summary-10/">last lecture</a>, I will summarize the main points that we have learned from each lecture.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello and welcome to this lecture where I want to briefly summarize the key points that we have learnt throughout the previous lectures.</p>
<p>I started off by looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/what-is-amazon-inspector/">what Amazon Inspector is</a> and what it does. Here I explained that Amazon Inspector is a managed service to help find security vulnerabilities within your applications and services. It uses hundreds of best practices and known security weaknesses to assess EC2 instances. Any findings are detailed to allow you to rectify the security risk within your environment. Amazon Inspector provides confidence in the level of security built into your applications and services. Threats and vulnerabilities can be reduced when using Amazon Inspector.</p>
<p>Next I explained the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/components-of-amazon-inspector/">components that make up the Amazon Inspector service</a>. The Amazon Inspector role. This role has read only access to all EC2 instances within your AWS account allowing you to run assessments.</p>
<p>Assessment targets. This is a grouping of AWS EC2 instances that you want to run an assessment against, which are grouped together using tags.</p>
<p>AWS agents. These are software agents that are installed on your EC2 instances that need to be assessed to track and monitor data across the network, file system, and any process activity of the instance.</p>
<p>Assessment templates. These templates define a specific <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/how-to-configure-amazon-inspector/">configuration</a> as to how an assessment is run on your EC2 instances.</p>
<p>Rules packages. Rules packages contain a number of individual rules which are individually checked against the telemetry data that comes back from the assessment. The rules packages are common vulnerabilities and exposures, Center for Internet Security benchmarks, and security best practices.</p>
<p>Assessment run. Once you have configured your Amazon Inspector role, installed the agents, and configured your assessment target and assessment templates, you can run the configured assessment on your assessment targets, which is known as the assessment run.</p>
<p>Telemetry. This is the data that is collected from an instance detailing its configuration, behavior, and process during an assessment run.</p>
<p>Assessment reports. On completion of an assessment run, an assessment report can be generated providing details on what was assessed and all the results of that assessment. This is available as either a findings report or a full report.</p>
<p>Findings. A finding is a potential security issue or risk against one of your EC2 instances within the assessment target following an assessment run.</p>
<p>Once there is an understanding of the different components of the Amazon Inspector service, I performed a demonstration where I showed you how to create and select an Amazon Inspector role, create an assessment target, define an assessment template, complete an assessment run, generate an assessment report, and automatically schedule future assessment runs via an AWS lambda function.</p>
<p>This was followed up by showing you how to review your findings through the use of assessment reports, filtering of findings, viewing the findings in a detailed view, remediation recommendations and steps, tagging of findings to allow you to manage the workflow of resolution.</p>
<p>I then looked at how Amazon Inspector has integration with Amazon CloudWatch and AWS CloudTrail.</p>
<p>Within CloudWatch there are a number of metrics for assessment targets, assessment templates, and aggregate metrics. From an AWS CloudTrail perspective, all API calls that are performed by Inspector are logged with CloudTrail.</p>
<p>Finally, I looked at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/service-limitations-and-costs/">service limitations and costings</a>. From a limitation perspective, a maximum number of agents per assessment is 500. There is a default limit of 50,000 assessment runs per AWS account. A default limit of 500 assessment templates are allowed per AWS account, and a default limit of 50 for assessment targets. Pricing is configured as follows. Amazon Inspector is priced at per agent, per assessment run, which is an agent assessment, per month. There are no other costs associated with Amazon Inspector, so there are no upfront or ongoing maintenance costs. And pricing for agent assessments start at $0. 30 per month.</p>
<p>You should now have a good understanding of Amazon Inspector and what it is used for and how it can use best of service within your own environment to help you mitigate against known security vulnerabilities and exposures to ensure your environment stays as secure as it can be.</p>
<p>If you have any feedback on this course, positive or negative, please do leave a comment on the course landing page. We do look at the comments and your feedback is greatly appreciated.</p>
<p>Thank you for your time and good luck with your continued learning of cloud computing.</p>
<p>Thank you.</p>
<h1 id="3Components-of-Amazon-Inspector"><a href="#3Components-of-Amazon-Inspector" class="headerlink" title="3Components of Amazon Inspector"></a>3<strong>Components of Amazon Inspector</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">IAM Course</a></p>
<p><a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install">Linux install</a></p>
<p><a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/windows/installer/latest/AWSAgentInstall.exe">Windows install</a></p>
<p><a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/rules-engine/CVEList.txt">CVE rules</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cisecurity.org/partner/amazon-web-services/">Amazon Inspectors CIS Certificates</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cisecurity.org/cis-benchmarks/">CIS benchmarks</a></p>
<p><a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_security-best-practices.html">Security Best Practices</a></p>
<p><a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_runtime-behavior-analysis.html">Runtime Behavior Analysis</a></p>
<h1 id="6Integration-with-CloudWatch-amp-CloudTrail"><a href="#6Integration-with-CloudWatch-amp-CloudTrail" class="headerlink" title="6Integration with CloudWatch &amp; CloudTrail"></a>6<strong>Integration with CloudWatch &amp; CloudTrail</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/">Amazon CloudWatch course</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail course</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:09" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:09-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:03:26" itemprop="dateModified" datetime="2022-11-19T23:03:26-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-AWS-Config-An-Introduction-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-AWS-Config-An-Introduction-16/" class="post-title-link" itemprop="url">AWS-Security-Specialty-AWS-Config-An-Introduction-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:07" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:07-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:16" itemprop="dateModified" datetime="2022-11-19T22:53:16-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-AWS-Config-An-Introduction-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-AWS-Config-An-Introduction-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to this course covering AWS Config. This service is classed as a management tool service which is fully managed and allows you to have visibility of your entire AWS infrastructure from a configuration perspective. As well as using the service to act as a resource inventory, compliance checker, and manage configuration changes of your resources it can also be used as a part of your security analysis procedure.</p>
<p>Before we go any further, I’d like to introduce myself. My name is Stuart Scott. I am on of the trainers here at Cloud Academy specializing in AWS Amazon web services. Feel free to connect with me with any questions using the detail shown on screen. Alternatively, you can always get in touch with us here at Cloud Academy using the community form where one of our cloud experts will reply to your question.</p>
<p>This course will be beneficial for people who are responsible for managing resource configuration changes within an AWS cloud environment. Also auditors who must have an awareness of all AWS infrastructure and their current configurations and compliants as well as security engineers who are responsible for implementing AWS security and analyzing logs and identifying weaknesses and breaches to their environment.</p>
<p>Throughout this course we are going to be taking a deep look at AWS config and as a result the following areas are going to be discussed. Starting with:</p>
<p>What is AWS Config? Within this lecture, we will understand exactly what the service is and what function it provides.</p>
<p>Next we’ll take a look at key components. This lecture breaks down the service to allow us to look at all the components and their relationships to each other and the role they play as a part of the AWS Config service.</p>
<p>Then we’ll look at service integration. This lecture will look at how the AWS Config service integrates with other AWS services.</p>
<p>Following this, we’ll look at how to manage compliance with AWS Config. Here we will focus on how to maintain compliance using AWS config whether these be internal or external requirements or standards.</p>
<p>And then finally, we’ll look at some use cases and best practices. And this lecture will focus on some of the use cases of when it’s best to use AWS Config to help you maintain, support and operate your AWS environment.</p>
<p>On completion of this course, you will be able to recognize and explain how the AWS Config service can be used by AWS customers to monitor environmental changes. You’ll be able to recognize and explain the core elements of the AWS Config service. You’ll be able to comfortably configure and implement AWS Config within your own environment and you’ll understand how to maintain compliance of your resources using AWS Config. For example, to help maintain security policies.</p>
<p>Although you do not need to be an AWS expert to appreciate and benefit from this course, students will benefit from having a basic understanding of cloud computing and the AWS platform. So we recommend completing the What is Cloud Computing course along with the AWS Fundamentals Learning path. A high level awareness of the following added services would also be beneficial but not essential. Simple notification service, SNS, simple queue service, SQS, simple storage service, S3, AWS CloudTrail, and identity and access management.</p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could use the comments section found on the landing page of this course.</p>
<p>That brings us to the end of this lecture. Coming up next, we’ll answer the question what is AWS Config?</p>
<h1 id="What-is-AWS-Config"><a href="#What-is-AWS-Config" class="headerlink" title="What is AWS Config?"></a>What is AWS Config?</h1><p>Hello, and welcome to this lecture, where we will talk about the AWS Config service itself, what it is, and what it does. So let’s get started.</p>
<p>As many of you will be aware, one of the biggest headaches in any organization when it comes to resource management of IT infrastructure is understanding the following. What resources do we have? What devices are out there within our infrastructure performing functions?</p>
<p>Do we have resources that are no longer needed, and therefore, can we be saving money by switching them off?</p>
<p>What is status of their configuration? Are there any security vulnerabilities we need to worry about?</p>
<p>How are our resources linked within the environment? What relationships are there, and are there any dependencies? If we make a change to one resource, will this affect another?</p>
<p>What changes have occurred on the resources, and by whom? Do we have a history of changes for this resource that shows us how the resourced changed over time?</p>
<p>Is the infrastructure compliant with specific governance controls, and how can we check to ensure that this configuration is meeting specific internal and external requirements?</p>
<p>And, do we have accurate auditing information that can be passed to external auditors for compliance checks?</p>
<p>Depending on the size of your deployment with AWS, trying to answer some of these questions can be very time consuming and laborious. Some of this information can be captured via the AWS CLI by performing a ‘describe’, or ‘list’, against the specific resource. But implementing a system to capture those results and output them into a readable format could be very resource intensive. And of course, this will only help you with a small piece of the puzzle.</p>
<p>AWS is aware that, due to the very nature of the cloud and its benefits, the resources within an AWS environment are likely to fluctuate frequently, along with the configurations of the resources. The cloud, by its very nature, is designed to do so, and so trying to keep up with the resource management can be a struggle. Because of this, AWS released AWS Config to help with this very task. The service has been designed to record and capture resource changes within your environment, allowing you to perform a number of actions against the data that helps to find answers to the questions that we highlighted previously.</p>
<p>So what did AWS design AWS Config to do? Well, in a nutshell, AWS Config can capture resource changes, so any change to a resource supported by Config can be recorded, which will record what changed along with other useful metadata, all held within a file known as a configuration item, a CI.</p>
<p>It can act as resource inventory. AWS Config can discover supported resources running within your environment, allowing you to see data about that resource type.</p>
<p>It can store configuration history for individual resources. The service will record and hold all existing changes that have happened against the resource, providing a useful history record of changes.</p>
<p>It can provide a snapshot in time of current resource configurations. An entire snapshot of all supported resources within a region can be captured that will detail their current configurations with all related metadata.</p>
<p>Enable notifications of when a change has occurred on a resource. The Simple Notification Service, SNS, is used with AWS Config to capture a configuration stream of changes, enabling you to process and analyze to changes to resources.</p>
<p>It can provide information on who made the change and when through AWS CloudTrail Integration. AWS CloudTrail is used with AWS Config to help you identify who made the change and when, and with which API.</p>
<p>You can enforce rules that check the compliancy of your resource against specific controls. Pre-defined and custom rules can be configured with AWS Config, allowing you to check resources’ compliance against these rules.</p>
<p>You can perform security analysis within your AWS environment. A number of security resources can be recorded, and when this is coupled with rules relating to security, such as encryption checks, this can become a powerful analysis tool, and it can provide relationship connectivity information between resources. The AWS Management Console provides a great relationship query, allowing you to quickly see and identify which resources are related to any other resource. For example, when looking at an EBS volume, you will able to see which EC2 instance it is connected to, and it does all of this and presents the data in a friendly format. This is a lot of incredibly useful data that can be used across a range of different scenarios, some of which we will cover later in this course.</p>
<p>AWS Config is region specific, meaning that if you have resources in multiple regions, then you will have to configure AWS Config for each region you want to record resource changes for. When doing so, you are able to specify different options for each region. For example, you could configure Config in one region to record all supported resources across all services within that region, and then add a pre-defined AWS-managed config rule that will check if EBS volumes are encrypted. In another region, you could select to only record a specific type of resource, such as security groups, with no pre-defined rules allocated.</p>
<p>Some of you may be wondering, what if the service you want to monitor is not region specific, such as IAM? Well in this case, there is a separate option to include global services, which IAM falls under.</p>
<p>Now we have an understanding of what AWS Config is used for, and what it does. In the next lecture, we will introduce the different components that make up the service, showing what each of them do and how these come together to deliver the service features.</p>
<h1 id="Key-Components"><a href="#Key-Components" class="headerlink" title="Key Components"></a>Key Components</h1><p>Hello and welcome to this lecture where we will be taking a look at the components that make up the AWS Config Service and the functions that each of them carry in delivering the service.</p>
<p>To understand how to get the most of this service, it’s important to understand how the service is pieced together and how it works. So let’s start by identifying the key components. Following this we will then break each of them down to understand their role and how they sit within the service.</p>
<p>The following identifies the main components to the service. AWS resources, configuration items, configuration streams, configuration history, configuration snapshots, configuration recorder, config rules, resource relationships, SNS topics, S3 buckets and AWS config permissions.</p>
<p>Let’s now take a look at each of these in turn in more detail starting with AWS resources. As discussed in the previous lecture, AWS resources are typically classed as objects that can be created, updated or deleted from within the AWS Management Console, or programmatically using the AWS CLI or supported SDK. AWS Config records changes to supported AWS resources within a specific region. For the current list of supported services and resources, please follow the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-resources">link</a> on the screen.</p>
<p>A configuration item or CI as it’s known, is a key component of AWS Config. It is comprised of a JSON file that holds the configuration information, relationship information and other metadata as a point-in-time snapshot view of a supported resource. All the information that AWS Config can record for a resource is captured within the CI. A CI is created every time a supported resource has a change made to its configuration in any way. In addition to recording the details of the affected resource, AWS Config will also record CIs for any directly related resources to ensure the change did not affect those resources too.</p>
<p>For example, if there was a rule change to a security group, additional rules could be added with new ports. AWS Config would record all CI information for that resource, but it would also gather CI information for any instances that were a part of that security group. The CIs would then be sent to a configuration stream, which we will cover next.</p>
<p>As so much data is gathered within these CIs, it’s important we look at this in further detail. So for every CI generated, they’ll be five different sections.</p>
<p>Firstly, metadata. This essentially contains details about the configuration item itself. So within this metadata, we have both a version ID and a configuration ID, which uniquely identifies the CI. In addition to this, other information can include an MD5Hash that allows you to compare other CIs already recorded against the same resource, as well as ensuring there are no duplications. And then we have the time of the capture and state ID, which puts the CI for a particular resource into an order of sequence, which is useful when multiple CIs for the same resource are sent the configuration stream.</p>
<p>Attributes. This holds common attribute information against the actual resource. Within this section, we also have a unique resource ID, and any key value tags that are associated to the resource. The resource type is also listed. For example, if this was a CI for an EC2 instance, the resource types listed could be the network interface, or the elastic IP address for that EC2 instance. The Amazon resource name, the ARN, for the resource would also be shown along with the availability zone that the resource belonged to. Bear in mind that for services and resources that are not fixed to a particular availability zone, such as IAM and IAM roles, then this section would not be applicable. Lastly, the time the resource was created would also be given.</p>
<p>Relationships. This holds information for any connected relationship that the resource may have. So within this section, it would show a clear description of any relationship to other resources that this resource had. For example, if the CI was for an EC2 instance, the relationship section may show the connection to a VPC along with the subnet that the EC2 instance resides in.</p>
<p>Current configuration. This will display the same information that would be generated if you were to perform a describe or list API call made by the AWS CLI. AWS Config uses the same API calls to get the same information. So depending on the API call and resource, different information will be returned, which is resource specific.</p>
<p>Related events. This relates to AWS CloudTrail. This will display the AWS CloudTrail event ID that is related to the change that triggered the creation of this CI. There is a new CI made for every change made against a resource. As a result, different CloudTrail event IDs will be created. This allows you to deep dive into who, or what, and when made the change that triggered this CI. A great feature allowing for some great analysis to be taken, specifically when this affects security resources.</p>
<p>As you can see, the configuration item is a fundamental aspect of AWS Config when recorded change is made to a supported resource. Because it captures so much data, these CIs are used by other features and components of AWS Config such as configuration history. CIs are used to look up all the changes that have been made to a resource. Configuration streams. CIs are sent to an SNS topic to enable analysis of the data. Configuration Snapshots. CIs are used to create a point-in-time snapshot of all supported resources.</p>
<p>Following CIs, let’s move on to configuration streams, which was briefly mentioned a few minutes ago when talking about the configuration items. When a create, update or delete API call is made against a supported AWS Config resource, as we now know, a new CI is created along with additional CIs for any resources that were related to the original modified resource. These CIs are then sent to a configuration stream and this stream is in the form of an SNS topic. However, this stream is also used by AWS Config to send information when other events occur, such as when the configuration history for a resource was delivered to your account, when a configuration snapshot was started and delivered to your account, when the state of your resource compliance changes against any config rules that have been configured, when evaluation begins for rules against resources, and when AWS Config failed to deliver notifications to your account.</p>
<p>This SNS topic can have different notification endpoints, such as email, or an endpoint more commonly used, is SQS, Simple Queue Service. This allows the information to be accessed programmatically to then analyze the data captured. However, you could, like I mentioned, set up your email notification for all stream items, although in a large corporate environment, this may not be ideal as your email inbox would get full up very quickly. If you do decide to do this, then it’s worth setting up an email filter to ensure the information you’re interested in is made available to you. More information on this configuration can be found on this <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html">link</a> here.</p>
<p>This stream for AWS Config should not be confused with the real-time streams that area available through the AWS Kinesis service, which continually capture huge amounts of data from multiple sources such as clickstreams, transactions, social media feeds, and IT logs, etc.</p>
<p>The configuration history uses configuration items to collate and produce a history of changes to a particular resource. This allows you to see the complete set of changes made to a resource over a set period of time. The information can be accessed either programmatically through the AWS CLI using the following command. You can also specify the resource type. So, for example, if you wanted to look at the configuration history for a subnet, you could enter the following into the AWS CLI. Or you could access the history via the AWS Management Console. Additionally, AWS Config also sends a configuration history file for each resource type to a S3 bucket that is selected during the set up of AWS Config. This configuration file is typically delivered every six hours, and like I say, it contains all CI changes for all resources of a particular type.  For example, there would be one configuration history file covering six hours for all RDS DB instance changes in one region.</p>
<p>The configuration snapshot also uses configuration items in its production. The configuration snapshot will take a point-in-time snapshot of all supported resources configured for that region. It will generate CIs for each resource in your AWS account for a specific region, and this configuration snapshot can then be sent to an S3 bucket. Alternatively, this information can be viewed via the AWS Management Console.</p>
<p>Now let’s take a look at the configuration recorder. So this component of AWS Config can be seen as the engine of the service. This element is responsible for recording all of the changes to the supported resources within your account and generating the configuration items. By default, the configuration recorder is automatically enabled and started when you first configure AWS Config. However, it is something that you can stop and then restart again at a later point. When you stop it, AWS Config will no longer track and record changes to your supported resources.</p>
<p>When you first configure AWS Config in the AWS Management Console, you are asked which resource types you would like to record. This is essentially setting up and creating the configuration recorder, as this information is required to enable the configuration recorder to start. If you only select to record specific resource types, then it will capture all creates, changes, and deletes for those resource types and will create a CI record for each occurrence. However, the configuration recorder will still record all creates and deletes of supported resource types within that region, but no changes to those resources will be recorded. Also, if you stop and change your configuration recorder settings to perhaps remove certain resource types from being recorded, then all captured information for those resources up to that point will remain and you will be able to view their history with all data from the previous CIs that had been captured.</p>
<p>AWS config rules are a great way to help you enforce specific compliance checks and controls across your resources, and allows you to adopt an ideal deployment specification for each of your resource types. Each rule is essentially a lambda function that when called upon evaluates the resource and carries out some simple logic to determine the compliance result with the rule. Each time a change is made to one of your supported resources, AWS Config will check the compliance against any config rules that you have in place. If there was a violation against these rules, then AWS Config will send a message to the configuration stream via SNS and the resource will be marked as non-compliant. It’s important to note that this does not mean the resource will be taken out of service or it will stop working. It will continue to operate exactly as it is with its new configuration.</p>
<p>AWS Config simply alerts you that there is a violation, and it’s up to you to take the appropriate action. These rules can be custom defined, or selected from a predefined list of AWS managed rules that AWS has created on your behalf. Being able to create your own rules allows you to adopt best practices that you may have internally within your own enterprise or with other security best practices. By allowing AWS Config to monitor resources at this level, it adds another level of automation, helping to prevent misconfigurations made by human error being left, which could lead to a security risk, or even worse, a breach. I highly recommend using config rules for maintaining security checks and configurations.</p>
<p>AWS have a number of predefined rules that fall under the security umbrella that are ready to use. For example, Rds-storage-encrypted. This checks whether storage encryption is activated by your RDS database instances. Encrypted-volumes. This checks to see if any EBS volumes that have an attached state are encrypted. Optionally you can specify the ID of a KMS key to use to encrypt the volume. Rootaccount-mfa-enabled. This checks whether your root account of your AWS account requires multifactor authentication for console sign in. IAM-user-no-policy-check. This checks that none of your IAM users have policies attached. Best practice dictates that permissions should be provided via roles or groups.</p>
<p>The great thing about these predefined rules is that you can also edit them to make subtle parameter changes as needed. As you can see, being able to ask AWS Config to check your resource’s compliance with rules such as these are invaluable. And when you couple this with being able to create your own custom rules, the scope and potential of automating compliance is huge across your supported resources.</p>
<p>When you first use AWS Config with a set of rules, whether they are predefined or custom, don’t be surprised by the results that you find. It’s likely that you will see more resources being flagged as non-compliant than you may have imagined. As I have already mentioned, these config rules are great as a security tool to help you mitigate potential security issues that may arise within your environment. These config rules are also a great way to help you meet specific compliance and governance controls for auditing. You can currently use 50 config rules per region. However, if you are finding that you require more, then you can request an increase via AWS.</p>
<p>Coming up later in this course, we shall have a demonstration on how to set up custom config rules where I shall talk more about the different options available.</p>
<p>As a part of the CI for a particular resource, for example, an EBS volume, AWS Config identifies relationships with other resources from that resource. In this case, it might be the EC2 instance that the volume is attached to. This allows AWS Config to build a logical mapping of resources and how they connect. This mapping of relationships allows you to quickly jump to other linked resources within the AWS Management Console to view their configuration history and CI data. This is very useful if you’re trying to troubleshoot an issue and pinpoint where the source of an incident may be. For a full listing of available relationship types, see the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-relationships">link</a> on the screen.</p>
<p>As we have already seen, an SNS topic is used as a configuration stream for notifications of various events triggered by AWS Config. You can have various endpoints associated to the SNS stream. Best practice indicates that you should use SQS and then programmatically analyze the results via SQS. The S3 bucket that was selected at the time of configuration is used to store all the configuration history files that are generated for each resource type, which happens every six hours. Also any configuration snapshots that are taken are also stored within the same S3 bucket. The configuration details used for both SNS and S3 are classed as the AWS Config delivery channel by which data can be sent to other services.</p>
<p>When setting up AWS Config, you’re required to select an IAM role. This role is required to allow AWS Config to obtain the correct permissions to carry out and perform a number of functions. For example, AWS Config will need read-only access to all the supported resources within your account so it can retrieve data for the configuration items. Also, we now know that AWS Config uses SNS and S3 both for streams and storage of the configuration history files and snapshots. So AWS Config requires the relevant permission to allow it to send data to these services.</p>
<p>That now covers the key elements of AWS Config and so I hope it makes it clear as to how each part plays a role within the service. To reiterate, let’s take a bulleted point look at how it all comes together. When you first turn on AWS Config, you will configure the elements required for the configuration recorder to begin capturing and recording data. AWS Config will then discover all your supported resources based upon the details entered within the configuration recorder within that region. When a create, change or delete against a resource is made, a CI will be created for this change and a notification will be sent to the configuration stream regarding the new CI. Also, following the CI, AWS Config will check its current config rules to evaluate if the change has made a resource non-compliant. If the evaluated state changed for the resource, a notification will be sent to the configuration stream. Your config rules can also be configured to run periodically, and so they will run at a set given time period regardless if there have been any changes to resources. If a configuration snapshot is taken, AWS Config will create a point-in-time snapshot of the resources with new CIs, and deliver this configuration to your specified S3 bucket within the configuration recorder. After six hours has passed from turning on AWS Config, a configuration history file will be created for each resource type, and again sent to your specified S3 bucket. The configuration file will be sent every six hours from that point.</p>
<p>Now we have learned the theory behind AWS Config, let me introduce you to the service with a demonstration. In this demonstration, I will run through how to set up AWS Config, pointing out components such as the configuration recorder along the way. We will also look at the AWS Config dashboard showing the timeline of events, and how by using relationships of your resources, you can navigate through your infrastructure. During this demo, I’ll also show you the configuration history file within S3.</p>
<p>Okay, so I’ve just logged into my AWS account, and I’m going go up to Services and you can see that AWS Config is under Management Tools. So if we select Config. And this is the first splash screen you’ll be presented with if you don’t have AWS Config started as yet. So, let’s get started by clicking on the Get Started button, and then there’s three steps to setting AWS Config up.</p>
<p>So the first step is configuring the settings, and as I mentioned earlier in the lecture, we have our resource types to record and this is where we essentially start the configuration recorder by identifying what resources we’d like included in our AWS Config configuration. So, starting from the top, we can either select all resources within this region. Remember AWS Config is region specific, so we can choose to record all the resources supported within this region, and currently I’m in the London region. And we can choose to include global services, like we also discussed, services such as IAM, etc. But for this demonstration, we’ll just choose to record all resources in this region. Here you can select specific types if you require, so if I untick this, we can then go down to this drop-down box and select specific resources types that we’d like set up for this configuration. So you can be quite specific, selecting the load balancer or different elements of IAM, and again, different element of RDS, etc. So, there’s a list there that you can go through if you just want to record a specific resource type, but like I said, for this demonstration, I’m just going to select all resources.</p>
<p>Next we need to define our S3 bucket, and this is where our configuration history and snapshot files are stored. So we can ask AWS Config to just create a new bucket for us, and it’ll prefix it with this bucket name here. Or we can choose an existing bucket from my account by selecting the drop-down list, or we can select a bucket from another account. So if you had multiple accounts, you can send the configuration history files and snapshot files for all those accounts to a single, primary AWS account in a single bucket. So for this example, though, for this demonstration, I’m just going ask AWS Config to create a new bucket for us. Config bucket cloudacademy.</p>
<p>Now I’m moving down to the SNS topic. Now this is the configuration stream, as we discussed, so for any events that AWS Config picks up, it will send it to this SNS topic for the configuration stream. Now we can create a new topic, and again, AWS Config will just give us a topic name here, or we can select to choose another topic from your account or again from another account. For this demonstration, we’ll just ask AWS Config to create the topic name for us. And I’ll just add in cloudacademy.</p>
<p>Now, finally, we have the AWS Config role, and this is where we spoke about the different permissions. And this is required to allow AWS Config to send theconfiguration history file and snapshots to S3 as well as having access to send events to the SNS topic. And not forgetting enabling AWS Config to be able to kind of go out and poll your resources as a describe or list API call to get the details of those resources. And again, for this demonstration, I’m just going to ask AWS Config to create a role for us. I’ll just add cloudacademy on the end.</p>
<p>So this screen is essentially your configuration recorder. By setting this up, you’re asking AWS Config to start recording. We have set up the S3 bucket to allow our configuration history files and our snapshot files to be captured. And we have created our configuration stream by configuring an SNS topic. And we have also allowed AWS Config to have the necessary permissions to carry out those functions. So once that is all set up, we click on Next.</p>
<p>And here we have a set of predefined AWS managed config rules. And, like I said earlier, this is a number of templates that you can select to check the compliance of you resources against these rules. We’ll cover more on AWS Config rules in a later lecture and we’ll also have a demonstration on how to create one and modify one of these existing AWS managed rules. So for now, I’m just going to click on Skip.</p>
<p>And here we’re on the final stage, which is review. So we can see here the resource types we’ve selected, which are all resources, and we’ve not chosen to include global resources. We have our S3 bucket set up for our history files and snapshots. And we have our SNS topic for our configuration stream configured, and we have the permissions given by the AWS Config rule. And that’s essentially the elements to setting up AWS Config. So once you’re happy with that, and once you’re done with that, you can click on Confirm, and you can see that it’s now setting up AWS Config for this region.</p>
<p>And that’s it. It’s set up and it’s running. So whatever create, deletes or changes are made to supported resource types within this region, AWS Config will now begin to monitor that and record it and log it within the configuration stream, and also within the configuration history files.</p>
<p>Now what I’ve done, I’ve already set this up in another region and I’ve made a few changes. So what I’ll do now, I’ll swap to that region and we can take a look at some of the other elements, such as the configuration history file, the timeline of events, etc., and look at how the relationships are set up within this dashboard. So let me swap over to another region, which is Ireland.</p>
<p>So as you can see, I’ve just swapped to the Ireland region and we don’t have the option of setting up AWS Config because I’ve already set it up and you only have it running once within the region. So let’s take a look at what we have here. Straight away we can see that I’ve had existing rules. I had a rule name of s3-Bucket versioning-enabled. So I wanted to check if my S3 buckets had versioning enabled. So this is one of the config rules. So if I just open that up, I can see that a number of these buckets do not have versioning enabled, and they have been marked as non-compliant. Now remember, I can still use these buckets. I can still write to the buckets. I can still delete objects from those buckets. It’s just notified me that these buckets are non-compliant with this specific config rule, which is to check if… Right, it says here, it checks whether versioning is enabled for your S3 buckets.</p>
<p>Now let’s have a look at some of the other resources that we have. So I can filter on a specific resource type that I want to take a look at. So let me take a look at my VPC. Let me look at everything to do with my VPC. So I can see here that I have two VPCs, and let’s take a look at one of them. And this is now taking us to the timeline of events of these VPCs. So on the 15th of March at 11:25, there was a resource discovery. So that’s probably when I activated AWS Config within this region and it went out and done a discovery of all resources. And then at 11:32, there was a change on this resource item. So let’s take a look at the rest of the page.</p>
<p>So under the configuration details, we have a number of details here. We have the Amazon resource name, the resource type, the ID… The CIDR block of the VPC. So, there’s different configuration details that we can see for different items. Now we can also see the tag name here, which is CA-Demo, and further down, we can look at the relationships.</p>
<p>So this will show other resources that have a direct relationship with this VPC. So we can see we have a couple of network ACLs there, we have an internet gateway, we have some root tables, security groups and some subnets. So each of these resource types has a direct relationship with this VPC. And if I wanted to, I can select straight from here to click on that network ACL, and it will take me to the details for that ACL. And again, we have some resource type information here, the ARN and the ID, etc. Again, if we click on the relationships, we should see the VPC that we just come from. So let’s go back to our VPC.</p>
<p>So as you can see, it’s very easy to look at the relationships between different resources, and navigate between them. And it’s kind of grouped in a logical manner to allow you to quickly get between the different resource types that you’d like to. Now we can see up here that on the 15th of March, 11:23, there was a change. Now if we go down to our changes here, we can have a look at what that change was. We can see it’s defined by two different categories, a configuration change or a relationship change. We can see that by the number, that this change was to do with relationships. And it looks as though a new subnet was added to this VPC.</p>
<p>And then finally, we have our CloudTrail events, and this will capture any API calls that made changes to resources. However, it will only capture these for the previous seven days. So, because this change was made on the 15th of March, and it’s now the 30th of March, so if we look at the CloudTrail events, there’s nothing there. They have expired. So what I’ll do, I’ll go and make a couple of changes. I’ll create a new subnet within this VPC. So I’ll pause the video, wait a few minutes and then I’ll start it again and we can analyze the CloudTrail event of that new change.</p>
<p>Okay, so I’ve gone off and I’ve created a new subnet within this VPC. So we can see that the 30th of March, which is today, that there was a change that occurred and two new events. So let’s go ahead and take a look at the change. So we can see that there is a new subnet. This is the subnet that I just created. And if we look at the CloudTrail events, we can see that we have the creation of the new subnet there. Now if we click on the actual CloudTrail event itself, it will take us to CloudTrail and we can look at additional information.</p>
<p>So now we’re looking at the details of the API call itself, and we can see a number of different information here. We can see the user that created it, the source IP address, the time, the event source and also the region as well, along with any affected resources as well. So here we can drill down into exactly what time it occurred. As you can see up here, the date and time, and by who, and what event actually was created. So, as you can see, clicking on that CloudTrail event, you can get additional information to kind of help you with resolving incidents, and looking at potential security breaches to try and gather more information as to identifying who is doing what and when.</p>
<p>So now let’s take a look at the configuration history, which is stored in S3. So let’s go across to S3. So if I go to the bucket that I use for Ireland region, and then navigate through the folders to the correct date and time of today, I can see the configuration history folder. And then if I download one of these files, and then open that. We can see here that that configuration history file contained the subnet creation that we just created for our VPC, and it shows you all the different information about it, the availability zone, the CIDR block used, etc., etc. And there’s the subnet ID. And it also highlights the relationships to other resources for that new subnet such as any network ACLs that may be associated. So that’s the configuration history file that’s stored in S3 in a JSON format.</p>
<p>That brings us to the end of this demo. So we looked at how to set up AWS Config for a region. We then looked at the details of a specific resource type, we looked at the VPC and how the timeline of events work. We looked at a couple of changes, and also the CloudTrail event logs as well. And then finally, we looked at one of the configuration history files as a JSON document. That brings us to the end of this lecture. Coming up next, we will look at how AWS Config integrates with other AWS services in a bit more detail.</p>
<h1 id="Service-Integration"><a href="#Service-Integration" class="headerlink" title="Service Integration"></a>Service Integration</h1><p>Hello and welcome to this short lecture on AWS Config Service Integration where we shall look at the relationships between AWS Config and other AWS services.</p>
<p>AWS Config has a specific relationship with the following AWS services, SNS, SQS, S3, CloudTrail and IAM. Let’s start by looking at SNS.</p>
<p>We have already covered much of this in the previous lecture where I explained how SNS is used as the configuration stream for CIs and other important event notifications. By using SNS you can subscribe multiple different endpoints to the SNS topic created as a part of your configuration recorder information to extract data and process information. And this is where SQS comes in. If you had multiple accounts, you may want to have AWS Config in each account subscribed to the same topic in a primary AWS account. This is possible by allowing access of the service principle to publish to the same topic in the primary account. See the ‘permissions for the Amazon SNS topic’ within the following AWS developer guide for a sample policy on how to do this.</p>
<p>The Simple Queue Service, SQS, can be subscribed to the AWS Config topic, the configuration stream, which gives you a highly available and decoupled environment for the data within your configuration streams. By using SQS it allows you to create and use your own applications to extract only information and data that is pertinent to you. There can be vast amount of data coming into the configuration stream but you might only want to be notified and made aware of any changes that may relate to any potential security issues. As a result, you may want to pull information from the queue that only relate to security groups, NACLS, IAM roles etc. or any other resource type that could affect the security of your environment.</p>
<p>If you did decide to have different configuration streams in each region, so effectively different SNS topics, then you could still subscribe the same SQS queue to multiple SNS topics preventing your application from poling from multiple queues to process data from the configuration stream. S3 is used to store the configuration history files and any configuration snapshots of your data within a single bucket. And again, this bucket is defined within the configuration recorder. You can get AWS Config to create a new bucket for you or select an existing bucket. If you have multiple AWS accounts you may want to aggregate your configuration history and snapshot files into the same S3 bucket for your primary account. However, you will need to grant the right access for the service principle which is config.amazonaws.com to be able to write to the S3 bucket. Take a look at the section ‘Granting AWS Config Access to an Amazon S3 Bucket in Another Account’ within the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/config-dg.pdf">link</a> for a sample policy on how to do this.</p>
<p>AWS CloudTrail interacts with AWS Config at the configuration item level. If we remember back to the section in the previous lecture the CI is comprised of five different sections. The final section, Related Events, displays the AWS CloudTrail event ID that is related to the change that triggered the creation of the CI for that resource. This feature is very useful when identifying who or what made the change to the effective resource. This CloudTrail data can be accessed via the AWS Config Dashboard within the AWS Management console, which will then link you directly to the event within CloudTrail. For more information on CloudTrail we have a course AWS CloudTrail, An Introduction that will define exactly what the services and how it works.</p>
<p>Conversely, when CloudTrail tracks and recalls changes made within the AWS Config itself, the following APIs are tracked, DeleteDeliveryChannel. This deletes the delivery channel. DeliverConfigSnapshot. This sends a configuration snapshot to S3. DescribeConfigurationRecorderStatus. This returns the status of a specified configuration recorder. DescribeConfigurationRecorders. This returns the details of a specific configuration recorder. DescribeDeliveryChannels. This returns information about a specific delivery channel. GetResourceConfigHistory. This retrieves a list of configuration items for a specified resource. PutConfigurationRecorder. This creates a new configuration record. PutDeliveryChannel. This creates a new delivery channel for an S3 bucket and SNS topic. StartConfigurationRecorder. This starts recording data for supported resources within your account as per your configuration. And finally, StopConfigurationRecorder. And this stops recording the data.</p>
<p>The final service that has a relationship with AWS Config is IAM. And again, we briefly covered this in the previous lecture. As AWS Config has relationships with other services, specifically SNS and S3, the use of an IAM role is required to enable the service to publish data to an SNS topic for configuration streams and S3 to store configuration history files and configuration snapshots. The policy for this access would look similar to the following on screen. In addition to this access, AWS Config must also be able to perform the described list and some get API calls against all supported services within the region. As a result, the same IAM role also has a second policy attached which allows access to perform these actions against those resources.</p>
<p>That brings us to the end of this lecture of how other AWS services interact with AWS Config. Coming up next we’ll start to look at how to manage specific compliance with AWS Config. We briefly touched on this earlier when we looked at the config rules, so we’ll now look at this in greater depth.</p>
<h1 id="Managing-Compliance-with-AWS-Config"><a href="#Managing-Compliance-with-AWS-Config" class="headerlink" title="Managing Compliance with AWS Config"></a>Managing Compliance with AWS Config</h1><p>Hello and welcome to this lecture where we’ll begin to look at how to best manage your compliance that you need to adhere to within your AWS environment.</p>
<p>Your compliance requirements can come from many different sources, for example, you may have a requirement for your environment to be HIPAA compliant if you’re managing healthcare records. Or perhaps your security theme requires certain criteria to be adhered to such as ensuring SSH is not applied to a specific security groups. Or your operations team may dictate to you that certain EC2 types should only be used for certain environments to conform to internal standards. Maintaining compliance and internal standards for numerous parties, both internal and external, can be difficult to manage and can lead to mistakes, which in turn will eventually lead to non-compliance services making their way into production. This can be a huge risk from an audit perspective and also from a security stance.</p>
<p>AWS Config allows you to utilize config rules to help you manage and organizes compliance which acts as an automatic resource compliance checker. When a change is made to a resource, AWS Config will check to see if the resource matches the rule with the help of a lambda function and if so, it will check the compliance of that resource against the rule following the changes made.</p>
<p>There are two different types of Config Rules within AWS Config, custom config rules and AWS Managed Config Rules. AWS Managed Rules are a set of predefined rules that cover a lot of best practices, so it’s always worth browsing these rules first before setting up your own as there is a chance that the rule may already exist. At the time of writing this course these Managed Rules cover five different topic areas, compute, database, management tools, security, identity and compliance, and storage. Do bear in mind that you can edit specific parameters of Managed Rules within in these topics, and so if there is a rule that very closely matches your requirements, then you may be able to edit it by selecting different triggers and parameters to reflect the changes that you need. This can save you a lot of time trying to recreate your own from scratch. Later in this lecture I will provide a demonstration of how to modify these managed rules, along with how to create your own custom rules.</p>
<p>Before creating, modifying and configuring your Config Rules you should first identify what compliance and standards that you need to adhere to. Define the requirements from all parties but remember, there is a limit of 50 config rules per region before you need to contact AWS to request an increase.</p>
<p>Let’s consider a simple scenario on how these Config Rules could be used. Let’s say we are security architects responsible for ensuring specific security requirements are met in the following deployment. A new web application is being released into the production environment that consists of multi-tiered infrastructure, with front-end web servers in a public subnet, application servers with EBS and a back-end RDS database. The application and the environment itself is expected to scale with demand, both up and down.</p>
<p>We have been told that from a security perspective SSH must not be used for the web server security group under any circumstance. Where used, all EBS volumes must be encrypted, the RDS database must be configured for high availability at all times and data stored by RDS must be encrypted. Now obviously, during deployment we can oversee the implementation and ensure that each of these elements are met. We can make sure the security groups are correctly configured, storage encryption is activated and that multi AZ is configured for high availability for the RDS instance. So, why the need for these config rules?</p>
<p>Well, now let’s consider the following. Once the initial deployment was carried out and security checks were carried out manually, the environment was then handed over to support and operations to maintain and look after. Over time the application scaled up, additional resources were added and there may have been some ad hoc incidence and general maintenance of the environment which was carried out. The support and operations team, although they were made aware of the security requirements, they may not have adhered to them at all times, perhaps due to human error, lack of knowledge or laziness. Your environment now has a situation where some volumes are not encrypted and SSH has been activated on the web server security group perhaps to troubleshoot an existing incident.</p>
<p>As you can see, over time your environment changes and maintaining the same level of security implemented at the start of the project may not be continuous throughout its lifetime, which leads to mistakes and security holes within your infrastructure. AWS Config Rules can notify you of these security misconfigurations. If for example, in this situation during deployment we activated the followed AWS Managed Rules, restricted-SSH, which would check whether security groups would disallow incoming SSH traffic, encrypted-volumes, this would check whether EBS volumes are encrypted, RDS-multi-az-support checks to ensure high availability is configured for your RDS instance, and rds-storage-encrypted, and this checks if storage encryption is enabled for your RDS instances. With these config rules in place, a notification would have been sent to the configuration stream stating that a resource had changed its state from being compliant to non-compliant.</p>
<p>These notifications could have been configured programmatically to notify your security team, who could have then investigated as to why this had happened, and understand who made the change through the use of the configuration item by utilizing the CloudTrail information recorded when the change happened to the resource. By using Config Rules it allows the appropriate action to take place when a notification is received of a non-compliant resource, which may include resolving the security issue identified and ensuring the resource becomes compliant again. Remember, just because a resource is flagged as non-compliant, does not mean it is removed from your environment. It will continue to function.</p>
<p>Identify who or what made the change. If an employee made the change advise and educate them on the importance of security and why that resource requires set configurations. If it was a service, check your automation configurations to ensure it doesn’t happen again. So, as you can see using AWS Config Rules allows a continuous monitoring solution for a wide range of uses to ensure compliance is maintained within your environment.</p>
<p>The example we have used was a simple security scenario, but the scope is huge, especially when implementing custom config rules. Before we finish this lecture, I just want to provide a quick demo to show you how to select and modify an AWS Managed Rule and also how to configure a custom rule where I will show you the different elements involved in the process.</p>
<p>Okay, so I’m at the AWS Config dashboard and at the Ruled page. So firstly, what we’re going to take a look at is how to modify the parameters of an AWS Managed Rule.</p>
<p>So, if we go to Add Rule, and for this demonstrations I’m going to use the Desired Instance Type Managed Rule, so this checks whether your EC2 instances of the specified instance type.</p>
<p>So, we have a name at the top which we can edit if we need to. The description of the rule, it checks whether your EC2 instances are of a particular type. Here’s the managed rule name which is the lambda function and here we have the triggers. Now, there’s two different trigger types here. When there’s changes to configurations or you can have periodic triggers, so at a set period of time. So, we got it as configuration changes for this managed rule.</p>
<p>Now look at the scope of changes. This identifies what resources are in scope for this rule. At the minute it’s selected Resources and under Resources we have EC2 instances. We can also select tags and then add our own tag key pairs in there, so whatever we enter there the managed rule will only apply to any resources that have those tags within them or all changes. But I’m going to leave it as Resources, so we want this rule to run against any EC2 instances, and then triggered when there’s any configuration changes.</p>
<p>Now, down here we can edit the parameters. So, we want to specify which instant types that we want to have running within our environment. Now, for our environment we want to make sure we’re just running T2.small instances or M4.large. So, just take a look at this AWS Managed Rule again. We have the name, the description, the actual lambda function itself, which is the evaluation of the rule, we have the trigger type and we’ve left it as configuration changes, and we have the scope of change, we’ve selected a resource, so we want this to be based on all EC2 instances, rather than specific tags. And then for the instance type we’ve edited and added values of T2.small and M4.large. Now, depending what kind of AWS Managed Rule you have, there’ll be different configuration parameters that you can change. I just wanted to highlight here that you can add your own parameters for specific AWS Managed Rules, and in this instance we’ve added our EC2 instance types here. Then once you’ve done that, you just click on save and then AWS Config will go off and evaluate your current environment. And then after a period of time, that evaluation will come back and give you your compliance results.So, what I’ll do, I’ll pause the video here, wait for that to finish and then show you the results.</p>
<p>Okay, so that evaluation has now been completed and we can see that under our new AWS Managed Rule, the desired instance type we have two non-compliant resources. So, let’s take a look. And here we have our two EC2 instances, both coming back with non-compliance checks. Now, we can take a look at these instances. If we follow around inside here you can see Managed Resource and this will give us a link to each instance. So, let’s take a look. So, we can see here that this instance type is actually a T2.nano, which is non-compliant with our T2 small and M4 large that we specified in the rule.</p>
<p>So, if we go back to Config and take a look at the second instance, and we can see here that this is a T2.micro, so again, non-compliant. So, that’s the end of this demo on showing you how to edit the parameters of a AWS Managed Config Rule.</p>
<p>We’ll now take a quick look at how to create your own config rule, which is very, very similar.</p>
<p>Okay, so to create your own config rule is very similar steps. Again, click on Add Rule and instead of selecting one of the predefined AWS Managed Rules as before, all we do is click on Add Custom Rule, and here you add a number of details. So, you can add your own name, so Cloud Academy Rule, description, this is a test for a demo. Here is where you’ll enter the ARN of the lambda function that you’ve created. That will then evaluate your resources to see if they comply with this rule. Now, I’m not going to go and create a new lambda function because it’s out of scope of this course, but for those who are experienced with lambda, then this where you would enter your ARN function.</p>
<p>Now, further down this is where we add our trigger types like we just discussed with the AWS Managed Rules. You can have the trigger to happen every time there’s a configuration change, or over a specific period. So, with regards to the period, you can select 1 hour, 3 hours, 6 hours, 12 or 24. So, this rule will run at that set period regardless of if there’s any configuration changes, but you can have both ticked regardless.</p>
<p>Now going down to the scope of changes, and again as we mentioned within the AWS Managed Rules, it can be against your resources, particular tags or for both. So, if we select Resources we can specify our resource type, for example, VPCs, subnets, security groups, so you can have multiple resource types applying to the same rule.</p>
<p>And then again, down here you can have your rule parameters. Here you would define any values that your lambda function would need to evaluate for this rule. And once you’ve entered all your information, then all you’d need to do is then just to click on save and then that will go ahead and create your custom rule with your own lambda function and again, it’ll follow the same process of evaluation and then return any non-compliant results.</p>
<p>And that brings us to the end of this demonstration. For a full list of current AWS Managed Rules that we spoke about within this lecture, then please visit this <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html">link</a>. That brings us to the end of this lecture. Coming up next we’re going to look at some different use cases for AWS Config.</p>
<h1 id="AWS-Config-Use-Cases"><a href="#AWS-Config-Use-Cases" class="headerlink" title="AWS Config Use Cases"></a>AWS Config Use Cases</h1><p>Hello, and welcome to this lecture on AWS Config Use Cases. We will look at some of the common scenarios of where and why you would want to use this service.</p>
<p>In an earlier lecture, we looked at some of the scenarios we are faced with, when looking at resource asset and change management and how hard it can be to have deep visibility of your infrastructure. Following this, there are a few key use cases, for when using AWS Config is ideal within your environment. Let’s take a quick look at each.</p>
<p>Security Compliance. As we learned in the previous lecture, AWS Config can be a great tool, when enforcing strict compliance against specific security controls. Being notified of noncompliant resource configurations from a security stance is critical, especially in highly sensitive environments, where these controls are imperative to protect both internal corporate and external customer data. Through the use of config rules, you can have the service continually monitor and check your resources remain compliant throughout its life cycle.</p>
<p>Discovery of Resources. When you first activate AWS Config, or run the configuration recorder, AWS Config will discover all supported resources types, allowing you to view them from within the AWS Config dashboard. A configuration item will be recorded for each and so these resources could also be found in the configuration history file on S3. Being aware of all the resources you have is key to understanding your environment. You may find that you have EBS volumes out there, that are no longer attached to instances, which you could then take a snapshot of to keep the data and then delete the volumes, saving you money or perhaps you have subnets configured, that no longer have any instances in, that you no longer need and so it allows you to perform some essential housekeeping within your network and VPC. There are many benefits to knowing what you have, where it is and what it’s connected to. Many of these benefits will end up saving you money and help you run a streamlined environment.</p>
<p>Audit Compliance. As well as using AWS Config for being compliant for internal security standards, there are also many external audit and governance controls, where the service can also enforce specific controls on resources to maintain compliance. For example, the Health Insurance Portability and Accountability Act, known as HiPAA and Payment Card Industry Data Security Standard, known as PCI DSS. These programs require strict controls in many different areas. Being able to set custom and manage configurals in place help adhere to these external governance controls. In addition to this, you could show the auditors all of your configuration history files, which will allow them to go back to any point in time to check the configuration of any of your supported resources. Having this kind of information to hand is essential from an audit compliance point of view.</p>
<p>Resource Change Management. When planning changes within your infrastructure, it’s often required that you have an understanding of what affect the change will have on other resources. More often than not, this information is not always known, as you may not have full visibility of other attached resources. With AWS Config, you are able to use the dashboard to list all related resources of a particular resource, thanks to the relationship section within the configuration item. This allows you to plan your changes more effectively, by ensuring all resources that have a relationship to the source being changed, continue to function as expected post-changes. This helps to prevent outages and configurational mistakes being made by having an overall better visual awareness of the environment.</p>
<p>Troubleshooting and Problem Management. AWS Config is a great tool to help you troubleshoot issues, that may arise within your environment. Using the config dashboard within the AWS management console, you can see a timeline of events allowing you to go back to any point in time and in the case of an in instant, you’ll be able to go back to just before it happened. By doing this, you can understand what changes happened on your supported resources. If there were changes made to a resource, that was affected by an incident, then this can significantly help you reduce the time to resolution, by identifying the possible cause of the problem. You would also be able to see the changes made to the resource and make any amendments to resolve the issue, not forgetting thanks to its incorporation with AWS CloudTrail, you can see who or what triggered the change, via which API call. If similar events occur frequently, then AWS Config can become a great tool to help you spot potential, underlying problems within your infrastructure, allowing you to find the root cause and manage them effectively.</p>
<p>You might want to look at some Real World Use Cases of other AWS customers. If so, then take a look at their customer success stories found here. That brings us to the end of this lecture. In the next lecture, we will summarize what we have learned throughout this course.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello and welcome to this short lecture where we will briefly look at a high level what we have learnt throughout this course.</p>
<p>In summary, we have identified that AWS Config is a service within the management tools category that can perform a number of useful function when it comes to resource configuration visibility and compliance, such as capturing resource changes, acting as a resource inventory, storing configuration history for individual resources, providing a snapshot in time of current resource configurations, enabling notifications of when a change has occurred on a resource, providing information on who made the change and when through the use of AWS CloudTrail integration, enforcing rules that check the compliancy of your resources against specific controls, allows you to perform security analysis within your AWS environment, and it provides relationship connectivity information between resources.</p>
<p>We also discussed that AWS Config only supports a number of different services and resource types, which can be found here.</p>
<p>Another important point is that AWS Config is configured on a region-by-region basis. As such you can have different resources being monitored and recorded in each region as you’ll have a different configuration recorder.</p>
<p>We also looked at how the service itself was constructed with regards to its different components, which were identified as follows, AWS resources, the resources in which you want to monitor and record. Configuration items, a record that contains information about the resource including specific configuration details. Configuration streams, an SNS topic that can be accessed programmatically to extract data. Configuration history. This allows you to view configuration information on a particular resource over a period of time through the timeline within the management console or via the configuration history file held on S3. Configuration snapshot, a complete point-in-time snapshot of all your supported resources, including all configuration information. Configuration recorder, the configuration used by AWS Config to determine what resources to record and to which SNS topic and S3 bucket data should be sent. Config rules, rules that allow AWS Config to check resources compliance against the rule set. Any non-compliant rules are identified and a notification is sent to the stream. Resource relationships. This allows you to clearly identify which resources link to other resources. SNS topic. This is used as the configuration stream. S3 bucket. This is used to store configuration snapshots and configuration history files. And AWS Config permissions. The use of an IAM role is required to perform, describe and list API calls to supported resources, along with right access to your selected SNS topic and S3 bucket.</p>
<p>Next we looked at how AWS Config is integrated with other AWS services, such as SNS, where a topic is used as a configuration stream. SQS. This is an ideal service to allow you to programmatically extract useful information from the configuration stream by using SQS as an endpoint within the SNS topic. S3. This is required to allow you to store configuration snapshots, along with your configuration history files which are stored there every six hours for each resource type. CloudTrail. This is used as a part of the configuration item to allow you to track the API which created the change on the resource. IAM. This used to create the role that allows AWS Config to perform its functions as already discussed.</p>
<p>We then looked at how to best manage compliance within your environment. Using security as an example, we explained that by using AWS Config you can implement specific config rules that monitor all changes within your environment notifying you when a resource becomes non-compliant. This allows you to rectify configuration mistakes ensuring that your AWS environment is not unnecessarily exposed to weak security configurational changes.</p>
<p>Finally, we looked at a number of different use cases of where AWS Config can be used to help support, maintain and optimize your AWS resources. These included security compliance, discovery of resources, audit compliance, resource change management, and troubleshooting and problem management.</p>
<p>That now brings us to the end of this lecture and to the end of the course. I hope it has given you a good understanding of the AWS Config service and has left you confident enough to start using this service as you need within your own organization.</p>
<p>If you have any feedback on this course, positive or negative, please do leave a comment on the course landing page. We do look at these comments and your feedback is greatly appreciated.</p>
<p>Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="3Key-Components"><a href="#3Key-Components" class="headerlink" title="3Key Components"></a>3<strong>Key Components</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-resources">Supported Services and Resources</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html">Email Filter</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-relationships">Available Relationship Types</a></p>
<h1 id="4Service-Integration"><a href="#4Service-Integration" class="headerlink" title="4Service Integration"></a>4<strong>Service Integration</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/config-dg.pdf">Sample Policy</a></p>
<h1 id="5Managing-Compliance-with-AWS-Config"><a href="#5Managing-Compliance-with-AWS-Config" class="headerlink" title="5Managing Compliance with AWS Config"></a>5<strong>Managing Compliance with AWS Config</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html">AWS Managed Rules</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:05" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:05-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:01:20" itemprop="dateModified" datetime="2022-11-19T23:01:20-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-AWS-CloudTrail-An-Introduction-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-AWS-CloudTrail-An-Introduction-14/" class="post-title-link" itemprop="url">AWS-Security-Specialty-AWS-CloudTrail:-An-Introduction-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:04" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:04-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:28" itemprop="dateModified" datetime="2022-11-19T22:53:28-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-AWS-CloudTrail-An-Introduction-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-AWS-CloudTrail-An-Introduction-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="AWS-CloudTrail-An-Introduction"><a href="#AWS-CloudTrail-An-Introduction" class="headerlink" title="AWS CloudTrail: An Introduction"></a>AWS CloudTrail: An Introduction</h1><p>Hello, and welcome to this course covering AWS CloudTrail.</p>
<p>CloudTrail is one of the services that falls under the Management Tools categorization within the AWS console. Throughout this course, I shall explain what the service is, what it does, and how it operates, along with its interaction with other AWS services.</p>
<p>AWS CloudTrail is a powerful service that is used to track, audit, and monitor all API requests made in your AWS account, making it an effective security analysis tool. And so it’s worth understanding exactly what it is and what it can do.</p>
<p>Before we start, I’d like to introduce myself. My name is Stuart Scott. I am one of the trainers here at CloudAcademy specializing in AWS, Amazon Web Services. Feel free to contact me with any questions using the details shown on screen. Alternatively, you can always get in touch with us here at CloudAcademy using the community forum where one of cloud experts will reply to your question.</p>
<p>This course has been designed for an audience who have an active roll in managing AWS security, such as a security consultant, security architect, security auditor, etc. Also, if you have a general interest in security or perhaps you are studying for an AWS certification that requires knowledge of AWS CloudTrail, then this course will certainly be of benefit to you as well.</p>
<p>In this course, I will cover a range of topics, including what is AWS CloudTrail? In this lecture, I will explain what CloudTrail is and does and give examples of how the service can be used for a number of different use cases. How does CloudTrail work? In this section, I’ll talk about CloudTrail and its components and elements, and we’ll discuss how would they all link together to create the service. Understanding CloudTrail permissions. In this lecture, we’ll talk about permissions for both read and write access, and also we’ll touch on some IIM policies and S3 bucket policies here as well. Understanding trails. In this section, we’ll define what a trail is, and we’ll go into the configuration components, and I’ll give a demonstration of how to create a trail here too. Insight into CloudTrail logs. Logs are a huge part of CloudTrail. It’s the output of the service itself. So we’ll dive into what logs are, and what you can do with them and how to share logs within your own account and across other accounts as well. And then finally, we’ll look at monitoring with CloudTrail. Here, we’ll look at how CloudTrail interacts with AWS CloudWatch and how to set up monitoring for specific API calls, etc.</p>
<p>As a student of this course, you will have a full understanding of the AWS CloudTrail service and how it interacts with other AWS services, allowing you to implement CloudTrail effectively, ensuring it fulfills your business requirements. You will have the knowledge to confidently configure Trails for your AWS account, whilst at the same time applying the correct level of encryption and access control against your sensitive log files. In addition to this, you will be able to combine CloudTrail with CloudWatch to implement a monitoring solution for your API calls if required.</p>
<p>Pre-requisites for this course include a basic understanding of the following AWS services: Simple Storage Service, so S3; Identity and Access Management, specifically around policies; AWS CloudWatch; Simple Notification Service, SNS, and the Key Management Service, KMS.</p>
<p>Your feedback on CloudAcademy courses are valuable to us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could use the comment section found on the landing page of this course.</p>
<p>That brings us to the end of this first lecture. Coming up next, I will introduce you to AWS CloudTrail with an explanation of what it is and what it can do.</p>
<h1 id="What-is-AWS-CloudTrail"><a href="#What-is-AWS-CloudTrail" class="headerlink" title="What is AWS CloudTrail?"></a>What is AWS CloudTrail?</h1><p><strong>Resourced referenced:</strong></p>
<p>AWS Whitepaper: <a target="_blank" rel="noopener" href="https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf">https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf</a></p>
<p><strong>Transcript:</strong></p>
<p>Hello and welcome to this lecture. In this lecture, I will explain the basic <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">fundamentals of AWS CloudTrail</a> to give you an overview of the service before we look deeper at the inner workings revealing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/how-does-aws-cloudtrail-work-1/">how the different elements work together</a>.</p>
<p>So what is CloudTrail and what does it do? CloudTrail is a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS command line interface, from within the AWS management console or even from a request made by another AWS service.</p>
<p>For example, when auto scaling automatically sends an API request to launch or terminate an instance. These API requests are all recorded by CloudTrail.</p>
<p>When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the log file. CloudTrail also records and associates other identifying metadata with all the events. For example, the identity of the caller, the time stamp of when the request was initiated and the source IP address. In a later lecture entitled Insight Into CloudTrail Logs I will look at these log files deeper, where I will provide an example of a log file showing the different attributes recorded.</p>
<p>For greater management, new log files are typically created every five minutes which are then delivered and stored within an S3 bucket that is defined by you during your CloudTrail configuration. This allows you to easily go back and review the history of all API requests made. There is also an option to have these logs delivered to a CloudWatch Logs log file as well. Having this association with CloudWatch enables custom metrics <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/monitoring-with-cloudtrail-1/">to be configured to monitor specific API requests</a>. Thresholds can be set against these metrics and when crossed, the simple notification service SNS can be triggered to notify your security teams to investigate. That, at a very high level, is the overall function of the AWS CloudTrail service.</p>
<p>Now let’s take a look at the CloudTrail architecture to understand where it can be implemented from an AWS region standpoint and which services can be supported. AWS CloudTrail is a global service with support for all regions. Support for the latest region EU-London was added in mid-December 2016. In addition to this worldwide coverage, CloudTrail also provides support for over 60 AWS services and features across a wide range of service categories. As you can imagine, with this extensive coverage CloudTrail can capture a vast amount of data if you have a multi-region, multi-service infrastructure environment deployed.</p>
<p>So armed with this information, what can you do with it? How can you use this data to help you manage and support your AWS infrastructure? Well, there are a number of ways you can use the data captured by CloudTrail to help you enhance your AWS environment. Firstly, and as mentioned earlier, it can be used very effectively as a security analysis tool. CloudTrail events provide very specific information about where an API call originated from and who or what initiated the request. As a result, if malicious activity was detected via irregular trends or restricted API call thresholds with the use of CloudWatch then a number of security controls can be quickly implemented to prevent the user from causing additional damage.</p>
<p>Another common use for CloudTrail is to help resolve and manage day-to-day operational issues and problems. Using built-in filtering mechanisms, it’s possible to quickly find who, what, and when a particular API was used which could’ve potentially caused an outage or service interruption. This enables quicker root cause identification resulting in a speedy resolution. Appropriate actions could then be taken to ensure the incident does not reoccur in your environment.</p>
<p>As API calls to add, modify, or delete resources are captured, CloudTrail can be an effective method of tracking changes to resources within your environment. There is another AWS service that is specifically designed to order and track changes to resources which is called AWS Config which CloudTrail interacts with. However, CloudTrail can be used to capture the actual API request and all associated data which made the change. And if you are not using AWS Config, then this at least provides some base level of monitoring and tracking.</p>
<p>From a governance and security legislation perspective, many certifications require the ability to recall and provide evidence of log files relating to specific changes to resources. CloudTrail provides all of this by default through the use of capturing events and writing them to a log file which is then stored on S3. AWS has a great white paper on achieving compliance using CloudTrail entitled Logging in AWS How AWS CloudTrail can help you achieve compliance by logging API calls and changes to resources. The following <a target="_blank" rel="noopener" href="https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf">URL</a> will take you to that white paper. If you need to be able to capture and track API requests within your AWS account for any of these reasons mentioned or perhaps for other reasons you may have of your own, then CloudTrail can do this for you and deliver the output as a log file into an S3 bucket of your choice.</p>
<p>That brings us to the end of this lecture. Next, we look at how CloudTrail is formulated and how the various components and elements work together.</p>
<h1 id="How-does-AWS-CloudTrail-work"><a href="#How-does-AWS-CloudTrail-work" class="headerlink" title="How does AWS CloudTrail work?"></a>How does AWS CloudTrail work?</h1><p>Hello and welcome to this lecture where I shall discuss the different features and components of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">CloudTrail</a> and how they work together to provide a customizable API call-tracking and monitoring solution.</p>
<p>Firstly, let’s look under the hood of CloudTrail to see what makes up the core features and components that create the service.</p>
<p>Trails. These are the building blocks of the service. You can create <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/understanding-trails-1/">many different trails</a> containing different configurations relating to API requests that you want to capture.</p>
<p>S3. S3 is used by default to store the CloudTrail log files and a dedicated S3 bucket is required during the creation of a new trail.</p>
<p>Logs. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">Logs are created by AWS CloudTrail</a> and record all events captured. A new log file is created approximately every five minutes and once processed, it is delivered to an S3 bucket as defined by its trail configuration. If no API calls have been made, then no logs will be delivered.</p>
<p>KMS. The use of AWS KMS is an optional element of CloudTrail, but it allows additional encryption to be added to your log files when stored on S3.</p>
<p>SNS. SNS is also an optional component for CloudTrail, but it allows for you to create notifications. For example, when a new log file is delivered to S3, SNS can notify someone or a team via an email. Or it could be used in conjunction with CloudWatch when metric thresholds have been reached.</p>
<p>CloudWatch Logs. Again, this is another optional component. But AWS CloudTrail allows you to deliver its logs to AWS CloudWatch logs as well as S3 for specific monitoring metrics to take place.</p>
<p>Event Selectors. Event selectors allow you to add a level of customization to the type of API requests you want the corresponding trail to capture.</p>
<p>Tags. Tags allow you to assign your own metadata to your trail. For example, you could add a project or department tag indicating which project or department the trail relates to.</p>
<p>Events. For every API request that is captured by CloudTrail it is recorded as an event in a CloudTrail log file.</p>
<p>API Activity Filters. These are search filters that can be applied against your API activity history in the management console for create, modify and delete API calls. These events are held in the management console for seven days, even if the trail itself is stopped or deleted.</p>
<p>Okay, so I’ve now covered the different components that essentially build CloudTrail. Let me introduce you to the process at a high level of how all of this fits together and in what order. In the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/understanding-cloudtrail-permissions-1/">coming lectures</a>, I will explain in greater detail each of the configurable elements.</p>
<p>By default, CloudTrail is not enabled within your AWS account, so the very first step is to create a trail. If no trail exists, then CloudTrail does not know what API calls to capture from which region and which services, global or otherwise. During this trail creation, you will need to specify an S3 bucket for your log files to be delivered to. This can be an existing bucket or a new bucket which can be created at this stage. You will then have the option to encrypt your log files with the Key Management Service, KMS, if required. Also if required, you can configure the Simple Notification Service, SNS, to notify you when new log files are delivered. If you want to add another layer of security integrity, then you can enable Log File Validation, which will ensure your logs have not been modified or tampered with since their delivery to S3.</p>
<p>Your trail is now ready to be turned on and created. Once it has been created, you are able to make further configurational changes that are not available during the trail creation itself. Upon selection of your new trail, you will have the option to configure CloudWatch logs, which allows you to deliver your CloudTrail log files to CloudWatch in addition to S3. This allows you to creat CloudWatch monitoring metrics against specific API calls and will receive notification from SNS when custom thresholds are reached. Another optional configurable element is that of CloudTrail Event Selectors. This allows you to specify the types of events, management or data that CloudTrail logs. Finally, at the last element of your newly created trail configuration, there is the ability to add tags just as you would with other resources within AWS. At this point your trail is configured and actively recording API calls as per your configuration. For every API call that matches the requirement of your trail, it will be captured and recorded in a log file as an event. Each API call will be recorded as a new event.</p>
<p>Once you have captured the data, you may need to find a particular event quickly, maybe for security reasons. This can be achieved using API Activity Filters which can be found within the CloudTrail service from the management console. So from a high level perspective, we know how a trail is configured.</p>
<p>But what happens when an API matching a trail is called upon by user or service? Let’s take a quick look. A user or service calls upon an API. Next, CloudTrail checks to see if this API call matches any configured trails. If a match is found, CloudTrail recalls the API as an event within its current log file. It also associates other identifiable metadata mentioned earlier. Eventually, the event with the log file will be delivered to S3 and possibly CloudWatch Logs depending on the trail configuration. If it is sent to CloudWatch Logs, the log file will be <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/monitoring-with-cloudtrail-1/">monitored by any configured metrics</a>. When in S3, the log file will be stored and the default server site encryption, SSE, unless KMS has been configured for increased security measures with the associated trail. If any S3 life cycle rules are applied to the bucket, then over time the log file may be archived to a different storage class or even glacier.</p>
<p>That essentially concludes the elements of CloudTrail and how they work together. Coming up in the following lectures we will dive deeper into these configuration components, providing you with a greater understanding of exactly what can be achieved at a more granular level.</p>
<p>That brings us to the end of this lecture. Coming up next, I will explain and cover the various permissions that need to be in place for CloudTrail to be effective.</p>
<h1 id="Understanding-AWS-CloudTrail-Permissions"><a href="#Understanding-AWS-CloudTrail-Permissions" class="headerlink" title="Understanding AWS CloudTrail Permissions"></a>Understanding AWS CloudTrail Permissions</h1><p>Hello and welcome to this short lecture on permissions. I’m going to talk about permissions that are required to set up and create trails and those required by S3 for CloudTrail login, where we will also take a look at Bucket policies.</p>
<p>As with all <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">resources within AWS</a>, to be able to use a service or feature you need to have the proper permissions and authorization. If you do not have the permissions to create trails on CloudTrail, or to configure it as you need, then you’ll need to speak to your AWS administrator or security team. If you are the administrator and you want to grant access to others, then you could create your own IAM policies to cover this access. Or use an existing AWS Managed policy. For more information on IAM policies you can see my previous course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-authentication-authorization-accounting/introduction-112/">Understanding of AWS Authentication, Authorization and Accounting</a>. Or <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-iam-policy/">my blog post here on IAM policies</a>. Currently AWS has two Managed policies in IAM, relating to CloudTrail: AWSCloudTrailFullAccess and AWSCloudTrailReadOnlyAccess. These are fairly self explanatory as to the level of access provided here. If someone needs to create or delete trails then they will obviously need the AWSCloudTrailFullAccess policy, associated to the user account.</p>
<p>I just want to quickly point out that if the existing Managed policies don’t quite meet your permissions needs then you can simply copy and modify them to meet your specific requirements. For example, if you only wanted particular users to have some of the full access permissions then you can remove any lines within the policy that you wanted to restrict. If we look at the actual policy for AWSCloudTrailFullAccess we can see that this also includes admin level access to SNS and S3, which you may want to restrict depending on how you control and manage these other services within your organization. It’s likely you have different teams, managing different services and solutions and so you might need to split up the permissions to allow some to configure S3 and others to configure SNS, Cloud Watch and KMS etc. However upon creation of your trails, it can be useful to have someone responsible to set the configuration for these services during the trail creation, for ease and to ensure it doesn’t get overlooked at a later date.</p>
<p>As we know <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">CloudTrail logs</a> are delivered to an S3 Bucket. When creating your trails, there is a step where you need specify which S3 Bucket to send logs to once processed. Here you will have two options. The first option being create a new S3 Bucket. And the second is to use an existing S3 Bucket.</p>
<p>By selecting the first option, CloudTrail applies and configures a Bucket Policy with the relevant permissions, allowing logs to be delivered to the Bucket. As a part of this process, CloudTrail configures the following attributes within the policy. The allowed Statement IDs (SIDs). The folder name where the log files will be saved. The service principal name for the current and future CloudTrail supported regions. The Bucket name. The optional prefix, if one was specified. And the ID of the owning account. This method is the easiest way to allow CloudTrail to write logs to your S3 Bucket.</p>
<p>If, however, you choose to select an existing Bucket, then you need to set up the correct permissions. This is achieved by applying your own Bucket Policy, which must allow CloudTrail to write and install logs within it. Thankfully AWS have provided a JSON template to allow you to do this with relative ease, as you can see from the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html#using-an-existing-bucket-for-ct-logs">link</a>.</p>
<p>Once you add this policy to your Bucket, you need to set the variable shown in the red italics with your own settings. The optional prefix allows for better organization, allowing you to create a tiered folder like structure for different trail log files if needed. If this existent Bucket already has a Bucket policy, then take caution as you may need to append or edit your existing policy to account for any new statements.</p>
<p>For users of services requiring read access to the CloudTrail logs, authorization will need to be given for S3 read permissions via the usual methods, such as through IAM or an S3 Bucket Policy or even an S3 access control list. If the logs have been encrypted using the Key Management Service, KMS, additional permissions will be required to read the logs. To read the logs that have been encrypted with KMS, the user or service will also need the decrypt permission associated with the Customer Master Key Policy, as well as existing S3 read permissions. Using KMS on your log files is a great way to add another layer of encryption, over the default encryption applied by CloudTrail, which is SSE-S3, Server-Side Encryption with S3 managed keys. For those unfamiliar with SSE, it’s an encryption method used in Amazon S3 to encrypt any object at rest. It’s completely managed by AWS along with the encryption keys, which themselves are also automatically encrypted and rotated regularly by S3. SSE-S3 uses the 256 bit advanced encryption standard, AES 256 algorithm, for its encryption.</p>
<p>You can choose to implement SSE-KMS encryption, which is Server-Side Encryption with AWS KMS managed keys during the creation of your Trail. If this is selected you must either select one of two options when defining your key. The first option is for CloudTrail to create a new KMS key for you. Which will be your Customer Master Key, CMK. And <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">CloudTrail</a> will apply the correct policy to that Key. Or secondly you can select an existing Customer Master Key, to which you must then apply your own policy with the correct permissions. Before moving on from this point, it’s important to note that if you are using an existing KMS Key, the Key and S3 Bucket must be in the same region. Thankfully in case you forget, you are reminded of this during the configuration process.</p>
<p>Again to save you writing your own policy for own your own chosen Key, you can visit this <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/default-kms-key-policy.html">link</a>. This will allow you to copy the default policy that is applied to a newly created CMK in the first option. Again you simply need to replace the areas as indicated in the example. As I mentioned earlier, if the logs are encrypted with SSE-KMS, then the user will need decrypt permissions which can be found in the following section of this default policy. If you wanted to restrict permissions, you would need to edit the Principal attribute as by default this is left wide open, as indicated by the asterix.</p>
<p>That now brings us to the end of this lecture. Coming up next, we will talk in greater depth <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">about Trails</a> that we touched on earlier in the previous lecture.</p>
<h1 id="Understanding-Trails"><a href="#Understanding-Trails" class="headerlink" title="Understanding Trails"></a>Understanding Trails</h1><p>Hello, and welcome to this lecture where we will explain CloudTrail trails and their configuration in further detail.</p>
<p>We learned earlier that Trails are an essential prerequisite for making use of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">the benefits of CloudTrail</a>. Simply put, without a Trail, the service is unable to capture API calls. We also learned that these Trails are the foundation building blocks that hold all configuration information for isolating which API calls are to be recorded. So let’s start at the beginning. Creating a Trail via demonstration. I’ll talk through how we create a Trail from within the AWS Management Console. Once we’re at the CloudTrail dashboard, I will then create a Trail and explain the different components to configure in each section. Let’s get started.</p>
<p>Okay, so let’s take a look at how to set up a Trail within CloudTrail. As you can see, I’m currently at the Service page within the AWS Management Console. And if we go across to Management Tools, and go down to CloudTrail, that’s where you’ll find the service. So if we click on that. And that will take us to the dashboard of the CloudTrail service. And if we click on Trails, here is the screen where you can add new Trails and see all your existing Trails that you already have configured.</p>
<p>So, for this demonstration, let’s add a new Trail. The first thing one needs to do is add a Trail name. So, let me label this CloudAcademy Trail. We next need to decide if we want to apply this Trail to all regions. It’s either yes or no. If we select no, then it will apply the Trail to the current region that you have configured within your Management console. For this demonstration though, I’m going to apply the Trail to all regions.</p>
<p>Next, we’re asked to create a new S3 bucket. So, the S3 bucket is the destination of where your log files will be stored, so all the events that are captured that relate to API calls are recorded into logs, and then these logs are delivered to an S3 bucket. So here we can either create a new S3 bucket or say no and select an existing bucket that we have. For this demonstration, I’m going to say yes, and by doing so, CloudTrail will apply all the correct bucket policy permissions to allow CloudTrail to deliver logs to that bucket. So, let’s give this bucket a new name. I’ll call it CloudAcademy Trail. Keep it the same name.</p>
<p>And if I click on Advanced, we’re then presented with a host of other options. Firstly is a log file prefix, and if we click on the information button here, you can see that a prefix makes log files easier to browse. And it does this by creating a folder-like structure for your prefix. So let’s give it a prefix of demo. So, as you can see here, the location is given, the prefix is demo, kind of a separate folder underneath your S3 bucket.</p>
<p>Our next option is to encrypt log files. By default, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">CloudTrail logs</a> are stored on S3 with server-side encryption. So it’s automatically encrypted. However, if you wanted to add an additional layer of encryption, you can say yes to this and select to use KMS, which is Key Management Service. So we can create a new Key Management Service here or use an existing one and select an existing key that we may have. Now, I won’t go into this hugely right now, because I do cover KMS encryption later on in the course. But I just wanted to point out at this stage that if you did want to apply additional security to your logs, then you can select yes to encrypt log files here using KMS. But for this demonstration, we’ll leave that as no.</p>
<p>Now, next on the configuration options is log file validation. We can either enable this as yes or no. Now what log file validation does, it will check to ensure that your log files haven’t been altered, modified, or tampered with in any way, once they’ve been delivered to your S3 bucket. Now, this is typically used for security and forensic investigations or to be compliant with certain security and governance controls. For this demonstration, I’m going to select no for log file validation. And again, later on in this course, I do go deeper into this topic.</p>
<p>Next, we have a notification option using SNS, so Simple Notification Service, that will notify us when a new log file is delivered to the bucket. If I click on yes here, then we can create a new SNS topic or select an existing topic. So if we did have this enabled, then any recipients associated to the topic will be notified every time a new log file is delivered. For this demonstration, I’m going to select no for that. And that’s it. So, to create your Trail, it’s just these few options and configuration details that you need. So now I’ve selected all those. I’m going to click on Create.</p>
<p>And there you can see in my list of Trails, you can see the new name, CloudAcademy Trail. And we specified that it was all regions, gave it a bucket name, a log file prefix of demo, we haven’t associated any CloudWatch logs yet, and the logging status is on. So, although we’ve created this new Trail, there are additional configuration options we can apply. But we can only do that once the Trail is created. So to add that additional configuration, such as adding the option to send the logs to CloudWatch, we’ll need to select the Trail here first.</p>
<p>And this opens up another page with additional configurable options on it. So, let’s run through these from the top. So, the first option here is Apply trail to all regions. And we already indicated yes on the previous screen. However, we didn’t have this option of Include global services. So what does this mean? Global services, such as IAM and CloudFront, aren’t region-specific. So, when events are triggered by these, those events are sent to any Trails that include global services. So, where this says yes. Most services are region-specific, so any events triggered from these are sent to the region that they were made from. It’s also worth pointing out as well that global services are management events, so if your event selectors within your Trail do not include management events, then these global services will not be logged. But I’ll cover event selectors as we go through the rest of this configuration.</p>
<p>As we scroll down to the S3 section, we can see all of the details that we added earlier, like the S3 bucket name, the prefix, whether or not to encrypt the log files, etc, etc.</p>
<p>Now, here on the next section is where we can configure CloudWatch logs. Now, this is an optional component, so what this enables you to do is to send your CloudTrail logs onto CloudWatch. And from there, you’re able to set up specific metrics and monitoring to allow you to monitor for specific events within your CloudTrail logs. So any event that’s recorded, you can set up monitoring for. And if you can monitor on that, then you can also set up an SNS alert against that monitoring when it meets certain thresholds that you have configured. I just want to be clear here, though, that just because we’re sending it to CloudWatch, it also sends it to S3. So the logs are delivered to both the S3 bucket and also the CloudWatch logs to allow you <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/monitoring-with-cloudtrail-1/">to set up monitoring</a>.</p>
<p>Now, if we go into Configure, we can enter a new or existing log group within CloudWatch. If you’re setting up a new log group, then as a part of this process, CloudTrail will set up all the necessary permissions and policies and roles as well, because for this to work, CloudTrail needs to assume a role to carry out a couple of API calls itself. But I’ll cover this in more detail in a later lecture. But just do be aware that if you want to configure CloudWatch logs, then it’s done in this section here.</p>
<p>Before I move on, though, I will click on Continue to quickly show you the roles, etc, that it uses. And you can see here that, in order to successfully deliver CloudTrail events to your CloudWatch logs group, CloudTrail will need to assume a role, and that is for these two API calls here. If you look at the details, we can select the IAM role and create a new role or an existing, and then a policy to go around that role as well. But like I said, I’ll go into this in more detail in a later lecture. We’ll cancel that.</p>
<p>Now we come down to event selectors. Again, this is an optional component. And event selectors specify the types of events that CloudTrail logs. Now, as you can see, there’s three different types of event selectors. There’s the Read&#x2F;Write events, Management events, and Data events.</p>
<p>If we go in to edit these, it gives us a couple more options. So, for the Read&#x2F;Write events, we can select either Read-only, Write-only, or all. So, this, remember, this is referring to what events this Trail will capture and write to a log. So any event that happens in our account, CloudTrail will look at all of the Trails and the event selectors, and if that event matches any event selectors within a Trail, then it will get written to the log of that Trail. So, for this particular Trail, we can have Read-only, Write-only, or All. For this example, I’m going to leave it on All, and this Read&#x2F;Write events applies to both Management events and Data events.</p>
<p>If I go down to Data events first. I’ll come back to Management events in a moment. We’ll go down to Data events first. So, Data events are object-level API calls to S3 objects, such as put, get, and delete object. Everything else is classed as a Management event. So, all other API calls fall under the Management event category. So, for the Data events, you would need to enter a bucket name and a prefix, if required, and then, if you have that enabled, then any object-level API calls to objects within that bucket will be logged as an event within your CloudTrail log.</p>
<p>And like I said, all other events are classed as Management events. You can either have that on or off. If we switch off Management events, then we need to have a Data event configured, because otherwise, if we switch off Management events and don’t have any Data events, then effectively this Trail has no event selectors to log. So, I’ll switch the Management events back to yes.</p>
<p>If we go down to Add new event selector, we can see here that we can add multiple events to the same Trail. So, for example, I can have this as Write-only and switch off Management events and then have a Data event that looks at our CloudAcademy Trail bucket. So, if we look at this first event selector, we can see that any read or write event that happens within our account, that is classed as a Management event, and it will be written to the logs. And also, if we have a write-only event, that falls under a Data event within this S3 bucket, so any kind of delete or put event that happens within this bucket will be recorded to the log as well. But I’m just going to cancel out of that. And leave as the default Management Yes, and the Data events No.</p>
<p>So, coming down to our last configurable item, which is Tags, which again, is optional. If we go into Edit, we can add a unique key value pair here, so, perhaps this relates to a particular project. And we can save our Trail project name of Security Course, or any other tag that’s relevant to your solution. And then simply click on Apply.</p>
<p>And that’s it. That is your CloudTrail configured. And just before I finish this demo, you can see that at the very top here, you can switch your CloudTrail logging on and off. So I can switch that off, and it will say you no longer collect log files in your S3 bucket or your CloudWatch log group. Say Continue. And your login is, from that point, turned off. Now, if we click on Trails, and there again, we can see that our CloudAcademy Trail, the login status is now off. And that’s how you create a new Trail in CloudTrail.</p>
<h1 id="Insight-into-AWS-CloudTrail-Logs"><a href="#Insight-into-AWS-CloudTrail-Logs" class="headerlink" title="Insight into AWS CloudTrail Logs"></a>Insight into AWS CloudTrail Logs</h1><p>Hello and welcome to this lecture on CloudTrail logs.</p>
<p>The logs are the output of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">CloudTrail service</a> and they hold all of the information relating to the API calls that have been captured. And so as a result it’s important to know what you can do with these logs in order to maximize the benefit of the data they contain.</p>
<p>So what is a log file and what does it look like? Log files are written in JSON, JavaScript Object Notation format, much like access policies within IAM and S3. This is a small section of a log file. Every time an API is captured as per the corresponding Trail it’s associated with, an event is written to the log. Remember, a new event is written for each API call. New logs are created approximately every five minutes or so, but they are not delivered to the nominated S3 bucket for approximately 15 minutes after the API was called. So if you are expecting to see a log file for an API you called seven minutes ago, then you might not see the log as expected for potentially another eight minutes. The log files are held by the CloudTrail service until final processing has been completed. Only then will it be delivered to the S3 bucket and optionally AWS CloudWatch logs.</p>
<p>When an event reflecting an API call is written to a log, a number of attributes are also written to the same event, capturing key data about that call as you can see from this example. Without going through every attribute here, I just want to point out some of the more interesting ones. These being eventName. This refers to the name of the actual API that was called. EventSource. This refers to the service as to which the API called was made against. EventTime. This is the time that the API call was made. SourceIPAddress. This disposes source IP address of the requester who made the API call. This is a great piece of information when trying to isolate an attacker from a security perspective. UserAgent. This is the agent method that the request was made through. Example values of these are signin.amazonaws.com. This is what we have in our example and it simply means that a user made this request from within the AWS management console. You also have console.amazonaws.com and this is the same as the previous. However if this was displayed, it would mean that the request was made by the root user of the account. And we also have lambda.amazonaws.com. This is fairly obvious and this would reflect that the request was made with AWS Lambda. UserIdentity. This contains a larger set of attributes that provides information on the identity that made the API request.</p>
<p>Once events have been written to the logs and then delivered and saved to S3, they are given a standard naming format of the following. The first three elements of this naming structure are self-explanatory. The account ID, name of the service delivering the log, CloudTrail and the region that it came from. The next part relates to the date and time, the year, month and days. The T indicates the next part is the time, reflecting hour and minutes. The Z simply means the time is in UTC. The unique string value is a random 16 digit alphanumeric character string that is simply used by CloudTrail as a unique file identifier to ensure that it doesn’t get overwritten with the same name of another file. Currently the file name format is defaulted to json.gz which is a compressed gzip version of a JSON text file. Here is an example of a file name of an existing log file.</p>
<p>Whilst we are looking at structures, let me also talk about the bucket structure where your logs are stored. You may think the logs are all stored in one folder within your S3 bucket. However there is a lengthy but very useful folder structure as follows. Firstly you have your dedicated S3 bucket name that you selected during the creation of your Trail. Next is the prefix that is also configured during the Trail creation and is used to help you organize a folder structure for your logs, corresponding to different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/understanding-trails-1/">Trails</a>. Following this, a fixed folder name of AWSLogs, followed by the originating AWS account ID. Then another fixed folder name of CloudTrail, indicating which service has delivered the logs. And after that, the region name of where the log file originated from. This is useful for when you have Trails that apply to multiple regions. The last three folders show the year, month and day that the log file was delivered. As you can see, although there are multiple folders underneath your nominated S3 bucket, it does provide an easy navigation method when looking for a specific log file. This folder structure comes into an even greater use if you have multiple AWS accounts delivering logs to the same S3 bucket.</p>
<p>Some organizations may be using more than one AWS account and having CloudTrail logs stored in different S3 buckets across multiple accounts can be inconvenient in certain circumstances and requires additional administration to manage. Thankfully AWS offers the ability to aggregate CloudTrail logs from multiple accounts into a single S3 bucket belonging to one of these accounts. This is why there is an account ID folder within your S3 bucket. Please note that you are unable to aggregate CloudTrail logs from multiple AWS accounts into CloudWatch logs. That belongs to a single AWS account.</p>
<p>So to have all your logs from all your accounts delivered to just one S3 bucket is a fairly simple process with the end result allowing you to essentially manage all your CloudTrail logs. Let’s take a look at how this solution is configured. Firstly you need to enable CloudTrail by creating a Trail in AWS account that you want all log files to be delivered to. Permissions need to be applied to the destination S3 bucket, allowing cross-account access for CloudTrail. Follow the instructions from lecture four on how to set bucket policy permissions. Once permissions have been applied to your policy, you need to edit the bucket policy and add an additional line for each AWS account requiring access under the resource attribute in the section shown here.</p>
<p>Create a new Trail in your other AWS accounts and select to use an existing S3 bucket for the log files. When prompted add the bucket name used in step one and when alerted accept the warning that you want to use a bucket from a different AWS account. An important point to make here when configuring the bucket selection is to ensure that you use the same prefix as the one you used when you configured the bucket in step one. That is unless you intend to edit the bucket policy to allow CloudTrail to write to the location of a new prefix you wish to use. When you have configured your Trail, click create and your new Trail will now deliver its log files to the S3 bucket in your AWS account used in step one of this process. Again this is a great solution that allows you to essentially manage all of your CloudTrail logs in one single account in S3 bucket. However there may be users such as system administrators who manage the other AWS accounts where the logs have come from that might need to access the data within these logs.</p>
<p>So how would they gain access to the S3 bucket to allow them to only access their CloudTrail logs that originated from their AWS account? It could be done quite easily by configuring a few elements within IAM. Firstly, in the master account, IAM roles will need to be created for each of the other AWS accounts requiring read access. Secondly, a policy will need to be assigned to those roles, allowing access to the relevant AWS account’s logs only. Lastly, users within the requesting AWS accounts would need to be able to assume this role to gain read access for their CloudTrail logs.</p>
<p>The easiest way to show you how to configure the permissions required is by a demonstration whereby I shall perform the following steps. I’ll create a new role, apply a policy to this role to only allow access for AWS account B’s folder in S3. I’ll show the trust relationships between account A and account B. I’ll then create a new user in account B and then I’ll create a policy and apply the AssumeRole permissions to this user, allowing them to assume the new role we created in account A. So let’s take a look at how and where we apply these permissions.</p>
<p>Okay so as I just said the first thing we need to do is create a new role in our primary account. So if we go across to IAM which is under Security, Identity &amp; Compliance and then once that’s loaded we need to go across to roles and then create new role. So let’s give this role a name. We’ll call it Cross-account-cloudtrail. Click on next step.</p>
<p>We then need to select a role type and what we want to do is select the role for cross-account access because we’ll be allowing users in another AWS account to access the log files in this primary AWS account. And this will set up the trust relationship between this account and then my secondary account. So for that we will select this top option of providing access between AWS accounts you own. Then next I’ll need to enter the secondary account ID that I want to create the trust relationship with. So I’ll just enter that number. Okay and then after you have entered your account ID, click on next step.</p>
<p>And now we need to attach a policy to this role. Prior to this demo I set up my own policy and this allows cross account access to read only from my secondary account to the bucket on this primary account, but I’ll explain this policy in a few moments and I’ll show exactly what it contains. And from here click on next step.</p>
<p>And this is just a review of the role. So we have the role name, the ARN, the Amazon resource name, the trusted entities, so this is the secondary account ID that I entered and then the actual policy and then that link that we can give to users in the secondary account to allow them to switch roles. So create role. And there we go. The Cross-account-cloudtrail role that we just created.</p>
<p>So let’s take a look at this. Firstly I’ll show you the trust relationships. So because we added cross-account role access and then we entered the secondary AWS account ID, we can see that this account is trusted by our primary account and that allows entities in this account to assume this role. Now I mentioned earlier that I previously set up a policy with permissions in. So let’s take a look at that policy. I named it Cross Account Read Only for CloudTrail so if I show the policy, I should say it’s only a very small policy, very simple. And we have an effect of allow which will allow any S3 get and any S3 list command so essentially read only access on this resource here specified by this line. Now this resource links to the bucket and folder where CloudTrail logs are delivered for our secondary account, as you can see here. So essentially what this policy does is allow read only access to any folders within the secondary account’s CloudTrail log folders. So this account won’t be able to access any other account’s CloudTrail logs which is important. So if we come out of this.</p>
<p>So let’s just have a quick recap of what we’ve achieved so far. So, so far what we’ve done, we’ve created a role in our primary account for our secondary account access and we’ve also assigned an access policy to this role in order for the secondary AWS account to access the relevant folder in S3. So now what we need to do is assign a user in the secondary account and then apply the permissions to that user to enable them to assume the new role in the primary account. So let’s go ahead and do that.</p>
<p>Okay so I’ve now logged into the secondary account where I need to create a new user and assign the correct permissions. So to start with I’m going to set up a permission policy to assign to the user. So if I go down to Security, Identity &amp; Compliance and select IAM. And then go across to policies and from here I want to create a new policy. And I am going to create my own policy, so I’m going to select the bottom option. I’m going to call this AssumeRoleforCloudTrail. My description will be assume role in primary AWS account. And for the policy document I’m just going to paste in a policy that I’ve already created. As you can see it’s only a very small policy again and we have an allow effect that allows the AssumeRole action from the security token service against the following resource. And this resource links back to a role on our primary account where we created the role Cross-account-cloudtrail. So this policy will allow the user to assume this role in the primary account. So let’s go ahead and create that policy. Let’s validate it first and then create.</p>
<p>Now what we need to do is to assign a user to use that policy. Now I created a new user earlier prior to this demo so let’s just find our new policy that we just created and here it is at the bottom, AssumeRoleforCloudTrail. And I’m going to attach a user. And I’ve called our user CloudTrailuser1 and then attach policy. And there we go. So we now have one user attached to this policy.</p>
<p>So that’s all the actions and steps necessary to allow a user in a secondary account to access CloudTrail log files that have been delivered to an S3 bucket in a primary account. And it would do this by using the permission policy that we just applied to that user to access the role in the primary account and that role has a policy attached that allows S3 read access to its own CloudTrail logs.</p>
<p>We won’t go through it again, but recall that you can use KMS to encrypt your log files to offer an additional layer of security. I don’t want to repeat the same information. However I just wanted to bring it to your attention again, highlighting that you can use KMS to offer great security of your log files. Remanding with the security aspect of your log files, CloudTrail allows you enable a feature called log file integrity validation which simply allows you to verify that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. This is typically used for security and forensic investigations whereby the integrity of the log files are critical to confirm that they have not been tampered with in any way. Log file validation is configured during the Trail process as shown in the previous demonstration.</p>
<p>When a log file is delivered to your S3 bucket, a hash is created for it by CloudTrail. A hash value is a set of characters that are unique that are created from a data source, in this case the log file. The hashing algorithms used by CloudTrail are SHA-256. In addition to a hash, for every log file created CloudTrail creates a new file every hour called a digest file which is used to help verify your log files have not changed. This digest file contains details of all the logs delivered within the last hour, along with a hash for each of them. These files are stored within the same bucket as the log files. However they are within their own folder for easier management. These digest files are then signed by a private key of a public and private key pair. When it comes to verifying the integrity of your log files, the public key of the same key pair is used to programmatically check that logs have not been tampered with in any way.</p>
<p>Verification of the log files can only be achieved via a programmatic access and not via the console. Using the AWS CLI, this can be checked by issuing the following command. The red text shows the optional parameters you can use. The folder structure for the digest is very similar to the CloudTrail logs as you can see. The digest files are clearly distinguishable by the CloudTrail digest folder.</p>
<p>This has now taken us to the end of this lecture. Coming up next I’ll explain how you can use CloudTrail and CloudWatch together as a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/monitoring-with-cloudtrail-1/">monitoring solution</a>.</p>
<h1 id="Monitoring-with-AWS-CloudTrail"><a href="#Monitoring-with-AWS-CloudTrail" class="headerlink" title="Monitoring with AWS CloudTrail"></a>Monitoring with AWS CloudTrail</h1><p>Hello, and welcome to this lecture, where we will look at how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail</a> interacts with AWS CloudWatch and SNS to create a monitoring solution. In addition to S3, the logs from CloudTrail can be sent to CloudWatch Logs, which allow metrics and thresholds to be configured, which in turn can utilize SNS notifications for specific events relating to API activity.</p>
<p>CloudWatch allows for any event created by CloudTrail to be monitored. This enables a whole host of security monitoring checks to be utilized. A great example of this is to be notified when certain API calls request any significant changes to your security groups or network access controllers within your VPC.</p>
<p>Other examples of these checks that are common within organizations are API calls relating to starting, stopping, rebooting and terminating EC2 instances. If instances are being created that shouldn’t be, your AWS costs could rise dramatically and quickly. Also, if instances are being rebooted or stopped, this could have a severe impact on your services if they’re not configured in a highly available and resilient solution.</p>
<p>Also, changes to security policies within IAM and S3. If changes are being made to your policies that shouldn’t be, access can be inadvertently removed for authorized users and access granted to unauthorized users, having a massive impact on operational services. Even a minor change to a policy can pave the way for an untrusted user to exploit the error.</p>
<p>Looking at failed login attempts to the Management Console. Monitoring failed attempts here can help to prevent unauthorized access at your environment’s front door.</p>
<p>API calls that result in failed authorization. Not only does CloudTrail track successful API calls, whereby the correct authorization was met by the authenticated identity, but it also tracks unsuccessful API requests, too, which would likely be due to permissions applied. Special attention should be applied to these unsuccessful attempts, as this could be a malicious user trying to gain access. However, it could also be a legitimate user trying to access a resource they should have access to for their role, but the incorrect permissions have been applied with their associated IAM policy.</p>
<p>To configure CloudTrail to use CloudWatch, you must first create a trail. Once your trail has been created, you can then configure it to use an existing CloudWatch Log Group, or have CloudTrail create a new one. Having CloudTrail create a new one for you is recommended if it’s the first time doing this, as CloudTrail will take care of all the necessary roles, permissions, and policies required.</p>
<p>You may be wondering why roles and policies are required. So let me give you a high-level overview of the simple process that takes place when sending CloudTrail logs to CloudWatch. When a log file is created by CloudTrail, it is sent to your selected S3 bucket and your chosen CloudWatch Log Group, assuming your trail has been configured for this feature. To allow CloudTrail to deliver these logs to CloudWatch, CloudTrail must have the correct permissions. And these are gained by assuming a role with the relevant permissions needed to run two CloudWatch APIs. The first one is CreateLogStream and this enables CloudTrail to create a CloudWatch Logs log stream in the log group. And the second one is PutLogEvents, and this allows CloudTrail to deliver CloudTrail events to the CloudWatch Logs log stream. Then finally, CloudTrail will then deliver logs to the CloudWatch Logs.</p>
<p>When using the AWS Management Console, you can have CloudTrail create this role for you along with the correct policy. By default, the role is called CloudTrail_CloudWatchLogs_Role. For those that are curious, the policy for this role looks as follows.</p>
<p>It’s important to point out that CloudWatch Log Events have a size limitation of 256 kilobytes on the events that they can process. Therefore, any events that are larger than 256 kilobytes will not be sent to CloudWatch by CloudTrail. Now that you have your logs with the associated events being sent to CloudWatch, you must now configure CloudWatch to perform <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">analysis of your CloudTrail events within the log files</a>. This is done by configuring and adding metric filters to the log within CloudWatch. These metric filters allow to search and count a specific value or term within your events in your log file, which then allows for customizable thresholds to be applied against them. When creating these metric filters, you must create a filter pattern, which determines what exactly you want CloudWatch to monitor and extract from your files. These filter patterns are fully customizable strings, but as a result, a very specific pattern syntax is required. So if you are creating these for the first time, you must understand the correct syntax. AWS has a great page within their documentation that gives full examples of how to create your filter patterns, which can be found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html">here</a>.</p>
<p>Just to reiterate what we’ve spoken about so far, I want to provide a demonstration on how to edit an existing trail to configure it to send logs to CloudWatch Logs. I will then configure a metric filter with the associated metric pattern. Finally, I will set up an SNS alert to notify me when a particular threshold is met, so let’s take a look.</p>
<p>Okay, so what we need to start with is going into CloudTrail to edit an existing trail to enable CloudWatch Logs. So if I go down to Management Tools and click on CloudTrail, and then across to Trails, and this will show our existing trail that we created earlier in the previous demo, which is ca-trail. As you can currently see, under CloudWatch Logs Log group, there’s no log group selected. So if we go into the trail, and then scroll down to CloudWatch Logs, click on Configure, and then we can get CloudTrail to automatically set up this group, and it will create the necessary roles and permissions, etc. So let’s call this CloudTrail&#x2F;Demo.</p>
<p>And then click on Continue, so we’ve given it a name, and here it just gives a message to say that for CloudTrail to deliver event logs to CloudWatch Logs, it needs to assume a role with permissions to run two API calls, which are these two here. If we go down into the details, we can see that the IAM role that it’s going to use is the CloudTrail_CloudWatchLogs_Role, and we’ll ask it to create a new policy. And here’s the policy document, which I showed earlier on the presentation. So go down to Allow. And then if we scroll down to our CloudWatch Logs section, we can now see that we have a log group created in CloudWatch called CloudTrail&#x2F;Demo.</p>
<p>So if we now go across to CloudWatch, if we click on Logs on the left-hand side here, we can see that we have our log group that was just created by CloudTrail, and it’s CloudTrail&#x2F;Demo. Now if we go into our log group, select it, you’ll see this log stream, which is the incoming stream of events being sent from CloudTrail. Now as we’ve only just started, there’s only a few events coming in here, so you might want to wait a few minutes before setting up your metric filters to give you more of a test pattern to search on. So what I might do is just leave it a couple of minutes for some more events to start streaming in before we set up our metric filters here, just so we have something to search on. Okay, so I’ve left it a few minutes. So let’s go back into the log group, and you can see we’ve now got a couple of streams, and if we go into these, we can see there’s a lot more events.</p>
<p>So if we go back a couple of pages, back to our log group, now we need to create our metric filters to allow us to define what we want to search on within our logs. So if we select the tick next to our log group, and then go up to Create Metric Filter, and here within the metric filter, we need to define a filter pattern. Now as I explained earlir, filter pattern will define what we’re actually searching for within our logs. So for this example, I’ll keep it fairly simple. I’m goning to search for any API call that’s being made from my machine, so from my IP address. So for that, I need to enter the following command: sourceIPAddress &#x3D; 2.218.11.188, which is my IP address. And now we can test to make sure that that filter pattern is okay using this Test Pattern box here, and what that does, that will run this test filter on some log data we see from this log here, and the output of that log is in this box here. So all we need to do is click on Test Pattern, and we can see at the bottom here that it found 47 matches out of 50 events in the sample log. So we know that the syntax is okay for this filter pattern, so I’m going to go ahead and assign this metric. And we can see up here that we’ve got our filter name and our filter pattern, and I’m going to create a new namespace for this metric, and I’ll call it Demo, and metric name will be IPAddress. And then all we need to do is click on Create Filter. Now as you can see, our filter has been created, and we have the details in this screen here.</p>
<p>Now what we can do at this point is create an SNS alarm so we can be notified if a certain threshold was met. So let’s go ahead and do that. So the first thing that we need to do is add a name. So I’m going to call this SourceIPAddress, and the description will be Too many calls from my IP. Now I’m going to set this to be 30. So whenever my IP address is used as a source IP address that is greater or equal to 30 times for one consecutive period over five minutes, then I’ll want it to set to a state of an alarm, and I’m going to want to be notified, so I’m going to enter a new list, give this a new topic, SourceIPAddressAlarm, and I want that to be sent to myself. So as we can already see, with the current data it’s got, that it has already breached the alarm, but it has dropped back down below, so we’ll see how this goes. And we’ll create the alarm. And this is a message just to say that I need to subscribe to that AWS notification, and I can do that in just a few moments.</p>
<p>So if we go across to Alarms, we can see that we have our SourceIPAddress alarm in the state of OK. So at the minute, it’s currently below the 30 threshold. As soon as it goes above that, it will alarm, and I will get a notification. Now over the past few minutes, I’ve just been having some activity within the Management Console, and as we can now see, we do have an alarm on our alert. We can see that it just crossed the threshold, and so I’ve received an email notification to say that it is now in a state of alarm, and if we take a quick look at that email, we can see here that it was crossed with a data point of 33, and the threshold was 30.</p>
<p>So that is how you set up CloudTrail to use CloudWatch with the inclusion of SNS to create alarms against API activity. </p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello, and welcome to this short lecture to close the course on AWS CloudTrail.</p>
<p>At this point, you should now have a greater understanding of what CloudTrail is and what it can do and some of the use cases for this service. It’s a very powerful tool in a never-ending attempt of enhancing your security solution. Being able to capture every API call made within your environment allows for exceptional auditing which, in turn, makes way for a compliance against certain governance controls.</p>
<p>Having the ability to create multiple Trails allows for different teams and departments to use CloudTrail for different use cases. For example, you may find that your security team want to use CloudTrail linked with CloudWatch and SNS to quickly identify unusual or restricted API calls that are not expected. Whereas another team, might want to a Trail to help with day-to-day operational issues when they occur. Being able to look at the last few API calls leading up to an outage or service interruption could be invaluable in identifying the root cause quickly and effectively.</p>
<p>Although the CloudTrail dashboard, via the management console, allows you to view events from the past seven days that relate to any modified create or delete API call, in a simple query, there are many third-party partners out there that are endorsed by AWS that can provide enhanced analysis of your Logs and Events providing yet an even greater insight into what’s happening within your environment. Many of them offer different unique selling points. So, be sure to look at the wide range of partners available. It’s possible you may already be using one of them within your organization for another service. The list of available partners can be found <a target="_blank" rel="noopener" href="https://aws.amazon.com/cloudtrail/partners/">here</a>.</p>
<p>So, to quickly recap on a few things that we have covered. We have learned that CloudTrail captures all API calls in your environment and in all regions that it is configured to do so. For every API call captured, a related Event is recorded with associated metadata within a Log file. How to set up a Trail and understand the different configurable components. CloudTrail Logs are delivered to a specified bucket in S3. CloudTrail Logs from different accounts can be sent to the same S3 bucket in one AWS account through specific permissions and trusted associations between AWS accounts. CloudTrail Logs are encrypted using SSE-S3 by default, but they can be encrypted with SSE-KMS for increased security. There are a number of different permissions required for creating and reading CloudTrail Trails and Logs. CloudTrail Logs can also be sent to CloudWatch Logs to be monitored against specific metrics using metric filters and filter patterns allowing for greater analysis. Further, CloudWatch can work with SNS to send notifications when configured thresholds are reached.</p>
<p>If you have any feedback on this course, positive or negative, please leave a comment on the course landing page. We do look at the comments in earnest and your feedback is greatly appreciated.</p>
<p>So, that now brings us to the end of this lecture and the end of the course. I hope you have found it useful, and it has answered some questions for you may have had surrounding AWS CloudTrail. Thank you for your time and good luck with your continued learning of Cloud computing. Thank you.</p>
<h1 id="2What-is-AWS-CloudTrail"><a href="#2What-is-AWS-CloudTrail" class="headerlink" title="2What is AWS CloudTrail?"></a>2<strong>What is AWS CloudTrail?</strong></h1><p><a target="_blank" rel="noopener" href="https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf">AWS Whitepaper</a></p>
<h1 id="4Understanding-AWS-CloudTrail-Permissions"><a href="#4Understanding-AWS-CloudTrail-Permissions" class="headerlink" title="4Understanding AWS CloudTrail Permissions"></a>4<strong>Understanding AWS CloudTrail Permissions</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html#using-an-existing-bucket-for-ct-logs">JSON Template</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/default-kms-key-policy.html">Default CMK Policy</a></p>
<h1 id="7Monitoring-with-AWS-CloudTrail"><a href="#7Monitoring-with-AWS-CloudTrail" class="headerlink" title="7Monitoring with AWS CloudTrail"></a>7<strong>Monitoring with AWS CloudTrail</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html">Filter Patterns documentation</a></p>
<h1 id="8Summary"><a href="#8Summary" class="headerlink" title="8Summary"></a>8<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/cloudtrail/partners/">List of Available Partners</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Amazon-EC2-Instance-Isolation-Challenge-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Amazon-EC2-Instance-Isolation-Challenge-13/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Amazon-EC2-Instance-Isolation-Challenge-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:02" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:02-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:02:06" itemprop="dateModified" datetime="2022-11-19T23:02:06-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Amazon-EC2-Instance-Isolation-Challenge-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Amazon-EC2-Instance-Isolation-Challenge-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-AWS-Incident-Response-Isolating-your-EC2-instances-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-AWS-Incident-Response-Isolating-your-EC2-instances-12/" class="post-title-link" itemprop="url">AWS-Security-Specialty-AWS-Incident-Response:-Isolating-your-EC2-instances-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:00" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:00-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:57:10" itemprop="dateModified" datetime="2022-11-19T22:57:10-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-AWS-Incident-Response-Isolating-your-EC2-instances-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-AWS-Incident-Response-Isolating-your-EC2-instances-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, my name is Will Meadows and in this course, we are going to go over a few strategies for isolating your EC2 instances in response to a security event. Although this might be a very rare occurrence, I think this course is beneficial to anyone looking to get a deeper understanding of network security.</p>
<p>If you have any questions about anything I cover in this series please let me know at <a href="mailto:will.meadows@cloudacademy.com">will.meadows@cloudacademy.com</a>.</p>
<p>Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> and one of our cloud experts will reply to your question, concern, or comment. </p>
<p>I would recommend this course for any solutions architects, developers, system administrators, and network administrators who are responsible for security of their architectures.</p>
<p>Our learning objectives for this course are: To understand how to isolate an EC2 instances’ network communication with various levels of granularity. We will also look to understand the positives and negatives associated with each technique we are about to cover.</p>
<p>Finally, you will be able to perform this isolation and know the steps required to isolate an EC2 instance at each network level.</p>
<p>You should have a decent understanding of cloud computing and cloud architectures, specifically with Amazon Web Services. You should know about VPC, Security groups, NACLS, and all the basic level networking concepts for AWS. It would be helpful if you have some background in IT or network security, but not required.</p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>Please note that, at the time of writing this content, all course information was accurate. AWS implements hundreds of updates every month as part of its ongoing drive to innovate and enhance its services.</p>
<p>As a result, minor discrepancies may appear in the course content over time. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. </p>
<h1 id="Isolation-of-Your-EC2-Instances"><a href="#Isolation-of-Your-EC2-Instances" class="headerlink" title="Isolation of Your EC2 Instances"></a>Isolation of Your EC2 Instances</h1><p>When you develop and create an incident response strategy, it is crucial that containment is one of your primary focuses. Appropriately containing compromised resources, such as EC2 instances, allows you to perform forensic investigations, removal of harmful elements from your architectures, and recovery of any important data that may be on that instance.</p>
<p>That is why it is very important to have an understanding of what you should do before an incident occurs. This means having a game plan ready. I would recommend having an incident response playbook already created for these kinds of situations. Having a step-by-step guide of what to do when an incident occurs, can greatly increase recovery time, reduce human error, and return a little bit of your peace of mind that everything will be ok.</p>
<p>For this lecture, we are going to cover isolation as a containment mechanism. Isolation of affected instances is fairly easy to understand for people just starting their incident response journey and provides a great deal of actual security when implemented well. Isolation is the concept of limiting the visibility and scope of an element so that its actions only affect itself. This means the instance will not be able to see other instances or nodes on the network, as well as not having the ability to reach out to the internet. It is a lot like putting the EC2 instance in a padded room so that it can’t hurt itself or others.</p>
<p>Starting off, in order to actually isolate an instance, we first need to detect that something is wrong.</p>
<p>AWS has created and implemented a number of very powerful services that can help you detect when something is wrong with your environment. The two obvious services that can help you detect issues with your EC2 instances are Amazon Guard duty and amazon inspector.</p>
<p>Amazon guard duty is a threat detection service that continually monitors and protects your AWS accounts, workloads, and data. It functions by monitoring and analyzing your metadata streams that come from AWS CloudTrail Events and VPC flow logs. Using this data, with the help of some machine learning, guard duty is able to watch for anomalies within your architectures. As an example, Guard duty is able to detect compromised EC2 instances that have been set up to serve malware to your users, or to mine for bitcoin.</p>
<p>If guard duty detects such a threat, it will notify you through detailed and actionable alerts that can be integrated into event management and other workflow systems.</p>
<p>Amazon inspector is another automated security service that can assess your network and the accessibility of your amazon EC2 instances. Additionally, Amazon Inspector can also assess the security state of your applications running on those instances. you can automate security testing against your fleets to make sure they are all running according to plan, and if it does find any issues Inspector can notify you directly by email, or it can message any service that accepts SNS notifications.</p>
<p>If you want some more education on Guard duty, please take a look at this course over here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/</a></p>
<p>And if you want to learn more about amazon inspector, please check out this course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">https://cloudacademy.com/course/amazon-inspector/introduction-82/</a></p>
<p>I just wanted to point out a few services that will help you detect when something is wrong with an instance, and that it might be time to isolate it. With that out of the way, let’s talk about what we should do when we find out something is wrong with an instance.</p>
<p>When you have determined that it is time to isolate an EC2 instance, there are a number of ways we can complete this task. Each of them have their pros and cons which include things like how easy they are to implement, the scope of their isolation, and use case specific hang ups.</p>
<p>Let’s start off this part of the conversation by looking at isolating an EC2 instance with a security group.</p>
<p>This method of isolation is probably the one you would think of first when building out your incident response playbook. Security groups are quite easy to add onto an existing EC2 instance, and can be configured to limit traffic ingress quite easily - seeing as how their default state is an implicit deny all for traffic. There are however a number of things to keep in mind when using security groups in this manner.</p>
<p>First off, when using security groups, you have to explicitly allow traffic using rules. These rules tell the security group what type of traffic and on what ports should be allowed out of the system. Now if an instance has multiple security groups active that overlap in their coverage, Amazon EC2 will apply the most permissive rule to that instance. Security groups are also stateful, which means they remember their connection and allow responses back to the EC2 instances or out of the instance automatically (regardless of any outbound or inbound rules)</p>
<p>So what that inherently means is that you can never shut off traffic to an instance by adding a security group. You can only allow specific traffic with these means. So in order to begin isolation of an instance, it would be very important to remove any existing security groups from the instance or delete all the rules from any security groups attached to the affected instance. And these Rule changes can happen at any time and take effect immediately.</p>
<p>You could then attach an isolation security group, a blank security group ( with no rules), to that instance to enforce its lack of connectivity. </p>
<p>However, we still have a problem. Since security groups are stateful, they keep track of certain connections to allow traffic back into the network. This helps with the whole implementation stateful connections business that everyone loves about security groups. </p>
<p>So now I get to introduce you to the horrid problem of tracked vs untracked connections.</p>
<p>Untracked connections are from traffic that come from a 0.0.0.0&#x2F;0 (all traffic) rule AND a 0.0.0.0&#x2F;0 (all traffic) from (0-65535) ports in the other direction. And this includes both inbound and outbound ways this rule can be written.</p>
<p>Any traffic that fits this category will be immediately interrupted when a rule from a security group changes that would normally stop the flow of traffic. Aka removing a rule, updating a rule, or deleting a security group.</p>
<p>However, there are some types of connections that security groups track, and these do not follow that president. Tracked connections apply to any traffic that has a specific IP or CIDR rule within the security group. This would be something like allowing 203.0.113.1&#x2F;32 for example. This is a specific IP address that has been allowed on the security group to do a thing.</p>
<p>This type of traffic will NOT be immediately interrupted if a rule that has previously allowed its traffic to flow is removed.</p>
<p>So you could imagine that if a bad actor had access to an instance with this specific type of tracked connection available to it, even if you removed all the old security groups and placed your instance in the isolation security group, they could still maintain access to that affected unit, through that connection.</p>
<p>In order to be 100% sure that there are no tracked connections still available to the instance using security groups, you will have to do the following.</p>
<ul>
<li>Create a dedicated “isolation” security group.</li>
<li>Create a single rule of 0.0.0.0&#x2F;0 for all traffic in both the inbound and outbound rules.</li>
<li>Remove any existing security groups attached to the instance.</li>
<li>Associate the Isolation security group to the instance.</li>
<li>Finally, delete both the inbound and outbound rules you created for the isolation security group.</li>
</ul>
<p>This will convert all traffic to untracked and then terminate those untracked connections from the instances.</p>
<p>You could also break this up into steps and have these as security groups ready to deploy instead of having to add and remove rules on the fly.</p>
<p>This would involve having a Step1 security group that already has the 0.0.0.0&#x2F;0 all traffic all ports rules applied to it. You will also need a Step2 security group with zero rules applied to it.</p>
<p>The order of operations would be to remove any existing rules on the affected instances. Then add the step 1 security group. Then you would remove the step 1 security group. Then you would associate the step 2 security group to that instance.</p>
<p>Either way is not very pretty, but it does work - and quite frankly it’s all we have. It would be nice if AWS were to implement a 1 button solution for this.</p>
<p>The next level on the EC2 network isolation chain would have to be the NACL (the network ACL). This handy security device helps you direct traffic into and out of your network at the subnet level.</p>
<p>NACLs work by explicitly allowing or denying access to a subnet based on rules you establish. NACLs are stateless, which means that there must be an explicit rule that allows response traffic back into the network or out of the network (unlike Security groups) which are stateful and do this for you.</p>
<p>All NACL rules are based on external IP addresses or CIDR blocks, and are not relative to any internal destinations. For example, we could allow all traffic from an IP address like 209.78.158.37 which exists outside our network. However we are unable to, for example, Deny all traffic to 192.168.0.1 which exists inside our network.</p>
<p>It is also important to note that a NACL and its associated rules can only be added to one subnet at a time.</p>
<p>Just like with subnets, I am going to go over a few of the benefits of using a NACL to isolate an EC2 instance, as well as some of the problems</p>
<p>First off, it is extremely easy to stop both inbound and outbound communications using a NACL. With just a single inbound and outbule rule, you can terminate both existing connections as well as prevent any future connections to the affected instance. There is no multiple step process involved here like you will have with a security group.</p>
<p>Unfortunately using a NACL like this is very much like trying to cut butter with an axe. You will most assuredly chop through your diary adversary, however you will also hit the table below it in the process. What I mean with this analogy is that NACLs can not be used in a targeted way like the security group. </p>
<p>When you change a NACL it will hit every single instance within that subnet. Which is good I suppose if you want that kind of thing, but if you only have one affected instance it will obviously cause issues.</p>
<p>Those thoughts aside, how do you actually go about isolating your EC2 instances within a subnet using a NACL?</p>
<p>Well, if you are already using a NACL Simply add a DENY ALL rule to your NACL on both the inbound and outbound rules as the very first rule - rule number 1 - for all traffic (0.0.0.0&#x2F;0)</p>
<p>If for some reason your NACL is already full, then you will have to delete an existing rule to make room for this new one. You should record the details of that rule so you can restore it sometime down the road.</p>
<p>If you are creating a new NACL, Add the rules as stated earlier, and just associate your new NACL with the subnet that hosts the EC2 instance that you wish to isolate.</p>
<p>We can now move up the network chain to the next logical level which is the route table. A route table lives &#x2F; is connected to a subnet, just like the Network ACLs we just spoke about.</p>
<p>The route table helps the subnet direct traffic around your VPC. You will have one route table per VPC, and if you do not create one yourself, a default one will be applied. </p>
<p>Now if you have a public subnet, your route table is connected to an internet gateway which allows external communication outside of your VPC. Since we are concerned about isolation, this is probably an important thing to remove.</p>
<p>In order to provide isolation from the outside world, or anything else for that matter, we simply need to remove all routes within a route table ( these could be internet gateway, Direct connections, or VPN connections) Whatever it is we need to get rid of them.</p>
<p>Now you could go through and obliterate your beautiful route tables that are already set up the way you like them, or you could simply create a new route table (these come empty by default) and just associate it with your subnet that you wish to isolate. </p>
<p>This will stop all external subnet communications, isolating all of your EC2 instances within. However, be warned the instances will still be able to communicate with each other within that VPC.</p>
<p>As we have worked our way up the network chain, we have seen how each piece of the puzzle might be shut down or limited in order to isolate an affected EC2 instance. Now we have reached the top of the chain, the internet gateway. </p>
<p>So if all else fails, could we just remove the internet gateway from the VPC to stop all outside communication with our affected instance? No. No you can’t. AWS will not let you remove an internet gateway from your VPC if there are any EC2 dependencies within your VPC that require the internet gateway. You would have to first remove each and every dependency from the network (aka shut down every instance) in order to actually remove the internet gateway.</p>
<p>However if you would like the same effectiveness as removing the internet gateway, you would need to remove all the internet gateway routes from all your route tables like we just discussed in the previous section or attach a custom route table with no routes to all those subnets.</p>
<p>Overall this isn’t a real answer to your problem, I would recommend using any of the previous ones we discussed, but I guess it’s still technically available.</p>
<h1 id="Wrap-Up"><a href="#Wrap-Up" class="headerlink" title="Wrap Up"></a>Wrap Up</h1><p>If you find yourself in need of isolating an EC2 instance it is important to have a plan going in. As we have discussed through this course, It is not necessarily an easy task to stop all communication to a compromised instance. There are a number of gotchas that could get you into trouble if you were not aware of how network communication truly works.</p>
<p>For example, security groups might seem fairly transparent and easy to stop network traffic with, however, dealing with both tracked and untracked connections incorrectly will leave any existing connections running on the affected instance. This vulnerability could leave attackers with an open door to a system you thought was isolated - giving them even more time to deal damage and steal information.</p>
<p>The level of isolation you use will also affect greater and greater amounts of your architecture as you work your way up the network chain. Although simpler than using a security group, isolating your instances at the NACL level will leave all instances within the subnet equally isolated even though they might not be affected in the same way. </p>
<p>If you are running production workloads on those instances that get isolated via this NACL collateral damage, you will of course worsen the experience for any customers who might have been using those servers to begin with. This might be the right course of action, however, so you will need to weigh the positives and negatives for this solution. I recommend doing that kind of soul searching ahead of time, and have your answers already written down in a playbook to speed up your decision-making.</p>
<p>Overall isolation of your EC2 instances is fairly easy to accomplish once you know what you need to do. It will give you some amount of relief and provide valuable time for your security teams to figure out how to deal with the problem both now and in the future. Isolation is but one part of a robust incident response strategy, but I think you can see how valuable this one piece is.</p>
<p>That’s all I have for you in this lecture. My name is Will Meadows and I’d like to thank you for spending your time here learning about isolating your EC2 instances. If you have any feedback, positive or negative, please contact us at <a href="mailto:&#x73;&#117;&#x70;&#112;&#x6f;&#114;&#x74;&#x40;&#x63;&#108;&#x6f;&#117;&#x64;&#x61;&#99;&#x61;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#111;&#x6d;">&#x73;&#117;&#x70;&#112;&#x6f;&#114;&#x74;&#x40;&#x63;&#108;&#x6f;&#117;&#x64;&#x61;&#99;&#x61;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#111;&#x6d;</a>, your feedback is greatly appreciated, thank you!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale-11/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:51:58" itemprop="dateCreated datePublished" datetime="2022-11-14T13:51:58-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:58:42" itemprop="dateModified" datetime="2022-11-19T22:58:42-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale"><a href="#Using-AWS-Identity-Federation-to-Simplify-Access-at-Scale" class="headerlink" title="Using AWS Identity Federation to Simplify Access at Scale"></a>Using AWS Identity Federation to Simplify Access at Scale</h1><p>Hello, my name is Stuart Scott and today I want to speak to you about AWS Identity Federation. I want to explain what it is, some of the AWS services that can be involved in federation and also highlight some scenarios where you might want to implement it.</p>
<p>If you have any feedback on this course, positive or negative, it would be greatly appreciated if you can contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>So, firstly, what is Identity Federation? </p>
<p>It’s basically a method where two different providers can establish a level of trust allowing users to authenticate from one, which authorizes them to access resources in the other. During the federation process, one party would act as an Identity Provider, known as an IdP, and the other would be the Service Provider, an SP.  The identity provider authenticates the user, and the service provider controls access to their service or resources based on IdPs authentication.</p>
<p>You’ve probably all been to websites where it presents you with a log in page, where you can either log in using existing credentials native for that service, or you might have an option to authenticate using credentials from a 3rd party provider, such as Facebook, Google, Twitter or LinkedIn etc. </p>
<p>On the screen here you can see the login page for medium.com you can use existing credentials from identity providers including Google, Facebook, Apple, Twitter and Gmail to create a new Medium account, or log back in to Medium if you’ve already created an account. In this scenario, Medium would be the Service Provider.</p>
<p>So if you didn’t have an account with Medium already, you could select one of these providers to create one. So as an example if you already had a Google account and were signed in with it, and selected ‘Sign in with Google’ you’d be presented with the following screen</p>
<p>Now here Medium has detected that I didn’t have a Medium account associated with my current Google Account, you can see it says “Sorry, we didn’t recognize that account’, and so you are asked if you would like to create one. By selecting Sign up with Google, Medium authenticates my access via my Google credentials, and so trusting the Google IdP, and this enables me to create a new Medium account based off of that authentication. </p>
<p>So in this example, Google was used as the Identity Provider to authenticate you to Medium with a new account. The next time I then need to log in to Medium, all I would have to do is select ‘Sign in with Google’ and as long as I were logged in to my Google account already, I would be logged straight into Medium with 1-click, and this directly correlates to the term ‘Single-Sign on’, or SSO. SSO relates to the method of signing in to one system, which can then be used to authenticate you into another without having to resupply additional credentials. </p>
<p>Now, the process behind this is fairly simple, when the identity provider has been selected, and the user is authenticated by that provider, an assertion is sent back to the service provider, in this case Medium. And this assertion is a message that contains metadata and attributes about the user such as their username. This assertion allows the service provider to authorize access to their services.**<br>**</p>
<p>So identity federation provides a great way to easily set up access control systems with flexibility and ease for the users and service providers. </p>
<p>So using this same principle AWS has a variety of services and methods that allows you to access your AWS services via federated access, meaning you don’t need to have a specific identity &amp; access Management user configured in AWS if you already have a user directory that is managed elsewhere that could be used as an Identity Provider. </p>
<p>Using many different identity standards, including OAuth 2.0, OIDC, which is OpenID Connect, and SAML 2.0, (Security Assertion Markup Language), AWS enables you to easily configure access for contractors and other 3rd parties, in addition to allowing scalable access to authenticate to your mobile and web applications.</p>
<p>Let’s take a quick look at the definitions of these standards according to Wikipedia:</p>
<p><strong>“OAuth</strong> <em>is an open standard for access delegation, commonly used as a way for Internet users to grant websites or applications access to their information on other websites but without giving them the passwords”</em> - Wikipedia</p>
<p><strong>“OpenID Connect</strong> <em>is a simple identity layer on top of the OAuth 2.0 protocol, which allows computing clients to verify the identity of an end-user based on the authentication performed by an authorization server, as well as to obtain basic profile information about the end-user.”</em> - Wikipedia</p>
<p><em>“</em><strong>Security Assertion Markup Language 2.0</strong> <em>(<em><strong>SAML 2.0</strong></em>) is a standard for exchanging authentication and authorization identities between security domains. SAML 2.0 is an XML-based protocol that uses security tokens containing assertions to pass information about a principal (usually an end user) between a SAML authority, named an Identity Provider, and a SAML consumer, named a Service Provider.” -</em> Wikipedia</p>
<p>Now we have more of an understanding of federation, I want to take a high level look at some of the services offered by AWS and how they fit into AWS Identity Federation, including</p>
<ul>
<li>AWS Single Sign-On, known as SSO</li>
<li>AWS Identity &amp; Access Management (IAM)</li>
<li>And Amazon Cognito</li>
</ul>
<p>Now, by the name alone we can safely assume that this has something to do with federated access. </p>
<p>This service has primarily been designed for users to easily access multiple accounts within their AWS Organization enabling a single sign-on approach negating the need to provide credentials for each account. For those unfamiliar with AWS Organizations, they provide a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization, helping to maintain your AWS environment from a security, compliance, and account management perspective.</p>
<p>You can either use the AWS SSO own built in user directory which allows you to create unique users based on e-mail addresses, or alternatively connect AWS SSO to one of the supporting identity providers, such as your own corporate MS Active Directory if you are using one. </p>
<p>Either way, AWS SSO enables you to create different groups while leveraging the power of IAM roles and permissions allowing you to control what access users or groups have in specific AWS Accounts within your organization. </p>
<p>When Users have been configured, they can connect via their own portal which will list all AWS accounts they have access to, along with roles they have access to adopt, and if they would like to access the Management console or use temporary credentials for AWS CLI access.</p>
<p>AWS SSO can also be used to manage access to cloud-based apps using SAML 2.0 such as Office 365 and Salesforce. </p>
<p>As I mentioned previously, Identity federation allows you to access and manage AWS resources even if you don’t have a user account within IAM. </p>
<p>Whereas AWS SSO allows you to create a single sign-on approach to multiple accounts in your AWS organization using the in-built user directory or MS-AD, AWS IAM allows you to configure federated access to your AWS accounts and resources using different identity providers for each AWS account. Examples include both SAML 2.0 for enterprise federation using MS-AD, and OpenID Connect, classed as web identity federation, such as Google, PayPal and Amazon. As a result you could allow access to your environment using these Identity Providers instead of setting up users with a new identity within AWS IAM.</p>
<p>The benefits of this is twofold. It minimizes the amount of administration required within IAM and it allows for a single sign-on solution. </p>
<p>As a part of the configuration process to implement federated authentication, a trust relationship between the identity provider and your AWS account is established. AWS supports two types of identity providers, OpenID Connect, also often referred to as web identity federation, and SAML 2.0.</p>
<p>Amazon Cognito has been built purely to enable the simplification of enabling secure authentication and access control for new and existing users accessing your web or mobile applications, rather than your AWS account. It not only integrates well with SAML 2.0 but also web identity federation as well. One of the biggest features of Amazon Cognito is that it has the capability to scale to millions of new users which is great when working with mobile applications.</p>
<p>Cognito also allows you to use a custom portal allowing you to add a personalized sign-in page with branding and your own logo.</p>
<p>There are 2 main components of Amazon Cognito, these being ‘User Pools’ and ‘Identity Pools’ and they perform different actions.</p>
<p>User Pools are essentially a scalable user directory that allows new users to sign up, or existing users to log in to your mobile application using their native credentials from the user pool, or they can alternatively federate their access via a web or enterprise IdP. </p>
<p>Identity Pools are different to user pools in that they actually provide you with the ability of accessing AWS resources called upon by your web or mobile app by using temporary credentials and the Security token service.</p>
<p>So let me summarize the key difference between these 3 AWS options to implement federated access to AWS.</p>
<ul>
<li>AWS SSO allows you to create a Single sign-on approach to access multiple AWS accounts within an AWS Organization using a single identity provider for all.</li>
<li>AWS IAM allows you to configure different OpenID or SAML identity providers for each of your AWS accounts.</li>
<li>Amazon Cognito enables secure authentication to your web or mobile applications using both SAML 2.0 and web identity federation.</li>
</ul>
<p>That brings me to the end of this introductory course covering AWS identity Federation. You should now have a greater understanding of what it is and some of the services that can be used to implement different kinds of federated Access.</p>
<p>If you have any feedback on this course, positive or negative, please send an e-mail to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/126/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/126/">126</a><span class="page-number current">127</span><a class="page-number" href="/page/128/">128</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/128/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
