<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/16/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/16/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Terraform-Associate-Deploying-Infrastructure-with-Terraform-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Terraform-Associate-Deploying-Infrastructure-with-Terraform-5/" class="post-title-link" itemprop="url">Terraform-Associate-Deploying-Infrastructure-with-Terraform-5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:48:47" itemprop="dateCreated datePublished" datetime="2022-11-19T00:48:47-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 21:35:38" itemprop="dateModified" datetime="2022-11-20T21:35:38-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Terraform-Associate/" itemprop="url" rel="index"><span itemprop="name">Terraform-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Terraform-Associate-Deploying-Infrastructure-with-Terraform-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Terraform-Associate-Deploying-Infrastructure-with-Terraform-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Terraform-Associate-Deploy-a-Highly-Available-Website-with-Terraform-on-AWS-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Terraform-Associate-Deploy-a-Highly-Available-Website-with-Terraform-on-AWS-4/" class="post-title-link" itemprop="url">Terraform-Associate-Deploy-a-Highly-Available-Website-with-Terraform-on-AWS-4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:48:45" itemprop="dateCreated datePublished" datetime="2022-11-19T00:48:45-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 21:34:14" itemprop="dateModified" datetime="2022-11-20T21:34:14-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Terraform-Associate/" itemprop="url" rel="index"><span itemprop="name">Terraform-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Terraform-Associate-Deploy-a-Highly-Available-Website-with-Terraform-on-AWS-4/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Terraform-Associate-Deploy-a-Highly-Available-Website-with-Terraform-on-AWS-4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Terraform-Associate-Manage-AWS-Resources-with-Terraform-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Terraform-Associate-Manage-AWS-Resources-with-Terraform-3/" class="post-title-link" itemprop="url">Terraform-Associate-Manage-AWS-Resources-with-Terraform-3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:48:44" itemprop="dateCreated datePublished" datetime="2022-11-19T00:48:44-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 21:35:56" itemprop="dateModified" datetime="2022-11-20T21:35:56-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Terraform-Associate/" itemprop="url" rel="index"><span itemprop="name">Terraform-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Terraform-Associate-Manage-AWS-Resources-with-Terraform-3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Terraform-Associate-Manage-AWS-Resources-with-Terraform-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Terraform-Associate-Creating-AWS-Resources-with-Terraform-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Terraform-Associate-Creating-AWS-Resources-with-Terraform-2/" class="post-title-link" itemprop="url">Terraform-Associate-Creating-AWS-Resources-with-Terraform-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:48:42" itemprop="dateCreated datePublished" datetime="2022-11-19T00:48:42-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 21:35:48" itemprop="dateModified" datetime="2022-11-20T21:35:48-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Terraform-Associate/" itemprop="url" rel="index"><span itemprop="name">Terraform-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Terraform-Associate-Creating-AWS-Resources-with-Terraform-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Terraform-Associate-Creating-AWS-Resources-with-Terraform-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Terraform-Associate-Terraform-v1-Provisioning-AWS-Infrastructure-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Terraform-Associate-Terraform-v1-Provisioning-AWS-Infrastructure-1/" class="post-title-link" itemprop="url">Terraform-Associate-Terraform-v1---Provisioning-AWS-Infrastructure-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:48:41" itemprop="dateCreated datePublished" datetime="2022-11-19T00:48:41-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 21:32:58" itemprop="dateModified" datetime="2022-11-20T21:32:58-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Terraform-Associate/" itemprop="url" rel="index"><span itemprop="name">Terraform-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Terraform-Associate-Terraform-v1-Provisioning-AWS-Infrastructure-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Terraform-Associate-Terraform-v1-Provisioning-AWS-Infrastructure-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to the Terraform Provisioning AWS Infrastructure course presented to you by Cloud Academy. In this lesson, I’ll cover off the course agenda, intended audience, learning objectives, and course prerequisites. I’m really excited to be taking you through this course.</p>
<p>Terraform has taken the infrastructure in DevOps community by storm. In this cloud-first era, Terraform has revolutionized the way we perform infrastructure provisioning. By way of a quick introduction, I’m Jeremy Cook, a Content Lead Architect here at Cloud Academy. My background is in Software Engineering and DevOps, and in more recent times, Kubernetes and containers.</p>
<p>So, who should be taking this course? Well, this course is suitable for anyone who has an interest in learning Terraform. Taking this course, you’ll learn how Terraform can be used to launch and provision AWS infrastructure. This course will take you through the Terraform fundamentals, making sure that you understand the basics.</p>
<p>During the course, you’ll also get to observe several hands-on demonstrations, where I’ll show you exactly how to codify your AWS infrastructure. I’ll also demonstrate a complete into in deployment of a cloud native application into AWS using Terraform.</p>
<p>The learning objectives for this Terraform course are: learn about Terraform and how it can be used to provision AWS infrastructure, learn how to build and create Terraform configurations and modules, and learn how to use the Terraform CLI to launch and manage infrastructure on AWS.</p>
<p>The prerequisites, which I consider are useful for this course, are: knowledge of the AWS Cloud Platform and its various services, particularly the VPC, EC2 and IRON services, basic system administration experience, basic infrastructure and networking knowledge, and basic SysOps and&#x2F;or DevOps knowledge.</p>
<p>In terms of feedback, any and all feedback that you may have is welcomed. If you have any unanswered questions about this course or would like to see and hear of other related Terraform features, then please reach out.</p>
<h1 id="Terraform-Introduction"><a href="#Terraform-Introduction" class="headerlink" title="Terraform Introduction"></a>Terraform Introduction</h1><p>Welcome back. In this lesson, I’ll provide a high-level overview of Terraform, highlighting some of the more important features that you’ll benefit from once adopted. After this lesson, you’ll be able to answer questions like: What is Terraform? Why would you use it? And a simplified version of how does it work? Let’s begin.</p>
<p>To begin with, Terraform is an open source infrastructure as code tool originally started by HashiCorp and contributed to by the open source community. HashiCorp is a company that specializes in producing tools and applications for DevOps, security and cloud computing infrastructure management. Terraform itself is a cloud agnostic infrastructure provisioning tool that helps to ease the burden of infrastructure builds and maintenance. Beyond the open source version of Terraform, which is installed locally, Terraform is also available in a cloud and enterprise edition.</p>
<p>Terraform Cloud is Hashicorp’s managed service offering and Terraform Enterprise is similar to Terraform Cloud, but is focused on being a self-hosted solution addressing the needs of data localization and operational security policies. Again, the intention of this course is to focus on the open source version of Terraform using it to provision AWS infrastructure. Having said that, it is worthwhile acknowledging that Terraform can be used to provision multi-cloud deployments.</p>
<p>The infrastructure Terraform managers can be hosted on public clouds such as Amazon Web Services, Azure and Google Cloud platform. It can even be used for on-prem or private clouds such as OpenStack, VMware vSphere or CloudStack. Terraform Infrastructure integrations also allow you to manage software and services including databases like MySQL, source control systems like GitHub, configuration management tools like Chef and Puppet and much more.</p>
<p>Currently, there are well over 100 publicly available infrastructure integrations. Before we dive deeper into Terraform itself, I’d like to step back and quickly review the concept of infrastructure as code, why it is important and why it has become so popular in recent times. Infrastructure as code allows us to codify our infrastructure requirements into machine readable definition files. In doing so, we are effectively creating executable documentation.</p>
<p>Anyone new to a project can examine the projects infrastructure as code templates and immediately understand the infrastructure configuration, et cetera. By using code to generate infrastructure, the same environment can not only be recreated multiple times, it can be done so consistently without era or unintentional divergence. Additionally, infrastructure as code can address environmental drift, situations where the initial infrastructure is drifted away from the initial day zero configuration.</p>
<p>Over time, the day one, day two et cetera, infrastructure, may encounter unintentional or intentional, but unapproved changed. By comparing the current site of an infrastructure and baselining it back against your existing infrastructure as code templates, you can deduce any drift and receipt it back to the recorded baseline. With infrastructure as code, your templates can be stored in a vision control system such as Git allowing teams to collaborate on infrastructure. Team members can get specific visions of code and create their own development or test environments. In the past, a pain point that often existed for developers before moving to cloud infrastructure, were the delays encountered with operations, having to budget, plan, create and deliver physical infrastructure.</p>
<p>Now with the elasticity of the cloud allowing resources to be created on demand, developers can instead provision the infrastructure they need when they need it. All combined, these benefits make infrastructure as code not only useful, but a must-have tool, particularly so being a SysOps and DevOps enabler.</p>
<p>So now that we know what infrastructure as code is, let’s return to Terraform itself and begin to understand how it can be used to codify our infrastructure requirements. Terraform, the open source version is packaged into a single executable file lightweight and easy to install regardless of operating system. Once installed, you access its features via the terminal.</p>
<p>A typical infrastructure provisioning workflow involving Terraform goes like this. Iteratively, codify your infrastructure requirements into one or several Terraform configurations. Within your local terminal, use the Terraform tool to first validate and plan the infrastructure an then later apply it. Later on in the project lifecycle, you can always use the Terraform destroy command to destroy your infrastructure if and when required.</p>
<p>Now, I’ll go a lot deeper into each of these steps in the coming slides. As an infrastructure engineer, you write or modify Terraform template files for your infrastructure. These configuration files declare the desired state of your infrastructure. Later on, you can modify existing configuration files to declare how you want to change your existing infrastructure.</p>
<p>To keep you productive and from having to code every requirement from the ground up, Terraform provides for your convenience, a public module registry, from which you can import and leverage any number of modules. A module encapsulates related resources together, which when combined, are use to achieve a particular requirement. During the demonstrations that I provide later on, I’ll demonstrate how to both build your own modules and how to work with the modules available within the public Terraform module registry.</p>
<p>When it comes to provisioning time, Terraform integrates with different cloud providers and or other infrastructure vendors through the use of providers. A provider encapsulates all of the mechanics to connect, authenticate and communicate with the infrastructure provider. This is one of the true benefits of using Terraform, that is, the potential for it to provision multi-cloud infrastructure.</p>
<p>Once you’re happy with your declared configuration, you can ask Terraform to generate an execution plan for it. The execution plan tells you what changes Terraform would need to make to bring your current infrastructure to the declared state in your configuration.</p>
<p>Now, if you were to accept the plan, you can then instruct Terraform to apply the changes. To do so, you proceed by using the apply command. The apply command can use the plan that you previously generated, or, if you don’t provide a plan, apply can generate one for you and ask for you to accept the plan before applying the changes. Terraform will then orchestrate the infrastructure API calls required to implement the infrastructure changes.</p>
<p>The Terraform Root Module is the entry point for all Terraform configuration. By convention, the Root Module gets populated with the following three Terraform conflict files that you produce, main.tf, variables.tf, and outputs.tf. However, as the complexity of your infrastructure requirements grow, you may refactor this initial arrangement in a number of ways. As already mentioned, it is only conventional to have the three previously named files. The actual Terraform configuration spread across these three files could in fact, actually be collapsed and contained within a single file named anything you like, as long as it has the extension .tf.</p>
<p>Going in the opposite direction, the Terraform configuration within the Root Module could also be refactored by storing parts of it across and within additional subdirectories. Such subdirectories within the Root Module become nested modules.</p>
<p>Finally, depending on how you have configured your Terraform state setup, Terraform state files may also co-exists in the Root Module. This however, is not the case when remote Terraform state has been configured, more on this later.</p>
<p>Another important concept to consider when setting up your initial Terraform environment is the concept of a Workspace. Terraform uses the concept of Workspaces to manage and separate multiple but different infrastructure environments using the same state of Terraform configuration files. This is particularly useful when you want to provision and mirror infrastructure for dev, test or prod environments. With Workspaces, we can establish a Workspace peer environment and then provision infrastructure specifically for that environment using the same Terraform configuration files. And I technical level, Workspaces isolate and manage multiple versions of Terraform state.</p>
<p>Workspaces are managed using the Workspace command. You can create additional Workspaces with its new subcommand and switch between Workspaces using the select subcommand. If you select a new Workspace, there is no state until you apply the configuration. Any resources created in other Workspaces still exist. As and when required, you can simply swap between Workspaces to manage resources assigned and provisioned within that Workspace.</p>
<p>I’ll now do a basic review of the three core files that are typically added to the Root Module, starting with the main.tf your file. The main.tf contains your core Terraform configuration, mostly resources that you had declaring, which when working with the AWS provider, at provisioning time, will get converted into actual AWS Cloud hosted infrastructure resources, such as EC2 Instances. Over time, larger and more complex infrastructure setups might require you to go back and refactor and split up the contents of the main.tf file across multiple.tf files.</p>
<p>Next up is the variables.tf file. This is another file that again, will often be edited into the Root Module. The variables file contains all possible variables that are then referenced and used within the main.tf and or other.tf files within the Root Module. When performing a Terraform plan or Terraform apply, the values assigned to each variable will be injected into any place the referenced variable name is used. Variables can be both typed and have default values as seen here, although, this is not mandatory.</p>
<p>As you’ll see later on, there are multiple ways in which the defaults can be overwritten. Rounding out the last of the three conventional default files which are added to the Root Module to compose a simple Terraform configuration is the Outputs file. The Outputs file is where you configure any messages or data that you want to render out to the end user within the terminal at the end of an execution of the Terraform apply command.</p>
<p>Additionally, when using end and beading modules in a parent Terraform template, module outputs can be referenced within the parent Terraform template by using the module.<MODULE name>.OUTPUT NAME&gt; notation. This will be demonstrated later on within the demonstrations. Terraform State.</MODULE></p>
<p>As earlier mentioned, Terraform is a stateful application. It has been purposely designed to keep track of all infrastructure provisioned through it. All state tracked is stored inside of a Terraform State file. Having performed a Terraform apply, Terraform will capture and record the infrastructure state in two files, terraform.tfstate and terraform.tfstate.backup located in your working directory when working with local state.</p>
<p>The state is written in JSON format, meaning you can parse these files if required. These files represent Terraform’s source of record, recording the last known state. The great thing about having Terraform track and maintain the last name at state of your infrastructure is that it enables you to detect any drift or divergence. If you’d like to check and see if the state files still matches what you last built, you can use the Terraform refresh command. Running this command will alert you to any detected change.</p>
<p>Terraform by default will store state on the local file system. However, you can update this configuration to store the state remotely, perhaps within a dedicated AWS S3 buckets. When using the local file system for state, this can become problematic when working in teams since the state file is a frequent source of merge conflicts. When this occurs, consider using remote state instead.</p>
<p>Using remote state is also considered more secure since the data can be encrypted at risk, and Terraform only ever stores remote state and memory, never on disk. Requests formal state are also encrypted during transit using transport layer security or TLS. Security is important because configurations can store secrets and sensitive information. You can also access remote state using data sources. This allows different projects to access a project state in a read only version.</p>
<p>Now, regardless of what you have at backend, you end up using four configuring Terraform state. If it supports locking, Terraform will lock the state while an operation that could potentially write state changes is happening, this has done so to prevent state corruption.</p>
<p>Now when it comes to connecting Terraform against a particular infrastructure provider, it’s good to know that Terraform provides a public registry located at registry.terraform.io, which contains a bunch of providers and modules. We’ll talk about modules later on. But for now, it is providers which are used to integrate against an infrastructure providers API.</p>
<p>When it comes to provisioning AWS infrastructure, you’ll want to work with the latest version of the AWS provider. Providers are visioned to maintain compatibility with the infrastructure providers API as it evolves over time. Once you have selected the AWS provider, the use provider link top right, provides an example of how to configure the AWS provider with a new Terraform code.</p>
<p>For the record, each available provider held within the registry provides a comprehensive documentation, including examples of how to work with it. To exit the documentation associated with the AWS provider, click on the documentation link, top right. All provider documentation is searchable allowing you to quickly navigate to the required documentation. This example shows documentation specific to launching an AWS EC2 Instance. You can easily copy and paste the provided examples, thereby quickening the pace of development.</p>
<h1 id="Terraform-CLI"><a href="#Terraform-CLI" class="headerlink" title="Terraform CLI"></a>Terraform CLI</h1><p>Welcome back. In this lesson, I’ll review the installation process for installing the open-source version of Terraform locally. Now the good news is since Terraform is packaged as a single binary, installation is very simple and quick, as you’ll soon see. Let’s begin.</p>
<p>Terraform provides operating system specific downloads for all of the most popular desktop or workstation operating systems, including macOS, Linux, and Windows. Clicking on any of the provided download links will download a ZIP archive. Once downloaded, it is just a case of unzipping the archive and then moving the single Terraform executable binary within to a location configured on your operating system’s system PATH. It’s literally that easy.</p>
<p>Now if you’re looking for a more automated or scripted approach for a macOS-based system, then the following script displayed here can be used to download and install the latest version of the Terraform CLI, hands-off. The script provided on the screen is specific to macOS. This script will also set up autocompletion for the Terraform subcommands. Likewise, for Linux, this script can, again, be used to install the latest version of Terraform CLI automatically. And for Windows, the process provided here is a bit more manual, but in a sense, just requires you to move the Terraform binary to a location, as previously mentioned, configured in the current system PATH environment variable.</p>
<h1 id="Visual-Studio-Code"><a href="#Visual-Studio-Code" class="headerlink" title="Visual Studio Code"></a>Visual Studio Code</h1><p>Welcome back. In this lesson, I’ll review the development tooling setup as will be used later on in the provided Terraform demonstrations. When creating your own infrastructure as code templates, it pays to use a productive editor, which for the purposes of this course will be Visual Studio Code.</p>
<p>Visual Studio Code is a freely available, open source IDA, which can be installed and run on most desktop workstation operating systems. It provides a lot of extremely useful editing features such as intellisense, which can guide you through the process of creating your Terraform code.</p>
<p>After installing Visual Studio Code on your workstation, it’s highly recommended to install HashiCorp’s own Terraform extension. This extension compliments existing editing features by making available Terraform syntax highlighting.</p>
<p>Additionally, another useful extension which helps to boost Terraform configuration development productivity is the Terraform doc snippets extension. This extension can be used to call up numerous snippets of Terraform code for all of the most popular AWS resource types. In fact, this extension can be used to call up snippets of code for other cloud infrastructure providers, such as Azure and GCP, making it more than useful for multi-cloud infrastructure projects.</p>
<h1 id="Terraform-CLI-AWS-Authentication"><a href="#Terraform-CLI-AWS-Authentication" class="headerlink" title="Terraform CLI AWS Authentication"></a>Terraform CLI AWS Authentication</h1><p>Welcome back. In this lesson, I’ll review the credential configuration options that you can use to allow Terraform to correctly and successfully authenticate into your own AWS accounts. If your authentication set up is broke, Terraform is not going to be able to authenticate, and this will mean no infrastructure gets provisioned. So, understanding how to do this successfully is essential to moving on. Let’s begin.</p>
<p>There are basically three different approaches to configuring your AWS account credentials to allow Terraform to connect and authenticate successfully. The first approach is to wire the credentials directly into the Terraform template file. In the example given here, the AWS provider is configured with both an access key and a secret key.</p>
<p>Now, although this approach is probably the quickest and easiest approach, it should be done so with care since it is considered bad practise or, at least, not good practice if you were to version control this file containing the credentials. The last thing you want is AWS account credentials floating around in your version control system, particularly ones that can perform infrastructure changes.</p>
<p>As you will see in the following two slides there are better alternatives that allow you to extract out the credentials and store them elsewhere. Pausing briefly here, regardless of the AWS credential management approach you end up using, within the AWS account you should create a dedicated IM user with programmatic access enabled.</p>
<p>For this user account, make sure to grant at least privileged based permissions, as this limits the potential blast radius that Terraform will have within the AWS account. Granting the Terraform AWS IM user full admin access within the AWS account might be convenient at first but it will likely be overkill, not to mention dangerous as Terraform effectively has granted permissions to do any and everything within the AWS account.</p>
<p>If, for example, you need it to only build a VPC, Subnets, Route Tables, and deploy EC2 instances of, say, a particular instance type, then create a custom IM policy and assign it to the IM Terraform user that allows just that and nothing else. The second approach is to leverage environment variables. The Terraform CLI Executable is designed to scan for and detect the presence of these environment variables. If detected, they will be used.</p>
<p>Clearly, this approach is better from the perspective of version control. With this approach, the main.tf file, as previously configured, could now safely be committed into version control, since, the AWS credentials have been extracted out and away from it. The third approach is to store them, as you may have already done so if you’re a user of the AWS CLI, on the local file system, in a credentials file.</p>
<p>The AWS Terraform provider, for example, can be configured via the shared credentials file attribute having it pointed at an existing credentials file, such as the one used in this example. For those who may be unfamiliar with the AWS CLI, the file path used within this example is actually a pointer to the same creds file as used by the AWS CLI. Note; the AWS CLI is not a prerequisite for the Terraform CLI. Regardless, the Terraform CLI can be configured, as seen here, to reuse and share the same creds file created and managed and used by the AWS CLI.</p>
<p>Note; if all three types of credentials are used, then the 3.X version of the AWS provider preferences static credentials first followed by environment variables and then, lastly, shared credentials. The same order is just given in the three explanations. The next example is really just a derivative of the previous example. Here, when we use and set the profile, together with the absence of the shared credentials file attribute, implies that the files will be sourced from the current users home&#x2F;.aws&#x2F;credentials file. Again, the same one that is used and managed by the AWS CLI.</p>
<h1 id="Terraform-CLI-Subcommands"><a href="#Terraform-CLI-Subcommands" class="headerlink" title="Terraform CLI Subcommands"></a>Terraform CLI Subcommands</h1><p>Welcome back. In this lesson, I’m going to now review the Terraform CLI and its available subcommands. Particular focus will be placed on the main commands: init, validate, plan, apply, and destroy. Let’s begin. When starting out with the Terraform CLI tool, check out the embedded help documentation. To do so, fire up your local terminal and type in terraform -help. This will display Terraform help regarding all of the available subcommands, which are grouped into those considered the main commands followed by all remaining commands, which are least commonly used and&#x2F;or for more advanced requirements.</p>
<p>As seen on the slide, the main commands are considered to be init, validate, plan, apply, and destroy. These commands, which I’ll go deeper into in the following slides are the ones that you will often cycle through when you are iteratively developing and building out your Terraform infrastructure. The remaining subcommands, as seen here, are as mentioned less commonly used and more so for advanced scenarios. Having said that, I’ll call out a couple that I tend to use frequently.</p>
<p>Console. The console subcommand fires up a REPL, read-evaluate-print-loop, interactive console. This allows you to test out and evaluate Terraform expressions. More than useful when experimenting or troubleshooting. The interactive console will actually use the available Terraform state during evaluations.</p>
<p>Format. The format subcommand is useful to reformat and standardize the layout of your Terraform code.</p>
<p>Output. The output subcommand can quickly re-render the output values for your root module.</p>
<p>Workspace. The workspace subcommand we covered earlier on in the course, but to reiterate, it is used to create and manage multiple workspace environments, which in turn can be used to build multiple versions of infrastructure of the same set of Terraform configurations. I’ll now move on and do a deeper dive on the main commands since they will be the ones that you will often use and will need to be confident with, to build AWS infrastructure.</p>
<p>Starting with the Terraform init command. This is a mandatory command required to initialize your Terraform workspace in the current directory. Before running the Terraform init subcommand, your root module directory will look something like the following. The key point here is the absence of the .terraform directory.</p>
<p>Now, if we were to examine a typical main.tf file that gets populated into the root module directory, it would look something like this. Here we can see that it has been configured with the AWS 3.55 provider and has been specified that we are configuring it to authenticate by using a profile name. In this case, the default profile. This tells Terraform to collect the AWS credentials from the currentusers.aws&#x2F;credentials file. The same one that the AWS CLI uses and manages.</p>
<p>Now, to initialize our current working directory, we enter the command terraform init. The initialization process will do a number of things for us. Firstly, Terraform reads our configuration files in the working directory to determine which plugins are necessary, searches for the installed plugins in several known locations, and then downloads the correct one. In this case, the AWS 3.55 provider. It will also create a log file to log down the version of the plugins that we have initialized our waking directory with. And finally, it will also pull down any external modules as used and referenced within our remaining Terraform templates.</p>
<p>If we were to rescan the current working directory after having performed a terraform init command, we would now see the updates made within it. Namely the presence of the .terraform directory, which holds a copy of the configured provider and any referenced external modules. The current working directory also now contains a .terraform.log.hcl file, which, as previously mentioned, is used to log down the version of the plugins that we have initialized our working directory with.</p>
<p>If we were to use the tree command on the .terraform directory, we would be able to see and examine its internal structure, which in this case, we have configured a single provider, that being the AWS version 3.55 provider. Note here that the AWS provider file is an executable file itself. Terraform plugins, and in our case, the AWS provider, are written in Go and are executable binaries invoked from the Terraform Core over RPC.</p>
<p>In this example, you can also see the presence of the AWS VPC module, which is being truncated for brevity purposes. As an FYI, the AWS VPC module is available in the Terraform public registry, and it’s super useful for building out VPC configurations and related networking components very quickly.</p>
<p>Next up is the Terraform validate command. The Terraform validate command does just that. It validates all of your local Terraform configuration, making sure that it is syntactically correct, et cetera. It is often used immediately after any save operation on the configuration. In the example provided here, the current Terraform configuration has successfully passed and is therefore considered valid.</p>
<p>In the next example, the cidr_block attribute in the VPC resource has been intentionally commented out to cause an error. Rerunning the terraform validate command will cause it to flag the problem. In this case, it is flagged expectedly with the “Error: Missing required argument” message and importantly highlights the offending Terraform configuration file and the problematic line number within it.</p>
<p>Next up is the Terraform plan command. The terraform plan command is a dry-run command, which is typically run just before the apply command. When executing this command, Terraform is just telling us what it would do if we perform the apply command. Running this command acts as a safety check. Sometimes our assumptions of what an apply would do might be slightly or considerably wrong. Fingers crossed here this is not the case. Regardless, the plan command will highlight exactly what would happen and provides us an execution plan that we can do when we’re doing the real thing.</p>
<p>Whenever you run a plan or apply, Terraform reconciles three different data sources. One, what you wrote in your Terraform templates. Two, the current Terraform state file. And three, what infrastructure actually exists within the infrastructure provider. In the plan example shown here, the plan results are indicating that nine new resources would be added, zero would be changed, and zero would be destroyed.</p>
<p>Because Terraform is convergent, it will play in the fewest required actions to bring the infrastructure to the desired configuration. Terraform also considers dependencies to determine the order that changes must be applied in. The plan stage is relatively inexpensive compared to actually applying changes. So you can often use the plan command while developing your configuration to see what changes would need to take place. </p>
<p>Moving on to the Terraform apply command. The terraform apply command reruns the plan or execution plan, and assuming you approve it, will then provision the changes within the provider as per the plan. Now, if anything goes wrong, Terraform will not attempt to automatically roll back the infrastructure to the state it was in before running apply. This is because apply adheres to the plan. It won’t delete your resource if the plan doesn’t call for it.</p>
<p>To address the need for a rollback position, if you version control your infrastructure configuration code, and we strongly encourage you to do so, you can use a previous version of your configuration to roll back to. Alternatively, you can use the destroy or taint command to target components that need to be deleted or recreated respectively. By default, the apply command will always prompt you first for confirmation before applying the plan changes.</p>
<p>Order confirmation can be set by attaching the order approve parameter. This is useful as a workflow optimization when doing frequent small incremental changes, perhaps within ADF test environment. Obviously, take care when considering doing this in production. When the apply command completes, it will report back the applied changes as those added, those changed, and those deleted. It will also render out any outputs that you have coded into your Terraform templates.</p>
<p>In the example shown here, the output section shows the subnet IDs for subnets one and two, the VPC ID, and the public IP address for the web EC2 instance. Finally, we have the Terraform destroy command, which is used to tear down all Terraform managed infrastructure that you have codified. The terraform destroy command is clearly a destructive command, so care must be taken using it, particularly so in production.</p>
<p>When it comes to production environments, authorizing destructive operations via Terraform within your AWS account can be and should be controlled by an appropriately designed IAM policy. This policy would then be attached to an IAM user whose credentials are securely managed and available to only a select few. When the terraform destroy command runs, it will again plan for and report out the required deletion operations to remove all Terraform managed resources within your AWS account.</p>
<p>By default, the destroy command will always prompt you first for confirmation before applying the deletions. Order confirmation can again be set by attaching the auto-approve parameter, but this should only be done if you’re a hundred percent certain of what the end result is, more so than ever for production environments. Similar to the plan and apply commands, the destroy command once executed and completed will report on the final number of resources that have been destroyed.</p>
<h1 id="Terraform-Language"><a href="#Terraform-Language" class="headerlink" title="Terraform Language"></a>Terraform Language</h1><p>Welcome back. In this lesson, I’ll review the more commonly used parts of the Terraform HCL language, which you’ll require a good understanding of to codify your own Terraform infrastructure as code templates. Let’s begin.</p>
<p>Formally, Terraform configuration is running using HCL, HashiCorp Configuration Language, a human-friendly, readable, and writeable syntax, perfect for codifying infrastructure requirements. HCL’s configuration was created to have a more clearly visible and defined structure when compared to other well-known configuration languages, such as JSON and YAML.</p>
<p>Now, at the top level, the HCL syntax comprises stanzas or blocks that define a variety of configurations available to Terraform. Stanzas or blocks are comprised of key value pairs. Terraform accepts values of type string, number, Boolean, list, and map. Single line comments start with hash, while multi-line comments use an opening slash, asterisk and a closing asterisk, slash.</p>
<p>An interpolated variable reference is constructed with the dollar sign, clearly brackets syntax. For example, the type tag in the provided example interpolates the variable named project. Single line strings are written in double quotes, whereas multi-line strings are specified using the heredoc format. In this case, an opening EOF, end of file, character sequence is paired with a closing EOF character sequence. In-between each line is considered part of the multi-line string. This multi-line string approach is often used to capture scripts as used within the user data attribute when bootstrapping EC2 instances.</p>
<p>Maps are defined using curly braces and are a collection of key value pairs. They are often used for creating variables that act as look up tables. In the example provided here, an AMI look up table has been created. The Terraform Core program requires at least one provider to build anything. You can manually configure which versions of a provider you would like to use. If you leave this option out, Terraform will default to the latest available version of the provider.</p>
<p>Remember to initialize the current working directory using the Terraform init command, which is required before attempting to perform a plan or apply. At the end of the day, it’s all about provisioning resources within your infrastructure provider. The resource keyword is used to declare the type of resource you want to provision.</p>
<p>In the example given here, we are declaring two AWS resources, a VPC and a subnet. Each resource is then configured with its required and optional attributes. Note in this given example, the subnet resource utilizes and sets the count attribute, which is considered a meta argument within Terraform. It allows you to create multiple versions of the resource it is declared within.</p>
<p>In the example provided here, Terraform will create one subnet per availability zone. The outcome of applying this Terraform configuration will be an AWS VPC, which has public subnets deployed into each of the AZs for which the VPC spans across. This type of syntax, although more abstract, is far more concise and compact when compared with hand-writing each subnet individually.</p>
<p>When declaring resources, the following layout is required. Resource is the top level keyword followed immediately by type and the name of the resource, both in double quotes. Although the more recent versions of Terraform do not mandate double quoting either the type nor name, it is still considered idiomatic to do so. In fact, if you were to run the Terraform format command to reformat your code, all unquoted resource types and names will become double quoted. The type represents the type of the resource to be provisioned.</p>
<p>In the two resource examples shown here, we are declaring types of aws_vpc and aws_subnet for an AWS VPC and an AWS subnet, respectively. The resource name is an arbitrary name that you come up with that you can then later use to refer to this instance of the resource. Every Terraform resource, regardless of type, is structured exactly the same way. This resource example demonstrates how to launch a single EC2 C5 instance type for the purposes of performing number-crunching, et cetera. Here the type is set to be an aws_instance, which represents an EC2 instance. The resource is then named NumberCruncher for lack of imagination.</p>
<p>Data sources are a way of querying an infrastructure provider for data about existing resources. Data sources, when declared, can leverage one or several filters to narrow down the return data, to be more specific about the requirement at hand. In the example provided here, a data source is declared to return AMI IDs for all available Ubuntu 20.04 images for the current AWS region. If more than one image is discovered, the most recent one will be returned based on the fact that the most recent attribute has been declared to be true.</p>
<p>The Ubuntu data source, as seen here, is then later used within the number cruncher AWS instance resource to specify its AMI. Using this type of approach instead of hard-coding the actual AMI ID directly within the AWS instance resource future-proofs your Terraform templates.</p>
<p>For example, consider the scenario of the Ubuntu 20.04 operating system being overhauled or patched by chronicle, perhaps due to a recently discovered security vulnerability. Having done this, they will likely also publish a new set of updated AMIs.</p>
<p>Now, the next time you perform a Terraform plan or apply command, Terraform will detect that your existing instance or instances, running the old AMI are now out of date and can be relaunched with the newer equivalent updated AMI. Or when launching a brand-new environment, you’ll always be safeguarded by the fact that the instance launched within it will be using the latest patched and up-to-date AMI.</p>
<p>In the second example of a data source, information about all available AZs for the current AWS region is queried for. The AZ data source is then referenced within the subnet resource being declared. Here, the availability zone attribute takes on the first value contained within the AZ’s data source. Taking this approach helps to keep our Terraform templates generalized such that they can be reused easily across different AWS regions.</p>
<p>Variables are another technique to assist in keeping your Terraform configurations generalized and reusable for multiple requirements. The idiomatic practice is to store variables in a file named variables.tf. Variables can have default settings. If the default is omitted, the user will be prompted to enter a value. In the example provided here, we are declaring the variables that we intend to use, but haven’t declared any default values. The declared variables can then be referenced from within the main.tf file and for the same meta elsewhere in all other .tf files in the current directory.</p>
<p>It’s important to understand that Terraform provides several ways in which you can see it and override the default value for any and all declared variables. If multiple approaches are used together, then Terraform follows a defined precedence in terms of which ones get used first. I’ll now review them in order from highest priority to lowest priority, as also displayed here currently.</p>
<p>Option one. Leverages command line variable flags. If they are defined on the command line, then these will have the highest priority. Option two allows you to define your variable values within a terraform.tfvars file. If this is detected unavailable, it will be automatically used. If required, you can have multiple distinctly named versions of the tfvars file. When you do so, you must declare which one is being used via the VAR file parameter. This approach is perhaps useful to alter the infrastructure provisioning process for say, different environments, et cetera.</p>
<p>Option three. Within the shell or terminal session from which the Terraform CLI is being used, you can see environment variables named with the following naming strategy. Capitals TF_VAR_ followed by the actual name of the variable itself, and then assign it with a value.</p>
<p>Option four uses the default values stored against the declared variables within the variables.tf file. And lastly, option five, the lowest priority option, manual entry. You will be prompted to supply a value at runtime within the terminal during the Terraform execution. Output values are like the return values of a Terraform module. The idiomatic practice is to store outputs in a file named outputs.tf.</p>
<p>Primarily outputs are used for the following two purposes. One, the root module uses outputs to print out values in the terminal for your convenience. In the example shown here, the public IP output would print out the AWS EC2 assigned public IP address to the terminal once the provisioning has completed. And two, a child module can use outputs to export a set of values which are required and used elsewhere within its parent module. From here, the parent module can then later pass these values as inputs to other child modules.</p>
<p>Modules are an abstraction that can be used to combine related resources together for reusability purposes. At implementation time, modules are containers of multiple related resources that are used together. A module consists of a collection of terraform.tf files, all kept together in the same directory. Modules are the main way to package and reuse resource configurations within Terraform. Every Terraform configuration has at least one module known as its root module, which consists of the resources defined in the .tf files in the main working directory.</p>
<p>A Terraform module, usually the root module of a configuration, can call other modules to include the resources into the configuration. A module that has been called by another module is often referred to as a child module. Child modules can be called multiple times within the same configuration, and multiple configurations can use the same child module.</p>
<p>Finally, as previously mentioned earlier in the course, Terraform has a public registry containing modules built by the Terraform community, all of which are available for use to cherry pick from as and when required. Expressions are used to refer to all compute values within a configuration. The simplest expressions are just literal values like the string hello, or the number five. But the Terraform language also allows for more complex forms, such as references to data exported by resources, arithmetic, conditional evaluation, and all those that utilize built-in functions.</p>
<p>In the provided example shown here, expressions are used to test whether the AWS security group variable is ND or not and react accordingly. To round out the Terraform language introduction, Terraform includes a number of built-in functions that you can call from within your expressions, as just previously explained, to transform and combine values.</p>
<p>The general syntax for function calls is a function name followed by parentheses containing a comma separated list of input arguments. The available built-in Terraform functions, and there are many of them, allow you to perform infrastructure provisioning operations more dynamically. In the provided example here, three different built-in functions are used: length, cidr subnet, and element, working together to codify the creation of multiple subnets for the scenario. The length function returns the length of a list.</p>
<p>In this example, returning the availability zone count. The cidr subnet function will create a cidr block string based on the inputs given, returning something like 10.0.0.0&#x2F;24, 10.0.1.0&#x2F;24, 10.0.2.0&#x2F;24, et cetera, et cetera. Keep in mind here that this function gets called multiple times since this resource sets and uses the count meta argument. The element function retrieves a single element from a list at the given position. If the given index is greater than the length of the list, then the index is simply wrapped around.</p>
<h1 id="AWS-Simple-VPC-EC2-Instance"><a href="#AWS-Simple-VPC-EC2-Instance" class="headerlink" title="AWS Simple VPC + EC2 Instance"></a>AWS Simple VPC + EC2 Instance</h1><p>Welcome back. In this demonstration, I’ll show you how to create a simple AWS VPC spanning two availability zones. Two public subnets will be created together with an internet gateway and a single route table. A T3.micro instance will be deployed and installed with Nginx for web serving. Security groups will be created and deployed to secure all network traffic between the various components. Lets begin.</p>
<p>Okay, so starting out in the following, CloudAcademy Github repo. If you want to follow along, then I highly recommend you clone this repo yourself locally. This repo contains four Terraform AWS infrastructure based exercises.</p>
<p>In this demonstration, we’ll perform exercise one. The AWS VPC architecture that we’ll build in this exercise is shown here. Nice and simple for our first example. The Terraform route module will consist of the following. Jumping into Visual Studio Code, you can see that have already git cloned the repo. As mentioned, the repo contains four exercises, and in this demo, we’ll focus on the Terraform templates and the exercise one folder. I’ll now proceed and open up the main.tf, outputs.tf, terraform.tf files and variables.tf files. Starting off with the main.tf file, I’ll highlight the AWS provider configuration.</p>
<p>Now, when it comes to building your own configurations, remember that you can always copy and paste this block from the terraform AWS online provider documentation. Most of the time, you will want to go with the latest version. With the AWS provider now configured, it’s time to jump into the terminal and initialize the Terraform working directory. To accomplish this, I’ll simply execute the Terraform init command. This would download the required plugins, in this case, the AWS is provider plugin.</p>
<p>Here you can see that this has now completed successfully. This is a one time operation that is required before you perform any Terraform plan or apply commands. Next I’ll do a directory listing to highlight the new .terraform directory and the .terraform.lock.hcl file that have been created as a result of executing the last command. I’ll now use the tree command to highlight the internal structure of the .terraform directory. Here we can see the AWS3.55 provider plugin binary that has been downloaded.</p>
<p>Okay, moving on, let’s examine the Terraform configuration within the main.tf file. The first resource block that we declare is that for the VPC. Here, we’re initializing it with the cidr block 10.0.0.0&#x2F;16. The largest VPC EDU space we can create with an AWS. I’ll also tag it with the following tags for identification purposes.</p>
<p>Next up, I’m creating two public subnets, which will be provisioned within the previously declared VPC. Subnet one will be created with the first &#x2F;24 block and subnet two will be created with the next second &#x2F;24 block. Subnet one will be deployed into the first AZ and subnet two will be deployed into the second AZ. Both AZs as used in this example, are stored in a list based variable named availability zones.</p>
<p>Taking a look at the variables.tf file, we can say each of the declared variables that the route module takes as imputs. Highlighting the availability zones variable, we can see that it is indeed typed as a list of string, but has no default value. Instead, the default value is passed in via the Terraform.tfrs file. Here we can see the two string values assigned to the availability zones or list, US West to A and US West to B.</p>
<p>Okay, jumping back into the main.tf file, the next resource I’ll highlight is the internet gateway. The internet gateway is required to facilitate internet traffic. Next, we declare a public route table containing a default route which will route outbound traffic through the internet gateway. This route table is then associated with both public subnets.</p>
<p>Next, we declare a security group to restrict inbound and outbound traffic to the EC2 instance that is to be later declared. This security group has two ingress rules and one egress rule. The first ingress rule allows inbound SSH traffic from my week stations parameter IP address. This IP address is stored in a Terraform variable named workstation IP.</p>
<p>Now, the default value for the workstation IP variable is set using an environment variable in the terminal. Within the terminal, I’ll run the command set pipe grip to search for, in capitals, TF_VAR. And here, we can see the value assigned to it. When the Terraform plan or reply command is executed, that will detect the presence of the environment variable and then use it within Terraform. The second ingress rule is used to allow inbound port 80 web traffic to the Xginx web server that we’ll install on the EC2 instance.</p>
<p>The single egress rule is required to allow the EC2 instance to connect out to the internet to pull down the Nginx package which’ll be installed. The final resource is the EC2 instance itself which will be bootstrapped, as mentioned, with the Nginx web service. That EC2 instance is configured using various settings stored within Terraform variables, such as it’s AMI, instance type, SSH key, subnet ID and security groups. Additionally, the EC2 instance is being configured to have a public IP address automatically assigned to it.</p>
<p>The Nginx web server is installed by virtue of configuring the user data attribute, which in turn, is configured with a multi line string containing the bash install screen. Note, the multi line sting uses heredoc format encapsulated within a pr of EOF strings.</p>
<p>Okay, at this stage, we’re ready to jump back into the terminal and perform a plan and apply. Before I do, I just want to highlight how the AWS user credentials are managed. For this example, I’m setting the credentials using environment variables set within the terminal, as seen here.</p>
<p>Right, we are ready to run a Terraform plan. In doing so, Terraform will generate an execution plan for us highlighting what will be created and or changed. Having reviewed this, we can proceed by running the Terraform apply command, and in this case, I will auto approve it by adding on the auto approve parameter. So here we can see that we have begun the AWS infrastructure provisioning process. The time it takes to complete is entirely dependent on the number and type of AWS resources being launched.</p>
<p>While this is happening, let me show you the Terraform extensions that I have set up within Visual Studio Code. The first extension that I have installed in the HashiCorp Terraform extension. Now, one of the customizations that I’ve applied on this extension is the format on save option, setting it to true. When doing so, this will have the extension automatically run the Terraform format command on your Terraform code within the file just saved. This is super useful as it keeps your code following best practices in terms of formatting and layout. For example, if I modify the main.tf file to have non standard formatting, when I save it, the Terraform extension will automatically apply best practice formatting, string quoting and indentation, et cetera.</p>
<p>Another thing to highlight is the snippet generation and intellisense options provided by the HashiCorp Terraform extension. Additionally, I can also pull up any number of snippets provided by the Terraform doc snippets extension, which I have also installed. In the example shown here, I’m entering the character sequence tf-AWS-resource to trigger the AWS snippets available. Each snippet has a preview of what to expect. In this example, if I go with the AMI snippet, I get the following AMI block pre configured with the commonly used attributes, et cetera. As mentioned, these snippets are provided by the Terraform doc snippets extension.</p>
<p>Okay, let’s do a little bit of clean-up and then head back over into the terminal as our Terraform apply command has now completed successfully. Here we can see that nine resources have been added and that we have several outputs indicating the subnet IDs, VPC ID and the public IP address assigned to the Nginx web server instance. These particular outputs have been declared within the outputs.tf file. Let’s now copy the public IP address and then perform a curl request to it. Excellent, our Nginx server is now up and running and has been able to respond to our HTTP git request. This is a great result.</p>
<p>Considering that all of the AWS VPC set up and networking configuration was performed automatically for us by Terraform. We can now confidently jump into our browser and test out the same address like so. And perfect, we get the default Nginx web page displayed. Heading over into the AWS console, we can examine the VPC section and view the newly provisioned VPC.</p>
<p>Likewise, the same for the EC2 instance. Here we can see the EC2 instance that is up and running. We can also navigate to the user data that was passed to it at launch time and indeed, that has the best script that has installed the Nginx web server, very cool. Back within the terminal, let’s run a Terraform refresh command. This will reconcile the local Terraform state with the actual infrastructure state, and again, print out the configured outputs. As expected, nothing has changed.</p>
<p>The next thing I will demonstrate is the concept of Terraform workspaces. Let’s first consult the workspace command help, but running the command Terraform workspace –help. Here we can see each of the workspace sub commands. Let’s examine the current list of workspaces. Here, we can see that we’ve just the default workspace.</p>
<p>Let’s now create a new workspace named test. When we create a new workspace, we are swapped into it automatically. We can confirm this by using the show command to display the currently active workspace and indeed, it is the test workspace. I’ll now run Terraform apply, auto approve to provision a new identical AWS infrastructure, albeit, managed within the local Terraform test workspace.</p>
<p>Again, the AWS provisioning has completed successfully, copying the latest public IP address for the new Nginx web server. We can test it out to see if it’s alive. Here, the HTTP request has failed. This is likely due to the fact that the Nginx web server is still in the process of warming up. That is completing its boot and installation processes. Repeating the curl command should eventually result in a successful HTTP response. Which it now does.</p>
<p>We can double check this by returning to our browser, and again, we are able to successfully pull up the Nginx web page. Jumping back into the AWS console, let’s check that we have additional AWS infrastructure. Here in the EC2 console, we can see the presence of another EC2 instance. And likewise, back in the VPC console, we have a second identically configured VPC, very cool.</p>
<p>Okay, at this stage, we are now finished with this exercise. To minimize our ongoing AWS costs, we can tear down the test workspace infrastructure by running the command, Terraform destroy. Returning to the AWS console, I’ll confirm that indeed, the test infrastructure has now been removed. We can also repeat the same Terraform destroy command in the default workspace. Let’s do this now. And again, confirming that our AWS resources for the default workspace have been successfully removed, which they have. Okay, that now completed this Terraform exercise.</p>
<h1 id="AWS-Advanced-VPC-ALB-EC2-Instances-v1"><a href="#AWS-Advanced-VPC-ALB-EC2-Instances-v1" class="headerlink" title="AWS Advanced VPC + ALB + EC2 Instances (v1)"></a>AWS Advanced VPC + ALB + EC2 Instances (v1)</h1><p>Welcome back. In this demonstration, I’ll show you how to create an advanced AWS VPC spanning two Availability Zones, which will have both public and private subnets, an internet gateway and net gateway will be deployed into it, public and private route tables will be established. An Application Load Balancer will be installed within it, which will load balance incoming traffic across an auto scaling group of NGINX web servers. Again, security groups will be created and deployed to secure all network traffic between the various components. Let’s begin.</p>
<p>As per the previous demonstration, all of the Terraform configuration, which will be demonstrated here, is available online, this time in the exercise two folder within the repo. The AWS VPC architecture that we’ll build in this exercise is shown here and is more advanced than the previous one by virtue of the VPC having public and private zones, combined with the introduction of an Application Load Balancer and an auto scaling group. Regardless, the Terraform configuration is still contained within a single route module.</p>
<p>Jumping into Visual Studio Code, I’ll open up each of the Terraform config files. Starting off in the main.tf file, again, we have the AWS provider, which will allow us to provision our AWS infrastructure. As with all Terraform projects, before we can provision actual infrastructure, we first need to run the Terraform init command to initialize our working directory, which I’ll do now.</p>
<p>Okay, that’s kicked off and initializing. While that is happening, let me explain the key configuration changes introduced into the main.tf file. Here, we have a data source which captures information about the available Ubuntu AMIs. We’ll leverage this later on in a launch template resource further down in this file. Next, is our VPC resource for establishing the VPC. The only difference here is that it now receives the CIDR block from a variable. If we look at the variables.tf file, we can see the variable that is used to store the CIDR block. The terraform.tfvars file actually holds the default value as seen here.</p>
<p>Now, within the VPC, we’ll establish public and private zones. Subnets one and two will be for the public zone and will have an attached route table that routes default traffic via an internet gateway. Subnets three and four, on the other hand, will be allocated to the private zone and will have an attached route table that routes default traffic through a managed net gateway. The subnet resource configuration demonstrates the use of calling an in-built Terraform function, in this case, the CIDR subnet function, to calculate the CIDR block for the subnet itself. By doing so, we can again, make our main.tf file more flexible for future requirements.</p>
<p>Now, to understand how the CIDR subnet function works, I will jump over into the terminal and fire up the Terraform console. Next, I can simply copy across the expression that uses the function and evaluate it. Here we can see that it has returned the CIDR block 10.0.1.0&#x2F;24. We can repeat this again for the second subnet. This time it returns 10.0.2.0&#x2F;24 and we can keep repeating this to understand how the CIDR blocks are being generated. This also demonstrates the usefulness of the Terraform console command.</p>
<p>Okay, moving down the main.tf file, next up is an elastic IP resource. This will be used by the following net gateway resource, which collectively allows privately zoned instances to route traffic out to the internet, which we will require since our auto scaling group of instances will reside in the private zone and they will need to call out to the internet to download the NGINX packages for web serving.</p>
<p>Next up is our route table configuration. Separate route tables are established, one for each of our zones. The public route table routes traffic via the internet gateway and the private route table routes traffic via the net gateway. The security group configuration has also been modified. Here we have separate security groups for both the web fleet and the Application Load Balancer. The web service security group, as seen here, should actually have it’s second ingress port 80 rule more restrictive to allow only inbound port 80 from the Application Load Balancer nodes.</p>
<p>For the record, I’ll now make this change and commit it back to the repo for your benefit. The Application Load Balancer security group allows all inbound port 80 traffic from the internet. Additionally, it is required to have an egress rule to allow it to forward downstream traffic to the web fleet.</p>
<p>Next up, we have configured an AWS launch template resource. This represents the launch configuration requirements for the web fleet that are managed within an auto scaling group and sit behind the Application Load Balancer in the private zone. The AMI ID is pulled from the Ubuntu data source that we earlier reviewed towards the top of this file. The network interface is block declared here, is used to attach the security group into explicitly disabled public IP address assignment.</p>
<p>Now, since recording this demo, I have replaced the network interfaces block in favor of using the VPC security group IDs attribute to attach to the security groups. This is more stable for the overall infrastructure when you reapply any updates through Terraform. The launch template finally configures user data to bootstrap the instances with the NGINX web server. In this example, the actual bootstrapping script is stored externally in its own file and is pulled in using the inbuilt filebase64 function, which in turn uses string interpellation to inject the current module path. Here we can see the actual contents of the referenced ec2.userdata file.</p>
<p>Next up, we have the Load Balancer. It’s configured to be an external facing Application Load Balancer. It’s configured with the ALB security group and is deployed across the two public subnets spanning both Available Zones for availability purposes. We then establish a web server target group with the target group port being set to port 80, the NGINX default listening port. Equally, on the Application Load Balancer itself, it’s configured to also listen on port 80, such that it simply forwards traffic from port 80 down to port 80. This is configured via both a default action configured directly on the Application Load Balancer’s listener itself and via a single listener rule, whose only condition is to forward the route path to the same target group.</p>
<p>Finally, an auto scaling group resource is configured to span the two privately zoned subnets, subnet three and subnet four. The desired min and max settings are all set to two, which will result in two instances always at runtime. Done so for demonstration purposes only. The auto scaling group references the earlier reviewed launch template and is configured to register its instances as targets within the target group. </p>
<p>Okay, with the route module review now complete, let’s head over to the terminal and launch the infrastructure by executing Terraform apply. Okay, that has now completed successfully and we have several outputs printed out for our viewing. They include the Application Load Balancer DNS, the subnet IDs for each of the four subnets and the VPC ID. I’ll copy the Application Load Balancer DNS value and then call for an HTTP response, using the -Y to indicate that I’m only interested in the HTTP headers for now. And excellent, it retains a HTTP 200 response code, indicating success and that the response appears to have originated from an NGINX web server, which is what we would expect.</p>
<p>From here, I’ll jump over into my browser and browse to the Application Load Balancer, like so. And what would you know, we have a valid response back from our auto scaled group of NGINX web servers via the Application Load Balancer. A top result. To round out this exercise, I’ll now examine the AWS infrastructure as just provisioned. In the EC2 console, we can see that we indeed have two web server instances. </p>
<p>Navigating to the Load Balancer section, we can see our newly provisioned Application Load Balancer with the same DNS address, which we just used to browse to. We can also see that it has a single HTTP port 80 listener and if we click on the view rules link, we can observe the listener rules we have configured. In this case, our custom rule and the default rule both forward traffic downstream to the same target group, which in our case, is the web server auto scaling group.</p>
<p>Drilling into the target group, we can see that the target group has successfully registered both EC2 instances and they importantly, both are registered as healthy. Okay, that now concludes this demo. If you’ve been following along, please don’t forget to perform a Terraform, destroy to tear down your AWS resources.</p>
<h1 id="AWS-Advanced-VPC-ALB-EC2-Instances-v2"><a href="#AWS-Advanced-VPC-ALB-EC2-Instances-v2" class="headerlink" title="AWS Advanced VPC + ALB + EC2 Instances (v2)"></a>AWS Advanced VPC + ALB + EC2 Instances (v2)</h1><p>Welcome back. In this demonstration, I’ll show you how to create the same AWS architecture as used in the previous demonstration. But in this demonstration, I’ll refactor the Terraform templates to use the count meta argument for configuring the public and private subnets, as well as their respective route tables, thereby simplifying the overall Terraform configuration. Let’s begin.</p>
<p>As per the previous demonstrations, all of the Terraform configuration, which will be demonstrated here, is available online, this time in the exercise three folder within the repo. The AWS VBC architecture that we’ll build in exercise three, is identical in structure to the AWS VPC architecture as used within exercise two, the previous exercise. The purpose of this demonstration is to show you an alternative approach to provisioning multiple similar resources. In this case, our subnets and route titles, et cetera. The key technique as used here is based on using Terraform’s resource count meta argument to dynamically create multiple similar resources for us, as will be used to create our subnets and route tables, for example. Jumping into Visual Studio Code, I’ll open up just the main.tf file since. This file is the only one that has modifications within it, compared to exercise two. Again, I need to perform an initialization of the current working directory, which I’ll do now. And then, while that is initializing, I’ll explain the modifications.</p>
<p>Jumping down to where the subnets are declared, you can see that we now only have two subnet resource blocks. Whereas previously we had four. The first subnet block declares a subnet per AZ for the public zone. The second subnet block declares a subnet per AZ for the private zone. Now the key attribute that makes this possible is what’s referred to is the count meta argument declared within both subnet resource blocks. The count value here is being derived by using the inbuilt length function to determine how many AZ’s have been declared within the availability zones variable. To make this clearer, let’s fire up the Terraform consult and examine the related expressions.</p>
<p>Firstly, I’ll examine the contents of the availability zones variable. Here we can see that it contains two values, US West two A and US West to B. Now, if we use the link functional in this variable, we expectedly get the value to returned. Next, the inbuilt cidr of subnet function is used to dynamically calculate the cidr block for the current subnet. And it does so by making use of the count.index, which is zero based and X as an index for the current resource.</p>
<p>In the scenario, Terraform creates two public subnet resources for us, even though we’ve only declared a single public subnet block. This is a language feature of Terraform and one that helps us to write less configuration, less is better, but for which is also more dynamic. To help with this explanation, let’s copy of the cidr sub-net function into the Terraform console, and then evaluate it for various values of count on index, starting with zero, then one and then two.</p>
<p>Here, we can see that the first public subnet will have the cidr 10.0.0.0&#x2F;24 and the second one will have 10.0.1.0&#x2F;24. I’ll also do the same for the element and built function, just for the sake of clarity. Note that the element function, which is being used to return a single AZ actually wraps around when the index is greater than the number of elements which exist in the list being indexed. The same element based expression is then also used here to interpolate the evaluation into the meta tag configured on the sub-net resource.</p>
<p>So, this should now make it clear how a single resource blog can be provisioned multiple times. The same configuration is used to configure the private subnets. The only difference is that the cidr sub-net function starts two&#x2F;24 blocks along, caused by the addition of the length of the AZ list. Therefore, as you can see, our two private subnets will be 10.0.2.0&#x2F;24 and 10.0.3.0&#x2F;24.</p>
<p>Okay, moving on, scrolling down to the net elastic IP resource, we can observe that it has been updated to use the count meta argument as well, which in this case will cause it to provision two EIPs, one per AZ. Each of which will be referenced within the immediately following net gateway resource, which as you might expect, also makes use of the count meta argument to create a net gateway in each AZ.</p>
<p>In the net gateway resource, the subnet ID key is set to take on one of the public subnet IDs. Here, the public subnet IDs are returned by making use of the split notation. That is the asterix character. This basically returns all matching public subnet IDs. The same syntax is used to wire up the elastic IPs from the previous resource block, assigning one to each of the net gateways.</p>
<p>Moving on down, a public route table as established, which contains a single default route entry for internet traffic to be sent out through the internet gateway. This public route table is then attached to all public subnets, again, by leveraging this split notation. Likewise, private route tables are created, two of them, one for each AZ. Each private route table has a default route entry, which routes outbound internet traffic out through a net gateway, which is co located in the same AZ. Since in this architecture, we now have AZ independent net gateways configured. Most of the remaining Terraform configuration from here down to the application load balancer remains the same as per the previous demonstration. The application load balancer resource is updated such that its nodes are deployed into the public subnets, which again are identified and using the previously explained split notation. And again, likewise for the auto scaling group. Here we can observe the VPC zone identifier has been updated. With the private subnets being identified, again, using split notation.</p>
<p>Okay, with all of these updates in place, let’s now build the AWS infrastructure. To do so, I’ll run the Terraform apply command to launch the infrastructure. Okay, I’ll speed it up to the point where it completes. Again, we get confirmation that our resources have been successfully created. And for our convenience, we have the various outputs available. This time, I’ve updated the outputs to print out the subnet IDs and cidr blocks for both the public and private subnets. We can see how this is accomplished by viewing the outputs.tf file Here we can see the use of the split notation to grab the subnet IDs and the cidr blocks for the two public subnets. The same goes for the private subnets.</p>
<p>Let’s finally confirm that again, we get a valid HTTP response back from our application load balancer. I’ll copy the updated DNS address and browse to it. And perfect, everything works again as per the last example. Okay, that concludes this demo. Again, if you’ve been following along, don’t forget to perform a Terraform destroy to tear down your AWS resources.</p>
<h1 id="AWS-Advanced-VPC-ALB-Cloud-Native-Application"><a href="#AWS-Advanced-VPC-ALB-Cloud-Native-Application" class="headerlink" title="AWS Advanced VPC + ALB + Cloud Native Application"></a>AWS Advanced VPC + ALB + Cloud Native Application</h1><p>Welcome back. In this demonstration I’ll now show you how to create an advanced AWS VPC to host and support a fully functioning cloud native application. The VPC in question will span two availability zones and have public and private subnets and internet gateway and managed net gateway will be deployed into it. Public and private route tables will be established. An application load balancer will be installed, which will load balance incoming traffic across an auto scaling group of nginix web service installed with the cloud native application front-end and API. A database instance running Mongo DB will be installed in the private side. Security groups will be created and deployed to secure all network traffic between the various components. For demonstration purposes only the cloud native application that we will deploy, which consists of the front-end API components will be deployed such that both components are on the same set of easy two instances. This is done so to reduce running costs only.</p>
<p>Okay, let’s begin. As per the previous demonstrations, all of the Terraform configuration, which will be demonstrated here is available online this time in the exercise four folder within the repo, those particular demonstration will show you how to deploy a fully working end-to-end deployment of a cloud native application. This wave app allows you to vote for your favorite programming language, with the votes being collected and stored in a MongoDB database. The voting data is seen from the front-end to an API using Ajax calls. The API will in turn read and write to the MongoDB database, an auto scaling group of app servers will be provisioned. Each app server will have both the front-end and API components deployed onto it. During provisioning time, each app server will pull down the latest release of the front-end and API components from git hub. Jumping across and to the front-end git hub repay, you can see the source code that makes up the react based front-end, the lightest fronting release that gets pulled down and installed onto the instances will be this one.</p>
<p>Next I’ll pull up the API, git hub repo. This contains the API as used by the vote app. The API is written and go, and as earlier mentioned, provides an interface to read and write the voting data into and out of the MongoDB database. Again, app provisioning time, the app instances, they get launched. We’ll pull down and install the latest release of the API found here. For your benefit, the actual bootstrapping user data script, as used to bootstrap the app instances is documented directly here. It’s encoded within a Terraform team plight cloud and at conflict block, it is used to first install nginix for web serving, then it pulls down the latest front-end release from git hub installing it into nginix default seeming directory, Next, it pulls down the lightest API release from GitHub and starts it up, pointing it at the MongoDB instances assigned private IP address connecting on port 27017.</p>
<p>The next diagram is seen here highlights the application load balances, target groups set up. Here we are seating up two target groups, one for the front-end and one for the API. The front-end target group will listen on port 80 whereas the API target group will listen on port 8080. The application load balancer itself will be configured to listen on port 80 and will forward traffic downstream to either of the target groups based on some forwarding roles that will be configured. More on this later.</p>
<p>The end result is that incoming requests will be layed balanced across the auto scaling group, which spans across two availability zones for high availability purposes. Now from a VPC point of view, the architecture that will build and leverage within this demonstration will again be the same as used in the previous three demonstrations, the VPC will span two AZs and have public and private zones. But this time the VPC and underlying networking components will be declared using AWS’s VPC Terraform module available within the public Terraform registry.</p>
<p>Within this exercise, the key Terraform objective that I want to demonstrate is to show you how to modularize your Terraform configurations. As seen here, the project structure for this demonstration will be the following. When your Terraform configurations become large and complex modularizing them helps to make them more maintainable and readable.</p>
<p>Okay, jumping into visual studio code within the terminal pain are on the tree command to again, highlight the project structure. Here we can see the route modules main.tf file, and then beneath it, we have a modules directory. Within this directory, we have several child modules. We have an application module, a bastion module, a network module, a security module, and a storage module. Back within the root module directory, we also have a variables.tf file, a terraform.tfvars file for default variable values and an output.tf file.</p>
<p>Let’s now take a look at the root modules main.tf file. This is our terraform entry point. Within it, we can see how references to the child modules are made starting with the network module, which has been reflected out to contain all the VPC networking conflict. Next is the security module, this has also been reflected out to contain all of the security grid configuration. Note, that the security module has a dependency configured on the networking module since it needs and references the VPC ID, which is configured as an output on the networking module.</p>
<p>It’s important to note that referencing values from other modules can only be done so if that other module creates an output for it. We can see that this is the case for the network module, by opening up its output.tf file and observing the fact that it has an output named VPC_ID. While in the network module, let’s look at its main.tf file, here we can see that the enclosed configuration is very concise. In fact, it is contained all within a single VPC module block. This is one of the very cool things when working with custom modules. It is your ability to abstract away, a lot of the underlying configuration. In this case, we simply configure the public subnet cider blocks, the private subnet cider blocks. We enable the net gateway seating, and then that’s enough for Terraform to be able to go away and create LVC subnets, routing tables, routing associations, internet gateway, net gateways, et cetera. Using this approach is very clean and super productive.</p>
<p>Okay, let’s return back to the root modules main.tf file. Next stop is the bastion module. This module is designed to launch a jump box, to allow us to connect to the privately zoned instances. Our ASG at fleet and MongoDB database, the bastion will be deployed into the first public subnet and will be secured with the best UN security grape created within the security module. Opening the bastion modules main.tf file, the key configure items to call out are, it explicitly declares the ami ID to be this value, which in this case is an Ubuntu 20.04 image. And that it also requires the instance to have a public IP address automatically associated with it hitting back to the route modules, main.tf file.</p>
<p>Next up is the storage module. This encapsulates the conflict for the MongoDB database, jumping into the storage module and looking inside its main.tf file. We can see a similar conflict to that used for the best year. However, in this case, we obviously don’t need a public IP address assigned to it. The Mongo instance leverages user data to install and configure the MongoDB service onto itself. The user data is pulled in by calling the inbuilt function filebase64, which reads in the contents of the install.sh file.</p>
<p>Opening the install.sh file we can see the commands required to download and install the MongoDB package. We then passed out some configuration to the file system and start up the MongoDB service. We then generate a db.setup.js file containing sample data, which when called upon will used to populate the MongoDB database. Now the setup for our database will be sufficient for demonstration purposes, but in a production environment, you’d likely want to configure a MongoDB and a replica CIT and perhaps have its data volumes stored on EBS provisioned with high ops for better performance, scalability, availability, and redundancy purposes.</p>
<p>Moving back to our root modules main.tf file one last time we have the application module, which contains all resources related to the application itself. In this case, it contains the application load balancer, the auto scaling group, the launch template, et cetera, et cetera. This module clearly has dependencies on the network, security and storage modules. And therefore has these dependencies explicitly declared in the depends on list at the bottom.</p>
<p>Let’s now jump over into the application modules main.tf file. Here we can see that it starts off with a data source of the Ubuntu 20.04 images. This is later reference within the launch template block for the day down in the file. Next up is the template cloud in a conflict resource. This contains the same script documented in the rebate within the exercise for folder. Again, I’ll highlight a couple of the more important parts of this template resource which once rendered is used as user data for the app instances.</p>
<p>On lines 33 and 34 string interpolation is used to embed two environment variables within the script. The first is the application load balances, fully qualified domain name. This is required by the front-end being used to tell the browser where to aim the voting API Ajax requests to. The second is used to configure the API service with the MongoDB databases private IP address, allowing it to know where to read and write data to.</p>
<p>Lines 39 to 51 are used to pull down the latest release of their react based front-end and unpack it unto the default engineer assuming directory. Lines 53 to 62 are used to pull down the latest release of the compiled API. Note how it references the MongoDB private IP address environment variable previously configured on line 34.</p>
<p>Finally, with the fronting and API components installed and ready, the nginix web server is started up. Next up is the app launch team plate. Nothing too special going on here other than the fact that the user data is configured by calling the inbuilt by 64 and code function, which types the rendered output of the previous cloud and a template and retains the base 64 encoded version of it.</p>
<p>Next up is the application load balancer resource. Again, nothing special here to call out other than the fact that it is an application load balancer. Behind the application load balancer are two target groups, one for the front-end nginx web server configured on port 80 with the following health cheat configuration. The second target group is for the API, which has configured to listen on port 8080. The API target groups health check is configured to send its health checks to the &#x2F;ok endpoint specific to the API, the application load balancer is configured with a single HTTP port 80 listener. This is useful both the fronting and API requests on the outside.</p>
<p>On the inside two listener rules are created, one for the front-end HTTP requests, and one for the API Ajax requests, the front-end HTTP listener role forwards to the front-end target group and the API HTTP listener role forwards to the API target group and has a lower priority value of 10, meaning it gets precedence. This has done site that we can detect any incoming API calls as per the condition configuration, and then forward these directly to the API target group.</p>
<p>Moving onto the auto scaling group resource, this is configured in much the same way as it was in the previous demos. The only difference here is that it is configured to register its instances into both fronting and API target groups. Additionally, the auto scaling group references the earlier launch template that was configured and purposely grabs the latest version of it. Finally, a new data sources configured to scan for any easy two instances that are tagged with the following tags, which map exactly to the tags specified in the app servers launch template. We filter all instances, which are either an IP ending or running site. This data source is set up with a dependency on the auto scaling group resource.</p>
<p>Now, the reason for this last data resource is to be able to report back to the terminal, the auto scaling group instances, private IP addresses. Looking backwards, if we first look at the application modules output.tf file we can see that it contains a private_ipsoutput, which references the instance data source mixed going up and out to the root modules, output.tf file we can see that it has an output named application private IPS, which references the application modules, private IPS output.</p>
<p>Okay, let’s now proceed and launch the setup. To do so, I need to first initialize our working directory using Terraform in it, once initialization is complete, I’ll proceed with the Terraform apply command. Okay, fast forwarding to the point where the apply command has now completed successfully. Here, we can say the outputs, including the application load balancer FQDN the application private IPS, the bastion public IP and the MongoDB private IP, I’ll take a copy of the application load balancer FQDN and cool for it to see if we get back a valid response, and so far so good.</p>
<p>Here we are receiving an HTTP 200 response code indicating success. However, the acid test is to call it up within our browser like so, and excellent, how good is this? The Vote App has successfully loaded within the browser. I can vote on various languages, which when I do results in Ajax calls going back to the application load balancer on port 80 and with the application load balancer forwarding them downstream to the API target group listening on port 8080, we can view this traffic by bringing up the browser’s developer tops and capturing the network traffic generated when clicking on any of the Vote buttons. Clicking on the vote network requests, we can then view the HTTP request and response heaters associated with it.</p>
<p>Moving on, let’s take a look at the IWCC two console and view the instances running. Here, we can see four instances, the Mongo database instance, the bastion instance and the two auto scaling group, front-end at managed instances. Peering into the light balances section, we can see our application load balancer. Drilling into the listeners, we have the one port 80 listener configured. Clicking on it’s rule set, we can see the three rules that have been established. The highest priority role is used to capture and re-iterate browser initiated Ajax requests directly to the API target group. The remaining lower priority roles are used to capture all other traffic and route it to the front end target group.</p>
<p>Navigating back to target groups, we can observe both the API target group and the front-end target group. The API target group has configured on port 8080 and the front-end target group is configured on port 80. Drawing into the API target group. We can see that it has successfully registered the two ASG managed app instances, and they’re both of them are in a healthy state, which we require. Then drawing into the front-end target group, again, we can observe the same two instance IDs have been registered successfully.</p>
<p>Next let’s take a quick look at the VPC set up that has been provisioned by Terraform for us. Within the VPC console, we can see the cloud academy nine VPC. If we jump over into the subnets, we see the four subnets, two public and two private. This slide of blocks match those as explicitly configured and widened to the network modules, main.tf file on lines eight and nine. Within the NAT gateways view, if we filter on those that are in an available state, we can see that they are two, one per AZ.</p>
<p>Let’s now jump back into the terminal and we’ll bounce over into the MongoDB instance to review the data that’s been captured within it. To do this easily, I’ll load the cloud academy demo ssh private key into my local ssh agent, and then use ssh- A parameter, enabling at the indication forwarding, and then at the indicate into the bastion host from here, I’ll bounce over to the MongoDB instance like so.</p>
<p>Now that I’m on the MongoDB instance, all far up the Mongo client using the Mongo command or swap into the Lang DB database and then execute the db.languages.fine.pretty command to display the data currently hold in the languages table. And here indeed, we can see that the MongoDB database has captured our voting data that we seem to it. Let’s now generate a few more votes within the browser. And then again check for the capture data back within the database, and excellent the data has been successfully transacted within MongoDB.</p>
<p>Okay, that concludes this demo. As per the other exercises, if you’ve been following along, please don’t forget to perform a final Terraform destroy to tear down your AWS resources.</p>
<h1 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h1><p>Okay, congratulations. That now completes this course. I’d like to personally thank you for putting time aside to take this course. I trust that you found it, not only informative, but helpful for your AWS Infrastructure provisioning job role.</p>
<p>Finally, I’d like to reiterate any and all feedback that you may have is welcomed. If you have any unanswered questions about this course, or simply would like to see and hear of other related Terraform features, then please reach out, either by our <a href="mailto:support@cloudacdemy.com">support email</a>, as seen here, or simply hit me up on LinkedIn. All right, that’s all for now. Best of luck with your Terraform adventures ahead.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Preview-Exam-Docker-Certified-Associate-DCA-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Preview-Exam-Docker-Certified-Associate-DCA-13/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Preview-Exam-Docker-Certified-Associate-DCA-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:45" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:45-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:24:26" itemprop="dateModified" datetime="2022-11-20T22:24:26-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Preview-Exam-Docker-Certified-Associate-DCA-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Preview-Exam-Docker-Certified-Associate-DCA-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p><object data="Preview-Exam-Docker-Certified-Associate-DCA.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Installing-and-Running-Applications-with-Docker-Enterprise-Universal-Control-Plane-UCP-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Installing-and-Running-Applications-with-Docker-Enterprise-Universal-Control-Plane-UCP-12/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Installing-and-Running-Applications-with-Docker-Enterprise-Universal-Control-Plane-UCP-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:43" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:43-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:24:38" itemprop="dateModified" datetime="2022-11-20T22:24:38-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Installing-and-Running-Applications-with-Docker-Enterprise-Universal-Control-Plane-UCP-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Installing-and-Running-Applications-with-Docker-Enterprise-Universal-Control-Plane-UCP-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-11/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Introduction-to-Kubernetes-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:41" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:41-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:23:04" itemprop="dateModified" datetime="2022-11-20T22:23:04-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to the introduction to Kubernetes Course. Kubernetes is a production grade container orchestration system that helps you maximize the benefits of using containers. Kubernetes provides you with the toolbox to automate the deployment, scaling, and operation of containerized applications in production.</p>
<p>In this course, we’ll teach you all about Kubernetes, including what it is and how to use it. Before we get into it, allow me to introduce myself. I’m JT Lewey and I’ll be your trainer for this course. I’m a content researcher and developer here at Cloud Academy. And I hold both the certified Kubernetes application developer certification, as well as the certified Kubernetes administrator certification. So feel free to reach out to me about either of those topics or other general DevOps questions you have.</p>
<p>Let’s talk about who should attend this course. This course is suitable for those who are looking to deploy containerized applications. This is especially useful if you already have an existing system and are evaluating deployment options. Container orchestration skills are relevant for the following positions, DevOps engineers, cloud engineers, site reliability engineers, and as well as just anybody who is a container enthusiast and looking to beef up their container orchestration skills.</p>
<p>In this course, we’re gonna be covering three main topics. And our first is gonna be an overview of Kubernetes, specifically addressing, what is it? Why is it so successful? And how can you start? From there, we’re gonna be deploying containerized applications into Kubernetes. And this involves a hands on approach. Specifically with microservices. I’ve created a <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/intro-to-k8s">GitHub repository</a> that has all the available files to you should you wish to follow along. I also suggest that you spin up our intro to Kubernetes lab where it has all the available GitHub repo files as well as a full Kubernetes environment.</p>
<p>The last topic we’re gonna be discussing, Personal Topics, I recommend you be introduced to should you wish to know more about Kubernetes. Let’s identify our concrete learning objectives. You will know these after you complete this course. Specifically, you’re gonna be able to describe Kubernetes and what it is used for. You’re going to be able to deploy single and multi container applications onto Kubernetes. You will be able to use Kubernetes services to structure any number of applications and you’ll be able to manage those applications through deployments and rollouts. You’ll also be able to ensure container pre-conditions are met and that these containers are kept healthy. You’ll be able to manage configuration maps, secrets, and how to control persistent data within Kubernetes. And lastly, you’re gonna be able to discuss the popular tools and how they can benefit you in your Kubernetes journey.</p>
<p>There are some prerequisites to this course, and to get the most out of it, you should have a solid understanding of Docker. But don’t worry. We have courses available if you’re interested in learning Docker and would like to take those before this. Next, you should have a solid understanding of YAML. But don’t worry, as it’s fairly easy to pick it up as we go. And lastly, we’re gonna be establishing a Kubernetes cluster. So any Kubernetes experience will obviously help you in learning more about the basics and more about how to configure Kubernetes.</p>
<p>My name’s Jonathan Lewey from Cloud Academy. And I’m so excited for you to take this introduction course. If you have any questions or concerns, please feel free to reach out to me on LinkedIn or <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. So enough’s enough. Let’s get into it.</p>
<h1 id="Kubernetes-Overview"><a href="#Kubernetes-Overview" class="headerlink" title="Kubernetes Overview"></a>Kubernetes Overview</h1><p>This lesson will provide a high level overview of Kubernetes. We will cover what you can do with Kubernetes including some of the core features that have driven Kubernetes success. We will also discuss the competitive landscape around Kubernetes. Kubernetes, often abbreviate as K8s, is an open-source container-orchestration tool designed to automate, deploying, scaling, and the operation of containerized applications.</p>
<p>Kubernetes was born out of Google’s experience running workloads in production on their internal Borg cluster manager for well over a decade, it is designed to grow from tens, thousands, or even millions of containers. Organizations adopting Kubernetes increased their velocity by having the ability to release faster and recover faster with Kubernetes self healing mechanisms. Kubernetes is a distributed system. Multiple machines are configured to form a cluster. Machines may be a mix of physical and virtual and they may exist on-prem or in cloud infrastructure each with their own unique hardware configurations.</p>
<p>Kubernetes places containers on machines using scheduling algorithms that consider available compute resources, requested resources priority, and a variety of other customizable constraints. Kubernetes is also smart enough to move containers to different machines as this machines are added or removed. Kubernetes is also container runtime agnostic which means you can actually use Kubernetes with different container runtimes.</p>
<p>Kubernetes most commonly uses Docker containers but can also be used with Rocket containers, for example. This kind of adaptability is a result of Kubernetes modular design. It also has a lead to Kubernetes widespread adoption and made Kubernetes one of the most active open source projects around Kubernetes also provides excellent end user abstractions by using declarative configuration for everything. Engineers can quickly deploy containers, wire up networking, scale and expose the applications to the real world. We’ll cover all of these features throughout the lesson.</p>
<p>Operation staff are not left in the dark either. Kubernetes can automatically move containers from failed machines to running machines. There are also built-in features for doing maintenance on a particular machine. Multiple clusters can also join up with each other to form a Federation. This feature is primarily for redundancy, such that, if one cluster dies, containers will automatically move to another cluster.</p>
<p>The following features also contribute to making Kubernetes a top choice for orchestrating containerized applications: the automation of deployment rollout and rollback, seamless horizontal scaling, secret management, service discovery and load balancing, support for both Linux and Windows containers, simple log collection, stateful application support, persistent volume management, CPU and memory quotas batch job processing, and role-based access control. With the popularity of containers, there’s been a surge in tools to support enterprises adopting containers in production. Kubernetes is just one example.</p>
<p>So let’s compare Kubernetes with some other tools because now that we know what Kubernetes can do it’s sometimes useful when we can compare one technology to another. We’ll compare DCOS, Amazon ECS, and Docker Swarm Mode, each has their own niche and unique strength. This section will help you understand Kubernetes approach and decide if it fits your particular use cases.</p>
<p>DCOS or Distributed Cloud Operating System is similar to Kubernetes in many ways DCOS pools compute resources into a uniform task pool, but the big difference here is that DCOS targets many different types of workloads including, but not limited to, containerized applications. This makes DCOS attractive to organizations which are not using containers for all of their applications. DCOS also includes a Package Manager to easily deploy it to his systems like, Kafka or Spark. You can even run Kubernetes on DCOS given its flexibility for different types of workloads.</p>
<p>Amazon ECS, or the Elastic Container Service is AWS’ ability to orchestrate containers. ECS allows you to create pools of compute resources and uses API calls to orchestrate containers across them. Compute resources are EC2 instances that you can manage yourself or let AWS manage them with AWS Fargate. It’s only available inside of AWS and generally, less feature compared to other open source tools. So it may be useful for those of you who are deep into the AWS ecosystem.</p>
<p>Lastly, Docker Swarm Mode is the official Docker solution for orchestrating containers across a cluster of machines. Docker Swarm Mode builds a cluster from multiple Docker hosts and distributes containers across them. It shows a similar feature set with Kubernetes or DCOS. Docker Swarm Mode works natively with the docker command. This means that associated tools like Docker Compose can target Swarm Mode clusters without any changes.</p>
<p>Docker Enterprise Edition leverages Swarm Mode to manage an enterprise-grade cluster. And Docker also provides full support for Kubernetes if you want to start out with Swarm and later swap over to Kubernetes. So if you’re not already fixed on you using Kubernetes I would recommend that you conduct your own research to understand each tool and its trade-offs. Cloud Academy has content for each option to help you make the right decision.</p>
<p>In the next lesson, we’ll go through some of our options for deploying to Kubernetes. So I’ll see you there.</p>
<h1 id="Deploying-Kubernetes"><a href="#Deploying-Kubernetes" class="headerlink" title="Deploying Kubernetes"></a>Deploying Kubernetes</h1><p>Once you’ve decided on Kubernetes, you have a variety of methods for deploying Kubernetes. This course focuses on the core concepts. But because it is only natural to ask how to get started using Kubernetes, this short lesson discusses some of your options for deploying Kubernetes.</p>
<p>Deploying Kubernetes single-node cluster. For development and test scenarios, you can run Kubernetes on a single-machine. Docker for Mac and Docker for Windows, both include support for running Kubernetes on the local machine in a single-node configuration. Just make sure Kubernetes is enabled in the settings. This is the easiest way to get started if you already have Docker installed.</p>
<p>Another option is to use minikube which supports Linux in addition to Macs and Windows. Lastly, Linux systems can use kubeadm to set up a single-node cluster. Kubeadm is used as a building block for building Kubernetes clusters, but it can effectively create single-node clusters. But be aware that kubeadm will install Kubernetes on the system itself rather than a virtual machine, like the prior methods.</p>
<p>Single-node clusters are also useful within continuous integration pipelines. In this use case, you want to create ephemeral clusters that start quickly and are in a pristine state for testing applications in Kubernetes each time you check a new code. Kubernetes in Docker, abbreviated K-in-D or kind is made specifically for this use case.</p>
<p>Deploying Kubernetes multi-node cluster. For your production workloads, you want clusters with multiple nodes to take advantage of horizontal scaling and to tolerate node failures. To decide what solution works best for you, you need to ask several key questions including, “How much control do you want over the cluster versus the amount of effort you are willing to invest in maintaining it?”</p>
<p>Fully-managed solutions free you from routine maintenance but often lag the latest Kubernetes releases by a couple of version numbers for consistency. New versions of Kubernetes are released every three months. Examples of fully-managed Kubernetes as a service solutions include Amazon Elastic Kubernetes Service or EKS, Azure Kubernetes Service or AKS, and Google Kubernetes Engine or GKE.</p>
<p>To have full control over your cluster, you should check out kubespray, kops, and kubeadm. The next question is, “Do you already have investment into and expertise with a particular cloud provider?” Cloud provider’s managed Kubernetes services integrate tightly with other services in their cloud. For example, how identity and access management is performed. There will be a lot less friction to staying close to what you already know.</p>
<p>After that, we have, “Do you need enterprise support?” Several vendors offer enterprise support and additional features on top of Kubernetes. These can include OpenShift by RedHat, Pivotal Container Service, or Rancher.</p>
<p>Another question to consider is, “Are you concerned about vendor lock-in?” If you are, you should focus on open source solutions, like kubespray and Rancher that can deploy Kubernetes clusters to a wide variety of platforms.</p>
<p>Some other questions that are not important are, “Do you want the cluster on-prem, in the cloud, or both?” Because Kubernetes provides users with an abstraction of cluster of resources to the underlining nodes that can be running in different platforms. Kubernetes itself is at the core of open source hybrid clouds. Even cloud vendor Kubernetes solutions allow using on-prem compute. For example, GKE on-prem lets you run GKE on-premise, EKS allows you to add an on-premise nodes to the cluster, and Azure Stack allows you to run AKS on-prem.</p>
<p>Another question to consider is, “Do you want to run Linux containers, Windows containers, or a mix? To support Linux containers, you need to ensure you have Linux nodes in your cluster. To support Windows containers, you need to ensure that you have Windows nodes in your cluster. Both Linux and Windows nodes can exist in the same cluster to support both types of containers.</p>
<p>All that being said, in the context of this course, Cloud Academy has you covered for the following along with a course using a real multi-node cluster. The introduction to Kubernetes playground lab provides the same cluster that will be used during this course. So if you want to follow along without setting up your own cluster, go ahead and start that lab now and feel free to use any other cluster if you’d like to.</p>
<p>In the next lesson, we’re going to be covering the Basics of Kubernetes Architecture. Continue on when you are ready.</p>
<h1 id="Kubernetes-Architecture"><a href="#Kubernetes-Architecture" class="headerlink" title="Kubernetes Architecture"></a>Kubernetes Architecture</h1><p>This lesson will cover Kubernetes Architecture. What we cover here will be enough to understand and reason about topics we’ll learn later in this course. It is intended to build a strong foundation rather than to be an exhaustive review. Kubernetes itself is a distributed system. It introduces its own dialect to the orchestration space. Internalizing the vernacular is an important part of success with Kubernetes. And we will define several terms as they arise but know that there is also a Kubernetes glossary available in the introduction to Kubernetes learning path such that you have a single point of reference for the terms you need to know and more comprehensive glossary maintained by Kubernetes is also linked from there.</p>
<p>You must also understand the architecture to have a basic understanding of how features work under the hood. The Kubernetes cluster is the highest level of abstraction to start with. Kubernetes clusters are composed of nodes and the term cluster refers to all of the machines collectively and can be thought of as the entire running system.</p>
<p>The machines in the cluster are referred to as nodes. A node may be a VM or a physical machine. Nodes are categorized as worker nodes or master nodes. Each worker node include software to run containers managed by Kubernetes control plane and the control plane runs on master nodes. The control plane is a set of APIs and software that Kubernetes users interact with. These APIs and software are collectively referred to as master components.</p>
<p>The control plane schedules containers onto nodes. So the term scheduling does not actually refer to time in this context. Think of it from a Kernel perspective the Kernel schedules processes onto CPU’s according to multiple factors. Certain processes need more or less compute or may have different quality of service rules. Ultimately the scheduler does its best to ensure that every container runs. Scheduling in this case refers to the decision process of placing containers onto nodes in accordance with their declared compute requirements.</p>
<p>In Kubernetes containers are grouped into Pods. Pods may include one or more containers. All containers in a Pod run on the same node. And the Pod is actually the smallest building block in Kubernetes. More complex and useful abstractions sit on top of Pods. Services, define networking rules for exposing Pods to other Pods or exposing Pods to the internet. And Kubernetes also uses deployment to manage the deployment configuration and changes to running Pods as well as horizontal scaling. These are fundamental terms you need to understand before we can move forward.</p>
<p>We’ll elaborate on these terms and introduce more terms as we progress throughout the course but I cannot overstate the importance of them. I suggest you replay this section as many times as you need until all of this information sinks in. Lets recap about what we’ve learned so far. Kubernetes is an orchestration tool. A group of nodes form a Kubernetes cluster. Kubernetes runs containers in groups called Pods. Kubernetes services expose Pods to the cluster as well as to the public internet. And Kubernetes deployments control rollout and rollback of Pods.</p>
<p>In the next lesson, we’re going to be seeing how to interact with Kubernetes clusters.</p>
<h1 id="Interacting-with-Kubernetes"><a href="#Interacting-with-Kubernetes" class="headerlink" title="Interacting with Kubernetes"></a>Interacting with Kubernetes</h1><p>As we have seen, the master components provide the Kubernetes control plane. The way that you retrieve and modify state information in the cluster, is by sending a request to the Kubernetes API server, which is the master component that acts as a front end for the control plane. This leads us to the first method of interacting with Kubernetes, directly communicating via rest API calls. It is possible but not common to need, to work directly with the API server. You might need to if you’re using a programming language that does not have a Kubernetes client library.</p>
<p>Client libraries are our second method of interacting with Kubernetes. Client libraries can handle the tedium of authenticating and managing individual REST API requests and responses. Kubernetes maintains official client libraries for Go, Python, Java, .NET, and JavaScript. There are also many community-maintained libraries if there isn’t official support for your language of choice. The client libraries are a great choice for the OPAs writing code to interact with Kubernetes.</p>
<p>The next method of interacting with Kubernetes is the most common, and what we will focus on in this course, it is the Kubernetes command line tool called cube control, or Kubectl. With cube control, you can issue commands that are at a high level of abstraction with each command, translating into the appropriate API server request. With cube control, you can also access clusters locally, as well as remote. Your success with Kubernetes directly correlates with your Kubectl skill. You can accomplish all your day-to-day work using Kubectl.</p>
<p>So it is vital to learn this command because it manages all different types of Kubernetes resources, and provides debugging and introspection features. Luckily, Kubectl follows an easy to understand design pattern. When you learn to manage one resource, you learn to manage them all. Let’s introduce some common sub commands to see what they look like and what Kubectl can do.</p>
<p>Starting with Kubectl create, Kubectl create creates a new Kubernetes resource. You can create several resources using the built-in sub commands of create, or you can use resources specified in a file. The files are most commonly in gamble format and are referred to, as manifests.</p>
<p>Kubectl delete, Kubectl does the opposite of create, in that it deletes a particular resource. You can do the same with a file, with resources declared inside of it. Kubectl get, returns a list of all the resources for a specified type. For example, Kubectl get, pods lists all the pods and the current namespace. Kubectl describe is going to print detailed information about a particular resource or a list of resources. As an example, Kubectl describe pod, server gives detailed information about the pod named server. Kubectl logs, print container logs for a particular pod or a specific container inside of a multi container pod.</p>
<p>We’ll go deeper into these commands and more as the course progresses. This is just enough to kickstart you for our next lesson. And our final method of interacting with Kubernetes, is through the web dashboard. The dashboard provides a nice list of dashboards, as well as, easy to navigate views of cluster resources. The web dashboard is covered in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/deploy-a-stateful-application-in-a-kubernetes-cluster/">Deploy Stateful Application in a Kubernetes Cluster</a> lab here on cloud Academy. But if you want, you can check it out after you complete this course.</p>
<p>The web dashboard is optional, so not all clusters will have it. Kubectl truly is the way to go for maximum productivity and it really doesn’t take long to get the hang of it. We are now ready to start getting our hands dirty with Kubernetes. We can’t cover everything in this introductory course, but I want to cover many of the main parts of Kubernetes. In the next lesson we’re going to be deploying our first application to Kubernetes. So see you then.</p>
<h1 id="Pods"><a href="#Pods" class="headerlink" title="Pods"></a>Pods</h1><p>This lesson will introduce you to working with Kubernetes cluster and we’re going to be specifically focusing on pods. By doing so, you will see first-hand patterns used by kubectl and some examples of manifest files. But first, let’s review the theory. Pods are the basic building block in Kubernetes. Pods contain one or more containers and we’re going to be sticking with one container per pod in this lesson but we’ll be talking about multi-container pods later. </p>
<p>All pods share a container network that allows any pod to communicate with any other pod, regardless of the nodes that the pods are running on. Each pod gets a single IP address in the container network so Kubernetes will do all the heavy lifting and make that happen. You get to work with the simple abstraction. All pods can communicate with each other and that each pod has one IP address. </p>
<p>Because pods include containers, the declaration of a pod includes all the properties that you would expect for example with Docker rhyme. These include the container image, any ports you want to publish to allow access to the container, choosing a recent policy to determine if a pod should automatically restart, when its container fails, and limits on the CPU and memory resources but there are also a variety of other properties that are specific to pods in Kubernetes. We’re going to be seeing many examples of those in the coming lessons. </p>
<p>All of the desired properties are written in a manifest file. Manifest files are used to describe all kinds of resources in Kubernetes, not only pods. Based on the kind of resource that the manifest file describes, you will configure different properties of that file. The configuration specific to each kind of resource is referred to as its specification or spec. The manifests are sent to the Kubernetes API server where the necessary actions are taken to realize what is described in the manifest. You will use kubectl to send a manifest to the API server and one way of doing this is with the kubectl create command. </p>
<p>For pod manifests, the cluster will take the following actions. Selecting a node with available resources for all of the pods’ containers, scheduling the pod to that node. The node will then download the pod’s container images and then subsequently run the containers. There are more steps involved but that is more than enough to get the idea. We mentioned before that kubectl also provides sub-commands to directly create resources without manifests. </p>
<p>It’s usually a good idea to stick with manifests for several reasons. You can check in your manifests into a source control system to check their history and rollback when needed. It makes it easy to share your work such that it can be created in other clusters and it’s also easier to work with compared to stringing together sequences of commands with many options to achieve the same result. So we’re going to be sticking with manifests for this course. </p>
<p>Now that we’re ready to see all this in action using kubectl and a Kubernetes cluster, our goal will be to deploy an Nginx web server using a Kubernetes pod. If you are using the Introduction to Kubernetes Playground, follow the instructions to the EC2 instance to connect to the Bastion or feel free to connect using a local terminal like I am. If you use a different solution for a Kubernetes cluster, simply follow their provided instructions to make sure kubectl can talk to the cluster. </p>
<p>So I’m here at my terminal, connected to the Bastion host which has kubectl configured to talk to the lab cluster. To confirm that kubectl has configured to talk to the cluster, we first can enter our first few kubectl commands. Kubectl get pods. The output tells us that no pod resources were found in the default name space of the cluster. If it wasn’t able to connect to the API server, you would have seen an error message instead so everything looks good. </p>
<p>Let’s start with a minimal example of a pod manifest to get a taste for manifests. We’ll gradually build them up as we go. I’ve prepared the 1.1 basic pod .yaml file for this. All the course files are preloaded into the source directory on the lab instance and also available on the course get hub repo. This manifest declares a pod with one container that uses the Nginx latest image. All manifests have the same top level keys, API version, kind, and metadata followed by the spec. </p>
<p>Kubernetes supports multiple API versions and version one is the core API version containing many of the most common resources such as pods and nodes. Kind indicates what the resource is. Metadata then includes information relevant to the resource that can help identify resources. The minimum amount of metadata is a name which is set to my pod. </p>
<p>Names must be unique within a Kubernetes name space and spec is specification with a clear kind and must match what is expected by the defined API version. For example, the spec can change between the beta and the generally available API version of a resource. The spec is essentially where all of the meat goes. You can refer to the official API docs for complete info on all versions and supported fields. I’ll explain the ones that we need for this course but know that there are far more left to discover. </p>
<p>The pod spec defines the containers in the pod. The minimum required field is a single container which must declare its image and name. This pod only has a single container but the yaml is a list allowing you to specify more than one. Back at the command line, we can create the pod by changing into the source directory with CD source. We then issue kubectl create -f 1.1-basic_pod.yaml. The f option tells us to create, that the create command, is going to be creating a manifest from a file. For any kubectl command, you can always depend –help to display the help page to get more information. </p>
<p>Now if we run kubectl get pods, we can see my pod is running. My pod is technically an object of a pod kind of resource but it is common to simply use resource to also describe the object as well as the kind. Kubectl shows the name, the number of running containers, the pod state, restarts, and the age of the pod in the cluster. You should memorize the get commands since you’ll use it all the time and I really mean all of the time. </p>
<p>Let’s see some more detailed information about this particular pod. Using the describe command to get complete information. Kubectl describe pod and we’re going to pipe it to more. Describe takes a resource kind just like get and to narrow in on specific resources of that kind, we add the name which you can also do with get. We’re going to be piping the output to more so we can press space bar to page throughout this output. </p>
<p>As you can see, there’s a lot more information than what get provides. The name, name space, and the node running the pod are given at the top along with other metadata. Also note that a pod is assigned an IP. No matter how many containers we include, there would be only one IP. In the containers section, we can see that the image and whether or not the container is ready. You can also the port and the container port are both set to none. </p>
<p>Ports are part of the container spec but Kubernetes assigns default values for us. Just like Docker, you need to tell Kubernetes which port to publish if you want it to be accessible. We’ll have to go back and declare our port after this. Otherwise, nothing is going to reach the web server and at the bottom, we have our events section. It lists the most recent events related to the resource. You can see that the steps Kubernetes took to start the pod from scheduling on the container image to starting the container. The events section is shared by most kinds of resources when you use describe is very helpful for debugging. </p>
<p>Let’s tell Kubernetes which port to publish to allow access to the web server. I’ve prepared the 1.2 port file specifically for that. Compared to the 1.1 file, we can see the ports mapping is added and the container port field is set to 80 for HTTP. Kubernetes is also going to be using TCP as the protocol by default and we’ll assign it an available host port automatically so that we don’t need to declare anything more. </p>
<p>Kubernetes can apply certain changes to different kinds of resources on the fly. Unfortunately, Kubernetes cannot update ports on a running pod so we need to delete the pod and recreate it. We’re going to be running our kubectl delete pod my pod to delete this pod. You can also specify with the -f with referencing to the 1.1 file and Kubernetes will delete all of the resources declared in that file. </p>
<p>Now, we can issue the command kubectl create -f 1.2.yaml. And describe the pod again. You don’t need to describe the pod every single time. I just prefer to do this to see the result of my work and make sure that everything went as I expected. Now we can see that port 80 is given as the port so you may think to try to send a request to port 80 on that noted IP but it still won’t work. Why do you think that is? Well the pod’s IP is on the container network and this lab instance is not part of the container network so it won’t work. But if we sent the request from a container in a Kubernetes pod, the request would succeed since pods can communicate with all other pods by default. We’ll see how we can access the web server from the lab instance in the next lesson. </p>
<p>Before we move on, I want to cover a couple more points and the first is shared between all resources and the second is specific to pods. In the describe section, you might have seen the labels field was set to none. Labels are key value pairs that identify resource attributes. For example, the application tier, whether it’s front end or back end or maybe a region such as US East or US West. </p>
<p>In addition to providing meaningful and identifying information, labels are used to make selections in Kubernetes. For example, you could tell kubectl to get only resources in the US West region. So our 1.3 manifest has a label added to identify the type of app that the pod is a part of. We’re using an Nginx web server and the label value is web server. You could have multiple labels but one is enough in this example. </p>
<p>Our last point that I want to make pertinent is that Kubernetes can schedule pods based on their resource requests. The pods that we’ve seen so far don’t have any resource requests set which makes it easier to schedule them because the scheduler doesn’t need to find nodes that have these requests for amounts of resources. It’ll just throw them onto any node that isn’t under pressure or starved of resources. However, these pods will be the first to be evicted if a node becomes under pressure, it needs to free up resources. That’s called best effort quality of service which was displayed in the describe output. Best effort pods can also create resource contention with other pods on the same node and usually it’s a good idea to set resource requests. </p>
<p>In the 1.4 yaml, I’ve set a resource request and limit for the pod’s container. Request sets the minimum required resources to schedule the pod onto a node and the limit is the maximum amount of resources you want the node to ever give the pod. You can set resource requests and limits for each container. There’s also support for requesting amounts with local disk by using the ephemeral storage. </p>
<p>When we create this pod, kubectl delete pod by pod and then subsequently kubectl -f 1.4, the pod will be guaranteed the resources you requested or it won’t be scheduled until those resources are available. Kubectl describe my pod will now list our pod and we’ll see this guaranteed quality of service. You need to do some benchmarking to configure a reasonable request and limit but the effort is well worth it to ensure your pods have the resources they need and best utilization of the resources in the cluster. This is one of the reasons why we are using containers in the first place. </p>
<p>For the rest of this course, we will use best effort pods since we won’t have any specific resource requirements in mind. This isn’t something you should do in protection environments, however. We’ve covered a lot in this lesson so let’s review what we covered. Pods are the basic building block in Kubernetes and contain one or more containers. You declare pods and other resources in manifest files. All manifests share an API version, kind, and metadata related to that resource. Metadata must include a name but labels are usually a good idea to also help you further filter down your resources. </p>
<p>Manifests also include a spec to configure the unique parts of each resource kind. Pod specs include the list of containers, which must specify our container name and image, but is often useful to set the resource requests and limits. We’re going to see more fields of pod specs in later lessons. </p>
<p>In our next lesson, we’re going to be making the web server running in the pods accessible from our lab VN with services. I’ll see you there.</p>
<h1 id="Services"><a href="#Services" class="headerlink" title="Services"></a>Services</h1><p>So in our previous lesson, we created a webserver Pod but at the moment, it’s inaccessible apart from other Pods in the container network, which isn’t really useful. Even for pausing the container network, it isn’t very convenient to access the webserver as it is because Pods if you’d be able to find the IP address of the webserver Pod and keep track of any changes to it. So remember that Kubernetes will reschedule Pods on to other nodes, for example if the node fails.</p>
<p>But what happens if you have a Pod that fails? Once the Pod is rescheduled it will be assigned an IP address from the available pool of addresses and not necessarily the same IP address it had before. So to overcome all of these networking issues, Kubernetes employs services. thinking back to the Kubernetes’ definition for a service, a service defines networking rules for accessing Pods in the cluster and from the internet you can declare a service to access a group of Pods using labels. And in our example, we can use our app label just like the webserver Pod for the services target. Clients could then access the service at a fixed address. And the services networking rules will direct client request to a Pod in the selected group of Pods.</p>
<p>In our example, there is only one Pod but in general there can be many. The service will also distribute these requests that come into it across the Pods to balance the load. Let’s visualize how we’ll use the service to solve our problem of accessing the webserver running in the Pod.</p>
<p>First, we’re gonna create a service that selects Pods with the app&#x3D;webserver label. That will cause the service to act as a kind of internal load balancer across those Pods. The service will also be given a static IP address and Port by Kubernetes that will allow us to access the service from outside of the container network, and even outside of the cluster.</p>
<p>Let’s see how we do it. Our first three fields are set to the same as before. The kind is now Service, metadata uses the same label as the Pod since it is related to the same application. This isn’t required but it is a good practice to stay organized. Now for the spec, the selector is our important field. The selector defines the labels to match the Pods against. At this example of targets Pods with the app&#x3D;webserver, which will select the Pod that we’ve already created. Services must also define port mappings. So, this service targets Port 80. This is the value of the Pods’ container port.</p>
<p>Lastly, is the optional type. This value defines actually how to expose the Service and we’re gonna set it to NodePort. NodePort allocates a port over this service on each node in the cluster. By doing this, you can send a request to any node in the cluster on the designated port and be able to reach that Service. The designated port will be chosen from the set of available ports on the nodes, unless you specify a NodePort as part of the specs ports.</p>
<p>Usually it is better like Kubernetes shows the NodePort from the available ports to avoid the chance that your specified port is already taken. That would cause the service to fail to create In future lessons, we’re gonna be covering alternate values to service types, so don’t worry.</p>
<p>Now, let’s create the service with our familiar kubectl create -f 2.1. And we’re gonna list the services with kubectl get services. Notice that both commands are really familiar. Kubectl follows a simple design pattern which makes it easy to manage and explore different resources. Kubectl also displays the name, Cluster-IP, External-IP, Ports, and Age of each service.</p>
<p>Let’s bring it down, starting with Cluster-IP. This is our private IP for each service. Our External-IP is not available for NodePort services but if it were, then this would be the public IP for a service. Note that the Ports column, Kubernetes will automatically allocate a Port in the Port range allocated for NodePorts which is commonly port numbers between 30,000 and 32,767.</p>
<p>Let’s describe the service to see what other information is available with kubectl describe service webserver. Just like before, you’ll see a bunch of useful debugging information. The Port was shown in the get services output and also in this output. But we also can see the Endpoint, which is the address of each Pod in the selected group, along with a container port. If there were multiple Pods selected by the label, then you would see each of them listed here.</p>
<p>Kubernetes automatically adds and removes these endpoints as matching Pods are created and deleted, and so you don’t need to do anything to manage those endpoints. Now that we know that the NodePort is on we need a nodes IP, it can be any nodes IP. And one way to list them is to grep for this address in the described node output and add the -A option to include lines after the match.</p>
<p>So let’s do that, kubectl describe nodes and we’re to pipe it to grep -i address -A 1. Nodes are resources in the cluster, just like Pods and services. So, you can use the get and describe commands on them. You can check out all the information in the describe output on your own, or for right now, we just need those IPs. The IP addresses are the internal or private IPs of our nodes inside of our cluster.</p>
<p>Our lab VM is in the same virtual network, so it can reach the nodes using these addresses. And I’ve allowed incoming traffic on the NodePort range from the lab instance in the firewall rules to allow the request. Choose any of the addresses and use the curl command to send an http request to the IP with a NodePort upended. That is the raw html output being served up by Nginx. You can try any of the node IPs and it will give your exact same result, all thanks to a Kubernetes service.</p>
<p>In this lesson, we saw that services allow us to expose Pods using a static address, even though the addresses of the underlying Pods may be changing. We also specifically used a NodePort service to gain access to the service from outside of the cluster on a static Port that is reserved on each node in the cluster. This allowed us to access the service by sending a request to any of the nodes, just not the node that is running the Pod. There is more to say about Pods and services. We will use more complex application in the future to illustrate some of the remaining topics in the next couple of lessons. Think microservices will start by covering multi-container Pods, to continue on when you’re ready.</p>
<h1 id="Multi-Container-Pods"><a href="#Multi-Container-Pods" class="headerlink" title="Multi-Container Pods"></a>Multi-Container Pods</h1><p>This lesson continues to expand upon what we’ve already learned about Pods. Specifically, we’re going to be exploring the details of working with Multi-Container Pods. This is where we really start to hit the good stuff. As we learn more about Multi-Container Pods, we’re also gonna be learning about Namespaces and Pod Logs.</p>
<p>We’re using a sample application for this lesson. It’s a simple application that increments and prints a counter. It’s split into 4 containers across 3 tiers. The application tier includes the server container that is a simple Node.js application. It accepts a post request to increment a counter and a get request to retrieve the current value of the counter. The counter is stored in the Redis container which comprises the data tier. The support tier includes a poller and a counter.</p>
<p>The poller container continually makes a get request back to the server and prints the value. The counter continually makes a post request to the server with random values. All the containers use environment variables for configuration and these Docker images are public, so we can reuse them for this exercise.</p>
<p>Let’s walk through modeling the application as Multi-Container Pods. We’ll start by creating a Namespace for this lesson. Remember that a Namespace separates different Kubernetes resources. Namespaces may be used to isolate users, environments, or applications. You can also use Kubernetes’ role-based authentication to manage users as access to resources in a given Namespace. Using Namespaces is a best practice.</p>
<p>So, let’s start using them now and we’ll continue to use them throughout the remainder of this course. The created, just like any other Kubernetes resource. Here is our Namespace manifest. Namespaces don’t require a spec. The main part is the name which is set to microservices and is a good idea to label it as well. Everything in this Namespace will relate to the counter microservices app. So let’s create the Namespace in kubectl. With kubectl create -f 3.1.</p>
<p>Use your kubectl commands, either use a –namespace or -n option to specify the Namespace, otherwise the default Namespace will be used. You could also use the kubectl create namespace command but for this course, we’re gonna be sticking to manifest. Now out of the Pod, I’ve named the Pod app. Off the top, I want to mention you can specify namespace in this metadata for this Pod but that makes this manifest slightly less portable because the Namespace can’t be overwritten at the command line.</p>
<p>Moving down to the Redis container, we’ll use the latest official Redis image. The latest version is chosen to illustrate a specific point. When you use the latest tag in Kubernetes, and it will always pull the image whenever the Pod started. This can introduce bugs, if a Pod restarts and pulls the new latest version without you realizing it.</p>
<p>Prevent always pulling the image in using an existent version, if one exist. You can set the imagePullPolicy field to IfNotPresent. It’s useful to know this but in most situations you’re better off specifying a specific tag rather than the latest. When specific tags are used, the default imagePull behavior is, IfNotPresent. So, the standard Redis port of 6379 is published with this container.</p>
<p>Now, onto the server container. The server container is straightforward. The image is the public image from this sample application. The tag is used to indicate the microservice within the microservices repository And the server, runs on port 8080, such that it is exposed. The server also requires a REDIS_URL environment variable to connect to the data tier. We can set this in the environment variable sequence.</p>
<p>So how does the server know where to find Redis? Well, because containers in a Pod share the same network stack, a result of which that they all share the same IP address. So, they can reach other containers in the Pod on the local host at their declared container port. The correct host port in this example is localhost:6379 Our imagePullPolicy is admitted because Kubernetes uses IfNotPresent when the explicit tag is given. </p>
<p>We can use the same approach for the counter and poler containers. These containers require the API_URL environment variable to reach the server in the application tier. The correct host port combo for this example is localhost:8080. </p>
<p>Now, let’s create the Pod this time but by adding the -n option to set the Namespace for this Pod, such that it’s created in a microservices Namespace. Kubectl create -f 3.2 yaml -n microservice. Remember to include the same Namespace option with oq control commands that are relating to the Pod. Otherwise you will be targeting the default Namespace. </p>
<p>If we wanted to get the Pod, we would issue kubectl get -n microservices pod app. The -n namespace option can be included anywhere after kubectl. It doesn’t have to be after get, it could be before or after. When you have tab completion enabled. It makes sense to put it earlier, to get to completions for your target namespace. </p>
<p>Let’s observe the output, and we’ll see a &#x2F;4 under the status, since we have 4 containers in the Pod. The status also summarize what is going on but it is best to describe the Pod to see what is going on in more detail. Kubectl describe -n microservice pod app. You’ll see the event log has more going on now that there are multiple containers. </p>
<p>The same events are being triggered for each container from Pulling to Starting as was the case for a Single-Container Pod is something goes awry. You should check the event log to see what’s happening behind the scenes to debug any issue. In this case, everything looks good. </p>
<p>Once the containers are running, we can look at the container logs to see what they’re doing. Logs are simply anything that is written to standard out or standard error in the container. The containers need to write messages to standard out or standard error, otherwise nothing will appear in the logs. Kubernetes records the logs and they can be viewed via the logs command the kubectl log command retrieves logs for a specific container in a given Pod. It dumps all of the logs by default or you can use the tail option to limit the number of logs present. </p>
<p>Let’s see the most 10 recent logs for the counter container in the app Pod. Was kubectl logs -n microservice app counter –tail 10. Here we can see the counter is incrementing by the count by random numbers between 1 and 10. Let’s check the value of the count by inspecting the logs for the poller container. This time we’ll use the -n -f to stream the logs in real time, which is short for follow. Kubectl logs -n microservice app poller -f. We can see the count is increasing every second as the counter continues to increment it. That confirms it, our first multi-container application is up and running. Press Control + C to stop following the logs.</p>
<p>In this lesson, we created a Multi-Container Pod that implements a 3-tier application. We use the fact that containers in the same Pod can communicate with one another using local host. We also saw how to get logs from containers running in Kubernetes by using the kubectl logs command. Remember that logs worked by recording what the container writes to standard out and standard error. The logs also allowed us to confirm that the application is working as expected by continuously incrementing that count.</p>
<p>But there are some issues with the current implementation. Because Pods are our smallest union of work, Kubernetes can only scale out by increasing the number of Pods and not the containers inside of the Pod. If we want to scale out the application tier with the current design we have to also scale out all other containers proportionately. This means that there would be multiple Redis containers running, each would have their own copy of the counter. That’s certainly not what we’re gonna be going for.</p>
<p>It is a much better approach, if we were able to scale each of these services independently. Breaking the application out into multiple Pods and connecting them with services is our ideal implementation. We’ll walk through the design in next lesson but before moving on, it’s worth noting that sometimes you do want each container in a Pod to scale proportionately. It comes down to how tightly coupled the containers are, and if it makes sense to be thinking of them as a single unit.</p>
<p>With that point out of the way, I’ll see you in our next lesson where we will leverage services to break our tightly coupled Pod design into multiple independent Pods.</p>
<h1 id="Service-Discovery"><a href="#Service-Discovery" class="headerlink" title="Service Discovery"></a>Service Discovery</h1><p>We’ve seen Services in action, and in the context of allowing external access to pods running in the cluster, when we created them, with a node port. It’s time to see how services are useful within the cluster. We’ll split our example Microservices application into three pots, one for each tier. Remember that we use the fact that the containers in the same pod can communicate with each other using the local host. But that’s not going to work with our multi-pod design.</p>
<p>That’s where Services come in. Services provide a static end point to access pods in each tier. We could directly use the individual pod IP addresses on the container network, but that would cause the application to break when pods are restarted, because their IP address could change. An added benefit of Services is they also distribute load across the selected group of pods, allowing us to take advantage of the scaling application tier across multiple server pods. </p>
<p>So to realize these benefits, we need to create a data tier service in front of the Redis pod, and an application to your Service in front of the server pod. There are two Service discovery mechanisms built into Kubernetes. The first are environment variables, and the second is DNS. Kubernetes will automatically inject environment variables into containers that provide the address to access services. The environment variables follow a naming convention so that all you need to know is the name of the service to access it. Kubernetes also constructs DNS records based on the service name and containers are automatically configured to clear the clusters, DNS, to discover those services. You’ll see examples of both techniques in this lesson.</p>
<p>We’ll start with creating a new namespace to organize the resources for this lesson. It’s called Service Discovery, and we’ll do that with kube control create, dash F 4.1. Moving on to the data tier. We have a manifest that includes multiple resources. With YAML, we’re allowed to create multiple resources by separating them with three hyphens. It’s possible to cram all the pods and services into one file, but separating them by tier mimics the way we want to manage each tier independently.</p>
<p>We have a service, and now we have our Redis pod. Both are named data tier. The pod has a tier label, which is used by the service as its selector. In our example, we only have one Microservice in the data tier, but that won’t be the case in general. You can include as many labels as necessary in the selector to get just what you need. We can get by with just this one label, in this case. Services can also publish more than one port, which makes a naming the ports mandatory to identify them. We only have one, so the name is optional.</p>
<p>In YAML, everything after our pound or hashtag symbol is a comment. Comments are for readability, and don’t affect how Kubernetes interprets the manifest. Lastly, we set the type to cluster IP, which is the default so that the line could be omitted. Cluster IP creates a virtual IP inside the cluster for internal access only. So we can now use kube control, create, dash F 4.2 YAML and append the namespace with dash N service-discovery.</p>
<p>To create the resources, the command is the same, regardless of how many resources are specified in the file. The resources in the file are created in the order they are listed in the file. Let’s check that the pod is running with kube control, get pod dash N service-discovery. Then describe the service with kube control describe service dash N service-discovery data tier. To make sure that our service has a cluster IP, and that one endpoint corresponds to the data tier pod selected by the service.</p>
<p>Let’s move on to the app tier. Again, we have a service and a pod. The service selects the pods with a tier label, matching the server pod declaration. The pod spec is the same as the one before with one exception, the value of Redis URL environment variable is set using environment variable set by Kubernetes over to service discovery. The value used to be local host 6379, but now we need to access the data tier service. </p>
<p>There are separate environment variables made available to you. The service cluster IP address is available using the environment variable, following the pattern of a service name in all capital letters, with hyphens replaced by underscores followed by underscore service, underscore host in all caps. By knowing the service name you construct the environment variable name, to discover that service IP address. </p>
<p>In our example, with the environment variable and its data tier service host, the port environment variable is similar with host replaced by port. In our example, that is data tier service port, if the port includes a name, you can also append and underscore port name in all caps, hyphens replaced by underscores, which is data tier service port Redis, in our example. The data tier service only declares one port, so the appended name is optional. </p>
<p>As a best practice you can append the service name to tolerate adding ports to the service in the future. When using environment variables in the value field, you need to enclose the variable name in parentheses and precede it with a dollar sign. This allows composing container environment variables from the Kubernetes provided values. When using environment variables for service discovery, the service must be created before the pod in order to use environment variables for service discovery. That is, Kubernetes does not update the variables of running containers. They only get set at startup. </p>
<p>The service must also be in the same namespace for the environment variables to be available. So let’s create our application tier with kube control, create dash F 4.3 YAML, dash N service discovery. Now onto the support tier. We don’t need a service for this tier, just a pod will do, and it contains the counter and polar containers used before. This time we’re gonna be using DNS for service discovery of the app tier service. </p>
<p>Kubernetes will add a DNS A records for every service. The service DNS names follow the pattern of a service name, dot service namespace. In our example that is, app dash tier.service dash discovery. However, if the service is in the same namespace, then you can simply only use the service name. The polar omits the namespace in this manifest.</p>
<p>No need to convert hyphens to underscores, or use all caps when using DNS service discovery. The cluster DNS resolves the DNS name to the service IP address. You can get service port information using DNS SRV records, but that isn’t something that we can use in the manifest file. So I’ll have to either hard-code the port information or use the service port environment variable. The counter uses a hard-coded port, and the polar uses the port environment variable for illustration. </p>
<p>It is possible to use the DNS SRV port record to configure the pod on start-up using something called iNET containers, but we’re going to be covering that later on in this course. So let’s create the support tier. Starting with Kube control create dash F 4.4 YAML, and then the namespace service discovery. </p>
<p>Now let’s check all the pods, with kube control, get pods, namespace service discovery. There are three running pods creating four containers in total. So let’s check the polar logs to see what’s going on with our account. With kube control logs dash N service discovery support tier polar, and let’s follow them. Look at that. The application is just plugging away, and that is such a satisfying result.</p>
<p>Let’s recap this lesson before jumping into the next one. We’ve covered structuring a varying number of applications using Services as interfaces between tiers. We use the cluster IP type of service. We’re accessing the data and application tiers within the cluster. We also covered how Kubernetes Services works with environment variables and DNS. That allowed us to refactor our multi container pod application into instead a multi-tier application that we stood up in this lesson.</p>
<p>When using environment variables for service discovery, the service must be created for the pod, before the pod, in order to use the environment variables for that service discovery. Their service must also be in the exact same namespace. DNS records overcome the shortcomings of environment variables. DNS records are added and removed from the clusters DNS as services are created and destroyed. The DNS name for services include a namespace, allowing communication with services and other namespaces. And finally SRV DNS records are created for service port information.</p>
<p>So what do you think? Are you getting excited about the capabilities that Kubernetes can do? It just keeps getting better and better. And it’s gonna get better in the next lesson. To put it in context, consider how we would scale our current application. We could increase the number of server pods by changing the name to something like example app tier dash one, then creating example app tier dash two, and so on. And we could glue this all together with some scripting, a bit of extra work to make the scaling easy. But what then happens when we would want to reconfigure the server container? Well, let’s see.</p>
<p>We could create an example app tier version one dash one and then example app tier version two dash one, with some updated scripting. These things could probably handle that, but what happens when something goes wrong? Or what if there’s an error in the new version? We could probably handle that by pulling the API and checking the status again, with probably some more scripting, include some more code, but there should probably be a better way to do this, which is exactly what the next lesson is going to be covering. And that is deployment. So let’s learn about it in the next lesson.</p>
<h1 id="Deployments"><a href="#Deployments" class="headerlink" title="Deployments"></a>Deployments</h1><p>The previous lessons have created pods directly, but I’ve gotta be honest with you, we’ve kind of been cheating a bit so far. You’re not really supposed to create pods directly. Instead, a pod is really just a building block. They should be created via a higher level abstraction such as deployments. This way, Kubernetes can add on useful features and higher level concepts to make your life easier.</p>
<p>This lesson is gonna be covering the basics of the deployments with the following lessons covering auto-scaling and rolling updates. So let’s start by covering some theory, and then we’ll see deployments in action. A deployment represents multiple replicas of a pod. Pods in their deployment are identical, and within a deployment’s manifest, you embed a pod template that has the same fields as this pod spec that we have written before.</p>
<p>So you describe a state in the deployment, for example, five pod replicas of Redis version five, and Kubernetes takes the steps required to bring the actual state of the cluster to that desired state that you’ve specified. If for some reason one of the five replica pods is deleted, Kubernetes will automatically create a new one to replace it. You can also modify the desired state and Kubernetes will converge the actual state to that desired state. We’ll see a bit of that in this lesson with more on updates in a later lesson. The Kubernetes master components include a deployment controller that takes care of managing the deployment.</p>
<p>Now, let’s see how this works in practice. We’ll use our microservices three tier application to demonstrate deployments and we’ll replace the individual pods with a deployments that manage the pods for us. Let’s start by creating a new namespace called deployments for this lesson. So we’re gonna do that with kubectl create dash f 5.1 yaml, just like we’ve done previously. Now, a deployment is a template for creating pods. </p>
<p>A template is used to create replicas, and a replica is a copy of a pod. Applications scale by creating more replicas. This will be more clear when you just see the YAML files and as we demonstrate more features throughout this lesson. Now, I’m comparing the data tier manifest from the last lesson to our current manifest that uses deployments. I wanna highlight how there are significant similarities, but just a few changes, and the first change is the API version is now apps version one. Higher level abstractions for managing applications are in their own API group and not part of the core API. The kind is set to deployment, and our metadata from the last lesson is directly applied to said deployment.</p>
<p>Next comes a spec. The deployment spec contains deployment-specific settings and also a pod template, which has exactly the same pod spec as the last lesson in it. It in the deployment-specific section, the replica key sets how many pods to create for this particular deployment. Kubernetes will keep this number of pods running. We set the value to one because there cannot be multiple Redis containers. We’ll have one Redis pod.</p>
<p>Next, there’s the selector mapping. Just like we saw with services, deployments use label selectors to group pods that are in the deployment. The match labels mapping should overlap with the labels declared in the pod template below, and kubectrl will complain if they don’t overlap. The pod template metadata includes labels on the pods.</p>
<p>Note that the metadata doesn’t need a name in the template because Kubernetes generates unique names for each pod in the deployment. Similar changes are made to the app tier manifest and the support tier manifest, mainly adding a selector in a template for the deployment. We can complete the same process for the app and support tiers, also setting replicas to one for both cases.</p>
<p>One is actually the default, so it isn’t strictly required, but it does emphasize that a deployment manages a group of identical replicas. So let’s create the tiers now. Be sure to set the deployments namespace, and we’ll use multiple f options to create them all in one go with kubectrl create namespace deployments f 5.2, 5.3, 5.4 YAMLs.</p>
<p>Now let’s get our deployments with kubectrl get namespace deployments deployments. Kubectrl displays three deployments and the replica information. Note that they all show one replica right now. So remember that horrible scenario I described at the end of the last lesson? Well, we can see how deployments solve the problem by asking kades for the pods.</p>
<p>Note that each pod has a hash at the end of it. Deployments add this uniqueness to the names, automatically allowing us to identify pods of a particular deployment version. We can see how this works by running more than one replica in a deployment. We’ll use kubectrl scale command for modifying replica counts. We’ll scale the number of replicas in the support tier to five, which will cause the counter to increase five times more quickly. The scale command is equivalent to editing the replica value in the manifest file and then running kubectrl apply to apply the change. It’s just optimized for this one-off use case.</p>
<p>Now, if we run kubectrl get pods in the namespace of deployments, we can see the pods again to see what happened. Note that the support tier pods continue to show two of two ready containers. This is because replicas replicate pods, not individual containers inside of a pod. Deployments ensure that the specified number of replica pods are kept running. So we can test this by deleting some pods with kubectrl delete, the namespace deployments pods support tier, and then the hash. And now watch as Kubernetes brings them back to life.</p>
<p>All right, so Kubernetes can resurrect pods and make sure the application runs the intended number of pods. As a side note, I used the Linux watch command with the dash n1 option to update the output every one second. Kubectrl also supports watching by using the w option and any changes are appended to the bottom of the output compared to overriding the entire output with the Linux watch command. You might prefer one over the other depending on what you’re watching. But let’s go ahead and scale out the app tier to five replicas, as well, with kubectrl scale namespace deployments deployment app tier replicas five.</p>
<p>Now let’s get the list of pods with kubectrl namespace deployments get pods. As you can see, Kubernetes makes it really quite painless. They did all the heavy lifting for us, and now we can confirm that the app tier service is load balancing requests across the app tier pods by describing the service, kubectrl describe namespace deployments service app tier. And now observe that the service now has five endpoints matching the number of pods in said deployment. Thanks to label selectors, the deployment and the service are able to track the pods in the tier.</p>
<p>Let’s review what we’ve done in this lesson. We’ve used deployments to have Kubernetes manage the pods in each application tier. By using deployments, we get the benefits of having Kubernetes monitor the actual number of pods and converge to our specified desired state. We also saw how we can use kubectrl scale to modify the desired number of replicas in Kubernetes. This will do what it takes to realize the number of replicas we specify. We also saw how it seamlessly integrates with services that load balance across the deployments’ pods.</p>
<p>A word of caution with scaling deployments is that you should make sure that the pods you are working with support horizontal scaling. That usually means that the pods are stateless as opposed to stateful. The data for the app tier is stored in the data tier, and we could add as many app tier pods as we like because the state of the application is stored inside of the data tier.</p>
<p>With our current setup, we can’t scale the data tier out ‘cause that would create multiple copies of the application counter. However, if we never scale the data tier, we still get the benefit of having Kubernetes return the data tier to its desired state by using deployments. We also get more benefits when it comes to the performing of updates and rollbacks, which we’ll see in a couple of lessons. So it still makes sense to use a deployment for the data tier. We rarely should be directly creating pods.</p>
<p>Kubernetes has even more tricks up its sleeve when it comes to scaling. We arbitrarily scaled the deployment, but in practice, you would like to scale based on CPU load or some other metric to react to the current state of the system to make the best use of available resources. So let’s see how to do that in the next lesson.</p>
<h1 id="Autoscaling"><a href="#Autoscaling" class="headerlink" title="Autoscaling"></a>Autoscaling</h1><p>We’ve seen deployments work their magic in the last lesson. We also saw how to scale the deployment replicas but it would be nice to not have to manually scale the deployment. That’s where autoscaling comes in. Kubernetes supports CPU-based autoscaling and autoscaling based on a custom metric that you can define. We’re gonna be focusing on CPU for this course.</p>
<p>Autoscaling works by specifying a desired target CPU percentage and a minimum and a maximum number of allowed replicas. The CPU percentage is expressed as a percentage of the CPU resource request of that Pod. Recall that Pods can set resource requests for CPU to ensure that they’re scheduled on a node with at least that much CPU available. If no CPU request is set, autoscaling won’t take any action.</p>
<p>Kubernetes will increase or decrease the number of replicas according to the average CPU usage of all of the replicas The autoscaler will also increase the number of replicas when the actual CPU usage of the current Pods exceeds the target and vice versa for decreasing the number of Pods. It will never create more replicas in the maximum nor will they decrease the number of replicas below your configuring minimum. You can configure some of the parameters of the autoscaler, but the default will work fine for us.</p>
<p>With the defaults, the autoscaler will compare the actual CPU usage to the target CPU usage. And either increase the replicas if the actual CPU is sufficiently higher than the target, or it will decrease the replicas if the actual CPU is sufficiently below the target. Otherwise it will keep the status quo. Autoscaling depends on metrics being collected in the cluster.</p>
<p>Kubernetes integrates with several solutions for collecting metrics. We’re going to be using the Metrics Server which is a solution that is maintained by Kubernetes itself. There are several manifest files on the Kubernetes Metrics Server GitHub repo that declare all of the resources. We will need to get Metrics Server up and running before we can use autoscaling.</p>
<p>Once Metrics Server is running, autoscalers will retrieve those metrics and then make calls with the Kubernetes metrics API. The lab instance includes a Metrics Server manifest in the Metrics Server sub-directory. It’s outside the scope of this course to discuss all the resources that comprise of the Metrics Server. So all we need to do is create them and we can count on metrics being collected in the cluster.</p>
<p>Here we can use the kubectl apply command and then specify the Metrics Server folder to create all of the resources within the Metrics Server folder. kubectl control will then create all of the manifests it finds in that directory. You can see quite a few of these resources are created. One of them is the deployment, in the Metrics Server, runs actually as a pod in the cluster, and that pod is managed by that deployment. It takes a minute or two for the first metrics to start trickling in.</p>
<p>Let’s confirm that the Metrics Server is running by watching the pod. With kubectl top pods namespace deployments. This will list the CPU and memory uses of each pod in the namespace. You can use the top command to benchmark a pod’s resource utilization, and then subsequently debug resource utilization issues. Our pods are all using a small fraction of one CPU. The m stands for milli. 1000 milli CPUs equals one CPU.</p>
<p>Now that we have metrics, the other thing the autoscaler depends on is having a CPU request in the deployments odd spec. Let’s see how that looks in the app-tier deployment. I’ve highlighted the change from the previous lesson. Each pod will now request 20 milli CPU. Kubernetes will only scale the pods and each node with at least 0.02 CPU’s remaining. I also set the replicas to five to keep five replicas running.</p>
<p>Now, if we try to create the resources, kubectl will tell us that they actually already exist. Create will check if a resource of a given type and name already exists and it will fail if it does. We could delete the deployment and then recreate it but it would be nice to avoid the downtime that is involved. Instead, Kubernetes provides a command that can apply changes to existing resources. That’s what kubectl applies. So let’s apply that to 6.1 now.</p>
<p>Apply will update our deployment and do include the CPU request. It will warn us about mixing create and apply, but we can go ahead and ignore that. I’d encourage you to take the certified Kubernetes administrator course here on CloudAcademy if you’d like to learn more about the differences between create and apply.</p>
<p>So we’ve set the request low enough that the five replicas can remain scheduled in the cluster as we can see if we get the deployments output. Five actual pods are ready matching the five pods we desired. This completes like the prerequisites for autoscaling. The autoscaler, which has the full name of HorizontalPodAutoscaler because it scales horizontally or out, it’s just another resource in Kubernetes we can use a manifest to declare.</p>
<p>The HorizontalPodAutoscaler kind is part of the autoscaling version one API. It’s spec includes a min and max to set and lower the upper bounds on running replicas. The targetCPUUtilizationPercentage field sets the target average CPU percentage across the replicas. With the target set to 70%, Kubernetes will decrease the number of replicas if the average CPU utilization is 63% or below and increase replicas if it is 77% or higher.</p>
<p>Lastly, the spec also includes a scale target reference, that identifies what is actually scaling. In this case, we are targeting the app-tier deployment. We’ve added the equivalent kubectl autoscale command to achieve the same result, but we’ll stick with the manifests for everything. So let’s create the autoscaler with kubectl create file 6.2. Now we can watch the deployment until the autoscaler kicks in with the watch command. Well, would you look at that, the kernel is already updated, Kubernetes does not disappoint.</p>
<p>We can also describe the HorizontalPodAutoscaler to see what events took place. Now, it would be painful to type out pod autoscaler many times, but fortunately kubectl accept shorthand notations for resource types. So we’ll just run kubectl api dash resources for a full list of those shorthand notations. The output is sorted by the API group that appears in the third column. The lone autoscaling resource is the horizontalpodautoscalers and we can use hpa as the short name.</p>
<p>So let’s describe it with kubectl describe deployments hpa. We can see the successful rescale events and the current metrics are all below the target. We can also get the HorizontalPodAutoscaler for a quick summary of the current state with kubectl get namespace deployments hpa. The first number in the target expresses the current average CPU utilization as a percentage of the CPU request. </p>
<p>We can see that we are well below the target but we are at the minimum replicas so it won’t scale any further down. Let’s say we wanted to modify the minimum to two replicas. We could modify the manifest, save it, and then use the apply command or we could use the kubectl edit command which combines those three actions into one.</p>
<p>So let’s edit the odd autoscaler. The server side version of the manifest is presented in the vai console editor. If you haven’t used vai before, don’t worry, I’ll tell you everything we need to do. In general it’s a good idea to stick with modifying our local manifest so the changes can easily be checked into a VCs, but I want you to know that the edit command is available. You’ll notice that the server’s manifest contains additional fields that we didn’t configure. The server includes several fields automatically to help it manage resources. Type dash space one to jump the cursor down to the first occurrence of space one, which is our minReplicas field value.</p>
<p>Now press A to start editing the file. Then press your right arrow key to move the cursor after the one then press backspace two to change the minReplicas to two. Then press escape to stop editing followed by colon write quit or wq, to write to the file and quit to the editor. And Kubernetes will go out and automatically apply those changes to the HorizontalPodAutoscaler. Now you can watch the deployment with the Linux watch command. It’ll typically happen within 15 seconds which is the default period for the HorizontalPodAutoscaler to check if it should scale.</p>
<p>This wraps up our tour for autoscaling Kubernetes. To recap, Kubernetes depends on metrics with being collected in a cluster before you can use autoscaling. We accomplish that by adding the Metrics Server to the cluster. You must also declare CPU request in your deployments pod template so that autoscaling can compute each pod’s percentage CPU utilization. With those prerequisites taken care of, you can use the HorizontalPodAutoscaler. You configure it with a target CPU percentage and then min and max replicas.</p>
<p>Kubernetes will do all the heavy lifting for us, dynamically scaling our deployment based on the current state of the load. While we were doing this, we were able to also pick up the kubectl apply command, to update a resources rather than deleting and recreating it. And the edit command, which is shorthand for editing a live resource and then having it automatically applied. In the next lesson, we’re gonna wrap up our coverage over deployments by discussing how to deployments help you when deploying code or configuration changes. I’ll see you there.</p>
<h1 id="Rolling-Updates-and-Rollbacks"><a href="#Rolling-Updates-and-Rollbacks" class="headerlink" title="Rolling Updates and Rollbacks"></a>Rolling Updates and Rollbacks</h1><p>The last topic we will discuss on deployments is how updates work. Kubernetes uses rollouts to update deployments. And a Kubernetes rollout is a process of updating or replacing replicas with new replicas matching a new deployment template. Changes may be configurations such as environment variables or labels, or also code changes which result in the updating of an image key of the deployment template. In a nutshell, any change to the deployment’s template will trigger a rollout.</p>
<p>Deployments have different rollout strategies, and Kubernetes uses rolling updates by default. Replicas are updated in groups, instead of all at once until the rollout is complete. This allows service to continue uninterrupted while the update is being rolled out. However, you need to consider that during the rollout there will be pods using both the old and new configuration of the application. In such, it should gracefully handle that.</p>
<p>As an alternative deployments can also be configured to use the recreate strategy which kills all of the old template pods before creating the new ones. That, of a course, incurs downtime. So we’re going to be focusing on the rolling updates in this course. We actually have already rolled out an update in the last lesson when we added the CPE request to the app tier deployments pod template.</p>
<p>Scaling is an orthogonal concept to rolling updates. So all of our scaling events do not create roll-outs. Kubectl includes commands to conveniently check, pause, resume, and rollback rollouts. So let’s check out those now. We’ll use our deployments namespace again and focus on the app tier deployment.</p>
<p>First, we will delete the existing auto scaling configuration. Auto-scaling and rollouts are compatible, but for us to easily observe rollouts as they progress we’ll need many replicas in action. Deleting the autoscaler is going to help us with that.</p>
<p>Next let’s edit the app tier deployment with the following command. We’re gonna be jumping down to replicas and start editing them just change them after two and instead enter 10. It’ll be easier to see the raw in action with a large number of replicas. Also remove the resource request by pressing escape to stop editing. Then jumping down to resources and D three D to delete the three lines comprising the resource request. This will avoid any potential problems with scheduling the replicas if all 10 of the CPU requests can be satisfied. We’ll go ahead and write quit now. And we’re going to be watching this with the Linux watch command.</p>
<p>Now it’s time to trigger a rollout. Open the app to your deployment with Q+control+edit. From here, we can see the server added the default values for the deployment strategy. Specifically, the type is rolling update in the corresponding match surge specifies how many replicas over the desired total are allowed during a rollout. A higher surge allows new pods to be created without waiting for old ones to be deleted.</p>
<p>In the maxunavailable controls how many old pods can be to be deleted without waiting for new pods to be ready. We’ll keep the defaults of 25%. You may want to configure them if you want to trade off the impact on availability or resource utilization with the speed of the rollout. For example, you can have all of the new pods start immediately, but in the worst case you can have all of the new pods and all the old pods consuming resources at the same time effectively doubling the resource utilization for a short period.</p>
<p>With those fields out of the way, let’s trigger a rollout.This command will replace server with name cloudacademy for all of our previous pods. This is just a nonfunctional change for us but it will demonstrate the rollout functionality. So we’re go ahead and apply this with right click. Then we can immediately watch the rollout status with Q+control if we’re fast enough Q controlled rollout status streams progress updates in real time. You’ll see the new replicas coming in and old replicas going out.</p>
<p>To repeat this exercise until you see the entire flow and experiment with the number of replicas maxsurge and maxunavailable as you please. Rollouts may also be paused and resume. I’m gonna be splitting my window into two to better illustrate what is going on by entering tmax. This is a terminal multiplexer and I’m going to press control+B followed by the percent symbol to split the terminal vertically.</p>
<p>To switch between the two terminals, you can enter control+B followed by the left or right arrow. In the right terminal, I’ll prepare the same rollout status command we used before so that I can watch the status change as soon as we apply an update. And then we’ll jump over to the left terminal and edit the app tier deployment again. Let’s change the container name again by entering the following.</p>
<p>Next, we will quickly write the file to apply the changes then watch the status rollouts in the right terminal and pause the rollout mid-flight in the left terminal. Now the rollout is paused, but pausing won’t pause replicas that were created before the pausing. They will continue to progress to ready. However, there will be no new replicas created after the rollout is paused. We can use the rollout resume command for exactly that purpose. The rollout picks up right where it left off and goes about its business.</p>
<p>So I’m going to stop the terminal multiplexer now by doing control+B, and Y. So now consider you found a bug in the new revision and you needed to roll back. So kubectl has a handy command exactly for that. With kubectl, rollout, undo. This will roll back to the previous revision. You can also roll back to a specific version. You can also use kubectl rollout history to get a list of all versions and then grab the specific version and pass it into that.</p>
<p>That’s all for this demonstration of rolling updates and rollbacks. But before we move on let’s scale back the app tier to one replica to give us some more CPU resources. Deployments, and rollouts are very powerful constructs. Their features cover a large swath of use cases.</p>
<p>So let’s reiterate what we’ve covered in this lesson. We learned that rollouts are triggered by updates to a deployments template. Kubernetes uses a rolling update strategy by default. We also learned that we can pause, resume, and undo rollouts of deployments. There’s still so much more that we could do with deployments and rollouts depend on container status.</p>
<p>Kubernetes assumes that created containers are immediately ready and the rollout should continue. But this does not work in all cases. We may need to wait for the web server to accept connections. So here’s another scenario. Considering an application using a relational database, the containers may start but it will fail until a database and tables are created.</p>
<p>These scenarios must be considered to build reliable applications. This is where probes in init containers come into the picture. So we’ll take a look at integrating probes and init containers in our next two lessons.</p>
<h1 id="Probes"><a href="#Probes" class="headerlink" title="Probes"></a>Probes</h1><p>The previous lesson covered deployment rollouts. Kubernetes assumes that a Pod was ready as the container was started, but that’s not always true. For example, if the container needs time to warm up Kubernetes should wait before sending any traffic to the new Pod. It’s also possible that a Pod is fully operational but after some time it becomes non-responsive. For example, if it enters a deadlock state, Kubernetes shouldn’t send any more requests to that Pod and will be better off to restart a new Pod.</p>
<p>Kubernetes provides probes to remedy both of these scenarios and probes are sometimes referred to as health checks. The first type of probe are readiness checks. They are used to probe when a Pod is ready to serve traffic. As I mentioned before, often a Pod is not ready after its containers have just started. They may need time to warm caches or load configurations.</p>
<p>Readiness probes can monitor the containers until they are ready to serve traffic. But readiness probes are also useful long after startup. For example, if the Pod depends on an external service and as service goes down, it’s not worth sending traffic to that Pod since it can’t complete it until the external service is back.</p>
<p>Readiness probes control the ready condition of a Pod. If a readiness probe succeeds, the ready condition is true, else, it is false. Services use the ready condition to determine if the Pod should be sent traffic. In this way, probes integrate with services to ensure that traffic doesn’t flow to Pods that aren’t ready. This is a familiar concept if you’ve used a cloud load balancer. Back in instances that fail health checks are not served traffic, just as services won’t serve traffic to Pods that aren’t ready.</p>
<p>Services are our load balancers in Kubernetes. The second type of probe is called a liveness probe. They are used to detect when a Pod has entered a broken state and can no longer serve traffic. In this case, Kubernetes will restart the Pod for you. That is the key difference between these two types of probes. Readiness probes determine when a service can send traffic to a Pod because it is temporarily not ready and a liveness probe decides when a Pod should be restarted because it won’t come back to life. You declare both probes in the same way. You just have to decide which course of action is appropriate if a probe fails. Stop serving traffic or restart.</p>
<p>Probes can be declared on containers in a Pod. All of the Pod’s container probes must pass for the Pod to pass. You can define any of the following as the action probe to check the container. A simple command that runs inside of a container, an HTTP GET request or the opening of a TCP socket. The command probes succeeds if the exit code of the command is zero, else, it will fail. A GET request succeeds if the response code is between 200 and 399. A TCP socket probes succeeds if a connection can be established. By default, the probes check the Pods every 10 seconds.</p>
<p>Our objective in the hands-on part of this lesson is to test our containers using probes. Specifically, we will add readiness and liveness probes to our application. We will use the application of manifest from the deployments lesson as the base of our work in this lesson. But before we get started creating probes, let’s first crystallize the concepts by relating these probes to our application. The data tier contains one redis container. This container is alive if it accepts TCP connections. The redis container is ready, if it responds to redis commands such as get or ping.</p>
<p>There is a small but important difference between the two. A server may be alive but not necessarily ready to handle incoming requests. The API server is alive if it accepts HTTP request but the API server is only ready if it is online and has a connection to redis to request an increment, the counter. The sample application has a path for each of these probes. The counter and polar containers are live and ready if they can make an HTTP request back to the API server.</p>
<p>So let’s apply this knowledge to the deployment templates. We will go in the same order we just discussed but skip the support tier because the server demonstrates the same functionality. Let’s start by creating the probes in the namespace to isolate the resources in this lesson. Now take a look at this comparison that shows the addition of a name for the port and that the probes are the only changes to the data to your deployment. The liveliness probe uses the TCP socket type of the probe in this example, and by using a named port, we can simply write the name rather than the port number. This will protect us in the future if the port number ever changes and someone forgets to update the probe port number. Also, by setting the initial delay seconds, we give the redis server an adequate time to start.</p>
<p>We can also configure failure threshold, delays and timeouts for all probes. The default value will work for this example but you can reference Kubernetes documentation for more information on different values. Next, the readiness probe uses the exact type of probe to specify command. What this does, is runs a command inside the container similar to docker exec if you’ve used that before. The redis-cli ping command test if the server is up and is ready to actually process redis specific commands. Commands are specified as a list of strings.</p>
<p>Given the consequences of failing a liveness probe is going to be restarting a Pod. It’s generally advisable to have the liveness probe at a high delay than the readiness probe. I’ll also point out that by default three sequential probes need to fail before a probe is marked as failed, so that we have some buffer. Kubernetes won’t immediately restart the Pod the first time the probe fails, but we can configure it that way if we need to.</p>
<p>The particular delay depends on our application and how long it reasonably requires to start up. Five seconds should be more than enough to start checking readiness. And by default, we only need to pass a single probe before any traffic is sent to the Pod. Having the readiness initial delay too high will prevent Pods that are able to handle traffic from receiving any. So let’s create the new and improved data tier.</p>
<p>Now we can watch the GET output for the deployment to observe the impact of the probes. Note the ready column. This will show one of one replicas when the readiness check passes. With the watch option, new changes are appended to the bottom of the output. So we can see from the bottom line, the Pod transitions to the ready state after the number of seconds in the age column in the bottom line of the output. Watch the deployment for a while to make sure things are running smoothly. If no new lines appear, there are no changes and everything has stayed up and running. However, if something did go awry, I’d recommend using a combination of the described and logs commands to debug the issue.</p>
<p>Unfortunately, failed probe events don’t show in the events output but you can use the Pod restart counter as an indicator of failed liveness probes. Logs are always the best direct way to get at them. We will add some debug logging to the service so that you can see all the incoming probe requests after this. Onto the app tier. Notice that the debug environment variable has been added which will cause all the service requests to be logged.</p>
<p>Note that this environment variable is specific to the sample application and not for general purpose settings. Further down, probes are declared and this time they are HTTP GET probes. They send request to end points built at the server specifically for checking its health. The liveness probe endpoint does not actually communicate with redis. It’s a dummy that will always return 200 okay as its response for every request. Your readiness probe endpoint checks that the data tier is available. We’re also gonna be setting the initial delay seconds so the process has adequate time to start.</p>
<p>So let’s create the app tier deployment. And we’ll subsequently watch that deployment to verify containers are alive and ready. It may take some time to start the containers and wait for the initial delay seconds on the readiness probe. But after a short delay, the replica will be ready. We can now stream the logs to see what’s happening behind the scenes.</p>
<p>First, we’re to be getting the Pods to find a Pod in the deployment, then use kubectl logs with the dash f option to follow the log stream. And I’ll use cut to filter down what is going on. We can see that Kubernetes is firing both probes in 10-second intervals. With the help of these probes, communities can take Pods out of service when they aren’t ready and restart them when they enter a broken state.</p>
<p>To summarize what we saw in this lesson, containers in Pods can declare readiness probes to allow Kubernetes to monitor when they’re ready to serve traffic and when they should temporarily be taken out of service. Containers in Pods can declare a liveliness probes to allow Kubernetes to detect when they have entered a broken state and the Pod should be restarted.</p>
<p>Both types of probes have the same format and manifest files and can make use of either command, HTTP GET or TCP socket probe types. Remember that probes kick in after containers are started. If you need to test or prepare things before the container start, there is a way to do that as well. And that is the role of init containers and it is a subject of our next lesson. I’ll see you there.</p>
<h1 id="Init-Containers"><a href="#Init-Containers" class="headerlink" title="Init Containers"></a>Init Containers</h1><p>Sometimes you need to perform some tasks or check some prerequisites before a main application container starts. Some the examples include waiting for a service to be created, downloading files, or dynamically deciding which port the application is going to use. The code that performs those tasks could be crammed into the main application, but it is better to keep a clean separation between the main application and supporting functionality to keep the smallest footprint you can for the images. However, the tasks are closely linked to the main application and are required to run before the main application starts.</p>
<p>So Kubernetes provides us with an init container as a way to run these tasks that are required to complete before our main container starts. Pods may declare any number of init containers. They run in a sequence in the order they are declared. Each init container must run to completion before the following init container begins. And once all of the init containers have completed the main containers in the pods can start.</p>
<p>Init containers use different images from the containers in the pod, and this can provide some benefits. They can contain utilities that are not desirable to include in the actual application image for security reasons. They can also contain utilities or custom code for setup that is not present in the application image. For example, there is no need to include utilities like sed or awk or dig in an application image if they are only used for setup.</p>
<p>Init containers also provide an easy way to block or delay the start-up of an application until some pre-conditions are met. They are similar to readiness probes in this sense but only run at pod startup. It can perform other useful work. All of these features together make init containers a vital part of the Kubernetes toolbox. There is one more important thing to understand about it init containers. They run every time a pod is created.</p>
<p>This means they will run once for every replica in a deployment. And if a pod restarts, to say, due to failed live-ness probes the init containers would run again as part of that restart. Thus, you have to assume that init containers run at least once. This usually means that init containers should be unique. Running it more than once should have no additional effect.</p>
<p>Let’s add an init container to our app tier that will wait for Reddis before starting any application. We’ll see that the init containers have the same field as regular containers in a pod spec. The one exception is init containers do not support readiness probes because they must run to completion before the state of the pod can be considered ready. You will receive an error if you try to include a readiness probe in an init container.</p>
<p>Let’s see what the manifest looks like in our case. We’ll just be updating the app to your deployment, so we won’t make a new namespace. I’m comparing the deployment from the previous lesson with our new version with init containers. You can see that the fields are the same as what we have seen with regular containers. I’ve used the same image as the main application for simplicity, and it has everything we need in it. The command field is used to override the image’s default entry point command.</p>
<p>For this init container, we want to run a script that waits for a successful connection with Reddis. The script is already included in the image and is executed with the NPM run script await Reddis command. This command will block until the connection is established with the configured Reddis URL provided as in an environment variable.</p>
<p>Now let’s apply those changes to the existing deployment. After that, describe the deployments pod. And observe the event log, as it now shows the entire lifecycle with init containers. The await Reddis init container runs the completion before the server container is created. You can also view the logs of init containers using the usual logs command and specifying the name of the init container as the last argument after the pod name. This is specifically important when debugging init containers which prevents the main container from ever being created.</p>
<p>This concludes our tour of init containers. They give you another mechanism for controlling the lifecycle of pods. You can use them to perform some tasks before the main containers have an opportunity to start. This could be useful for checking preconditions, such as checking that depended upon services are created or preparing dependent upon files. The files use case requires knowledge of another Kubernetes concept, namely volumes which can be used to share files between containers. We’ll discuss all we should know about volumes in the next lesson. So continue on when you’re ready.</p>
<h1 id="Volumes"><a href="#Volumes" class="headerlink" title="Volumes"></a>Volumes</h1><p>Containers in a pod share the same network stack, but each has their own file system. It could be useful to share your data between containers. For example, having an init container prepare some files that the main container depends upon. The file system of containers are also limited to the lifetime of the container, so this could present some undesirable effects. For example, if the data tier container we are using in our examples crashes or fails a likeness probe, it will be restarted, and all of our data will be lost forever.</p>
<p>So this lesson is gonna cover the different ways Kubernetes hands non-ephemeral data that bring data from containers, Kubernetes volumes, and Kubernetes persistent volumes. Our goal for this lesson is to deploy the data tier from our sample application, using a persistent volume so the data can outlive the data tier pod. Again, this lesson builds on the code from the previous lessons, so let’s first discuss more about the options for storing persistent data and then apply them to our data tier.</p>
<p>Kubernetes includes two different data storage types. Both are used by mounting a directory in one container, and then that could be shared by containers in the same pod. Pods can also use more than one volume or persistent volume. Their differences are mainly in how their lifetime is managed. One type exists for the lifetime of a particular pod and the other is independent from the lifetime of the pods. Volumes are tied to a pod and their life cycle. Volumes are used to share data between containers in a pod and to tolerate container restarts.</p>
<p>Although you can configure volumes to use durable storage types that survive pod deletion, you should consider using volumes for non-durable storage that is deleted when the pod is deleted. Default type of volume is called emptyDir and it creates an initially empty directory on the node running the pod to back the storage used by the volume. Any data written to the directory remains if a container in the pod is restarted.</p>
<p>Once the pod is deleted, the data in the volume is permanently deleted. It’s worth noting that since the data is stored on a specific node, if a pod is rescheduled to a different node, that data will be lost. If the data is too valuable to lose when a pod is deleted or rescheduled, you should consider using persistent volumes. Persistent volumes are independent from the lifetime of pods and are separately managed by Kubernetes. They work a little bit differently.</p>
<p>Pods may claim a persistent volume and use it throughout their lifetime. The persistent volumes will continue to exist outside of their pods. Persistent volumes can even be mounted by multiple pods on different nodes if the underlying storage supports multiple readers or writers. Persistent volumes can be provisioned statically in advanced by a cluster admin or dynamic for a far more flexible self-serve use case.</p>
<p>Pods must make a request for storage before they can use a persistent volume. The request is made using a persistent volume claim, or PVC. A PVC declares how much storage the pod needs, the type of persistent volume, and the access mode. The access mode describes the persistent volume and whether it is mounted in read-only, read-write, or read-write many. There are three supported access modes to choose from, read-write once, read-only many, or read-write many. If there isn’t a persistent volume available to satisfy the claim and dynamic provisioning isn’t enabled, the claim will stay in a pending state until such persistent volume is ready. The persistent volume claim is connected to the pod by using a regular volume with the type set to persistent volume claim.</p>
<p>Both volumes and persistent volumes may be backed by a wide variety of volume types. It is usually preferable to use persistent volumes for more durable types and volumes for more ephemeral storage needs. Durable volume types include the persistent disks in many cloud vendors, such as Google Cloud Engine persistent Disks, Azure Disks, and Amazon Elastic Block Store. There’s also support for more generic volume types, such as network file system or NFS and iSCSI. That is quite a lot to take in, but everything should solidify with an example.</p>
<p>So our objective is to use a persistent volume for the sample application’s data tier, since we want the data to outlive the pod. In our example, the cluster has an Amazon Elastic Block Store volume statically provisioned and ready for us to use. To see dynamic provisioning in action, I’d encourage you to complete the lab on CloudAcademy entitled “Deploy a Stateful Application in a Kubernetes Cluster.”</p>
<p>Before we get into volumes, I want to cement the issue we are trying to solve. We can illustrate the issue of a pod computer losing data when they restart by forcing a restart of the data tier pod. First off, let’s look at the counter that’s been running since our deployments lesson. That is the value of our counter at the moment, and every second, it will keep increasing.</p>
<p>Now, if I force the pod to be restarted, we can observe the impact on the counter. One way to do that is to kill the Redis process, which’ll cause the data tier container to exist, and the data tier pod will automatically restart. We can use the exec command and run a command inside of the container the same way Docker exec does. So let’s open a bash shell inside of that container.</p>
<p>The change of a command prompt tells us we’re in the container now. We can now use the kill command to stop the main process of the container. But what is that ID? The ID of the main process, which is Redis in this case, will always be one, since it is the first process that runs inside of the container. We can see the command prompt change back, since the container terminated, so our shell was also terminated.</p>
<p>Now we can look at the counter value through the poller logs again and see that it is much lower than before because the data tier was completely wiped out when the pod restarted. This is what we want to avoid. So let’s create our new namespace and get on to the volumes. Now on to the data tier. There are three additions to the manifest, a persistent volume, a persistent volume claim, and a volume to connect the claim to the pod.</p>
<p>First is our persistent volume. This is the raw storage where the data is ultimately written to by the pod’s container. It has a declared storage capacity and other attributes. Here we’ve allocated one gibibyte. The access mode of read-write once means this volume may be mounted for reading and writing by a single node at a time. Note that it is a limit on the node attachment, not pod attachment.</p>
<p>Persistent volumes may list multiple access modes in the claim that specifies the mode it requires. The persistent volume can only be claimed in a single access mode at any time. Lastly, we have an AWS Elastic Block Store mapping, which is specific to the type of storage backed by the PV. You would use a different mapping if you were not using an EBS volume for storage. And the only required key for AWS’s Elastic Block Store is the volume ID, which is uniquely identified by the EBS volume. It will be different in your environment than mine, so I’ve added an insert volume ID placeholder that we will place before we recreate the PV. </p>
<p>Next, we have the persistent volume claim. The PVC spec outlines what it is looking for in a persistent volume. For a persistent volume to be bound to a PVC, it must satisfy all the constraints in the claim. We are looking for a persistent volume that provides the read-write once access mode and has at least 128 mebibytes of storage. The claim request is less than or equal to the persistent volume’s capacity and the access mode overlaps with the available access modes in the persistent volume. This means that the PVC request is satisfied by our persistent volume and will be bound to it.</p>
<p>Lastly, the appointment’s template now includes a volume which links the PVC to deployments pod. This is accomplished by using the persistent volume claim mapping and setting the claim name to the name of the persistent volume, which is data tier volume claim. You will always use persistent volume claim when working with PVs. If you wanted to use an ephemeral storage volume, you would replace it with an emptyDir mapping or other types that don’t connect to a persistent volume.</p>
<p>Volumes can be used in the pods containers and init containers, but they must be mounted to be available in the containers. The volume mounts list includes all the volume mounts for given container. The mount pass for different containers could be different even if the volume is the same. In our case, we only have one, and we are mounting the volume at slash data, which is where the Redis is configured to store its data. This will cause all of the data to be written to the persistent volume.</p>
<p>Now we are left with replacing the volume ID placeholder with the actual ID of the Amazon EBS volume. You can get it from the EC2 console in your browser, but we’ll just use the AWS CLI in this example. The volume can be obtained from the AWS EC2 describe command as follows. The full command is available to copy in the introduction to Kubernetes playground lab. The filter only selects the persistent volume which is labeled with the type equals PV tag, and the query outputs only the volume ID of the property volume.</p>
<p>The introduction to the AWS CLI on CloudAcademy explains this more so in greater detail. We only need to get the volume ID in this case. Then we can use the stream editor or set to substitute the occurrence of insert volume ID with the volume ID stored in vol_id. And with that, we are ready to create the data tier using a persistent volume. We’ll also create the app and support tiers, which don’t have anything new compared to the previous versions.</p>
<p>Let’s get the persistent volume claim, which has a short name of PVC in kubectrl, to confirm the claims request is satisfied. The status of bound confirms that the persistent volume claim is bound to the persistent volume. Now, if we describe the data tier pod, we’ll see that the pod initially failed to schedule because the claim needs to wait a while before it is bound to the persistent volume.</p>
<p>Once it is bound, the pod is scheduled and we can see a successful attachment with the volume event. Not only can our new design tolerate data tier pod container restart, but the data will persist even if we delete the entire data tier deployment. If everything goes to plan, we should be able to recover the Redis data if we then replace the deployment. That is because the deployment template is configured to use the PVC, and the PVC is still bound to the persistent volume storing our original Redis data.</p>
<p>So let’s verify all of this. Before we delete the data tier deployment, let’s get the last log line from the poller to see where our counter is at. If we delete the deployment and then replace it, we should see a number higher than this if the data is persistent. So let’s do that. And now we’re going to confirm that there are no data tier pods left running, and we’ll now recreate the data tier deployment.</p>
<p>Now, it takes a couple of minutes for all the readiness checks to start passing again and for some old connections to time out. This is mainly a side effect of the example application not being particularly good at handling this type of situation. So after a minute or two, we can get the poller’s last log, and voila, the counter has kept on ticking upward from where we left off before deleting the deployment.</p>
<p>Our persistent volume has lived up to its name. This concludes our lesson on volumes. We’ve covered volumes, persistent volumes, and persistent volume claims. In our example, we’ve shown how to use a persistent volume to avoid data loss by keeping the data independent from the life cycle of the pod or the pod’s volume. We also saw how kubectrl exec allows us to run commands in existing containers when we demonstrated how container restarts cause data loss when volumes aren’t used. You now have a solid foundation for volumes and persistent volumes.</p>
<p>In our next lesson, we’re gonna be covering two other useful Kubernetes features that you should keep in your toolbox. They also have a nice tie-in with volumes. There’s a few more lessons to go. Keep it up and I’ll catch you in the next one.</p>
<h1 id="ConfigMaps-and-Secrets"><a href="#ConfigMaps-and-Secrets" class="headerlink" title="ConfigMaps and Secrets"></a>ConfigMaps and Secrets</h1><p>Up until now, the deployment template has included all of the configuration required by Pod containers. This is a big improvement over storing the configuration inside the binary or container image. Having configuration in the Pod spec, also makes it a lot less portable. Furthermore, if the configuration involves sensitive information, such as passwords, API keys, this also presents a security issue.</p>
<p>So Kubernetes provides us with ConfigMaps and Secrets, which are Kubernetes resources that you can use to separate the configuration from the Pod specs. This operation makes it easier to manage and change configurations. It also makes for more portable manifests. ConfigMaps and Secrets are very similar and used in the same way when it comes to Pods. One key difference is that Secrets are specifically for storing sensitive information. Secrets reduce the risk of their data being exposed. However, the cluster admin also needs to ensure that all the proper encryption and access control safeguards are in place to actually consider Secrets being safe. We’ll focus on Secrets and leave out the security details from this introductory course.</p>
<p>Another difference is that Secrets have specialized types for storing credentials, such as requiring to pull images from registries. They also are good at storing TLS private keys and certificates. But I’ll refer you to the official documentation we need to make use of those capabilities. ConfigMaps and Secret store data as key value pairs. Pods must reference ConfigMaps or Secrets to use their data. Pods can use the data by mounting them as files through a volume or as environment variables. We’ll see examples of these in the demo.</p>
<p>We’re going to be using a ConfigMap to configure Redis using a volume to mount a Config file will use and a Secret to inject sensitive environment variables into the app tier. First, let’s create a Config namespace for this demo. With key control create -f 10.1.namespace.yaml.</p>
<p>Now let’s see how the ConfigMap manifest looks. First notice that there is no spec rather than we have key value payers of the ConfigMap stores and they’re under a mapping named data. Here we have a single key named Config. You can have more than one but one is enough for our purpose. The value of Config is a multiline string that represents the file contents of a Redis configuration file. The bar or pipe symbol after ConfigMap is Yaml for starting a multiline string and causes all the following lines to be the value of Config including the Redis Config comment. The configuration files set the TCP keep alive in max memory of Redis. These are arbitrarily chosen for this example. Separating the configuration makes it easy to manage configuration separately from the Pod spec. we will have to make some initial changes to the Pod to make use of the ConfigMap but after that, the two can be managed separately.</p>
<p>Let’s take a look at the updated data tier. I’m comparing what the data tier from our probes lesson which doesn’t include the persistent volume to avoid not being able to satisfy the persistent volume claim. Starting from the volumes, a new ConfigMap type of volume is added and it references the Redis config ConfigMaps we just saw. Items declare which key value pair we want to use from the ConfigMaps. We only have one in our case, and that is Config.</p>
<p>If you have multiple environments, you could easily do things like referencing a dev configuration in one environment and a production configuration in another. The path sets the path of the file that will be mounted with a Config value. This is relative to the mount point of the volume. Up above in the container spec, the volume mounts mapping declares the use of the Config volume but amounts of that etc Redis. So the full absolute path of the Config path, will be etc Redis, Redis Comf.</p>
<p>The last change that we need is to use a custom command for the container so that Redis knows to load the Config file when it starts. We do that by setting the Redis server, etc Redis comp as the command. With this setup, we can now independently configure Redis without touching the deployment template. As a quick side note, before we create the resources, if we’re dealing with the Secret rather than a ConfigMap, the volume type would be Secret rather than ConfigMap. And the name key would be replaced with secret name. Everything else would be the same.</p>
<p>Let’s create the resources. Now let’s start a shell in the container using Q control exec to inspect the effect of our ConfigMap. Start by cutting out the contents of the etc Redis, Redis Conf file. See that the contents match the ConfigMap value that we specified. Now to prove that Redis actually loaded the Config, we can output TCP, keep alive configuration value and make sure it matches the two 40 value in the file. And there we have it, separation of configuration and Pod spec is complete, so let’s exit out of the container.</p>
<p>Before we move on, I wanna highlight how changes to the ConfigMap interact with volumes and deployments. So let’s use Kube control edit to update the ConfigMap. And let’s change the TCP keep alive value from 240 to 500. Within around a minute, the volume will reflect the change we made to the ConfigMap. That is pretty slick, but Redis only loads the configuration file in start-ups so it won’t impact the running Redis process. And because we never updated the deployments template, we’d never triggered a rollout.</p>
<p>So let’s confirm the TCP keep alive Redis hasn’t been updated using the Redis CLI. There is something to keep in mind when you separate the configuration from the Pod spec to cause the deployments Pods to restart and have Redis supply the new configuration changes, we can use kube control rollout, namespace Config, restart deployment data-tier. This will cause a rollout using the current deployment template, and when the new Pod start, the Redis containers will use the new configuration. We can verify that with the Redis CLI.</p>
<p>Now we can quickly see how secrets work and see the similarities they have with ConfigMap. We will add a secret to the app here using an environment variable. It won’t have any functional impact but it will show the idea. Here is our Secret manifest, I mentioned upfront that you usually don’t wanna check in secrets to source control given their sensitive nature. It makes more sense to have secrets managed separately. You could still use manifest files as we are here, or the Secret could be created directly with kube control.</p>
<p>The command at the bottom of the file shows how to create the same Secret without a manifest file. Focusing on the manifest file, we can see that itself has a similar structure as our ConfigMap, except for the kind being Secret rather than ConfigMap. And Secrets can use a string data mapping in addition to the data one we use in our ConfigMap. As part of the effort to reduce the risk of Secrets being exposed in plain text, they are stored as base-64 encoded strings and Kubernetes automatically decodes them when using a container. I also have to point out that basics and foreign coding does not really offer any additional security. It’s not encrypting the values, and anyone can decode base-64, so to continue to treat the encoded strings as sensitive data.</p>
<p>With that cautionary statement out of the way, the stream data mapping allows you to specify Secrets with first encoding because kubernetes will coding them for you. It’s simply a convenience. If you use the data mapping, you must specify in coded values. In the API key secret is the one that we will use in the app tier, but I’ve included the encoded and decoded key value pairs to illustrate the basics for encoding.</p>
<p>In the data mapping, the encoded value is hello, base-64 encoded. So let’s create the Secret to see this. Now, if we describe the Secret, we can only see the keys. The values are hidden as part of the best effort to shield Secret values. We can see what the values are with kubecontrol edit. From here, we can see that string data mapping is not actually stored. The values are based 64 encoded and then added to the data mapping. The decoded value we entered in string data was hello, but now it is the base-64 encoded string beginning with AGV.</p>
<p>Shifting over to the app tier deployment, the API key environment variable is added. If value from mapping is used to reference it from the source for the value. Here, the source is Secret, so the secret key ref is used. If you need to get the environment variable from a ConfigMap rather than a Secret, you would use the ConfigMap key ref instead of Secret key ref. The name is the name of the Secret, and the key is the name of the key and the Secret you want to get the value from.</p>
<p>So let’s create the app tier now. And we can use the ENV command and the container dump all the environment variables. We can find the API key variable amid the wash of variables and observe the value as the decoded value that we entered in string data of our Secret manifest file and not encoded value. There is no need to decode the value inside of the container. I’ll just mention before wrapping up, that just like with using volumes to reference Secrets or Config maps, you should restart a roll out to how the deployment pods restart with a new version of the environment variables. Environment variables do not update on the flight like volumes. So actively managing the rollout is must.</p>
<p>This concludes the lesson on ConfigMaps and Secrets. So let’s recap what we’ve learned. ConfigMaps and Secrets are used for separating configuration data from Pod specs or what would otherwise be stored in container images. ConfigMaps and Secrets both store groups of key and value data. Secret should be used when storing sensitive data. Both can be accessed in the Pod containers by the referencing them using volumes or environment variables.</p>
<p>This was our last hands-on lesson for course. And the last hands-on lessons have prepped you to get started with managing and deploying applications in Kubernetes. For our next lesson, we’re gonna be highlighting some of the areas that the Kubernetes ecosystem is particularly strong with and that I think you should know about. You’re almost at the finish line, so join me in our next lesson and we’ll cross it together.</p>
<h1 id="Kubernetes-Ecosystem"><a href="#Kubernetes-Ecosystem" class="headerlink" title="Kubernetes Ecosystem"></a>Kubernetes Ecosystem</h1><p>The Kubernetes ecosystem is a very vibrant and healthy mixture of tools that can help you get more done and work more efficiently with Kubernetes. So I wanted to give you a sense of what’s happening around the core of Kubernetes. There are really way too many tools and topics to choose from, so I’ve only selected a few that are popular and worth knowing about. So, just know that I’m touching the surface of this ocean. </p>
<p>The first tool that I’ll mention is Helm. Helm is Kubernetes’ package manager. You write packages called charts. Then you use the helm CLI to install and upgrade charts as releases on your cluster. Charts contain all the resources like services and deployments required to run a particular application. Helm charts make it easy to share and complete applications built for Kubernetes. Helm charts can also be found on the Helm hub, similar to how you would find Docker images on Docker hub. </p>
<p>As an example of how you might use Helm. In our sample application, we used Redis for our data tier. So rather than build the data tier up from scratch and managing the resources ourselves we could take advantage of the register charts available for home. There is a highly available Redis chart that comes from the single point of failure in our example application. Charts can be installed with just a single Helm CLI command. Using available charts for running common applications can really free you up to focus on the applications that are core to your business. So definitely take a look at what’s available before deciding to roll your own solution.</p>
<p>Here’s another example. You can create a chart for your entire microservice application and pack up onto the services, deployments, everything and then subsequently publish it so other members of your organization or the public could use it. The next tool I want to mention is Kustomize with a K. Kustomize allows you to customize YAML manifests in Kubernetes. It can help you manage the complexity of your applications running inside of Kubernetes.</p>
<p>An obvious example is managing different environments such as tests and stage. We saw how we could use config maps and secrets to help with that. But Kustomize makes it even easier. Kustomize works by using customization dot YAML file that declares rules for customizing or transforming resource manifest files. The original manifests are untouched and remained usable as they are, which is an, a massive benefit compared to other tools that required templating in the manifest, rendering them unusable on their own.</p>
<p>Some examples of the kinds of rules you can create with Kustomize are generating config maps and secrets from files. In our data tier we had to write the contents of the Redis config file instead of the config map data. With Kustomized you can generate it directly from the config file rather than trying to keep track of the config file and config map and keep them tied together and in sync. You can also configure common fields across multiple resources. For example, you can set the namespace, labels, annotations, and name prefixes and suffixes using Kustomize itself. That makes it easy to customize around your organization’s conventions without polluting the original manifest files. The original manifest remain pristine and easy to share, and also reusable in other situations.</p>
<p>In our example, we had to keep our tier labels and prefixes manually synchronized across all of the resources. And then we had to specify the namespace at the command line every time to avoid hard coding in any space in the manifest. Kustomize can contain all of that complexity for us. The other thing that Kustomize can do is apply patches to any field in a manifest. Kustomize allows you to find a base group of resources and apply an overlay to customize to a base. This is an easy way to manage separate environments by applying a dev name prefix and a label for development environments.</p>
<p>Kustomize has been directly integrated with kubectl since Kubernetes 1.14. The kubectl customized command prints the customized resource manifests with customization defined in customization dot YAML. To accept the customization and then realize them in your cluster you can include the –kustomize or - k option to cube control create or apply.</p>
<p>The next one I’d like to discuss is Prometheus. Prometheus is an open source monitoring and alerting system. Prometheus’s built on top of many components, but at its core is a server for pulling in time series metric data and storing it. Prometheus was originally inspired by an internal monitoring tool at Google called borgmon. Similar to how Kubernetes itself was inspired by the board project at Google. Given that history it should come as no surprise that Prometheus is the de facto standard solution for monitoring Kubernetes.</p>
<p>Kubernetes components supply all their own metrics and Prometheus format makes it easy to integrate. You can collect a lot more metrics in Prometheus in the basic metric server we used in this course. That includes metrics outside of Kubernetes. There’s also adapters that allow Kubernetes to get metrics from Prometheus so you can do things like auto-scale pods based on custom metrics in Prometheus instead of only the CPU utilization that we saw in the course. </p>
<p>Prometheus has some built in options for visualization but it is commonly paired with Grafana to to create visualizations and dashboards. Prmoetheus also lets you define alert rules to send out notifications. It’s incredibly easy to install Prometheus in a cluster, and one way to do it is by using a Helm chart. </p>
<p>I’ll round out our talk about all these tools with two members of the Kubernetes ecosystem that relate to the types of applications that you deploy on top of Kubernetes. The first is Kubeflow. Kubeflow is aimed at making deployment of machine learning workloads on Kubernetes simple, scalable, and portable. </p>
<p>Kubeflow is a complete machine learning stack. You can use it for complete end to end machine learning including building models, training them, and serving them all within Kubeflow. Being built on Kubernetes, you can deploy it anywhere and get all of the nice features that Kubernetes provides like auto-scaling. Definitely check out Kubeflow if your requirements involve machine learning. </p>
<p>And the last one is Knative. Knative is a platform built on top of Kubernetes for building, deploying, and managing serverless workloads. Serverless has gained a lot more momentum because it allows developers and companies to focus more on the code and less on the servers that run it. This trend started with AWS Lambda which is synonymous with serverless. However, as the industry shifts to multi-cloud and avoiding vendor lock-in solutions built on top of Kubernetes can be deployed anywhere. This gives you the portability that you would get with containers, but for your entire serverless platform. Knative is not the only game in town when it comes to serverless but it does have the support of industry heavyweights like Google, IBM, and SAP. </p>
<p>This concludes our lesson about the Kubernetes ecosystem. We only touched on a few topics but I hope you can see the breadth of the ecosystem. I hope you can use some of these tools that we’ve seen but know that there’s a lot more to explore on your own.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>This is the last lesson in this course. I wanna congratulate you for completing this course in Kubernetes. In this lesson, I’m gonna be summarizing the high-level learning outcomes you’ve achieved and gonna be providing you with the next step for your Kubernetes journey. We’ve covered a lot of ground together. We started by explaining what Kubernetes is in our overview lesson. We followed that up with a summary of the architecture of Kubernetes and some of your options for deploying Kubernetes itself. </p>
<p>Next, we discussed the options available to you for interacting with Kubernetes. No doubt you’ve gotten to know kubectl much better than since when we first introduced it. Then we moved into the technical basics and built an application from a single pod all the way up to a self-healing, auto-scaling multi container, multi-tier application. Along the way, we’ve learned the following, Kubernetes terminology, how any number of deployments with service discovery work. Triggering, pausing, resuming, restarting, and undoing deployment roll-outs. Monitoring container liveness and readiness with probes. Preparing pods within init containers. How to configure persistent data storage which involves persistent volumes, persistent volume claims in volumes. And lastly, how to separate configuration and sensitive data from pod specs with containers using ConfigMaps and Secrets.</p>
<p>These learning outcomes should give you the confidence to start deploying applications with Kubernetes. The previous lesson took a step back and allowed you to see some of the exciting tools available to you in the vibrant Kubernetes ecosystem. So now that you’ve built a solid foundation in Kubernetes, where should you go next? Well, I’ve got a few recommendations. Cloud Academy has a lot more content on Kubernetes. In addition to the Kubernetes learning path, I’d encourage you to take a look at the certified Kubernetes administrator learning path or the certified Kubernetes application developer learning path. Both get into more details and describe more about topics that we’ve already covered but just didn’t have the time to fully explore. You can decide which one is best for you based on your current role or the kind of role you want to pursue, whether that’s more on the administrative side or more on the developer side. Both learning paths include tips on how to become a kubectl ninja and do things like generating manifests and explaining field values amongst other tips.</p>
<p>I’d also recommend that you convert your own application to run on Kubernetes. We did a similar exercise in this course, but I recommend taking an application that you know well and modeling it out to be a Kubernetes application. This will reinforce everything we have learned, and it can help you find different solutions to already existing challenges. You can also try to package your solution as a Helm chart if you want something you can share with others.</p>
<p>My last recommendation is to participate in the Kubernetes community. There are many ways to get involved. And on GitHub, you can report issues, help others solve their own issues, contribute updates and improvements to documentation, or submit pull requests for new features. If there’s a specific area of Kubernetes you are interested in, you can consider joining a special interest group, which is how Kubernetes organizes the community. There are also conferences, Slack channels, and Google groups to follow what’s happening so you can connect with other members of the community.</p>
<p>KubeCon is the flagship conference and is hosted virtually every year, and when available, in person. And with that, our journey through Kubernetes comes to an end. Please share your feedback so I can continue to improve and deliver just what you’re looking for. It may be the end of this course, but it’s only the beginning of a long and productive relationship with Kubernetes. Thank you for taking this course. And until next time, I’m Jonathan Lewey with Cloud Academy.</p>
<h1 id="1Introduction"><a href="#1Introduction" class="headerlink" title="1Introduction"></a>1<strong>Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/intro-to-k8s">Course GitHub repo</a></p>
<h1 id="11Autoscaling"><a href="#11Autoscaling" class="headerlink" title="11Autoscaling"></a>11<strong>Autoscaling</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/metrics-server">Metrics Server GitHub repo</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:40" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:40-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:25:30" itemprop="dateModified" datetime="2022-11-20T22:25:30-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Introduction-to-Kubernetes-Playground-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9/" class="post-title-link" itemprop="url">Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:44:38" itemprop="dateCreated datePublished" datetime="2022-11-19T00:44:38-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 22:24:52" itemprop="dateModified" datetime="2022-11-20T22:24:52-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Docker-Certified-Associate/" itemprop="url" rel="index"><span itemprop="name">Docker-Certified-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Docker-Certified-Associate-Software-Development-Testing-and-Delivery-with-Docker-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/15/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><span class="page-number current">16</span><a class="page-number" href="/page/17/">17</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/17/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
