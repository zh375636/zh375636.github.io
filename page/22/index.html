<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/22/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/22/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-102-LPIC-1-102-Linux-certification-Linux-Desktops-2-of-6-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-102-LPIC-1-102-Linux-certification-Linux-Desktops-2-of-6-3/" class="post-title-link" itemprop="url">Linux-LPIC-102-LPIC-1-102-Linux-certification---Linux-Desktops-2-of-6-3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:11:44" itemprop="dateCreated datePublished" datetime="2022-11-19T01:11:44-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 21:16:02" itemprop="dateModified" datetime="2022-11-20T21:16:02-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-102/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-102</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-102-LPIC-1-102-Linux-certification-Linux-Desktops-2-of-6-3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-102-LPIC-1-102-Linux-certification-Linux-Desktops-2-of-6-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Even though the vast majority of videos in this series are devoted to working with the Linux server, in fact, the certification we’re working towards is called Server Professional. A Linux admin must also know how to build and manage graphic-based interfaces so users will be able to get much of their daily work done.</p>
<p>This rather brief course will focus on the display managers and graphic desktop environments that make all that possible.</p>
<p>We’ll explore how to make sure that the hardware peripherals you’d like to work with are actually compatible with Linux, and if they are, how to get even non-mainstream hardware configurations working properly with your system. We’ll also demonstrate the use of some particularly useful diagnostic tools for displaying hardware profile information and for controlling remote graphic login sessions. We’ll learn about display managers, what they do, and how we can get them do it, and then particularly LightDM display manager, which the latest LPIC exam released considers the most important in its class. In the System Accessibility lecture, we’ll learn about the Linux screen, keyboard, and mouse adaptation utilities meant to improve the computing experiences for users with partially or fully impaired vision, hearing, or movement. Finally, we’ll have some fun building a graphic user interface on top of a virtual server running on an Amazon Web Services EC2 Instance.</p>
<h1 id="Install-and-configure-desktops"><a href="#Install-and-configure-desktops" class="headerlink" title="Install and configure desktops"></a>Install and configure desktops</h1><p>As you are probably already aware, these days the XWindows system on most Linux distros will automatically recognize the hardware peripherals, like monitors and mice, that are connected to the system. But this won’t always be the case, especially if you ever have to work with minimal or older hardware running some of the less consumer-oriented software packages.</p>
<p>Therefore, a Linux sysadmin needs to know how to work with all of the available configuration tools. But before we get to that point, it’s important to know how to find out whether a particular hardware device is actually compatible with the Linux kernel. Checking in advance can obviously save you both unnecessary expense and trouble. Depending on the class of peripheral you’re looking at, there’s actually quite a range of excellent community and vendor based resources. Video cards are often very complicated devices to get working on non-mainstream systems, and there’s plenty of help available online.</p>
<h3 id="Configuring-Linux-desktops-using-xwindows"><a href="#Configuring-Linux-desktops-using-xwindows" class="headerlink" title="Configuring Linux desktops using xwindows"></a>Configuring Linux desktops using xwindows</h3><p><a target="_blank" rel="noopener" href="http://xorg.freedesktop.org/wiki/projects/drivers">http://xorg.freedesktop.org/wiki/projects/drivers</a> contains links to information on most major and some minor chip sets. As it turns out, in many cases, you won’t even need to go that far. The man help system has more useful information on this than you might think. Running Man ATI for instance, will give you a compatibility overview of all ATI graphing drivers. Man trident will do the same for trident drivers.</p>
<p>Once you’re confident that your device should work, you’ll have to actually get it going. If Linux doesn’t pick up the device on its own, you might need to work with configuration files. For XWindows, you’ll need to edit either xorg.conf or xf86Config. Both of these files will usually live in the &#x2F;etc&#x2F;X11 directory. Note the X of X11 is upper case. But here, things get a bit weird. If you’re running Ubuntu or Fedora, or another modern consumer friendly distribution, then neither of those two files will likely exist. The standard setup simply doesn’t use them anymore and hasn’t for years. But that doesn’t mean you can’t create your own. And as I mentioned just before, if you’re trying to get a non-standard hardware combination going, or you’d like to change the mode used by a video driver, you may very well need to create one.</p>
<p>Here’s a sample XOrg.com file. You can see that the file is divided into sections, like server layout and files. Most sections contain information identifying a device and setting the mode and configuration in which it’ll run. Thus, the first input device section identifies a mouse, its driver, and a number of options. The second input device section does the same for a keyboard. The monitor section identifies your monitor and the device section, your video driver.</p>
<p>This next section, called Screen is interesting. It binds together the video driver from device, and the monitor from the monitor section. The most important section of them all is server layout, which in this example, is all the way at the top. Server layout glues all the sections together into a single working hardware profile.</p>
<p>Linux comes with some very useful configuration diagnostic tools. XWinInfo will display information about a particular window. Let’s run XWinInfo from this terminal and then click on the terminal itself. Of course, we could also have clicked on any other window that happens to be open on our desktop. We’re given the window’s current screen coordinates, size, color depth, and other details. If you’re on a system that has access to multiple monitors, you might want to confirm how a particular one of them is designated. This you can do through the display variable. Running echo display, taking care to type display in upper case characters, tells us what we want to know.</p>
<p>XDPYinfo, which I believe stands for X Windows display information, will return the current system display configuration, along with all the alternate display and modes that are possible using your hardware profile. As you can see, the problem with XDPY info is that there’s really too much information to absorb so quickly. Let’s slow that down a bit by piping it to less, which will let us view it one screen at a time. Now we can see information about display size, a list of available extensions, and then details about the screen’s capabilities. I have to admit that I don’t understand too much about this, but one day you might really appreciate it.</p>
<h3 id="Controlling-access-to-Linux-desktop-sessions"><a href="#Controlling-access-to-Linux-desktop-sessions" class="headerlink" title="Controlling access to Linux desktop sessions"></a>Controlling access to Linux desktop sessions</h3><p>Since it’s possible for remote users to log into your machine and start a graphic session, for security reasons, it’s very important to know how to control who and if anyone will be allowed. Naturally, your private IP address should never be exposed to anyone outside your local network, and if your routing rules are properly set up, that should never happen. But you might also want to control which local network users are granted access.</p>
<p>You do this with xhost. xhost followed by the plus sign, will enable graphic access to anyone. xhost and the minus sign will restrict access to anyone who hasn’t explicitly been allowed. You can permit access from computers using a particular IP address using xhost + followed by the IP address. And xhost with the minus sign and an IP address, will remove access from an IP address that had been previously allowed.</p>
<p>Finally, the LPIC expects you to be familiar with the X font server. Now I have to tell you that the people responsible for maintaining the LPIC exam, who do a terrific job by the way, already three years ago, seem pretty much an agreement that the X font server should be removed from the list of exam expectations. They know that there’s no way anyone will ever again have to actually use it, nor has it been used for many years. I’m not exactly sure why then, after all these years have passed, X font server is still there. But in any case, consider yourselves familiar with it.</p>
<p>Let’s review. You can use the man system to view configuration details for a graphic hardship sets, the files, XOrg.com and XF86config found in the &#x2F;etc&#x2F;x11 directory are used to configure system peripherals. The screen section of the XOrg.com file binds together data from the monitor and device sections, and the server layout section glues all sections together to create a total system profile. XWin Info displays information about a particular window. The display variable contains your display designation. XDPY info returns information about your current display. xhost + will allow access to any remote graphic logins, xhost - will restrict access to all but explicitly approved logins. xhost + and an IP address will add the address to the allowed list. And xhost - and an address will remove that address from the allowed list. Finally, X Font server doesn’t do anything at all.</p>
<h1 id="Working-with-Display-Managers"><a href="#Working-with-Display-Managers" class="headerlink" title="Working with Display Managers"></a>Working with Display Managers</h1><h3 id="Working-with-Linux-display-managers-including-LightDM"><a href="#Working-with-Linux-display-managers-including-LightDM" class="headerlink" title="Working with Linux display managers including LightDM"></a>Working with Linux display managers including LightDM</h3><p>The role of Linux display managers has historically been something of a moving target, while managers like XDM, KDM, and GDM have and in some cases, still do enjoy widespread adoption, as of the most recent update to their exam topics, the LPIC wants us to focus most of our attention on LightDM. Nevertheless, we should spend just a few moments discussing those three older display managers. XDM, the X window<br>display manager has been around since 1988. That’s 1988, long before Linux was born. It’s a very minimal environment that’s really only useful for supporting standalone X terminal-like systems.</p>
<p>The Gnome display manager, GDM was designed as a graphical replacement for XDM, and allows users to manage their environment settings for both attached and remote displays through GUI rather than command line tools. KDM is the display manager of the KDE desktop environment. KDE, by the way, has been competing for some years against Gnome and among others, Ubuntu’s Unity, for the hearts and minds of free desktop<br>environment users.</p>
<p>Because it’s designed to work without the need to load much overhead at all, LightDM is quite a bit faster than some of the older alternatives. It also does quite a bit less than traditional display managers, which would provide window control elements, manage virtual desktops and provide windows frames functionality. LightDM, on the other hand, is normally only there to start X servers, user sessions, and the greeter login screen.</p>
<p>Let’s see how system administrators can manage LightDM configurations. While LightDM will read configuration details from files in the &#x2F;user&#x2F;share&#x2F;lightdm&#x2F;lightdm.conf.d directory, you can only edit those values<br>from either files in the &#x2F;etc&#x2F;lightdm&#x2F;lightdm.conf.d directory, or from the lightdm.com file in the &#x2F;etc&#x2F;lightdm directory. In my case, there are no files in the &#x2F;etc&#x2F;lightdm&#x2F;lightdm.conf.d directory, and<br>lightdm.conf, as you can see, states that my current default desktop session will use Gnome Fallback. If I wanted to change that value, I would either edit this file or create a file with the same contents in the lightdm.conf.d directory, named something like 50-myconfig.conf.</p>
<p>You can see a sample configuration file with all the available options in your &#x2F;user&#x2F;share&#x2F;doc&#x2F;lightdm directory. Since the file is compressed, we’ll use gunzip to make it readable. And since we’re in the &#x2F;user directory tree, we’ll need to become sudo to save the decompressed file.</p>
<h3 id="Configuring-the-lightDM-display-manager"><a href="#Configuring-the-lightDM-display-manager" class="headerlink" title="Configuring the lightDM display manager"></a>Configuring the lightDM display manager</h3><p>There are a number of configurations you can control from the LightDM.conf file. By default, users can log into a lightDM system without a password, as guest. If you would like to disable this, add a line<br>reading allow-guest&#x3D;false to the lightdm.conf file. Normally, the greeter screen displays a menu list of system users. If you’d like to disable this, which would require users to type their user names in manually, add greeter-hide-users&#x3D;true. You can set the system to automatically log in a particular user by adding the auto login-user line to the lightdm.conf file, followed by the particular user name. If<br>you’d like to set a grace period within which users can override the auto login default, add the time outline, where you can specify the length of the delay period in seconds. If you’re building some kind<br>of public access kiosk, you might like to set the auto login to a guest session. You can do this with auto login-guest&#x3D;true, and obviously without the timeout option. To change the greeter itself, you can<br>reference a greeter.desktop file in your &#x2F;user&#x2F;share&#x2F;xgreeters directory from the lightdm.conf file, using something like this. Finally, if you want LightDM to run a script on start up, you can add a reference to this command as part of a session-setup-scriptline. LightDM has more options that you can see in the sample.com file we viewed earlier, in &#x2F;user&#x2F;share&#x2F;doc&#x2F;lightdm.</p>
<p>Let’s review what we now know about the LightDM display manager. LightDM reads files in the &#x2F;user&#x2F;share&#x2F;lightdm&#x2F;lightdm.conf.d directory, but the config values are edited, using &#x2F;etc&#x2F;lightdm&#x2F;lightdm.conf, or individual files in &#x2F;etc&#x2F;lightdm&#x2F;lightdm.conf.d. By adding entries to the lightdm.conf file, you can control the display manager’s login behavior. Adding allow-guest&#x3D;false will prevent guest user logins. Greeter-hide-users&#x3D;false will prevent a user menu from being displayed. Autologin user&#x3D;username will automatically log the specified user into the system on boot. And autologin guest&#x3D;true will create a public access kiosk. You can change the greeter itself by pointing to the appropriate file in &#x2F;user&#x2F;share&#x2F;xgreeters. And finally, you can have light DM run scripts or commands for you at start up.</p>
<h1 id="Manage-system-access-settings"><a href="#Manage-system-access-settings" class="headerlink" title="Manage system access settings"></a>Manage system access settings</h1><h3 id="Managing-Linux-system-accessibility-for-people-with-disabilities"><a href="#Managing-Linux-system-accessibility-for-people-with-disabilities" class="headerlink" title="Managing Linux system accessibility (for people with disabilities)"></a>Managing Linux system accessibility (for people with disabilities)</h3><p>Linux developers have done terrific work opening up the operating system to users with disabilities. There are now utilities to assist individuals with limited vision, hearing, or movement. In this video, we’ll explore how to work with the accessibility features available for Linux.</p>
<p>I will note that each Linux desktop, whether it’s Unity, KDE, or some flavor of Gnome will place menu links to their accessibility control interfaces in different places. In fact, individual desktops will often undergo significant changes to their menu structure between one release and the next, which can sometimes be rather annoying. But for our purposes, it’s not really so important exactly which menu sequence you use to get to them, as long as you are aware of these features and know how to apply them.</p>
<h3 id="Linux-visual-and-audio-access-tools"><a href="#Linux-visual-and-audio-access-tools" class="headerlink" title="Linux visual and audio access tools"></a>Linux visual and audio access tools</h3><p>As it happens, I’m working with Ubuntu 14.04’s Gnome fallback desktop. Accessibility tools are meant to help users access their screen, audio, keyboard, and mouse. We’ll start with screen utilities. When you toggle the Orca Screenreader on, a voice will automatically audibly read whichever screen element currently has focus. It will therefore, read out each of the menu items that I click on or hover over as I work through options within a new menu I’ve opened. Using the keys on the number pad of your keyboard, you can also make Orca read previous or subsequent lines of text in relation to your cursor or window title and menu bars.</p>
<p>Turning on high contrast will temporarily reset your desktop theme to be more readable. It may look a bit ugly to us, but it can make a computer usable for someone with impaired vision. Large text will increase the text scale factor by about 25%. There is currently a known bug in the large text module that’s affecting some Linux installations, including mine. Braille display will use the BRLTTY daemon to output text to USB connected, refreshable Braille display devices. Just in case you aren’t aware, Braille is a universally used writing system that allows blind people to read using their sense of touch.</p>
<p>Oddly enough, the Linux screen magnifier has not always been installed by default over the past few years. As you can see from my universal access interface, there’s currently no link. While I can’t explain why that should be, you can easily fix it by installing a screen magnification software package, like KMag. Clicking on the mouse button on the type right will cause the KMag display to follow your mouse around your screen, displaying enlarged images.</p>
<p>For users with hearing impairments or perhaps individuals whose PCs lack speakers, missing audio alerts can be a concern. To add a visual cue to every audio alert, you can toggle visual alerts, and select between having just the window title flash or the whole screen.</p>
<h3 id="Linux-keyboard-and-mouse-access-tools"><a href="#Linux-keyboard-and-mouse-access-tools" class="headerlink" title="Linux keyboard and mouse access tools"></a>Linux keyboard and mouse access tools</h3><p>Using a keyboard and mouse can be particularly challenging for people with reduced motor control. Just imagine how hard it can be to execute certain multiple keystroke combinations if you can barely lift your arm, or even if your arm is temporarily in a cast. Sticky keys will interpret the pressing of a modifier key, like control or shift, followed by any other keystroke, as though they were pressed together. This can be helpful for people who find simultaneous keystroke combinations a problem. Slow keys, bounce keys, and repeat keys can add greater accuracy for individuals unable to completely control their finger movement.</p>
<p>Slow keys lengthens the time between the initial key stroke and the point where the key stroke will register. Bounce keys inserts a longer delay between key strokes, ignoring accidental key strokes caused by unwanted movements between one key and another. Repeat keys, accessed through the keyboard menu will allow more time for user to release a key at the end of a key stroke before considering it as a repeated command. You can lengthen or shorten the delay or disable repeat keys altogether.</p>
<p>The way the system reacts to your mouse can be controlled by a number of utilities. Enabling mouse keys will allow your keyboard arrow keys to take over the mouse cursor movements, providing easier control and movement accuracy. If double-clicking your mouse is difficult, you can set your mouse to accept a longer, single click in place of a double click through simulated secondary click. The acceptance delay setting lets you define how long you’ll need to hold the mouse button down before a second click is simulated. Hover click, which is also sometimes referred to as dwell click, tells the system to interpret a mouse cursor that hovers for a set time as a click. This can help people who find it difficult to accurately click the left mouse button. It can also help children with smaller hands.</p>
<p>Finally, for people who are unable to use a keyboard at all, an onscreen keyboard displays a virtual keyboard that can be used with mouse clicks. Clicking on the large X at the top right of the keyboard will disable the tool. It took me a few anxious moments to figure that one out for myself.</p>
<p>Let’s briefly review. People with reduced vision can use the audible screen reader, high contrast, a screen magnifying tool, and the USB Braille interface. Visual alerts provide visual cues in place of audio alerts. Users with movement issues can get help with their keyboards through sticky keys, slow keys, bounce keys, and repeat keys. Help with mouse control is available through mouse keys, simulated secondary click, hover click, and through an onscreen keyboard.</p>
<h1 id="Add-a-graphic-interface-to-an-AWS-instance"><a href="#Add-a-graphic-interface-to-an-AWS-instance" class="headerlink" title="Add a graphic interface to an AWS instance"></a>Add a graphic interface to an AWS instance</h1><h3 id="Build-a-remote-graphic-Linux-desktop-on-an-AWS-EC2-instance"><a href="#Build-a-remote-graphic-Linux-desktop-on-an-AWS-EC2-instance" class="headerlink" title="Build a remote graphic Linux desktop on an AWS EC2 instance"></a>Build a remote graphic Linux desktop on an AWS EC2 instance</h3><p>Since we’ve been discussing how to manage Linux graphics desktops, it might be fun to see if we can actually build one on top of a Linux server. We’ll do that in a place you’d never normally expect to see anything graphic, on an Amazon web services EC2 virtual machine. Naturally, the real value of cloud based virtual machines is in their ability to launch light and serve web applications. That generally means terminal command line interfaces. However, you never know when you might have a need for a GUI Linux desktop in the cloud, so let’s give it a try.</p>
<p>We’ll first go to the EC2 dashboard and click on launch instance. We’ll select a 64 bit Ubuntu 14.04 Amazon machine image, and go with the T2 micro instance type. This type, by the way, will be too small and slow for most useful purposes, but it will serve as a nice illustration. We’ll accept the instance detail defaults and go onto storage, whose defaults we’ll also accept. Choosing a security group to control traffic into and out of our instance will be important. We’ll create a new security group, and for security purposes, first limit SSH access to my local IP address, which Amazon inserts automatically. Then, we’ll add a new custom TCP rule, which will allow traffic through port 5901, which happens to be the default port for VNC - VNC stands for virtual network computing - and limit its access to my local IP. We’ll review, confirm that my key pair is available, and launch the instance.</p>
<p>We’ll head back to the EC2 dashboard where, after a minute or two, our instance’s new public IP address will be displayed. Once we get that IP address, we can SSH into a terminal session, where we can set up our graphic desktop. I use -i to point to the newkey.pem key pair that’s in the current directory, and then ibuntu @ the IP address I was given. Since this is obviously the first time I’ve logged into this instance, I’ll have to acknowledge that the new authentication key will be added to my allowed list.</p>
<p>The first thing we will do in our brand new virtual server is create a new user who’ll be the owner of the VNC hosting session. He’ll need a password, too. Next, we’ll add our new user to the admin group, using user mod. Now we’ll use su to switch users to become our new user. There’s a simple change we’ll have to make to the sshd_config file to allow password based access to our system. We’ll scroll down to the password authentication line, and change its no value to yes. So that this change can take effect, we’ll restart SSH, using service restart.</p>
<p>We’re now ready to install the software that Ubuntu will need to host graphic sessions. In fact, Ubuntu desktop is a very convenient metapackage, that includes the dozens or even hundreds of packages that are necessary to get the graphic desktop running. Once that’s done, we’ll install the VNC4 server package to permit graphic hosting sessions. Now we’ll run VNC server, which will prompt us to create a VNC password. Remember the password because we’ll need it later.</p>
<p>Since we’re going to edit the VNC configuration, we’ll shut down VNC server using -kill. The VNC config files can be found in a hidden directory in my user’s home directory. ls -a will show all directory contents, including hidden. We’ll cd to the .vnc directory, and using Nano, open the X startup file for editing. Now here, we’re faced with a number of possibilities. For some people, it’s enough to simply uncomment these two lines and add sh after exec. But I found that for me, that configuration hasn’t been successful. So I’m going to replace the entire script with this text. Notice how the script calls Nautilus, the Linux file manager. This will start a session within Nautilus. If you would leave that line out, this session would open with a terminal window instead. That would be really nice and all but hardly useful, considering that we’re already in a terminal window.</p>
<p>We’ll exit and save the file and restart VNC server. We’ll now go back to our local computer and use Remmina, a very nice little remote desktop client to open a session on our AWS server. We’ll start a new connection using the VNC protocol. We’ll insert our EC2 instance’s public IP address, followed by a colon and the number one into the server field, and add our user name. That should be it. We’ll click connect and provide our VNC password.</p>
<p>And we’re in. It’s not much, and the connection is rather slow but it’s a living, breathing, GUI desktop running on the Amazon cloud.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-102-Linux-Command-Line-Byte-Session-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-102-Linux-Command-Line-Byte-Session-2/" class="post-title-link" itemprop="url">Linux-LPIC-102-Linux-Command-Line-Byte-Session-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:11:43" itemprop="dateCreated datePublished" datetime="2022-11-19T01:11:43-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:10:44" itemprop="dateModified" datetime="2022-11-20T20:10:44-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-102/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-102</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-102-Linux-Command-Line-Byte-Session-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-102-Linux-Command-Line-Byte-Session-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-102-LPIC-1-102-Linux-certification-Linux-shells-scripting-and-databases-1-of-6-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-102-LPIC-1-102-Linux-certification-Linux-shells-scripting-and-databases-1-of-6-1/" class="post-title-link" itemprop="url">Linux-LPIC-102-LPIC-1-102-Linux-certification---Linux-shells-scripting-and-databases-1-of-6-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:11:41" itemprop="dateCreated datePublished" datetime="2022-11-19T01:11:41-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:46:48" itemprop="dateModified" datetime="2022-11-20T20:46:48-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-102/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-102</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-102-LPIC-1-102-Linux-certification-Linux-shells-scripting-and-databases-1-of-6-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-102-LPIC-1-102-Linux-certification-Linux-shells-scripting-and-databases-1-of-6-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-introduction"><a href="#Course-introduction" class="headerlink" title="Course introduction"></a>Course introduction</h1><p>With this sixth course in Cloud Academy’s <a target="_blank" rel="noopener" href="https://cloudacademy.com/learning-paths/linux-certification-lpic-1-8/">Linux Certification Series</a> we’re going to encounter the first content that’s part of the LPIC-1 102 exam. The 102 is the second and final exam you’ll need to pass in order to earn your LPIC certification. But it’s also a terrific way to make sure that you’ve covered all the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/cloud-fundamentals/">skills</a> that you’ll need to be an effective system administrator, or as the LPIC calls Server Professional.</p>
<p>Specifically, in this course we’ll learn how to closely manage <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/working-with-linux-shells-1/">shell</a> sessions to leverage the full range of Linux system features. We’ll dive head first into scripting, the task automation platform which lets you apply<br>the logic and versatility of programming languages to the immediacy and high level control of the command line. And we’ll learn how to create and administer simple database installations using MySQL.</p>
<p>Finally we’ll learn how to connect to an Amazon RDS MySQL instance in the cloud and then my grade A local MySQL database into our cloud instance. This last topic may not be required for the LPIC exam, but you’ll be amazed how easily it can be done and it’s fun.</p>
<h1 id="Working-with-Linux-shells"><a href="#Working-with-Linux-shells" class="headerlink" title="Working with Linux shells"></a>Working with Linux shells</h1><p>While discussing the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/course-introduction-25/">Linux</a> Shell Environment in this video, we’re going to touch on three topics. How to control environment profiles for both login and non-login shell sessions. How to identify and create shell variables, aliases and functions. And a brief look at how you can execute multiple contingent commands.</p>
<p>Whenever you find yourself at the command line of a Linux terminal, you’re inside what is commonly called a shell session. It’s important to be aware, however, that not all shell sessions are created equal, and that you can control the environment parameters of your shell through various configuration files. These files will determine things like which flavor of shell, like bash or sh, will be used, what color scheme if any will be displayed, and what information will be included in the command prompt.</p>
<h3 id="Linux-shell-session-types"><a href="#Linux-shell-session-types" class="headerlink" title="Linux shell session types"></a>Linux shell session types</h3><p>First of all, we’ll divide shell session types into two broad categories, login, and non-login. When you SSH into a remote server or sit at a terminal attached to a non-gui computer, you’ll be required to provide some kind of authentication information before you’re given access. These sessions are called login shells. When, on the other hand, you open a terminal window from inside a GUI desktop session, you won’t normally need to authenticate. So it’s considered a non-login shell.</p>
<p>Why should we care? Because the configuration files used to control shell properties for login sessions are not the same as those read by non-login sessions. A login shell will first read and apply the contents of &#x2F;etc&#x2F;profile, and then look in the user’s home directory for each of bash_profile, bash_login, and .profile in that order. The parameters included in the first of these files that’s found will be applied. The environments for non-login shell sessions are set based on both the system-wide values in the &#x2F;etc&#x2F;bash.bashrc file and the .bashrc file in the user’s home directory. You can edit the settings in any of these files, but you should also know that doing it wrong can sometimes lead to rather unexpected results.</p>
<p>The dot at the beginning of these files tells the system that they’re hidden. Had I used ls without the -a to list the directory contents, these wouldn’t have shown up. There is actually an unconnected use of the dot, that all the same happens to be very closely related to our discussion. If you want to change profiles in the middle of a shell session, you can type say . space .bashrc, which will activate the parameters of the .bashrc file. You can similarly use source.bashrc to accomplish the same thing.</p>
<p>You should also be aware that the .bashlogout file controls how shell sessions end. Typically, the file will instruct Linux to clear the shell screen on exit out of security considerations. While this isn’t directly related to shell sessions as such, the LPIC exam expects that you’ll also be familiar with the &#x2F;etc&#x2F;skel directory. Files and directories that you place in skel, which is short for skeleton, will be copied to the home directories of all new user accounts that you create. This could be a great place to create a single company guide, or policy document that will automatically show up in all new accounts.</p>
<p>Let’s review. Shell environments are controlled by profile configuration files. For login sessions, Linux will read &#x2F;etc&#x2F;profile, and then the first of bash_profile, bashlogin, and profile. For non-login sessions, &#x2F;etc&#x2F;bash-bashrc comes first, and then bashrc. Source, or the dot can be used to tell Linux to switch to a different profile. And bash_logout controls shell behavior at exit.</p>
<h3 id="Understanding-Linux-shell-variables"><a href="#Understanding-Linux-shell-variables" class="headerlink" title="Understanding Linux shell variables"></a>Understanding Linux shell variables</h3><p>The truth is that we’ve already touched on shell variables in this series once in the introduction, and again in the Working on the Command Line video from the fourth course. So we’ll rather briefly cover this topic, focusing mainly on a couple of details we haven’t yet seen.</p>
<p>You can create a variable that’s unique to your current shell using something like user&#x3D;myname. Echo $user will print the value of user to the screen. Export will pass a variable to all child shells within the current one. Set without any command line arguments will print all current shell variables and functions. Env, again without arguments, will list only variables. Unset will remove a particular value.</p>
<p>Now we’ll move on to material that we haven’t yet covered. Alias can create a local customized command, complete with it’s own arguments. Alias on it’s own will list all current aliases. Note the alias, alert. It’s a little script that will send an alert notification to your desktop upon completion of a task. All you need to do to run alert, is to add it to the end of your command after a semicolon, and it will execute when the command itself returns a success code.</p>
<p>Let’s try creating our own alias. Suppose, from time to time, I need to remind myself of which time zone my computer is set to. I’ll run alias followed by the name of the alias I’d like to create, print tz in my case. Then within quotation marks, I’ll simply type the commands I’d like to run, separated by semicolons. Once that’s done, I can type my alias name and the operation will execute. Removing an alias works using unalias.</p>
<p>You can also create mini programs that you can run from the command line using function. Here’s an example that will accept user input through the $1 input variable, and then, using ls -l, list the files and directories that match the input. You can remove a function using unset -f and the function name.</p>
<p>To briefly review, you can create a system variable using name&#x3D;value. Export variable name will make the variable globally accessible. Unset variable name will delete the variable. You can create an alias that will run a series of commands, and unalias will remove the alias, as will exiting the shell. You can create a mini command line launched <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/linux-scripts-basics-1/">script</a> using function, and unset-f will remove the function.</p>
<h3 id="Scripting-contingent-commands-using-lists"><a href="#Scripting-contingent-commands-using-lists" class="headerlink" title="Scripting contingent commands using lists"></a>Scripting contingent commands using lists</h3><p>You can execute more than one command in a sequence using what are called lists. Now there’s no command called list so don’t look for details using man list. If you do want to read up on lists, enter the bash man file, and search using the forward slash for lists. In any case, using lists allows you to link the executions together. So for instance, you can make a command contingent on the successful completion of a previous command.<br>Let’s use ls to list all the files in this directory, but grep for only those files that contain stuff in their file names. There’s obviously only one of those. Then by adding two ampersand symbols, we’ll cat the contents of the file only if the ls operation was successful. However, suppose I change the grep string to stug. Since there is no stug file, the ls operation will fail, and the shell won’t get to the cat command.</p>
<p>If, on the other hand, I would use a semicolon rather than two ampersands, I’ll be telling the shell to execute the second command regardless of whether the first command was successful or not. Thus, the file stuff is printed to the screen. That’s because semicolon just acts to tell the shell that these are two separate commands. You can get a similar effect, but for a different reason, using two pipe symbols. This one works because two pipes has the effect of or. Using or, however, will mean that only one of the two commands can execute. Therefore, if we change the commands so that the ls operation does execute, then the file will be listed through ls, but not displayed through cat.</p>
<p>Let’s review what we’ve learned about lists. Two ampersands will execute the command to the right only if the first command succeeds. Two pipes will execute the command to the right only if the first command fails. And a semicolon will execute both commands in all cases.</p>
<h1 id="Linux-scripts-the-basics"><a href="#Linux-scripts-the-basics" class="headerlink" title="Linux scripts: the basics"></a>Linux scripts: the basics</h1><p>In this video and the next, we’re going to learn about Linux scripting. Scripting lies somewhere in between system administration and programming on the one hand, like programming, Bash scripts allow for loops, control structures and dynamic data handling. But on the other hand the syntax and execution is very closely integrated with the command line. In fact, any command that’s possible to run directly on the command line can be used as part of a script. Because scripts share the qualities of both of these worlds, they’re perfectly suited for their core task of automating administration tasks.</p>
<h3 id="Introduction-to-Linux-scripting"><a href="#Introduction-to-Linux-scripting" class="headerlink" title="Introduction to Linux scripting"></a>Introduction to Linux scripting</h3><p>Let’s begin at the beginning. A script is a plain text file that will often use a .sh extension. The first line of the script, known for some reason as the shebang line, identifies the interpreter you plan to use. In our case, we’ll type #!&#x2F;bin&#x2F;bash which tells Linux that we want this script to use the rules of the Bash shell. Besides that first line, any line that begins with a hash is ignored by Bash and serves only as a comment. It is a very, very good practice to clearly comment every step of your scripts so that anyone reading it later will have some clue of what it’s supposed to do. This will not only help the poor fellow who comes after you but could serve you extremely well if you’re troubleshooting your own script months after you wrote it trying to figure out what stopped working and why. So I’ll take my own advice and add a comment to this first script.</p>
<p>For this simple example, we’ll echo a message exactly the way we’ve seen it done on the command line. On the next line of our script, I’ll type date which is a Linux command that prints the system date to the screen. “Exit 0” will end the script execution. Now we’ll save the script file and exit nano so we can actually run the script.</p>
<p>To run a script you type a dot and a forward slash and the name of the file. Whoops something went wrong. I can’t tell you many times I made this mistake and just sat staring stupidly at the screen and wondering what I could possibly have done to Linux to make it hate me so much. But here’s the problem. Remember back in the previous video when we discussed file permissions? We learned that there are three categories of permission: read, write and execute. Well it seems that you can’t execute a script unless the script file is executable. This one obviously isn’t. Let’s change that now using “chmod +x”and the name of the file.” Now let’s try that one again.</p>
<p>Let’s review. You begin each script with the shebang line. All other lines that start with a hash are ignored and used for comments. You need to make a file executable before it can be run as a script and running a script requires that the file name is prefaced by a dot and slash. \</p>
<h3 id="Using-user-inputs-and-arithmetic-in-Linux-scripts"><a href="#Using-user-inputs-and-arithmetic-in-Linux-scripts" class="headerlink" title="Using user inputs and arithmetic in Linux scripts"></a>Using user inputs and arithmetic in Linux scripts</h3><p>We are now going to explore quite a few examples designed to illustrate various key scripting syntax and command tools. I would advise you to pause the video now and then so you can think through each example for yourself. Make sure you understand what each line does and indeed what each character does. Ideally, you’ll open up your own terminal and play around with each of these tools for yourself. I would also advise you to type in the text of the scripts yourself so you get used to the syntax and get a feel for the kinds of mistakes that are commonly made.</p>
<p>Okay we’re ready to try something a bit more interactive. This script will ask the user to input some text then using the “read” command it will assign whatever the user types to a variable called answer. The script will print the answer and the words, “Okay it’s,” and then print the system date. Let’s exit and run it. Let’s see if we can work with numbers. If you have experience programming, you’ll probably have noted that we didn’t declare our variable in the last example. However, that’s only for simple text strings. If you want Bash to understand your input as an integer, you will have to declare it.</p>
<p>This example first declares three variables as integers. It will then ask the user for two numbers and use read to assign them to the number one and number two variables. Using simple arithmetic symbols like the equal sign or a plus sign you can create an equation that will give the sum of our two input variables to the variable total and print it out. Let’s run it.</p>
<p>It’s review time again. The read command will accept user input and assign it to a variable. You need to declare variables as integers if you want your script to work with them as numbers. And you can use the normal arithmetic symbols to perform math functions in a very simple way. We’ll continue with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/linux-scripts-control-structures-1/">scripts</a> in the next video.</p>
<h1 id="Linux-scripts-control-structures"><a href="#Linux-scripts-control-structures" class="headerlink" title="Linux scripts: control structures"></a>Linux scripts: control structures</h1><p>In this video, we’ll continue our study of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/linux-scripts-basics-1/">scripts</a>. Let’s learn how to test for an environment condition and only then execute a command. This script will ask the user for a directory you’d like to add to the system path, read the value into NewPath, then use -e, which I believe stands for “exists,” to test for the existence of the directory the user has entered. If the directory does exist, it will use PATH&#x3D;$PATH:$NewPath to add NewPath to the PATH, then export it. And finally, echo the new complete path variable. If, on the other hand, the directory does not exist, then this script will output a suitable message. Note, by the way, how calling an existing variable requires that it be prefaced by a dollar sign for Linux to understand that it’s a variable name we’re referring to, and not a text string or command. Let’s run the program and enter a valid directory. Now we’ll run it, but this time we’ll enter a non-valid directory.</p>
<p>Let’s try out an if-else statement. This script will use test to test for the existence of a file called stuff. If it does exist, in other words, if the test result is positive, then the “stuff exists” message will be printed. In every other case, meaning if else is satisfied, “stuff does not exist” will be printed. Note how you need to close the if-else filter with the letters fi, which is if backwards. We’ll run the script.</p>
<h3 id="Testing-for-environment-conditions-using-“test”-in-Linux-scripts"><a href="#Testing-for-environment-conditions-using-“test”-in-Linux-scripts" class="headerlink" title="Testing for environment conditions using “test” in Linux scripts"></a>Testing for environment conditions using “test” in Linux scripts</h3><p>We should look more deeply into test. As you saw, we added the -f argument, which tells test to confirm that the file not only exists, but that it’s a regular file. -d will make test confirm that the target is a directory, and -r will confirm that it’s a file, and that it’s readable. Test can also be used to compare two values. Here we’ve given the value of hello to the text1 variable and goodbye to the text2 variable. I would imagine that both of us could tell pretty quickly whether the two are equal. Hint: they’re not. Let’s see if Linux can figure it out. Using an if-else statement, we’ll ask test to confirm whether text1 and text2 are not identical. The exclamation mark and equals sign mean not equal. We’ll run the script to show that indeed, just as we suspected, hello and goodbye are not identical. We could also use an equal sign on its own to confirm that two values are identical.</p>
<p>Tests run with the -eq argument will confirm whether two integers are equal to each other. -lt tells us that one integer is less than the other, and -gt tells us that one integer is greater than the other.</p>
<p>Let’s review. -e, which is an alternative to test, tests if a resource exists. If-else statements provide simple yes&#x2F;no deicsion-making. Test -f will test for the existence of a file, -d for a directory, and -r for readable. Test with the equal sign between two strings will test to see if they are the same. Exclamation mark and equal sign will test if they are not the same. You can compare integers with a test -eq, confirming that they’re equal, -lt, that the first value is less than the second value, and -gt, that the first value is greater than the second value.</p>
<h3 id="Using-conditional-statements-if-else-in-Linux-scripts"><a href="#Using-conditional-statements-if-else-in-Linux-scripts" class="headerlink" title="Using conditional statements (if-else) in Linux scripts"></a>Using conditional statements (if-else) in Linux scripts</h3><p>If-else statements work well when there are only two possible options: either something is true or it’s not. If your choices are more complicated and you need to allow for multiple possible answers, you’re better off choosing case. Here’s how it could work. This script will take a user input and assign it to the variable color. The words case $COLOR in could be understood as “in the case where color equals one of these possibilities.” Meaning either red or blue, the script will echo, “Nice! I love that color.” As you can see, a user input of orange or brown will generate a “Not bad . . . “ response, and black or purple will get the “Yuk!” message. All other input, covered by the asterisk, will produce the “Sorry, I’m not familiar” error message. Note the rather odd syntax needed to make a case structure work, like the double colons and closing parentheses, or the pipe symbol in place of or. And note also how we close a case structure with the word esac, which is case backwards.</p>
<p>Let’s try running the script and inputting blue, then brown and finally green, which wasn’t one of our listed examples.</p>
<h3 id="Working-with-loops-in-Linux-scripts"><a href="#Working-with-loops-in-Linux-scripts" class="headerlink" title="Working with loops in Linux scripts"></a>Working with loops in Linux scripts</h3><p>A while loop will continue executing a command as long as, or in other words, while a specified condition is met. In this case, we’ve given the variable COUNTER the numeric value of 10. And using COUNTER&#x3D;COUNTER-1, we’re going to reduce that value by one each time through the loop. As long as the value of COUNTER is still greater than two, we’ll print the value at the end of the “The counter is” sentence. Once the value of COUNTER drops below three, the done line tells the script to stop. Let’s try it.</p>
<p>You can also execute commands through each cycle of a loop using until rather than while. Until will cause the commands to repeat until a certain condition is met, rather than while a condition is still true. In this case, we’ve set counter to 20 to start off. And again, we’ll reduce that by one each cycle through the loop. Now, however, we’ll continue the loop only until COUNTER is less than 10. Let’s see how that works.</p>
<p>For can execute a command against each item in a list. In this example, we’ll use the list of files and directories generated by ls as input, assign it to the variable i, replacing any previous value it might have had, and echo the value to the screen.</p>
<p>Finally, you can, for example, feed the for loop with the output of seq. Let’s just run seq directly from the command line to get a feel for how it works. Seq 15 will count up one number at a time, in sequence, hence the name seq, until it gets to 15. Seq 5 15 will count up between five and 15. Seq 3 6 21 will count between three and 21, but by increments of six. Now let’s see how that might work within a script. Here we’ll feed our variable i with the simple seq 15 command, which will count up by ones between one and 15. For each loop cycle, the script will output the current value of i and the message. Let’s run it.</p>
<p>These last few examples may have seemed rather simple, and in fact a bit redundant. However, each of them was included to illustrate a distinct and unique tool which can be used to produce very different results.</p>
<p>Now let’s review this material. Case can handle multiple possible conditions, while loops continue executing a command as long as a set condition is still true. Until loops will execute only until a condition is met. For loops will execute a command against items on a list. And you can seed for loops with the output of the seq command, which counts in sequence.</p>
<h1 id="Databases-MySQL"><a href="#Databases-MySQL" class="headerlink" title="Databases: MySQL"></a>Databases: MySQL</h1><p>In truth managing databases is a skill all its own that isn’t directly connected to operating system administration. But because so many of the deployments you’ll administrate involve or even depend on databases you really have no choice but to pick up at least a simple understanding of how they work.</p>
<p>Broadly speaking there are two kinds of databases. Heavily structured relational databases, which can be managed by structured query language, SQL environments, and nonrelational, noSQL databases whose popularity’s been growing due to a simpler design that emphasizes speed over consistency. While both database types are important, the LPIC exam expects you to focus on relational databases and specifically the MySQL relational database management system.</p>
<p>It doesn’t hurt to be aware that the original developers of MySQL actually left the project a few years ago to create MariaDB. At this point, MariaDB is completely compatible with MySQL and can be used and accessed in exactly the same ways. It has even begun to replace MySQL in some major <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/course-introduction-25/">Linux</a> software stacks. So while my references will be to MySQL they’re all equally relevant to MariaDB.</p>
<p>A relational database is made up of tables, records and fields. Let’s imagine a database with two tables used by a company to track incoming communications. The first table contains information about customer complaints and the second customer compliments. Since the compliments table is likely to be mostly empty, we’ll focus on the complaints. Each record represents a single email complaint. The record contains fields for the date and timestamp, the senders email address, the message itself and any response sent by the company.</p>
<h3 id="Working-with-MySQL-relational-databases-in-Linux"><a href="#Working-with-MySQL-relational-databases-in-Linux" class="headerlink" title="Working with MySQL relational databases in Linux"></a>Working with MySQL relational databases in Linux</h3><p>Let’s spend some time working on an actual MySQL database. First, we’ll install MySQL on the Ubuntu virtual machine. When prompted during the setup process, I’ll create a password for the root account. Once everything’s in place, you can enter the MySQL environment using “mysql -u username -p.” I’ll use root for username. “-p” will prompt you for the root password.</p>
<p>We’re now in MySQL land where every command has to end with a semicolon. If you forget to add a semicolon - and you definitely will at some point - the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/working-with-linux-shells-1/">shell</a> will simply dump you onto a new line and patiently wait for the rest of the command until you wake up and remember what it’s expecting. “Show databases” by the way will display all the databases currently part of this installation.</p>
<p>Let’s create a new database called “Communications” and run “show databases” once again. Our new communications database is there. Since we’ll want to work with communications, we’ll have to let MySQL in on our plans. Otherwise, it won’t know that it’s supposed to apply the coming commands to this particular database. We’ll do that with use. We’ll run show tables to confirm that communications doesn’t yet have any tables. So we’ll create a new table. We’ll use create table, give it the name complaints and map out the fields each record will have. The date field will use the date format. The email field, will use the varchar format and allow as many as 20 characters. The message and response fields will allow up to 250 characters.</p>
<p>When we hit enter, MySQL will parse the command to make sure the syntax is correct. If there’s anything wrong, you’ll get an error message. Now let’s enter some data using insert into the complaints table by first defining the specific values we’d like to populate. You don’t need to add data for every column but you do need to tell MySQL which columns you’re adding. We’ll then add these values. As I enter the date, email, message and response values note how fields are created by commas but no spaces. Users won’t usually enter data this way. It’s far more common for database managers or developers to create a front-end application that makes data entry intuitive but under the bright and shiny surface of that front end this is exactly the structure that will be used. We’ll have to make sure that our insert operation worked so we’ll run select everything from complaints where the asterisks represents everything.</p>
<p>Let’s enter one more record but this time we’ll insert data into only three fields, rather than all four. We’ll run select again to make sure this one worked. Although if there was anything wrong, the MySQL command line would almost certainly have complained loudly about it already. I’ve added some more records to our table to make it easier to illustrate the next operations. Let’s take another look at the table to see how it looks now.</p>
<p>Let’s review. You list the databases in your MySQL environment using show databases. Create databases. Database name will create a new, empty database. You tell MySQL that you want to work with a specific database using “use” database name. You create a new table within a database with the create table command and by specifying the columns, their character types and maximum length. You can add data to a table using “insert into” specifying to which columns you’re going to add data. And then the data for each field enclosed by apostrophes. You can display data from a table using “select.”</p>
<h3 id="Accessing-and-manipulating-MySQL-data-using-select"><a href="#Accessing-and-manipulating-MySQL-data-using-select" class="headerlink" title="Accessing and manipulating MySQL data using select"></a>Accessing and manipulating MySQL data using select</h3><p>You can also select records based on precise filters. So for instance, you could specify all records associated with a particular customer by selecting all records whose email field equals that customer’s email address. If you need to delete records, suppose Joe at cranky.com finally realized that he wasn’t getting anywhere with his constant complaints and decided to take his business elsewhere, you could run the select operation we just did to confirm it’s picking up exactly the records we need and then run it again using delete instead of select to remove all records whose email field is <a href="mailto:&#74;&#111;&#101;&#64;&#x63;&#114;&#x61;&#110;&#x6b;&#121;&#46;&#99;&#x6f;&#x6d;">&#74;&#111;&#101;&#64;&#x63;&#114;&#x61;&#110;&#x6b;&#121;&#46;&#99;&#x6f;&#x6d;</a>.</p>
<p>We can use update and set to edit the contents of individual or groups of fields. Here we’ll change Joe’s email and message fields to update them to new values. By the way, if you don’t specify where these values should change, MySQL will assume you want to give these new values to every email and message field in the whole table. You can also choose a column to display our records according to a specified order using either “order by” or “group by.” This will arrange all of our records in ascending order by date. You could now, if necessary, submit just these records to some operation. Adding desc, on the other hand will arrange our records by date in descending order.</p>
<p>To illustrate group by I’ll create a new table that contains customer orders including the item purchased and the price paid. Note how the customer and price columns use integer as their format type so we can use their contents as numeric values. I’ll insert some data so we can work with it. Let’s take a look at our table. We can see that books seems to be our most popular item. We’ve already sold three of them. If you wanted MySQL to report the total revenue per item type, you could select all the items and using the contents of the price and item columns calculate the sum of the prices. The three books we sold generated a total of $30 in revenue.</p>
<p>Finally, we should at least briefly describe how to use data from two separate tables. If we look once again at our purchases table, we see that every customer has an ID number. I’ll create another table called customers in which I’ll place some customer information. For simplicity sake, I’ll use only their first names. Again, I’ll create some data. Let’s take another look at both our purchases and customers tables. Now suppose you wanted to use our data from purchases but incorporate customer names into the output. We’ll use a simple join operation that looks like this. We’ll select from purchases and from customers separated by a comma all records where the customer field from the purchases table equals the ID field from the customers table. This will display each purchase with the correct customer ID data appended to the right.</p>
<p>It’s review time. You can select only records corresponding to specified filters using select with “where.” “Delete” follows the same syntax as select but will actually delete the return data. You can edit a record using “update set” and “where.” You can control the order data is displayed and used with “order by” and tally totals with “select item” and “group by.” We can intelligently join data from two tables by using select and where to reference the specific tables and columns you need to use.</p>
<h1 id="Databases-the-AWS-way"><a href="#Databases-the-AWS-way" class="headerlink" title="Databases: the AWS way"></a>Databases: the AWS way</h1><h3 id="Migrating-a-MySQL-database-to-the-AWS-Relational-Database-Service-RDS"><a href="#Migrating-a-MySQL-database-to-the-AWS-Relational-Database-Service-RDS" class="headerlink" title="Migrating a MySQL database to the AWS Relational Database Service (RDS)"></a>Migrating a MySQL database to the AWS Relational Database Service (RDS)</h3><p>In this video, we’re going to spend some time exploring <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/databases-mysql-1/">MySQL</a> the AWS way. We’re going to learn how to launch and then login to an Amazon RDS hosted MySQL instance and then lift and drop a locally created database into it.</p>
<p>Hosting your database operations on AWS’s relational database service has some enormous advantages. The hardware is fully managed by AWS so you don’t have to worry about server failure, software updates, data replication, and backup and to a large degree even security. Once you’ve set things up, there’s really nothing for you to do besides focus on your data.</p>
<p>Part of the stability of AWS databases is the way they structure their hardware infrastructure for you. If you choose to create a multi-AZ deployment, your data is spread across more than one geographic availability zone to ensure that even if something should cause one to crash the others will survive and continue to provide full service.</p>
<p>To make this work we’ll make use of a DB subnet group that we’ll specify during configuration. Let’s create our DB instance. From the RDS menu, check to make sure we’re in the right AWS region then click on “Launch DB Instance.” We’ll select “MySQL.” We’ll say, “Yes,” to be given the option of multi-AZ deployment and provisioned IOP storage and click “Next Step.” We’ll choose the most recent MySQL release version, select a db.t2.micro as our instance, say “Yes” to multi-AZ and leave the storage options as default. We can change any of these hardware configurations later - something that would be unspeakably complicated in a local data center. We’ll use “New DB” as DB identifier, “Admin” as a master username, and a password that’s at least eight characters long.</p>
<p>Now we’ll place our new DB inside our default VPC, select the “MySQL group,” subnet group that we have available in our particular VPC and leave the instance privacy setting as not publicly available. For our security group, which will control all traffic into and out of our RDS instance, we’ll select an existing group and come back to it a bit later to make sure that it’s set up the way we want. We won’t create a database right now and we’ll leave the rest of the settings as they are. Note that MySQL will use port 3306 for communications.</p>
<p>Now we’ll click on “Launch DB Instance” and head back to the RDS dashboard to see what’s happening. Since AWS is still creating our instance, it doesn’t yet display an endpoint. While we’re waiting, we’ll click on the “Configuration Details” tab and click on “Our Security Group.” We’ll click on the “Inbound” tab and then on the “Edit” button. Here we’ll add a new rule that will allow MySQL traffic into the instance from my IP address. Note how AWS automatically filled in the 3306 port. You should remove all other rules, because in our case at least, there’s no other traffic that should be allowed in. We’ll save the rule.</p>
<p>Once our DB instance is up and running, we’re shown the endpoint. It’s important to ignore the :3306 at the end. As for what we’re doing, it’ll just get in the way. Now let’s start a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/lpic1-linux-shells-scripting-and-databases/working-with-linux-shells-1/">shell</a> in our local machine and connect directly to our RDS instance. Let’s create a new database. That’s enough for now. We’ll exit the session.</p>
<p>Now back on our local machine we’ll create a full backup of the communications database we were working on for the last video using the mysqldump program and pipe it to the file communications.sql. It really amazes me just how simple this next step is. We’re going to upload the dump of our local database into the communications database on RDS using one simple command “-h” indicates that the next value is the endpoint address of the database host, “-u” is followed by the master username we chose, “-p” will prompt for a password. Communications specifies the remote database we want to work with and the arrow character will apply the .sql file to the working database. That’s it.</p>
<p>Now to make sure that everything actually worked we’ll log in to our RDS instance again just as we did the first time. We’ll load the communications database that we created before. And we’ll use “show” to display the database tables which are those we’ve created for our local deployment.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-101-First-Steps-Into-the-Linux-Console-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-101-First-Steps-Into-the-Linux-Console-8/" class="post-title-link" itemprop="url">Linux-LPIC-101-First-Steps-Into-the-Linux-Console-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:08:53" itemprop="dateCreated datePublished" datetime="2022-11-19T01:08:53-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:09:40" itemprop="dateModified" datetime="2022-11-20T20:09:40-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-101/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-101</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-101-First-Steps-Into-the-Linux-Console-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-101-First-Steps-Into-the-Linux-Console-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>·

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Linux-Partitions-and-Filesystems-5-of-5-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Linux-Partitions-and-Filesystems-5-of-5-7/" class="post-title-link" itemprop="url">Linux-LPIC-101-LPIC-1-101-Linux-certification---Linux-Partitions-and-Filesystems-5-of-5-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:08:52" itemprop="dateCreated datePublished" datetime="2022-11-19T01:08:52-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:04:18" itemprop="dateModified" datetime="2022-11-20T20:04:18-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-101/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-101</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Linux-Partitions-and-Filesystems-5-of-5-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Linux-Partitions-and-Filesystems-5-of-5-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-101-Linux-Command-Line-Fundamentals-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-101-Linux-Command-Line-Fundamentals-6/" class="post-title-link" itemprop="url">Linux-LPIC-101-Linux-Command-Line-Fundamentals-6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:08:50" itemprop="dateCreated datePublished" datetime="2022-11-19T01:08:50-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:10:18" itemprop="dateModified" datetime="2022-11-20T20:10:18-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-101/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-101</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-101-Linux-Command-Line-Fundamentals-6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-101-Linux-Command-Line-Fundamentals-6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Command-Line-Basics-4-of-5-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Command-Line-Basics-4-of-5-5/" class="post-title-link" itemprop="url">Linux-LPIC-101-LPIC-1-101-Linux-certification---Command-Line-Basics-4-of-5-5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:08:49" itemprop="dateCreated datePublished" datetime="2022-11-19T01:08:49-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:24:28" itemprop="dateModified" datetime="2022-11-20T20:24:28-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-101/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-101</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Command-Line-Basics-4-of-5-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Command-Line-Basics-4-of-5-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-introduction"><a href="#Course-introduction" class="headerlink" title="Course introduction"></a>Course introduction</h1><p>Welcome to the fourth course in our series preparing you for the LPIC1, Linux Server Professional certification exam. In this course, we’ll learn about the Linux command line. And especially how, by using text based tools you can closely and efficiently control just about everything that’s happening on your system. We’ll explore terminal shells and environment variables.</p>
<p>Manipulating and redirecting text streams. File management, and archive backup tools. How to control systems and processes.</p>
<p>How to launch sophisticated conditional tech searches in the world of the terminal text editor, in particular VI. We’ll also learn how to use Linux’s “man” system of in line help documentation.</p>
<p>I must warn you, that some of the videos in this course are going to be packed end to end with commands and more commands.</p>
<p>Unfortunately, there really is no way around it. The power of the Linux command line lies partly in having access to the full tool kit. And there are a lot of tools in this kit. But successfully absorbing and mastering the material, will simply be impossible without trying everything out yourself. Even better, you should try to work on actual projects using these tools.</p>
<p>Finally, while we will normally connect Linux topics to their cloud computing counterparts and show how system administration is done on AWS, since the material on this course lies so close to the very core of systems administration, there really is nothing here that would be done any differently in the cloud. But I have little doubt that over the course of a career administrating AWS infrastructure, that you will happily make good use of many of these tools.</p>
<h1 id="Working-on-the-command-line"><a href="#Working-on-the-command-line" class="headerlink" title="Working on the command line"></a>Working on the command line</h1><h3 id="Understanding-Linux-shells"><a href="#Understanding-Linux-shells" class="headerlink" title="Understanding Linux shells"></a>Understanding Linux shells</h3><p>A Linux administrator’s primary working environment is the command line which is, to be more precise, a command language interpreter. That is to say in much the same way as a computer language like C or Java, the interpreter will translate your commands into terms your operating system can understand, allowing them to be executed. Technically speaking every command line is a shell. A relatively independent compute environment bound by it’s own set of rules and restrictions. Of course, a shell exists within the computer and network that spawned it, but can also be given unique characteristics. Bash which stands for Bourne Shell, perhaps the best known Linux command language interpreter, is the one we’ll use for this course. Of course, there’s nothing stopping you from switching between different interpreters. Typing “sh” by the way will open a new shell using the sh environment. As you can see, the sh shell isn’t currently set to provide as descriptive a command prompt as bash.</p>
<p>Typing “Exit” will drop us back into our original bash shell. Typing “Exit” once again would close the window altogether.</p>
<p>While the terminal command line is the most visible place we use these tools, and the place where we’ll do most of our learning in these courses. In administration terms, they’re usually most productively utilized as parts of scripts.</p>
<p>Bash scripts are effectively executable text files containing commands that automate processes harnessing some extremely sophisticated resource manipulation tools. We’ll talk more about scripting later in this series. For now, it’s enough to know that everything we’ll discuss here, as useful as it is, will prove even more useful later.</p>
<p>Let’s talk about some of the more basic command line tools. Typing “Echo,” followed by some text will cause that text to be output on the next line. Nothing too exciting.</p>
<p>Let’s try “echo $USER,” and see what’s printed to the screen. So user it would seem is a pre-existing system variable that contains the name of the current user. And the dollar sign tells bash that the following text string is a name of a variable, rather than just text to be printed.</p>
<p>Typing “echo $user” in lower case by the way won’t work. Bash is generally very case sensitive. We could however add a new variable using “user&#x3D;myname” and then run echo again, “echo $USER.” This time the new value of user is printed. Where do these system variables come from? The “set” command when run without argument will output a list of all current shell variables and functions. Note how user is the last, meaning most recent, value in the list. Interestingly some of these variables will actually change dynamically. So for instance, the pwd, which stands for present work directory, right now is &#x2F;home&#x2F;ubuntu. But if I change directories to say, &#x2F;etc and run set again, you’ll see the value of pwd has followed us. Typing pwd in lowercase in the command line will by the way display our current work directory. Another system variable that’s often really useful is uname. Although you could probably have figured out that we’re using Linux on your own. Adding a “-a” to that however will output the Linux kernel version our system architecture and the current date and time.</p>
<p>Let’s review. Your command line exists within a shell of which bash and sh are two examples. Commands can be executed from the command line or through scripts. Echo will print a string or the value of a variable to the screen. You can create new variables using name&#x3D;value.</p>
<h3 id="Working-with-Linux-environment-variables"><a href="#Working-with-Linux-environment-variables" class="headerlink" title="Working with Linux environment variables"></a>Working with Linux environment variables</h3><p>Set has many command line arguments but I believe that simply being aware of its role as a reporter of shell variables is enough for the LPI exam. Unset will, when followed by the name of a variable, remove that value. Let’s run “unset user” using lowercase. And then run set again. Our original value of user has been removed, although we can still see a trace of it’s existence in this _&#x3D;user. If we were to exit this shell and start a new one even that trace would no longer be there.</p>
<p>Like set, running env without arguments will display environment variables. However, unlike set env will not display functions. Also unlike set env will not include a variable until it’s been exported to the environment. From that point however the variable would be available to child processes. Let’s illustrate that. We’ll create a variable called “stuff” and give it the value of hello. Running set will display our new value. Running env on the other hand comes up with nothing. Even when we grep for stuff specifically. Now if we type bash we’ll find ourselves in a new shell session. One level below our original shell. And at this level our stuff variable fails to show up even for set.</p>
<p>Let’s exit this shell and now back in our home shell we’ll run set again. Stuff is right there where it’s always been. Now let’s export the stuff variable using export followed by the variable name.</p>
<p>Now running env again and grepping for stuff shows us that stuff is part of the environment. We’ll open a new lower level bash shell and run “echo $stuff.” And what do you know, the value of stuff has followed us down to this new child shell.</p>
<p>To review, “set” contains functions and variables even before they’ve been exported. Running env will only display exported variables. Exporting a value with the export command will make the variable available to child shell sessions.</p>
<h3 id="Using-man-the-Linux-help-documentation-resource"><a href="#Using-man-the-Linux-help-documentation-resource" class="headerlink" title="Using man, the Linux help documentation resource"></a>Using man, the Linux help documentation resource</h3><p>One of the most basic needs you’ll probably encounter in your shell work is access to useful and contextual help. Did that command require a -a or –append? Was the a supposed to be upper or lower case? How is that command actually spelled? Linux comes with a very complete help system called man. Typing man, followed by the name of the command you’re working with will usually return a manual page describing the use and function of the particular command. This simple man page nicely shows us how man pages break down including sections for name, synopsis, description, author, bug information, copyright and related sources.</p>
<p>Note the number one at the top of the page indicating the page type. There are nine page types in the man system, as illustrated here. The LPI exam will actually expect you to be familiar with these types and their corresponding numbers. Man bash is a huge man file that has a lot to teach you about how shells work. If there doesn’t seem to be a man page for a very basic command like unset you’ll probably find it within man bash, which can be searched by hitting the forward slash and entering a string.</p>
<p>If you can’t remember the precise name of a command, which can make finding the right man page a bit of a challenge you can use apropos.</p>
<p>Finally, past commands that you’ve executed remain accessible to you through history. Typing history alone will display previous commands in reversed order. You can search the record by running history with grep. Or read the hidden .bash_history file itself, which can be edited. I can’t tell you how many times history has made my job and my life so much easier.</p>
<h1 id="Process-text-streams-using-filters"><a href="#Process-text-streams-using-filters" class="headerlink" title="Process text streams using filters"></a>Process text streams using filters</h1><p>When people talk about Linux you’ll often hear the expression nearly everything in Linux is a text file.</p>
<p>Whether it’s the configuration files kept in &#x2F;etc, executable scripts or logs so much of what goes on in Linux is really nothing more than the system reading plain text or lightly formatted files. With all that can be controlled using text, it’s stands to reason that there’s a great deal that can be accomplished by filtering and editing that text. And especially by filtering and editing it on the fly as processes are actually an execution.</p>
<p>We’re going to illustrate the text filtering and manipulation tools any Linux sysadmin will need by performing rather insignificant actions but the skills themselves are directly transferable to the most sophisticated and even elegant tasks. But this is one of those times I referred to earlier when you’re unfortunately going to be deluged with commands and their arguments. Take it slowly don’t be afraid to listen to something over again and don’t forget that I will insert periodic review to help all these digest properly. And most of all, work through all these commands on your own.</p>
<h3 id="Working-with-Linux-text-manipulation-tools"><a href="#Working-with-Linux-text-manipulation-tools" class="headerlink" title="Working with Linux text manipulation tools"></a>Working with Linux text manipulation tools</h3><p>You should note that each of these tools has many possible arguments and use cases that we’re not going to go into right now. You’ll generally need to have a decent overview level knowledge of text processing commands for the LPI exam and won’t be expected to dig deeply into complex options.</p>
<p>As we’ve seen already many times in this series, cat will print a file to the screen. We didn’t yet mention that cat is actually short for “concatenate” which means to join things together. Adding the “-n” argument will print the text with numbered lines. And using uppercase A will print all characters. If you need a more simplified version of that file, we can use cut to strip away everything we don’t need. Here the “-d” followed by the colon sets the field delimiter to colon meaning that every instance of a colon marks the beginning of a new field. “-f1” means that we only want to print the first field.</p>
<p>This doesn’t actually change the contents of the file nor could we in the case of the password file without becoming the administrator but we could easily save our newly edited text by piping the string to a file. Let’s see what it looks like.</p>
<p>“Expand” will convert tabs in a text string to spaces. We’ll work with the file called file that contains two lines one whose words are separated by tabs and the other separated by spaces. Running “expand -t” followed by the number 15 and the name of our file will replace every tab in the file with 15 spaces. Running “unexpand” with the number one and the name of our file will replace every set of one spaces with a tab. Running it again with the value two will replace every set of two spaces with a tab.</p>
<p>“Fmt” formats large bodies of text. “Fmt -w” will force a file to break to a new line every X number of characters. “Fmt -t” will indent all paragraph lines after the first line, “Pr” will also add formatting to the text.</p>
<p>“Pr -d” will print double spaced “-l” will set a limit to the total number of screen lines. Head will print only the indicated number of lines from the top, the head of the file.</p>
<p>“Od” which stands for octal dump will print the characters of a file in different formats. “Od -a” for instance will display our text with “ht” representing each tab and “sp” for each space. “Od -c” will print tabs as “&#x2F;t” and new lines as “&#x2F;m.” And here’s OD’s default output.</p>
<p>Let’s review. Cat prints to the screen, cut will isolate a single column and print only that. Expand converts tabs to spaces while unexpand converts spaces to tabs. “Fmt -w” formats the width of the text that’s displayed to the screen, “pr -d” and “-l” control line spacing and screen length. Head prints only the first defined number of lines of a file and “od” will print files in different formats.</p>
<p>Less is an old friend we’ve used previously to view text. Less will display large bodies of text one screen at a time allowing you to use regular controls like the arrow keys or page up and page down to move through a document.</p>
<p>Join is a tool for merging columns of data for multiple files assuming that they share a common field. I’ve created two very simple files part one and part two containing simple numbered columns. By running join and specifying our two files, the data from both will be usefully displayed. Paste besides printing a single file will also by default print the data from two files side by side, adding “-s” will print the two file sequentially rather than side by side. Although I admit that this particular example doesn’t look that useful.</p>
<p>“Nl” like “cat -n” will print the lines of a file with numbers. Sort will reorder the contents of a file either by number or alphabetic sequence. Running sort with “-n” telling Linux that we want this sorted by number will display the lines in ascending order. “Sort -nr” will do the same but in reverse or descending order. Sort without the “-n” switch will list the files in alphabetic order. This is a version of the file without numbers again adding “-r” will reverse the output.</p>
<p>Split is one of those rare text tools that actually does something permanent by default. Running split will take a file or text stream and create smaller files from it according to your specified size. So to create files each no greater than say two lines long use “split -2” and the file name. In this case, listing the files in this directory reveals six files named xaa to xaf each no longer than two lines. If you don’t specify a file length, split will default to 1,000 lines.</p>
<p>Tail will print only the last lines of a file. The number of lines it does print depends on your specification. Therefore, “tail -n 3” will print the last three lines of the etc&#x2F;group file. When you add the “-f” switch, tail will continue printing any new lines that are subsequently added to the file. This can be really useful when you’re trying to monitor a system event. Running tail against a log file like syslog will allow you to see system events as they happen. “Tr” will translate text from one format to another.</p>
<p>Let’s take off the new split files that we created and read it using cat but rather than display it to the screen as is we’ll pipe it (using the shift and backslash keys) to tr, in this case converting all lowercase characters to uppercase. Here’s a file I created with a couple of such duplicates.</p>
<p>“Uniq -u” will print only unique lines. Quickly printing document’s statistics is the job of “wc.” “Wc” will tell you the total number of lines, words and bytes of a file.</p>
<h3 id="Introduction-to-sed-on-Linux"><a href="#Introduction-to-sed-on-Linux" class="headerlink" title="Introduction to sed on Linux"></a>Introduction to sed on Linux</h3><p>Finally, we come to “sed” which stands for stream editor. In truth, sed could easily fill a course all its own and is treated with some awe by those admins familiar with its magic. It’s almost embarrassing to reduce sed to just a couple of simple examples but this should actually cover you for the LPI exam and I’ll let you get away with it as long as you make a solemn promise to dig deeper on your own later.</p>
<p>Let’s take another look at our part one file and note its contents. Suppose you’ve grown tired of dogs and would prefer a horse. You can cat the file again and pipe it to sed using “s” and forward slash tells sed to substitute horse for dog, which is exactly what it does. Running sed again with a “d” following dog will delete the dog line entirely. My apologies to dog lovers.</p>
<p>Let’s review. Less displays text files one screen at a time. By the way, there’s another similar tool called “More.” Join merges columns from multiple files that share a common field. Paste prints multiple files together. “Nl” prints a file with its line numbers. Sort controls the order by which you can print a files contents. Split creates smaller files out of a single large file. Tail prints or monitors the end of a file. “Tr” translates text from one format to another. Unique will isolate duplicated lines. “Wc” displays documents statistics. And sed does just about everything else.</p>
<h1 id="Perform-basic-file-management"><a href="#Perform-basic-file-management" class="headerlink" title="Perform basic file management"></a>Perform basic file management</h1><p>When most people think about managing files on a computer, it’s a GUI interface like Gnome Nautilus that usually comes to mind first. However, while navigating through your files visually does certainly have some benefits, once you get used to the speed and power of command-line file management you may never want to go back. But in any case you’ll have no choice but to use these skills when you’re logged into a headless server, remote machine, or LXC vm. So consider these commands to fall pretty much in the “basic survival skills” category.</p>
<h3 id="Basic-Linux-file-and-directory-management"><a href="#Basic-Linux-file-and-directory-management" class="headerlink" title="Basic Linux file and directory management"></a>Basic Linux file and directory management</h3><p>Let’s start at the beginning with some commands that we’ve come across already in these courses. We’ll begin by listing the files and directories in our current location using “ls.” You can create a new empty file using touch followed by the file name. If a file with that name already exists, then touch will simply update its last updated information. You can create a new directory using mkdir. If you want to create a directory somewhere on your file system besides the current location, simply include the absolute address in your command, and if you’ll need admin rights, sudo. You’ll sometimes need to create an entire directory tree. This is common when you’re installing a package that expects a data file to exist in a certain location whose parent directories don’t yet exist on your system. This requires just one simple command using mkdir with the “-p” switch. See how long it would take you to do that in the GUI file manager.</p>
<p>Let’s return to our home directory. You can copy files or directory trees between locations using “cp.” “Cp file1 newplace&#x2F;“ will make a copy of file1 in the new place directory. If you want to copy a directory and its file and subdirectories all at once, we would add “-r” to the “cp” command to make the operation recursive. Let’s go take a look to make sure both the directory and the copy of the file called “file1” have been copied faithfully. Remind me to clean up that poor &#x2F;etc&#x2F;perl directory once we’re done with all this.</p>
<p>You can move a file or directory tree from one place to another or just rename them using “mv.” Where “file1” is the original name of the file we’d like to change and “file2” is the new name we’d like it to have. Since we’ve been using “ls” to list the contents of our files a bit, now’s probably a good time to show how using “ls -l” you can list files and directories with longer descriptions. And running “file” against a file directory will display useful file information.</p>
<p>“Rm” will delete or remove a file. “Rm -r” followed by the name of the directory will remove the directory and all the files and directories that it contains. Be aware, be very aware, that “rm” by default will not ask you if you’re sure you want to remove these files and it won’t place the files in a trash can that can be recovered later. Once you run “rm,” your files are for all intents and purposes gone. Having said that if you run “rm” with the “-i” switch, you’ll be asked if you’re sure before anything happens.</p>
<p>One of the things that really makes the command line stand far apart from GUI file managers is globbing. Even if the word itself sounds like it might be referring to some mildly toxic liquid that oozes from between the keys of your keyboard, it’s actually about adding global identifiers to your commands.</p>
<p>To illustrate let’s create a few new files to replace the ones we’ve just removed. Now let’s use “ls” and “grep” to list all the files in this directory whose names begin with the letters “F” and “I.” If we now run “rm file” with a question mark, it will delete file 1 and file 2 but not file 22. Why not? Because the question mark will include exactly one instance of any character. If you would have wanted to remove file22, which has two characters after the word file as well we would have used an asterisk instead.</p>
<p>This kind of pinpoint accuracy in identifying files is obviously going to be useful far beyond simple file deletion but can also play a huge role in sophisticated scripting.</p>
<p>Let’s take a moment and review what we’ve seen. Touch will create or update a new file. Mkdir will create a new directory. “Mkdir -p” will add any parent directories that may be necessary. “Cp” creates copies of a file and “cp -r” copies directories and their contents. “Mv” moves or renames files or directories. And “rm,” “rm -r,” and “rm -dir” remove files and directories. The question mark when used for file globbing will include any single character while an asterisk will include any number of characters.</p>
<h3 id="Archives-and-file-compression-using-tar"><a href="#Archives-and-file-compression-using-tar" class="headerlink" title="Archives and file compression using tar"></a>Archives and file compression using tar</h3><p>Your lives as Linux system administrators will never be complete unless you can create archives and backups of large groups of files and directories. Now that we’re able to use tools like globbing to identify specific subsets of file systems, we should also be able to pipe the lists of files to generate entirely new backup archives. Tar, which stands for Tape Archive, although it obviously works just as well without backup tapes, uses existing archiving and compression software to create, amend and restore file archives. “Tar-cvf” where c stands for create, “V” for verbose and “F” for the name of a file will create a new archive called newarch on the USB device indicated. The asterisk tells tar to create that archive from all the files but not directories in the current directory.</p>
<p>An archive is simply a structured collection of files that are bundled into a single file. Adding the letter z will tell tar to compress the archive as well.</p>
<p>An uppercase A will append rather than create an archive. By the way, tar always expects the name of the new archive file you want to create to immediately follow the “f” switch. And the files or directories to be included to follow after that. This can be a bit confusing but you absolutely must remember this detail for the LPI exam. “-x” used this way will extract the files of an archive into the current directory. As you can see, tar archives take a -tar file extension, and archives that were compressed using gzip will end with tar.gz.</p>
<p>Besides working through tar, you can also directly compress or decompress archives using the gzip, gunzip, bzip2 or xz programs. You’ll probably run into archives in these formats during your career and once again the LPI exam expects you to be familiar with them.</p>
<h3 id="Archives-and-file-compression-using-cpio"><a href="#Archives-and-file-compression-using-cpio" class="headerlink" title="Archives and file compression using cpio"></a>Archives and file compression using cpio</h3><p>The cpio utility does a lot of what tar can do although it usually works with pipe data rather than as a standalone. Let me show you what I mean. Let’s list all the files in this directory. There’s currently only one actual file besides the directories. Now let’s list the files and pipe the list to cpio. “-o” will create the archive called “lsarch.cpio.” Adding gzip to the command will compress the archive. Now let’s move to the root directory and use the find tool to search to a maximum depth of four directories for any file with a file extension of .txt. There are a couple of such files. Now let’s use cpio to create a new archive of just those files. We’ll just jump back into our home directory to make sure the archive has in fact been created.</p>
<p>Finally, we must discuss “dd,” which some believe stands for “disc destroyer,” since it’s so easy to misuse it to wipe out vast volumes of valuable data. Still dd’s great risk is more than balanced by its still-greater value. Dd always takes an “if-of” structure. That is, “if” equals the files I’d like to copy and “of” equals the place to which I’d like them copied. The beauty of dd is that it will copy a file system or an entire partition in its exact original format. It can use this to copy an entire hard drive and move it over to a new computer. And you’ll be able to boot from it as though it was the original. Here’s a simple example where the goal is to copy the sdb1 partition referenced by the link the &#x2F;dev directory to the location of your backup media. This will copy a whole drive sdb to the backup drive you’ve mounted in mnt&#x2F;drive. We’re not going to talk about it in this video but adding tools like SCP and Rsync to the mix will allow you to perform all of these functions not just locally but between remote computers often all using a single command.</p>
<p>So let’s review all of this. Tar with the “-c” switch will create a new archive. Adding a z will compress it too. And uppercase A will append new files to an existing archive. Tar-x will extract the contents of an existing archive. cpio when you pipe data streams to it, will create archives and when gzip is added these archives will be compressed. You can use the powerful find tool to carefully select which files to pipe to cpio or to any one of dozens of other Linux command-line tools for that matter. And dd will copy an exact image of a partition, drive, or selection of files.</p>
<h1 id="Use-streams-pipes-and-redirects"><a href="#Use-streams-pipes-and-redirects" class="headerlink" title="Use streams, pipes and redirects"></a>Use streams, pipes and redirects</h1><p>In the previous video, we discussed file management how you can control and manage your files directly but also indirectly through piping file names and attributes between commands. But if pipes increase the versatility of file manipulation just imagine what it can do to data streams.</p>
<p>Now don’t get all worried. Data streams don’t have to be all that complicated. It can be something as simple as using “cat” to display the contents of a file to your screen. That streamed file one to the screen as we’ve seen before. They can also use cat to redirect the contents of the file to another file like this. As we’ve seen many times already, you can also pipe data from one program to another where the pipe symbol is produced by hitting the shift and back slash keys together. Let’s use the cat program to stream the contents of the syslog file to the grep program to print to the screen only those lines containing the words eth0, which is of course, the name of my first network interface.</p>
<h3 id="Working-with-Linux-data-streaming"><a href="#Working-with-Linux-data-streaming" class="headerlink" title="Working with Linux data streaming"></a>Working with Linux data streaming</h3><p>Linux categorizes all data flows as one of three file descriptors stdin, standard input, which is designated by the number zero, even if isn’t technically a number. Stdout, standard output identified by one and stderr, standard error, identified as two.</p>
<p>Standard input obviously is the data that is input into a program while standard output is the data that’s output from a program. Let’s illustrate this.</p>
<p>Let’s run the program “tail” which if you’ll remember outputs a file’s last lines against syslog and redirects the contents using one and forward arrow to a new file called say, “log data.” The number one tells Linux to redirect the standard output meaning whatever lines of the syslog file tail delivers to become the contents of the file log data. You don’t actually need to include the one in this case, since one happens to be the default via the redirect sign anyway. If you wanted to simply append the contents of syslog to the end of the current contents of the file without overriding whatever’s there already you would use two forward arrows like this. That’s how standard output works.</p>
<p>Let’s see what standard error will produce for us. We’ll cat our file one file redirecting the standard error output to a file called errors. Viewing errors shows us that the file is still empty. That makes sense because if the previous cat command worked it wouldn’t have produced an error. Now let’s try to cat a file that doesn’t actually exist and view the errors file once again. This time there’s something there.</p>
<p>Linux doesn’t restrict a stream to a single output target: Tee allows you to send data off in two separate directions at the same time. This command will list using the long form the contents of the current directory on the screen but also add them to the file list.text.</p>
<p>Finally, you can redirect data streams as input to the xargs program. xargs will execute commands like rm in cases where you need more complicated filters than the command itself would normally allow. From our root directory and using sudo let’s use find to search for all files with a .c extension but pipe that list to grep that will filter only those files that contain a text string, “stdlib.h.”</p>
<p>Let’s review. You can redirect a stream to overwrite the contents of a file using the forward arrow. Two forward arrows will append the new text to the file’s existing contents. The pipe symbol will stream data to the program to the right of the pipe. There are three file descriptors: standard input, standard output, and standard error. One and forward arrow will direct standard output, while two and forward arrow will direct standard error. Tee directs data to multiple streams and XRX controls command execution on pipe data.</p>
<h1 id="Create-monitor-and-kill-processes"><a href="#Create-monitor-and-kill-processes" class="headerlink" title="Create, monitor and kill processes"></a>Create, monitor and kill processes</h1><p>While working with multiple processes from a command line interface might not be quite as intuitive as it is with graphic desktops which give you all kinds of visual indicators telling you what’s happening and where moving between programs is as simple as clicking on a window. Linux provides server admins with a very rich set of tools nonetheless. In this video, we’re going to explore how you can manage and keep track of the processes running on your machine.</p>
<h3 id="Killing-off-misbehaving-Linux-system-processes"><a href="#Killing-off-misbehaving-Linux-system-processes" class="headerlink" title="Killing off misbehaving Linux system processes"></a>Killing off misbehaving Linux system processes</h3><p>First of all you will sometimes need to know exactly what is currently active. The quickest way to do that is by using ps. Ps displays useful information about a different live process on each line, indicating the user who started the process, who is often root, the process ID or PID, the percentage of CPU resources and memory this process is currently using and skipping over to the final column at the right, the command use to launch the process. On most systems and especially systems running graphic desktops, the number processes returned by ps will run in the hundreds. You will therefore often want to narrow down your search using grep. You can also get PIDs for individual processes using pgrep. This example will search for a process called SSHD that was launched by the user root.</p>
<p>Free will show you how much system memory you’re currently using and how much is still free. Free-h will display that in more human readable terms. And uptime will tell you how long it’s been since the last time you booted the system. One company I worked for took great pride in having uptime on some servers return well over a year.</p>
<p>If you’re experiencing system slowdowns or other trouble, you can easily check which processes are selfishly grabbing most of the resources by running top. Top until you exit by pressing the Q key will display a regularly updated list of the processes using the most resources. The percentage of CPU and memory columns are the ones that the most interesting for us. Once you spot the culprit and decide you’re going to have teach it a lesson it soon won’t forget, you should note its PID or command name because that’s what you’ll use to shut it down. This is something that unfortunately I have to deal with almost daily. While Linux software is generally highly reliable there’s one video production package in particular that while offering fantastic features and quality will often freeze on me. Shutting it down using the close button or other normal approaches just won’t work so I have no choice but to kill its process from the command line. When a process, well let’s call it my app, does freeze I have two choices, killall followed by the name of the process which is usually the same as the command you see listed in top or ps would do the job nicely, while just kill and the PID that was displayed in top will also work.</p>
<p>The kill command, by the way, can also be accompanied by a signal code. Here are the most common signal codes. Number one, or SIGHUP which stands for Signal Hangup signals to a process that its parent shell is closing. Number two, or SIGINT interrupt is the equivalent of a Ctrl-c that interrupts a process. Ckill -9 will force the process to shut down while SIGTERM15 will immediately terminate a process. Running kill with no signal code will default to 15. Pkill works much the same as killall with the important difference that it will guess what you meant with misspelled processes thus if rather than pkill SSHD we were to run SSH, pkill would kill SSHD and in the process of course shutdown our SSH session so be extra careful with pkill.</p>
<p>There is chance by the way that you will kill the process but its GUI window will still appear on your desktop. This is called a zombie process. Unlike real life zombies, these won’t do you any harm and will disappear when you reboot.</p>
<p>To review ps will list all live processes. Pgrep allows you to narrow the listing down by user and process name. Free shows how much memory you’re using. Top displays those processes using the most resources. Killall or pkill will shut down a process by name and kill will shut it down based on PID.</p>
<h3 id="Using-screen-for-multi-screen-terminal-sessions"><a href="#Using-screen-for-multi-screen-terminal-sessions" class="headerlink" title="Using screen for multi-screen terminal sessions"></a>Using screen for multi-screen terminal sessions</h3><p>Believe it or not you can actually have multiple working windows each with its own running processes within a single terminal. Screen is a rather complicated program that creates an entirely keyboard based environment for managing these multiple windows or more accurately screens. The LPIC exam expects you to be at least familiar with the basics of Screen so I’ll give you a short walkthrough. First of all if it isn’t already, you need to install Screen using App Get. To launch a new screen, type screen. After the initial information pages your terminal will look the way it normal does. Ctrl-a w will display a window bar at the bottom with your current shell. You should be aware that for some reason Screen documentation uses uppercase C-a rather than Ctrl-a to describe their escape key combination.</p>
<p>You can detach from your current session using Ctrl-a and d. This will dump you back in your normal Linux parent shell. Part of the challenge of using Screen is keeping track of where you are and indeed, whether you’re inside or outside Screen. To list the currently running screen sessions from outside Screen type screen-ls. To get back into your screen type screen-x. Once inside you can create a new window using Ctrl-a c and split your screen horizontally using Ctrl a S. You can jump between screens using Ctrl-a tab. Ctrl-a&#x2F; will exit a screen and its programs.</p>
<p>That should be enough to get you started with Screen. Let’s quickly review Screen commands. Screen run from a regular shell will open a new screen. Screen-ls will list all current screens and screen-x will attach you to a live screen. From inside a Screen shell Ctrl-a c will create a new screen. Ctrl-a S will split your screen horizontally. Ctrl-a d will detach you from a screen. Ctrl-a tab will move between screens and Ctrl-a&#x2F; will exit a screen and all its programs.</p>
<h3 id="Controlling-running-Linux-processes"><a href="#Controlling-running-Linux-processes" class="headerlink" title="Controlling running Linux processes"></a>Controlling running Linux processes</h3><p>Until now besides our detour into Screen we’ve discussed identifying and if necessary killing processes. Now we’ll talk about directly managing ongoing processes that we don’t want to kill. By default, every job you start will take complete control of the shell from which it was launched. As we’ll see in a minute, it can be useful to see both its job number and its PID at startup. To do that add the &amp; character to the command. I’ve created a very large file called Big File that I’ll now copy the big file to, something that will take some time. Therefore we’ll run it with the &amp;. This will run the copy operation in the background. But besides learning the PID issued by the Linux kernel, we’re also told that this is job ID one within this shell. Since everything is happening in the background, we’ve got control of our command line prompt. Should we want to bring the job back to the foreground, we would use fg, standing for foreground, and the job ID. Should we now want to send the job back to the background, we hit Ctrl-z and then bg and then the job ID. Let’s delete the file we just created. A good Linux admin always cleans up his messes, right?</p>
<p>Now let’s run this again in the background. To see what’s going on we can use the jobs command to display all background processes and their status. We can also use ps by itself to display all processes running in this particular shell. Unlike psaux which displays all processes running on the whole system.</p>
<p>Finally we can run a command with nohup that’s no hang up in front of it to ensure that even if I should close this particular shell the process will survive until it’s complete. Just to review these last points adding an &amp; character after a command will force the process to run in the background. Fg and the job ID will bring it back to the foreground. Ctrl-z and then bg along with the job ID will send it back to the background again. Jobs and ps will both display current shell processes. And adding nohup to a command will ensure that it completes regardless of what might happen to the shell itself.</p>
<h1 id="Modify-process-execution-priorities"><a href="#Modify-process-execution-priorities" class="headerlink" title="Modify process execution priorities"></a>Modify process execution priorities</h1><p>Anytime you’ve got more than one process running on a system there’s a risk that competition for finite resources might cause trouble. Because default priorities don’t always work for every configuration, an operating system needs a tool that allows administrators to edit the priority scheduler.</p>
<h3 id="Using-nice-to-manage-multiple-processes"><a href="#Using-nice-to-manage-multiple-processes" class="headerlink" title="Using nice to manage multiple processes"></a>Using nice to manage multiple processes</h3><p>In Linux, that tool is called “nice.” To make it a bit easier to see this actually working, using screen, I’ve opened two terminals. Running “top” in the upper terminal you can see that every process has a priority number under the PR column and under the NI column a nice value. By default, every process has a nice value of 0 and a PR value of 20. You can change a processes nice value to make it as high as 19, which would raise the PR to 39 or as low as minus 20, 19 or really, really nice would lower the priority of a process to the point that the needs of all other processes would take precedence. In other words, our process is being nice to everyone else.</p>
<p>Lowering its nice value to minus 20, will give a process a very high priority in the competition for system resources or in other words it’s not being very nice to other processes at all. If you’re launching a process, you can manually set the value through nice.</p>
<p>Let’s say that you want to download a package using apt-get but you don’t want it to take too many CPU cycles away from other programs that you’ve got going. You might launch the process through “nice” like this. You can see that the process being run by root has a nice value of 10. Note by the way that the minus 10 I used here is not negative 10 but -10. If you wanted to lower the nice value to say minus 10 I would do it with two dashes the first being a dash and the second the minus sign.</p>
<p>For processes that are already running, you can edit the nice value using “re-nice.” Looking at “top” we can see the SSHD process is currently running with a nice value of 0. If I’d like to change that to minus 10 or to make it less nice I would note the process ID and in any other terminal run “re-nice minus 10-p” for process and the process ID. See how within a second or two the SSHD’s nice value changes. Re-nice doesn’t take a dash as part of the syntax so a single dash would in fact mean the minus sign. Don’t blame me for all of this. I didn’t come up with these rules. I just have to learn to live with them just like you.</p>
<p>Besides “top” you can also use “ps” to view all processes nice values using the “-elf” switch. You can also of course “grep” for a particular program or PID. Not only will nice and re-nice control the priority given to an individual process but they can also be used to control all processes launched by particular users or groups. To raise the nice level of the user Ubuntu to 10 use re-nice followed by “-u” and the user name. To lower the nice value of a group use “-g” and then the name of the group. Let’s review. A really nice process can have a value as high as 19. While a nasty, un-nice process can be selfish and go as low as minus 20. Launching a new process to control its nice value can be done using the nice program. Adjusting the nice value of a running process is done using “re-nice.” You can edit nice values for users and groups and view nice values using either “top” or “ps.”</p>
<h1 id="Search-text-files-using-regular-expressions"><a href="#Search-text-files-using-regular-expressions" class="headerlink" title="Search text files using regular expressions"></a>Search text files using regular expressions</h1><p>Regular expressions often described as Regex. regex are text strings that contain some mixture of regular letters that are meant to be understood literally as simple characters and meta-characters that have meaning at the system or processing level. Thus, characters like the dot, brackets, the caret, dollar sign, backslash and asterisk can all be used for instructions for tools like grep.</p>
<p>In this video, we’ll learn how to properly notate and use these instructions both in their regular and extended levels. We’re also going to learn how to use the grep tool much more effectively. By the way, as I don’t believe I’ve mentioned this before grep stands for Global Regular Expression Print.</p>
<h3 id="How-to-run-sophisticated-text-searches-using-regex-and-other-text-streaming-tools"><a href="#How-to-run-sophisticated-text-searches-using-regex-and-other-text-streaming-tools" class="headerlink" title="How to run sophisticated text searches using regex and other text streaming tools"></a>How to run sophisticated text searches using regex and other text streaming tools</h3><p>Here’s a silly but widely-used example. Looking at the fruits.txt file I created, we see that it contains the word “banana. “Rather than just searching for the whole word, something even a Windows user could do, we’ll do it like trained Linux admins and use grep to search for “banana,” specifically taking advantage of the repeated incidence of the letters A-N. Naturally, this won’t be so efficient in our case but you can no doubt imagine how it can be applied to more complicated cases. This will search for any letter B followed by any number of A-Ns and then an A.</p>
<p>The problem is that telling GREP to look for A-N and just A-N requires the use of parentheses and parentheses are reserved special characters. To tell grep to treat the parentheses the way we’d like we need to prefix each one with a backslash can quickly become tiresome. Using egrep or extended grep, on the other hand, tells Linux to treat the parentheses exactly the way we want them in this case. This produces the exact same results with a simpler command.</p>
<p>If you need to search through a file or directories of files for a complicated string, you can use either fgrep or grep -f. Fgrep, which stands for fast grep, works by ignoring any meta meanings that a special character like a dollar sign or an asterisk might sometimes have and searches for their literal presence. Let’s say that you’ve got a file that contains a high-level password that you’d like to find since the password contains say a dollar sign which is normally a special character you can use fgrep on the text.txt file to get it. You could also use grep with the “-f “switch to read the password from a file I’ve created, which contains nothing but the password like this. This can be very useful if you often search for the same complicated string and don’t want the hassle of retyping it over and over again. By the way, from a security perspective it’s very bad practice to store passwords in unencrypted files.</p>
<p>Processing text while taking into account special characters can add a dimension of complexity to just about any streaming operation. You can always simply remove special characters either in a live stream or by saving to a new file.</p>
<p>Let’s use the substitution tool “sed” to illustrate. We’ve created an html file called text.html that predictably contains html-formatting tags. Suppose you’d like to use the files text but without the tags. You can pipe the text to sed and have sed strip all the formatting. Here we’re simply removing each pair of left and right arrows and the characters between them. The “&#x2F;g,” by the way, tells sed to apply the changes globally or on every instance it encounters, rather than only on the first one. We’ve already seen how to pipe text to grep which then filters for specific strings. In fact however, grep can also be used on its own in a significant range of functions. Let’s try to find the word “sdb1, “ the designation for one of my disk partitions in the dmesg log file. We’re used to running it after cat like this. We can do the exact same thing without cat this way.</p>
<p>If you’d like to find the string “sdb1” wherever it’s found in any of the log files in the &#x2F;var&#x2F;log directory or in sub-directories below it add the “-r” recursive switch. You can use grep with “-v” to display only those lines without a particular string. Looking at the dmesg file we can see that the number three does show up a whole lot of times. We can exclude all lines containing the number three this way.</p>
<p>Let’s do that again adding the “-n” switch to print line numbers along with our output. This can be really useful if you would now want to find or edit specific lines within the file itself.</p>
<p>Let’s review. Regex meta-characters include the dot, or period, brackets, the caret, dollar sign, backslash and asterisk. To apply Regex functionality to grep filters you’ll often need the backslash character. Egrep will produce the same effect without the need for escape characters. Fgrep will search for literal strings, while grep-f will read a search string from any file. You can strip special formatting characters from a file using sed. “Grep -r” will search all files within a direct retrieve for a string. “-v” will display all the lines that don’t contain a specified string. And “-n” will number the lines displayed.</p>
<h1 id="Perform-basic-file-editing-operations-using-vi"><a href="#Perform-basic-file-editing-operations-using-vi" class="headerlink" title="Perform basic file editing operations using vi"></a>Perform basic file editing operations using vi</h1><p>Since text files are the core of everything that happens in Linux, and since Linux administrators and developers spend so much of their time working with text, you can easily see how choosing a text editing tool is a big decision. This is something with which you’ll probably end up spending more time than with your closest family members, for better or for worse. Linux has a great range of editors freely available to suit every need. Of course, since you’ll need to produce clean plain text, you’re not likely to settle for a full-featured office suite like LibreOffice.</p>
<p>Once that’s off the table, you could always go with a GUI package like Gedit, which includes a great library of built-in formatting tools for bash or all major programming languages. However, since so much of Linux administration takes place in the terminal, most admins prefer a terminal-based text editor. As you might already have noticed, my favorite is nano because of its balance of standard navigation tools and quick keystroke shortcuts like CTRL+X to save and exit, and CTRL+K to delete an entire line.</p>
<p>But admins and developers with very deep experience will often look sadly at people like me who insist on using nano or Gedit, as though we’re part of some unfortunate lower level of humanity. These are VI or VIM users, and having tasted a bit of the power of their chosen text editors, I can certainly appreciate why they think this way. The LPIC exam does too, as they’ve made it an important part of their exam requirements. But first, a little bit of server history. Once upon a time, hardware was much simpler, and system memory was much less available. It wasn’t uncommon for the keyboards attached to servers to have pretty much nothing more than the alphabet and numbers. There were no arrow or page up and page down keys, and certainly no function keys. And even if there were, not a lot of operating systems would have known what to do with them. Since there was so little to work with, software interfaces had to be creative and resourceful. Thus, was VI born.</p>
<h3 id="Understanding-VI-modes-and-keystroke-commands"><a href="#Understanding-VI-modes-and-keystroke-commands" class="headerlink" title="Understanding VI modes and keystroke commands"></a>Understanding VI modes and keystroke commands</h3><p>It’s a text editor that never asks your fingers to leave the main keys. And don’t even think about using a mouse. There are, therefore, keystroke sequences for doing absolutely everything. By the way, these days, you’re much more likely to encounter VIM, which stands for VI Improved, which incorporates the functionality of the original VI, along with some generous nods to modern computing habits. In any case, this style of commands and operations that came to exist from necessity had the fortunate benefit of also being incredibly efficient and fast. Someone who’s invested the time and energy to familiarize themself with the workings of VI will almost certainly be able to outperform people on similar tasks using other tools.</p>
<p>But here is the problem, if VI operations require the use of regular alphanumeric keys, then how are you supposed to use those keys to actually enter regular text? To make this possible, VI works in multiple modes.</p>
<p>Normal or command mode, is the mode that by default you will see when you start up VIM. Normal mode doesn’t allow you to actually edit text, but it does let you quickly perform just about any other task.</p>
<p>From normal mode, hitting the uppercase I or insert key will take you to insert mode, where you’ll do most of your typing. You can add, change and delete text, and even navigate through the page using arrow keys. But you can’t launch commands. Command line mode, which is where you perform file management tasks and when necessary, exit VI, is accessed by typing a colon followed by your command.</p>
<p>Let’s try it out. From the command line we’ll open VI, using either vi, or vim on my Ubuntu system, and the name of a file. Since we are by default in command mode, we can’t actually edit the text. I’ll hit the insert key and enter insert mode and add a few words. Notice how I can move around the text using the arrow keys. If I hit the ESC key, I’ll move back to command mode. Even if I can’t edit the text directly in command mode, the position of the cursor is important, as we’ll soon see.</p>
<p>You can now move around using the keys between h, and l. H will move one position to the left, j will go down one line, k goes up a line, and l moves one position to the right. The o key will move your cursor back to the start of the line you’re on. Now, based on the current cursor position, here’s how you use the command line to edit. dw will delete the word to the right of the cursor. U will undo your last operation. D$ will delete everything from the cursor to the end of the line. Dd will delete the entire line.</p>
<p>P for paste, will take the text that was most recently deleted and paste it at the current cursor position. P will also paste text copied using yy, which copies the entire current line. &#x2F; will take the next text you write and search a document for it, while N will repeat the previous search. Uppercase ZZ will save the current file and exit VI. Using command line mode, as you’ll remember always prefaced by a colon, w and enter will save your file, w a space and a new name followed by enter will save the file with it’s new name. W! overwrites the current file. Exit, or wq will exit VI, but only if the file hasn’t been changed, and wq! will exit without saving the file.</p>
<p>Let’s review. There are three modes to VI. Command mode, the default, allows you to edit your text, but not directly. Insert mode, reached through the uppercase I or insert keys, allows you to navigate and edit text directly. Command line mode permits file management operations. Command line operations include :w to save a file, exit or wq to exit VI, and q closes VI without saving. In command mode, dw deletes the word to the right of the cursor, d$ deletes to the end of the line, and dd deletes the entire line. P inserts recently deleted text to the current position, while u undoes the last action. Yy copies the current line, and uppercase ZZ saves and edits. Finally, h moves one position to the left, l one position right, j moves down, and k moves up.</p>
<p>To be honest, you can review these commands all you like and that will get you through the exam. But to really begin to master VI or VIM probably requires at least a week of actually working with it on a real project. While I can’t say that I’ve made it to that level, I’m still a solid nano kind of guy, I strongly suspect that you’ll never regret the investment.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-101-Create-Your-First-Amazon-EC2-Instance-Linux-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-101-Create-Your-First-Amazon-EC2-Instance-Linux-4/" class="post-title-link" itemprop="url">Linux-LPIC-101-Create-Your-First-Amazon-EC2-Instance-Linux-4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:08:47" itemprop="dateCreated datePublished" datetime="2022-11-19T01:08:47-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:09:04" itemprop="dateModified" datetime="2022-11-20T20:09:04-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-101/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-101</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-101-Create-Your-First-Amazon-EC2-Instance-Linux-4/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-101-Create-Your-First-Amazon-EC2-Instance-Linux-4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Boot-and-Package-Management-3-of-5-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Boot-and-Package-Management-3-of-5-3/" class="post-title-link" itemprop="url">Linux-LPIC-101-LPIC-1-101-Linux-certification---Boot-and-Package-Management-3-of-5-3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:08:46" itemprop="dateCreated datePublished" datetime="2022-11-19T01:08:46-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:19:38" itemprop="dateModified" datetime="2022-11-20T20:19:38-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-101/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-101</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Boot-and-Package-Management-3-of-5-3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-Boot-and-Package-Management-3-of-5-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-introduction"><a href="#Course-introduction" class="headerlink" title="Course introduction"></a>Course introduction</h1><p>Welcome to the third course in CloudAcademy’s LPIC 1 Certification Preparation series. In this course, we’ll learn about the preparation and process of effective Linux installations, and how to manage software packages through their entire life cycles. We’ll explore partitions and file systems, and the roles that they play together. We’ll learn how to select and install a boot manager and about software libraries, and how they can be shared among processes. We’ll also understand how dpkg and RPM, the two largest Linux managed software repositories work, and how you can use each of their front end applications, APT, and Yum.</p>
<h1 id="Partitions-and-Filesystems"><a href="#Partitions-and-Filesystems" class="headerlink" title="Partitions and Filesystems"></a>Partitions and Filesystems</h1><h3 id="Designing-Linux-disk-partitions"><a href="#Designing-Linux-disk-partitions" class="headerlink" title="Designing Linux disk partitions"></a>Designing Linux disk partitions</h3><p>While mature Linux distributions like Debian, Ubuntu, and Fedora come with their own automated GUI installation progRAMs, that can smoothly handle most people’s needs, at some point in your career as an administrator, you’re probably still going to have to manually configure an installation that’s meant to perform some special function. Coaxing nonstandard combinations of hardware and software to get along nicely with each other can still sometimes be a delicate and demanding process. So whether your future deployment security needs require that you completely separate parts of the file system from each other, or whether your reliability concerns demand a Raid 10 disk array, the solid understanding of Linux partitions and file systems you gain now will pay off solid dividends later. The principles of good disk design require balancing two opposites: separation and accessibility.</p>
<p>For example, you don’t want regular users having access to each other’s private files or to your system configuration files. But they can’t be so completely cut off that they can’t function either.</p>
<p>A partition is a portion of a disk drive that’s been logically walled off from the rest of the disk. Thus, a single one terabyte disk can contain four primary partitions, each say 250 gigabytes in size. From the perspective of software and users, these four partitions can look and behave like four physically separate disks, with all the security benefits and access headaches that go with it. On the other hand, you can also design your media so that resources on multiple physical disks can appear to reside on a single disk through either the logical volume manager, LVM or mount points. We’ll talk about LVMs a bit later in this video.</p>
<p>And even though we’ll discuss mount points in a later course in this series, it can’t hurt to quickly show you how it’s done right now. I have a USB drive that’s been designated as sdc. So it’s primary data partition would be &#x2F;dev&#x2F;sdc1. We’ll go to the &#x2F;tmp directory and use mkdir (make directory) to create a directory called drive. We’ll then type sudo mount &#x2F;dev&#x2F;sdc1 drive, then CD into the drive directory. ls will list all the contents, which are actually the contents of my USB drive, meaning that we’ve successfully mounted that drive to the directory &#x2F;tmp&#x2F;drive.</p>
<p>Let’s begin with a look at the Linux file system itself. I divided the top level directories into four sections. The home directory, under which users directories and files are kept. The system directories like etc, var, and lib. Virtual or pseudo directories like dev and proc and boot which as we’ll soon see is in a category all its own.</p>
<p>Under normal circumstances, you’ll spread your installation over three partitions. The root file system, a single 500 megabyte partition containing the Boot file system, which includes the Master Boot record, and a swap partition for virtual memory for use when you run out of RAM. You will usually create a swap partition that’s twice the size of your system RAM per graphic desktop computers and the same as your RAM for servers. But there’s also times when you’ll to keep the home directory tree on a separate partition or sometimes even on a separate disk entirely. This way, if something should happen to corrupt any or all of your system files, rendering your computer unbootable, it will be much less likely to have any effect on your user’s personal and work files. That will give you the option of simply copying the entire home system intact to wherever you’d like to rebuild your computer. You will have to create their accounts and assign new passwords, something that shouldn’t require more than a few minutes work . But their data will all survive.</p>
<p>Similarly, if you’ve got a development server that’s accessed by a team of coders and admins using it as test bed for deployment experiments, you might want to keep the &#x2F;var directories in a separate partition from everything else to contain the consequences of an experiment that might go wrong. However you decide to organize your partitions, you’ll want to accurately anticipate how much space each will need.</p>
<p>It is possible to re-size a partition once it’s been created, but it’s something you’d far prefer to avoid because every re-size operation carries a risk of failure. And failure, in this area, usually means the complete loss of all disk data. So how much space will you need. The main Linux system can be comfortable on less than four gigabytes.</p>
<p>You might think you should save everything else for your &#x2F;home directories and the terabytes of all those funny cat videos they’re eagerly waiting to download. But there are other important things to consider. Linux systems can generate an awful lot of data, especially log files. And they’re mostly kept in the &#x2F;var&#x2F;log directory. I’ve seen individual log files larger than 100 gigabytes. Naturally, as we’ll discuss in a future video, you will want to properly rotate your logs so things don’t get out of hand. But some projects might not let you. And there’s always a chance, as I’ve seen happen many times, that someone will launch at test process on a virtual container and forget about it, allowing its logs to cheerfully consume a higher and higher percentage of total disk space. This, by the way, is one reason many administrators create a separate partition for var so that if things do get out of hand, it won’t clog up the whole system.</p>
<p>So you’ll have to carefully access how your system is going to be used in order to properly accommodate your actual needs. We’ll talk about how you can actually create partitions in a later video. But for now, we’ll break with our regular practice so we can illustrate what partitions might look like on a real system. Just this once , we’ll use a GUI program. GParted is a front end for the command line partitioning tool, parted. And while Linux admins normally prefer the command line to anything else, some have even been known to order pizza from a terminal. gparted is something of an exception. As you can see, there are two hard drives in my system designated as sda and sdb.</p>
<p>sda is a semi-retired disk that, until it began showing some indications that it might soon fail, was my main system disk. I now keep it running just in case I ever need to access old files. My luck, it’s been running this way for more than a year and hasn’t yet failed. sdb is my current boot drive containing my live deployment of Ubuntu 14.04. You can see that it has three partitions. sdb1 is a 500 megabyte partition mounted to &#x2F;boot&#x2F;efi. It’s obviously my boot partition with the Master Boot Record. Five hundred megabytes is probably far more than the boot partition will ever need. But in an age of incredibly cheap storage space, this has become pretty much a standard practice.</p>
<p>Note that the boot flag has been set, which signals to BIOS that is a bootable drive. Note also that it was formatted as fat32, which will allow it to be booted to Window Operating System should that be necessary. sdb2 is the largest partition, nearly 500 gigabytes and is mounted in two places. But the only detail that interests us right now is root represented by a single slash. And sdb3 is my swap partition set as a Linux swap File System. You can also see that all of these partitions are represented as devices in &#x2F;dev directory.</p>
<p>Gparted’s main purpos, of course, is to create, delete, and re-size partitions. We’re not going to do that now though. By the way, you should definitely be aware that you can display the partition information for all your block devices from the command line using lsblk and a great deal more information using sudo fdsk -l.</p>
<p>As a quick review, partitions organize and protect data and file systems. You can mount file systems so they’ll seem to live in different places than they actually do.</p>
<h3 id="Introduction-to-the-Linux-Logical-Volume-Manager-LVM"><a href="#Introduction-to-the-Linux-Logical-Volume-Manager-LVM" class="headerlink" title="Introduction to the Linux Logical Volume Manager (LVM)"></a>Introduction to the Linux Logical Volume Manager (LVM)</h3><p>Normal Linux installations will create three partitions. Root, represented by a slash, boot, and swap. But for business and practical considerations, you might want to separate other directories. And you always need to care for the future space and access needs. Another way to manage drives and partitions is through LVM, Logical Volume Manager. If your system is already configured with LVM, which is something you’re not expected to do for the LPI exam, you can use it to treat every physical hard drive attached to your computer as a single drive and to easily and safely re-size and rearrange any individual parts of those drives just about any way you like. For the exam, it’s enough to be familiar with some of the basic LVM tools. First of all, PV stands for physical volume. VG stands for volume group, which is a collection of one or more physical volumes and LV represents logical volume, a virtual volume you create from the contents of a volume group. To create a volume group, you could use VG Create followed by the name you like to give your group, say My VG. You follow that with the physical partition or partitions you’d like to build the group.</p>
<p>In our case, that will be &#x2F;dev&#x2F;sdb2 and &#x2F;dev&#x2F;sdb3. To create a logical volume, use LV Create &#x2F;l and the total size you want for your volume, 250 gigabytes in our case, &#x2F;n followed by the name you’d like to give to the logical volume. And then the name of the volume group from where you’d like the space to come. Finally, use LV Scan to confirm that everything worked out the way you wanted it to .</p>
<p>To review, in LVM2, physical volumes are called PV, Volume Groups, VG, and Logical Volumes, LV. You create a new Volume Group using VG Create. Create a new Logical Volume with LV Create, and display your volumes using LV Scan.</p>
<h1 id="Working-with-a-boot-manager"><a href="#Working-with-a-boot-manager" class="headerlink" title="Working with a boot manager"></a>Working with a boot manager</h1><p>In the previous course, we discussed how to edit the Grub menu to control the boot process. Now we’re going to take a bit of a step backwards, to learn how to install Grub on a computer from scratch, and to identify and update existing Grub settings. I should note that Grub is a bit of a moving target. Not only have there been different versions, known as Grub Legacy and Grub2, that for awhile at least, existed in parallel with each other, but the configuration files and commands will vary among Linux distributions, and even from one release to the next of a single distribution.</p>
<p>It can be especially confusing trying to figure out what someone means by Grub2. Is it the Grub Stage 2 that we discussed earlier, the launched by either Grub stage 1 or Grub stage 1.5, or is it Grub version 2? We’ll do our best to clear things up. One question did originally bother me a bit. If I’ve got a Linux computer running happily, why should I ever want to install or update Grub? This question is especially strong, considering how good Linux installation processes are at installing Grub automatically. In fact, many users can go for years without even being aware that Grub exists. There are two main answers to this question. First of all, there is always the possibility that you may come across a computer, especially an older application server that isn’t booted very often, that’s still running an older version of Grub, that is now known to have bugs. Just to be sure, we’ll actually reboot the next time, it can be a good idea to install the latest version.</p>
<p>But by far, the most common reason people have for considering Grub updates is to figure out how to recover from a Grub configuration that’s become corrupted. Did Windows wipe out your Grub? Did an update go badly? You might well be a candidate for some of what we’re going to talk about here.</p>
<h3 id="Managing-the-grub-bootmanager"><a href="#Managing-the-grub-bootmanager" class="headerlink" title="Managing the grub bootmanager"></a>Managing the grub bootmanager</h3><p>Okay. First of all, let’s check to see if Grub is even installed on a system by querying a package manager. By a Fedora install, use RPM-Q for query, Grub2. Note how Fedora calls it Grub2 and not just Grub. If it’s there, RPM will output the current version. If it’s not installed, run again on Fedora, Grub2-install &#x2F;dev&#x2F;sda, where dev sda is the partition where you’d like it installed. Believe it or not, that is often all you’ll need.</p>
<p>If you’ve already got a living, breathing version of Grub installed, but you’d like to edit the way it works, you need to know about a few files. Grub itself uses a script called Grub.cfg on Ubuntu or Fedora, or menu.lst on SUSE or Debian. These files are kept in the &#x2F;boot&#x2F;grub directory, except for Fedora of course, which keeps it in boot&#x2F;Grub2. The first thing you’ll see when you view the file, however, is that editing this file directly won’t do you any good. the grub script is, in fact, generated by grub-mkconfig (or grub2.mkconfig for users of you-know-which distribution) using templates from &#x2F;etc&#x2F;grub.d and settings from &#x2F;etc&#x2F;default&#x2F;grub.</p>
<p>To be more specific, &#x2F;etc&#x2F;grub.d contains files defining available operating systems and allows you to add others for your own custom menu entries. &#x2F;etc&#x2F;default&#x2F;grub is a config file that sets the rules grub will use - like the way the grub menu should appear, how to determine which OS to boot by default, and how long the default timeout should last (before automatically launching the default OS).<br>So, assuming that we’ve made some important edit to the settings files, you can update the grub.cfg (or menu.lst) file using something like grub2-mkconfig -o &#x2F;boot&#x2F;grub2&#x2F;grub.cfg (for Fedora, obviously).</p>
<p>You may sometimes come across references to a command called update-grub or update-grub2. As you can see from the contents of &#x2F;usr&#x2F;sbin&#x2F; on my Ubuntu system, update-grub2 is a symlink to update-grub, and update-grub is nothing more than a script that will run the grub-mkconfig file. So I guess all those rivers lead to the same ocean.</p>
<p>Finally, while this has nothing to do with the LPIC exam, here’s a tip that can - and probably will - save your skin someday when you’re under pressure. There’s a wonderful free Linux package called Boot-Repair that will do just that: repair a broken boot process. I know what you’re thinking: if you can’t boot your computer, how are you going to load some software package? Boot-Repair thought of that, too. You can easily find simple instructions on creating a USB disk that you can plug into your comptuer and boot right into Boot-Repair. You can also launch a regular live-session of Linux, download Boot-Repair, mount your hard drive, and let Boot-Repair’s magic do the rest. You’ll definitely thank me for this one day.</p>
<p>Let’s review. The current version of grub, sometimes known as grub2, is the replacement for what we now refer to as legacy grub. grub2-install (or grub-install) followed by a target partition will install grub, grub.cfg, grub2.cfg, or menu.lst - all to be found in &#x2F;boot&#x2F;grub - are scripts generated by files in &#x2F;etc&#x2F;grub.d and &#x2F;etc&#x2F;default&#x2F;grub. the GRUB_DEFAULT setting controls which OS will boot by default (if none is selected before the timeout), other settings you can control include menu color and timeout.</p>
<h1 id="Working-with-shared-libraries"><a href="#Working-with-shared-libraries" class="headerlink" title="Working with shared libraries"></a>Working with shared libraries</h1><p>The speed and quality of an operating system is largely determined by the way it accommodates the application packages that are meant to run on it. If every office productivity and media viewing package you installed was more or less expected to create its own way of storing and handling files, identifying the local hardware environment, and connecting to the Internet, then there would be a lot less software packages available. And, those that did exist, would be a great deal larger and slower.</p>
<p>Linux ensures that software developers don’t have to reinvent the wheel with each program by providing libraries. Now, rather than, say, programming an interface to the filesystem from scratch, developers can simply have their program call a local library that will do it for them.</p>
<h3 id="Working-with-Linux-shared-libraries"><a href="#Working-with-Linux-shared-libraries" class="headerlink" title="Working with Linux shared libraries"></a>Working with Linux shared libraries</h3><p>There are two kinds of libraries: static and dynamic. A program will read a static library as it is initially installed, and incorporate its contents into its own code. This can make the program run a bit slower, but it has the significant advantage of remaining independent of external content once it’s running. For a system recovery tool, for instance, this can be a huge advantage.Dynamic libraries must remain available as it is accessed whenever the program needs its information. This, obviously, allows for much leaner and faster tools.<br>When a package is prepared for a Linux system, it will establish the libraries - or dependencies - that it will need. The various curated software repositories that serve various Linux distributions - like apt and yum - will be aware of these dependencies and will do a great job of making sure that they’re all met during the installation process. </p>
<p>However, as an administrator, you will sometimes have to work with software that’s not part of the mainstream repositories, and you’ll therefore need to handle access to libraries yourself. In this video, we’re going to learn how these libraries are designed and how you can work with them for your own projects.As you might imagine, many libraries live in or beneath the &#x2F;lib directory. From my Ubuntu system, you can see some libraries in &#x2F;lib itself, and others organized by category in subdirectories. Let’s break down a filename to understand how the naming conventions can tell us quite a lot about a library. The file begins with the letters l<em>i</em>b - telling us that it’s a library. the next section - until the first dot - is the unique library name. s*o means that this is a dynamic library - a static library would have a letter “a”, instead. The number after the second dot is the library’s version.<br>Applying the ldd program against a library will print its dependencies - libraries, you will discover, are often built on other libraries. ldd displays a library’s dependencies and the locations of their files. By the way ldd, when applied to a binary file - like these in &#x2F;bin - will also display whether the binary is dynamically linked, and if it is, what its dependencies are.<br>When a new software package prepares to install itself on Linux, it looks to a file in the &#x2F;etc directory called ld.so.cache for information on all the libraries currently available to the system. Since this file is not human-readable, there’s another file in &#x2F;etc - ld.so.conf - that mirrors the information from ld.so.cache. On some systems, ld.so.conf will contain thousands of lines of data, but at least on Ubuntu and Fedora, it only contains a pointer to the &#x2F;etc&#x2F;ld.so.conf.d directory, which, in turn, contains files pointing to libraries kept elsewhere on the system, like &#x2F;usr&#x2F;lib. Either way, with this information, you will know how to find what’s available to you.</p>
<p>You can also access the names and locations of all your currently installed shared dynamic libraries by using ldconfig Although, since I have more than 1,700 libraries right now, that might be a lot of reading. As always, you can use our old friend grep to make our lives easier. So, if we were looking to see if anything related to libQtGui might be available, we would run ldconfig -p | grep libQtGui…which will be much more manageable.</p>
<p>If you manually add or change a library, you’ll have to give Linux the good news. You can do this either by adding your library to LD_LIBRARY_PATH, or by creating a file in &#x2F;etc&#x2F;ld.so.conf.d&#x2F; called something like my_lib_name.conf in which you simply point to the directory that will host your library. You then need to run ldconfig -v to update Linux’s list - where -v tells ldconfig to be verbose and keep you up on what it’s doing.</p>
<p>To review, shared libraries provide important system information to software packages. Dynamic libraries - identifiable by the letters s<em>o, can be called whenever a program is used, while static libraries - taking an “a” rather than s</em>o - are used only during installation. Most libraries are kept in or below &#x2F;lib. ldd will display the dependencies and file locations of either a binary program or a library. &#x2F;etc&#x2F;ld.so.cache is where software packages look for a list of system libraries, while we humans can get that information through &#x2F;etc&#x2F;ld.so.conf, which points to the files in &#x2F;etc&#x2F;ld.so.conf.d&#x2F;ldconfig -p will display all current libraries, while ldconfig by itself will update the official library list.</p>
<h1 id="Debian-package-management"><a href="#Debian-package-management" class="headerlink" title="Debian package management"></a>Debian package management</h1><p>As we mentioned in the previous video on shared libraries, the various Linux package management systems do a spectacular job handling installation and dependency tracking for us. Of course, they also do much more. Since these repositories are closely curated, you can be confident that the software you download will be useful, fully functional and malware-free. And considering just how many thousands of software packages these repositories contain, the fact that everything works as well as it does is something of a marvel.</p>
<h3 id="Managing-packages-on-Debian-Linux-systems-using-dpkg-and-apt-get"><a href="#Managing-packages-on-Debian-Linux-systems-using-dpkg-and-apt-get" class="headerlink" title="Managing packages on Debian Linux systems using dpkg and apt-get"></a>Managing packages on Debian Linux systems using dpkg and apt-get</h3><p>Linux distributions mostly align themselves with one or another of the larger package management systems. The two largest of these - Debian’s dpkg, and RPM - serve some of the most popular modern distributions: dpkg can count Debian, Ubuntu, and Linux Mint among its “customers,” while RPM (whose “r” originally stood for Red Hat) serves Red Hat, Fedora and openSUSE, among others. Through this video and the next, we’ll focus specifically on these two managers - which just happens to line up nicely with the exam expectations.<br>Besides maintaining the software repositories themselves, to ensure easy compatibility, package managers also establish rules and practical guidelines for how the packages are built.</p>
<p>Thus, a .deb package meant for dpkg won’t work on a system that’s looking for .rpm. That’s seldom a problem, since most software that’s available on one, will be similarly accessible through the other. But it does mean that the fine working details of the two package managers will be a bit different, and you will definitely be expected to be familier with them for the exam. So let’s take a good look at Debian’s dpkg - and leave RPM for the next video. </p>
<p>You can install, remove, status-check and generally manage your software using the dpkg command and its arguments. However, there are a number of frontend programs - each with its own unique features - that can do a great deal more. We’ll explore three of these: apt, aptitude, and synaptic a bit later. In the meantime, we’ll go through some of dpkg’s own functions. dpkg’s default behavior is controlled by the dpkg.cfg file found in the &#x2F;etc&#x2F;dpkg directory. This particular copy has only two settings concerning package signatures and log locations. If you’ve directly downloaded a .deb package you can install it with root authority using dpkg -i</p>
<p>We’ll use this fictitious package to illustrate Debian naming standards, where my_package is the package name, the number is the package version, i386 is the architecture it was written for (32-bit, in this case), and deb identifies it as a Debian package. You can use dpkg…to unpack package files without actually going through configuration process. I would imagine this might be useful if you plan to manually create a non-standard configuration profile.</p>
<p>You can remove a package that’s currently installed, identifying it through its short name, using dpkg -r. To remove the package files AND its user settings, use purge. And dpkg-reconfigure will reconfigure an already-unpacked package. You can use debconf if you want to simply display a package’s configuration:</p>
<p>To review. You can manage local software packages using dpkg. dpkg defaults are kept in the dpkg.cfg file. You can install a package using dpkg -i. dpkg –unpack will unpack but not install. dpkg -r will remove a package, dpkg -P will purge it, and dpkg-reconfigure will allow you to configure a package’s settings over again</p>
<p>Now, as you might have noticed, dpkg only works with local packages, and not external repositories. The best way to access the wide world of Linux software, therefore, is through APT (which stands for Advanced Packaging Tool). You tell Linux how to find the Debian repositories through the sources.list file that’s kept in the &#x2F;etc&#x2F;apt directory.</p>
<p>You can see pairs of links to Internet URLs: one of each pair called deb - obviously for Debian - and the other deb-src. Each link points to the repo associated with a particular release - in this case Ubuntu’s Trusty - and a specific category. We can see main, universe, and multiverse here. Each of those contains a different range of programs. deb URLs will contain program packages including binary files, while deb-src sites contain program source code files. If you want to manually add a previously inaccessible repo source - perhaps a trusted third party source - this is the file you will update. Just be careful to get the syntax right or it might not work.</p>
<p>The most popular command line tool for software management is apt-get. apt-get update will tell your local apt system about any new package information available from appropriate remote repositories. apt-get install, followed by the package name, will install a package. apt-get upgrade will search for and apply all available package upgrades. apt-get dist-upgrade is similar to apt-get upgrade but, in addition, will also intelligently remove dependencies made unnecessary by overall changes. apt-get remove will, predictably, remove a package you no longer need. Finally, running apt-get install using the -s flag will display dependency and installation information, but without installing:</p>
<p>Another way to view package data is through apt-cache. apt-cache stats will display some package information about a specified package. apt-cache depends will display package dependencies and apt-cache unmet will display any unmet dependencies associated with a specificed package. Running apt-cache unmet without specifying a package will output all unmet dependencies on the whole system. Be prepared to go through a lot of reading!</p>
<p>Besides running apt from the command line, you can also use Aptitude, which is a terminal-based interface for people who aren’t yet comfortable with apt-get but too embarrassed to use a true GUI interface like Synaptic. To be honest, I’m not really sure what value Aptitude has these days (although, now that I’ve said that, I’m sure that I’ll receive hate mail from it’s supporters), but the LPI exam expects you to know about it. Hitting the ? key will give you a help page, and hitting q will exit. What more could you possibly need?</p>
<p>Let’s review. The sources.list file tells Linux where to find your remote software repositories. Links in the sources.list file point to repo URLs, including repo type, address, distribution, and category. apt-get update will update information about available upstream packages. You can install packages using apt-get install, upgrade all packages using apt-get upgrade or apt-get dist-upgrade, apt-get remove will, predictably, remove a package you no longer need. apt-get install -s will display package information but not install it, and apt-cache stats, depends, or unmet will also display package information.</p>
<p>Finally, Synaptic will nicely manage your packages from the Desktop. Besides doing just about everything apt-get can do, Synaptic is also a great research tool, allowing you to search for packages [chess] and look up file and version information.</p>
<h1 id="RPM-x2F-YUM-package-management"><a href="#RPM-x2F-YUM-package-management" class="headerlink" title="RPM&#x2F;YUM package management"></a>RPM&#x2F;YUM package management</h1><p>We’re going to continue discussing the topic of package managers, a topic the LPI considers more important than most.</p>
<p>Interestingly, the job description for the first full-time Linux sysadmin job I got demanded familiarity with Linux package management more than anything else. Many months later, I asked my boss why he focused on that the way he did, especially considering how many other skills I had to draw on for the day-to-day work I’d been doing. He replied he’d done it that way for two reasons. Because package management is, in fact, a major foundation for so much else in Linux administration, and because being comfortable with apt and yum was a particularly good indication of good general experience. I now agree. As with the dpkg system that we discussed in the previous video, RPM software packages that already exist locally can be installed and removed directly using, in this case, the RPM program.</p>
<h3 id="Working-with-RPM-and-Yum-package-management"><a href="#Working-with-RPM-and-Yum-package-management" class="headerlink" title="Working with RPM and Yum package management"></a>Working with RPM and Yum package management</h3><p>The file naming conventions for RPM work much the same way as for dpkg as we can see from this package I downloaded to my Fedora machine. In case you’re wondering, it’s ApacheDS, an LDAP and Cerberus server. Here, ApacheDS is the package name. The number is the package version, X86_64 is the architecture it was built for. This time, it’s 64-bit, and the .rpm extension tells our system that it’s an RMP package.</p>
<p>As the root user, we would install the package using rpm -i. We can also install - or upgrade it - using rpm -U …which, if you ask me, can be a bit dangerous, as, under the circumstances, you’d normally associate u with uninstall. rpm -q will query the existence of a package. We’ll run that against our ssh server to make sure it’s there. Of course, since I’m currently logged into Fedora using ssh, I’d definitely assume it is. rpm -i –test will test a package for dependencies. In this case, it’s a good thing we checked, because apacheds is a 64-bit package and the machine Fedora is running on is - if you can believe it - only 32 bit. rpm -e will remove a package. Running rpm -e with the –nodeps argument will only remove files that aren’t dependencies elsewhere.</p>
<p>You can (and should) get the checksum from an rpm package using rpm -vK (where v adds more verbose messages). You should then compare the output to the checksum provided by your package source. rpm -V will verify a package. In our case, we’re correctly told it’s not yet installed.</p>
<p>Let’s review. rpm packages place the package name first, the version next, followed by the architecture and the .rpm extension. rpm -i or -U will install a package. -U will also upgrade the package. rpm -q queries the existence of a package. rpm -i –test returns a package’s dependencies. rpm -e will remove a package, and adding –nodeps will respect other package’s dependencies. rpm -K returns a checksum. And rpm -V will verify a package.</p>
<p>Like apt-get on the Debian system, yum provides a powerful front end for rpm. The folder &#x2F;yum.repos.d&#x2F; will contain repo files for each repo that’s currently in use on your system. Here, rather than throwing them all into a single file the way apt does it, yum provides a separate file for each software category. Repo information can also be stored in the &#x2F;etc&#x2F;yum.conf file.</p>
<p>Using yum to work with packages is straightforward. You can install a package using yum install xboard. xboard, by the way, is a chess game. If you’re already bored with that, you can remove it using yum remove. To update a package, use yum update. And to search for an installed program, use yum search. yum list will list all available programs. Although, since there are bound to be lots of them, you might prefer to use grep to narrow down your search. yum list installed will list installed programs. You can also directly download packages from online repositories without installing them using yumdownloader, where –resolve will take care of dependencies.</p>
<p>Since rpm files are usually compressed using the cpio file archiver, if you ever want to access all or some of the files from an rpm archive, you’ll need to use rpm2cpio. In this example, we’ll pipe the contents of the apacheds package to cpio itself, and where i tells cpio to restore the archive, and d tells it to create leading directories where needed.</p>
<p>By way of review, the folder &#x2F;etc&#x2F;yum.repos.d&#x2F; or the &#x2F;etc&#x2F;yum.conf file will contain pointers to repo sources. Yum is really simple: install will install a package, yum update will update, yum remove will remove, and yum search will search. yum list and yum list installed will display packages according to your needs. You download packages without installing them using yumdownloader</p>
<h3 id="Working-with-Yum-on-AWS-Linux"><a href="#Working-with-Yum-on-AWS-Linux" class="headerlink" title="Working with Yum on AWS Linux"></a>Working with Yum on AWS Linux</h3><p>AWS Instances using the Amazon Linux distribution use rpm and yum for file management. By default, the &#x2F;etc&#x2F;yum.repos.d directory contains links to specially managed Amazon repos like amzn-main. Of course, if you choose an Ubuntu-based instance, you will find everything through dpkg and apt. The software installed on AWS instances can also be provisioned by script using deployment services like Elastic Beanstalk.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-System-Architecture-2-of-5-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-System-Architecture-2-of-5-2/" class="post-title-link" itemprop="url">Linux-LPIC-101-LPIC-1-101-Linux-certification---System-Architecture-2-of-5-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 01:08:44" itemprop="dateCreated datePublished" datetime="2022-11-19T01:08:44-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 20:16:50" itemprop="dateModified" datetime="2022-11-20T20:16:50-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/LPIC-1-101/" itemprop="url" rel="index"><span itemprop="name">LPIC-1-101</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-System-Architecture-2-of-5-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/Linux-LPIC-101-LPIC-1-101-Linux-certification-System-Architecture-2-of-5-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to this, the second, course in Cloud Academy’s LPIC 1 certification prep series. This course will focus on understanding Linux system architecture, how Linux identifies and manages its hardware environment and how you can control and direct that process to achieve the results hereafter.</p>
<p>Through the videos of this course, we’ll learn how to identify existing hardware peripherals and how to use Linux file systems and kernel modules to find, add or remove device drivers. We’ll learn about the Linux boot process and each of the three historically significant process managers, Init, Upstart and System D.</p>
<p>We’ll explore how to use system tools to ensure that computers boot properly and to figure out what’s going on when they don’t. We’ll learn about changing the system states through run levels and process management.</p>
<h1 id="Identifying-hardware-peripherals"><a href="#Identifying-hardware-peripherals" class="headerlink" title="Identifying hardware peripherals"></a>Identifying hardware peripherals</h1><p>If you want to really understand how an operating system works, you’ll need to have a good sense of how it connects, tracks, and controls its hardware environment. Linux, when it boots, or experiences some change to its host hardware profile, will maintain a system of virtual - or pseudo - files describing the devices and drivers it can see. By “virtual files” I mean regular text files that are saved to volatile memory rather than to a disk drive. Their contents will be lost whenever the system is shut down, but that’s no big deal: because the boot process will create appropriately updated versions when it starts up the next time.</p>
<h3 id="Linux-virtual-filesystems-and-hardware-peripherals"><a href="#Linux-virtual-filesystems-and-hardware-peripherals" class="headerlink" title="Linux virtual filesystems and hardware peripherals"></a>Linux virtual filesystems and hardware peripherals</h3><p>The Linux kernel writes hardware and driver data to virtual files in two separate directory hierarchies. Everything under &#x2F;proc (for “process”) is part of the sysctl system, and the &#x2F;sys directory contains the sysfs - fs, by the way, stands for filesystem. What’s the difference between the two? Apparently sysfs - being a bit more recent in design - is built using a more sophisticated structure. So, for instance, many files under &#x2F;sys are actually nothing more than symbolic links (symlinks) pointing to the devices themselves. We’ll learn more about symlinks and how they work in a later video.</p>
<p>Let’s take a quick look at the &#x2F;sys directory tree. Most of the files that will be of interest to us live beneath &#x2F;sys&#x2F;class where you’ll find all devices organized by type (or, class).</p>
<p>So if we drill down through &#x2F;printer&#x2F;lp0 - which is the designation Linux has given my Brother laser printer - and then down further through &#x2F;subsystem &#x2F;lp0 and &#x2F;device, we’ll be able to take a look at details of my printer configuration by reading - using cat - the id, resources, and options files.</p>
<p>To explore my hard drive configuration, you would go to &#x2F;sys&#x2F;class&#x2F;block - which is where block devices are described. Since my primary drive is designated as sda1, we’ll follow through to &#x2F;sda and then &#x2F;sda1. From here we could examine details like partition, size, and status.</p>
<p>The &#x2F;proc system has a somewhat different design. To learn about some system drives, for instance, we could move to &#x2F;proc&#x2F;sys&#x2F;dev [ls] and then down to cdrom. Reading the contents of the info file tells us that our CDRom drive - it’s actually an RW DVD - is called SR0 and runs at 12x speed.</p>
<p>But if we head back up to the &#x2F;proc top directory, we’ll also see files like cpuinfo - which, using the text reader program less, identifies the type of processor we’re running: in my case, since I’m using a quad core processor, each of the four cores is listed separately. Similarly, the devices file contains information about block devices like my hard drives.</p>
<p>Beyond the files in &#x2F;sys and &#x2F;proc, the &#x2F;dev directory contains another pseudo file system created during the boot process. As you can probably tell from their filenames, each of these files represents a specific hardware device. We’ll spend more time with the files in &#x2F;dev when we learn about mounting and unmounting devices in later videos.</p>
<p>Just to quickly review what we’ve seen: Linux populates three pseudo filesystems with configuration data defining your hardware environment. Information about physical devices and their drivers can be easily found beneath the &#x2F;sys directory - and especially beneath &#x2F;sys&#x2F;class - and beneath &#x2F;proc…&#x2F;sys…&#x2F;dev. The &#x2F;dev directory contains symbolic links to actual devices through which you can control their use and accessibility.</p>
<h3 id="Using-Linux-command-line-tools-and-modules-to-manage-hardware"><a href="#Using-Linux-command-line-tools-and-modules-to-manage-hardware" class="headerlink" title="Using Linux command line tools and modules to manage hardware"></a>Using Linux command line tools and modules to manage hardware</h3><p>Besides those virtual files, you can access the same system hardware and driver information using some command line utilities. So, for instance, if you wanted to list all the PCI devices currently known to your system, you could simply run lspci - where “ls” stands for list. Here, for instance, you can see my Radeon video controller and, down at the end of the list, my two network cards. Running lspci with the -vvxxx argument will display a great deal more information and can be very useful for diagnosing missing or misbehaving devices. V stands for verbose, and xxx will show a hexadecimal dump.</p>
<p>Running lsusb will display all the registered usb devices, including my keyboard and mouse.</p>
<p>lsmod will display all the kernel modules that are currently loaded. In fact, lsmod is really nothing more than a nicely formatted output of the &#x2F;proc&#x2F;modules file.</p>
<p>I should take a moment right now to explain what a module is. The Linux kernel is, as you might imagine, the core of the operating system, containing many of the basic instructions for managing processes and system resources. But, in order to extend the kernel’s control over peripheral devices, individual modules can be added or removed without directly effecting the kernel or its operations. We’ll learn more about actually loading and removing modules in the next video. While details shouldn’t concern us right now, you should at least be aware of the D-bus message bus system that permits integration and proper coordination between processes running desktop - rather than server - applications</p>
<p>To review: lspci will list all devices connected to the PCI bus, lspci -vvxxx will include more detailed information in its output, and lsmod will display currently loaded kernel modules. By the way, it can be worth knowing that running lshw - preferably with sudo - will output all your hardware specs at once.</p>
<p>Until now, we’ve discussed the way that Linux manages devices. But you must also be aware of the various tools employed by Linux to handle processes: that is, how access is given to individual programs so they can play nicely with each other and share system resources.</p>
<p>In the beginning there was init. Init acted like an air traffic controller, waving processes through busy air space and determining the order by which waiting programs would be permitted to take off. Init’s greatest weakness was that it could only process actions synchronously, which proved very inefficient.</p>
<p>Somewhere around 2006, the developers behind Ubuntu released an asynchronous replacement for init called Upstart, which has been the mainstay of some of the biggest distributions in the business. Most. But not all. Over the past year, after some rather excited debate and more than a few spoiled friendships, Upstart’s competition, systemd, won over so many hearts and minds, that even Ubuntu has now agreed to make the switch. The final move took place with the release of Ubuntu 15.04. We’ll talk about some systemd tools later in this series.</p>
<h1 id="Managing-hardware-peripherals"><a href="#Managing-hardware-peripherals" class="headerlink" title="Managing hardware peripherals"></a>Managing hardware peripherals</h1><p>Many of the most painful memories I have of those dark days before I discovered Linux, involved trying to figure out why I sometimes couldn’t get even integrated sound cards or network interfaces to work. It’s possible that the way Microsoft handles such things has improved over the years, but creating an operating system that can anticipate and successfully identify a very wide range of peripheral devices is no small accomplishment. In my experience, Linux, by shipping with large module libraries and by separating the kernel from the device modules - which, by the way, has always allowed most changes to complete without the need for a reboot - has always been ahead of the game.</p>
<h3 id="Understanding-and-managing-Linux-system-modules"><a href="#Understanding-and-managing-Linux-system-modules" class="headerlink" title="Understanding and managing Linux system modules"></a>Understanding and managing Linux system modules</h3><p>For a device to work with Linux, it must have a defining module that’s been loaded into the kernel. Modules for the vast majority of devices on the market today are already installed on Linux distributions by default. For those modules that aren’t already installed, there are excellent management tools we’ll see in a moment.</p>
<p>First of all, though, you can list all the modules currently loaded into the kernel using lsmod, but Linux installations will usually also keep all kinds of modules that you might one day need under &#x2F;lib&#x2F;modules. For some reason, many systems now no longer support the really handy modprobe -l tool - which listed all modules that were potentially available. Instead, you’ll have to use something like this: where “find” will search the specified directories for files containing the ko extension. Let’s take a look at &#x2F;lib&#x2F;modules so we can better understand what uname -r does. Each of the directories that we can see here is named after a specific Linux kernel release. 3.13.0-49 is, right now, the most recent kernel I’ve got installed on the machine. Wisely, Linux will save older versions of the kernel on the system - along with their device modules - in case something should ever go wrong with an upgrade and we should ever need to take a step back to a previous point in time.</p>
<p>Uname, as you can see, is a Linux command that outputs the current system which, predictably, is Linux. Uname -r will output the current kernel version. Therefore, if you want to use find to display all the available modules associated with our current kernel, you would use uname -r to insert the kernel name into our directory location.</p>
<p>This ability to combine system information with basic commands is part of the power - and the fun - of Linux. So while getting rid of the simple and straighforward modprobe -l function seems to have been just a little bit odd, we’re certainly not left without other perfectly usable options.</p>
<p>Now that we’ve learned how to find modules - both loaded and available - we should also know how to add or remove modules. This can be really useful when you’re working with very new hardware whose drivers haven’t yet made it into standard Linux installations and need to be added manually. It can also save the day when you’re working with hardware that’s so old it’s no longer supported. Some time ago, I found myself working with modules while trying to get a mini-pci WiFi radio working with some development boards running a very minimal Debian installation. Downloading and adding the right module to the kernel worked nicely - although I should warn you that kernel upgrades will ignore the module and you’ll need to do it all over to get your device running again.</p>
<p>So let’s try it out by installing the lp module. Of course, since it happens to be installed already, We’ll have to remove it first. We’ll begin with lsmod and grep, to confirm that lp is currently loaded. Then, as the sudo admin, we’ll run modprobe -r to remove the module. Another round of lsmod should convince us that that worked. And now we’ll reload it…run lsmod once more, and everything is once again just as it should be.</p>
<p>Let’s review. You can use lsmod to list all the modules currently loaded on the kernel. A clever use of the “find” command will list all available modules. Modprobe followed by the module name (without its .ko extention) will load a module and modprobe -r will remove it. By the way, insmod is another command you can use to load a module, and rmmod will, like modprobe -r, remove a module. udev, Linux dynamic device management, is the system tool that, you guessed it, manages devices. You can manually change some udev settings through .rules files kept in one of three directories: Udev will read files from these three directories in that order, with precedence given to the one read earliest. Since &#x2F;run is a pseudo filesystem that’s re-written whenever the computer is rebooted, you can use rules files in &#x2F;run for changes you want to stay in effect only for this session.</p>
<p>Let’s take a look at &#x2F;etc&#x2F;udev on my system. You can see my printer’s scanner rule - which was provided by Brother itself to allow their all-in-one to run on Linux. This rule has the number 40 - meaning it will be executed by udev before the other two rules with higher numbers. If, for some reason, you ever wanted to make sure that this rule was executed later, you could simply change the number in the filename to, say 90.</p>
<p>Let’s use cat to actually read the 70-persistent-net.rules file. The file points to my two network interface cards, identifying their MAC addresses and, significantly, assigning them system designations: eth0 and eth1. If you like, you can edit either of these entries to give them new values, say eth3 or em0. As I discovered some time ago, replacing a failed network card might cause udev to give it a designation that’s not the same as the one used by the original card. That could cause services to break if you have software that’s expecting to find the network through eth0, but that’s now been moved to eth3. Editing this rule can return your system to its original, happy working state.</p>
<p>Since we’ve mentioned default device designation (like eth0), it’s probably a very good time to review the way that Linux assigns names to all its devices. As we’ve seen, network interfaces are given eth names, beginning with eth0 and moving up. Recently, some systems have changed that to em0, em1 and so on.</p>
<p>Hard disk drives (including solid state drives) are usually named sda, sdb, etc. However, individual partitions on a disk might be named sda1, sda2, sda3, etc. Floppy drives - if you can still find any - are usually called fd0 and fd1. And CDRom or DVD drives will usually be designated sr0 and sr1.</p>
<p>Again, all of these designations are controlled and managed by udev, and have symbolic links that you can find in the &#x2F;dev directory. If you’re unsure how, say, Linux has named a USB data drive you’ve just inserted, you can view the dmesg log (by running dmesg from the command prompt) and look for recent entries. In this case, we can easily see the the USB drive that I just plugged in. lsusb will also list all your USB devices and hubs. The drive I just plugged in is mentioned here.</p>
<h3 id="Accessing-environment-data-on-AWS-instances"><a href="#Accessing-environment-data-on-AWS-instances" class="headerlink" title="Accessing environment data on AWS instances"></a>Accessing environment data on AWS instances</h3><p>Most of what we’ve seen regarding system hardware in these first two videos of this course will have little connection to the cloud computing world because managing system hardware on an Amazon Web Services virtual machine is largely the responsibility of our host, Amazon. Still, AWS does give you access to a great deal of system information through Instance Metadata, which you can access from a shell session inside the instance using the curl tool and the special IP address, 169.254.169.254.<br>This request, for instance, will return a list of available metadata categories. Amazon documentation provides plently of guidance for finding and making use of metadata. And don’t worry, we’ll learn much more about using ssh to access AWS instances - and other resources - later in this series.</p>
<h1 id="How-Linux-boots-your-computer"><a href="#How-Linux-boots-your-computer" class="headerlink" title="How Linux boots your computer"></a>How Linux boots your computer</h1><p>Familiarity with the boot process can be hugely important for properly managing your systems - and particularly for troubleshooting when something doesn’t work the way you expect. It just makes sense: a user complains that his PC isn’t booting. But that can mean all kinds of things. If you’re not getting any display on your monitor, it’s likely a hardware problem and you should consider replacing failed components - or maybe just properly plugging in a cable. If you’re stuck at a later stage, the kinds of intervention you’ll require to correct a bootloader configuration issue will be quite different from what you might do to fix a corrupted filesystem. But if you have no idea how the system is supposed to work, how will you ever know the difference?</p>
<p>This video is meant to cover the basic process of booting a computer running Linux, introducing you to your firmware (BIOS) bootloader (GRUB), and filesystem. We’ll go into greater detail for each of these in future videos, both in this course, and in future courses in this series.</p>
<h3 id="Understanding-the-Linux-boot-process"><a href="#Understanding-the-Linux-boot-process" class="headerlink" title="Understanding the Linux boot process"></a>Understanding the Linux boot process</h3><p>For our purposes, the very first thing that happens when you push your computer’s power button is that your motherboard’s build-in firmware BIOS (Basic Input&#x2F;Output System) takes an inventory of all the hardware that’s currently running on the computer, identifies which drive contains the boot software, and loads the Master Boot Record (MBR) of that drive. The MBR makes up only the first 512 bytes of its drive and uses that space to describe the disk partitions and how to start loading the operating system.</p>
<p>When you originally installed Linux on your computer, the installation process automatically created all necessary partition tables, along with the MBR.</p>
<p>BIOS (or UEFI) will execute GRUB (the GRand Unified Bootloader) Stage 1 from the MBR, which reads the details of the boot drive filesystem and launches either GRUB stage 1.5 (which would itself load GRUB stage 2) or GRUB stage 2 directly. GRUB stage 2 loads into memory, presents the user with menu choices if so configured, and executes the kernel and loads either the initrd or initramfs images.<br>The kernel will mount the root filesystem specified by GRUB and execute the &#x2F;sbin&#x2F;init program (giving init the process ID (PID) of 1). The kernel will use initramfs (or initrd) until the real filesystem is fully booted.</p>
<p>I should mention that SysVinit manager’s &#x2F;sbin&#x2F;init file continues to exist even on distributions using the newer Upstart or systemd process managers although, at least in the case of systemd, only as a symlink to systemd itself.</p>
<p>Once the operating system has been mounted and loaded into memory, the environment run level specified by boot parameter is executed. Linux allows you to choose between seven - well, actually six - running modes: run level 0 - which instructs the machine to shutdown - could hardly be called a “running” mode.</p>
<p>Each run level is defined by files found within appropriately named directories, like &#x2F;etc&#x2F;rc0.d - which obviously controls run level 0, or &#x2F;etc&#x2F;rc1.d - which controls run level 1. We’re going to discuss run levels and how to manage them in a coming video. As far as we’re concerned right now, it’s your startup configuration that determines the run level you’ll eventually be given once the boot process is complete.</p>
<p>You probably noticed that I mentioned UEFI - Intel’s Unified Extended Firmware Interface - just before. While you don’t have to know anything about this for the LPIC exam, at some point, you’ll probably run into machines built with UEFI - a replacement for BIOS. Because of incompatibilities between UEFI and BIOS, many PCs shipped with more recent Windows releases will not make it easy for you to load Linux without first destroying the Windows install. While that has caused many serious headaches, I can say that the developers for a growing number of Linux distributions have adapted nicely, and configuring a dual-boot, Linux-Windows computer is becoming easier all the time.</p>
<h1 id="Managing-and-troubleshooting-the-boot-process"><a href="#Managing-and-troubleshooting-the-boot-process" class="headerlink" title="Managing and troubleshooting the boot process"></a>Managing and troubleshooting the boot process</h1><p>In the previous video we explored the Linux start up process and the roles played by various resources. In particular, we mentioned the contributions of the GRUB bootloader and the kernel. Now we’re going to learn how to manage the process as it moves through the GRUB stage, and also how to listen to the kernel when it tries to tell us about problems it encounters along its journey.</p>
<h3 id="Understanding-and-managing-the-GRUB-bootloader"><a href="#Understanding-and-managing-the-GRUB-bootloader" class="headerlink" title="Understanding and managing the GRUB bootloader"></a>Understanding and managing the GRUB bootloader</h3><p>Depending on your boot settings, the GRUB menu may or may not be visible for a few seconds as your computer starts up. If it’s not visible, you can manually cause it to display by repeatedly clicking on the right Shift key during the early boot stages. If that doesn’t work, then you can always manually abort the startup while it’s in progress (by pressing the power button for five seconds) and the GRUB menu should appear the next time you boot.</p>
<p>Once you’re in, you will be presented with a number of boot options. The default will be a full, normal boot of the latest version of your main operating system. There might also be alternate operating systems and a couple of memory tests - for troubleshooting RAM. </p>
<p>Pressing the “e” key will allow you to edit the boot parameters of your default option. We’ll come back to that in a minute. Highlighting the Advanced Options item with your arrow key and then pressing enter will reveal a number of further options, including booting to older kernels - something that can be especially useful if a recent upgrade has gone wrong. These kernel files, by the way, actually live in the &#x2F;boot directory.</p>
<p>There are also recovery mode options for each kernel version that provides a menu of troubleshooting tools to rescue a damaged installation. Pressing “c” or ctrl-c, by the way, will give a limited command line interface.</p>
<p>But let’s get back to the main GRUB menu and press “e” for edit. Feel free to edit any of the parameters you see here. Just remember: anything you do may well render your computer unbootable. Fortunately, as Linux system administrators, you can easily access the tools you’ll need to edit yourself out of any trouble you’ve caused.</p>
<p>Here are a couple of examples. Look at the Linux line towards the bottom. The vmlinuz-3.13 value points to the latest Linux kernel available on the system. The value of root is the identifier for my hard drive. Either of these values can be changed.</p>
<p>You could also add a line like rw init&#x3D;&#x2F;bin&#x2F;bash to boot into a root shell session that will allow you full write access to the whole filesystem. This is one way to restore key system files and recover from a non-bootable condition. If you’re uncomfortable leaving your computer open to such access, you might consider protecting it with a BIOS password.</p>
<p>To review, you can force access to the GRUB boot menu by hitting the right shift repeatedly during start up. “e” will send you to a simple text editor where you can edit the boot parameters of a particular GRUB item. “c” will send you to a limited command line (from which esc will send you back to the main GRUB menu). And selecting a recovery mode option from the Advanced Options menu will provide a list of utilities suited for recovering from some system problem.</p>
<h3 id="Diagnosing-boot-problems-in-Linux"><a href="#Diagnosing-boot-problems-in-Linux" class="headerlink" title="Diagnosing boot problems in Linux"></a>Diagnosing boot problems in Linux</h3><p>If something does fail to load properly during the boot process, you’ve got lots of good options. I’m not sure if there’s anything that happens on a Linux system that doesn’t leave behind some kind of record in the logs - most of which live happily in &#x2F;var&#x2F;log. Now I can hear some of you complaining that if the system won’t properly boot, then what good are inaccessible log files? To which the answer is: but they ARE accessible. You will often be able to get in using one of the recovery mode options we described before. But, assuming you didn’t encrypt your filesystem when you installed it, you can also boot your PC to a Live Linux session using a DVD or USB stick containing ISO images of any of the popular Linux distributions, or using a super lightweight purpose-built utility distribution like SystemRescueCD or PuppyLinux. Once that’s running, you can mount your main hard drive and set about exploring the log files to find out what went wrong, and then repairing your broken files.</p>
<p>The logs that most closely relate to boot problems are kern.log, boot.log, and dmesg. The problem with any of these is that a single boot will generate so many hundreds of lines of messages that they become nearly unreadable. So now’s a great time to properly introduce you to your new best friend, grep. </p>
<p>Let’s output the contents of the dmesg logfile using cat, but rather than writing directly to the screen as we normally would, we’ll pipe it using the pipe character (which you get by pressing shift and backslash together) and then type grep followed by the string we’d like to search for. Let’s say that we were alarmed during boot by a serious-sounding warning message about power&#x2F;level that appeared during boot. We can simply enclose the entire string in quotation marks and see what comes up.</p>
<p>That’s a whole lot easier to read! But it might not provide enough context to help. So we could also simply open dmesg within a text processing tool like “less” and then search for our string to see it in its full context. We can enter a search string by pressing the forward slash key, and then type in our string. We now have the warning’s full context. In this particular case that might not be all that helpful, but I’m sure you can see how it could.</p>
<p>If the dmesg, kern, and boot log files proved unhelpful, you should next turn to your favorite Internet search engine or perhaps the user forum associated with your Linux distribution…or even the ServerFault.com user forum. I am constantly amazed at how much genuinely useful information is readily available online.</p>
<p>While the range of hardware issues that should concern system administrators of virtual machines like Amazon EC2 instances is narrower than for physical deployments, they still matter. An Ubuntu virtual machine on Amazon Web Services will, by default, include most of the regular log files like boot.log, dmesg, and kern.log. However, AWS provides a more direct method for troubleshooting system issues. In the Instances dashboard of the AWS console, right click on the instance that concerns you, select Instance Settings, and then Get System Log. When you find a suspicious-looking entry, you can copy it and go to AWS’s “Troubleshooting Instances with Failed Status Checks” to see if there’s anything helpful there. Naturally you can also draw on all your usual troubleshooting resources in your search for a solution.</p>
<h1 id="Linux-run-levels-and-boot-targets"><a href="#Linux-run-levels-and-boot-targets" class="headerlink" title="Linux run levels and boot targets"></a>Linux run levels and boot targets</h1><p>A few videos back, when we were talking about managing the boot process, we briefly explored the history of Linux process management through init (SysVinit), Ubuntu’s Upstart, and the now dominant systemd. In this video, we’re going to see all three in action through the way they control system runlevels.</p>
<h3 id="Controlling-Linux-system-runlevels"><a href="#Controlling-Linux-system-runlevels" class="headerlink" title="Controlling Linux system runlevels"></a>Controlling Linux system runlevels</h3><p>A runlevel defines the operating system resources and access that are made available at a given time. Runlevel 0 will remove all access by shutting down the system altogether, while runlevel 3 will provide full access to all resources - depending of authorization, of course.</p>
<p>In the old init system, the default system runlevel was defined by an entry in the &#x2F;etc&#x2F;inittab file. The first line in this snippet - id:3:initdefault - sets the runlevel to 3.</p>
<p>Most Linux distributions no longer rely on the inittab file - something that’s a source of great confusion considering the wealth of older documentation that can still be found around the Internet and in book stores. Ubuntu’s Upstart hides its default runlevel definition in the rc-sysinit.conf file within the &#x2F;etc&#x2F;init&#x2F; directory. The entry that interests us will look something like env DEFAULT_RUNLEVEL&#x3D;2.</p>
<p>Upstart also uses scripts that, when run, control the behavior of specific services. Scripts compatible with older SysVinit processes can be found in &#x2F;etc&#x2F;init.d, while scripts used by Upstart processes are found in &#x2F;etc&#x2F;init&#x2F;</p>
<h3 id="How-systemd-controls-the-Linux-environment"><a href="#How-systemd-controls-the-Linux-environment" class="headerlink" title="How systemd controls the Linux environment"></a>How systemd controls the Linux environment</h3><p>While you have to know all these details for the LPIC exam - and in case you’re called on to work with older systems, as we’ve already meantioned, the clock is ticking on the relevance of init and Upstart, as the world is moving to systemd.</p>
<p>So let’s talk about systemd. For this shell session, we’ve left Ubuntu behind and are now enjoying Fedora - the earliest major distribution to adopt systemd. We’ll begin by noting that systemd itself doesn’t focus directly on overall system runlevels, but on the status of specific processes. Systemd works with files called Units, each of which describes in plain text a service, device, socket, or mountpoint.</p>
<p>We can manually manage units in the &#x2F;etc&#x2F;systemd&#x2F;system directory, while packages install their unit files in &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system. As an example, let’s take a look at &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;tmp.mount which points to an available temporary directory created as a virtual tmpfs where other processes can safely dump their short-term data. The location of this directory is &#x2F;tmp - a common directory that most Linux processes expect to be able to access.</p>
<p>Systemd comes with a couple of really useful tools to help diagnose boot problems: systemd-analyze tells you how long - in seconds - each part of the startup process took. You can obviously use this to narrow down your search for whatever it might be that’s taking more time than it should. Systemd-analyze blame will show you now long - in miliseconds - each specific unit - whether a service, mount, or socket - took to load.</p>
<p>However, the command line utility that does most of the heavy lifting for systemd is systemctl. To give you an idea how this works, we’ll play with just a few examples here. systemctl list-units will, as you might expect, list key information about all the units that are currently running. Since the list is quite long, you might want to narrow down your search using your old friend grep:</p>
<p>Let’s try activating a service. We’ll use the Apache webserver, known in the Fedora world as httpd. Since Apache is a service, we’ll identify it by appending .service to its name. First, let’s see if it’s running already. It’s not. Next we’ll use the enable command to instruct systemd to run Apache each time the system boots. But that won’t get it going until we actually restart the system. If we just can’t wait, we’ll use start to launch Apache right away. And just to confirm that it is actually functional, we’ll run list-units once more, again grepping for httpd</p>
<p>It looks good.</p>
<p>That’s pretty much what you’ll need to know about systemd for the LPIC exam. Let’s review. systemd manages services, mountpoints, devices, and sockets through text files kept in either &#x2F;etc&#x2F;systemd&#x2F;system or &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;. systemd-analyze will display each part of the startup process took, and systemd-analyze blame breaks that down further to the service level. systemctl list-units will display all running processes, systemctl enable will force a process to load at start-up, systemctl start will start a specified process immediately and, while we’re on the topic, systemctl stop will stop a specified service.</p>
<p>Now we can return to Ubuntu to describe the ways you can control system wide runlevels that are universal to all process managers. In a previous video we showed how, by pressing “e” from the GRUB menu, we would be able to edit the boot parameters. So, to illustrate, by adding the word “single” to the end of the kernel command line, Linux will start up in single-user mode rather than multi-user. Single-user mode is often helpful for emergency recovery efforts. You can achieve the same result from the command line of a running session with telinit 1 - for which you’ll need admin powers. You can power down the system using telinit 0, or sudo shutdown -h now - where “h” stands for “halt” and “now” tells Linux to close up shop immediately.</p>
<p>The only runlevel-related topic left to cover is communications. It’s all very nice to have the power to control processes and to shutdown powerful servers, but you’re not going to make a lot of friends if you don’t warn all the other users who happen to be logged in from their own terminal sessions and doing their work.</p>
<p>Assuming that all logged in users are actually looking at their terminals when you run a telinit or shutdown command - and, considering how many windows most people can have open at a given time, that’s not necessarily a particularly strong assumption - Linux will automatically send a warning for you.</p>
<p>I’ll demonstrate how this works while logged in to two separate user accounts on a single LXC container. The larger window in the background is logged in as the normal user Tony, and the smaller window in the foreground is an admin user. This way we can simultaneously watch the terminals of both. As admin, I’ll run sudo shutdown -h 2 …which will halt the system in two minutes. Notice how the shutdown message is displayed for both accounts. Because I don’t really want to shutdown in two mintues, I’ll hit ctrl-c to cancel the action.</p>
<p>Now, besides messages that will automatically accompany shutdown (or telinit) commands, Linux also provides “wall” to permit communication between users. In truth, while the LPIC exam is mostly interested in wall as a tool to keep your users up to date on your nefarious plans to change their work environment, it can also be used for more general communications. I’ve created a text file that I will read using cat but, instead of displaying it to the screen, I’ll pipe it to wall, which will send it to the screen of every user currently logged in. I guess you can think of “wall” as kind of a prehistoric social networking tool.</p>
<p>To review, adding an argument (like single) to the kernel command line in GRUB will control its startup mode. telinit and shutdown offer similar control from the command line of a running machine. You can schedule a shutdown or reboot using something like sudo shutdown -r 5 - which will cause a reboot in five minutes. And piping a text file to wall will display text to all logged in users.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/21/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/21/">21</a><span class="page-number current">22</span><a class="page-number" href="/page/23/">23</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/23/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
