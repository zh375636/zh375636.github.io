<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/110/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/110/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Azure-Key-Vault-and-Disk-Encryption-29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Azure-Key-Vault-and-Disk-Encryption-29/" class="post-title-link" itemprop="url">AZ-204-Lab-Azure-Key-Vault-and-Disk-Encryption-29</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:44:00 / Modified: 11:44:02" itemprop="dateCreated datePublished" datetime="2022-11-14T11:44:00-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Azure-Key-Vault-and-Disk-Encryption-29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Azure-Key-Vault-and-Disk-Encryption-29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Designing-for-Azure-Identity-Management-28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Designing-for-Azure-Identity-Management-28/" class="post-title-link" itemprop="url">AZ-204-Designing-for-Azure-Identity-Management-28</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:30:51" itemprop="dateCreated datePublished" datetime="2022-11-14T11:30:51-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 09:31:52" itemprop="dateModified" datetime="2022-11-15T09:31:52-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Designing-for-Azure-Identity-Management-28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Designing-for-Azure-Identity-Management-28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to Designing for Azure Identity Management. My name is Thomas Mitchell and I’ll be taking you through this course on the key technologies to consider when designing an identity management solution. I’m an Azure Content Author at Cloud Academy and I have over 25 years of deep IT experience, several of those with cloud technologies. If you have any questions, feel free to connect with me on LinkedIn, or send an email to <a href="mailto:&#115;&#117;&#x70;&#112;&#x6f;&#114;&#x74;&#64;&#x63;&#x6c;&#x6f;&#117;&#100;&#x61;&#99;&#x61;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;">&#115;&#117;&#x70;&#112;&#x6f;&#114;&#x74;&#64;&#x63;&#x6c;&#x6f;&#117;&#100;&#x61;&#99;&#x61;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;</a>. This course is intended for IT professionals who are interested in earning Azure certification, those who wish to become Azure architects, and those who are tasked with designing an Azure identity management solution. To get the most from this course, you should have at least a moderate understanding of Microsoft Azure, and of identity management concepts. We’ll kick off the course by discussing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure Active Directory</a> and what it offers. We will then talk a bit about hybrid identities and what purpose they serve. Next, we’ll cover Azure AD Domain Services and how to deploy this feature. </p>
<p>After covering Azure AD Domain Services, we’ll get into <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/single-sign-on-overview/">single sign-on</a>, and how to configure it for a web application. Azure MFA or multi-factor authentication is another cool feature that we will discuss. We’ll cover what it is, what it offers, and how to enable it. After covering MFA, we will get into Azure AD B2B and Azure AD B2C. These are Azure AD Business to Business and Azure AD Business to Consumer. You will learn what they are, what they offer, and how to deploy them. We’ll then move into Privileged Identity Management, otherwise known as PIM. Rounding out the course are topics on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/self-service-password-reset/">self-service password reset</a>, self-service group management, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/overview-of-managed-identities-for-azure-resources/">managed identities for Azure resources</a>. There are demonstrations sprinkled throughout the course. These demos will show you how to deploy and use the technologies covered. By the end of this course, you should have a good understanding of all key technologies and features as they relate to designing for Azure identity management. You should also be able to deploy each feature with an understanding of what it offers and how it fits into any identity management solution. We’d love to get your feedback on this course, so please give it a rating when you’re finished. If you’re ready to learn about Designing for Azure Identity Management, let’s get started.</p>
<h1 id="Introduction-to-Azure-AD"><a href="#Introduction-to-Azure-AD" class="headerlink" title="Introduction to Azure AD"></a>Introduction to Azure AD</h1><p>Microsoft’s Azure Active Directory is a cloud-based identity and access management service. With it, users can sign in and access external resources such as Office 365, the Azure portal, and other software as a service applications. Azure AD, of course, also allows users to access internal resources as well. Such resources include applications inside the corporate network and on the internet along with cloud applications that have been developed and deployed by your organization. Azure AD is used to control access to applications and resources according to business requirements. For example, Azure AD can be configured to require multi-factor authentication or MFA when a user needs access to important company resources. In addition, Azure AD can be used to automate user provisioning between an existing on-prem Windows server AD and corporate cloud applications like Office 365. With Azure AD, organizations have access to tools that can used to automatically help protect user identities and credentials which allows them to meet access governance requirements. Microsoft Online services like Office 365 and Microsoft Azure leverage Azure AD for sign-in and for identity protection. As such, an organization that subscribes to any of the Microsoft Online business services automatically gets at least the free version of Azure AD along with those services. </p>
<p>Adding paid services to a tenant can enhance an Azure AD implementation. Such paid services include Azure Active Directory Basic, Azure Active Directory Premium 1, and Azure Active Directory Premium 2. These Azure AD paid licenses ride on top of an existing free directory, and they can provide additional services such as self-service, security reporting, enhanced monitoring, and secure access for mobile users. One thing to note, however, is that while all of these added features can add cool functionality and security, Azure Active Directory Basic, Azure Active Directory Premium 1, and Azure Active Directory Premium 2 are not currently supported in China. The free version of Azure Active Directory offers basic user and group management functionality and on-prem directory synchronization. It also offers basic reporting and single sign-on or SSO across Office 365, Microsoft Azure, and many popular SaaS applications. In addition to the free features available in the Azure AD Free version, Azure AD Basic provides cloud-centric application access as well as group-based access management. Other features included with the Basic version include self-service password reset for cloud apps and Azure AD Application Proxy which is a feature that allows you to publish on-prem web applications using Azure AD. Azure Active Directory Premium P1 offers quite a bit more than either Free or Basic. In addition to what those versions offer, Azure AD Premium P1 offers hybrid users access to both on-prem and cloud resources. Premium P1 also supports advanced administration tasks like self-service group management, dynamic groups, and it even integrates with Microsoft Identity Manager or MIM. Microsoft Identity Manager is an advanced, on-prem identity and access management solution. </p>
<p>Azure AD Premium P1 also offers cloud write-back capabilities which are used to allow self-service password reset for your on-prem users. Azure Active Directory Premium P2 builds upon what is offered in the P1 edition by offering everything included in the Free, Basic, and P1 versions plus Azure Active Directory Identity Protection which helps provide risk-based conditional access to applications and critical company data. Azure AD Premium P2 also offers Privileged Identity Management or PIM which is useful for discovering, restricting, and monitoring administrators as well as their access to corporate resources. Privileged Identity Management also provides just-in-time access when it’s needed, meaning access to resources can be limited to only those times when it’s required and then be taken away automatically when the access is no longer needed. Other pay-as-you-go feature licenses such as Azure AD Business-to-Consumer are also available to assist with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">identity management</a>. Azure AD B2C as it’s called helps provide identity and access management solutions for customer-facing applications.</p>
<h1 id="Create-Directory-and-Add-Custom-Domain"><a href="#Create-Directory-and-Add-Custom-Domain" class="headerlink" title="Create Directory and Add Custom Domain"></a>Create Directory and Add Custom Domain</h1><p>To create a new directory and add a custom domain for it, log into the Azure portal. While in the portal, open <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure Active Directory</a> and click on the create a directory link. Provide an Organization name along with the domain to use for the directory. Select a country or region and then click create. Once the directory has been provisioned, click the link to manage the new directory and sign in if required to do so. From within Azure Active Directory click custom domain names and then add your custom domain name. Provide and internet routable custom domain name and then click add domain. </p>
<p>At this point you’re provided with the option to verify the domain via either a text record or an MX record. My preference is to typically use the text record. Copy the record value and then visit your domain’s DNS management console. Create a text record and supply the value that you were provided. Save your new text record and switch back over to Azure. Click the verify button to verify the domain using the DNS record that you’ve just created. After verifying the domain, set its primary. And then click yes to confirm.</p>
<h1 id="Azure-AD-Domain-Services-Overview"><a href="#Azure-AD-Domain-Services-Overview" class="headerlink" title="Azure AD Domain Services Overview"></a>Azure AD Domain Services Overview</h1><p>Azure AD Domain Services is a Microsoft cloud-based offering that provides managed domain services, such as domain join, group policy, LDAP, and Kerberos and NTLM authentication. The domain services provided are fully compatible with traditional on-prem Active Directory and they can be deployed without any need for deployment or management of domain controllers in the cloud. Azure AD Domain Services integrates with the existing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure AD</a> tenant and makes it possible for users to log in with their corporate credentials. Existing user accounts and groups can be leveraged to secure access to resources. As such, this offers a smoother transition of on-prem resources to Microsoft Azure. Companies leveraging a hybrid IT infrastructure will typically synchronize their <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">identity</a> information from their on-prem directories to the Azure AD tenant. As more of their on-prem applications are migrated to Azure, Azure AD Domain Services can become more useful. An important caveat to consider when deciding if Azure AD Domain services are the right solution is that password sync is mandatory for a hybrid organization to use Azure AD Domain Services. </p>
<p>This is required because users’ credentials are needed for authentication via NTLM and Kerberos in the managed domain that is provided by Azure AD Domain Services. When considering Azure AD Domain Services, keep in mind that the managed domain is actually a stand-alone domain. It is not, and I repeat, it is not an extension of the on-prem AD domain. This is a common misconception among IT professionals. Also, because the domain is managed, the IT administrator does not need to, nor can he, manage, patch, or monitor the domain controllers for the managed domain. Likewise, there is no need to manage or even monitor AD replication within the managed domain. Quite frankly, there really isn’t much for the administrator to do in the managed domain, especially since the administrator doesn’t even have Domain Admin or Enterprise Admin privileges on the managed domain.</p>
<h1 id="Deploy-Azure-AD-Domain-Services"><a href="#Deploy-Azure-AD-Domain-Services" class="headerlink" title="Deploy Azure AD Domain Services"></a>Deploy Azure AD Domain Services</h1><p>To deploy <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/azure-ad-domain-services-overview/">Azure AD Domain Services</a> log into the Azure portal. Once in the portal click on Create a Resource and then search for Domain Services. Select Azure AD Domain Services from the search results and then click Create. Provide the basic information for the domain, including the DNS name for the domain as well as the Subscription you are deploying to, along with a Resource group. You’ll also need to supply a Location. Click Okay to move onto the next step. At this point you’re prompted to create a Virtual network for the managed domain that you are setting up. Provide a name for the network along with an address space that works for your environment. You will also need to define a Subnet and a Subnet address range that you’d like to use for the managed domain. </p>
<p>The managed domain controllers and associated infrastructure that gets deployed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">Azure</a> as part of the Manage Domain setup will reside on this network and on this subnet. Click Okay and then Okay again. Next you are advised that the AAD DC Administrators group has been created. This group is used to manage the domain. It’s similar to but not quite like Domain Admins in an On-Prem Active Directory Domain. Click on the group to add members to it. When you’re done adding members that need to manage the domain go back and click Okay. This synchronization step has nothing to do with Azure AD Connect or with On-Prem AD if one exists. Instead, this synchronization step is meant to sync users and groups from Azure AD into Domain Services. You have a choice of syncing all users or scoping the sync to just certain groups. In this demonstration I’m just going to sync all users and groups from Azure AD to Domain Services. Clicking Okay takes you to the next step where you can review your options. Click Okay to commence the deployment of Azure AD Domain Services. </p>
<p>The actual deployment can take the better part of an hour because it’s deploying lots of stuff in the background. It’s deploying the network infrastructure that you’ve defined as well as two managed domain controllers. It’s also setting up the managed domain itself. After enabling Azure Active Directory Domain Services you need to enable computers within the Virtual Network to connect to and consume these services. To do so you need to update the DNS Server Settings for your Virtual Network so that it points to the two IP addresses where Azure Active Directory Domain Services are available. To update the DNS Server settings for the Virtual Network hosting your Azure Active Directory Domain Services browse to the Azure portal and open up your instance of Azure AD Domain Services. The Overview tab will list a set of required configuration steps. One of those is to update the DNS Server settings for the Virtual Network. You will be presented with two different IP addresses. These are the IP addresses where Azure AD Domain Services is available. These are essentially the managed domain controllers that were stood up as part of the deployment of Azure AD Domain Services. After making a note of these IP addresses, click the Configure button and update the DNS Server settings for the Virtual Network. With Azure AD Domain Services provisioned and your DNS updated you can now move on and enable password hash synchronization using Azure AD Connect.</p>
<h1 id="A-Word-About-Personas"><a href="#A-Word-About-Personas" class="headerlink" title="A Word About Personas"></a>A Word About Personas</h1><p>Personas can be an important tool that drives adoption and business value realization. What happens quite often is that organizations deploy technology solutions without a full understanding of the users that those solutions are intended for. When this happens, the deployed solutions do not get used, and the value of those solutions goes unrealized.</p>
<p>If an organization deploys an Azure solution, but nobody uses it, the organization doesn’t realize the value – and it becomes a waste. In order to generate value from an Azure solution, an organization must determine what users should START doing or STOP doing, in order to realize the value of the solution.</p>
<p>Essentially, its user behavior change that becomes the yardstick for evaluation, adoption, and use of solutions deployed in Azure. Personas can be useful when trying to drive adoption of new Azure solutions.</p>
<p>A persona is really just a fictitious character that represents a certain user type. Personas refer to the “who” when discussing an organization. The use of personas helps organizations characterize different sets of users, and to capture details about what a typical day looks like to those users. Personas can be used to capture the pains, needs, and desired outcomes that users have as they perform their daily tasks. </p>
<p>To build personas that reflect the workforce, organizations need to communicate with end users. Organizations need to know how work currently gets done in order to deploy Azure solutions that help users get that work done. Defining personas for each user type can help organizations accomplish this. More specifically, from an Azure perspective, an organization can build personas to assist with the creation of roles. By defining different personas, the organization has better visibility into what roles are necessary – and if custom roles are needed.</p>
<p>As a result, organizations can realize more value when deploying Azure solutions, because the solutions that are deployed can then be tailored to the users for whom the solutions are intended. Management of those solutions can be streamlined via built-roles that match the defined personas, or via custom roles, when the built-in roles are not sufficient.</p>
<h1 id="Hybrid-Identities"><a href="#Hybrid-Identities" class="headerlink" title="Hybrid Identities"></a>Hybrid Identities</h1><p>As the move to the cloud gathers steam, corporations are finding themselves supporting a mixture of on-prem and cloud applications. Users obviously are finding themselves requiring access to those applications as well. This, of course, becomes a challenge to implement. The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">identity</a> solutions we previously discussed are solutions that span on-prem and cloud-based capabilities. Leveraging these solutions allows an organization to create a common user identity for authentication and authorization to all resources, regardless of whether they are on-prem or in the cloud. This concept is called hybrid identity. Achieving hybrid identity requires the development of one of three authentication methods. The authentication method that is deployed is dependent on the specific scenario being addressed. The three authentication methods include Password Hash Synchronization, Pass-Through Authentication, and Federation. Password Hash Synchronization is also referred to as PHS, while Pass-Through Authentication is referred to as PTA. Federation is referred to as, well, Federation. Password hash synchronization is a sign-in method used as part of a hybrid identity solution. This is accomplished with Azure AD Connect by synchronizing a hash, of the hash, of a user’s on-prem AD password to a cloud-based <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure AD</a> instance. This feature is useful for signing in to Azure AD services like Office 365 with the same password as an on-prem AD account, which reduces end-user impact. The password hash synchronization strategy reduces, to just one, the number of passwords that an organization’s users need to maintain. As such, password hash synchronization can improve user productivity and reduce helpdesk costs. Password hash synchronization, which is the most common hybrid identity solution, requires an organization to install Azure AD Connect. </p>
<p>Once Azure AD Connect is installed, directory synchronization between the on-prem Active Directory instance and the Azure Active Directory instance is configured. As part of the synchronization configuration, password hash synchronization is enabled. Azure AD Pass-through Authentication, or PTA, much like Password Hash Synchronization, allows users to sign in to both on-prem and cloud-based apps with the same password. And much like password hash synchronization, this option offers a better end user experience. However, pass-through authentication validates user passwords directly against the on-prem Active Directory, instead of using a synced password hash. A key benefit of pass-through authentication over password hash synchronization is that it affords organizations the ability to enforce their on-prem AD security and password policies, since pass-through authentication is actually leveraging the on-prem credentials. By combining Pass-through Authentication with Seamless Single Sign-On, organizations can allow users to access applications on corporate machines inside the network without the need to type in their passwords again. Azure AD Pass-through Authentication provides an improved end-user experience because it offers end users the ability to complete self-service password management tasks in the cloud. Deployment and administration are easy because Pass-Through Authentication only requires a lightweight agent to be installed on-prem. Since the agent automatically receives updates, there is no management overhead. Pass-Through Authentication offers improved security over Password Hash Synchronization because on-prem passwords are never stored in the cloud. </p>
<p>Because it works with Azure AD Conditional Access policies, including MFA, Pass-Through Authentication offers additional account protection. Another benefit of Pass-Through Authentication is the fact that the agent only makes outbound connections from the network, removing all requirements for a DMZ as part of a solution. Communication between the on-prem agent and Azure AD is secured via cert-based authentication, which adds another layer of security. Further, the certificates that are used are automatically renewed every few months by Azure AD, removing the requirement to manually maintain them. In addition to high security, Azure AD Pass-Through Authentication offers high availability by allowing the installation of additional agents on multiple on-prem servers. Federation is a bit different from the other two solutions. It is a collection of domains with an established trust, which typically includes authentication, and almost always includes authorization. In a common configuration, a federation might include multiple organizations that have established trust for shared access to a specific set of resources. Federating an on-prem environment with Azure AD allows and organization to use the federation for authentication and authorization. Federation ensures that all user authentication happens on-prem and it provides administrators with the ability to implement more rigorous levels of access control. Federation with ADFS and PingFederate is available. To protect against a failure of the ADFS infrastructure when using federation with ADFS, organizations can set up password hash synchronization, or PHS, as a backup. Doing so allows authentication to continue, despite an ADFS infrastructure failure. All three of these authentication methods including PHS, PTA, and Federation provide single-sign on capabilities, which automatically signs users in when they are on their corporate devices inside the corporate network.</p>
<h1 id="DEMO-Managing-Users-Groups-and-Devices-in-Azure-AD"><a href="#DEMO-Managing-Users-Groups-and-Devices-in-Azure-AD" class="headerlink" title="DEMO: Managing Users, Groups, and Devices in Azure AD"></a>DEMO: Managing Users, Groups, and Devices in Azure AD</h1><p>Hello and welcome back! In this brief demonstration, I just wanted to walk you through the process of managing users and groups within Azure Active Directory. Now on the screen here, you can see I’m logged into my Azure Portal for the directory called Test9878.org. This is the custom domain I’m using for my Azure AD here.</p>
<p>Now to get to this screen from Azure, what I’ll do here is I’ll bounce back out to my homepage and here’s the homepage. To get into Azure AD, I can simply select Azure Active Directory from the top here, or I can go to the hamburger and select Azure Active Directory.</p>
<p>So from this overview page, I can browse to lots of different pieces of Azure AD. Under the manage section is where I’ll manage my users and groups along with devices, app registrations, all of this fun stuff where you’ll do your day to day management of your Azure Active Directory. Down the bottom is where you’ll perform your monitoring. And then down at the very bottom, you’ll do your troubleshooting and support.</p>
<p>So from this page, let’s go ahead and create a user in Azure Active Directory. And to do that, it’s pretty straight forward. We simply select users here. And from this screen, we can see all of the existing users in our AD.</p>
<p>We can see, we have two accounts here. One is an admin in the actual Azure Active Directory as shown here under source. The ThomasMitchell.net account is actually an external Azure Active Directory account from another directory.</p>
<p>What we’re going to do here is create a new user and we’ll just call this Dave. And in this dropdown we can select the actual domain name we want to use, we’re using the custom domain, so we’ll leave it at Test9878.org and we’ll give our username. And then we have the option to auto-generate a password or create one, we’ll leave the auto-generate here. And this will show us what the password is.</p>
<p>We can then provide that password to the new user. I would not send this out via any kind of electronic methods when possible. That’s obviously a security risk. As we create our user, we can then select a group to add our user to. So we’ll go ahead and make him a user within our box users group.</p>
<p>This is actually a group I created earlier for some other demonstration. So we’ll go ahead and we’ll select him. So now what we’re doing here is creating our <a href="mailto:&#x44;&#x61;&#118;&#x65;&#x40;&#84;&#x65;&#115;&#x74;&#57;&#56;&#x37;&#56;&#46;&#111;&#x72;&#x67;">&#x44;&#x61;&#118;&#x65;&#x40;&#84;&#x65;&#115;&#x74;&#57;&#56;&#x37;&#56;&#46;&#111;&#x72;&#x67;</a>, we’re auto generating a password and we’re placing this new user in the box users group.</p>
<p>The block sign in is pretty self explanatory. We can either block sign in by selecting yes or allow sign-ins by leaving the no option selected here. And then we have some additional info we can add. We can add a job title in a department.</p>
<p>We can also specify the usage location. So we’ll go ahead and we’ll select United States down here. And we’ll go ahead and create the user. And at this point, we now have a new user in our directory. If we select our user from our list, we can then look at the user’s profile, which includes his identity, his job information, any kind of special settings, contact info. This is where we manage this information as far as the profile goes.</p>
<p>We can then go into assigned roles where we can actually look at any roles that have been assigned to this user. And we can add assignments. You can see all of the different directory roles here that are included by default.</p>
<p>Now we can also see, we can assign custom roles, but if you look at the little icon up here, it tells us that if we want to assign custom roles to a user, the organization needs Azure AD Premium P1 or P2. I’m using the free Azure edition right now. But if he wanted to make, you know, Dave an application administrator, we simply check the box and add the role. And now we can see under assigned roles for Dave, we have application administrator, and of course the description tells us what an application administrator can do.</p>
<p>If we want to remove this assignment for this user, we select the assignment and remove it. Now, if we go back out to our directory, let’s take a look at groups and how we can manage groups here. By selecting groups, we can look at what groups are currently defined within our Azure AD.</p>
<p>If we want to create a new group, we simply select a new group and we can choose whether it’s a security group or an office 365 group. We’ll leave this at security and we’ll just call this marketing. And we’ll give it a description. We can see, we currently have no owners or members defined for this group.</p>
<p>So we’ll go ahead and select an owner. We’ll just make myself an owner here. And then we’ll add a new member. Let’s go ahead and add Dave. And then we create it. So now we have a new group called marketing. It’s a security group and the membership type here is assigned, which means we’re going to manually add and remove users from this group. So we’ll go ahead and select marketing here. And from here, we can look at the different properties of this group.</p>
<p>We can see when it was created, the membership type, whether it’s a cloud sourced group and what type of group it is. We can see the different direct members and any kind of group memberships. We look at properties here. We can see here, we have the name and we can actually change it here. And the description, and then members and owners will tell us who the members are and who the owners are.</p>
<p>If we select group memberships here, we can see that the marketing group is not a member of any other groups. So this is where you could see group nesting. Now in this applications area, we can see what applications are assigned to our group. So if we have a group of users that needs access to a specific application, we can create that group, assign the application to that group, and then add users to that group.</p>
<p>Any users that get added to that group are the ones that will get access to the application. Same thing with licensing. We can assign licenses to specific groups. So whatever users get added to that group also get those licenses. And same thing for Azure roleassignments.</p>
<p>So that’s the quick and dirty on how to create users and groups and how to manage them through the Azure Portal for Azure Active Directory. And before we run away here, let’s just take a look at devices.</p>
<p>Now, this devices page is where you manage any devices that are joined to your Azure AD. We can see here, I have a workstation that has been joined. And if I select that workstation, I can take a look at its device ID, its object ID, and all kinds of information about this specific device. I can then even disable the device so it’s no longer able to access Azure AD. We’re not going to do that here, but we will bounce back to our directory.</p>
<p>Now from this Azure Active Directory overview page, you can also manage password resets, manage Azure AD connect. You can take a look at the provisioning status. You can manage your custom domain names, even managed company branding. So there’s a lot you can do from your Azure Active Directory page here. And this is where all of your management will happen.</p>
<p>All of your users, all of your groups, all of your devices, all of your branding, all of your domains, all this stuff is done right here from this Azure Active Directory page. So I really recommend that you go in and you play around in here and kind of learn what you can do and do some experimentation because you really want to try to get some stick time to get a real good understanding of all of the different features in Azure Active Directory.</p>
<h1 id="Deploy-Azure-AD-Connect"><a href="#Deploy-Azure-AD-Connect" class="headerlink" title="Deploy Azure AD Connect"></a>Deploy Azure AD Connect</h1><p>In this demonstration, we’re going to deploy Azure AD Connect so we can sync our on-prem active directory users to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure AD</a>. We are also going to need to ensure that we synchronize NTLM and Kerberos credential hashes to Azure AD, since this isn’t done by default. To get started, login to the server that will run Azure AD Connect and download the Azure AD Connect software. Launch the Azure AD Connect installer that you downloaded. Because our lab environment in a single forest with an internet routable domain name, we can use the express option. </p>
<p>When prompted, we’ll connect to Azure AD by providing our global admin <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">credentials</a>. And then, we can then connect to the on-prem AD by providing an enterprise admin credential. Clicking install begins the setup of Azure AD Connect. When the installation completes, click Exit. Because Azure AD Connect does not by default synchronize NTLM and Kerberos credential hashes to Azure AD, we must ensure that these hashes get synchronized if we want to use Azure AD Domain Services. To enable synchronization of the required credential hashes from your on-premises directory to the Azure AD tenant, run the script that you see on your screen:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$adConnector = &quot;&lt;CASE SENSITIVE AD DS CONNECTOR NAME&gt;&quot;</span><br><span class="line">$azureadConnector = &quot;&lt;CASE SENSITIVE AZURE AD CONNECTOR NAME&gt;&quot;</span><br><span class="line">Import-Module -Name &quot;C:\Program Files\Microsoft Azure AD Sync\Bin\ADSync&quot;</span><br><span class="line">$c = Get-ADSyncConnector -Name $adConnector</span><br><span class="line">$p = New-Object Microsoft.IdentityManagement.PowerShell.ObjectModel.ConfigurationParameter &quot;Microsoft.Synchronize.ForceFullPasswordSync&quot;, String, ConnectorGlobal, $null, $null, $null</span><br><span class="line">$p.Value = 1</span><br><span class="line">$c.GlobalParameters.Remove($p.Name)</span><br><span class="line">$c.GlobalParameters.Add($p)</span><br><span class="line">$c = Add-ADSyncConnector -Connector $c</span><br><span class="line">Set-ADSyncAADPasswordSyncConfiguration -SourceConnector $adConnector -TargetConnector $azureadConnector -Enable $false</span><br><span class="line">Set-ADSyncAADPasswordSyncConfiguration -SourceConnector $adConnector -TargetConnector $azureadConnector -Enable $true</span><br></pre></td></tr></table></figure>

<p>What this script will do is enable all on-premises users’ NTLM and Kerberos password hashes to be synchronized to the Azure AD tenant. This script will also initiate a full synchronization in Azure AD Connect. </p>
<p>The values for $adconnector and $azureadconnector variables can be found in the Azure AD Connect synchronization service manager. What I’ve done is create a PowerShell script called NTLMSync.ps1. This script includes all of these commands. I just need to open PowerShell on the Azure AD Connect server to run the script. The output tells me that the password hash sync configuration has been updated.</p>
<h1 id="Azure-Role-Based-Access-Control"><a href="#Azure-Role-Based-Access-Control" class="headerlink" title="Azure Role-Based Access Control"></a>Azure Role-Based Access Control</h1><p>Hello and welcome back. A tool that goes hand-in-hand with Azure active directory is Azure role-based access control, or RBAC. Azure RBAC is used to manage who has access to what resources and to manage what those users can do with those resources. It’s an authorization system that is built upon Azure resource manager, and it offers fine-grained access management to your Azure resources.</p>
<p>Azure RBAC relies on role assignments for access control. Each role assignment consists of three parts, a security principal, a role definition, and a scope.</p>
<p>The Security principal is essentially an object that represents either a user, a group, a service principal, or a managed identity that requires access to resources in Microsoft Azure. </p>
<p>The role definition is essentially a collection of permissions. Role definitions are usually referred to simply as roles. A specific role will list the operations that can be performed by someone with that role. These operations include things like read, write, and delete.</p>
<p>Although there are many built-in roles in Azure that you can use, there are four fundamental roles available. These fundamental roles include owner, contributor, reader, and user access administrator. A person assigned the owner role has full access to all resources, including the rights to delegate access to others. A person assigned the contributor role can create and manage all kinds of Azure resources. However, a contributor cannot grant access to those resources to other users. The reader role allows you to view existing Azure resources, while the user access administrator role allows you to manage user access to your Azure resources.</p>
<p>In addition to the built-in roles you can also create custom roles. This allows you to tailor access to your resources in a way that best suits your organization.</p>
<p>A scope is a set of resources that the access you are setting up applies to. For example, you may provide access to a scope that includes a subscription or maybe a resource group or even a specific resource. A scope in Microsoft Azure can be specified at the management group level, the subscription level, the resource group level, or to specific resources. Because scopes are structured in parent-child relationship, access that is granted at the parent scope level will be inherited by the child scopes.</p>
<p>The process that Azure RBAC uses to determine if a user has access to a specific resource is pretty straightforward. First, the user requesting the access acquires a token from Azure resource manager. This token includes any group memberships for the user. </p>
<p>Next, the user makes a rest API call to Azure resource manager with the attached token.</p>
<p>What Azure resource manager will do next is retrieve all the role assignments and deny assignments for the resource that the user is trying to access. Azure resource manager will then narrow those role assignments down to only those that apply to the user or to groups that the user is a member of. It will also determine which roles have been assigned to the user for the resource being accessed.</p>
<p>Next, Azure resource manager will determine whether or not the requested action in the API call is allowed by the role that the user has for the specific resource being accessed.</p>
<p>Assuming the user is assigned a role that allows the action being requested at the requested scope, access is granted. Otherwise, access is blocked.</p>
<p>Azure RBAC is free and is included with all Azure subscriptions.</p>
<p>To learn more about Azure RBAC, visit the <a target="_blank" rel="noopener" href="https://docs.microsoft.com/azure/role-based-access-control/overview">URL</a> that you see on your screen.</p>
<h1 id="Single-Sign-On-Overview"><a href="#Single-Sign-On-Overview" class="headerlink" title="Single Sign-On Overview"></a>Single Sign-On Overview</h1><p>Unless an organization deploys a single sign-on solution, its users will be required to remember multiple usernames and passwords, one for each different application in use. Additionally, the IT department needs to maintain all of these different accounts and passwords manually. Single sign-on allows users to sign in once with one account in order to access domain-joined devices, corporate resources, SAS applications, and even web apps. After signing in, users can then launch apps right from the O365 portal or via the MyApps access panel. With single sign-on, IT administrators can centralize user account management, allowing them to automatically add or remove user access to applications based on group membership. In this lecture, we are going to discuss the various single sign-on options that are available when designing an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">identity management</a> solution that incorporates single sign-on. </p>
<p>Choosing a single sign-on solution for an application will largely depend on how the application is configured for authentication. Of all the single sign-on methods we are about to discuss, disabled is the only one that does not automatically sign users into applications without requiring a second sign-on to occur. When deciding on a single sign-on solution, it’s important to know that cloud apps can use SAML, password-based, linked, and disabled methods for single sign-on. Of the bunch, SAML is the most secure method. On-prem apps, when configured for Application Proxy, can use password-based, Integrated Windows Authentication, or IAW, header-based, linked, or the disabled methods for single sign-on. The table that you see on your screen provides a summary of the single sign-on methods that are available. Use SAML for single sign-on whenever possible. </p>
<p>This method works when applications are configured to use a SAML protocol. Use password-based single sign-on when an application authenticates with a username and password. Using password-based single sign-on offers secure application password storage and replay via a web browser extension or via mobile application. Password-based single sign-on uses the existing sign-in process that’s provided by the application while allowing an administrator to manage the passwords for it. The linked single sign-on method is useful when an application is configured for SSO in another identity provider service. This SSO option doesn’t add single sign-on to the application. Use disabled single sign-on when an application isn’t ready for single sign-on. Users of the application will need to provide their username and password each time they launch the application.</p>
<p> Use IWA SSO for applications that use Integrated Windows Authentication or for claims-aware applications. When using this method for SSO, Application Proxy connectors use Kerberos Constrained Delegation to authenticate users to the application. Header-based single sign-on should be used when an application uses headers for authentication. It should be noted that header-based single sign-on requires PingAccess for Azure AD. When using header-based SSO, Application Proxy uses <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure AD</a> to authenticate the user and then passes traffic through the connector service.</p>
<h1 id="Enable-Single-Sign-On-for-Dropbox"><a href="#Enable-Single-Sign-On-for-Dropbox" class="headerlink" title="Enable Single Sign-On for Dropbox"></a>Enable Single Sign-On for Dropbox</h1><p>In this demonstration, we’re going to walk through the process of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">enabling</a> single sign-on for an application. We’re going to enable single sign-on for Steven Davis so that he can access his Dropbox for Business account. To configure single sign-on for Dropbox, browse to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure Active Directory</a>. From here, click on Enterprise applications, and then click New application. Find the Dropbox application in the gallery, and then select the application and click Add. After adding the application to Azure, click Single sign-on located under Manage. Select SAML to open the SAML configuration screen. Retrieve the sign on URL from the application vendor and provide that URL in the Sign on URL field for the application in Azure. Provide an identifier value. In this case, we’ll just use the word Dropbox. And then next, download the SAML signing certificate from Azure. This certificate needs to be provided to the application vendor. After downloading the certificate, switch over to the application vendor’s configuration dashboard and provide the certificate you downloaded. </p>
<p>The type of certificate you download will be largely dependent on the application itself so you’ll need to refer to the vendor’s documentation for setting up single sign-on. Retrieve the login URL for the application from Azure and provide it to the application. Save the application configuration, and then after the configuration is complete in both Azure and the app vendor’s portal, you can go ahead and test the login. To test single sign-on, assign a user to the application in Azure by clicking Users and groups within the application’s Azure dashboard. Find a user and then click Assign. Ensure that there is a user account for the assigned user also configured in the application itself. Next, open an incognito window and launch the application panel and login as the user to whom the application was assigned. Launch the application from the application panel and confirm that single sign-on works.</p>
<h1 id="MFA-Overview"><a href="#MFA-Overview" class="headerlink" title="MFA Overview"></a>MFA Overview</h1><p>Two-step verification provides a layered approach to security. Even the most enterprising attacker would have problems compromising multiple authentication factors, because, even if the attacker obtains a user’s password, the password would be useless without also being in possession of the additional authentication method, a mobile phone, for example. Multi-factor authentication works by requiring two or more authentication methods, which typically include something like a password that the user knows, something the user owns, typically a mobile phone, and something the user is, biometrics, for example. Azure MFA offers the ability to safeguard access to apps and data while maintaining a simple end-user experience. By providing additional security via a second form of authentication, MFA provides much sought after security for end-user authentication, and it does so via a range of easy to use authentication methods, including passwords, security questions, email address, application, OATH hardware token, SMS, voice call, and app passwords. Multi-Factor Authentication comes as part of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure AD</a> Premium.</p>
<p> A subset of MFA capabilities is also available as part of an Office 365 subscription and as a means to protect Global Administrator accounts. As part of Azure AD Premium, Azure MFA is offered in two flavors, including the Azure Multi-Factor Authentication Service, which is cloud-based, and the Azure MFA Server option, which is a good option for organizations who have deployed ADFS and that want to or need to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">manage</a> MFA on-prem. Because most users are used to using only passwords to authenticate to applications and services, it’s critical that an organization communicate with the user base when rolling out an MFA solution. Doing so will invariably reduce the number of helpdesk calls that come in during any MFA rollout. Now, despite the best laid plans and deployments, there will be times when you may need to disable MFA in a one-off scenario. For example, if a user can’t sign in because he has lost access to his authentication methods, a lost phone, for example, in such a case, you could use a conditional access policy for Azure MFA. </p>
<p>With a conditional access policy in place, you can create a user group that is excluded from the policy that requires MFA. Placing the user in the excluded group would temporarily allow access until MFA functionality or access can be restored. A way to temporarily bypass MFA for Azure MFA Server users is to allow them to authenticate without two-step verification. Such a bypass can be configured but it expires after a specified number of seconds. Two-step verification prompts can be minimized for users that are on the local network. This can be accomplished with trusted IPs or named locations. By leveraging this features, an administrator for a managed or federated tenant can bypass two-step verification for users that are signing in from a trusted network location.</p>
<h1 id="Enable-MFA-for-O365-User"><a href="#Enable-MFA-for-O365-User" class="headerlink" title="Enable MFA for O365 User"></a>Enable MFA for O365 User</h1><p>In this demonstration, we’re going to enable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/mfa-overview/">Multi-Factor Authentication</a> for an Office 365 user, so you can see how the process works and have a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">log in</a> process works once it’s been enabled. To enable Multi-Factor Authentication for an Office 365 user, open the Office 365 admin portal as a global administrator. Click on Users, and then Active Users. Find the user in the list for whom you wish to enable MFA and double click that entry. On the user’s property page, down at the bottom, click “manage multi-factor authentication”. Select the user in the multi-factor authentication screen and click Enable. Confirm that you want to enable multi-factor authentication by clicking the enable multi-factor auth button, and then close the box out. You’ll notice that the multi-factor auth status now shows enabled for this particular user. Open an incognito browser window and launch the Office 365 portal. Login as the user for whom you just enabled MFA. If you’ve configured MFA properly, the user, Steve in this case, will be prompted for more information before being allowed to login. Continue the login process by providing the information requested. Be sure to save the application password, so that it can be used for other applications if necessary, and then go ahead and click Done. After providing the MFA info, you’re prompted to log back in. Log back in, and be sure to supply the proper MFA info. As you can see on your screen, MFA is now working for Steve.</p>
<h1 id="Azure-AD-B2B"><a href="#Azure-AD-B2B" class="headerlink" title="Azure AD B2B"></a>Azure AD B2B</h1><p>Azure Active Directory Business-to-Business collaboration, also known as Azure AD B2B, allows an organization to securely share company applications and company services with guest users from other organizations while retaining control over company data. With Azure AD B2B, an organization can work with external partners, even if they don’t use Azure AD. The invitation and redemption process of Azure AD B2B allows users in a partner organization to use their own credentials to access a company’s resources. Because the partner organization uses its own <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">identity management</a> solution, external administrative overhead for the sharing organization is essentially non-existent. There’s no requirement to manage external accounts or passwords, nor is there a need to synchronize accounts or manage account lifecycles. When guest users are invited to access resources in a partner organization, they sign into the shared applications and services with their own identities. Guest users without a Microsoft account or Azure AD account have one created for them when they redeem their invitations. Inviting a guest user to access an app or service, using AD B2B, is as simple as sending an invite to the guest user, using the guest user’s email address.</p>
<p>The guest user then follows a few easy redemption steps to sign in. Azure AD B2B offers the ability to use authorization policies to protect corporate content. Conditional access policies like <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/mfa-overview/">MFA</a> can be used to protect corporate applications and data. Such policies can be enforced at the tenant level, the application level, and even for specific guest users. With Azure AD B2B, administrators can add guest users to the organization right from the Azure portal. When the administrator creates the new guest user, which is done in a similar fashion to adding a new internal user, the guest user receives an invitation that allows him to sign into the Access Panel for that user. Guest users can be assigned to apps and even groups. By delegating guest user management to application owners, you can reduce the workload of the Azure administrators in your organization. Delegating user management allows application owners to add guest users to any application that they want. By delegating guest user management to application owners, you can reduce the workload of the Azure administrators in your organization. Delegating user management allows application owners to add guest users to any application that they want to share, even if it’s not a Microsoft application. To make this work, an administrator needs to set up self-service app and group management. Once this has been configured, non-admins can use their Access Panel to add guest users to applications or to groups.</p>
<h1 id="Add-Guest-Users-to-the-Directory"><a href="#Add-Guest-Users-to-the-Directory" class="headerlink" title="Add Guest Users to the Directory"></a>Add Guest Users to the Directory</h1><p>In this demonstration, we’re going to add a guest user to our Azure AD. To add a guest user, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">log in</a> to the Azure portal and browse to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure Active Directory</a>. Click users and then click new guest user. Provide the new guest users email address and then click Invite. The user you’ve invited receives an invite email. Upon clicking the get started button in the invite email, the guest user is prompted to accept the invite. After accepting the invite, the guest user is taken to the applications pane, where he will be able to access applications that ultimately get assigned to him.</p>
<h1 id="Azure-AD-B2C"><a href="#Azure-AD-B2C" class="headerlink" title="Azure AD B2C"></a>Azure AD B2C</h1><p>Azure Active Directory Business-to-Consumer, also known as Azure AD B2C, is an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">identity management</a> service that offers organizations the ability to customize and control how customers interact with corporate applications. It allows organizations to control how users sign up, sign in, and how they manage their profiles when using the applications. Azure AD Business-to-Consumer enables this functionality while also protecting customer identities. Applications registered with Azure AD B2C can be configured to handle many identity management tasks. </p>
<p>For example, you can allow users to sign up to use a registered application, you can enable a signed-up user to edit his profile, and you can even enable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/mfa-overview/">MFA</a> in the application. Other identity management tasks that can be handled include allowing users to sign up and sign in with specific identity providers, such as Facebook, for example. You can even customize the look and feel of the signup experience for users, as well as the sign-in experience. Azure AD B2C completes identity tasks by interacting in sequence with identity providers, also known as IdPs. It also interacts with users, other systems, and with the local directory. The Identity Experience Framework establishes multi-party trust and completes these steps. Along with a Trust Framework policy, this framework defines the actors, actions, protocols, and sequence of steps that need to be completed in order to make things work. Azure AD B2C makes use of SYN cookies and rate and connection limits to protect against denial-of-service and password attacks against applications. It also includes mitigation for brute-force password attacks, as well as dictionary password attacks. </p>
<p>A service that authenticates customer identities and issues security tokens is called an identity provider. Azure AD B2C offers the ability to configure several different identity providers in the tenant. Common identity providers include Microsoft accounts, Facebook, and even Amazon. Before configuring an identity provider in an Azure AD B2C tenant, the application identifier, client identifier, password secret, and client secret, or a combination of each, depending on the identity provider itself, must be recorded from the identity provider application that is created. This identifier information is then used to configure the application that will be accessed via the identity provider being configured. Every Azure AD B2C tenant is distinct and separate from other Azure AD B2C tenants. To leverage the features of AD B2C, you must deploy a B2C tenant, and link it to your Azure subscription. If you wish to allow users to sign in to an application using Facebook, Amazon, or some other identity provider, you must first register the application in the Azure AD B2C tenant.</p>
<h1 id="Create-a-Link-and-Azure-AD-B2C-Tenant"><a href="#Create-a-Link-and-Azure-AD-B2C-Tenant" class="headerlink" title="Create a Link and Azure AD B2C Tenant"></a>Create a Link and Azure AD B2C Tenant</h1><p>In this demonstration, we are going to create a new <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/azure-ad-b2c/">Azure B2C</a> tenant and then, we’re going to link the new B2C tenant to our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">Azure subscription</a>. To get started, login to the Azure portal and make sure you’re in the directory that contains the Azure subscription. We only have one subscription here, so we’re good on that front. Click create a resource and then search for Azure Active Directory B2C. Click on Azure Active Directory B2C in the search results and then click create. Choose the option to create a new Azure AD B2C tenant. Provide an organization name and a domain name. Select a region and then click create. After creating the Azure AD B2C tenant, you now have to link it to your Azure subscription. While, again, ensuring you’re in the directory that contains your Azure subscription, create a new resource and search for Azure AD B2C, just as you did the first time. Click on Azure SD B2C in the results and click create. This time, however, choose the second option to link an existing Azure AD B2C tenant. In the Azure AD B2C tenant dropdown, choose the B2C tenant that you created earlier. Leave the subscription where it’s at and then select a resource group. Click create to complete the linking process. When the process completes, click Go to resource to manage the newly created B2C tenant.</p>
<h1 id="Privileged-Identity-Management"><a href="#Privileged-Identity-Management" class="headerlink" title="Privileged Identity Management"></a>Privileged Identity Management</h1><p>Azure Active Directory Privileged Identity Management, otherwise known as PIM, is an Azure offering that allows you to manage and control access to resources within <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">Azure</a> and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure AD</a>, as well as within other services, such as Intune and Office 365. A valid Azure AD Premium P2 license is required for all users that will interact with or benefit from Privileged Identity Management before enabling the service on a tenant. Alternatively, you can assign an Enterprise Mobility + Security E5 license for each user that interacts with Privileged Identity Management. Generally speaking, licensing is required for users that are assigned to the Privileged Identity Role Administrator role or who are assigned as eligible to other directory roles that are manageable through Privileged Identity Management. If a user can approve or reject requests in Privileged Identity Management, that user also requires a license. Users assigned to a role with time-based assignments, such as Just in time or Direct, or those assigned to an access review role, also require licensing. With Azure AD Privileged Identity Management, an organization can see which users are assigned privileged roles that are used to manage Azure resources. Organizations can also see which users are assigned administrative roles within Azure Active Directory. </p>
<p>Privileged Identity Management also offers the ability to enable on-demand, or just in time, administrative access to services such as Office 365 and Intune, as well as to Azure subscriptions, resource groups, and even individual Azure resources, like virtual machines and such. Azure AD Privileged Identity Management offers the ability to view a history of administrator activation, along with a history of changes that administrators have made to Azure resources. Alerts can also be configured to notify you about changes in administrator assignments. Privileged Identity Management also allows you to require approval for activation of Azure AD privileged admin roles, to review membership of such administrative roles, and to force users to provide justification for ongoing membership in these roles. In Azure Active Directory, Privileged Identity Management can be used to manage users that are assigned to built-in Azure AD roles, such as Global Admin. In Azure itself, Privileged Identity Management can manage users and groups assigned via Azure RBAC roles, such as the Owner and Contributor roles.</p>
<h1 id="Enable-PIM-and-Manage-Subscriptions"><a href="#Enable-PIM-and-Manage-Subscriptions" class="headerlink" title="Enable PIM and Manage Subscriptions"></a>Enable PIM and Manage Subscriptions</h1><p>In this demonstration, we’re going to enable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/privileged-identity-management/">Privileged Identity Management</a>, and we’re going to set up a subscription to be managed by Privileged Identity Management. To enable Privileged Identity Management or PIM for short, Login to the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">Azure</a> portal, and then click on All services and search for privileged identity management. Click on the Azure AD privileged identity management in the search results. Next, click on “consent to PIM” and then verify your identity when prompted to do so. Provide the additional security verification info that is requested and then click Next. Click Verify to verify your information. At this point you’re taken back to the PIM consent screen. To complete the consent process, click the “consent” link and then click Yes to confirm. With PIM enabled, you can now sign up for PIM for Azure AD role management. To do so, click on Azure AD roles, under Manage. Click sign up for PIM in the left pane and then click the sign-up link at the top. When prompted to confirm, click Yes. After setting up PIM to manage AD roles, you need to discover resources in the subscription, so they, too, can be managed with PIM as well. To do this, go back to the quick start page and click Azure Resources. In this demo, here, we’re going to manage all resources in the subscription. Click Discover Resources and then select the subscription. Click Manage Resource to onboard the resource for management, and then click Yes to confirm. At this point, you’ve deployed PIM and configured Azure AD roles and Azure resources to be managed by it.</p>
<h1 id="Self-Service-Password-Reset"><a href="#Self-Service-Password-Reset" class="headerlink" title="Self-Service Password Reset"></a>Self-Service Password Reset</h1><p>Self-service password, or SSPR, is pretty self-explanatory. This feature allows end users to reset forgotten passwords without the need for a call to the help desk. It’s a feature that many organizations request, as it helps provide a more streamlined and pleasant end-user experience. Depending on the SSPR functionality that is required, license requirements may vary. For example, Self-Service Password Change functionality for cloud users is included in all editions of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure AD</a>. In these cases, we are talking about cloud-only users who wish to change their passwords to something new. Self-Service Password Reset functionality for cloud users is only included in Azure AD Basic and higher. It’s not offered in the free version of Azure AD. This functionality applies to cloud-only users who wish to reset their passwords to something they know. Self-Service Password Reset, Password Change, and Password Unlock, leveraging on-premises write-back, requires either Azure AD Premium P1 or Premium P2. This functionality applies to hybrid users that are synced to Azure AD from an on-prem AD. So, when considering Self-Service Password Reset, it’s important to understand the user base and how users are <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">managed</a> and provisioned.</p>
<h1 id="Configure-Self-Service-Password-Reset"><a href="#Configure-Self-Service-Password-Reset" class="headerlink" title="Configure Self-Service Password Reset"></a>Configure Self-Service Password Reset</h1><p>Enabling <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/self-service-password-reset/">self-service password</a> for end users take a huge burden off of the shoulders of the help desk. In this demonstration, we will enable self-service password reset, and test that it’s working with a test account. Before enabling self-service password reset, make sure that password writeback is turned on in Azure AD Connect. To do this, launch Azure AD Connect on the server running it. Click configure and then view the current configuration. If its disabled, relaunch Azure AD Connect, click configure, and then select customize synchronization options. Provide the global admin account to connect to Azure AD. Click next to leave the directories and domain and ou filtering options unchanged. Under optional features, check the password writeback box and click Next. Click Configure to update the configuration and then click Exit. After turning on password writeback, switch over to the Azure portal and then browse to Azure active directory. From here, click Password Reset. For this demonstration, I’m going to enable password reset for all of my users so I’m going to click All, and then save. After enabling password reset, click on Authentication methods and select the authentication methods you wish to make available, along with how many are required to set a password. </p>
<p>This is obviously going to be different for every environment. If you select the security questions option, you’ll be presented with more info to supply. We aren’t using security questions here, so I’ll turn this off. For this demonstration, I’m going to make mobile phone available as the only option. After clicking Save, I’m going to click on registration. I’m asked if I want to require users to register when <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">signing in</a>. I’m going to leave the default value set to yes. I’m also going to leave the window set to the default of 180 days. Clicking notifications allows me to set the notification options for when a user resets a password. I’m going to set both options here to yes. this ensures users receive a notification when they reset their passwords, but it also ensures that all administrators are notified when another administrator resets his password. In the customization pane, I can provide a customized helpdesk link for users to visit if they need assistance. I don’t have helpdesk here in the lab, so I’m going to leave this off for this demonstration. When I click on the on-premises integration link, Azure confirms that the on-prem writeback client is running and allows me to determine if I want passwords to writeback to on-prem AD. I need to leave this set to yes. The second option designates whether or not users should be given the option to unlock their accounts without resetting their passwords. </p>
<p>I’m going to leave this set to no for this demonstration. With password reset enabled, we can now test it. To test self-service password reset, open an incognito browser window and launch the single sign on setup process with a test account. In my case here, we previously enabled <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/mfa-overview/">MFA</a> so let me get through the authentication process here first. As you can see here, I’m prompted to to work through the process of setting up self service password reset for my account. Once I’ve completed the self service password reset setup, I’m presented with my application dashboard. To reset my password, I need to open the password reset URL in my browser. The password reset URL is <a target="_blank" rel="noopener" href="https://aka.ms/sspr">https://aka.ms/sspr</a>. On the next screen, I’m prompted to begin the reset process. I just have to provide my user ID and I need to complete the captcha. Clicking Next takes me to the verification screen, where I can request a code via mobile phone. After I click the Text button, I receive a code on my phone that I need to enter. After providing my verification code and clicking Next, I’m then prompted to create a new password. Clicking Finish completes the process.</p>
<h1 id="Self-Service-Group-Management"><a href="#Self-Service-Group-Management" class="headerlink" title="Self-Service Group Management"></a>Self-Service Group Management</h1><p>To further improve the end user experience, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure AD</a> offers the ability for users to create and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">manage</a> their own security groups and Office 365 groups. In addition, users can also request security group memberships as well as Office 365 group memberships. In such cases, the owner of these groups can approve or deny their membership requests. By delegating group membership control, organizations can ensure that the people who best understand the business context for such memberships are the ones controlling group membership. It’s important to note, however, that this feature applies only to security groups and Office 365 groups. It does not apply to mail-enabled security groups nor does it apply to distribution lists. There are essentially two flavors of self-service group management currently available. These include delegated group management and self-service group management. An example of delegated group management would be a scenario where an organization uses a SaaS application, whose access is managed by an administrator. As the company grows, access management becomes a bit cumbersome for the administrator to handle. To ease the burden of granting access and revoking access to the application, the administrator requests that the application owner create a new group. </p>
<p>After the group has been created, the administrator assigns access to the application for the new group. He also adds all users who need access to the application to this group. Because the application owner created the group, he has he ability to add and remove users from it. Users added to the group are automatically granted access to the application, while users that are removed from the group have their access to the application revoked when the leave the group. By delegating group membership management to the application owner, the application owner no longer needs to wait on the administrator to provide and revoke access to the application. Of course, however, the administrator would still be able to see who has access to the application, and he could block access as necessary. A self-service group management example would be a scenario where an app owner manages an application. In order to grant a group of business users access to the application, the app owner creates a group in Azure AD, grants the group access to the application, and then enables self-service group management on the group. When a user in the organization needs access to the application, the user simply requests access to it from the Access Panel. When the request is approved, the user receives access to the application.</p>
<h1 id="Demo-Self-Service-Group-Management"><a href="#Demo-Self-Service-Group-Management" class="headerlink" title="Demo: Self-Service Group Management"></a>Demo: Self-Service Group Management</h1><p>In this demonstration we are going to enable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/self-service-group-management/">Self Service group management</a>, by allowing users to create their own security groups and to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">manage</a> them. To enable Self Service group management, sign into the Azure portal and browse to Azure Active Directory. You’ll need to do this with a Global Admin account for the directory. From here select groups, and then go into the general settings for groups. Set the option for owners can manage group membership requests to yes. And then set the option for users can create security groups to yes as well. With these settings enabled users in the directory are allowed to create new security groups and to add members to those groups. These new groups will also show up in the access panel for all other users. In addition, if the policy settings on the group allow it, other users can create requests to join these groups as well.</p>
<h1 id="Overview-of-Managed-Identities-for-Azure-Resources"><a href="#Overview-of-Managed-Identities-for-Azure-Resources" class="headerlink" title="Overview of Managed Identities for Azure Resources"></a>Overview of Managed Identities for Azure Resources</h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">Managed Identities</a> for Azure Resources is a feature of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure Active Directory</a>. There are several Azure services that support Managed Identities for Azure Resources. For those who build cloud applications, management of credentials within a code for authenticating to cloud services is often a common challenge. It’s critical to ensure credentials are kept secure. As a matter of fact, preferably, credentials should never appear on a developer’s workstation, nor should credentials ever be checked into source control. With Azure Key Vault, administrators have a way to securely store credentials. However, to be effective, the application code needs to authenticate to Key Vault to retrieve those credentials that are stored. Enter Managed Identities for Azure Resources. Using the Managed Identities for Azure Resources feature in Azure AD solves this problem. This solution provides supported Azure services with an automatically managed identity in Azure AD that can be used to authenticate to any service that supports Azure AD authentication without the need to store any credentials in code. Key Vault is one of the Azure services that is supported. Managed Identities for Azure Resources is a free feature that comes with all Azure AD editions. There are two types of managed identities to choose from. </p>
<p>They include a system-assigned managed identity and a user-assigned managed identity. A system-assigned managed identity is enabled directly on an Azure service instance. Once the identity has been enabled, Azure will create an identity for the instance in the Azure AD tenant that’s trusted by the subscription of the instance. The credentials for the identity are provisioned onto the instance after the identity had been created. System-assigned identities are tied to the Azure service instances that they are enabled on. Deleting an instance with a system-assigned managed identity attached to it will cause Azure to automatically clean up the credentials, along with the identity in Azure AD. A user-assigned managed identity is essentially a standalone Azure resource. Via a creation process, Azure creates the identity in the Azure AD tenant. The subscription in use, in turn, trusts the identity. An identity, once created, can be assigned to one or more Azure service instances. Unlike a system-assigned managed identity, the lifecycle of a user-assigned identity is managed separately from that of the Azure service instance, or instances, to which it’s been assigned. Application code can use managed identities to request access tokens for services that support Azure AD authentication. Azure will handle the rolling of the credentials that are used by the service instance.</p>
<h1 id="Enable-a-Systems-Designed-Management-Identity-on-a-VM"><a href="#Enable-a-Systems-Designed-Management-Identity-on-a-VM" class="headerlink" title="Enable a Systems Designed Management Identity on a VM"></a>Enable a Systems Designed Management Identity on a VM</h1><p>To enable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/overview-of-managed-identities-for-azure-resources/">System Assigned Managed Identity</a> on a VM that was originally provisioned without it your account needs at least the Virtual Machine Contributor Role assignment. In the case of our demo here I’m using a global admin so that’s a non-issue. To complete this task sign into the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">Azure</a> portal, navigate to the Resource Group that contains the VM that you wish to configure. Clock on the desired Virtual Machine and then select Identity. Under System Assigned Status select On and then click Save. Once the System Assigned Managed Identity is enabled the VM will be registered with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure Active Directory</a>. After being registered you can then control its access to other services like Storage Accounts and the like.</p>
<h1 id="DEMO-Azure-AD-Reporting-and-Monitoring"><a href="#DEMO-Azure-AD-Reporting-and-Monitoring" class="headerlink" title="DEMO: Azure AD Reporting and Monitoring"></a>DEMO: Azure AD Reporting and Monitoring</h1><p>Hello and welcome back. In this brief demonstration here, what I wanna do is give you a tour, so to speak, of some of the Azure Active Directory Reporting and Monitoring options that are available.</p>
<p>Now, on the screen here, you can see I’m logged in to my Azure Portal, and I’m at the Overview page for my Azure Active Directory. We are working in the test9878.org directory. Now, the Azure Active Directory Reports that are available provide you with a view of the different activities that are going on in your environment.</p>
<p>You can use reports to determine how applications and services are utilized by your users, and you can detect potential risks that affect the health of your environments. You can use reports to also troubleshoot issues.</p>
<p>With Azure Active Directory Monitoring, you can route your Azure AD activity logs to different endpoints. You can then view this data to see what’s going on within your Active Directory.</p>
<p>Let’s take a look at some audit logs and some sign-in activities. Now, to take a look at the audit log for Azure Active Directory, what you do here is scroll down in the left pane from the Overview page of your directory and you select Audit logs under Monitoring. Now, what this Audit Log Report does is provide you with a record of the different system activities that have occurred.</p>
<p>For example, we can take a look and see who had access to an admin group and who gave them that access. We can also see information regarding password resets and the like. Now, in this dashboard here for our audit logs, we can see a few different activities that have occurred. We’ve seen some Add user activities. We’ve seen some Add member activities. We’ve also seen some activities revolving around roles.</p>
<p>If we select an activity here, so for example, we’ll choose this Add owner to group, what this tells us is what happened, when it happened, and what the status was. It also tells us who performed this action. Selecting the target gives us more information about the activity that occurred. And if we select Modified Properties, we can see what properties were actually modified.</p>
<p>So when you need to look at information for compliance, for example, the Audit logs report would be the report to go to. Now, if we click over to Sign-ins, we can see what was going on regarding sign-ins to our Azure Active Directory. Typically, you’d use the Sign-ins Report to find the sign-in pattern of specific users, or to see how many users have logged in in the last week, or what the sign-in status is.</p>
<p>On the screen here, we can actually see two different sign-ins that occurred. One was interrupted and one was successful. If we select the one here that was interrupted, we can see information about that specific sign-in. We can see when it happened, what happened, what the error code was, and what the failure reason was for the interrupted sign-in. We can see which user generated the alert and even the application where the sign-in occurred.</p>
<p>Along this top tab, we have lots of other different information we can look at. We can look at the location where this occurred. The Device Info. We can see that this failed login occurred on a Windows 10 machine from the Chrome browser. If we look at the Authentication Details, we can see that was a CloudOnlyPassword and that it was false. We can see any Conditional Access information that applied here.</p>
<p>In this case, there were no policies applied. And any Additional Details. Now, to configure monitoring for Azure Active Directory, what you can do is go into Diagnostic settings here below Monitoring, and we can see we have no diagnostic settings configured yet. So what we’ll do is add a diagnostic setting.</p>
<p>So we’ll go ahead and call this MyDiags. Now, what we can do here is we can collect audit logs or sign-in logs. And then when we do that, we can send them to either Log Analytics or to a storage account, or we can stream them to an event hub. For this demonstration here, what we’ll do is we’ll gather our audit logs and send them to a storage account.</p>
<p>Now, if we leave Retention here set to zero, if you look down here, you can see that setting it to zero does not apply a retention policy. This means the data that you collect is retained forever. I’ll just retain this for one day for this demonstration.</p>
<p>Now, when we select our storage account, we need to choose what storage account we want to archive to. So I already have a test9878 storage account here, so that’s what’s selected here. But I could change this to a different storage account if I wanted to. So what we’re doing here is collecting our audit logs with a retention of one day and sending them to our storage account, and then we’ll go ahead and save this.</p>
<p>Now, what we could also do here is instead of sending to a storage account, we can send to Log Analytics. Now, to send to Log Analytics, you’ll need to have a Log Analytics workspace already created. We have a default workspace here. We actually have a couple. So what we’ll do here is we’ll send to Log Analytics, and we’ll save it, and we can see we get the success here.</p>
<p>Now, to take a look at our logs, what we can do is go back out to our directory and then go into Logs here under Monitoring. Now, from the Log Analytics page here, what I can do is run different queries to track down the information I’m looking for as it relates to my Azure Active Directory. So that will call it a wrap.</p>
<p>Just keep in mind that you can run reports to track down specific information that’s reported in the Sign-ins Report and in the Audit logs report. And you can use Monitoring to send your information through logs into the Azure Monitor and&#x2F;or Log Analytics workspace.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>We’ve covered quite a bit of ground in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction/">this course</a>. With so many terms and features to keep track of, it’s critical to be able to distinguish them from one another and understand what each does. By understanding what each identity management piece does, it becomes far easier to design an identity management solution. With that said, let’s walk through a high-level recap of all of the key players in the identity management space and what each offers. Microsoft’s <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/introduction-to-azure-ad/">Azure Active Directory</a> is a cloud-based identity and access management service. With it, users can sign in and access external resources, such as Office 365, the Azure portal, and other SaaS applications. Azure AD is used to control access to applications and resources according to business requirements. A hybrid identity is an identity that spans on-prem and cloud-based capabilities. Leveraging hybrid identities allows an organization to create a common user identity for authentication and authorization to all resources, regardless of whether they are on-prem or in the cloud. Azure AD Domain Services is a Microsoft cloud-based offering that provides managed domain services, such as domain join, group policy, LDAP, and Kerberos and NTLM authentication. These services are fully compatible with traditional on-prem Active Directory and they can be deployed without any need for deployment or management of domain controllers in the cloud. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/single-sign-on-overview/">Single sign-on</a>, also referred to as SSO, allows users to sign in once with one account in order to access domain-joined devices, corporate resources, software as a service apps, and even web applications. </p>
<p>After signing in, users can then launch apps right from the O365 portal or via the MyApps access panel. With single sign-on, IT administrators can centralize user account management, allowing them to automatically add or remove user access to applications, based on group membership. Also known as MFA, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/mfa-overview/">multi-factor authentication</a> works by requiring two or more authentication methods, which typically include something like a password that the user knows, something the user owns, which is typically a mobile phone, and&#x2F;or something the user is, biometrics would be a good example. Azure MFA offers the ability to safeguard access to applications and data while maintaining a simple end-user experience. Azure Active Directory Business-to-Business collaboration, also known as AD B2B, allows an organization to securely share company applications and services with guest users from other organizations, while retaining control over company data. Azure Active Directory Business-to-Consumer, also known as Azure AD B2C, is an identity management service that offers organizations the ability to customize and control how customers interact with corporate applications. It allows organizations to control how users sign up, sign in, and how they manage their profiles when using the applications. Azure AD B2C enables this functionality while also protecting customer identities. Azure Active Directory Privileged Identity Management, also known as PIM, is an Azure offering that allows you to manage and control access to resources within Azure and Azure AD, as well as within other services such as Intune and Office 365. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/self-service-password-reset/">Self-Service Password Reset</a>, also known an SSPR, allows end users to reset forgotten passwords without the need for a call to the helpdesk. It’s a feature that many organizations request, as it helps provide a more streamlined and pleasant end-user experience.</p>
<p>Depending on the SSPR functionality that is required, license requirements may vary. Through self-service group membership, Azure AD offers the ability for users to create and manage their own security groups and Office 365 groups. In addition, users can also request security group memberships as well as Office 365 group memberships. In such cases, the owner of such groups can approve or deny their membership. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-azure-identity-management/overview-of-managed-identities-for-azure-resources/">Managed Identities for Azure Resources</a> provides supported Azure services with an automatically managed identity in Azure AD that can be used to authenticate to any service that supports Azure AD authentication without the need to store any credentials in code. Key Vault is one of the Azure services that is supported. Managed Identities for Azure Resources is a free feature that comes with all Azure AD editions. So, like I said, lots of terms. To learn more about each feature, you can, and should, read Microsoft’s published documentation on each. Be sure to also watch for new Microsoft Azure courses on Cloud Academy, because we’re always publishing new ones. Please give this course a rating, and if you have any questions or comments, please let us know. Thanks for watching and happy learning!</p>
<h1 id="10Azure-Role-Based-Access-Control"><a href="#10Azure-Role-Based-Access-Control" class="headerlink" title="10Azure Role-Based Access Control"></a>10<strong>Azure Role-Based Access Control</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/azure/role-based-access-control/overview">Azure RBAC overview</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Azure-Resource-Manager-Templates-In-Depth-27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Azure-Resource-Manager-Templates-In-Depth-27/" class="post-title-link" itemprop="url">AZ-204-Lab-Azure-Resource-Manager-Templates-In-Depth-27</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:30:11 / Modified: 11:30:12" itemprop="dateCreated datePublished" datetime="2022-11-14T11:30:11-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Azure-Resource-Manager-Templates-In-Depth-27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Azure-Resource-Manager-Templates-In-Depth-27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Resource-Manager-26/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Resource-Manager-26/" class="post-title-link" itemprop="url">AZ-204-Knowledge-Check-Introduction-to-Azure-Resource-Manager-26</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:29:29 / Modified: 19:31:36" itemprop="dateCreated datePublished" datetime="2022-11-14T11:29:29-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Resource-Manager-26/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Resource-Manager-26/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="3.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Introduction-to-Azure-Resource-Manager-25/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Introduction-to-Azure-Resource-Manager-25/" class="post-title-link" itemprop="url">AZ-204-Introduction-to-Azure-Resource-Manager-25</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:24:40" itemprop="dateCreated datePublished" datetime="2022-11-14T11:24:40-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:48:56" itemprop="dateModified" datetime="2022-11-15T00:48:56-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Introduction-to-Azure-Resource-Manager-25/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Introduction-to-Azure-Resource-Manager-25/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Hi, and welcome to this introduction to this Azure Resource Manager course. My name is Hallam Webber and I will be your instructor for this course. This is a beginner’s course that’s aimed at those who want to know more about managing and configuring their Azure environment. So whether you’re in DevOps, IT support, or a developer that wants to get a better understanding of fast-tracking deployments, there is something in this course for you.</p>
<p>In this course, we will start with an overview of what Azure Resource Manager is and where it fits into the overall Azure picture. Then, we’ll get an understanding of why it’s important. and then, we will delve into some hands-on demonstrations of how you can use it to simplify what would otherwise be quite labor-intensive tasks. </p>
<p>As an introductory course, we will be learning by doing, to give you a basic “how-to” foundation in ARM templates. When we’re not in the Azure portal I’ll be using Visual Code with the ARM template extension which provides IntelliSense for Azure Resource Manager syntax. This is a very helpful tool if you’re not familiar with ARM instructions.</p>
<p>We welcome all comments and feedback, so please feel free to reach out to us at <a href="mailto:&#x73;&#x75;&#x70;&#x70;&#x6f;&#114;&#116;&#64;&#99;&#x6c;&#111;&#117;&#100;&#x61;&#99;&#x61;&#100;&#x65;&#109;&#121;&#46;&#99;&#111;&#109;">&#x73;&#x75;&#x70;&#x70;&#x6f;&#114;&#116;&#64;&#99;&#x6c;&#111;&#117;&#100;&#x61;&#99;&#x61;&#100;&#x65;&#109;&#121;&#46;&#99;&#111;&#109;</a> with any questions or comments. Let’s get started.</p>
<h1 id="Azure-ARM-Intro-Overview"><a href="#Azure-ARM-Intro-Overview" class="headerlink" title="Azure ARM Intro Overview"></a>Azure ARM Intro Overview</h1><p>So, what is Azure resource manager? Well, it is what it says it is. For a lot of people when they think of Azure they think of the complete Microsoft cloud product and all that it entails. But in reality, Azure is a collection of services or resources. When you sign up to Azure for the very first time as an organization with a new subscription and you log in to the portal there is nothing there. As you require services whether that’s a virtual machine, a web app, or database server you create instances of those resources. Azure resource manager is the glue behind-the-scenes that makes it substantially easier for you to deploy and manage all those different resources. </p>
<p>ARM in Context</p>
<p>You might think that the portal is Azure, but the portal is one of several interfaces to Azure resource manager. The portal is merely a graphical interface over the resource manager, which in itself is an interface to the various resources, especially in terms of deployment, and managing the order and dependencies involved in deploying various resources. Apart from the portal, there is also an API interface to Azure resource manager and more importantly for us, an SDK interface.</p>
<p>Command Line Interface</p>
<p>PowerShell for Windows and the Azure command-line interface are the preferred methods for interacting with Azure resource manager when it comes to large, complex, and repeated deployments. One thing I want you to take notice of on the slide is where authentication fits into the scheme. Because there are multiple ways to interact with resource manager authentication is handled directly by it. This is demonstrated by the fact that when you log into Azure through PowerShell or the Azure CLI a browser window will pop up and you can authenticate through that when using two-step authentication or a Microsoft account.</p>
<h2 id="Scope"><a href="#Scope" class="headerlink" title="Scope"></a>Scope</h2><p>As we can see here Azure resource manager is mostly used when managing resources within a resource group. All resources are created within a group even if they are spread over multiple geographical regions. By grouping resources, you can perform actions on all the resources in that group at one time. A simple example would be creating a database server with databases and a web app service for some testing and then deleting all of them at one time by deleting the resource group that contains them. Having said that depending on the type of billing account you have with Microsoft you can also create subscriptions and management groups through resource manager.</p>
<p>Why You Need to Know ARM</p>
<p>But why do you need to know about Azure resource manager? After all, the portal works just fine and is easy enough to use. Infrastructure as code is the reason. In the cloud, hardware does not exist; everything is virtual and can be defined by code. It’s fine for you to deploy a simple resource or even a couple of simple resources using the portal in a one-off situation. But having to set up by hand multiple and complex resources, perhaps several times for different environments, like development test and production, would not only be tedious and time-consuming but also prone to error. You can create resource deployments with scripts and run them through the Azure CLI or PowerShell. As resource creation and deployment are so important Azure has developed templates for defining your resources.</p>
<p>A template is a JSON formatted text specification of a resource. The JSON template file contains all the information about a resource necessary for Azure to create it. It contains vital information like the type of resource, for example, virtual machine, database or storage, and the resource’s name. What resource group, and where the resource should be deployed, as in the geographic region are other configurable attributes in the JSON definition. The template specifies the size of the resource, whether that is disk space for a database or compute power for a virtual machine. As I said, every detail about the resource you want to create, including other resources that the target resource will depend on. A template won’t be that useful if it just specifies exactly the same resource. What I mean by that, is you can’t have resources with identical names. The template architecture allows you to use input parameters to reuse a template to deploy the same type of resource, with a different name and attributes. We will see that all the attributes we choose from drop-down lists, radio buttons, and checkboxes as well as text we enter during the resource creation process in the portal become template parameters.</p>
<p>I’m going to introduce you to templates by creating a virtual machine resource in the portal and then saving the template. We will look at the elements of the template like how a resource is defined and how parameters are used to modify a resource deployment. Creating a VM will also demonstrate how one resource is dependent on other resources. A virtual machine doesn’t exist in isolation, it requires a network and storage in the form of a disk. Don’t worry if you don’t get resource templates after the first demonstration. I will examine the different elements of a template in more detail later. The first demo is to show you the relationship between a resource created in the portal and the corresponding template that gets produced.</p>
<h1 id="DEMO-Modifying-Existing-Templates"><a href="#DEMO-Modifying-Existing-Templates" class="headerlink" title="DEMO:Modifying Existing Templates"></a>DEMO:Modifying Existing Templates</h1><p>Without a doubt, the easiest way to get started with templates is to go to a current resource and download the deployment as an ARM template. This process can be likened to recording an Excel macro and then inspecting the code to see how it works. </p>
<p>I’ll start by creating a virtual machine resource. Let’s just fast forward through the portal process by going with most of the defaults. It’s not important what we choose here as template creation will work for any resource. I’ll go with premium SSD and the standard networking setup. I will turn on OS guest diagnostics, take the advanced defaults, and not bother with tags. We can see before we hit the create button, we have the opportunity to download the template before getting the portal to deploy the VM. I’m going to download the template now for use later on.</p>
<p>Having a look at the template we can see it is made up of 22 input parameters that can be changed or substituted to alter how the resources are created. The parameters have a name and a data type. There are 3 variables that are used within the template, and of course, there are the 6 resources being created in the deployment. The parameters file contains the values that are currently used for the resource deployments. These are default values, or values manually entered in the portal when creating the VM resource, like location, machine name, and resource group. Scripts just links you to Azure documentation. The same information is accessible while the deployment is happening. For an existing resource, you can download the template or deploy from the Export Template page. Some of you might be thinking, “hang on, now there are only 3 parameters and 1 resource”. That’s because this template is just the virtual machine component and doesn’t include the supporting network infrastructure resources. If you need to get the templates for all resources in a deployment after the fact, you can go to the resource group and select the resources and click export template.</p>
<p>Back at our VM resource, Azure lets you save templates to an online library, which is what I’ll do now by giving the template a name and simple description. You can view templates saved to the library through More Services, then All Services, and then select Templates. With a template, you can redeploy the resource or make a similar resource with different attributes by changing some of the input parameters.</p>
<p>We’ve got the template what now? The usual method for deploying resources using templates is to use a command-line interface, either PowerShell or Azure CLI. We will look at both of these later, but initially let’s create the VM resource through the portal using the downloaded template.</p>
<p>Search for deploy on the home page of the portal and select Deploy a custom template. Under common templates, there are templates for creating common resources that you can adapt and modify. We want to build our own template in the editor. Here we have a blank boilerplate template, which I’m going to replace with the template I previously downloaded. I’ll upload the file and save it. Once that’s been loaded, we can see that there are 6 resources in our template – correct. Now we have an interface for entering the parameters for the resource creation. Typing in all those values does seem a bit painful, but if I click edit parameters, I can upload the associated parameters.json file with all the values in it and click save. Boom, and there we go, the values that are entered as part of the resource creation wizard are filled in as parameters. We can do a side by side comparison of the parameters as they relate to the manual portal process Resource Group, virtual machine name, region, VM size, user name, disk type, and networking. I’ll fill in the password and agree to the terms and conditions. The purchase button is a bit misleading, and I assume I won’t be charged for my own template. Anyway, the template validates correctly as I would expect, and the deployment successfully completes.</p>
<p>As with most things Azure, there is a marketplace for templates where you can browse or search for a template that meets, or closely meets your requirements at Azure quick-start templates. I’ll search for something a little bit interesting involving Cosmos, like Create an Azure Cosmos account for MongoDB API autoscale. Clicking on the link takes us to a description of the template and its parameters, along with instructions on how to deploy the template using either PowerShell or Azure command-line interface. Browse on GitHub will take you to the template’s repo where you can download the template JSON files.</p>
<p>Clicking Deploy to Azure takes back to the same Deploy Custom Template page where you can enter the missing parameters using the GUI interface.</p>
<h1 id="ARM-Templates"><a href="#ARM-Templates" class="headerlink" title="ARM Templates"></a>ARM Templates</h1><p>An ARM template is a JSON file describing the resources or services that you want to deploy and it has many advantages. Templates are idempotent. This means changes are applied only once, which is exactly what we want. Associated with this is repeatability, and that each time we apply the template we get the same result. Unlike a procedural script where the order of commands is crucial, the template is submitted as one object and Azure Resource Manager works out the order that resources should be created in. So dependency management and orchestration of deployment operations are carried out correctly. That JSON format also allows for nested and hierarchical resources and you can divide up a deployment into multiple linked files to make complex deployments modular and easier to manage. Azure Resource Manager includes commands for testing templates, and Azure DevOps has a task for including ARM templates in pipelines.</p>
<p>A template file is made up of five sections. There are parameters that can be passed into the template. A parameter has a name and a type, for example, string or int for integer, so like programming data types. There is a functions section, this is where you define your own functions. This is in addition to the ARM template built-in functions. Any values you want to use within the template are defined in the variables section. So not variables in the traditional programming sense, but more like constants. Of course, we have a resources section which is obviously the most important part of the template as this is where we define the resources or services we want to deploy. Finally, we have an outputs section that could be used for displaying information or daisy-chaining templates. Also at the root of the template, we have the JSON schema, which we shall see is very important when building a template and also the content version.</p>
<p>Parameters allow you to pass variables into the template to make its functionality more dynamic. Each parameter object starts with its name. A parameter has a type that can be an int, bool, array, object, string, secure object, or secure string. You can also define a default value, allowed values and minimum and maximum lengths or values. Within the metadata section, you can give your parameter a description.</p>
<p>Before I move on to the function section, I just briefly want to touch on the built-in functions that are available within ARM templates. As you can see from this table, there are quite an extensive array of built-in functions, ranging from array functions through to string, although not all of the string functions have been listed here. Many of the functions are what you would expect to find in most programming languages and definitely in the .NET environment. Resource functions are there to provide access to your Azure resources and account settings, while the deployment value functions allow you to access different sections of the template.</p>
<p>User-defined functions are more like formulas or expressions rather than functions that you might find in a procedural programming language. They do take parameters that are defined with a name and a data type and the return type called output also has a defined data type. Template variables and parameters are global and can be used within user-defined functions. While you can use built-in logical functions within your user-defined function, there is no facility for iterations or for-loops. Essentially user-defined functions are a way of separating out complex or compound functionality that you want to use more than once into a separate piece of template code.</p>
<p>Variables are key-value pairs that are defined once and can be used throughout the template. As I said earlier, they are more like constant values as they can’t be assigned to once defined. Variables don’t have to be simple data types, they can be nested and complex JSON objects. The resource section is the main event, if you will. This is where we define the resources or services that we want to deploy. For obvious reasons, different resources will have different properties but there are some properties that all resources must contain. Every resource needs a name and every resource is of a type and is located somewhere. The resource shown here is an app service and depends on another resource of the type app service plan.</p>
<p>Output values are essentially the inverse of parameters. They enable you to output values from your template for either informational purposes or as inputs to linked templates, that we shall look at later, or to be used in scripts that executing the template.</p>
<h1 id="DEMO-Virtual-Machine-Template"><a href="#DEMO-Virtual-Machine-Template" class="headerlink" title="DEMO: Virtual Machine Template"></a>DEMO: Virtual Machine Template</h1><p>Let’s look at how the template structure relates to the virtual machine template we previously downloaded. I’m using Visual Studio Code to view the JSON files. Here we have those 22 parameters, followed by the three variables, and then the 6 resources. A virtual machine deployment involves more than just the machine. There is all the supporting network and security infrastructure that allows you to safely connect to and use the VM. Here we have a resource name which is one of the parameters that we input to this template. That’s the parameter definition specifying its data type. If I go over to parameters.json I can see the value that is being inserted for network interface name. Going back to template.json we can see each resource has a type, that is what resource is created. This is a network interface which is defined in the region specified by the location parameter. We have a security group name, virtual network resource, a public IP address resource, the actual virtual machine itself with the various nested properties that define that machine like its size, which is also retrieved from the machine size parameter. Here we have the virtual machine’s dependsOn section specifying that the VM requires the network interface and storage resources.</p>
<h1 id="DEMO-Simple-Template"><a href="#DEMO-Simple-Template" class="headerlink" title="DEMO: Simple Template"></a>DEMO: Simple Template</h1><p>I know what you’re thinking, this is very complex, and where do I start? Well, as luck would have it, Visual Studio has some fantastic features for creating and editing ARM templates. To access the ARM template intellisense, you need to have the Azure Resource Manager Tools extension installed in VS Code.</p>
<p>First thing I’m going to do is create a new file, and save it with the JSON extension. Straightaway, we can see that VS Code recognizes this as a JSON file. This is where the magic starts, or more correctly intellisense, when I type ARM for Azure Resource Management. We can see I have code auto-complete here with some prompts. I’m just going to select the top one, which is a resource group template and there we have all the sections that we have talked about earlier. So if I want to create a simple web app deployment, all I have to do is go into resources and once again type ARM, and we can see here we have a lot of resources to choose from.</p>
<p>Now this is all context-based, and I’m going to select web app, and there we have all the properties that we need to create a web app deployment. Obviously, I don’t want my app to be called webApp1. So I’m going to create a parameter of type string, called appname. We can see here with the yellow squiggly line that part of the template functionality is that it recognizes that I haven’t used that parameter yet. So, I’ll go down here and type open square bracket P and you see the auto-complete prompts with parameters. I’ll pick parameters and type open bracket and I’m prompted with the parameter appname. I can then go and I can just replace all the instances of webApp1 with my appname parameter. Every resource has a type and in this case it’s a web site and it’s already put in the ARM tags and properties for us.</p>
<p>In the dependsOn section, we can see a service plan. But before I define that I’m going to show you how we can test our template before deploying it. If I go to my PowerShell window, which I’ve already logged into Azure and enter Test AzResourceGroupDeployment, I’m prompted for the parameter appname, which I’ll supply, called howdeploywebapp. It’s returned with an invalid template error. It’s complaining that it doesn’t have an appServicePlan1.</p>
<p>Okay, so I need to back in the resources section and I will add a service plan and I will also add another parameter called planname. I can use that parameter everywhere I’ve got appServicePlan1, and that is even within the app service itself, like in this concat function. As you can see, functions, parameters and variables are used within square brackets. The error in the concat function indicates a missing comma. Right, so I’ll save that. Just notice here that the location is picked up from the resource group.</p>
<p>So when we are testing, we are testing it with AzResourceGroupDeployment, and that relies on the fact that it belongs to an existing resource group as opposed to using the Test-AzDeployment command. I’ll go back here and re-run this, howdeploywebbapp and the plan name is howdeployplan and we can see that has been successful. So let’s go ahead and deploy that. I’ll just go back here and change my test to new. Now we can go over to the portal and see what’s happening. If I refresh deployments, we can see we’ve got a deployment happening. Let’s click on that and go into it, and it’s complete. I’ll navigate back to the resource group home page so we can see the newly created resources.</p>
<p>Right, so here we have the plan and the app, and as you would expect. Clicking on the app service resource takes us to the app service, where we can click on the app service URL to see it deployed.</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;$schema&quot;: &quot;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#&quot;,</span><br><span class="line">  &quot;contentVersion&quot;: &quot;1.0.0.0&quot;,</span><br><span class="line">  &quot;parameters&quot;: &#123;</span><br><span class="line">    &quot;appname&quot;: &#123;</span><br><span class="line">      &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;description&quot;: &quot;description&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;planname&quot;: &#123;</span><br><span class="line">      &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;description&quot;: &quot;description&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;    </span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;functions&quot;: [],</span><br><span class="line">  &quot;variables&quot;: &#123;&#125;,</span><br><span class="line">  &quot;resources&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;[parameters(&#x27;planname&#x27;)]&quot;,</span><br><span class="line">      &quot;type&quot;: &quot;Microsoft.Web/serverfarms&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;2018-02-01&quot;,</span><br><span class="line">      &quot;location&quot;: &quot;[resourceGroup().location]&quot;,</span><br><span class="line">      &quot;sku&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;F1&quot;,</span><br><span class="line">        &quot;capacity&quot;: 1</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;tags&quot;: &#123;</span><br><span class="line">        &quot;displayName&quot;: &quot;[parameters(&#x27;planname&#x27;)]&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;properties&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;[parameters(&#x27;planname&#x27;)]&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;[parameters(&#x27;appname&#x27;)]&quot;,</span><br><span class="line">      &quot;type&quot;: &quot;Microsoft.Web/sites&quot;,</span><br><span class="line">      &quot;apiVersion&quot;: &quot;2018-11-01&quot;,</span><br><span class="line">      &quot;location&quot;: &quot;[resourceGroup().location]&quot;,</span><br><span class="line">      &quot;tags&quot;: &#123;</span><br><span class="line">        &quot;[concat(&#x27;hidden-related:&#x27;, resourceGroup().id, &#x27;/providers/Microsoft.Web/serverfarms/&#x27;, parameters(&#x27;planname&#x27;))]&quot;: &quot;Resource&quot;,</span><br><span class="line">        &quot;displayName&quot;: &quot;[parameters(&#x27;appname&#x27;)]&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;dependsOn&quot;: [</span><br><span class="line">        &quot;[resourceId(&#x27;Microsoft.Web/serverfarms&#x27;, parameters(&#x27;planname&#x27;))]&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;properties&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;[parameters(&#x27;appname&#x27;)]&quot;,</span><br><span class="line">        &quot;serverFarmId&quot;: &quot;[resourceId(&#x27;Microsoft.Web/serverfarms&#x27;, parameters(&#x27;planname&#x27;))]&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;outputs&quot;: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
</blockquote>
<h1 id="DEMO-Linked-Template"><a href="#DEMO-Linked-Template" class="headerlink" title="DEMO: Linked Template"></a>DEMO: Linked Template</h1><p>In this demonstration, I want to show you how to use linked templates. That’s how you can separate resources into separate template files to make management easier and to reuse resources and deployments. This is a little bit of a contrived example but what I’m going to do is separate out the app plan resource from the app service deployment. </p>
<p>First of all, create a new file that I will save as appsplandeploy.json and start with the ARM template boilerplate structure. Then move the plan name parameter along with the plan resource from the existing template file to the new one. </p>
<p>In the appsplandeploy template, I will need to define an output parameter that will pass the planname to the parent or calling template. That output parameter will be the plan name and will be of type String and the value is simply going to be the parameter planname that is being passed in. </p>
<p>I’m going to create a new file for my web app deployment called websitedeploy. This will be the parent or master template. It will just be the content from the original template, so yes I could have just renamed the file.</p>
<p>We can see here that we already have some issues with our missing parameter and service plan, so I will need to add a link to the appsplandeploy template, and I will do that by creating a nested resource. </p>
<p>While link and nested might not be intuitive, I guess that is exactly what we are doing; we are nesting this linked resource within our main resource template. One thing to be aware of is that the template file cannot be accessed locally, you must have that file accessible by HTTP, which means storing it somewhere on the Internet, and in my case, I’ve created container storage where I will upload that template file to. I’ll just upload that file to my storage container templates inside the howarmtemplate storage account. I’ll grab the link to the file and I will paste it into the URI property of the template link section. Obviously the other issue we need to address is the now redundant planname parameter and we do that by using a reference function to reference the output value from our linked template. So the reference name is the name of my linked template resource. I’ll just change the name of that resource from nested deployment to linkedTemplate. </p>
<p>We reference our outputs with the word outputs, then it picks up the name of the output, which is planname, and then we want to use the keyword value to get its value.</p>
<p>We no longer need the dependsOn section, so I will get rid of that. Another thing you need to be aware of is that the reference function does not work as a parameter within another function, that is, you cannot nest it with another function. Replacing the planname parameters within the linked template reference inside the concat function will throw a template error.</p>
<p>One thing I forgot to mention was that in the app service plan template as it is being linked from our website deploy template we will not be prompted for our plan name parameter. Just for the purposes of demonstration, and it is definitely not best practice, I will use the default parameter to specify a plan name. Later on we will look at a much better option. I’ll just save appsplandeploy and refresh it to my blob storage. Before we continue I’ll demonstrate how that reference function within the concat function will give an invalid template error by running a Test-Az- ResourceGroupDeployment. Here we have the invalid template error’s telling us there is a problem at line 30 column 9 and the function reference is not expected at this location. Line 30 column 9 isn’t really accurate but the closest reference we have to that is within the tags property, but I know this to be the case and I will change it and we will retest. Having changed that tag property it passes the test deployment so I can now deploy for real using the new-azresourcegroupdeployment command. Going back to the portal and into deployments of the resource group, we can see the link template and the website deploy are both in action. If we go and have a look at the link template deployment and in outputs we can see the plan name output value and obviously that is also the input for the website deploy deployment.</p>
<p>Let’s recap what’s gone on here. We started with a resource template that had an app service plan and a website. Then we took the app service plan and we put it in another template file; this is called a linked or nested template. The app service plan template file is linked to the master template using a deployments resource. Values are passed from the master into the linked template with parameters, and values generated in the linked template can be passed back to the master with outputs. Input parameters take the same format as we’ve already seen, while outputs are analogous to return values of a programming function. Linked or nested templates must be stored on-line and referenced with a URL. Template deployments can be tested with the Azure CLI test-azresourcegroupdeployment command, taking the resource group and template file as parameters. Once successfully tested a template can be deployed to Azure with the new-azresourcegroupdeployment command.</p>
<h1 id="DEMO-Parameter-Template"><a href="#DEMO-Parameter-Template" class="headerlink" title="DEMO: Parameter Template"></a>DEMO: Parameter Template</h1><p>Obviously, using a default parameter in our linked template, which is essentially the same as hard coding, the value is completely unacceptable and almost pointless. Before I show you how to use a parameter file to dynamically change parameters, I’ll just delete the web app and plan. Let’s go back to visual studio code and create a new JSON file called deploy parameters, and from the autocomplete, I will choose the parameter template. We just need the app name and plan name parameters with values inside the parameter template and add the plan name parameter to the website deploy template. </p>
<p>With the parameters set up I’ll save and close the the template file, and go back to appsplandeploy.json and add the plan name parameter to the website deploy template. The plan name parameter in this template, website deploy is where the plan name comes in from externally. Then we will pass it to the linked or nested template via a parameter within the linked template resource below.</p>
<p>Now in the parameters section of the linked template resource I will just add the plan name parameter and pass it the parameters planname value. This is a completely ridiculous and circular scenario in reality as the plan name parameter is being passed to the link template and then subsequently retrieved through the outputs value. But this is just to give you an idea of how to use parameters in linked templates using the previous example. Now, to deploy the resources, we’ve got the parameters in a file. That file will be passed as a parameter to the new-azresourcegroupdeployment command. The parameter values get pulled out of the JSON file and matched by name to the parameters defined in the ARM template file. Those parameters, can in turn be passed to linked or nested templates, by matching on name.</p>
<p>Let’s switch over to the PowerShell command prompt. First I’ll test the deployment, specifying the resource group name, the ARM template file, and the parameters file. Okay having successfully tested that I will now redeploy by executing the new resource group deployment command with the template parameter. We can see in the PowerShell CLI and the portal that it has successfully executed.</p>
<h1 id="DEMO-Database-Deployment"><a href="#DEMO-Database-Deployment" class="headerlink" title="DEMO: Database Deployment"></a>DEMO: Database Deployment</h1><p>As I’ve said the demos so far have been a little contrived. Now I want to show you a few of the more advanced features of ARM templates like the use of user-defined functions and how to create multiple instances of a resource type using something that could be called a looping mechanism. We will then deploy that template via a PowerShell script which will involve creating a new resource group and then submitting the template parameters using a template object. Let’s begin by creating a new template. I’m going to call it arm deploy and once again that will be a resource group template. In terms of resources, we are going to need a database server and then obviously some databases to run on that server. From the ARM template autocomplete I’ll select arm-sql-server and then arm-sql-db for my resources.</p>
<p>For parameters, I will need a server name and because I am creating more than one database I’m going to need a parameter that lets me tell the template how I databases I want. I will call that dbcount. Because we are accessing Azure SQL I will also need to provide an IP address for the database firewall. That will be startip and endip to define the IP address range. In terms of naming the databases, I’m going to call them test_db and that name will have a number appended to it, and also define an admin name and password. I’m defining these as variables to demonstrate the variable use, but you could just as well pass them in as parameters. I’ll replace SqlServer1 with the servername parameter, which will have to be changed in several places. I’ll also replace the administrator login and password with my variables. Next, I’ll replace the firewall start IP and end IP addresses with the corresponding parameters.</p>
<p>When it comes to the database name this is where it all gets a little bit interesting. So the database name is made up of the server name&#x2F;followed by the database name so the first thing I’m going to have to do is concatenate my server name parameter with the database name. But of course, the database name is a compound of the DB prefix variable plus the database number. To make up this compound name I’m going to define a function that will return the name. Let’s go into the functions section and we’ll look for arm-user, which is for user-defined function. I’ll give my function a name of databasename, and like all functions it takes parameters. This is where the auto complete doesn’t fully meet expectations. The function section has to have a namespace property and all functions have to be defined within a members section of the functions section. I’ll need a parameter for my database name prefix and I will need an integer parameter for the number that will be appended to the name. The return value or output is a string and it is a concatenation of name prefix and index, which I’ll join together with the built-in conact function. Going back to the database name I will use the databasename function firstly by starting with the namespace and the function name. Then I will use the DB prefix variable as my first parameter and then a built-in function called copy index. I guess you could say copyindex is a little bit like the index of a for loop, although we have yet to tell the database template to make copies. I’ll just replace the SqlServer1 text with the server name parameter in the depends on section for my database, as obviously, a database does depend on a database server. Now I’ll add the copy section. So it has a name and it has a count and that count is the number of copies to make and I will get that from my dbcount parameter. I’ll just replace the display name and tags with my compound database name.</p>
<p>Now that we are done with the template let’s move on to the PowerShell script. So this is going to be called dbarmdeploy.ps1 and Visual Studio Code recognizes the file extension and that I’m working with a PowerShell script and it is doing a nice job of syntax highlighting for me. So I’ll start by declaring a resource group variable and I will give it a name and a location. The location is the Azure abbreviation for the West US region. Next, I want to issue the command to create a new resource group. So that’s New-AzResourceGroup with a name and the location, which is the location element of the resource group variable. Next, I’m going to create a compound template variable which will have the name of my template as file name and nested within it the parameters that I want to submit. So that will be server name dbcount, startip and endip. I’ll create a new deployment with New-AzResourceGroupDeployment and use the resource group variable to get the ResourceGroupName parameter. Then the template variable to get the file name for -TemplateFile and then the template parameters object will be for the -TemplateParameterObject parameter, my word that’s a mouthful of parameters. Just in case that wasn’t clear, the TemplateParameterObject is a compound parameter, whereas the other parameters are strings. Right, let’s save that and go to the PowerShell command prompt, and run the script. Straightaway we can see the resource group has been created so that’s a good start, and if we pop over to the portal we can see that the deployment is running. If I open up an Azure SDK command prompt I can issue a deployment group list command to see which deployments are running for my resource group. At the top, I can see the name of the deployment is armdeploy so using that I can issue and az deployment group show command with my resource group and then the name of the deployment. This will also return all the details of the deployment plus the provisioning state at the bottom which is currently showing it as running.</p>
<p>Okay, the server has been deployed and Azure resource manager has accepted the database deployments. Going back to the PowerShell window we can see that the whole deployment has finished successfully and it returns our input parameters. I’ll just open up SQL Server management and log into the database server and check out my to test databases. Finally, I will remove the resource group with the force parameter so I’m not prompted.</p>
<h1 id="Deploying-with-a-Script"><a href="#Deploying-with-a-Script" class="headerlink" title="Deploying with a Script"></a>Deploying with a Script</h1><p>As I said at the beginning of this course in the architecture section, Azure Resource Manager accepts commands from PowerShell and the Azure CLI. In a way, templates are a little bit like the portal in terms of providing an alternative interface. Everything that you can do with a template you can do with a script. Here is an example of what a script might look like for creating a resource group in an Azure SQL server with firewall rules and a database. In some ways, this does seem more concise and easy to understand what exactly is going on here, but in one crucial respect, templates have an advantage over scripts when it comes to deployment mode.</p>
<p>Azure Resource Manager has two deployment modes; the default, which is incremental, means that whatever is in your deployment is added to your resource group, whereas complete says the resource group will become whatever is in your deployment. This essentially means that the resource group is cleaned out before the deployment is applied or all resources that are not in your deployment are removed from the resource group.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Let’s recap what we have learned about Azure Resource Manager. It’s not the Azure portal. It’s the behind-the-scenes process that’s mostly involved with managing the deployment of resources in the Azure cloud. There are multiple interfaces for interacting with Azure Resource Manager apart from the graphical interface of the portal.</p>
<p>You can issue commands through PowerShell and the Azure CLI, but the preferred method is to use ARM templates. Azure Resource Manager templates are JSON files with sections that allow you to specify parameters, variables, functions, and resources. There is also an outputs section so you can pass values from one template to another.</p>
<p>A template can specify multiple resources, and multiple resources can be specified in multiple template files that are linked to a master template deployment file. Visual Studio Code has an IntelliSense extension for creating ARM templates. This extension has auto-complete and syntax checking and greatly simplifies the creation of templates.</p>
<p>As well as defining your own functions, there are many built-in functions to work with strings, arrays, logical comparisons, and Azure resources. Once a template has been defined, it can be tested with the Test-AzResourceGroupDeployment command. The template can then be deployed to Azure using the New-AzResourceGroupDeployment command. Both of these commands can be actioned either through PowerShell or the Azure CLI.</p>
<p>When a deployment is in progress you can monitor it through the portal or by using the Az group deployment list and show commands. You don’t have to create your template from scratch. You can download deployments as templates that you have created previously through the portal. This reaffirms the architecture of the portal as an interface to the Azure Resource Manager.</p>
<p>There is also a marketplace of Quickstart templates that you can use as-is, or download and modify. ARM templates simplify deployment by taking care of dependency management and orchestration. Unlike deploying resources with a script, you don’t have to worry about the order you define your resources in. Testing a deployment will let you know if dependencies are missing, and if you deploy without testing either the whole template is successful or none of it succeeds. You won’t end up in a situation with half of your resources deployed due to missing dependencies.</p>
<p>When a new resource or service becomes available on Azure often deploying an instance is only available using ARM templates and commands. Quite often a resource is released and there may be some time before the portal is updated to support it. But the main reason to become familiar and experienced with Azure Resource Manager is that it is by far the easiest method to ensure that your deployments are consistent and repeatable. This is of particular importance when deploying infrastructure as code through a DevOps pipeline.</p>
<h1 id="Course-Introduction-1"><a href="#Course-Introduction-1" class="headerlink" title="Course Introduction"></a><strong>Course Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/intro-to-arm">Github Repo</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Developing-with-the-Cosmos-DB-Core-API-and-Change-Feed-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Developing-with-the-Cosmos-DB-Core-API-and-Change-Feed-24/" class="post-title-link" itemprop="url">AZ-204-Lab-Developing-with-the-Cosmos-DB-Core-API-and-Change-Feed-24</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:24:10 / Modified: 11:24:12" itemprop="dateCreated datePublished" datetime="2022-11-14T11:24:10-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Developing-with-the-Cosmos-DB-Core-API-and-Change-Feed-24/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Developing-with-the-Cosmos-DB-Core-API-and-Change-Feed-24/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Introduction-to-Azure-Cosmos-DB-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Introduction-to-Azure-Cosmos-DB-23/" class="post-title-link" itemprop="url">AZ-204-Introduction-to-Azure-Cosmos-DB-23</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:18:10" itemprop="dateCreated datePublished" datetime="2022-11-14T11:18:10-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:42:40" itemprop="dateModified" datetime="2022-11-15T00:42:40-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Introduction-to-Azure-Cosmos-DB-23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Introduction-to-Azure-Cosmos-DB-23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Greetings. Welcome to Cloud Academy’s course on Microsoft Cosmos DB. I’m delighted to have you join me on what is bound to be an educational and delightful adventure into the world of database technology. First, I’ll let you know a little bit about myself before I get into the course outline. My name is Jonathan. I’m one of the course developers with Cloud Academy. I work professionally as a technical consultant specializing in DevOps, data engineering, and security. Long ago in another life, I was a public school teacher, so I love educating people and I’m thrilled to be doing it again, only now, with technology. </p>
<p>This course is designed to be very practical. It’s meant for technology professionals, DevOps engineers, data architects, CTOs, etc, with the goal of helping them get a solid understanding of Cosmos DB in a short time. So, what exactly are the prerequisites then for a course like that? Well, we’re going to be doing a pretty deep dive on the Cosmos DB architecture and its varying data models. For that reason, we recommend that people have some basic knowledge of database technologies. You should also have at least a passing familiarity with <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a> and cloud hosting generally. So, when I talk about how an Azure service can talk to Cosmos DB, I’m going to assume that the students know what that means at a high level. In short, if you have any technical experience working with cloud providers and databases, you’ll probably be fine. If this is your first time learning about databases, then maybe you’ll struggle a bit.</p>
<p>So, what are the exact learning objectives for a course like this? Well, I’ve narrowed it down to three big ones that will guide each section. Objective one, is to ensure that the student gains a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/intro-azure-cosmos-db/">basic understanding of the Cosmos DB</a> technology, including its feature set and design philosophy and a little bit of history too. Objective two, is to teach students how to use Cosmos DB, via its APIs, Microsoft CLI tools, and the Azure web console. Finally, objective three, is to give students a practical understanding of how to integrate Cosmos DB with other Azure services with the goal of creating a working app. So in short, what is Cosmos DB, how do we use it, and then concrete example. So, lastly, I would like to encourage everyone to leave feedback. Email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> if you have any questions, comments, suggestions, or concerns. We always appreciate people taking the time to provide feedback. So now without further ado, let’s get started.</p>
<h1 id="Introduction-to-Cosmos-DB"><a href="#Introduction-to-Cosmos-DB" class="headerlink" title="Introduction to Cosmos DB"></a>Introduction to Cosmos DB</h1><p>The fact that you’re here with me taking this course implies that you know, or at least care a little bit about data based technology. If that’s the case, then you know there are a lot of different products out there. There are relational databases like PostgreSQL and MySQL as well. You have no sequel systems like MongoDB or key value stores, collum databases, things like Cassandra, numerous enterprise offerings from big companies. Things like Amazon’s DynamoDB. So with such a crowded ecosystem, the first question anyone would rightly ask about Cosmos DB is what makes it so special? Before we try to tackle that question, let’s talk a little history.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a> is actually an extension of Microsoft’s DocumentDB which was released back in 2014. Microsoft had been developing Cosmos DB since 2010 and in 2017, it officially migrated all DocumentDB users to Cosmos DB. Cosmos DB supports the same SQL-like API as DocumentDB as well as several other APIs and features. Now according to <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>‘s own documentation, Cosmos DB is best described as a globally distributed multi-model database. Now the two descriptive terms there are key. </p>
<p>Globally distributed and multi-model, that should already give you an idea of the problems Cosmos DB is meant to solve. For those of you familiar with CAP theory, C-A-P CAP theory, you could infer that availability is one of the chief guarantees. This is critical because achieving reliable database performance across multiple geographic regions is very difficult, especially for relational databases. So what do we mean when we say that Cosmos DB is a multi-model database? When we use the term model when talking about databases, we’re referring to how the data is actually stored and what sort of APIs are used for clients to read and write data. So for example, Cassandra has a certain API and storage paradigm that makes it more suitable for some use cases than others. MongoDB and PostgreSQL have their own models. So what does it mean to say that Cosmos DB is multi-model? Well, as you might guess, this simply means that with Cosmos DB, you have the flexibility to use a variety of different APIs for different use cases. In fact, the available models for Cosmos DB come from well known database technologies. Cosmos DB supports Table API, Cassandra, SQL, MongoDB, and Gremlin. </p>
<p>There are STKs available in multiple languages for clients. Cosmos DB is sort of like DynamoDB in that it is a database as a service. That is, you don’t install it on your own server and configure it like you would with Cassandra or Postgres. Rather, what you do is you sign up for access through Azure, and behind the scenes Microsoft handles the scaling. Microsoft Azure offers 99.999% uptime SLA to ease concerns about entrusting your precious data to them. Still, it is important to keep in mind the relative lack of control you have if you rely on Cosmos DB. It isn’t like a typical database system where you can SSH to the server and tune things the way you want. This is understandably a deal breaker for some people. On the flip side, as a former DynamoDB user, I can say that it can be a really big boon for a small startup to have the reliability and performance guarantees of a large company like Microsoft backing your app. </p>
<p>It lets you focus on your business logic and scale very quickly. So, that’s it in a nutshell. Cosmos DB is a powerful database as a service system designed to support a variety of data models and work across multiple geographic regions. In the next lesson, we’ll dive into the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/cosmos-db-features-and-capabilities/">feature set</a> and talk about how companies actually might use Cosmos DB. See you there.</p>
<h1 id="Cosmos-DB-Features-and-Capabilities"><a href="#Cosmos-DB-Features-and-Capabilities" class="headerlink" title="Cosmos DB Features and Capabilities"></a>Cosmos DB Features and Capabilities</h1><p>For this section we’re gonna focus on Cosmos DB’s unique capabilities. We are not going to cover literally every single thing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a> can do. There is a lot of overlap with other database technologies and quite frankly if you just want a feature list you’re better off reading their documentation, linked below. Instead, in this lesson, we’re going to focus on the six most important and compelling capabilities unique to Cosmos DB. Those six features are, in order, global distribution of data, serverless architecture, multi-model support, throughput consistency guarantees, partitioning, and security. Let’s start by talking about Cosmos DB multi-region support. This is one of the main reasons enterprises choose to use Cosmos DB. The fact that it is designed from the ground up to support access patterns from all over the planet. With over 50 geographic locations for its data centers, Cosmos DB users can ensure minimal latency for their users. What’s more, new locations are regularly added each year. Multi-region logic is deeply integrated into the Cosmos DB service. As the user, you can associate as many geographic regions with your data as you want.</p>
<p>You can tune consistency levels for read and write operations, to improve availability or data precision. You can set entire regions as read only, write only, or read-write. Furthermore, you get built-in failover that lets your set priority lets you set priorities for each region, so you can decide what happens if one of your US data centers goes down, for example. You can plan for exactly which regions take precedence and how you will recover. In section two, we’ll go into how all of these geolocation features are used via their Cosmos DB rest API and web console. The next key thing to introduce is Cosmos DB’s serverless architecture. As described previously, Cosmos DB is an example of a database as a service. You do not set up database servers and manage them, instead you just get an endpoint for your app to utilize. The currency for making use of the Cosmos DB endpoint is known as request units. This will determine how much you pay and what sort of performance guarantees you can get. The larger your data, the more frequent your queries, the more indexing you do, the more consistency you demand, the larger the number of request units you will need. The nice thing about this system is it greatly simplified your data layer. You don’t need to think about memories, CPU, hardware provisioning, OS optimization, updates and patches, SSL certs, et cetera, et cetera. All of this operational overhead of managing a database is gone. The time saved alone could translate into more than enough cost savings to offset the cost of needed request units. Cosmos DB’s serverless architecture also ensures strong SLAs. You get a guarantee of 99.999% uptime, far better than what a typical tech company achieves on their own. You’ll also get first order integration with other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> services and great support. So let’s move on and talk about the multi-model data support. </p>
<p>This I think is perhaps Cosmos DB’s most intriguing feature. Cosmos DB offers APIs for Cassandra, Gremlin, MongoDB, SQL, and Table Key-Value API. This means that with a single Cosmos DB account, you can run multiple database engines. So if a portion of your data is best suited for Cassandra, you can set up Cassandra key spaces. And then if a subset of your data needs a document paradigm, you can use Mongo. If you happen to need a graph database and a relational database as well, you can add them as well using Gremlin and SQL. This means you have the flexibility of using the right data model for the job. It means you can easily migrate an existing heterogeneous data architecture into Cosmos DB with little hassle. Now there are some important trade-offs and restrictions that come from using multiple APIs. We’ll dig into that in later sections. Next, let’s talk about throughput and consistency. As far as the cap theorem goes, Cosmos DB is very strong on partition tolerance and availability. Like Cassandra, consistency is tunable and throughput is a function of your request units and consistency settings. Cosmos DB features five different consistency settings. </p>
<p>In order from strongest to weakest guarantees, they are strong, bounded staleness, session, consistent prefix, and eventual. If you need to ensure that always the most recent data is read, choose the strong consistency level. It ensures that no reads are processed until rights are completed durably by a quorum of replicas. With bounded staleness, you get a configurable level of consistency. Reads will lag behind writes by either an adjustable time interval or a number of item revisions. Then there’s the session consistency level. This gives you a read your own writes guarantee suitable for scenarios where you need guarantees at the level of individual clients. It’s considerably cheaper than bounded staleness and strong consistency levels, but you get no consistency guarantee outside of individual client session. And then lastly you have the consistent prefix and eventual consistency levels. Both of these guarantee that your data will eventually converge to the most recently written. With the consistent prefix, at least you get an additional guarantee that data will never be out of order. So even if you don’t get the most recent data on read, you can at least be sure that you’re not skipping over data inadvertently. Both of these consistency levels allow for fast throughput and are relatively inexpensive. The more inconsistency you can tolerate, the more you can save money on request unit usage. </p>
<p>Let’s now talk about partitioning and indexing a bit. In Cosmos DB, there are physical partitions that compromise compute hardware resources. SSD storage, CPU, memory, logical partitions, which are a subset of the physical ones. In other words, a physical partition may be made up of several logical partitions. The basic abstraction for sets of data is a container. A container in Cosmos DB can span multiple physical partitions and will be responsible for storing your collections, graphs, SQL tables, et cetera. Every document in Cosmos DB is uniquely identifiable by the combination of its partition key and row key. The partition key, specifically, acts as a logical partition for your data and helps to create boundaries to enable cosmos DB to map data to specific physical resources. In Cosmos DB, the data for a single logical partition must reside on a single physical partition. When designing collections of data, two critical things to think about are partition key and indexing. On the latter point, Cosmos DB automatically indexes all of your data. However it’s possible to create custom indexing policies that let you tune trade offs between query throughput and consistency. Now regarding partition keys however, you’ll have to think carefully about the nature of your data to decide on a proper partition key. People coming from the Cassandra world will have some good intuition here. </p>
<p>The main thing you want is a column with high cardinality and a large variety of values to help distribute your workloads evenly. See Azure’s best documentation for more details. We will cover both of the indexing and partition key selection in more depth in section three of this course when we get into the practical application. And finally let’s talk a little about security. Now as a cloud-based service, Cosmos DB has many of the same security considerations as any other provider. You need to control who has access to your Azure account, who has credentials to use the API and ensure that sensitive data is properly isolated. The nice thing about Cosmos DB though is that it has very sane defaults when it comes to security. Without taking any action at all, Cosmos DB encrypts all data both at rest and in transit. Cosmos DB supports HTTPS, TLS for all client to server interactions. It also includes two different types of credentials for different use cases, master keys, and resource tokens. </p>
<p>The former are useful for administrators that need to make significant changes, while the latter can be used by clients with narrower needs. So there you have it, consider this your crash course into the world of Cosmos DB. For a more detailed breakdown, please take a look at the Cosmos DB documentation, it will answer many of your questions. Our goal in the following two sections is to go beyond the documentation and get you to actually use Cosmos DB on your own. In section two, we’ll dig into how to set up and utilize Cosmos DB futures that were described here and in section three we’ll walk through <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction-to-using-cosmos-db/">creating an actual software service</a>. Good luck and see you there.</p>
<h1 id="Introduction-to-Using-Cosmos-DB"><a href="#Introduction-to-Using-Cosmos-DB" class="headerlink" title="Introduction to Using Cosmos DB"></a>Introduction to Using Cosmos DB</h1><p>In this section, we will get a little more hands-on with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a>. We’re gonna learn how to actually start using Cosmos DB via the web console in various APIs. This is not a lab, so you’re not required to actually follow all of the stops. However, if you have access to an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> account, I would suggest setting up Cosmos DB and actually trying some of what we cover on your own. We’ll start with just the initial setup via the web console. We will create a Cosmos DB account, select a data model API, and show how to manage our data via the browser. This will be a good opportunity to review several key <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/cosmos-db-features-and-capabilities/">features</a>. Next, we’ll explain how to use libraries for languages like Python and .NET to interact with our Cosmos DB storage. We will also cover using CLI tools, Powershell, and the REST API for Cosmos DB. By the time you’re done with this section, you should have a theoretical knowledge, enough to dive in and do some real work. So if you’re ready, let’s <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/getting-started-with-cosmos-db/">get started</a>.</p>
<h1 id="Getting-Started-with-Cosmos-DB"><a href="#Getting-Started-with-Cosmos-DB" class="headerlink" title="Getting Started with Cosmos DB"></a>Getting Started with Cosmos DB</h1><p>Section two, part two, Getting Started with Cosmos DB. The first and most obvious prerequisite for using Cosmos DB is having a Microsoft Azure account. Once that’s ready, you can go ahead and create a Cosmos DB account. This is as simple as clicking Create a Resource in the top left of the web UI and then clicking on Databases or just starting a search for Cosmos DB by typing that in.</p>
<p>You will then be prompted to input some basic information about your new cosmos DB resource, a name to uniquely identify it with your account, as well as a resource group name, a region, and potentially an option for replica failover region. Also, most important, you’ll have to select your API type. This is what’s gonna define the data model as we said and recall that you can select from MongoDB, Cassandra, SQL, and Gremlin and Azure table.</p>
<p>Now, once everything is filled in click create and wait for a few minutes. It’ll validate and your account will be generated. Now, remember at this point you have a Cosmos DB account resource that’s ready to use but there’s no actual database at this point. What we wanna do next is to use the web console to define a database and then a collection or a table to which we could write some data.</p>
<p>Now before we walk through the steps to do that it’s important to clarify some of the nomenclature here. Different database engines use different terms to describe database schema. So for example, Cassandra uses terms like keyspaces and column families while Mongo uses terms like collections and documents, and in SQL generally, you have a database that has tables and then a row within a table to describe an individual item and that’s kind of your hierarchy.</p>
<p>So depending on the API we pick, we’ll actually see different terms used in the web UI. Now, using the core SQL API we’re gonna see terms like database and container as our general terms. The latter container, this refers to the units of scalability and are analogous to MongoDB collections. So, the process for actually setting up our database in the web console goes something like this.</p>
<p>In the Cosmos DB dashboard we wanna select the Data Explorer option on the left menu. Now we could also start by selecting Add Container in the top menu; however, going through the data explorer makes what we’re doing a little bit more transparent and obvious.</p>
<p>So, in the data explorer UI, we can do all sorts of typical database tasks such as executing queries and changing configuration. Now once you’re in the data explorer page simply click on the New Database button to define your first database. All you need to fill in is a name and a number of RUs, request units, and remember that the RU value will define the kind of throughput performance you can expect.</p>
<p>Now once you have a database, its name should appear in the left side menu, the one that’s to the right of the leftmost menu with the other Cosmos DB options. So click on the little three dots next to your database name and you’ll get an option there to create a container.</p>
<p>So you give your collection a name, you give it an ID, and you give it a partition key, which should be optional, and then you’re done. You’ve got a database and you’ve got a container within your database. Now, that’s just the basic setup and with that clarified let’s step back for a minute and try to fully understand how request units work, because this determines our throughput and it determines our cost to a large extent. </p>
<p>How many request units should we set for our account? What is the formula? Azure offers some really good general advice in the <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-gb/azure/cosmos-db/request-units">documentation</a>. So of course, be sure to check out the <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-gb/azure/cosmos-db/request-units">links</a>. They also offer a nice request unit calculator tool where you can upload a sample JSON document and input some parameters such as reads and deletes, updates per second, kind of your expected use case, and then the calculator will spit out an estimated optimal value for how many RUs you need.</p>
<p>So, without using that, the basic formula for determining RUs is to multiply the number of desired reads per second by the average item size in kilobytes and then perform the same calculation for desired writes per second and then add those two numbers.</p>
<p>So, for example, if your item size is two kilobytes on average and you need 300 reads per second and 500 writes per second you would get 600 and 1,000 by multiplying those two numbers by two, two kilobytes. And therefore in that scenario you add those two numbers together, you should opt for roughly 1,600 RUs, okay?</p>
<p>Now, there are a few important other variables to consider here; document indexing, complexity of query patterns, and critically the desired consistency level. Now remember there’s five; strong, bounded staleness, session, consistent prefix, and eventual. All of these parameters will have an impact on the proper RU setting. So for this reason we strongly recommend taking a look at the documentation and testing out the RU calculator but do keep in mind the formula above. And that’s basically it.</p>
<p>That’s it for this lesson. Congrats, you now know how to set up the Cosmos DB account, the endpoint, and also calculate RUs, all of this using the GUI. You can create your basic config, you can execute queries, you can add arbitrary data. You’re ready to handle basic maintenance for things like request units and other config. Very magical stuff, but of course we can’t stop there. Unless you have a very unusual app, you probably want software to handle querying your database instead of humans. So, in the next lesson, we will learn all about the Cosmos DB API and how you can get computers to do all of this stuff for you. See ya there, space cowboy.</p>
<h1 id="Cosmos-DB-API"><a href="#Cosmos-DB-API" class="headerlink" title="Cosmos DB API"></a>Cosmos DB API</h1><p>So we’ve got our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a> account set up. We can select a desired data API, create databases, write arbitrary data, and execute queries all from the browser. We’re ready to administer our data intelligently. Swell right? Well, in all likelihood you’ll not be doing all of your database reading and writing manually via GUI. You need applications and tools to be able to talk to Cosmos DB. You may want your web app to serve requests to Cosmos DB or you may have devops scripts that have to execute Cosmos DB queries. This is where the Cosmos DB API comes into play. The API documentation is divided by data model, meaning there are separate docs for a Cassandra API, MongoDB, Gremlin, SQL, Azure Table. When you select one you will get information for how to set up your development environment depending on your language choice. Cosmos DB has strong support for many languages including .NET, Java, Python, Ruby, JavaScript. The best way to get started is to look through the SDK documentation for the language of your choice. So for example, let’s say we wanted to use Cosmos DB’s SQL API with, oh I don’t know, how about Java. </p>
<p>The documentation will link us to a GitHub repo for the Java Cosmos DB SDK. There you’ll find a nice README and walkthrough to help you set up the SDK with your Java IDE and begin writing code that could interact with Cosmos DB. Now in some cases you won’t use a Cosmos DB SDK specifically, but rather a driver meant for whatever data model API you selected. So, for example, if we go through the Cosmos DB Python quick start guide for a Cassandra API server, the setup documentation links to the Python Cassandra driver GitHub repo. The Python code you write for Cosmos DB in this instance will be just like Python code for a Cassandra app. You will import the Python Cassandra driver and use its methods to connect, write, and query your Cosmos DB. This flexibility, I think, is one of the most powerful things about Cosmos DB. What it does is it makes it so that if you already have expertise in one database technology, like Mongo or SQL, you can leverage that and often reuse the same code in SDKs. The only new overhead is ensuring your applications have the right permissions and authorization to actually connect to your Cosmos DB account. We’ll drill down about that in section three when we build a real service. Furthermore, with Cosmos DB we have more than just SDKs, and in the web console we have CLI tools and a REST API and Powershell. </p>
<p>Using the Azure CLI tool we can do all of the things we did with the web console in the previous lessons. Now see the links below for the complete command reference. We’re not gonna go through every command. There is also a nice set of BASH scripts in that link that make use of the CLI tool to set up and manage your databases. They have them throughout the Azure documentation. With Powershell we can do everything from the CLI as well as scripting. So if you’re comfortable writing Powershell code you can do many of the same things as you would in BASH scripts, only it’ll probably require fewer lines of code. And then finally you have the REST API. This lets you do everything your own way with any programming language that can make HTTP requests or you can just use curl, if it pleases you. So as we can see from this section, Cosmos DB is both very powerful and very flexible. We can do most of our setup, configuration, and maintenance from the web console or using command line tools. When it comes time for our software to read and write data we have options for many languages and workarounds for unusual use cases. All of this surely sounds great, and by now you have enough theoretical understanding to dive in, build your own little app with Cosmos DB. However, to help ensure you start strong in section three we’re gonna walk through <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/app-creation-with-cosmos-db/">building an application</a>, using <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> services, and Cosmos DB. It’s gonna be a blast, see you there.</p>
<h1 id="Introduction-to-Creating-an-App-with-Cosmos-DB"><a href="#Introduction-to-Creating-an-App-with-Cosmos-DB" class="headerlink" title="Introduction to Creating an App with Cosmos DB"></a>Introduction to Creating an App with Cosmos DB</h1><p>By now, you should have a pretty strong theoretical understanding of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a>. We walked through its feature set, explained its <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/azure-cosmos-db-api/">API</a>, demonstrated its tooling and generally covered how the technology can serve as a backend for a variety of use cases. What we have not done is actually tried to build something so that we can see all this cool stuff in action. In this final section of the course, we’re gonna do just that. We’re gonna stop being polite and start getting real. </p>
<p>This isn’t a Cloud Academy lab, so you won’t actually need to create anything in a <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a> account to follow along. Instead, we’ll just very carefully cover step-by-step how to create a working application backend using Cosmos DB. Now, to keep things simple, we’re not gonna create a frontend. The app we create will be a simple event processing system. It’ll take in event data using Azure Event Hub, transmit the events to an Azure Function where we can perform transformations, and then finally, save everything to Cosmos DB. The setup for Azure Event Hub and Azure Functions will be minimal. We will include some screenshots and explanation, but we won’t focus too much on those systems since they’re out of scope for this class. </p>
<p>There are other classes if you wanna dig into those tools. You’re welcome to skip those short lessons if you don’t feel like they’re of use to you. The meat of this section will be the creating our app backend and validating our app lessons where we will walk through the Cosmos DB endpoint setup and then run some test data through <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/azure-event-ingestion/">Event Hub</a> to see if it actually persisted in Cosmos DB. Again, you don’t actually need to do any of this yourself, but if you have access to an Azure account, it’s a great way to reinforce all that we have covered. So if you’re ready, let’s get to it.</p>
<h1 id="Event-Ingestion-in-Azure"><a href="#Event-Ingestion-in-Azure" class="headerlink" title="Event Ingestion in Azure"></a>Event Ingestion in Azure</h1><p>Okay. Welcome to our Cloud Academy demo for this course on Cosmos DB in Azure. And now for this demo section, there’ll be multiple videos and we’re going to put together a backend application using Azure Event Hubs as your functions and as your Cosmos DB.</p>
<p>So for this first video, we’re gonna start by creating an Event Hub. Now what you see here on the screen, this is my personal account. There’s some test infrastructure here. That’s pretty recent. And we’re gonna run through how to create this piece, the Event Hub.</p>
<p>Now, if we just click Event Hubs, we’ll see here it takes us first to this section, which these are namespaces. You have to first create a namespace for your Event Hub. So if you click on it here, we have this Bethune namespace. This is my last name. And then there is a bethune Event Hub, and we can create click plus to create an additional Event Hub within that. But let’s start from scratch.</p>
<p>Let’s say we don’t have any namespace at all. We can go through the, create a resource menu here. And if you want, you can actually just type Event Hubs if you can’t find it and you can click create, and it’ll take you to the wizard and it will tell you first to create a namespace.</p>
<p>Now there’s some information you fill out here. We’ll stick with pay as you go, resource group, we have an existing one, but let’s say we wanna create a new one. We’ll call it, let’s create a new one and we’ll call it bethune2 and so we have a new resource group that will be for this namespace. And then the namespace name we’ll also just call that bethune2.</p>
<p>We wanna pick a region for the data center. I think East US is fine. We’ll stick with the standard pricing tier we can go with, let’s say one consumer group is enough for us and throughput units one is fine for a test. And then we can just go to review and create.</p>
<p>Now after you click review and create, it’s not actually gonna create it right away, it’s gonna just do validation. So don’t navigate away, it’s just done the validation piece. So we know that this configuration is okay, and then we have to click create. And now it’s actually deploying the infrastructure for our Event Hub namespace. That doesn’t take very long, but we have to, we’ll give it a minute and we’ll refresh. Okay.</p>
<p>So eventually your deployment for the Event Hub namespace will complete. It’ll get to this point and we can just click on, go to resource and we can see here at bethune2 Event Hub namespace, and we can navigate there from the home menu as well.</p>
<p>If we click on Event Hubs and we’ll see there’s the additional namespace right there, it’s still in status activating, but that’ll be done very soon. So at this point, we don’t have an actual Event Hub yet. We just have a namespace. So to create an Event Hub, we click on the namespace bethune2 that we created and we want to do we wanna add an Event Hub we do plus Event Hub. And this is pretty simple as well. We just have to give it a name. We’ll call this as well bethune2 partition count of two is fine. Just for test purposes. We don’t need to worry about message retention or capture at this point.</p>
<p>So we’ll just click create and that’ll trigger the validation. And it’ll take a second, but eventually, we’ll see spin up right there bethune2. Down there, we can see the Event Hub. These create very, very quickly, but after you’ve done that, congrats, you’re done. You have your ready-to-use Event Hub right here. No messages or traffic yet, but we’ll get to that soon enough.</p>
<h1 id="Creating-our-App-Backend-with-Cosmos-DB"><a href="#Creating-our-App-Backend-with-Cosmos-DB" class="headerlink" title="Creating our App Backend with Cosmos DB"></a>Creating our App Backend with Cosmos DB</h1><p>Okay, so now in this next demonstration, we’re gonna go through the process of setting up an Azure Cosmos DB account. Now that we have a new event hub, namespace, and event hub, we need a backend to save data. And in the final section, we’ll do. In the section after this, we’ll talk about Azure functions, something to actually process data.</p>
<p>So we have here a sample account, bethune. Now I’ll walk through the steps of how we created it. Similarly, we can do the Create a Resource wizard if we want, and we can just search for Cosmos DB. And so we click on create, and again, we just go, to the fill in the information. We’re gonna go through the same resource group we just created before, bethune2, we’ll say pay as you go and a name for the account. Let’s just keep it as bethune2.</p>
<p>API, this is very important. As you know, Cosmos DB can support many different data structures and API types. So we can do Cassandra. We can do Gremlin. We’re going to just stick with the SQL for this demo. We’ll keep it, keep it simple.</p>
<p>We’ll do the provision throughput capacity mode here. There’s a serverless version in preview mode as of this recording but we’ll stick with the provision for the location. Let’s keep it in the same place as our event, hub US East. And then there’s some additional configuration options here. We don’t really need any of this stuff. Multi-region geo-redundancy. This is, if you want a greater, greater resilience and more robust deployment, we don’t really need to worry about this stuff.</p>
<p>So review and create, and as before it’ll do a validation first. So it’s valid. There’s nothing wrong with this config. We then click create, and this will just take a moment to deploy. So the account is in progress and it’s important to remember once this is done, you don’t have a database yet. You just have the account. So you have to actually create the actual schema, the collections and, you know, DB instances manually, or you can actually have your software created. You can have an Azure function, or you could submit a SQL query. There’s many ways to, to create the schema afterward, but here we can see a deployment is in progress. </p>
<p>Usually only takes about a minute or so, and we can see, well, if we click refresh, we’ll see if it’s making any progress. Alright, well, let’s, let’s give this a sec. We’ll come back when this is done.</p>
<p>Okay. So it took a few minutes longer than I thought, but it did eventually complete. We have here our Azure Cosmos DB bethune2 account, which successfully finished. So what we can do is now actually look at the data, although there’s no data there, I’ll, I’m gonna show you how we can create a database, which will be our, our storage for our application.</p>
<p>So there’s the bethune1 here, which was done for test purposes. This is bethune2 account that we just created. Now, if we go into it, there’s nothing there right now. So to create a database within the account, there’s a lot of ways you can do that. You can get with the API, you can get with it with the query. We’re going to go through the Data Explorer piece here. And in this window, you can see, they give us a few options. You can create a container, which is a sort of a fixed storage and then throughput unit. Or you can go through here to create the database directly, which will give us our initial schema. And remember, this is all SQL API.</p>
<p>So what we’ll do is we’ll, we’ll just create a database to start. We’ll give it a unique ID to call it. We’ll call it the database bethunetest, just to give it a slightly different name. You know, the, the throughput is fine. We can pick the default. We don’t really need to worry about scaling or anything right now. So we’ll just click okay for that. And in a few seconds we should have our database backend. Good to go. Fetching. Okay. There it is.</p>
<p>Now that’s coming up and we can see there’s no data in there. Now, if we go to this older one that does have data, we can see what it would look like. What, what we should hope to get. This one, bethune, has working data in it. I believe it’s under the bethune1 collection. Yeah, here we go.</p>
<p>So you can see here items. If you do have data, you’ll see, you know, a separate collection and you’ll see some items that will load. So this one has one unit there and this one, oh, this one actually has more of than this one under Bethune has more items, these message items here. So we’ll know that we’ve succeeded with our app. When we can go back to this one here that we just created, bethune2, and we should see a collection and items.</p>
<p>So in order to do that, we need to create an answer function, to process some data. And then we’re going to send some messages that will be captured by the event hub processed by the function and stored in our Cosmos DB backend. So next up is, Azure function, see you there.</p>
<h1 id="Azure-Functions-and-Cosmos-DB"><a href="#Azure-Functions-and-Cosmos-DB" class="headerlink" title="Azure Functions and Cosmos DB"></a>Azure Functions and Cosmos DB</h1><p>Okay, so now that we have a working Cosmos DB service and backend running and we have an Event Hub for receiving events and messages, the next thing we need is a Azure Function. That’s gonna be our actual service for processing the data we receive and we have an example here called bethune, and we’ll dig into the code and a little bit but before we do that, I wanna show how it is we set up an Azure Function app.</p>
<p>As with a lot of Azure services, there’s two parts essentially. There’s the shell, the Function app. And then there’s the code itself. The function that’s actually running. So as before we can just go through the create a resource wizard or Function app. We click Create. Pay as you go is fine. We want an existing resource group. We’ll put it in bethune2. We’ll name it bethune2 as well.</p>
<p>Our runtime stack, this is important. We’re gonna do Node.js ‘cause we wanna do a JavaScript app. And how we publish it here, we’re gonna just publish it as code. You can also save as a Docker Container. Version 12 is fine. Most recent default, Node.js version from the time of this recording. And we’ll stick to US East for our region.</p>
<p>There’s some additional configuration here. We go through our hosting. It’s gonna need a storage account so we’ll just, there isn’t an existing one in this namespace, in this account so we’ll create a new one by default. We’ll just call it new storage account, bethune, whatever, that’s fine. Operating system, Windows is okay. We’ll stick with the Windows default. And our plan type is serverless. We don’t need to pick one of the options, other options here.</p>
<p>There’s an app service plan and premium plan. If you know you need, you know, more storage and more resources. And then monitoring we can accept, well actually we’ll go with no. We don’t need to enable application insights. That’s more if we need more deep level granular monitoring. You can add that if you’d like, but for this tutorial we won’t need it. And for tags, we’re not gonna add any tags. That’s fine.</p>
<p>So we’ll go ahead and review and create. And as always, it’ll do a validation first. Config seems to be okay so we click Create and we’ll just give this a few moments and we will have our bethune2 Function app. Right? There we go. Deployment in progress, let’s give that a minute. Okay, all right.</p>
<p>So eventually the deployment will create, will complete and it shouldn’t take more than a minute or so. So once it’s finished, we can just click Go to resource, or we can find it from the dashboard. And you’ll see we now have a bethune2 Function app, and this is our sort of our container for our serverless Azure Function, our actual code.</p>
<p>So right now there’s not really any code executing, there’s just this account. So what we have to do is click on Functions and here is where we can go ahead and create our event processor. So we’ll click on Add, and this will take us through a pretty simple wizard to set something up.</p>
<p>Now, when you click New Function here, it’s gonna ask for the type of function, there’s a number of templates here. HTTP trigger, Timer trigger. These are the conditions that will determine when the code executes. So there’s many types of triggers here. For our purposes, we’re interested in this one, the Azure Event Hub trigger.</p>
<p>So with an Azure Event Hub trigger, what’ll happen is the code for the Azure Function will execute whenever a message or an event is received by that event hub. So we’ll go ahead and we’ll call this EventHubTrigger2, and then we will come up with some connection. We’ll have to create a new connection here.</p>
<p>So in order to create a connection, you’d need an existing event hub. Now fortunately we have an existing one. We just created the bethune2 event hub a little while ago. So we’ll use that. It’ll put in the rest of the config automatically. We’ll call it bethune2 connection. We’ll use the route manage access the route manage shared access key policy for that. Click OK. And now we have an event hub trigger for the event hub we created.</p>
<p>Event hub name, we’ll go ahead and call this bethune2 and we’ll give it the default consumer group. As you know, event hubs can have multiple consumer groups. So we click Create Function and this will, again, it’s not gonna give us the code for the function itself, but it will give us the configuration for the trigger. And that’s key for getting everything to be wired together properly. So just give this a second.</p>
<p>We can refresh it after a while and we should see, boom, there it is. There is our function EventHubTrigger2. And if we click on that, we can see here that the basic configuration, there’s a code here and there’s the integration.</p>
<p>Now the integration, we’ll start with that. This is defined by our trigger, by the type of function. Then we selected Event Hub. So we can see here eventHubsMessages is our trigger. And then this is where the actual function code is. I’ll come to that in a second here. But if we look at the trigger here, this is the configuration that we just set. And we can actually look at this as code in a little bit.</p>
<p>If you go to Code + Test, we can see. So here’s some default JavaScript code that’ll give us a message if we run it, we’ll try that in a second. But there’s another bit of code, this is function.json. This code is defined by the integration that we just looked at, which was the Azure Event Hub integration.</p>
<p>So we see here it’s a bindings config type eventHubTrigger. name, eventHubMessages, direction in. So this is, you know, if a message comes into the event hub, it’ll trigger this. Event Hub name, bethune2. The one we created and here’s our connection. Cardinality, many. I’ll come back to that in a bit, it deals with whether we’re dealing with individual messages or arrays of objects. And we care about the default consumer groups. So this function.json file is very important. This is our configuration for integrating with Event Hub.</p>
<p>Now the code, the actual JavaScript code is in this index.js file. And this doesn’t really do anything too fancy. If we look through it line by line, we can see it’s, it’s basically just logging some text here, context.log JavaScript eventhub trigger function called for message. It’ll print out this message object, and then context.log processed message giving us the message.</p>
<p>There’s a little for-loop here. It’ll go through every message in this object. It’s assuming that it’s receiving an array of messages. See, there’s that cardinality thing. It’s assuming that there’s multiple messages here. So we can do a test run of this if we’d like. We can just do Test and Run and click Run, and we can see it’ll connect to our function. And if we click Run we’ll see, it should just output some basic message text.</p>
<p>Okay, give that a moment. Welcome, we’re now connected to our streaming and we’ll do a quick run. So there’s 202 accepted, that’s good. And there it is, there’s our output right there. The same as the code, right? Processed message, message, right. Process to message, test message. That’s all it was setting as input, this test message here. And then we saw JavaScript eventhub trigger called, right. This is the output there and so it succeeded.</p>
<p>So we now have a working event hub triggered Azure Function with some default to JavaScript code. So now how do we integrate this with Cosmos DB? Well that’s our next challenge. Okay, so we have our function set up. We have some basic JavaScript code. We have our Event Hub trigger. So now we need to integrate with Cosmos DB and in order to do that, there’s a little bit more configuration we need to set up.</p>
<p>So what we’re gonna do is we’re gonna go back home and we’re gonna go back to our Cosmos DB account, bethune2, and we’re gonna go into the Data Explorer, and we’re gonna set up a container that will give us a collection within the database that we can use.</p>
<p>Now here in the Azure Function configuration, the main thing we need to do, if we go here we can say that, that was not set. We’ll go back to the integration and we can see here there’s no outputs defined. This is the main goal. We have to define an output so that the function knows to save its results or its output somewhere, and that’s going to be Cosmos DB.</p>
<p>Now what we need to do is we need to create a container within our bethune test DB. So we’ll click New Container. We’re gonna use this existing database and the container ID will be bethunecontainer. We’ll just call it bethune, we’ll call it bethune2container, why not. And then we will also give this a partition key. Let’s just call this testkey1, simple enough. So we have a partition key. This will help us ensure things work the right way. So click OK. And very quickly we will have a collection that we can use for storing the output of our Azure Function. All right, perfect.</p>
<p>So that’s already set and good to go. And if we click on the collection we’ll see there’s no data in it. There’s no items there. And hopefully if we do a test, we’ll see items show up. So let’s go back to our event hub and our Azure Function here, and we will add an output for this new collection.</p>
<p>Now the output, we have to pick Cosmos DB, that’s the output type we want. And then we get a few interesting bits of configuration here. This output document, that’s the parameter that we’re gonna care about in the actual code. And then the database name. We wanna use this bethune test database so we’ll put that in. And the collection we want to use would be the same as this container name, bethune2container.</p>
<p>One of the nice things is there’s this feature here called if true, create if doesn’t exist, basically. It means that if you don’t have this container or if you don’t have this database or collection existing, then the function will create it for you. So we can leave that as yes, you can leave it as no if you don’t want it automatically doing that. But what we have to also do is create a connection to the Cosmos DB account.</p>
<p>What we’ll do is we’ll click New and want to connect to the bethune2 database. We click OK. And we’ll have this connection here. Actually they’re already is an existing connection we can use, but in any event, in this part here, as long as you already have a Cosmos DB account, you should be able to just find, fill it in here and put in your connection. And then for the partition key, the partition key we had before here in this container we saw was testkey1. So we can literally just copy that and paste it and click OK.</p>
<p>So now we have an output. Now we actually have an output for our function. So how can we test this? How can we make sure that it works appropriately? So we’re not gonna test the event hub piece. We just want to test that the function can output into Cosmos DB. Well, there’s a very simple way to test that.</p>
<p>Okay, so now that we have the output defined for Cosmos DB, in order to test that it works, we have to make two changes to our function set up here. We’re gonna make a change to the JavaScript code, and we’re gonna make a change to the event hub integration piece. So the integration piece here, we can change it by just going through the wizard. You can also change the JSON which I’ll show in a little bit, but basically we wanna go to this cardinality section and change this to one.</p>
<p>Now cardinality tells the code what type of object to expect, whether we’re dealing with a one-to-one or many to one relationship. What we want is to deal with a single message object. We don’t want an array of messages to iterate through. So we change that to one, that we click Save, and then we’re gonna go, and we’re going to edit our JavaScript code, we can click here. And actually before I go into the JavaScript code, I’ll just show you the change in the function.json, we can see that this code has changed.</p>
<p>We’ve added this output section for the Cosmos DB integration. We have the parameter name for the output document here for what will actually go into Cosmos DB. We have the config for the database name, the collection, create if not exist. The string for the connection and then the partition key and the type direction. So this has all changed. And of course here, the cardinality has changed.</p>
<p>It now says one instead of many. So, you know, we can update our integration config here either using the wizard by clicking on it and we have the little boxes or we can edit this function.json file and save it. So we go to our JavaScript code, index.json, you can see that the problem here with this code is that it’s expecting an array, it wants to do a for-loop for each message on this eventHubMessages object, it wants to log them all. And what we’re gonna do is instead update the code to expect a single object, a single thing instead of an array. So we’ll just paste in some different codes so you don’t have to hear me type it all out. And really all we’ve changed is we’re still doing the logging here, but there’s no longer a for-loop.</p>
<p>So we’re taking in a single object. We have this processed output. And then this line is important. We have here the output document parameter. This is what’s gonna go into Cosmos DB. So we call this output document and we call a JSON.stringify to make sure we get a string. And this is the thing that will go into Cosmos DB. So what we’ll do is we will save this and we will do a quick test run to see if it works.</p>
<p>So we’ll click Test. Now if we go into Cosmos DB now we can see in the bethune2container collection. If we click on items there’s nothing there, right? There’s no items for us now. So if our code works, we should see something show up there. So we have a little silly message there, a test message banana face. We know for sure that that’s us. And so we just run, click Run here and cross fingers. Hopefully we should see this execute without any failures. And there we go.</p>
<p>Okay it executed. And now let’s see if this actually went into Cosmos DB, we’ll click on Items. And there it is, there’s our message, Test Message banana face. So we know that our function works in terms of the output piece. We know that we can have the Azure function connect to Cosmos DB and save messages in a collection of our choice in the database of our choice.</p>
<p>Now the last thing we need to do is test the event hub piece. We wanna see the whole thing work end to end. We want to see a message go into event hub, and then that trigger the Azure function. And then after that function is triggered, it should then save it into Cosmos DB. We know this Cosmos DB part works, but we wanna see the event hub part as well.</p>
<p>So that’s what we’ll do in the last section of this demo. It should be fun.</p>
<h1 id="Validating-Our-App"><a href="#Validating-Our-App" class="headerlink" title="Validating Our App"></a>Validating Our App</h1><p>Okay, so now we’re coming into the final section of our demo. We’re going to do an end-to-end validation of our app. What we did before with Cosmos DB, we were able to send a test message through the Azure function dashboard, and we were able to see it persisted in Cosmos DB. So that’s good, but what we really want to see is that the Event Hub works is able to capture a message and then that will trigger the Azure function, and then it will be persisted in Cosmos DB.</p>
<p>So we wanna see all three steps. Now, in order to do that we have to send a message to our Event Hub. And there’s a lot of ways we can do this. If you’re familiar with PowerShell or command line, if you can use a Python Repl, you can write some code. you could send a curl command with the right authentication and all that. But these aren’t the most user-friendly approaches if you are not, if you don’t have experience in programming.</p>
<p>So if you’re not an experienced programmer, then I would say the easiest thing to do is to just use Visual Studio and the SampleSender library that you could get from Microsoft GitHub. So we’ll give you <a target="_blank" rel="noopener" href="https://github.com/Azure/azure-event-hubs/tree/master/samples/DotNet/Microsoft.Azure.EventHubs/SampleSender">links</a> for that, downloading and installing Visual Studio is not too difficult. You can do it right, obviously on Windows works great or other platforms. And then the SampleSender library, you’ll just download that from GitHub. And once you have it, you’re just gonna open it, you’re gonna go into you’re gonna wanna open a project solution yeah that’s correct. You can look for an SLN file and that should just pop up in SampleSender directory. And you’re gonna want here SampleSender.SLN, okay. And once you have that, you’re ready to go. You’re basically ready to send your messages, although actually sorry, you’re not quite ready to go.</p>
<p>You have to change two things here. You have to put in the right name for your Event Hub and you have to put in the right authentication, the right connection string. This is the credential it’s using it. And right now it has the old ones for my previous test. So we have to get the correct name and connection string. So we have to figure out where can we get that from.</p>
<p>Well, you can get it from the dashboard and I’ll show you how to do that in one sec. Okay, so in order to get the connection string, we’re gonna go into the dashboard and I’ll show you how that’s, how you obtain it. First, we can change this to bethune2, because we know already the Event Hub name is bethune2, that’s in the dashboard.</p>
<p>If we click on bethune2 namespace, we will see here, this is the Event Hub itself bethune2. Now that connection string, in order to get that, we need to have a policy, a connection policy that will generate that credential. So in here where it says shared access policies, there’s none by default. What we can do is we click add, and then this will let us create a policy. We’ll just call it default. And we’ll say that this is for sending and for listening, we don’t need to give it management permission. I mean we can, it doesn’t really matter. This is just for a test, but for this one, we’ll go ahead and click create, we’ll call it default and we’ll see it should generate those connection strings if we… Okay yeah already created it.</p>
<p>So we click here and then here are the strings that we need. There’s a primary key, secondary key, connection string, a secondary key here. We care about this one connection string, primary key. We wanna copy this to our clipboard. And then we go into the code and we’re gonna get rid of this one here. We don’t need this anymore. This is the old one, and then we’ll just paste in the new one and there you go. And so now we have our credentials set properly.</p>
<p>So the last thing to do is so we’ll save it. We’ll make sure that this is all set there. The last thing we have to do is actually run this code and we should see the messages appear in the Cosmos DB backend.</p>
<p>Now just to show that they’re not already there as a some kind of slight of hand trick here, we’ll go into Cosmos DB now. We’ll go into our Data Explorer and we should see as it connects, if we go into the collection, we should see that there’s only maybe one or two items from when we ran the tests with the Azure function.</p>
<p>So if we go here and then go to this container, we click on items. It’s only loading that one silly test message we have here, this banana face thing. So there’s nothing else there. Cause this is selecting everything, it’s not finding anything. So if we run this code and it succeeds we should see more messages propagate.</p>
<p>So, let’s see if it works. We click on this green arrow here to run the code samplecenter, and it should start spinning everything up. We’ll see a terminal pop-up and there we go. All right, our code is executing and it should send exactly 100 messages. They’re all just small string that says the message and the number. And then if everything is working properly, these messages will hit the Event Hub trigger the Azure function, and then the function output will put them into Cosmos DB.</p>
<p>So we’ll go ahead and click anything here. Continuous X out I suppose and then we’ll close this. Now one thing we can do to validate the traffic is we can actually look at the Event Hub logs as well. We can go here to bethune2 and look at this bethune2 Event Hub. And this will tell us if any traffic or messages are going through.</p>
<p>Now it’ll take a while for them to show up. We don’t see it right away. We can also look at the, at the Azure function. We can see the logs for the Azure function. We can see if it’s executing at all. So we can go to our bethune2 function app here, and we can see if we go to our functions, we can see here this one, and we can look at the monitor for it. Well actually we don’t need the insights monitor. We can actually this, we didn’t configure application insights so we can just go to the activity log and it should tell us if anything is happening.</p>
<p>So we’re not seeing anything yet, but let’s check Cosmos DB. Let’s see if there’s any activity there. So we will go ahead and reload that and give it a moment. Close this, oh sorry one sec. Let’s go back to that Cosmos DB and we’ll see if we have any messages coming through. Oh, I hope this works all right, we’re connecting to Cosmos DB and we’ll go to our bethune test database container. We’ll click on items, drum roll please. And there it is, there are all the messages that we sent.</p>
<p>So the messages are now in Cosmos DB, which means that the function executed properly. Now we’re not seeing them in the activity logs here. It might take a minute or so for it to show up because we don’t have any advanced insights configured just yet. Actually yes, we are seeing it now.</p>
<p>So if we look at the overview, we could see this spike in activity for the function app. This shows us that the function executed, and then we may see something similar in the Event Hub. If we look at throughput, we might we should see requested messages. Well, it might take a moment for that to propagate, but the most important thing is here are the actual messages are there.</p>
<p>So now we know that the code worked successfully. Our Cosmos DB, Event hub, and Azure function app is working as designed. So congrats if you’re able to get this working on your own and thanks for playing along, cheers.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Congratulations, you made it. Give yourself a pat on the back, because it’s been a long and tough ride. We went through a lot of pretty dense material, so before we pop the champagne bottles, let’s take a minute to briefly review what we have accomplished. By completing this course, you should now have a working knowledge of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/introduction/">Cosmos DB</a>. You should understand the basic service and its features, you should know how to make use of it with an Azure account, and you should have a pretty solid idea of how to integrate Cosmos DB with a real-world application.</p>
<p>Recall our three learning objectives. Number one, the student will have a basic understanding of the Cosmos DB technology, including its feature set and design philosophy. We covered that in section one where we talked about Cosmos DB’s history, its unique capabilities, and the general architecture and design. Number two, the student will know how to use Cosmos DB via its APIs, CLI tools, and the Azure web console. Section two went over this pretty thoroughly. We walked through setting up Cosmos DB in the web console using Azure data explorer. We also talked about how to start coding with Cosmos DB SDKs and how to use both PowerShell and Azure’s CLI tool with Cosmos DB. And then number three, students will have a practical understanding of how to integrate Cosmos DB with other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> services with the goal of creating a working app. This was section three where we introduced a few other Azure services and explained how to make them work with Cosmos DB. We walked through creating an app backend and even showed how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-azure-cosmos-db/validating-app-cosmos-db/">validate everything</a>. You should now be ready to work with Cosmos DB, both at work and in your side projects. </p>
<p>Now, remember, practice makes perfect. The best way to really solidify your knowledge is to actually go build something, so get out there and make some magic. Now that you’re done, I’d like to invite you to send any feedback you have about the course to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. We greatly appreciate your comments, questions, and suggestions. Congratulations again on fighting through the whole course, and good luck in your future endeavors.</p>
<h2 id="5Getting-Started-with-Cosmos-DB"><a href="#5Getting-Started-with-Cosmos-DB" class="headerlink" title="5Getting Started with Cosmos DB"></a>5<strong>Getting Started with Cosmos DB</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-gb/azure/cosmos-db/request-units">Cosmos DB Request Units</a></p>
<h2 id="11Validating-Our-App"><a href="#11Validating-Our-App" class="headerlink" title="11Validating Our App"></a>11<strong>Validating Our App</strong></h2><p><a target="_blank" rel="noopener" href="https://github.com/Azure/azure-event-hubs/tree/master/samples/DotNet/Microsoft.Azure.EventHubs/SampleSender">Microsoft GitHub repo</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22/" class="post-title-link" itemprop="url">AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:17:39 / Modified: 11:17:40" itemprop="dateCreated datePublished" datetime="2022-11-14T11:17:39-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Working-with-Azure-Storage-Using-PowerShell-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21/" class="post-title-link" itemprop="url">AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:15:42" itemprop="dateCreated datePublished" datetime="2022-11-14T11:15:42-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:33:16" itemprop="dateModified" datetime="2022-11-15T00:33:16-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata"><a href="#Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata" class="headerlink" title="Setting and Retrieving Azure Blob Storage Properties and Metadata"></a>Setting and Retrieving Azure Blob Storage Properties and Metadata</h1><p>Welcome to “Setting and Retrieving Azure Blob Storage Properties and Metadata”. I’m Guy Hummel. To get the most from this course, you should already have some knowledge of Azure Blob Storage. You should also have some experience with programming, especially C#.</p>
<p>In this short course, I’ll explain how to manage blob properties and metadata using the Azure Portal and the Azure Storage client libraries for .NET.</p>
<p>So what’s the difference between blob properties and metadata? Properties are automatically created by Azure, while metadata is user-defined. That is, metadata is optional, and you only create it if you need it.</p>
<p>Let’s have a look at some properties. I’m in the Azure Portal, and I’m looking at a blob container in a storage account. These are the blobs in the container. To see the system properties for one of them, you right-click on the blob and select “Properties”.</p>
<p>Some of the properties are pretty straightforward, such as Last Modified, Creation Time, and Size. You’ll notice that these properties can’t be modified, which makes sense because there’s only one true value for each of them. Some of the properties will change if you change the blob itself. For example, if you add more data to the blob, then its size will increase, or if you move the blob from the Hot tier to the Cool tier, then the access tier property will change.</p>
<p>You can see that there are a handful of properties that you can change, though. The first one only applies if this blob is configured to be cached in the Azure Content Delivery Network. The rest of them are things like content type and content language. Azure doesn’t necessarily know what these should be set to, so it allows you to set them. For example, you could set the content language to “en-us”, which means US English. Then, you’d click “Save”.</p>
<p>There are actually a few more you can set using the portal even though it doesn’t look like you can. These three lease properties can be set indirectly by clicking the “Acquire lease” button. If you apply a lease to a blob, then the blob can’t be modified by anyone else until the lease has expired. It’s a good way to prevent two people or programs from making conflicting changes to a blob.</p>
<p>When we click “Acquire lease”, it changes the lease status to “Locked”, the lease state to “Leased”, and the lease duration to “Infinite”. The duration is infinite because we didn’t set a time when the lease should expire. Of course, we weren’t given the option to set an expiration time, so we didn’t have a choice. To set one, we’d need to use the Azure Storage client libraries. To end the lease, we can click “Break lease”. Now, these three properties have changed again.</p>
<p>Okay, down here we can see a Metadata section. It’s empty because, as I mentioned, metadata is optional. Suppose we wanted to set a category for each of our documents. Then we’d create a key called “category”, and we’d set the value to the appropriate category for the document, such as “finance”. Then we’d click “Save”, and our key&#x2F;value pair would show up in the metadata list.</p>
<p>Using the Azure Portal to read and write blob properties and metadata is fine if you only need to change a few blobs, but if you need to make a lot of changes, then it would be easier to write a script. You could do that using either the Azure command-line interface or Azure PowerShell. If you have an application that needs to manage blob properties and metadata, then you can use Azure Storage client libraries. They’re available for a number of languages, but I’ll show you an example from Microsoft’s documentation that uses the .NET version.</p>
<p>This is an example C# method called SetBlobPropertiesAsync. It shows how to set the ContentType and ContentLanguage properties. First, it retrieves the existing properties by calling the GetPropertiesAsync method. It does this because when it creates a new BlobHttpHeaders class, it needs to set all of the properties for the blob, so it needs to know what to set the other properties to. By the way, even though it’s called BlobHttpHeaders, it just means BlobProperties.</p>
<p>First, it sets the ContentType to “text&#x2F;plain” and the ContentLanguage to “en-us”, which is the whole purpose of this method. Then it sets the remaining properties to the values it retrieved at the beginning. If it didn’t set these properties, then their existing values would be wiped out, so it has to set them.</p>
<p>Now it calls the SetHttpHeadersAsync method to actually set the properties of the blob to the values in the BlobHttpHeaders class that was created above. Then there’s a catch block to say what to do if the operation fails.</p>
<p>Okay, now how about setting blob metadata? It’s pretty similar. Here’s an example method called AddBlobMetadataAsync. First, it creates a Dictionary object called “metadata” that you can put key&#x2F;value pairs in.</p>
<p>Then it shows you two different ways to add a key&#x2F;value pair. The first is to use the Add method and pass it the key and value. In this example, it’s setting a key called “docType” to the value “textDocuments”, but you could set these to whatever you want because they’re user-defined.</p>
<p>The second way is to use this key&#x2F;value syntax where the key, called “category” in this example, is in square brackets, and the value (“guidance”, in this example) comes after the equal sign.</p>
<p>When you’re done adding key&#x2F;value pairs to the dictionary, you call the SetMetadataAsync method and pass it the dictionary object that contains the metadata. Finally, there’s a catch block to handle errors.</p>
<p>Now, I’ll show you one more code sample. This one shows you how to read metadata from a blob. Interestingly, you use the GetPropertiesAsync method, which seems weird considering we’re trying to get the metadata, not the properties. Well, this method actually gets both the properties and the metadata. We already saw how to read the properties from this in the first example, but to retrieve the metadata, you use a foreach loop to read all of the key&#x2F;value pairs from the Metadata dictionary.</p>
<p>And that’s it for setting and retrieving properties and metadata in Azure Blob Storage. Please give this course a rating, and if you have any questions or comments, please let us know. Thanks!</p>
<h2 id="1Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata"><a href="#1Setting-and-Retrieving-Azure-Blob-Storage-Properties-and-Metadata" class="headerlink" title="1Setting and Retrieving Azure Blob Storage Properties and Metadata"></a>1<strong>Setting and Retrieving Azure Blob Storage Properties and Metadata</strong></h2><p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-properties-metadata#set-and-retrieve-properties">Manage blob properties and metadata with .NET</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/" class="post-title-link" itemprop="url">AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:14:57" itemprop="dateCreated datePublished" datetime="2022-11-14T11:14:57-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:32:02" itemprop="dateModified" datetime="2022-11-15T00:32:02-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Configuring-Azure-Blob-Storage-Lifecycle-Management"><a href="#Configuring-Azure-Blob-Storage-Lifecycle-Management" class="headerlink" title="Configuring Azure Blob Storage Lifecycle Management"></a>Configuring Azure Blob Storage Lifecycle Management</h1><p>Welcome to “Configuring Azure Blob Storage Lifecycle Management”. I’m Guy Hummel. To get the most from this course, you should already have some knowledge of Azure Blob Storage. In this short course, I’ll explain how you can save money by setting up lifecycle management policies to automatically move your blobs to less expensive access tiers when certain conditions are met.</p>
<p>First, let’s review Azure Blob Storage access tiers. The Hot tier is designed for blobs that will be accessed frequently. This is the default tier. It has the most expensive storage costs but the lowest access costs.</p>
<p>The Cool tier has lower storage costs but higher access costs. That’s why you should only put infrequently accessed blobs in the Cool tier. If you were to put frequently accessed blobs in this tier, then the access costs would very quickly outweigh the lower storage costs. Also, you have to leave data in the Cool tier for at least 30 days. If you delete it or move it to a different tier in less than 30 days, you’ll have to pay an early removal penalty.</p>
<p>The Archive tier has the lowest storage costs and the highest access costs. It’s intended for data that you rarely need to access, such as long-term backups. To avoid an early deletion penalty, you need to leave data in the Archive tier for at least 180 days. It’s significantly different from the hot and cool tiers because you can’t access your data right away when you need it. That’s because it uses offline storage. So you have to wait for your data to be rehydrated before you can access it. This process can take up to 15 hours.</p>
<p>Since there’s a big difference in cost between the different tiers, it’s a good idea to set up an automated system to move your data between tiers when the time is right. For example, you might have monthly reports that get accessed frequently in the first month after they’re created, are accessed much less frequently for the next 11 months, and are rarely accessed after that but need to be retained for a total of 10 years for compliance reasons.</p>
<p>In this case, you might want to create a policy that:</p>
<ul>
<li>Moves these reports from the Hot tier to the Cool tier after 30 days,</li>
<li>Moves them from the Cool tier to the Archive tier when they’re 12 months old,</li>
<li>And deletes them from the Archive tier after they’re 10 years old.</li>
</ul>
<p>Notice that I said 12 months instead of 11 months for the second one because the blobs would have already been in the Hot tier for 1 month before being moved to the Cool tier, so they would be 12 months old. I’ll show you two different ways to set up these rules: using the Azure portal and using the command line.</p>
<p>First, let’s use the portal. I’ve already created a storage account and a blob container. In the menu on the left, select “Lifecycle management”. This is where we create the rules we want. Notice that it says lifecycle management is only available for general-purpose v2 accounts and blob storage accounts. Since most storage accounts are general-purpose v2 accounts these days, you probably won’t need to worry about having the right account type.</p>
<p>Now click “Add a rule”. Let’s call it “LifecycleForReports”. We’ll leave the rule scope as “Apply rule to all blobs in your storage account”. We’ll leave the blob type as just “Block blobs”. Append blobs are used for files that frequently have new data appended to them, which likely wouldn’t be the case for our monthly reports.</p>
<p>You might not be familiar with the blob subtype. If you turn on blob versioning, then a copy of the blob will be saved every time the blob is modified. That way, you could retrieve a previous version of the blob if you needed to. Snapshots are similar except that you create them manually. If you don’t use either versioning or snapshots, then you can just leave “Base blobs” checked.</p>
<p>Okay, now we can click “Next” and define the rule. A rule needs to contain one or more conditions. The first condition we want to have is to move blobs to Cool storage after 30 days, so let’s change this to “If base blobs were created more than 30 days ago, then move to cool storage. Then we click “Add conditions” again to add the next one, which is to move blobs from Cool storage to Archive storage when they’re 12 months old. So, we’ll put 365 days since they were created.</p>
<p>Finally, we’ll click “Add conditions” again, and this time, we’ll say that after 3,650 days (which is 10 years if you don’t count leap years), then delete the blob.</p>
<p>Now we click “Add” and we’re done. However, this lifecycle policy won’t go into effect immediately. Azure only runs policies once a day, so it can take up to 24 hours before any actions triggered by a new policy will take place.</p>
<p>This example was pretty simple, but there are some other features we can use to customize it. First, we could create a rule that only applies to certain blobs. To do this, we’d need to create a filter. Technically, when we set the blob type to only block blobs, that was a filter, but we can also create a filter that looks at the name of a blob.</p>
<p>For example, suppose we want a lifecycle rule to only apply to blobs that start with the word “report”, then we’d select “Limit blobs with filters”. Now there’s a tab called “Filter set”. There are two types of filters we can apply: blob prefix and blob index match. The second one is more complicated and requires tagging your blobs before it’ll work. Blob prefix is the one we need for our example. We just need to type “report” in this field. And we have to click the Update button. Now the rule will apply to any blobs that start with those letters.</p>
<p>Here’s another change we can make. Suppose you don’t want a blob to be moved to the Cool tier until a certain number of days after the last time it was accessed rather than since it was created. You might have noticed that this wasn’t an option in the list of possible conditions, so how would we do it? First, we have to check the “Enable access tracking” box. Then, when we go back to the first condition, the list includes “Last accessed”, so we can select it. And click “Update”.</p>
<p>This visual interface makes it very easy to create lifecycle rules, but if you had to apply lifecycle policies to lots of different storage accounts, it would probably be faster to create a standard configuration and apply it using the command line. Then you wouldn’t have to keep pointing and clicking in the interface over and over again.</p>
<p>To do this, you need to create a JSON file that contains your rules. But this can be a bit of a daunting task, so there’s a shortcut you can use. Once you’ve created one or more rules in this interface, you can go to the Code View tab, and there’s the JSON code you need.</p>
<p>The rule definition is divided into two sections: the rule actions and the rule filters. For each of the conditions we created, it shows the action to take and the condition required to take that action. Here’s the one for moving blobs to the Cool tier. Here’s the one for moving to the Archive tier, and here’s the one for deleting blobs. In the filters section, it shows the blob type filter and the prefix match filter that we set.</p>
<p>If we wanted to change anything, we could actually edit it right here and click the Save button, which would apply the updated policy to this storage account. But in most cases, we’d probably want to download the code, modify it, and use the command line to apply it to other storage accounts.</p>
<p>So, we’ll click “Download”. Then, to avoid having to install the Azure command-line utility on your desktop, we can upload the JSON file to the Cloud Shell and run the command there. So, I’ll open the Cloud Shell. Then I’ll select “Upload”. Here’s the file we downloaded. It’s called “policy.json”.</p>
<p>I’m not going to modify anything, but I’ll show you what command you’d use to apply this lifecycle policy to a storage account. I put the command in the transcript below in case you want to try this yourself.</p>
<p>It starts with “az storage account management-policy create”. Then you specify the account name. My storage account is called “camonthlyreports”. Then you type “–policy” and the name of the JSON file. And finally, you tell it the resource group where your storage account resides. I called mine “camonthlyreportsrg”. Now hit Enter. It might not be obvious from this output, but it successfully applied the policy to that storage account. </p>
<p>And that’s it for Blob Storage lifecycle management. Please give this course a rating, and if you have any questions or comments, please let us know. Thanks!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/109/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/109/">109</a><span class="page-number current">110</span><a class="page-number" href="/page/111/">111</a><span class="space">&hellip;</span><a class="page-number" href="/page/244/">244</a><a class="extend next" rel="next" href="/page/111/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2432</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
