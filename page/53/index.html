<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/53/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/53/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-3/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-18 22:13:03 / Modified: 22:13:06" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:03-04:00">2022-11-18</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Compute-SAA-C03-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:02" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:02-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-22 12:08:54" itemprop="dateModified" datetime="2022-11-22T12:08:54-04:00">2022-11-22</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Compute-SAA-C03-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Compute-SAA-C03-Introduction"><a href="#Compute-SAA-C03-Introduction" class="headerlink" title="Compute (SAA-C03) Introduction"></a>Compute (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on compute in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the various compute services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#64;&#99;&#108;&#111;&#117;&#x64;&#97;&#99;&#97;&#100;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;">&#x73;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#64;&#99;&#108;&#111;&#117;&#x64;&#97;&#99;&#97;&#100;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various compute services in AWS in preparation for the exam.</p>
<p>The objective of this course is to provide an introduction to compute services in AWS for solution architects, including:</p>
<ul>
<li>Elastic Compute Cloud, or EC2;</li>
<li>The Elastic Container Service, also known as ECS;</li>
<li>The Elastic Container Registry, or ECR;</li>
<li>The Elastic Container Service for Kubernetes, known as EKS;</li>
<li>AWS Elastic Beanstalk;</li>
<li>AWS Lambda; and</li>
<li>AWS Batch.</li>
</ul>
<p>We’ll also introduce the concept of Elastic Load Balancing, along with the different types of load balancers you can provision within the AWS Cloud. We’ll discuss EC2 Auto Scaling and see how it works together with Elastic Load Balancing to help you build robust, highly available web applications. And finally we’ll discuss options for on-premises hybrid cloud architectures using AWS Outposts and VMware Cloud on AWS.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#117;&#x70;&#x70;&#x6f;&#114;&#116;&#64;&#x63;&#108;&#x6f;&#x75;&#x64;&#97;&#x63;&#97;&#100;&#x65;&#x6d;&#x79;&#46;&#x63;&#x6f;&#x6d;">&#115;&#117;&#x70;&#x70;&#x6f;&#114;&#116;&#64;&#x63;&#108;&#x6f;&#x75;&#x64;&#97;&#x63;&#97;&#100;&#x65;&#x6d;&#x79;&#46;&#x63;&#x6f;&#x6d;</a>. Thank you!</p>
<h1 id="What-is-Compute"><a href="#What-is-Compute" class="headerlink" title="What is Compute?"></a>What is Compute?</h1><p>Hello, and welcome to this very short lecture where we are going to answer the question, what is Compute in AWS? Before we begin to explore Compute services, resources and features, we must first understand what is meant by the term Compute. So what is it? </p>
<p>Put simply, Compute resources can be considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions. So essentially Compute is closely related to common server components, which many of you will already be familiar with, such as CPUs and RAM. With that in mind, a physical server within a data center would be considered a Compute resource, as it may have multiple CPUs and many gigs of RAM to process instructions given by the operating system and applications. </p>
<p>Within AWS, there are a number of different services and features that offer Compute power to provide different functions. Some of these services provide Compute, which can comprise of utilizing hundreds of EC2 instances, or virtual servers, which may be used continuously for months or even years, processing millions upon millions of instructions. On the other end of this scale, you may only utilize a hew hundred milliseconds of Compute resource to execute just a few lines of code within AWS Lambda before relinquishing that Compute power. Compute resources can be consumed in different quantities, for different lengths of time across a range of categories, offering a wide scope of performance and benefit options. So it will really depend on your requirements as to which Compute resource you use within AWS. </p>
<p>As a quick high-level reference, AWS offers a Cloud Compute Index, which can be found using the <a target="_blank" rel="noopener" href="https://aws.amazon.com/products/compute/">link</a> onscreen. And this shows different examples and scenarios of where you might use different Compute deployment units. That brings me to the end of this very short lecture. Now we are aware of what Compute is, let’s start by looking at some of the services offered by AWS that provide this Compute resource, starting with Elastic Cloud Compute, EC2.</p>
<h1 id="Amazon-EC2"><a href="#Amazon-EC2" class="headerlink" title="Amazon EC2"></a>Amazon EC2</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Blog Post about Shared Responsibility Model and Security Groups</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/create-your-first-amazon-ec2-instance-1/">Lab: Create your first Amazon EC2 Instance (Linux)</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/">Lab: Ceate your first Amazon EC2 Instance (Windows)</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture where I will explain what the EC2 service is and does, and how to configure an EC2 instance, so let’s get started. As EC2 is one of the most common <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> services used within AWS. I will discuss this service in greater detail over the other services that we’ll cover in this course. </p>
<p>EC2 is arguably the first compute service that you will encounter when working with AWS. It allows you to deploy virtual servers within your AWS environment and most people will require an EC2 instance within their environment as a part of at least one of their solutions. There are a number of elements in creating your EC2 instance, which I want to break down and explain. This will hopefully help to define how the service works and answer a number of questions that you may have. The EC2 service can be broken down into the following components. Amazon machine images, AMIs, instant types, instance purchasing options, tenancy, user data, storage options and security. Let’s look at each of these individually. </p>
<p>The first point I want to cover are AMIs, Amazon Machine Images. These are essentially templates of pre-configured EC2 instances which allow you to quickly launch a new EC2 instance based on the configuration within the AMI. This prevents you from having to install an operating system or any other common applications that you might need to install on a number of other EC2 instances. From a high level perspective an AMI is an image baseline that will include an operating system and applications along with any custom configuration. AWS provides a large number of AMIs covering different operating systems from Linux to Red Hat to Microsoft Windows among others. when configuring your EC2 instance, selecting your AMI is the first configuration choice you’ll need to make. You can also create your own AMI images to help you speed up your own deployment. For example you would start with selecting an AWS AMI, let’s say a Linux server. And then once it is up and running you may then need to install a number of your own custom applications and make specific configuration changes. Now if you needed another server to perform the same functionality, you could go through the same process of selecting a Linux AWS AMI, and again manually installing the applications and making your configurations. Or once you have made those changes on the first instance you can then simply create a brand new AMI or template of that instance with all the applications installed and configurations already made. Then if you need another instance of the same configuration all you would need to do is to select your custom AMI as the base image for your instance and it will launch with Linux server, your custom applications already installed and any configurations already made. As you can see this has many benefits and certainly comes in useful when implementing auto scaling. </p>
<p>In addition to both AWS managed and your custom managed AMIs, you could also select an AMI from the AWS marketplace. The AWS marketplace is essentially an online store that allows you to purchase AMIs from trusted vendors like Cisco, Citrix, Alert Logic et cetera. These vendor AMIs may have specific applications and configurations already made, such as instances that are optimized with built-in security and monitoring tools or contained database migration systems. Lastly community AMIs also exists which are a repository of AMIs that have been created and shared by other AWS members. </p>
<p>Let’s now take a look at instance types, once you have selected your AMI from any of the different sources already discussed, you must then select an instance type. An instance type simply defines the size of the instance based on a number of different parameters, these being ECUs. This defines a number of EC2 compute units for instance, vCPUs this is the number of virtual CPUs on the instance. Physical processor, this is the process speed used on the instance. Clock speed, it’s clock speed in gigahertz. Memory, the amount of memory associated. Incident storage this is the capacity of the local instance store volumes available. EBS optimized available, this defines if the instance supports EBS optimized storage or not. Network performance, this shows the performance level of rate of data transfer. IPV6 support, this simply indicates if the instance type supports IPV6. Process architecture this shows the architected type of the processor. AES-NI, this stands for advanced encryption standard new instructions and it shows if the instance supports it for enhanced data protection. AVX this indicates if the instance supports AVX which is advanced vector extensions, which are primary used for applications focused on audio and video, scientific calculations and 3D modeling analysis. And finally Turbo which shows if the instance supports intel turbo boost and AMD turbo core technologies. The key parameters here to primarily be aware of for general usage of an EC2 instance, could be summarized as vCPUs and memory, instant storage and network performance. But obviously this really depends on your actual usage and application. Having this flexibility of variant instances allows you to select the most appropriate size or power of an instance that you need for optimal performance within your applications. These different instance types are categorized into different family types that offer distinct performance benefits, which again helps you to select the most appropriate instance for your needs. Within each of these instance families you will have a range of instant types with varied CPU, memory, storage and network performance et cetera. </p>
<p>These instance families can be summarized as follows. Micro instances, these instances have a low cost against them due to the minimal amount of CPU and memory power that they offer. These are ideal for very low throughput use cases such as low traffic websites. General-purpose, instance types within this family have a balanced mix of CPU memory and storage making them ideal for small to medium databases, tests and development servers and back-end servers. Compute optimized, as the name implies instance types within this family have a greater focus on compute. They have the highest performing processes installed allowing them to be used for high-performance front end servers, web servers, high-performance science and engineering applications and video encoding and batch processing. GPU, GPU stands for graphics processing unit. And so the instances within this family are optimized for graphic intensive applications. FPGA, this family of instances allows you to customize field programmable gate arrays. To create application specific hardware accelerations when used with applications that use massively parallel processing power such as genomics and financial computing. Memory optimized, this family include instance types that are primarily used for large-scale enterprise class in-memory applications, such as performing real time processing of unstructured data. They are also ideal for enterprise applications such as Microsoft SharePoint. These instances of the lowest cost per gigabyte of RAM against all other instance families. Storage optimized, as expected these are optimized for enhanced storage. Instances in this family use SSD backed instant storage for low latency and very high I&#x2F;O, input&#x2F;output performance, including very high IOPS which is input&#x2F;output operations per second. And these are great for analytic workloads and no SQL databases. Data file systems and log processing applications. </p>
<p>Instance purchasing options. You can purchase EC2 instances through a variety of different payment plans. These have been designed to help you save cost by selecting the most appropriate option for your deployment. The different EC2 payment options are as follows, on-demand instances, reserved instances, scheduled instances, spot instances and on-demand capacity reservations. It’s good to be aware of these different options as well having an understanding of these can help you save a considerable amount of money depending on your use case. Let me run through each option to help explain. Starting with on-demand instance</p>
<p>These are EC2 instances that you can launch at any time and have it provisioned and available to you within minutes. You can use this instance for a shorter time or for as long as you need before terminating the instance. These instances have a flat rate and is determined on the instance type selected and is paid by the second. On-demand instances are typically used for short term uses where workloads can be irregular and where workload can be interrupted. Many users of AWS use on-demand instances within their testing and development environments. And when you stop or terminate your on-demand instance you’ll stop paying for the compute resource. </p>
<p>Reserved instances allow you to purchase a discount for an incidents type with set criteria for a set period of time in return for a reduced cost compared to on-demand instances. This reduction can be as much as 75%. These reservations against instances must be purchased in either one or three year time frames. Further reductions can be achieved with reserved instances depending on which payment methods you select. There are three options available to you, firstly all upfront. The complete payment for the one or three year reservation is paid. And this offers the largest discount and no further payment is required regardless of the number of hours the instance is used. Partial upfront, here a smaller upfront payment is made and then a discount is applied to any hours used by the instance during the term. And finally no upfront, no upfront or partial payments are made and the smallest discount of the three models is applied to any hours used by the instance. Reserved instances are used for long-term predictable workloads allowing you to make full use of the cost savings to be had when using compute resources offered by EC2. </p>
<p>Scheduled instances, these are similar to reserved instances and the fact that you pay for the reservations of an instance on a recurring schedule, either daily, weekly or monthly. For example you might have a weekly task that is scheduled that performs some kind of bulk processing for a number of hours at the same time every week. With scheduled instances you could set up a scheduled instance to run during that set timeframe once a week. And this prevents you for having to use the on-demand instances which would incur a higher price. You should note that when using scheduled instances but even if you didn’t use the instance you would still be charged. This allows you to provision instances for scheduled workloads that are not continuously running. Which is where a reserved instance would be the preferred choice. </p>
<p>Spot instances allows you to bid for unused EC2 compute resources, however your resource is not guaranteed for a fixed period of time. To you to spot instance you must bid higher than the current spot price which is set by AWS. And this spot price fluctuates depending on supply and demand of the unused resource. If your bid price for an instance type is higher than the spot price, then you’ll purchase that instance. But as soon as your bid price becomes lower than the fluctuating spot price, you will be issued a two-minute warning before the instance automatically terminates and is removed from your AWS environment. The bonus for spot instances is that you can bid for large EC2 instances at a very low cost point saving a huge amount on cost. Due to the nature of how the instances can be suddenly removed from your environment, spot instances are only useful for processing data and applications that can be suddenly interrupted. Such as batch jobs and background processing of data. </p>
<p>Capacity reservations allows you to reserve capacity for your EC2 instances based on different attributes. Such as instance type, platform and tenancy et cetera. Within a particular availability zone for any period of time. This ensures that you always have the available number of instances you require within a specific availability zone immediately. This capacity reservation could also be used in conjunction with your reserved instances discount providing you additional savings. </p>
<p>Let me now talk to you about EC2 tenancy and this relates to what underlying host your EC2 instance will reside on. So essentially the physical server within an AWS data center. Again there are different options available to you with pros and cons to each. Shared tenancy, this option will launch your EC2 instance on any available host with the specified resources required for your selected instance type. Regardless of which other customers and users also have EC2 instances running on the same host, hence the share tenancy name. AWS implement advanced security mechanisms to prevent one EC2 instance from accessing another on the same host. How the security is applied and operated is out of scope of this course and it is maintained by AWS themselves. Dedicated tenancy, this includes both dedicated instances and dedicated hosts. Dedicated instances are hosted on hardware that no other customer can access. It can only be accessed by your own AWS account. You may be required to launch your instances as a dedicated instance due to internal security policies or external compliance controls. Dedicated instances do incur additional charges due to the fact you are preventing other customers from running EC2 instances on the same hardware and so there will likely be unused capacity remaining. However the hardware might be shared by other resources you have running in your own account. Dedicated host, a dedicated host is effectively the same as dedicated instances. However they offer additional visibility and control, how you can place your instances on the physical host. They also allow you to use your existing licenses, such as PA-VM license or Windows Server licenses et cetera. Using dedicated hosts give you the ability to use the same host for a number of instances that you want to launch and align with any compliance and regulatory requirements. If you don’t need to address any compliance or security issues that require dedicated tenancy, then I recommend using shared tenancy to reduce your overall costs. </p>
<p>User data, during the configuration of your EC2 instance there is a section called user data. Which allows you to enter commands that will run during the first boot cycle of the instance. This is a great way to automatically perform functions upon boot, such as to pull down any additional software you want installing from any software repositories you may have. You could also download and get the latest OS updates during boot. For example you could enter yum update dash y, for a Linux instance which will then update its own software automatically at the time of boot. Storage options, as a part the configuration when setting up an EC2 instance, you are asked to select and configure your storage requirements. </p>
<p>Selecting storage for your EC2 instance will depend on the instance selected, what you intend to use the instance for and how critical the data is. Storage for EC2 can be classified between two distinct categories, persistent storage and ephemeral storage. Ephemeral meaning temporary. Persistent storage is available by attaching elastic block storage EBS volumes. And a ephemeral storage is created by some EC2 instances themselves using a local storage on the underlying host known as instance back storage. Let’s look at each of these storage options in greater depth. EBS volumes are separate devices from the EC2 instance itself. And so it’s not physically attached like ephemeral storage is. EBS volumes are considered network attached storage devices which are then logically attached to the EC2 instance via the AWS network. This principle is not dissimilar to attaching an external hard disk to your home laptop or PC. With the external hard disk represent your EBS volume and your PC represents your EC2 instance. The data on EBS volumes are automatically replicated to other EBS volumes within the same availability zone for resiliency which is managed by AWS. You can disconnect an EBS volume from your EC2 instance and the data will remain intact. Allowing you to reattach it to another EC2 instance if required. You can also implement encryption on these volumes if needed and take backup snapshots of all the data on the volume to S3. EBS volumes can be created in different sizes again with different performance capabilities depending on your requirements. </p>
<p>Ephemeral storage or instance backed storage is the storage that is physically attached to the underlying host on which the EC2 instance resides on. Looking back at our previous example, this would be similar to your own laptop or PC’s hard disk. There is a difference here though, with AWS EC2 instances as soon as the instance is stopped or terminated all saved data on a ephemeral storage is lost. If you reboot your instance then the data will remain but not if you stop it. Therefore if you have data that you need to retain it is not recommended that you use instance backed storage for this data. Instead use EBS volumes for persistent data storage. Unlike EBS volumes you are unable to detach ephemeral instance store volumes from the instance. </p>
<p>Security, security is fundamental with any AWS deployment. As so I just want to highlight a couple of points relating specifically to EC2 security. Firstly and during creation of your EC2 instance you will be asked to select a security group for your instance. A security group is essentially an instance level firewall allowing you to restrict both ingress and egress traffic by specifying what traffic allowed to communicate with it. You can restrict this communication by source ports and protocols for both inbound and outbound communication. Your instances are then associated with this security group. More information on security groups can be found in my blog post, found here covering instance level security. At the very end of your EC2 instance creation, you will need to select an existing key pair or create and download a new one. But what is a key pair and what is it used for? A key pair, as the name implies, is made up of two components, a public key and a private key. The function of key pairs is to encrypt the login information for Linux and Windows EC2 instances. And then decrypt the same information allowing you to authenticate onto the instance. The public key encrypts data such as the username and password. For Windows instances, the private key is used to decrypt this data allowing you to gain access to the login credentials including the password. For Linux instances the private key is used to remotely connect onto the instance via SSH. The public key is held and kept by AWS, and the private key is your responsibility to keep and ensure that it is not lost or compromised. So going back to when you create your EC2 instance and a new key pair. You’re given the opportunity to download the key pair, once you have done this you must keep that file safe until you’re ready to log on to the associated EC2 instance. It’s worth noting that you can use the same key pair on multiple instances to save you managing multiple private keys. Do bear in mind however should the private key become compromised access could be gained to all the instances where that key pair was used. Once you have authenticated to the EC2 instance the first time, you can set up additional less privileged access controls such as local windows accounts allowing other users to connect and authenticate to or even utilize Microsoft Active Directory. One final point regarding security on your EC2 instance it is your responsibility to maintain and install the latest OS and security patches released by the OS vendor as dictated within the AWS shared responsibility model. More information on this can be found in this blog post. </p>
<p>We have now covered the main elements of the EC2 service that should hopefully allow you to get started by creating your first EC2 instance and selecting the most appropriate configuration for your needs. But to reiterate what we have covered and make it all fit together, I will demonstrate how to create a new EC2 instance from within the console, quickly highlighting the elements we have discussed as I go through. </p>
<p>Okay so I’m logged into my AWS management console, and to start with we need to go to EC2 which is under the compute category. Now this take us to the EC2 console and from here we can simply select launch instance. Now this is the first stage of the configuration where we have to select our Amazon Machine Image, AMI. And here are a number of AWS AMIs that they supply, covering Windows, Red Hat, Linux et cetera. On the left hand side there’s just a few other options if you’ve created any AMIs yourself that would be stored here. I mentioned the AWS marketplace earlier and here you can see lots of different AMIs from other suppliers such as Trend Micro, Juniper Networks, Barracuda et cetera. And also the community AMI as well. So let’s get started with the Quick Start and look at some of the AWS supplied AMIs and I’m just going to launch this Amazon Linux box. So I’ll select that as my AMI, now we get to choose instance type. And we can filter up here with different types of instance types, general purpose, computer optimized et cetera. Just going to leave it as all, and then down here we can see the different families and the types, the vCPUs, memory et cetera that each of these has. So I’m going to leave it on the T2 micro general purpose instance. </p>
<p>So I’m going to select next configure instance details. Now I have a number of options here, the number of instances that we want to launch. I’m just going to keep it as one. If we want to launch this as a spot instance then we can select this box here to do so. But I’m just going to create it as an on-demand instance. We can select VPC that we want to run it in and we’ll have different VPCs there. You can then select the subnet if you’d like. And if you’d like to auto-assign a public IP address. We can assign a role to an EC2 instance if we need to and we can also control the shutdown behavior. So when we shut down all EC2 instance, do you want to terminate that instance or just stop. I’m going to leave that as a default or stop. There’s a couple of other controls you can put in here, enable termination protection. And what that will do, that will prevent you from terminating your instance until you uncheck this box. And you can also enable detail cloud watch monitoring if you need to. With regards to tenancy we discussed this earlier rather we shared or one of the dedicated instance or on a dedicated host, I’m going to leave that as shared. I’ll leave all the other options as default and then I’ll click on next to go to storage. This shows the current storage that comes with the AMI. We consider there’s 8 gig in size and it’s a general-purpose SSD drive. It’s a tick box here to delete the volume on termination and we can see that it’s not encrypted. If we wanted to add a new volume, we can add an EBS volume here. Again we can specify the size, eight gig or if you want to add it to 30 for example. And then we can also again delete on termination and we have the option to encrypt the EBS volume if we wanted to it’s by selecting the default AWS EBS encryption key. So now we have two drives, one of them is the root volume and an additional drive which is an EBS volume. </p>
<p>Click on next add tags, here we can add a key value pair tag to this instance. So for example a key of name and a value of my instance. And we can add additional tags as well as you can see here we can add up to 50 tags if we wanted to. So we can add a project that it belongs to or the cost et cetera, any tags that make it more usable to you. Once you’ve finished adding your tags click on configure security group. And this is where we control what can and can’t access your instance. You can create a new secure group or select an existing security group that you might already have. If we create a new security group and call it My Security Group, then you can add a description. And here are the rules for the security group. So at the moment we have SSH using TCP across port 22 and can you have the source as a custom IP address range or a single IP address. So you might just want a specific subnet in your VPC to talk to this instance or you can have anywhere, or you can have just your own IP address. Let’s put in a custom IP address range of 10.0.1.0&#x2F;24. And again you can add a description in there if you want to. You can add a new rule, for example HTTP traffic. And again you can add your source, cite anywhere then once you’re happy with your security groups click on review and launch. And this just provides a summary of all the configuration options that you’ve made up to this point. If you need to edit any other details you can just click on the right hand side here, edit the instance type the security groups et cetera. Once you’re happy with all of your information click on launch. And this is where you can select or create a new key pair to connect your instance. </p>
<p>So let’s go ahead and create a new key pair, call it My Instance, and now I need to download this key pair. Which will download the private part, once that key pair is downloaded I can then click on launch instance. Now if we go back to our dashboard by clicking on view instances. We can see here that it’s trying to launch at the minute, the status is pending and it shouldn’t take too long for that to become active. And here we go we can now see it’s up and running. </p>
<p>Before I finish this demonstration I just want to point out one last point relating to status checks that we can see from within our EC2 dashboard. These status checks are used to check the health and status of your EC2 instance and understanding what kind of faults could trigger these checks to fail. Kinda help you troubleshoot issues with your EC2 resources. There are two types of status checks. System status checks and instant status checks. If the system status check fails then it is likely to be an issue with the underlying host rather than a configuration issue with your EC2 instance. Common issues that trigger system status checks to fail are loss of power, loss of network connectivity and hardware and software issues on the underlying host. Basically a system status check failure is out of our control as the fault lies with components that AWS are responsible for. The best way to resolve this will be to stop the instance and restart. This is likely to cause the instance to launch on another physical host resolving the problem. Do not reboot the instance as this will cause the instance to continue running on the same physical server. Instance data checks, these differ from system status checks as if this fails then it would likely require your input to help them resolve the issue. This check looks at the EC2 instance itself, rather than focusing on the underlying hosts. Common issues that trigger this checks to fail are incorrect network configuration, corrupted file systems, exhausted memory or incompatible kernel. These faults will require you to troubleshoot and resolve the issue, for example changing the network configuration. If you’d like some hands-on experience with EC2, then we do offer two labs in which you can practice creating your own EC2 instances for both Linux and Windows. </p>
<p>That now brings me to the end of this lecture on EC2, like I mentioned previously this is going to be the longest and most in-depth lecture, simply due to how much of a key compute service it is in a wide range of use cases.</p>
<h1 id="ECS-Elastic-Container-Service"><a href="#ECS-Elastic-Container-Service" class="headerlink" title="ECS - Elastic Container Service"></a>ECS - Elastic Container Service</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/">Introduction to Docker</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/basics-of-using-containers-in-production/">Basics of using Containers in Production</a></p>
<p><strong>Transcript</strong></p>
<p>Hello, and welcome to this short lecture which will provide a high-level overview of the Amazon EC2 Container Service, commonly known as Amazon ECS. This service allows you to run Docker-enabled applications packaged as containers across a cluster of EC2 instances without requiring you to manage a complex and administratively heavy cluster management system. The burden of managing your own cluster management system is abstracted with the Amazon ECS service by passing that responsibility over to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a>, specifically though the use of AWS Fargate. </p>
<p>If you’re new to some of these terms such as Docker, containers, and AWS Fargate then let me quickly, in a single sentence, define what they are to help you understand this service a little easier. AWS Fargate is an engine used to enable ECS to run containers without having to manage and provision instances and clusters for containers. Docker is piece of software that allows you to automate the installation and distribution of applications inside Linux Containers. So what are containers? A Container holds everything that an application requires to enable it to run from within it’s isolated container package. This may include system libraries, code, system tools, run time, etcetera. But it does not include an operating system like a virtual machine does, and so reduces overhead of the actual container itself. </p>
<p>Containers are decoupled from the underlying operating system, making Container applications very portable, lightweight, flexible, and scalable across a cloud environment. This ensures that the application will always run as expected regardless of it’s deployment location. With this in mind, if you are already using Docker, or have existing containerized applications packaged locally, then these will work seamlessly on Amazon ECS. For more information on Docker and Containers, please see our existing content found here. Let’s now take a deeper look at the EC2 Container Service and some of the additional functions that it provides. </p>
<p>As I mentioned before, EC2 Container Service removes the need for you to manage your own cluster management system thanks to its interactions with AWS Fargate. You don’t even have to specify which instance type to use. This can be very time consuming and requires a lot of overhead to continue to monitor and maintain and scale. With Amazon ECS there is no need to install any management software for your cluster, neither is there a need to install any monitoring software either. All of this, and more, is taken care of by the service, allowing you to focus on building great applications and deploying them across your scalable cluster. </p>
<p>When launching your ECS cluster you have the option of two different deployment models: a Fargate launch and an EC2 launch. The Fargate launch requires far less configuration and simply requires you to specify the CPU and memory required, define the networking and IAM policies in addition to you having to package your applications into containers. However, with an EC2 launch you have a far greater scope of customization and configurable parameters. For example, you are responsible for patching and scaling your instances, and you can specify which instance types you used, and how many containers should be in a cluster. </p>
<p>There are use cases for both modes. You may need more granularity and control with some of your clusters due to security and compliance controls. Monitoring is taken care of through the use of AWS CloudWatch, which will monitor metrics against your containers and your cluster. Those of you who have used CloudWatch before will be aware you can easily create alarms based off of these metrics providing you notification of when specific events occur such as your cluster size scaling up or down. An Amazon ECS cluster is comprised of a collection of EC2 instances. As such, some of the functionality and features that we’ve already discussed in this course can be used with these instances. For example Security Groups to implement instance level securely at a port and protocol level, along with Elastic Load Balancing and Auto Scaling. Although these EC2 instances form a cluster, they still operate in much the same way as a single EC2 instance. So again, for example, should you need to connect to one of your instances itself, you could still use the same familiar methods such as initiating an SSH connection. </p>
<p>The clusters themselves act as a resource pool, aggregating resources such as CPU and memory. The cluster is dynamically scalable, meaning you can start your cluster as a single small instance, but it can dynamically scale to thousands of larger instances. Multiple instance types can be used within the cluster if required. Although the cluster is dynamically scalable, it’s important to point out that it can only scale within a single region. Amazon ECS is region-specific, so it can span multiple availability zones, but it cannot span multiple regions. With ECS you can schedule your containers to be deployed across your cluster based on different requirements, such as resources requirements or specific availability requirements, through the use of multiple availability zones. The instances within the Amazon ECS cluster also have a Docker daemon and an ECS agent installed. These agents communicate with each other allowing Amazon ECS commands to be translated into Docker commands.</p>
<h1 id="ECR-Elastic-Container-Registry"><a href="#ECR-Elastic-Container-Registry" class="headerlink" title="ECR - Elastic Container Registry"></a>ECR - Elastic Container Registry</h1><h3 id="Resources-referenced-within-this-lecture"><a href="#Resources-referenced-within-this-lecture" class="headerlink" title="Resources referenced within this lecture:"></a><strong>Resources referenced within this lecture:</strong></h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Overview of AWS Identity &amp; Access Managment (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Docker Push</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">Docker Pull</a></p>
<h3 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a><strong>Transcript</strong></h3><p>Hello and welcome to this lecture covering the Elastic Container Registry service, known as ECR. This service links closely with the previous service discussed, the EC2 Container Service, as it provides a secure location to store and manage your docker images that can be distributed and deployed across your applications. </p>
<p>This is a fully managed service, and as a result, you do not need to provision any infrastructure to allow you to create this registry of docker images. This is all provisioned and managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a>. This service is primarily used by developers, allowing them to push, pull, and manage their library of docker images in a central and secure location. </p>
<p>To understand the service better, let’s look at some components used. These being, registry, authorization token, repository, repository policy, and image. Let’s take a look at the registry first. The ECR registry is the object that allows you to host and store your docker images in, as well as create image repositories. Within your AWS account, you will be provided with a default registry. When your registry is created, then by default, the URL for the registry is as follows:</p>
<p><a target="_blank" rel="noopener" href="https://aws_account_id.dkr.ecr.region.amazonaws.com/">https://aws_account_id.dkr.ecr.region.amazonaws.com</a></p>
<p>where you’ll need to replace the red text with your own information that is applicable to your account or medium. Your account will have both read and write access by default to any images you create within the registry and any repositories. Access to your registry and images can be controlled via IAM policies in addition to repository policies as well, to enforce tighter and stricter security controls. As the docker command line interface doesn’t support the different AWS authentication methods that are used, then before your docker client can access your registry, It needs to be authenticated as an AWS user, which will then allow your client to both push and pull images. And this is done by using an authorization token. To begin the authorization process to allow your docker client to communicate with the default registry, you can run the get login command using the AWS CLI, as shown:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws ecr get-login --region region --no-include-email</span><br></pre></td></tr></table></figure>

<p>where the red text should be replaced with your own region. This will then produce an output response, which will be a docker login command.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u AWS -p password https://aws_account_id.dkr.ecr.region.amazonaws.com</span><br></pre></td></tr></table></figure>

<p>You must then copy this command and paste it into your docker terminal which will then authenticate your client and associate a docker CLI to your default registry. This process produces an authorization token that can be used within the registry for 12 hours, at which point, you will need to re-authenticate by following the same process. The repository are objects within your registry that allow you to group together and secure different docker images. You can create multiple repositories with the registry, allowing you to organize and manage your docker images into different categories. </p>
<p>Using policies from both IAM and repository policies, you can assign permissions to each repository allowing specific users to perform certain actions, such as performing a push or pull IP line. As I just mentioned, you can control access to your repository and images using both IAM policies and repository policies. There are a number of different IAM managed policies to help you control access to ECR, these being the three shown on the screen.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AmazonEC2ContainerRegistryFullAccess</span><br><span class="line">AmazonEC2ContainerRegistryPowerUser</span><br><span class="line">AmazonEC2ContainerRegistryReadOnly</span><br></pre></td></tr></table></figure>

<p>For more information on IAM and policies, please refer to our system course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">here</a>, which covers IAM and policy creation and management. Repository policies are resource-based policies, which means you need to ensure you add a principle to the policy to determine who has access and what permissions they have. It’s important to be aware of that for an AWS user to gain access to the registry, they will require access to the ecr get authorization token API call. Once they have this access, repository policies can control what actions those users can perform on each of the repositories. These resource-based policies are created within ECR itself and within each other repositories that you have. Once you have configured your registry, repositories, and security controls, and authenticated your docker client with ECR, you can then begin storing your docker images in the required repositories, ready to then pull down again as and when required. </p>
<p>To push an image into ECR, you can use the docker push command, and to retrieve and image you can use the docker pull command. For more information on how to perform both a push and a pull of images, please see the following links.</p>
<p><strong>Docker Push</strong>: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</a></p>
<p><strong>Docker Pull</strong>: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html</a></p>
<p>That now brings me to the end of this lecture covering the Elastic Container Registry service. Coming up in the next lecture, I shall be looking at the Amazon <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/eks-elastic-container-service-kubernetes/">Elastic Container Service for Kubernetes</a>, known as EKS.</p>
<h1 id="EKS-Elastic-Container-Service-for-Kubernetes"><a href="#EKS-Elastic-Container-Service-for-Kubernetes" class="headerlink" title="EKS - Elastic Container Service for Kubernetes"></a>EKS - Elastic Container Service for Kubernetes</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-kubernetes/">Introduction to Kubernetes</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Install Kubectl</a></p>
<p>IAM Authenticator:</p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/aws-iam-authenticator">Linux</a></p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator">MacOS</a></p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/windows/amd64/aws-iam-authenticator.exe">Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">Configuration map to joing the Worker Node to the EKS Cluster</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">Introduction to EKS</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture covering the Elastic Container Service for Kubernetes, more commonly known as EKS.</p>
<p>Firstly, for those unfamiliar with Kubernetes let me briefly explain what it is at a high level.  Kubernetes is an open-source container orchestration tool designed to automate, deploy, scale, and operate containerized applications. It is designed to grow from tens, thousands, or even millions of containers. Kubernetes is also container-runtime agnostic, which means you can actually use Kubernetes to run rocket and docker containers.</p>
<p>So back to EKS, with EKS, AWS provides a managed service allowing you to run Kubernetes across your AWS infrastructure without having to take care of provisioning and running the Kubernetes management infrastructure in what’s referred to as the control plane. You, the AWS account owner, only need to provision and maintain the worker nodes.</p>
<p>What is a control plane and what are worker nodes?</p>
<p>Kubernetes Control Plane:</p>
<p>There are a number of different components that make up the control plane and these include a number of different APIs, the kubelet processes and the Kubernetes Master, and these dictate how kubernetes and your clusters communicate with each other.  The control plane itself is run across master nodes.</p>
<p>The control plane schedules containers onto nodes. The term scheduling does not refer to time in this context. Scheduling, in this case, refers to the decision process of placing containers onto nodes in accordance with their declared, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> requirements.  The Control Plane also tracks the state of all kubernetes objects by continually monitoring the objects. So in EKS, AWS is responsible for provisioning, scaling and managing the control plane and they do this by utilising multiple availability zones for additional resilience.</p>
<p>Worker nodes:</p>
<p>Kubernetes clusters are composed of nodes and the term cluster refers to the aggregate of all of the nodes.  A node is a worker machine in Kubernetes and runs as an on-demand EC2 instance and includes software to run containers managed by the Kubernetes control plane.  For each node created, a specific AMI is used which also ensures docker and kubelet in addition to the AWS IAM authenticator is installed for security controls. These nodes are what us as the customer are responsible for managing within EKS.  Once the worker nodes are provisioned they can then connect to EKS using an endpoint.</p>
<p>For more information on Kubernetes, please see our existing course ‘Introduction to Kubernetes’ here</p>
<p>Let me provide a brief overview of what’s required to start using the EKS service.</p>
<ol>
<li><p>Create an EKS Service Role: Before you begin working with EKS you need to configure and create an IAM service-role that allows EKS to provision and configure specific resources.  This role only needs to be created once and can be used for all other EKS clusters created going forward. The role needs to have the following permissions policies attached to the role: AmazonEKSServicePolicy and AmazonEKSClusterPolicy</p>
</li>
<li><p>Create an EKS Cluster VPC: Using AWS CloudFormation you need to create a and run a CloudFormation stack based on the following template: <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml</a> which will configure a new VPC for you to use with EKS</p>
</li>
<li><p>Install kubectl and the AWS-IAM-Authenticator: Kubectl is a command line utility for Kubernetes and can be installed following the details supplied here The IAM-Authenticator is required to authenticate with the EKS cluster.  Depending on your client OS (Linux, MacOS or Windows) it can be downloaded from here:</p>
</li>
<li><p>Create your EKS Cluster: Using the EKS console you can now create your EKS cluster using the details and information from the VPC created in step 1 and 2</p>
</li>
<li><p>Configure kubectl for EKS: Using the update-kubeconfig command via the AWS CLI you need to create a kubeconfig file for your EKS cluster</p>
</li>
<li><p>Provision and configure Worker Nodes: Once your EKS cluster shows an ‘Active’ status you can launch your worker nodes using CloudFormation based on the following template: <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml</a></p>
</li>
<li><p>Configure the Worker Node to join the EKS Cluster: Using a configuration map downloaded here:</p>
</li>
</ol>
<p>curl -O <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml</a></p>
<p>You must edit it and Replace the &lt;ARN of instance role (not instance profile)&gt; with the NodeInstanceRole value from step 6</p>
<p>Your EKS Cluster and worker nodes are now configured ready for your to deploy your applications with Kubernetes.</p>
<p>For more information on EKS, please see our existing course ‘Introduction to EKS’ which will cover these points and more in greater detail <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">https://cloudacademy.com/course/introduction-to-aws-eks/</a></p>
<h1 id="AWS-Elastic-Beanstalk"><a href="#AWS-Elastic-Beanstalk" class="headerlink" title="AWS Elastic Beanstalk"></a>AWS Elastic Beanstalk</h1><p>Hello and welcome to this lecture on AWS Elastic Beanstalk. AWS Elastic Beanstalk is an AWS-managed service that allows you to upload the code of your web application, along with the environment configurations, which will then allow Elastic Beanstalk to automatically provision and deploy the appropriate and necessary resources required within AWS to make the web application operational. These resources can include other AWS services and features, such as EC2, Auto Scaling, application health-monitoring, and Elastic Load Balancing, in addition to capacity provisioning. This automation and simplification makes it an ideal service for engineers who may not have the familiarity or the necessary skills within AWS to deploy, provision, monitor, and scale the correct environment themselves to run the developed applications. Instead, this responsibility is passed on to AWS Elastic Beanstalk to deploy the correct infrastructure to run the uploaded code. This provides a simple, effective, and quick solution to deploying your web application. </p>
<p>Once the application is up and running, you can continue to support and maintain the environment as you would with a custom-built environment. You can additionally perform some of the maintenance tasks from the Elastic Beanstalk dashboard itself. Elastic Beanstalk is able to operate with a variety of different platforms and programming languages, making it a very flexible service for your DevOps teams. Currently, at the time of writing this course, Elastic Beanstalk is compatible with the following. One important point to note is that the service itself is free to use. There is no cost associated with Elastic Beanstalk, however, any resources that are created on your application’s behalf, such as EC2 instances, you will be charged for as per the standard pricing policies at the time of deployment. </p>
<p>So now we know at a high level what AWS Elastic Beanstalk is and does, let me run through some of its core components that create the service. The application version. An application version is a very specific reference to a section of deployable code. The application version will point typically to S3, simple storage service, to where the deployable code may reside. </p>
<p>The environment. An environment refers to an application version that has been deployed on AWS resources. These resources are configured and provisioned by AWS Elastic Beanstalk. At this stage, the application is deployed as a solution and becomes operational within your environment. The environment is comprised of all the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code. </p>
<p>Environment configurations. An environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave. The environment tier. This component reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. If the application manages and handles HTTP requests, then the app will be run in a web server environment. If the application does not process HTTP requests and instead perhaps pulls data from an SQS queue, then it would run in a worker environment. I shall cover more on the differences between the web server and work environment shortly. </p>
<p>The configuration template. This is the template that provides the baseline for creating a new, unique environment configuration. Platform. The platform is a culmination of components in which you can build your application upon using Elastic Beanstalk. These comprise of the operating system of the instance, the programming language, the server type, web or application, and components of Elastic Beanstalk itself, and as a whole can be defined as a platform. Applications. Within Elastic Beanstalk, an application is a collection of different elements, such as environments, environment configurations, and application versions. In fact, you can have multiple application versions held within a single application. You can deploy your application across one of two different environment tiers, either the web server tier or the worker tier. </p>
<p>These tiers are configured differently depending on the use case of your application. The web server environment is typically used for standard web applications that operate and serve requests over HTTP port 80. This tier will typically use services and features such as Route 53, Elastic Load Balancing, Auto Scaling, EC2, and Security Groups. The worker environment is slightly different and are used by applications that will have a back-end processing task that will interact with AWS SQS, the Simple Queue Service. This tier typically uses the following AWS resources in this environment, an SQS Queue, an IAM Service Role, Auto Scaling, and EC2. </p>
<p>Now you are aware of some of the terminology and components, we can look at how AWS Elastic Beanstalk operates a very simple workflow process for your application deployment and ongoing management in what can be defined in four simple steps. Firstly, you create an application. Next, you must upload your application version of the application to Elastic Beanstalk, along with some additional configuration information regarding the application itself. This creates the environment configuration. The environment is then created by Elastic Beanstalk with the appropriate resources to run your code. Any management of your application can then take place, such as deploying new versions of your application. If the management of your applications has altered the environment configuration, then your environment will automatically be updated to reflect the new code should additional resources be required. For further information and to get some hands-on experience with AWS Elastic Beanstalk to deploy an application, take a look at our two labs which will guide you through the steps and processes we have discussed.</p>
<h1 id="AWS-Batch"><a href="#AWS-Batch" class="headerlink" title="AWS Batch"></a>AWS Batch</h1><p>Hello, and welcome to this lecture where I’ll provide a high level overview of AWS Batch. As the name suggests, this service is used to manage and run Batch computing workloads within AWS. Before we go any further, I just want to quickly clarify what Batch computing is. </p>
<p>Batch computing is primarily used in specialist use cases which require a vast amount of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> power across a cluster of compute resources to complete batch processing executing a series of jobs or tasks. Outside of a cloud environment, it can be very difficult to maintain and manage a batch computing system. It requires specific software and requires the ability to consume the resources required, which can be very costly. However, with AWS Batch, many of these constraints, administration activities and maintenance tasks are removed. You can seamlessly create a cluster of compute resources which is highly scalable, taking advantage of the elasticity if AWS, coping with any level of batch processing while optimizing the distribution of the workloads. All provisioning, monitoring, maintenance and management of the clusters themselves is taken care of by AWS, meaning there is no software to be installed by yourself. </p>
<p>There are effectively five components that make up AWS Batch service which will help you to start using the service, these being: Jobs. A job is classed as a unit of work that is to be run by AWS Batch. For example, this can be a Linux executable file, an application within an ECS cluster or a shell script. The jobs themselves run on EC2 instances as a containerized application. Each job can at any one time be in a number of different states, for example, submitted, pending, running, failed, among others. Job definitions. These define specific parameters for the jobs themselves. They dictate how the job will run and with what configuration. Some examples of these may be how many vCPUs to use for the container, which data volume should be used, which IAM role should be used, allowing access for AWS Batch to communicate with other AWS services, and mount points.</p>
<p>Job queues. Jobs that are scheduled are placed into a job queue until they run. It’s also possible to have multiple queues with different priorities if needed. One queue could be used for on-demand EC2 instances, and another queue could be used for the spot instances. Both on-demand and spot instances are supported by AWS Batch, allowing you to optimize cost, and AWS Batch can even bid on your behalf for those spot instances. </p>
<p>Job scheduling. The Job Scheduler takes control of when a job should be run and from which Compute Environment. Typically it will operate on a first-in-first-out basis, and it will look at the different job queues that you have configured, ensuring that higher priority queues are run first, assuming all dependencies of that job have been met. </p>
<p>Compute Environments. These are the environments containing the compute resources to carry out the job. The environment can be defined as managed or unmanaged. A managed environment means that the service itself will handle provisioning, scaling and termination of your Compute instances based on the configuration parameters that you would enter regarding the instance type, purchase method, such as on-demand or spot. This environment is then created as an Amazon ECS Cluster. Unmanaged environments are provisioned, managed and maintained by you, which gives greater customization. However, it does require greater administration and maintenance and also requires you to create the necessary Amazon ECS Cluster that the managed environment would have done on your behalf. </p>
<p>If you have a requirement to run multiple jobs in parallel using Batch computing, for example, to analyze financial risk models, perform media transcoding or engineering simulations, then AWS Batch would be a perfect solution.</p>
<h1 id="EC2-Auto-Scaling"><a href="#EC2-Auto-Scaling" class="headerlink" title="EC2 Auto Scaling"></a>EC2 Auto Scaling</h1><p>Hello and welcome to the first of the lectures that will be covering EC2 Auto Scaling. So what exactly is EC2 Auto Scaling? Put simply, Auto Scaling is a mechanism that automatically allows you to increase or decrease your EC2 resources to meet the demand based off of custom defined metrics and thresholds. </p>
<p>In AWS, there is EC2 Auto Scaling which focuses on the scaling of your EC2 fleet, but there’s also an Auto Scaling service. This service allows you to scale Amazon ECS tasks, DynamoDB tables and indexes, in addition to Amazon Aurora replicas. For this course I will just be focusing on EC2 Auto Scaling. Let’s look at an example of how EC2 Auto Scaling can be used in practice. </p>
<p>Let’s say you had a single EC2 instance acting as a web server receiving requests from the public users across the Internet. As the requests and demand increases, so does the load on the instance. Additional processing power will be required to process the additional requests and therefore the CPU utilization would also increase. To avoid running out of CPU resource on your instance, which would lead to poor performance experienced by your end users, you would need to deploy another EC2 instance to load balance the demand and process the increased requests. With Auto Scaling, you could configure a metric to automatically launch a second instance when the CPU utilization got to 75% on the first instance. By load balancing traffic evenly, it would reduce the demand put upon each instance and reduce the chance of the first web server failing or slowing due to high CPU usage. Similarly, when the demand on your web server reduces, so would your CPU utilization. So you could also set a metric to scale back. In this example, you could configure Auto Scaling to automatically terminate one of your EC2 instances when the CPU utilization dropped to 20% as it would no longer be required due to the decreased demand. </p>
<p>By scaling your resources back helps to optimize the cost of your EC2 fleet as you only pay for resources when they are running. Through these customizable and defined metrics, you can increase, scale out, and decrease, scale in, the size of your EC2 fleet automatically with ease. This has many advantages and here are some of the key points. Firstly, automation. As this provides automatic provisioning based off of custom defined thresholds, your infrastructure can elastically provision the required resources, preventing your operations team from manually deploying and removing resources to meet demands put upon your infrastructure. Greater customer satisfaction. If you are always able to provision enough capacity within your environment when the demand increases, then it’s unlikely that your end users will experience performance issues, which will help with user retention. And cost reduction. With the ability to automatically reduce the amount of resources you have when the demand drops, you will stop paying for those resources. You only pay for an EC2 resource when it’s up and running, which is based on a per second basis. When you couple Auto Scaling with an Elastic Load Balancer, you get a real sense of how beneficial building a scalable and flexible architecture for your resources can be. </p>
<p>In the next lecture, I shall be explaining the different components of Auto Scaling before providing a demonstration on how to configure it.</p>
<h1 id="Components-of-EC2-Auto-Scaling"><a href="#Components-of-EC2-Auto-Scaling" class="headerlink" title="Components of EC2 Auto Scaling"></a>Components of EC2 Auto Scaling</h1><p>Hello and welcome to this lecture where I’ll focus on the different components of EC2 auto scaling, to help you understand how the process and service works. There are two distinct steps to the configuration. The first step is the creation of the launch configuration or launch template. And the second part is the creation of an auto scaling group. </p>
<p>When using EC2 auto scaling, you can either create a launch configuration or launch template. Both define how an auto scaling group builds new EC2 instances. They both answer a number of questions required when launching a new instance, such as which Amazon Machine Image to use or AMI, which instance type to select. If you’d like to use Spot Instances to help lower costs. If and when public IP addresses should be used for your instances. If any user data is required for automatic scripting on first boot. What storage volume configuration should be used, and what security group should be applied. You will probably be familiar with most of these steps if you have ever created an EC2 instance manually, it’s much the same. </p>
<p>A launch template is essentially a newer and more advanced version of the launch configuration. Being a template you can build a standard configuration allowing you to simplify how you launch instances for your auto scaling groups. Let me now demonstrate how to create both a launch configuration and a launch template. </p>
<p>As you can see, I’m logged into my AWS account and I’m at the Management Console. And to create our launch templates and launch configurations we need to go into EC2 under compute. So let’s take a look. Now I’m going to start by creating the launch template first. And then after that, I’ll create the launch configuration so you can see the differences between them. </p>
<p>Now on the left hand side here, under instances, you can see launch templates. So if you select that, then it’s just a quick splash screen here, just saying welcome to launch templates. And this gives you a brief summary of what it is. So to create a launch template we’ll click on the blue create launch template button. Now here we have a single page with a number of configurable parameters on them. So let’s go through each of them and take a look. So firstly, we can either create a new template or create a new template version. Now as I don’t have an existing template, we can’t create a new version of that template. So let’s start from scratch by creating a new template. Let’s give this a name. I’ll just call this launch template. And a description of demo. Now here we can specify the source template, which essentially allows you to create a template from an existing template that you might already have. And as I explained, I don’t have any other templates at the moment. So we can’t do that and I just want to show you how to create a template from scratch anyway. Now further down we have launch template contents. Now this is where we started getting to the actual configuration of what we’re going to launch. </p>
<p>And we can start by selecting the AMI ID. If we click on the search for AMI, we can have a look at the different catalogs. Either quick start if you have any of your own AMIs on the marketplace or the community AMIs. Let’s just go with a quick start. And then we can select an AMI, let’s just go with the top one here, the Amazon Linux. And select AMI, now we can select an instance type. Let’s just go for a t1 micro. And if we have any existing key pairs, we can select an existing key pair to allow us to connect to our instances. For this one, I’ll just select an existing key pair. And then network type that this instance will reside in, whether it’s in a VPC environment, or the classic environment, we’re going to go with the VPC. Now here we can also attach any security groups Again as a drop down list to allow you to select an existing security groups that you might have. So I’ll just select a couple of different groups here. Now further down, if you want to add any additional network interfaces, then you can do so here simply by clicking on add network interface and filling out the relevant fields. Don’t need to do that in this example. Now here we have storage volumes. So I’ll come with an eight gig EBS volume, general purpose. And we can specify encryption here if we want to, and the delete on termination, either yes or no, and the IOPS et cetera. And we can add additional volumes if we want to as well here. Further down we can instance tags. So let’s add a tag, say for example project cloud academy. And this will tag both the instance and the volume. If we go into advanced details. We can select if you want this to be a Spot Instance or not. We can select an instance profile which allows you to associate a role to your EC2 instance when it launches. We can select the shutdown behavior, whether you want it to terminate or simply stop when we shut it down. And there’s a number of other more advanced options that you can select with regards to your instance. And then at the very bottom we have user data if you want to run any commands on boot. Now once you’re happy with all of your information, all you need to do is simply click on create launch template. And that’s it. </p>
<p>If we get on to close, we can now see our launch template has been created. And the default version is one and the latest version is one. We can create another launch template based on this and give it a different version if we want to. So let’s see how the launch template compares to the launch configuration. </p>
<p>Now the launch configuration is further down on the left hand side on the auto scaling right at the bottom here. So if you click on launch configurations, and this is essentially the same as the launch template. Although the launch template has a few more options, and it’s more simplistic in its creation and is the preferred method. However, you can still create launch configurations, so let’s take a look. Click on create launch configuration. Now here we can select our AMI. And again, we have the different catalogs here like we do with the launch template. It’s just presented differently essentially. Select the AMI. Here we can select our instance type. And again we had this option in the launch template. Here we can give it a name. Let’s call this launch configuration. Again, you can select if you want to use Spot Instances, and you can select an IAM role. Whereas in the launch template you can select the instance profile. If we get on to advanced, there’s not as many advanced options here as there is with the launch template as we had a much longer list. However, you do still have a number of options here should you need it such as user data, specifying the Kernel ID et cetera. If we go to storage. Again, it comes with the default storage for the instance type you selected. And again you can add new volumes if you need to. So again very similar to the launch template. Here you can select your security groups, so you can select an existing group. So again, you can just select the security groups that you need here. Then click on review. And here’s a summary of all the options that you’ve selected. And once you’re happy with those, simply click create launch configuration. And finally, here you can also choose a key pair or create a new key pair should you need to. So again, let’s just let that cloud academy key pair. Then click create launch configuration. And there you go, there’s our launch configuration. </p>
<p>So there’s two different methods of creating that configuration to allow your auto scaling groups to know what instances to launch and how they should be configured. The main difference between the two is that the launch templates is presented all on a single page to allow you to quickly select your options rather than going through a number of different screens. And it also has a few more advanced features and options as well. Okay, that’s the end of the demonstration, thank you. </p>
<p>Without either the launch configuration or launch template, auto scaling would not know what instance it was launching and to which configuration. So before you create your auto scaling group, you need to have your launch configuration defined. But what does the auto scaling group do? Well, the auto scaling group defines the desired capacity and other limitations of the group using scaling policies and where the group should scale your resources, such as which availability zone. Let’s look at each of these details further via another demonstration on how to create an auto scaling group. And during this demonstration, I will create a new auto scaling group based on our previous launch template. And I’ll set up an auto scaling policy defining when to both increase and decrease the group size. Let’s take a look. </p>
<p>Again I’m within the AWS Management Console. And to create our auto scaling group, we need to go into EC2, which is under compute. And then if you scroll to the bottom on the left hand side, we can see under auto scaling, auto scaling groups, so let’s click on that. And this is where we can create our group. Firstly we click on the blue button create auto scaling group. And here we can either create it from a launch configuration or a launch template. So let’s select the launch template which is the new and preferred option. And down here we can select which launch template we would like to use. And this is the one that I created in the previous demonstrations. So once I’ve selected my launch template that I’d like to use, click on next step. Now we can give it a group name. So I just call this demo. If we had multiple versions of our launch template, then we can select the different versions there. But at the moment we just have the single version. With regards to the fleet, we can adhere to our launch template configuration. Or we can use a combination of different purchase options and instances. I just want to use the configuration that we used within our launch template. With regards to the group size, let’s start with two instances in our auto scaling group. And I can select the appropriate VPC that I’d like this to be launched in. And once I select the VPC, then I can select the subnets that I’d like. So let’s just select a couple of subnets in our VPC. Now if we go down to advanced details, here we have a number of other options. Now if you want to associate our auto scaling group to a load balancer, then we can do so here. And we can select our load balancer and target groups. But at the moment I’m going to leave this blank because in a later demonstration in the next lecture, I’m going to show you how to associate an existing auto scaling group with one of your new load balancers. So I’m just going to leave that blank for now. But if you did want to associate your auto scaling group to a load balancer during creation. then this is the place you do it. We have our instance protection down here. And we have an option protect from scale in. So if this is selected then during the scale in procedures, auto scaling won’t terminate any instances that are protected. I’m just going to remove the options for now. And also and finally a service linked role is selected. And this enables access to AWS services and resources that are used or managed by auto scaling. Once we have our configuration set, we can move on to configuring scaling policies. </p>
<p>Now here, we have two options, we can keep this group at its initial size. And as you know I set it to two instances, or we can you scaling policies to adjust the capacity of the group. And that’s what I’d like to do. Now I want to scale between two and five instances say. ‘Cause that’s the minimum and maximum number of instances that this group will scale to. Now I want to scale more auto scaling group using step or simple scanning policy. So I’m going to click on that. Now we have a policy here for increasing the group size and also a policy here for decreasing the group size. So the name for this policy, I’m just going to leave as increase group size. So auto scaling knows when to execute this policy, we need to set an alarm. So click on add new alarm. And this will send a notification. At that the moment I’ve got a notification set up as CADemo, which is an SNS topic. And I want this alarm to trigger whenever the average CPU utilization is greater than or equal to 75% for one consecutive period of five minutes. The name of this alarm will be deploy new instances. And then I simply click on create alarm. Now I want to take the action of adding just a single instance when the CPU utilization is greater than 75%. </p>
<p>Now we can do the same for the decrease group size, so whenever the average CPU utilization is less than or equal to 30%, for two consecutive periods of five minutes, and I’m going to call this remove instances and create that alarm. I’m going to say remove one instance when the CPU is less than 30%. Once your policies are set, you can click on configure notifications. So if you want to configure your notifications, simply click on add notification. And that’s my SNS topic that I had. So whenever instances are launched, terminated, fail to launch and fail to terminate, I want to receive those notifications. Let’s click on configure tags. And here, you can add any tags to auto scaling group. Just going to leave that blank for this demonstration, click on review. And here we have all of our configuration that we just made. Once you’re happy with that, click on create auto scaling group. </p>
<p>And we now have our auto scaling group configured and except here the minimum and maximum instances two and five and the availability zones et cetera. Now at the bottom here you can go into the auto scaling group details. Here we can see it’s launching two new instances because we said we want to start with two instances to start with. We can review our scaling policies. We can look at instances, monitoring, notifications, et cetera, et cetera. So if we go over to our instances, we should see two new instances that are launching. and here we can see that these two here are both initializing. So that’s our two new instances that are running because of our auto scaling group which was based off of our launch template. And that’s it. </p>
<p>In the next lecture, I should be looking at how both ELB and auto scaling combined can be used to manage your EC2 infrastructure.</p>
<h1 id="What-is-an-Elastic-Load-Balancer-ELB"><a href="#What-is-an-Elastic-Load-Balancer-ELB" class="headerlink" title="What is an Elastic Load Balancer (ELB)?"></a>What is an Elastic Load Balancer (ELB)?</h1><p>Hello and welcome to this lecture, which is going to focus on what the AWS Elastic Load Balancer service is and does. </p>
<p>Now the main function of an Elastic Load Balancer, commonly referred to as an ELB, is to help manage and control the flow of inbound requests destined to a group of targets by distributing these requests evenly across the targeted resource group. These targets could be a fleet of EC2 instances, Lambda functions, a range of IP addresses, or even Containers. The targets defined within the ELB could be situated across different Availability Zones for additional resiliency or all placed within a single Availability Zone. Let’s look at this from a typical scenario. </p>
<p>Let’s suppose you have just created a new application, which is currently residing on a single EC2 instance within your environment, and this is being accessed by a number of users. At this stage, your architecture can be logically summarized as shown. If you are familiar with architectural design and best practices, then you would realize that using a single instance approach isn’t ideal although it would certainly work and provide a service to your users. However, this infrastructure layout brings some challenges. For example, the single instance where your application is located can fail, perhaps from a hardware or software fault. And if that happens, your application will be down and unavailable to your users. Also, if you experience a sudden spike in traffic, your instance may not be able to handle the additional load based on its performance limitations. As a result, to strengthen your infrastructure and help remediate these challenges, the unpredictable traffic spikes and high availability, et cetera, you should introduce an Elastic Load Balancer, an additional instance that’s running your application into the design as shown. </p>
<p>As you can see, in this design, the AWS Elastic Load Balancer will act as the point for receiving incoming traffic from users and evenly distribute the traffic across a greater number of instances. By default, the ELB is highly available as this is a managed service provided by AWS. And so, they ensure its resilience so we don’t have to. Although it might seem that ELB is a single point of failure, the ELB is in fact comprised of multiple instances managed by AWS. Also in this scenario, we now have three instances running our application. Now let me revisit the challenges we discussed previously. If any of these three instances fail, the ELB will automatically detect the failure based on defined metrics and divert any traffic to the remaining two healthy instances. Also if you experience a surge in traffic, then the additional instances running your application would help with the additional load. One of the many advantages of using ELB is the fact that it is managed by AWS, and it is, by definition, elastic. This means that it will automatically scale to meet your incoming traffic as the incoming traffic scales both up and down. </p>
<p>If you are system administrator or DevOps engineer running your own load balancer by yourself, then you would need to worry about scaling your load balancer and enforcing high availability. With an AWS ELB, you can create your load balancer and enable dynamic scaling with just a few clicks. Depending on your traffic distribution requirements, there are three ELBs available within AWS to choose from. </p>
<p>Firstly, the Application Load Balancer. This provides a flexible feature set for your web applications running the HTTP or HTTPS protocols. The Application Load Balancer operates at the request level, and it also provides advanced routing, TLS termination, and visibility features targeted at application architectures, allowing you to route traffic to different ports on the same EC2 instance. </p>
<p>Next, there is a Network Load Balancer. This is used for ultra-high performance for your application while at the same time managing very low latencies. It operates at connection level, routing traffic to targets within your VPC, and it’s also capable of handling millions of requests per second. </p>
<p>Finally, the Classic Load Balancer. This is primarily used for applications that were built in the existing EC2 Classic environment and operates at both the connection and request level. We’ll now talk a little bit about the components of an AWS ELB and some of the principles behind them. </p>
<p>Listeners. For every load balancer, regardless of the type used, you must configure at least one listener. The listener defines how your inbound connections are routed to your target groups based on ports and protocols set as conditions. The configurations of the listener itself differs slightly depending on which ELB you have selected. I will dive into the configuration of these as I discuss each ELB in further detail in upcoming lectures. </p>
<p>Target groups. A target group is simply a group of resources that you want your ELB to route requests to, for example a fleet of EC2 instances. You can configure your ELB with a number of different target groups, each associated with a different listener configuration and associated rules. This enables you to route traffic to different resources based upon the type of request. Rules. </p>
<p>Rules are associated to each listener that you have configured within your ELB, and they help to define how an incoming request gets routed to which target group. As you can see, your ELB can contain one or more listener. And each listener can contain one or more rules, and each rule can contain more than one condition, and all conditions in the rule equal a single action. An example rule could look as follows where the if statement resembles the conditions and the then statement acts as the action if all the conditions are met. So, depending on which listener request was responded to by the ELB, a rule based upon a priority listing would be associated containing these conditions and actions. If the request came from within the 10.0.1.0&#x2F;24 network range, which is the first condition, and was trying to carry a HTTP PUT request, the second condition, then the request would be sent to the target group entitled Group1, which is the action. </p>
<p>Health checks. The ELB associates a health check that is performed against the resources defined within the target group. These health checks allow the ELB to contact each target using a specific protocol to receive a response. If no response is received within a set of thresholds, then the ELB will mark the target as unhealthy and stop sending traffic to that target. </p>
<p>Internal or Internet-facing ELBs. There are two different schemes that can be used for your load balancers, either internal or Internet-facing. Internet-facing, as the name implies, the nodes of the ELBs that are defined as Internet-facing are accessible via the Internet and so have a public DNS name that can be resolved with public IP address. This would be in addition to an internal IP address as well. This allows the ELB to serve incoming requests from the Internet before distributing and routing the traffic to your target groups, which in this instance could be a fleet of web servers receiving HTTP or HTTPS requests. When your Internet-facing ELB communicates with its target group, it will only use the internal IP address, meaning that your target group does not need public IP addresses. An internal ELB only has an internal IP address. This means that it can only serve requests that originate from within your VPC itself. For example, you might have an internal load balancer sitting between your web servers in the public subnet and your application servers in the private subnet. </p>
<p>ELB nodes. During the creation process of your ELBs, you’re required to define which Availability Zone you’d like your ELB to operate within. For each Available Zone selected, an ELB node will be placed within that Availability Zone. As a result, you need to ensure that you have an ELB node associated to any Availability Zones for which you want to route traffic to. Without the Availability Zone associated, the ELB will not be able to route traffic to any targets within that Availability Zone even if they are defined within the target group. This is because the nodes are used by the ELB to distribute traffic to your target groups. </p>
<p>Cross-Zone load balancing. Depending on which ELB option you select, you may have the option of enabling and implementing Cross-Zone load balancing within your environment. Let’s presume you have two Availability Zones activated for your ELB with each associated load balancer receiving equal amount of traffic. One Availability Zone has six targets, and the other has four as shown. When Cross-Zone load balancing is disabled, each ELB and its associated AZ would distribute its traffic with the targets within that Availability Zone only. As we can see from the image, this results in an uneven distribution of traffic for each target across the Availability Zones. With Cross-Zone load balancing enabled, regardless of how many targets are in an associated Availability Zone, the ELBs would distribute all incoming traffic evenly between all targets, ensuring each target across the Availability Zones have an even distribution. </p>
<p>That now brings me to the end of this lecture. In the lecture, I shall be discussing server certificates and how they are used with load balancers to help terminate encrypted requests.</p>
<h1 id="SSL-Server-Certificates"><a href="#SSL-Server-Certificates" class="headerlink" title="SSL Server Certificates"></a>SSL Server Certificates</h1><p>Hello and welcome to this short lecture which will provide a high-level overview of server certificates and how they are used within your elastic load balancers. </p>
<p>As I mentioned in the previous lecture the Application Load Balancer provides a flexible feature set for your web applications running the HTTP or HTTPS protocols. As such, the ALB listener options available when creating your ALB are either the HTTP or HTTPS protocol on port 80 and 443 respectively. Configuration of your HTTP port 80 listeners is a fairly simple process, and I’ll cover this in the next lecture. However, there will times when you would need to use the HTTPS encrypted protocol as a listener and this requires some additional configuration. </p>
<p>So let me run through some of the points when using HTTPS as a listener. HTTPS is an encrypted version of the HTTP protocol and this allows an encrypted communication channel to be set up between clients initiating the request and your Application Load Balancer. However, to allow your ALB to receive encrypted traffic over HTTPS it will need a server certificate and an associated security policy. </p>
<p>SSL or Secure Sockets Layer, to give it its full name, is a cryptographic protocol, much like TLS, Transport Layer Security. Both SSL and TLS are used interchangeably when discussing certificates for your Application Load Balancer. The server certificates used by the ALB is an X.509 certificate, which is a digital ID that has been provisioned by a Certificate Authority and this Certificate Authority could be the AWS Certificate Manager service also known as ACM. This certificate is simply used to terminate the encrypted connection received from the remote client, and as a part of this termination process the request is then decrypted and forwarded to the resources in the ELB target group. </p>
<p>When you select HTTPS as your listener, you will be asked to select a certificate using one of four different options available. Either choose a certificate from ACM, upload a certificate to ACM, choose a certificate from IAM, or upload a certificate to IAM. The first two options relate to ACM. An ACM is the AWS Certificate Manager and this service allows you to create and provision SSL&#x2F;TLS server certificates to be used within your AWS environment across different services. This integration with ACM simplifies the configuration process of implementing a new certificate for your elastic load balancer and as a result, it’s the preferred option. </p>
<p>The last two options allow you to use a third-party certificate by using IAM as your certificate manager and you would select this option when deploying your ELBs in regions that are not supported by ACM. For a list of supported regions, please see the following link. For detailed information on how to upload, retrieve, and list server certificates via IAM, please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html">AWS documentation</a>. Using ACM as your certificate manager allows you to both create certificates from within ACM itself and also import existing certificates created from outside of AWS adding additional flexibility for your current third party certificates. The configuration of ACM is out of scope for this course. However, you can find further information on this service using the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">link</a>. </p>
<p>Now it’s brought me to the end of this lecture. In the next few lectures I shall be looking at the configuration of each of the defined load balancers, application, network, and classic, to provide you with more information on their components starting with the Application Load Balancer, the ALB.</p>
<h1 id="Application-Load-Balancers"><a href="#Application-Load-Balancers" class="headerlink" title="Application Load Balancers"></a>Application Load Balancers</h1><p>Hello and welcome to this lecture covering the Application Load Balancer, the ALB. The first of the three load balancers that I shall be discussing. If you are familiar with the open systems interconnection model, the OSI model, then you won’t be surprised that the ALB operates at layer seven, the application layer. The application layer of the OSI model serves as the interface for users and application processes to access network services. Everything at this layer is application specific. The application layer of the model helps to provide network services to the applications. And examples of the application process or services it offers are http, ftp, smtp and nfs. For more information on the OSI model, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/">here</a>.</p>
<p>As you can see AWS suggests you use the application load balancer if you need to provide a flexible feature set including advanced routing and visibility features aimed purely for application architectures such as microservices and containers when used in HTTP or HTTPS. Before configuring your ALB, it’s good practice to set up your target groups. Now I explained in a previous lecture that a target group is simply a group of resources that you want your ALB to route requests to. You might want to configure different target groups depending on the nature of your requests. For example, let’s say you had an internet-facing ALB, you might want a target group allocated to handle and process HTTP port 80 requests and a different target group configured to process requests from the secure HTTPS protocol using port 443. In this scenario, you could configure two different target groups and then route traffic, depending on the request, to different targets through the use of listeners and rules. </p>
<p>I now want to demonstrate how to configure an ALB and in this demonstration, I will also show you how to set up target groups as well. Let’s take a look. </p>
<p>As you can see, I’m in the AWS management console and the first thing we want to do is create our target groups and I can do this by going into the EC2 service which is found here under compute. And then if I scroll down on the left-hand side, I’ll get to the load balancing section here. Then in here, I have load balancers and target groups, but first I want to set up our target groups. So, if you select target group, as you can see there’s no target groups currently configured. So if I click on the blue button, Create target group, I now have a page of information that I need to complete. </p>
<p>So, firstly the target group name and I’m going to call this Web Servers. And then I have my target type and here I can specify it by instance, IP or Lambda function. I’m going to leave it as instance, then we can select what protocol we want. As this is going to be a web service, I’ll leave it as HTTP on port 80 and here we can select our VPC that we want this target group to exist in. So, select my appropriate virtual private cloud there. At the bottom we just have some health check settings and this is the path and protocol that the load balance will use when performing its health checks. So, for the path I’ll just put in index.html as an example. If we take a look at the advanced health check settings, here we have a number of other options that we can select. </p>
<p>This value specifies the healthy threshold which means the load balancer will have to receive five responses from the instance before deeming the previously unhealthy target healthy again, and the unhealthy threshold means that the load balancer only has to receive two failures before marking the instance as unhealthy. The timeout is simply the number of seconds that the load balancer will wait for a response, and the interval is how many seconds between each health check. Once we’re happy with our configuration, we’ll click on Create. Looks like I left this space in the name and you can’t have any spaces. So let’s just delete that and click on Create. And there you go, our target group was successfully created, and now we can see it in our list of target groups. </p>
<p>However, we don’t have any targets associated with this group yet. That was just simply the configuration of the group. So, down here we have the Description, the Targets, the Health checks, Monitoring and Tags, but if we click on the Targets tab, then here we can start adding our targets associated with this target group. And if we click on Edit, we can see here at the bottom that there’s two instances that I have running, web server one, and web server two. Now here I can select which instance I want to add and associate to this target group. For this demonstration, I’m going to select both instances and then add these as registered targets to this group. And as you can see, these two instances have now been added under the Registered targets section. Click on Save. And we can see here, that we have two registered targets which are the ones I just added, web server one and web server two, now associated to this target group. </p>
<p>Let’s just quickly look at these other tags here as well, the Health checks, that’s the health check information that we configured during the creation of the target group; Monitoring, this shows a number of CloudWatch metrics associated with the target group such as number of healthy hosts and unhealthy hosts et cetera. And then we also have Tags if you wanted to create a key-value pair for your target group and you can do so here. So, as you can see, it’s very easy to create different target groups as you need to for your load balancing. </p>
<p>Let us now go ahead and create an Application Load Balancer. So, back on the left-hand side here, again under Load Balancing, we have Load Balancers. So if you select that. Now I don’t have any load balancers configured here. So if I click on Create Load Balancer, and I can create an Application Load Balancer and Network Load Balancer or the Classic. In this example, I’m going to create the Application Load Balancer. So, click on Create. Now here we have a number of different steps. Firstly, we need to give it a name. So this would be WebServerALB, and we’ll have this as internet-facing using ipv4. Now down here we have our listeners. So this is the port and protocol that we want the load balancer to listen on and as this is our web server, let’s leave it as HTTP on port 80. If you want to add additional listeners, then you can do so just by selecting Add Listener and selecting the different protocols et cetera. Now if we scroll down to the bottom here, we can select our Availability Zones that we want to enable for our load balancer. So for eu-west-1a, let me select this subnet and for eu-west-1b I, shall select this subnet. So there are the two subnets that I want to associate with the load balancer, and each of them are in a set per availability zone as you can see here. </p>
<p>Now I need to go and configure my security settings. And I have a message here to say that the load balancer security is not using a secure listener. Now, if we were to go back and change that to HTTPS, then we would be using a secure listener and we’d also have to set up server certificates as well, but for this demonstration, I just want to show you how to create the Application Load Balancer, but generally in a wide-scale production environment, if you’re creating a load balancer for your internet-facing resources, then you’d probably want to use https for that additional security. Click on next. </p>
<p>Now we’ll need to select the security group that is going to be associated to our load balancer. So we could create a new security group, call this our Application Load Balancer. So we’ll have HTTP from any IP address, and we click on that Next Configure Reading. And this is where we can specify our target group for the Application Load Balancer. So we can create a new target group here and go through the same process as we did earlier or click on the dropdown list and select an existing target group and here we can see that we have our WebServers target group that we created earlier with all the settings already pre-filled. Click on Next Register Targets, and here we can see that these are the two targets associated with the target group. Click on Next Review. And this is just a review of all the configuration options that we made during the creation of this. Once you’re happy with that, simply click on Create. And then we have it. </p>
<p>We have our new load balancer, our WebServerALB was successfully created. So, let’s take a look. This might take it a couple of minutes for it to be provisioned. While that’s being provisioned, if we take a look at the bottom here, we can see that we have some basic configuration that we’ve set up with the availability zones, the fact that it’s internet-facing, and we have the ARN et cetera as well. We have our listener configuration here that we can change if we need to. As we can see, at the minute we’re listening on port 80. Again, we have some monitoring metrics here being carried out by CloudWatch. We’ll see a number of different CloudWatch metrics. Actually, we can now see that that the state is now active. So that’s our Application Load Balancer set up and configured. </p>
<p>Now before I finish this quick demonstration, I just want to show you the rules that I mentioned earlier with regards to listeners. So, if we go down to our Listeners tab here, we can see that we can view and edit our rules for our listener. So if we click on that, at the moment we can see that we have our default action here listening on port 80. We can see that this rule cannot be moved or deleted. That’s basically saying that this listener is listening on port 80 and for any requests then forward it to the WebServers. But we can add additional rules in here. So let’s take a look. So if we click on this plus button, we can see that we now have this option here of Insert Rule. So, let me select that, and that allows me to add a new rule in. So, first we need to add a condition. So, for example, let’s have the condition of a Source IP, then we just put in a random IP address here. So, this is saying if the source IP is this IP address, then add the following action, and here we could choose to forward it to another target group. I mean, I’ve only got one target group configured at the minute called WebServers, but if I had other target groups here with different instances associated to those target groups, then I could select a different target group to forward any requests that are received from this IP address. So that allows you to customize how your load balancer directs traffic, depending on what rules you create with your listeners. So, when I was talking about conditions and rules in a previous lecture, then this is the section that I was referring to. So I just wanted to show you that quickly within this demonstration, where you can edit your rules and add customization with conditions and actions. </p>
<p>Okay, and that’s the end of this demonstration.</p>
<h1 id="Network-Load-Balancers"><a href="#Network-Load-Balancers" class="headerlink" title="Network Load Balancers"></a>Network Load Balancers</h1><p>Hello and welcome to this lecture focusing on the network load balancer and its configuration. </p>
<p>Between the ALB and the NLB, the principles are the same as to how the overall process works, so to load balance incoming traffic from a source to its configured target groups. However, whereas the ALB work to the application level analyzing the HTTP header to direct the traffic, the network load balancer operates at Layer 4 of the OSI model enabling you to balance requests purely based on the TCP and UDP protocols. As such, a request to open a TCP or UDP connection is established to load balance the host in the target group. The listener supported by the NLB include TCP, TLS and UDP. The NLB is able to process millions of requests per second making the NLB a great choice if you need ultra high performance for your application. Also if your application logic requires a static IP address, then the NLB will need to be your choice of elastic load balancer. Unlike the application load balancer that has cross-zone load balancing always enabled, for the NLB this can either be enabled or disabled. When your NLBs are deployed and associated to different availability zones, an NLB node will be provisioned in these availability zones. The node then uses an algorithm which uses details based on the sequence, the protocol, source port, source IP, destination port and destination IP to select the target in that zone to process the request. When a connection is established with a target host, then that connection will remain open with that target for the duration of the request. Let me now provide a demonstration on how to configure and set up a network load balancer. </p>
<p>As you can see, I’m in the AWS management console. So to create our network load balancer, let’s go to EC2 under Compute. Then if we go down the left-hand side again under Load Balancing, click Load Balancers, we can see here our existing application load balancer we created before. So let’s click on Create Load Balancer and this time we’re going to create a network load balancer. So click on Create. And again, it’s very similar configuration to the application load balancer. So let’s firstly give it a name. Let’s call this DNS-NLB. This time we’ll have it internal facing. For our listener, let’s select the UDP protocol and the load balancer port is port 53 which is DNS. Again, we can select our availability zones where we want our load balancer to reside. So under eu-west-1a, let me select that subnet. And on the b, that one there. Next, configure security settings. Again, we receive this message because we’re not using a secure listener and for this demonstration that’s okay. Configure routing, now we need to associate our target group. Let’s create a new target group this time and we’ll call this DNS. For the target type, I shall leave as instance. We have our port and protocol there. Health checks under TCP. And if you wanted to, you can make any changes to your advanced health check settings there. Next, click on Register Targets. As we can see, we don’t have any registered targets as yet. If I scroll down, I can see I have one instance here so I’m going to add that to the registered list of targets. Once that’s been added, click on Next Review. Once you’re happy with all your configuration settings, click on Create. And there you have it. Your network load balancer is now created. We can see here provisioning which is our network load balancer. This is our previous application load balancer that we created earlier. So it’s a very similar process with different ports and protocols available between the load balancers. And that’s the end of this demonstration.</p>
<h1 id="Classic-Load-Balancers"><a href="#Classic-Load-Balancers" class="headerlink" title="Classic Load Balancers"></a>Classic Load Balancers</h1><p>Hello and welcome to this lecture covering the last of the load balancers that are available, the classic load balancer. The classic load balancer supports TCP, SSL&#x2F;TLS, HTTP, and HTTPS protocols. However, it does not offer as wide a range of features as the other load balancers. It is considered best practice to use the ALB over this classic load balancer unless you have an existing application running in the EC2-Classic network. Now, many of you will be unfamiliar with the EC2-Classic platform, and this is because it is no longer supported for newer AWS accounts. In fact, any account created after the 12th of April 2013 will not support EC2-Classic. </p>
<p>The EC2-Classic platform was originally introduced when the first release of EC2 was made generally available a number of years ago. The EC2-Classic platform enabled you to deploy your EC2 instances in a single flat network shared with other customers instead of inside a VPC. Although the classic load balancer doesn’t provide as many features as the application load balancer, it does offer the following which the ALB does not. It supports EC2-Classic, it supports TCP and SSL listeners, and it has support for sticky sessions using application-generated cookies. Again, the classic load balancer works in much the same way as the other load balancers already discussed, and again, cross-zone load balancing can either be enabled or disabled. Let’s now take a look at the creation of a classic load balancer. </p>
<p>So let’s now create the last type of load balancer, the classic load balancer. So again, let’s go to EC2. Down the left-hand side to load balancers. We have our previous application load balancer and our network load balancer. Let’s now create the classic load balancer. So we go across here to create. Give this a name. I’ll just call it classic. Select the VPC that I’d like to do. Now here we have our listener configuration, so for ease, let’s just have this listed on port 80. And then here, we need to select our availability zones that we’d like. So let’s select this one and also this one here. Once we’ve selected our subnets for our load balancer, we can then assign security groups. I’m going to use an existing security group that I’ve created previously. Once that’s selected, click on Configure Security Settings. Again, it’s telling us we’re not using a secure listener. Again, for this demonstration, that’s more than okay. Now we can configure our health checks. This will probably look familiar to you when we’re discussing the application load balancer. So the port and protocol using and the path, as well, the ping path, which is what the load balancer will check to make sure it can reach to determine if the instance is healthy or not. Once you’re happy with those details, select Add EC2 Instances. </p>
<p>Now here we can select the instances that you want to associate to the load balancer, and this is different to the application load balancer and the network load balancer, where we used target groups. With the classic, we simply select the instances that we want included, so we don’t use target groups for a classic load balancer. So for this example, we can select those two options, coming down across to add tags. Put in any tags you want associated for the load balancer. Click on Review and Create, confirm that you’re happy with your settings, and then click on Create. And there we have it. So if we go back here, you can now see that we have our three different load balancers that we’ve created. Here we have our application load balancer, this was our network load balancer, and here we have our classic load balancer. And it’s as simple as that. </p>
<p>Before I finish this lecture, it’s a good time to take a quick look at the comparison between the three load balancers that we’ve looked at. To help with this, AWS Provides a great table to show the feature differences between each ELB, which can be found using the link shown on screen. We can clearly see that the ALB is the most feature-rich. However, the NLB supports some significant differences to that of the ALB, such as support for static IPs, EIPs, and preserving source IP addresses. </p>
<p>That now brings me to the end of this lecture. Coming up next, I shall be looking at auto scaling and the benefits that this feature brings.</p>
<h1 id="Using-ELB-and-Auto-Scaling-Together"><a href="#Using-ELB-and-Auto-Scaling-Together" class="headerlink" title="Using ELB and Auto Scaling Together"></a>Using ELB and Auto Scaling Together</h1><p>Hello and welcome to this short lecture where I shall discuss the relationship of ELBs and EC2 auto scaling. As you saw in the demonstration on the previous lecture, it’s easy to associate your EC2 auto scaling group to an elastic load balancer and this is because the two services go hand in hand to provide optimal efficiency for both the performance and cost perspective. Each service by itself provides a great way to solve particular operational hurdles. The ELB allows you to dynamically manage loads across your resources based upon target groups and rules whereas EC2 auto scaling allows you to elastically scale those target groups based upon the demand put upon your infrastructure. However, one without the other can cause an operational burden. </p>
<p>For example, let’s say you have an ELB configured but without any auto scaling. You will need to ensure that you manually add and remove targets based upon the demand. You will need to monitor this demand allowing you to manually add or remove instances as required. Now let’s look at the reverse. Let’s assume you have EC2 auto scaling configured but no elastic load balancer. How are you going to evenly distribute traffic to your EC2 fleet? </p>
<p>Hopefully, you can see the benefit of combining an ELB and auto scaling to help manage and automatically scale your EC2 compute resources both in and out. When you attach an ELB to an auto scaling group, the ELB will automatically detect the instances and start to distribute all traffic to the resources in the auto scaling group. When you want to associate an application load balancer or network load balancer, you associate the auto scaling group with the ELB target group. When you attach a classic load balancer, the EC2 fleet will be registered directly with the load balancer. </p>
<p>Let me now provide another demonstration on how to associate your ELBs with an auto scaling group. </p>
<p>Okay, so to attach an existing load balancer to your auto scaling group is very quick and simple. So firstly we go to our EC2 dashboard under Compute. We then go down to our Auto Scaling Groups at the very bottom on the left-hand side. We can see here our auto scaling group that we created in a previous demonstration. Now you’ll notice on the Details section at the bottom here there’s a section for classic load balancers and target groups and both of these are empty fields. So let’s look at how you would associate either a classic load balancer or your target group. So once our auto scaling group is selected, we go to up to Actions and then Edit. Now if we scroll down, we get to the section here Classic Load Balancers and Target Groups. Now if we want to add a classic load balancer, then you can select here and select any classic load balancers that you have configured so I can select that one for example or if you’re using an application load balancer or network load balancer, then you associate it with the target groups because the target groups are the pool of resources that the application load balancer or network load balancer are associated to. Now here we have two target groups that we created in a previous demonstration either DNS or WebServers. So we could select WebServers as our target group and simply click on Save. </p>
<p>And as you can see now, this entry here for target groups, the auto scaling group is now associated to the WebServers target group. And it’s as simple as that. So all you need to do to associate an existing load balancer to your auto scaling group is to edit the auto scaling group and if it’s a classic load balancer, add in the classic load balancer or if it’s an application load balancer or network load balancer, then you associate it to the relevant target group. And that’s it. </p>
<p>That now brings me to the end of this lecture. Coming up next, I will provide a summary of the key points made throughout this course.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I now want to quickly highlight some of the main points from each of the previous lectures. I started off by looking at what ELBs were and within this lecture, I explained that the main function of an ELB is to elastically manage and control the flow of inbound requests destined to a group of targets distributing requests evenly. These targets could be a fleet of EC2 instances, AWS Lambda functions, a range of IP addresses or even Containers. ELBs help you maintain your solution by minimizing outages from hardware of software faults. ALBs are also used to help handle unpredictable traffic spikes, and by default, the AWS ELB is highly available. ELBs will automatically detect failed hosts on defined metrics and divert any traffic to the remaining healthy instances. ELBs will automatically scale to meet your incoming traffic, and there are three different types available within AWS, the Application Load Balancer, the Network Load Balancer, and the Classic Load Balancer. ELB listeners define how your inbound connections are rooted to your target groups based on ports and protocols set as conditions. And target groups allow you to group resources that you want your ELB to route requests to. Rules are associated to each listener to help define how an incoming request gets routed to which target group. Health checks allow the ELB to contact each target using a specific protocol to receive a response. If no response is received within a set threshold, then the ELB will mark the target as unhealthy and stop sending traffic to that target. There are two different schemes of ELBs, firstly Internet-Facing, and the ELB nodes are accessible via internet and have a public DNS name that can be resolved to its public IP address, in addition to an internal IP address as well. The internal ELB only has an internal IP address, and this means that it can only serve requests that originate from within the VPC itself. ELB nodes are placed within the AZ where you want to perform load balancing, and nodes are used by ELBs to distribute traffic to your target groups. Cross-zone load balancing will distribute all incoming traffic evenly between all targets in all configured AZs ensuring each target across the AZs have an even distribution. </p>
<p>Next I looked at server certificates for encrypted requests and how ELBs manage these connections and during this lecture I covered the following points. When using HTTPS as a listener, it requires additional configuration relating to server certificates. HTTPS is an encrypted version of the HTTP protocol and this allows encrypted communication between clients initiating the request and your ALB. To receive encrypted traffic over HTTPS your ELB needs a server certificate and an associated security policy. The server certificate used is an X.509 certificate, and certificates are provisioned by a certificate authority which could be the AWS Certificate Manager or ACM. Certificates are used to terminate encrypted connections where the request is then decrypted and forwarded to a target group. Using the ACM simplifies the configuration process of implementing a new certificate for your ELB and this is the preferred option. It is also possible to use a third party certificate by using IAM and your certificate manager, and this option is required in regions not supported by ACM. </p>
<p>Next I looked at the individual load balancers, and during these three lectures, we learned that the application load balancer operates at layer seven of the OSI model, and you should use the ALB if you need to provide a flexible feature set, including advanced routine and visibility features aimed purely for application architectures. The ALB has cross-zone load balancing always enabled, and the listeners supported by the ALB includes HTTP and HTTPS. The network load balancer operates at layer four of the OSI model, enabling you to balance requests purely based upon the TCP and UDP protocols and the listeners supported by the NLB included TCP, UDP and TLS. The NLB is a great choice if you need ultra high performance for your application, and cross-zone load balancing on the NLB can be enabled or disabled. The classic load balancer supports TCP, SSL, TLS, HTTP and HTTPS protocols. The classic load balancer does not offer as wide a range of features as the other load balancers. It is best practice to use the ALB over the Classic Load Balancer unless you have an existing application running in the EC2-Classic network. The classic ELB has support for EC2-Classic, TCP and SSL listeners and sticky sessions. Cross-zone load balancing can either be enabled or disabled for the classic load balancer. I also performed a number of demonstrations. We showed you have to configure each of these load balances. </p>
<p>The course then had a slight change of focus as I moved onto Auto Scaling and temporarily away from Elastic Load Balancing. I began by explaining what EC2 Auto Scaling was and here I covered the following. Auto Scaling is a mechanism that automatically allows you to increase or decrease your EC2 resources to meet the demand based off of custom defined metrics and thresholds. Using metrics, you can automatically add and remove instances as and when load both increases and decreases. By scaling your resources back helps you to optimize the cost of your EC2 fleet. Advantages of Auto Scaling include automation, with the provisioning and removal of instances, greater customer satisfaction, through reliable and stable infrastructure, and cost reduction thanks to scaling-in policies. </p>
<p>This then led me onto the next lecture where I discussed the different components of Auto Scaling. The main components are covered with the Launch Configuration and Launch Template in addition to the Auto Scaling group. The Launch Configuration or Launch Template define how an Auto Scaling group builds new EC2 instances, and the Launch Template provides additional features over the launch configuration including versioning and simplifies your auto scaling configuration. The Auto Scaling Group defines the desired capacity and other limitations of the group using scaling policies and which Availability Zones the Group should scale resources in. I then performed a demonstration in this lecture on how to create the launch configuration and template in addition to auto scaling groups so you could see how they were constructed. </p>
<p>In the final lecture I touched on some points relating to how both ELBs and EC2 Auto Scaling can be used in conjunction with each other and here I explained that both ELB and Auto Scaling goes hand in hand to provide optimal efficiency from both a performance and cost perspective. However one without the other can cause an operation burden. When you want to associate an ALB or NLB to an auto scaling group, you actually associate the auto scaling group with the target group. When you attach a classic load balancer, the EC2 fleet will be registered directly with the load balancer. That now brings me to the end of this lecture and to the end of this course. You should now have a full understanding of AWS Elastic Load Balancing and EC2 Auto Scaling to help you build elastic, reliable, efficient and cost optimized EC2 solutions within your VPC. </p>
<p>If you’d like some hands on experience on some of the topics I’ve discussed in this course, then we do offer a number of labs, so please do take a look. We have the Working the Application Load Balancer lab, Working with Amazon EC2 Auto Scaling Groups and Network Load Balancer, and How to Create Your First Auto Scaling Group.</p>
<p>If you have any feedback on this course, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#115;&#117;&#x70;&#112;&#111;&#x72;&#116;&#64;&#99;&#108;&#x6f;&#x75;&#100;&#x61;&#99;&#97;&#100;&#101;&#x6d;&#121;&#x2e;&#99;&#111;&#109;">&#115;&#117;&#x70;&#112;&#111;&#x72;&#116;&#64;&#99;&#108;&#x6f;&#x75;&#100;&#x61;&#99;&#97;&#100;&#101;&#x6d;&#121;&#x2e;&#99;&#111;&#109;</a>. </p>
<p>Thank you for your time and good luck with your continued learning of cloud computing. Thank you. </p>
<h1 id="Introduction-to-Gateway-Load-Balancer"><a href="#Introduction-to-Gateway-Load-Balancer" class="headerlink" title="Introduction to Gateway Load Balancer"></a>Introduction to Gateway Load Balancer</h1><p>Many of AWS services and innovations are customer-driven. The AWS Gateway Load Balancer service is no exception. Many customers have relied on virtual appliances from AWS partners and the AWS marketplace. However, the deployment process and scaling for virtual appliances was difficult to say the least. First, we will need to be able to direct all traffic, inbound and outbound, from an Internet Gateway or Virtual Private Gateway to an Elastic Network Interface of a specific EC2 instance in a VPC. </p>
<p>This feature is essential and happens to be implemented using VPC Ingress routing for the Internet Gateway. Using a VPC Ingress routing, we can forward traffic to a Gateway Load Balancer by updating the route tables in a VPC. The next feature needed is to deal with IP tunneling, such as not to incur errors or conflicts with IP addressing. In a nutshell, we need to be able to grab all traffic, inbound and outbound for the VPC, and redirect it to a virtual network appliance for security processing, and not interrupt the normal flow and interactions of the request and the response. </p>
<p>The process needs to be transparent. The AWS Gateway Load Balancer uses a single point of access for all inbound and outbound traffic, and allows you to scale your virtual appliance with demand as done with other Elastic Load Balancers, like the Application Load Balancer. The Gateway Load Balancer routes traffic through healthy virtual appliances and stop sending traffic if an appliance becomes some healthy. </p>
<p>Using Gateway Load Balancer, you can also add your own logic into any networking path in AWS when you want to inspect and take action on packets. The AWS Gateway Load Balancer sends inbound and outbound traffic transparently over the same consistent route and using the same target. This implements sticky, transparent, and symmetric flow.</p>
<h1 id="Anatomy-of-the-AWS-Gateway-Load-Balancer"><a href="#Anatomy-of-the-AWS-Gateway-Load-Balancer" class="headerlink" title="Anatomy of the AWS Gateway Load Balancer"></a>Anatomy of the AWS Gateway Load Balancer</h1><p>The Gateway Load Balancer consists of two parts. The first part is basically a VPC interface endpoint to the Gateway Load Balancer. Let’s call it a VPC Gateway Load Balancer Endpoint. This endpoint is expected to be defined in the VPC where you want to protect the traffic. The second part is the actual Gateway Load Balancer, which sends traffic to a fleet of EC2 instances running third party network appliance software. The Gateway Load Balancer is required to forward packets without alteration. </p>
<p>In order to make this happen, the Gateway Load Balancer uses a tunneling protocol called GENEVE. More formally, by definition, GENEVE is a tunneling mechanism which provides extensibility while still using the offload capabilities of Network Interface Cards for performance improvement. GENEVE works by creating a Layer 2 logical network that is encapsulated in UDP packets. If that sounded a bit technical, let’s break it down. </p>
<p>A tunnel is created between the Gateway Load Balancer and the fleet of instances on the back-end. Traffic is encapsulated and sent through the tunnel to the security appliances implemented in EC2 instances, which will examine and act on packets as they’re sent or received. The Gateway Load Balancer encapsulates the packets to the target to provide separation and add some additional information about which Gateway Load Balancer Endpoint the packet came from. GENEVE uses port 6081 to get traffic from the Gateway Load Balancer, and it uses HTTP port 80 for health checks.</p>
<h1 id="Gateway-Load-Balancer-Architecture"><a href="#Gateway-Load-Balancer-Architecture" class="headerlink" title="Gateway Load Balancer Architecture"></a>Gateway Load Balancer Architecture</h1><p>One of the most fundamental architecture diagrams for the Gateway Load Balancer is shown as a way to review some of the details that we just discussed. As shown in the diagram, it’s not unusual to see the Gateway Load Balancer endpoint listed as GWLBe and define each in their own subnet per availability zone. Gateway Load Balancer endpoints can be added to a route table as the next hop and integrate the Gateway Load Balancer into the traffic flow of a VPC.</p>
<p> A Gateway Load Balancer endpoint is similar to AWS private link and operates across many accounts and VPCs with centralized control and administration. Also note, the ingress route table associated with the internet gateway in the customer VPC pointing to the Gateway Load Balancer endpoints accordingly. In the general process of setting up a Gateway Load Balancer, you need to provision a VPC dedicated to the Gateway Load Balancer and the third party virtual appliance software you’re running on EC2 instances. You provision the Gateway Load Balancer and target groups as you would any other Elastic Load Balancer like the Application or Network Load Balancer. </p>
<p>On the VPC where your application lives, you create Gateway Load Balancer endpoints on their own dedicated subnets and update the route tables to include the Gateway Load Balancer endpoints for traffic coming from your application subnets to them and traffic from the Internet Gateway to them as well, in order to integrate the security VPC to the traffic flow. </p>
<p>In general, the steps are: </p>
<p>Number 1, locate the partner’s virtual appliance software, perhaps in AWS Marketplace. </p>
<p>Step 2, launch the appliance instances in your security VPC. </p>
<p>Step 3, create a Gateway Load Balancer and target group with those appliance instances. </p>
<p>Step 4, create Gateway Load Balancer endpoints in the VPC where the traffic needs to be inspected. </p>
<p>And step 5, update route tables to make Internet Gateway move traffic to and from the Gateway endpoints and the Gateway Load Balancer endpoint as the next-hop. </p>
<p>Let’s discuss this traffic flow that includes the Gateway Load Balancer with corresponding Gateway interface endpoints in a general architecture diagram.</p>
<h1 id="Traffic-Flow-Steps-for-Gateway-Load-Balancers"><a href="#Traffic-Flow-Steps-for-Gateway-Load-Balancers" class="headerlink" title="Traffic Flow Steps for Gateway Load Balancers"></a>Traffic Flow Steps for Gateway Load Balancers</h1><p>In the architecture diagram shown, we have an application deployed to private subnets in an auto-scaling target group service by an Application Load Balancer. The Application Load Balancers are deployed to public subnets. We also have a separate security VPC with a Gateway Load Balancer and security appliances in a target group for auto-scaling. This will allow for the appliance fleet to adjust based on application load, and therefore, scaling horizontally. </p>
<p>We can follow the steps, a packet will travel in this architecture. </p>
<p>Step 1, a customer access your web application and a request is generated. </p>
<p>Step 2, the landing place for public traffic request is the Application Load Balancer. However, the Internet Gateway is configured with an Ingress route table to direct traffic to a Gateway Load Balancer endpoint. </p>
<p>In step 3, the Gateway Load Balancer endpoint directs traffic to the Gateway Load Balancer in the security VPC. </p>
<p>Step 4, at the Gateway Load Balancer, the packets are wrapped using the GENEVE tunneling protocol and dispatch through the security appliance selected. </p>
<p>Step 5, the packet analysis takes place in the security appliance. What actually happens depends on the appliance being used and the configuration that you define. </p>
<p>Step 6, after analysis, the packets are sent back, still encapsulated to the Gateway Load Balancer where the encapsulation is removed and traffic is sent to the Gateway Load Balancer endpoint where it originally came from. </p>
<p>Step 7, the corresponding Gateway Load Balancer endpoint will direct traffic to the Application Load Balancer and will target your application. </p>
<p>From step 8, the response flow is very similar in that the application response will pass through the Application Load Balancer and into the subnet with the Gateway Load Balancer endpoint in the same availability zone. This will send traffic to the Gateway Load Balancer yet again in the return path for the security VPC. </p>
<p>Step 9, the packets are encapsulated yet again and sent to the security appliance where they are processed. </p>
<p>Step 10, the packets are sent back to the Gateway Load Balancer where encapsulation is removed and packets moved to the Gateway Load Balancer endpoint. </p>
<p>Step 11, the Gateway Load Balancer endpoint will send traffic out through the Internet Gateway. </p>
<p>And in step 12, the response is received by the customer. So, as we get to see from this example flow, the Gateway Load Balancer allows you to leverage and horizontally scale third-party security appliances from the AWS Marketplace in your Amazon VPCs.</p>
<h1 id="AWS-Outposts"><a href="#AWS-Outposts" class="headerlink" title="AWS Outposts"></a>AWS Outposts</h1><p>This service is all about hybrid functionality, helping your align your applications and infrastructure from your on-premises environment with that of the AWS cloud. There are many reasons that organizations use a hybrid cloud mode, utilizing services from both public cloud providers and also their own local data centers. For example, they might need to run applications locally due to latency, security or even governance requirements. AWS understands this and, as a result, they’ve developed AWS Outposts. </p>
<p>With AWS Outposts, it’s now possible to bring the AWS cloud to your data center. This includes the same hardware used by AWS within their data centers. By bringing in AWS hardware to your data center, it allows you to use native AWS services, including the same tools and APIs as you would when running your infrastructure within AWS, the difference being is that the hardware and services will be running locally to help you maintain the need for local applications and workloads, et cetera. </p>
<p>There are two different options available when using Outposts. You can either use VMware on AWS, which will seamlessly run your existing VMware management and infrastructure, or you can use a native AWS variant, which means you can use the same APIs and management tools as you would in AWS but on premises. For those new to VMware Cloud on AWS, it is sold as a service by VMware that allows you to run your applications across VMware’s vSphere suite of products within a software-defined data center, hosted on top of the AWS public cloud. By utilizing VMware underlying cloud foundation, it provides the ability to give you access to many native AWS services and features. Couple this with the ability to continue managing your infrastructure with vSphere, vSAN, NSX and vCenter Server, it enables you to create a secure, flexible and scalable hybrid cloud infrastructure model for your organization. By bringing the same native AWS tools, API, management console and services to your on premises data center, it enables you to manage and implement a hybrid cloud approach with seamless migration abilities, burst capacity and management between local an AWS cloud environments. </p>
<p>To use and organize AWS Outposts for use in your data center, you can order them through the AWS Management Console where you can select the different compute and storage options available to you. Depending on how much hardware you need, you can simply order a single server or variants of rack sizes, including a quarter or half or full-size racks of equipment. From a technical perspective, there are a wide number of EC2 instances available at launch. For example, C5, M5, R5, as well as storage options for EBS volumes, local instance storage and local disk options. AWS Outpost at launch and in the coming months will allow you to run services such as EC2, EBS, RDS, ECS, EKS, Sagemaker and EMR. From a connection perspective, customers can make use of the PrivateLink gateway endpoint to securely and privately connect in resources to others, including Amazon S3 and DynamoDB. </p>
<p>One final point on AWS Outposts is that the service itself is fully managed. This means that you do not need to maintain a level of patch management across your infrastructure. AWS will ensure the infrastructure is patched as and when needed. </p>
<p>That brings me to the end of this lecture. Next I’ll be discussing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-compute-services-2019-reinvent-reminders/license-manager/">AWS License Manager</a>.</p>
<h1 id="What-is-VMware-Cloud-on-AWS"><a href="#What-is-VMware-Cloud-on-AWS" class="headerlink" title="What is VMware Cloud on AWS?"></a>What is VMware Cloud on AWS?</h1><p>Hello, and welcome to this lecture, where I shall explain what <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-vmware-cloud-on-aws/introduction-59/">VMware Cloud on AWS</a> is, along with an overview of its underlying architecture.</p>
<p>VMware Cloud on AWS is sold as a service by VMware that allows you to run your applications across VMware’s vSphere suite of products within an SDDC hosted on top of the AWS Public Cloud. While utilizing VMware’s underlying Cloud foundation, it provides the ability to give you access to many native AWS services and features. Couple this with the ability to continue managing your infrastructure with vSphere, vSAN, NSX, and vCenter Server, it enables you to create a secure, flexible, and scalable hybrid Cloud infrastructure model for your organization.</p>
<p>If you are currently using VMware on-premises in your data center, and you’re looking for a way to migrate your workloads to the Cloud to take advantage of some of the Cloud technology’s key characteristics, such as on-demand resourcing, scalability, flexibility, high availability, security, utility-based metering, and regional expansion, then using VMware Cloud on AWS could be a great solution for you.</p>
<p>One thing to bear in mind, however, is that, at the moment, the service is only available in the US West (Oregon) region. But there are plans to distribute the service to all other AWS regions throughout 2018. Now we know what the service is. Let me now talk a little about the underlying architecture that the service runs on.</p>
<p>The AWS architecture used for VMware Cloud on AWS is different to your standard compute services on AWS, such as EC2 that runs on top of a Xen hypervisor installed on the host, where VMware Cloud on AWS runs on bear-metal AWS infrastructure. This primarily means two things. Firstly, the host itself belongs to a single customer.</p>
<p>And secondly, the host is not running any virtualization software, such as a standard Xen hypervisor that AWS normally uses. Typically, within a normal AWS environment, many customers can share the same underlying host to run their EC2 instances by selecting an option to run their resources on shared-tenancy hosts.</p>
<p>Although it is possible to request a dedicated host if this is required, this is not, however, the same as bare-metal infrastructure used with VMware Cloud on AWS. The difference being is that the EC2 dedicated host will still use the Xen hypervisor to manage underlying virtualization, whereas bear-metal servers are stripped of this virtualization software that is normally included with AWS hosts.</p>
<p>This allows VMware to optimize the AWS host with its own ESXi bare-metal type-1 hypervisor, removing any nested virtualization. In fact, VMware Cloud on AWS does not support nested ESXi virtual machines at all.</p>
<p>From a compute perspective, and during initial availability, there is a minimum and maximum compute cluster size with your SDDC. VMware clusters are comprised of a number of physical hosts. Where the memory and CPU power from those hosts is aggregated into a pool of resources for all virtual machines in the cluster to consume. So the minimum cluster size that you can provision for VMware Cloud on AWS is comprised of four ESXi hosts.</p>
<p>An ESXi host is simply an AWS bare-metal host with VMware’s ESXi hypervisor installed on top of the hardware. Each of these four hosts contains the following hardware. 512 gig of memory, dual CPU sockets containing Intel Xeon processors, and each socket contains 18 cores running at 2. 3 gigahertz. As a result, the minimum cluster-size configuration contains 2 terabytes of memory and 144 CPU cores.</p>
<p>The maximum cluster size, if you need to scale your compute resources out, currently stands at 16 ESXi hosts, meaning that the total resources in a maximum cluster configuration will contain 8 Terabytes of memory and 576 CPU cores. The scale and boundaries of this configuration for these clusters are likely to change over time.</p>
<p>So it’s always good practice to check on the VMware site for the latest information. Now we know the compute capacity. Let’s take a look at the storage.</p>
<p>vSAN storage clusters also draw their resources from a host within the cluster which contains an all-flash array. Each host in the cluster contains the following storage. 8 non-volatile memory express devices, which allows for flash storage to be directly connected to the host, in this case, low-latency SSDs, and solid-state drives. This provides a total of 10 terabytes of raw storage capacity. As a result, the minimum cluster size would provide a 40-terabyte vSAN datastore enabled by 32 NVMe devices.</p>
<p>At the other end of the scale, if we maximize the cluster size to 16 ESXi hosts, the datastore would grow to 160 terabytes across 128 NVMe devices.</p>
<p>It’s worth noting that during the initial availability of VMware Cloud on AWS, it’s not possible to encrypt data at the datastore level or VM level. To ensure your data remains secure, AWS performs encryption at the firmware level for all NVMe devices. The encryption keys are then managed and controlled by AWS and are not shared with VMware.</p>
<p>There is a restriction regarding clusters when it comes to location, in that the clusters created cannot span multiple availability zones or regions. They are restricted to a single AWS AZ within a single region.</p>
<p>Finally, let me take a look at the networking element of VMware Cloud on AWS, which utilizes VMware NSX. Understand that the networking component on VMware Cloud on AWS is probably the most complicated part. It’s important to understand how you can connect to the service from on-premises and also how to integrate the service with your existing AWS account and infrastructure.</p>
<p>VMware NSX is a fundamental component of VMware Cloud on AWS, as it’s used for all network connectivity and provides a bridge between the three environments: your own on-premises datacenter, the SDDC running on AWS, and our virtual private cloud’s VPC in your AWS account. A VPC is an isolated segment of the AWS Cloud which allows you to provision AWS resources in a virtual network.</p>
<p>Each host within the SDDC cluster contains an Elastic Networking Adapter, an ENA, and these ENAs are network interfaces that provide high networking performance and allow throughput of up to 25 gbps. To allow connectivity between your vSphere environment within your own on-premises datacenter, your SDDC, and extending through to your AWS VPC, two gateways are required for two different networks; one is for management traffic, such as administration of vCenter Server, and another will be used for compute and application traffic, such as workload traffic of your virtual machines.</p>
<p>These two gateways are as follows. A Management Edge Gateway, MGW, and a Compute Gateway, CGW. The Management Edge Gateway for the management networking traffic works in conjunction with NSX Edge, which provides network edge security and allows users to connect through to your SDDC vCenter Server via the Internet.</p>
<p>From the management network, it’s then possible to carry out a number of network-administration tasks, such as creating IPsec VPNs, back to your on-premises datacenter, or configuring firewalls. The IPsec VPN can allow communications between your on-premises vCenter-Server instance and components running in the SDDC.</p>
<p>In addition to this, a second VPN connection can be created to allow the connectivity of the VM workloads to transition between on-premises to the SDDC via the compute gateway. Once your management edge gateway is configured, you can then use a feature called Hybrid Linked Mode to connect your vCenter Server in your SDDC with your on-premises vCenter Server.</p>
<p>The compute gateway is used for compute and VM workload traffic, and uses an NSX Edge instance along with a Distributed Logical Router, a DLR, which allows inbound and outbound traffic from your VMs over the second IPsec VPN and to an AWS VPC. Before we move on, I just want to give a little bit more information around the Hybrid Linked Mode.</p>
<p>As I just said, this allows connectivity between your on-premises vCenter Server and the one running in your SDDC. With this active, it allows you to perform a number of management activities, such as performing cold migrations of your workloads between your on-premises environment and your VMware SDDC.</p>
<p>You can use the same credentials that you use for your on-premises vCenter Server with your Cloud SDDC vCenter Server. And using a single vSphere client interface, you can monitor and manage the inventories of both on-premises and in-Cloud SDDC environments. NSX is a complex and essential component, as it manages all networking infrastructure and security across the network, which remains decoupled from the AWS VPC and networking components.</p>
<p>During the creation of your SDDC setup and configuration, you must associate it to an existing VPC within your AWS account. Having your own AWS account is a prerequisite of running VMware Cloud on AWS. During this process, an ENI, Elastic Network Interface, will be created within your own AWS account.</p>
<p>This ENI will then link back to the compute gateway within your VMware SDDC. And this connectivity allows your VMs running in your SDDC to take advantage of and to communicate with other AWS resources, such as S3, EC2, etc. The ENI, essentially, acts as an endpoint for your VMware SDDC to gain access to native AWS services.</p>
<p>Your VMs will use the compute gateway as a bridge between your VMware Cloud and AWS SDDC and the ENI running in your VPC. This traffic between your SDDC and ENI is completely private and will use AWS’ own internal network to provide the connection. It does not use an Internet gateway or any public channel. It’s an internal private link between your SDDC and your AWS VPC.</p>
<p>The creation of the network configuration of your SDDC can be seen as a split between two roles: Cloud network administrators and Cloud administrators. The Cloud network administrator role will use the VMware Cloud on AWS Web portal to configure the following components. The initial network setup, VPN connectivity, to configure firewall access rules for VM workloads and setting administrator access to vCenter Server.</p>
<p>The Cloud administrator, on the other hand, can then use the vSphere Web client to utilize the infrastructure and configuration made by the Cloud network administrator. The vSphere Web-based client allows you to connect to the SDDC vCenter Server to manage your vSphere environment. Once connected, the Cloud administrator can attach VMs to networks, create new logical networks, and control IP addressing for VMs.</p>
<h1 id="An-Overview-of-AWS-Lambda"><a href="#An-Overview-of-AWS-Lambda" class="headerlink" title="An Overview of AWS Lambda"></a>An Overview of AWS Lambda</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="An-Overview-of-AWS-Lambda-1"><a href="#An-Overview-of-AWS-Lambda-1" class="headerlink" title="An Overview of AWS Lambda"></a>An Overview of AWS Lambda</h1><p>To really understand serverless compute, you have to first understand servers. For example, I want you to think about all the work that goes into running an EC2 instance: you have to install software, patch the instance, manage scaling and high availability, configure storage, and then write your code for your application and deploy it to the instance. </p>
<p>Now think about all that infrastructure maintenance and administration going away - enabling you to focus solely on your code and business logic. That’s the idea behind serverless. Now of course, this maintenance and server administration still exists behind the scenes, however, it’s no longer your job to do it - it becomes the service’s responsibility.</p>
<p>The serverless compute service we’ll focus on in this course is called AWS Lambda. Understanding Lambda is the same as understanding almost any function in a piece of code. There are three major parts: </p>
<ol>
<li>The input </li>
<li>The function, and</li>
<li>The output.</li>
</ol>
<h3 id="The-Function"><a href="#The-Function" class="headerlink" title="The Function"></a>The Function</h3><p>Let’s start with the function. Just as EC2 is made up of instances, Lambda is made up of functions. Functions are the code that you write that represents your business logic. In this function you also configure other important details, such as permissions, environment variables, and the amount of power the function needs. The way you specify power is by choosing how much memory you want to allocate to your function. The service then uses this number and provides proportional amounts of CPU, network and disk I&#x2F;O. </p>
<p>To upload your code to the service, you can either write the code directly in the service itself or you can upload this code via a zip file or files stored in Amazon S3. The programming language you write your code in must match the runtime you select in the service. </p>
<p>There are several options for runtimes. You can use a runtime that lambda natively supports, such as:</p>
<ul>
<li>Java,</li>
<li>Go, </li>
<li>Powershell, </li>
<li>Node.js, </li>
<li>C#,</li>
<li>Python, and </li>
<li>Ruby.</li>
</ul>
<p>Or, if you want to use a language that isn’t in that list, you can choose to bring other languages by using the custom runtime API. So if you’re thrilled at the idea of running PHP or C in Lambda, the custom runtime feature is for you. </p>
<h3 id="The-Input"><a href="#The-Input" class="headerlink" title="The Input"></a>The Input</h3><p>Once you’ve uploaded or written your code - how does your code run? Well, it has to be invoked. This is where the first piece of the equation - the input - comes into play. There are several options for your function to be invoked: </p>
<ul>
<li>Your function could be invoked directly through the console, SDK, AWS toolkits, or through the CLI. </li>
<li>It could be invoked using a function URL, which is an HTTP endpoint you can enable for your Lambda function</li>
<li>Or it could be invoked automatically by a trigger, such as an AWS service or resource. These triggers will run your function in response to certain events or on a schedule that you specify. So, if you want to run your function every day at 8 am, you can do that.</li>
</ul>
<p>When you invoke your function, you can pass in events for the function to process. If a service invokes your function, they can also pass in events - however, the service will be responsible for structuring those events. For example, your code could run in response to a request from API Gateway or an S3 event, such as a PUT object API call. So once the PUT object API call is made, AWS Lambda will run your code, just as you’ve written it, using only the compute power you defined. </p>
<h3 id="The-Output"><a href="#The-Output" class="headerlink" title="The Output"></a>The Output</h3><p>Then you have the third and final piece, which is the output. Once the function is triggered, and your code runs, your Lambda function can then make calls to downstream resources. This means that from your code, you can make API calls to other services like Amazon DynamoDB, Amazon SQS, Amazon SNS and more. </p>
<p>When your Lambda function is triggered, the service automatically monitors your function through logs and metrics. You can additionally choose to write custom logging statements in your code that will help you identify if your code is operating as expected. These log streams act as a recording of the sequence of events from your function. Lambda also sends common metrics of your functions to CloudWatch for monitoring and alerting. </p>
<h3 id="Costs"><a href="#Costs" class="headerlink" title="Costs"></a>Costs</h3><p>So what do you pay for with this service? You only pay for what you use - which means three things. </p>
<ol>
<li>You are charged for the amount of requests that you send to your function. </li>
<li>The Lambda function begins charging you when it is triggered, and stops charging you when the code has been executed. Otherwise known as the duration it runs. This is rounded up to the nearest 1 millisecond of use. </li>
<li>You’re charged based on the amount of compute power you provision for your function. So if you provision the maximum amount of memory, you’ll be charged for that.</li>
</ol>
<h1 id="Invoking-a-Lambda-Function"><a href="#Invoking-a-Lambda-Function" class="headerlink" title="Invoking a Lambda Function"></a>Invoking a Lambda Function</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="Invoking-a-Lambda-Function-1"><a href="#Invoking-a-Lambda-Function-1" class="headerlink" title="Invoking a Lambda Function"></a>Invoking a Lambda Function</h1><p>There are several ways to invoke a Lambda function. You can invoke it directly using the console, the CLI, SDKs, or you can invoke it automatically by using a trigger such as an AWS service. </p>
<p>No matter how you invoke a Lambda function, even if you’re invoking it directly through the Lambda service itself, you’re using the service’s API. Every invocation goes through the Lambda API. </p>
<p>What this API provides to you is three different models of how you can invoke your function:</p>
<h3 id="Synchronous-Push-Based-Model"><a href="#Synchronous-Push-Based-Model" class="headerlink" title="Synchronous (Push-Based) Model"></a>Synchronous (Push-Based) Model</h3><p>The first choice is the synchronous or push-based model. This follows the request&#x2F;response model. For example, let’s say we have a service like API Gateway that gets a request from a client. API Gateway then sends that request to a backend, in this case Lambda. API Gateway does this by making an invoke call to that function. After the Lambda function executes, it then returns a response back to API Gateway, which returns the response to the client. Request goes out, response comes in. </p>
<p>For synchronous invocations, if the function fails, the trigger is responsible for retrying it. In some cases, this might mean that there are no retries. For example, with API Gateway, it can send the error message back to the client. </p>
<p>To invoke a Lambda function synchronously, you can set the invocation type using the AWS SDK or you can use the CLI invoke command to do this. For example, I can use the command</p>
<p><strong>aws lambda invoke –function-name my-function –cli-binary-format raw-in-base64-out –payload ‘{ “key”: “value” }’ response.json</strong></p>
<p>You can also use the –invocation-type parameter and set it as RequestResponse to invoke it synchronously. </p>
<p><strong>aws lambda invoke –function-name my-function  –invocation-type RequestResponse  –cli-binary-format raw-in-base64-out –payload ‘{ “key”: “value” }’ response.json</strong></p>
<h3 id="Asynchronous-Event-based-Model"><a href="#Asynchronous-Event-based-Model" class="headerlink" title="Asynchronous (Event-based) Model"></a>Asynchronous (Event-based) Model</h3><p>The second model is the asynchronous model, also called the event-based model. In this configuration, the response does not go back to the original service that invoked the lambda function. In fact there’s no path back up to the service that triggered the function to run, unless you write that logic yourself. </p>
<p>For example, this is common with Amazon S3 and Amazon Simple Notification Service. Say you want to trigger a lambda function to run once an object is placed in a bucket. You can do this, but it’s not going to send a response back to S3 once it finishes executing, without additional business logic. The nice thing about the asynchronous model, is that it handles retries if the function returns an error or is throttled. It also uses a built-in queue. So, any event sent to your function is placed on this queue first and then eventually sent to the function.</p>
<p>If any of these events can’t be processed for whatever reason, you can send that failed event to a dead letter queue or use Lambda destinations to send a record of the invocation to a service. The dead letter queue will receive only the content of the event, while using Lambda destinations records will include both the request context and payload as well as the response context and payload. While both are a great way to troubleshoot failed events, Destinations is the more feature rich option.</p>
<p>To invoke a lambda function asynchronously, I can use the invoke command, except this time, I would need to use the invocation type parameter and set it to be “event”. </p>
<p><strong>aws lambda invoke –function-name my-function –invocation-type Event –cli-binary-format raw-in-base64-out –payload ‘{ “key”: “value” }’ response.json</strong></p>
<h3 id="Stream-Poll-based-Model"><a href="#Stream-Poll-based-Model" class="headerlink" title="Stream (Poll-based) Model"></a>Stream (Poll-based) Model</h3><p>The last model is the stream model, also called the poll-based model. This is typically used when you need to poll messages out of stream or queue-based services such as DynamoDB streams, Kinesis streams and Amazon SQS. The Lambda service runs a poll-er on your behalf, and consumes the messages and data that comes out of them, filtering through them to invoke your Lambda function on only messages that match your use case. With this model, you would need to create an event source mapping to process items from your stream or queue.</p>
<p>An event source mapping links your event source to your Lambda function, so that the events generated from your event source will invoke your Lambda function. These mappings, the response you get back, the permissions you set up, the polling behavior and even the event itself, can be very different based on the event source you’re using. However, the way you create event source mappings stays the same. You can do this by using the SDK or CLI. Here is an example of how to create an event source mapping by using the CreateEventSourceMapping CLI command: </p>
<p>*<em>aws lambda create-event-source-mapping –function-name my-function –batch-size 500 –maximum-batching-window-in-seconds 5 –starting-position LATEST *</em></p>
<p><strong>–event-source-arn arn:aws:dynamodb:us-east-2:123456789012:table&#x2F;my-table&#x2F;stream&#x2F;2019-06-10T19:26:16.525</strong></p>
<p>You can see that in this command, I’m creating a mapping between a DynamoDB stream and my Lambda function. I’m also specifying that the event source mapping batches records together to send to my function. You can control how it batches records using the batch size and the batching window. When your batching size is met or the batching window reaches its maximum value, in this case 5 seconds, your lambda function will be invoked. </p>
<h3 id="Choosing-the-Right-Model"><a href="#Choosing-the-Right-Model" class="headerlink" title="Choosing the Right Model"></a>Choosing the Right Model</h3><p>So which one of these models is right for you? </p>
<p>Well, the easiest use case is if you’re processing messages from a stream or queue, the best choice for that is to create an event source mapping and use the polling type of invocation for Lambda. </p>
<p>The next case is if your application needs to wait for a response, then synchronous invocation is the best choice and can help you maintain order. </p>
<p>However, if you have a function that runs for long amounts of time, that does not need to wait for a response, then invoking asynchronously is the preferable option, as it offers automatic retries, a built-in queue, and a dead letter queue for failed events. </p>
<p>Now - keep in mind that if an AWS service invokes your function, the ability to select an invocation type is removed. The service gets this choice instead, and selects the invocation method for you. So all of these hard choices go away. </p>
<h1 id="Monitoring-your-Lambda-Function"><a href="#Monitoring-your-Lambda-Function" class="headerlink" title="Monitoring your Lambda Function"></a>Monitoring your Lambda Function</h1><p><strong>Instructor: Alana Layton</strong></p>
<h1 id="Monitoring-your-Lambda-Function-1"><a href="#Monitoring-your-Lambda-Function-1" class="headerlink" title="Monitoring your Lambda Function"></a>Monitoring your Lambda Function</h1><p>Fortunately, monitoring and troubleshooting with Lambda is more straightforward than with some of the other AWS compute Services. That’s because a lot of important monitoring and logging metrics are already configured with CloudWatch by default and built into the Lambda dashboard. </p>
<p>Let’s take a look at some of the default metrics that are provided to you through the service. I won’t get into every metric that Lambda supports but I will call out some of the main ones. There are three main categories of metrics:</p>
<ul>
<li>Invocation metrics</li>
<li>Performance metrics </li>
<li>And concurrency metrics</li>
</ul>
<h3 id="Invocation-Metrics"><a href="#Invocation-Metrics" class="headerlink" title="Invocation Metrics"></a>Invocation Metrics</h3><p>Invocation metrics are related to the outcome of the invocation. For example, there is a metric called invocations that tracks the number of times the function has been invoked. Similarly, in this category, you also have the errors metric which counts the number of failed invocations of the function. </p>
<h3 id="Performance-Metrics"><a href="#Performance-Metrics" class="headerlink" title="Performance Metrics"></a>Performance Metrics</h3><p>Performance metrics on the other hand, follow exactly what the name suggests, and provide performance details about an invocation. These metrics include duration, which measures how long the function runs in milliseconds from when it is invoked until it terminates. This category also supports the IteratorAge metric which is only used for stream-based invocations such as Amazon Kinesis. It measures in time how long Lambda took to receive a batch of records to the time of the last record written to the stream. This IteratorAge is measured in milliseconds.</p>
<h3 id="Concurrency-Metrics"><a href="#Concurrency-Metrics" class="headerlink" title="Concurrency Metrics"></a>Concurrency Metrics</h3><p>Then the last category is concurrency metrics. Before we get into the metrics, let’s understand what concurrency is. When you want to monitor metrics for concurrency, you can use concurrent executions metric, which is a combined metric for all of your Lambda functions that you have running within your AWS account in addition to functions with a custom concurrency limit. It calculates the total sum of concurrent executions at any point in time. </p>
<h3 id="Logs"><a href="#Logs" class="headerlink" title="Logs"></a>Logs</h3><p>In addition to these metrics, CloudWatch also gathers log data sent by Lambda to help you better troubleshoot and understand issues. For each function that you have running, CloudWatch will create a different log group. </p>
<p>The log group name will be prefixed with aws and lambda. When you look at Lambda logs, you’ll see details about your function execution. You can also add in custom logging statements to your function that will display in these logs. For example, if you add a print statement to a Python Lambda function, you will see the output of that print statement in these logs. This enables you to push data to CloudWatch logs automatically in addition to the managed messages that are sent by default. </p>
<p>If you want more information - check out the AWS documentation on monitoring with Lambda.</p>
<h1 id="Compute-Summary"><a href="#Compute-Summary" class="headerlink" title="Compute Summary"></a>Compute Summary</h1><p>So you’ve reached the end of the compute section and that was a big section to complete. So congratulations on getting through. So in that last section, we covered services and features such as Amazon EC2, auto scaling, elastic load balancing and serverless compute as well, which focused on AWS Lambda. So to help you with your studies for the exam I want to call out some key points that you should keep in the forefront of your mind. As my one core focus is to ensure that you are prepared and have the knowledge in need when you’re sitting in that exam chair. </p>
<p>So let’s run through when you might select certain services or make specific configuration changes to meet the requirements of different questions. So starting with EC2 two as this is most frequently mentioned compute service on the exam. We start off by looking at AMI’s, Amazon Machine Images. So these are used as the baseline template of your EC2 instances. And now the first element you need to select when creating your instance for the exam, you should be aware of the different options that they offer such as the operating system that you’ll be running in addition to any other additional software. So you will be expected to know what comes with the AMI and what doesn’t?</p>
<p>When it comes to understanding a service, it always helps to get some hands on experience with it and EC2 is no exception. And you can use our labs for this and it will really help you to establish familiarity with the different steps involved in creating an instance. And this will help you answer a lot of questions. You need to be aware of the different instance types that are available. And how the compute power and performance values fluctuate with instant size. As we discussed, some instances provide better performance depending on if your workloads are memory intensive or perhaps require that accelerated computing performance to help with data pattern matching. So having an insight into these, will help you answer any questions relating to EC2 workload efficiency. </p>
<p>Now more than likely, you’ll be asked at some point to determine the best instance purchase option to help you optimize the cost of your environment. I think I’ve at least one or two questions on this each and every time I’ve set the exam. So it’s imperative that you know the difference between on-demand, spot and reserved instances. Now depending on the scenario you will have to demonstrate your understanding of these different purchase options to help you determine under which circumstance you should use each of them. If the question talks about how your workload is predictable and will be required for perhaps one or three years and you need to optimize costs, then reserved instances should come to mind and would likely be the answer. If the question highlights how the workload can be interrupted. And again you’re looking to build a cost efficient solution. Then this would be a good use case of spot instances. So review the key differences of the purchase options and understand their specific use cases to help you optimize costs.</p>
<p>From security point of view, tenancy options of your instances can come into play. Now by default, our instances run on shared tenancy. Whereby we share the underlying host with other customers. However, you might receive questions explaining that you need to secure your infrastructure to maintain compliance and ensure that your EC2 instances do not share any underlying host with any other customer. So how could you do that? Well, the answer would fall under your tenancy options. Either dedicated instances or dedicated hosts would resolve this issue. Now with dedicated hosts, it provides additional control over the placement of your EC2 instances on those hosts. So ensure you have a good understanding of your options here.</p>
<p>Another common question scenario that comes up. Test your knowledge and understanding of how to automatically run commands on the first boot cycle of your instance. For example, you might need to perform operating system updates or install additional software from a repository when your instance first boots up. So how would you achieve this? Well, the answer lies in the user data section of your instance during its configuration. It allows you to enter commands to do exactly that. Also on this point, you can also use metadata of the instance to see the user data configuration for that instance. And this can be found by going to 169.254.169.254&#x2F;latest&#x2F;meta-data.</p>
<p>As you may or may not know, security will always be a part of every AWS certification. And the solutions architect is no different. So what are the types of security questions that may appear from an EC2 point of view? Well, key pairs could be one topic to come up and these are used to encrypt the credentials to your instances, allowing you to connect to them. Ensure you are familiar with how to connect to both Windows and Linux based instances. Now Windows uses RDP on port 3389 and Linux uses SSH, which is on port 22.</p>
<p>Let’s now take a look at auto scaling and how this might present itself in the exam. When questions on auto scaling come up, you’ll be expected to know the main function of the service and the benefits it brings such as the ability to automatically increase or decrease your EC2 resources to meet the demands of your applications. For example, you might be asked to implement an efficient way to enhance the performance of the application after users complained of poor response. Now this might be caused by a bottleneck in your EC2 resources not being able to handle and process the amount of traffic. Now by implementing auto scaling, you could automatically increase your ET2 fleet size. Thereby you would increase the amount of resources and remove the bottleneck. </p>
<p>Now you might also be assessed on your ability to optimize the cost of your EC2 fleet. Now, one way would be to remove unused resources. By implementing auto scaling, you can scale in your EC2 fleet by terminating unused capacity based on set thresholds. So auto scaling is all about optimizing performance and cost. So look out for this as an option whenever you receive a question covering this topic. Now you will likely see questions with auto scaling interlinking with elastic load balances as well. And they work very well together. Elastic load balances allow you to manage loads across your target groups. Whereas EC2 auto scaling allows you to elastically scale those target groups based upon the demand. So from an exam perspective, ensure you can differentiate between auto-scaling and ELBs. Also, ensure you are familiar with the different ELBs that exist as you’ll be assessed on when to use one ELB over another in a particular situation. </p>
<p>For example, you might be presented with a network scenario where you need to determine when your ELB should be placed. Should it be an internal or external ELB? And we’ll be using to serve encrypted traffic. In which case, what do you need to configure? Well, if using HTTPS you’ll need a service certificate perhaps issued by AWS Certificate Manager. Another scenario that I’ve come across, assesses your ability of understanding how your ELBs react to targets in your target group that are marked as unhealthy following a health check. Now, does the ELB restart the instance? Does it launch another instance or does it just ignore it? Well for the ELB, it just ignores it and continues to send request to healthy instances. It’s the job of auto-scaling to launch replacement instances not the ELB.</p>
<p>Okay, so the last area I want to cover is AWS Lambda. Now this service isn’t covered extensively on the exam but you certainly need to be aware of it and when it would be used. Now, the key is knowing that it is a serverless compute service designed to run in event-based environments to run application code without having to manage and provision your own EC2 instances. It’s really cost-effective as you only pay for compute power when Lambda functions are invoked. In addition to being charged based on the number of times your function runs, known as invocations. So you might be presented with a question where you have an application that allows you to share photos that are uploaded to S3. But every time a new object is created, you want to process code to create a thumbnail of that object. What service would you use to do this with the least administrative effort? Now, this is a perfect example of when Lambda would be used. As its code-triggered by an event. And in this case, when a new object is uploaded is that event. And there were no resources to provision, to administer as it’s serverless.</p>
<p>Okay, so that now brings me to the end of this summary. We’ve highlighted some of the key points that we’ve learned from the previous course and we’ve looked at how to approach a number of different questions that might come up that relate to compute. So hopefully you should feel ready and prepared to tackle any questions in this area. So let’s now move on to the next section.</p>
<h1 id="4ECS-Elastic-Container-Service"><a href="#4ECS-Elastic-Container-Service" class="headerlink" title="4ECS - Elastic Container Service"></a>4<strong>ECS - Elastic Container Service</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/">Course: Introduction to Docker</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/basics-of-using-containers-in-production/container-orchestration-1/">Basics of using Containers in Production</a></p>
<h1 id="5ECR-Elastic-Container-Registry"><a href="#5ECR-Elastic-Container-Registry" class="headerlink" title="5ECR - Elastic Container Registry"></a>5<strong>ECR - Elastic Container Registry</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Course: Overview of AWS Identity &amp; Access Managment (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Docker Push</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">Docker Pull</a></p>
<h1 id="6EKS-Elastic-Container-Service-for-Kubernetes"><a href="#6EKS-Elastic-Container-Service-for-Kubernetes" class="headerlink" title="6EKS - Elastic Container Service for Kubernetes"></a>6<strong>EKS - Elastic Container Service for Kubernetes</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-kubernetes/">Course: Introduction to Kubernetes</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">Course: Introduction to EKS</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Install Kubectl</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/aws-iam-authenticator">Linux IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator">MacOS IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/windows/amd64/aws-iam-authenticator.exe">Windows IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">Configuration map to joing the Worker Node to the EKS Cluster</a></p>
<h1 id="7AWS-Elastic-Beanstalk"><a href="#7AWS-Elastic-Beanstalk" class="headerlink" title="7AWS Elastic Beanstalk"></a>7<strong>AWS Elastic Beanstalk</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/deploy-php-application-using-elastic-beanstalk/">Lab: Deploy a PHP application using Elastic Beanstalk</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/run-controlled-deploy-aws-elastic-beanstalk/">Lab: Run a Controlled Deploy With AWS Elastic Beanstalk</a></p>
<h1 id="12SSL-Server-Certificates"><a href="#12SSL-Server-Certificates" class="headerlink" title="12SSL Server Certificates"></a>12<strong>SSL Server Certificates</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#acm_region">Regions supported by ACM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html">How to retrieve and list server certificates via ACM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">Additional information on ACM</a></p>
<h1 id="13Application-Load-Balancers"><a href="#13Application-Load-Balancers" class="headerlink" title="13Application Load Balancers"></a>13<strong>Application Load Balancers</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/">Course: OSI and TCPIP Networking Models</a></p>
<h1 id="15Classic-Load-Balancers"><a href="#15Classic-Load-Balancers" class="headerlink" title="15Classic Load Balancers"></a>15<strong>Classic Load Balancers</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/elasticloadbalancing/features/#compare">Table showing the differences between the Load Balancers</a></p>
<h1 id="17Summary"><a href="#17Summary" class="headerlink" title="17Summary"></a>17<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/working-application-load-balancer/">Lab: Working with the Application Load Balancer</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/working-amazon-ec2-auto-scaling-groups/">Lab: Working with Amazon EC2 Auto Scaling Groups and Network Load Balancer</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/creating-your-first-auto-scaling-group/">Lab: Creating your first Auto Scaling Group</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-SAA-C03-Learning-Path-Introduction-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-SAA-C03-Learning-Path-Introduction-1/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-SAA-C03-Learning-Path-Introduction-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-18 22:13:01 / Modified: 22:13:02" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:01-04:00">2022-11-18</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-SAA-C03-Learning-Path-Introduction-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-SAA-C03-Learning-Path-Introduction-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:57" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:57-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:56" itemprop="dateModified" datetime="2022-11-20T19:01:56-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Cert-Prep-Certified-Cloud-Practitioner-for-AWS-26/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Cert-Prep-Certified-Cloud-Practitioner-for-AWS.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:55" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:55-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:54" itemprop="dateModified" datetime="2022-11-20T18:58:54-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Billing-Pricing-and-Support-CLF-C01-25/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Billing, Pricing, and Support in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce some Billing, Pricing, and Support resources currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Billing, Pricing, and Support resources in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to important Billing, Pricing, and Support resources in AWS, including:</p>
<ul>
<li>AWS Cost Management tools such as Cost Explorer and AWS Cost and Usage Reports;</li>
<li>The role of tags in cost allocation; and</li>
<li>AWS customer support resources and support plans.</li>
</ul>
<p>These objectives are covered by Domain 4 in the official AWS Certified Cloud Practitioner exam blueprint: Billing and Pricing, which accounts for 16% of the exam content. The other courses in this learning path will cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="Cloud-Economics-Basics-Part-One"><a href="#Cloud-Economics-Basics-Part-One" class="headerlink" title="Cloud Economics Basics Part One"></a>Cloud Economics Basics Part One</h1><p>Welcome to your very first Lecture “Understand how AWS Billing works.” Here we’ll give you the basics you need to understand AWS Billing.</p>
<p>We take you on a journey through the development from the classic data center to today’s public cloud. </p>
<p>In addition to important information about the modern Cloud Economy, we also show you the problems with the Public Cloud. </p>
<p>Cloud costs affect everyone, so it is important that IT, Finance, and Executive Departments understand each other. We convey the most important terminology from these areas and ensure mutual understanding. </p>
<p>Even though we are fans of practical learning, this will be a theoretical lesson to lay the foundations for the rest of the course.</p>
<p>Please join me into the first lecture.</p>
<p>Let’s take a look at where we actually come from. Until the year 2006, companies still relied on their own IT. IT costs were fixed, also called Capital Expenses or CapEx. </p>
<p>That has changed with the move to the cloud. IT and cloud costs are now much more variable and difficult to plan than before. We have moved to operational expenses, also known as OpEx. No worries if you don’t know exactly what these terms mean, we’ll cover them later.</p>
<p>We are currently at a tipping point where more is already being spent on cloud resources than on-premise. And the trend is rising. Forecasts predict that by 2022 we will have a total cloud spend of around 360 billion dollars.</p>
<p>In addition to many advantages, the cloud also has its downsides and we can see what these are in this graphic. At re:Invent 2019 310 business and IT executives were asked where their biggest problems lie in cloud operations. </p>
<p>The graphic speaks for itself, 29% stated that cost management is their biggest challenge. </p>
<p>This is still underpinned by a survey by Gartner. According to the survey, 80% of companies will overrun their Infrastructure as a service budget in 2020. </p>
<p>Reason enough to bring the knowledge about cloud cost optimization to the world, right?</p>
<p>Working in the Cloud comes with many benefits and variable expense control. However, due to the often new and unfamiliar environment, many users end up with paying more and struggling with cost management.</p>
<h1 id="Why-Cost-Optimization-Matters"><a href="#Why-Cost-Optimization-Matters" class="headerlink" title="Why Cost Optimization Matters"></a>Why Cost Optimization Matters</h1><p>Starting early with cost optimization is essential and can save a lot of trouble.</p>
<p>Many AWS cost projects do not start when you migrate to AWS, but costs get out of hand. And then it usually becomes even more difficult to change entrenched processes and behaviors. But do not be discouraged by this if you are in such a situation. This is why we have created this course, to help you relax, even in such difficult situations.</p>
<p>AWS offers fantastic opportunities for faster development, lower TCO, and increased agility.</p>
<p>No matter if a startup or large corporation, AWS grows with the requirements and always provides exactly the services that are currently needed - of course, only when used correctly.</p>
<p>Cost Optimization helps you meet your financial and business objectives while only paying for what you need and and use.</p>
<p>AWS is not a traditional data center and therefore should not be used as such. </p>
<p>AWS allows us to be much more agile and instead of spending time on administrative overhead, we can invest that time in innovation.</p>
<p>Companies see the greatest potential for savings by taking advantage of the agility of AWS and developing applications that scale and are highly automated.</p>
<p>Later in the course, we will cover the five pillars of the Well-Architected Framework, clarifying the key concepts and design principles of cloud workloads.</p>
<p>Cost optimization will help you manage your resources and money appropriately and in time to make economically wise decisions before expenses get out of hand and need to be fixed.</p>
<h1 id="Cloud-Economics-Basics-Part-Two"><a href="#Cloud-Economics-Basics-Part-Two" class="headerlink" title="Cloud Economics Basics Part Two"></a>Cloud Economics Basics Part Two</h1><p>Welcome to the section about the basics of cloud economics. The value of cloud extends beyond cost savings for your infrastructure. Cloud users can see significant improvements in other areas, including staff productivity, operational resilience, and business agility.</p>
<p>Let’s dig a little deeper into cloud economics to see what exactly is part of it, and where companies can improve their business. Of course, there’s cost savings or a lower TCO. Or in other words, infrastructure cost savings, or avoidance, from moving to the cloud. This can be reached through: A better ability to match supply and demand, and improving utilization; Elastic cost base driven by usage patterns; And through the elimination of hardware refresh and maintenance programs. Example for this is 50% reduction to total TCO.</p>
<p>Then there’s also the staff productivity, or efficiency improvement by function on a task-by-task basis. This can be reached through a higher maintenance efficiency through automation, the elimination of hardware-related tasks, and increased developer productivity. An example for this would be over 500 hours per year of server configuration time saved.</p>
<p>There’s also operational resilience, or the benefit of improved availability, security, and compliance. This can be reached through the reduced cost of planned and unplanned outages, a reduced risk profile, or cost of risk mitigation, and an improved service level agreement. And an example for this is the critical workload that is run in multiple AZs and Regions for strong disaster recovery.</p>
<p>Last but not least, there is business agility, or the faster deployment of new features and applications, while reducing errors. And this is reached through reduced time to market, increased operational agility, and the reduce costs, and increased pace of innovation. An example for this is a 75% faster launch of a new product.</p>
<p>Now let’s take a look at some real numbers to see what this means exactly. In 2018, IDC, one of the largest market research firms in the world, asked 27 major corporations how their use of AWS has impacted their business. And as we can see here, they had a 62% more efficient IT infrastructure staff, nearly three times more features delivered, and 94% less time lost to unplanned downtime.</p>
<p>This is of course only a sample of 27 companies. And of course, the perspectives are not supplied everywhere. But you get a good overview of what is possible when using cloud properly. Cloud economics is not only about cost savings. However, with the right usage the cloud can help your entire business become more efficient.</p>
<p>In the past, engineering teams needed the approval of the finance team to get new resources. This process works, but it’s incredibly slow, and would destroy all of the agility in the cloud. So how can this process be adapted to the cloud? The biggest hurdle here is not to get financial issues in the way of IT teams, and to involve financial teams in planning, procurement, and forecasting.</p>
<p>Based on many companies’ experience, AWS has created a high-level best practice framework that is also called the five pillars of cost optimization. The five cost optimization pillars apply, regardless of your workload or infrastructure across nearly all environments.</p>
<p>The pillars of cost optimization are defined by AWS as follows:</p>
<p>Right-sizing: Ensure that what you provision matches what you need. For example, for compute, you provision for CPU, memory, storage, and network throughput.</p>
<p>Increase elasticity: Traditional IT costs and hardware requirements are tailored for peak usage and are rarely turned off. In the cloud, you can optimize costs to meet dynamic needs and turn resources off when they are not needed. For example, you can usually turn off non-production instances for 70% or more of any given week.</p>
<p>Leverage the right pricing model: AWS provides a range of pricing models. For example, On-Demand and Spot Instances for variable workloads, and Reserved Instances for predictable workloads. Choose the right pricing model to optimize costs based on the nature of your workload.</p>
<p>Optimize storage: AWS provides multiple storage tiers at prices designed to meet performance. By identifying the most appropriate destination for specific data types, you can reduce Amazon Elastic Block Store and Amazon Simple Storage Service while maintaining the required performance and availability. For example, where performance requirements are lower, using Amazon EBS Throughput Optimized HDD typically cost half as much as the default General Purpose SSD.</p>
<p>Measure, monitor, and improve: To ensure that you extract the full economic potential of the AWS Cloud at any scale, you want to: Define and enforce cost allocation tagging. And you will hear me talking about tagging a lot within this course.</p>
<p>Forecast usage and costs: Define metric set targets and review at a reasonable cadence. Enable teams to architect for costs via training, visualization of progress goals, and a balance of incentives, and assign optimization responsibility to an individual or to a team.</p>
<h1 id="Total-Cost-of-Ownership"><a href="#Total-Cost-of-Ownership" class="headerlink" title="Total Cost of Ownership"></a>Total Cost of Ownership</h1><p>Welcome to this section about TCO or Total Cost of Ownership. TCO was already mentioned a few times. TCO stands for Total cost of ownership and is a comprehensive assessment of IT’s total costs or other costs over time.</p>
<p>For IT, TCO includes hardware and software acquisition, management and support, communications, end-user expenses, and the opportunity cost of downtime, training, and other productivity losses.</p>
<p>Comparing the TCO of different environments is not an easy task. Especially in the “old world,” the on-premise world, many factors play a role in this assessment, as this overview shows.</p>
<p>Let’s take the facility costs for example. Something like space, power, cooling. It’s nothing you have to worry about when using AWS resources. </p>
<p>Using the cloud has greatly simplified the TCO assessment.</p>
<p>Because most additional costs are included in AWS’s service prices, there are far fewer factors to take into account.</p>
<p>As someone who started his IT career by assembling servers and configuring switches, I can assure you that physical IT operation involves a lot of effort, costs, and stress.</p>
<p>Today, an EC2 instance can be started with just a few clicks; in the past, a server had to be assembled, installed, mounted in the data center, and wired. Also, there were costs for maintenance, cooling, space, power, licenses, and other factors.</p>
<p>Understanding TCO improves the overview on the business and helps you recognize issues and opportunities at an early stage.</p>
<h1 id="Economies-of-Scale"><a href="#Economies-of-Scale" class="headerlink" title="Economies of Scale"></a>Economies of Scale</h1><h1 id="Welcome-to-this-section-about-economies-of-scale-Every-time-you-attend-AWS-Re-Invent-the-largest-AWS-conference-worldwide-you-can-see-this-slide"><a href="#Welcome-to-this-section-about-economies-of-scale-Every-time-you-attend-AWS-Re-Invent-the-largest-AWS-conference-worldwide-you-can-see-this-slide" class="headerlink" title="Welcome to this section about economies of scale. Every time you attend AWS Re:Invent, the largest AWS conference worldwide, you can see this slide."></a>Welcome to this section about economies of scale. Every time you attend AWS Re:Invent, the largest AWS conference worldwide, you can see this slide.</h1><p>The only thing that changes every year is the number of price reductions. Currently, this number is 69. </p>
<p>How does AWS manage to reduce prices continuously?</p>
<p>The answer is quite simple: Economies of scale. But what does that mean?</p>
<p>Let’s take a look at this cycle. More customers lead to higher AWS usage which leads to more infrastructure. The economies of scale effect occurs because costs can now be spread over a larger number of customers. The infrastructure becomes cheaper, prices can be reduced and new customers benefit from the lower prices. And then the cycle starts all over again. </p>
<p>With AWS you’re likely to experience lower prices over time as their infrastructure grows and the economies of scale takes place.</p>
<h1 id="Pricing-Calculator"><a href="#Pricing-Calculator" class="headerlink" title="Pricing Calculator"></a>Pricing Calculator</h1><p>With the AWS Pricing Calculator, you have a powerful tool to estimate costs, reduce running expenses and find the cheaper, better suited solution for your environment. Welcome to the section about the AWS Pricing Calculator.</p>
<p>The Pricing Calculator has undergone many name changes and innovations over the years. The current version as of fall 2020 is a great help to estimate the total cost of your AWS environment, find the right instance sizes and compare services across different regions.</p>
<p>Let’s have a look at how the tools work exactly. When you generate an estimate, you can either add services directly to your estimate or create a group and add the services to your group. This time I will show to set up a group with an Amazon EC2 instance that you can use to perform tasks, such as, run a small program or host a website.</p>
<p>We’ll now create an estimate and assign it to a region. So we click on, Create Estimate and we need to look up the service we’re looking for. Wanna estimate an EC2 instance, so we’re looking here for EC2 and click on, Configure. We have to choose a region.</p>
<p>Bear in mind, every region has a different pricing. There are cheaper and there are more expensive regions. We will take Frankfurt as this is where I’m sitting currently. You can either create quick estimate or an advanced estimate. We will go for the advanced estimate, because this gives us the option to also choose the kind of workload that we have.</p>
<p>Let’s assume we have an online blog and we have many users or visitors over the weekends. So we choose daily spike traffic. And we assume that we have a lot of visitors on Friday, Saturday and Sunday. We have a baseline of, let’s say, three instances and during peak times over the weekend, we have 10. Peak times are eight hours and we need to find an instance.</p>
<p>Let’s say we need two CPUs and four gig of RAM and the tool tells us which instance would be the cheapest, as you can see here. Let’s say we take the t3a. The A stands for an AMD chip set. We can see we have four gigs of memory. Two CPUs. Enough network traffic. And that’s good enough.</p>
<p>Here you can choose the pricing models. We will cover this later within the course. So we’ll just go for On-Demand here. 10 gigs of storage is enough. Daily, two daily snapshots. We also have some data transfer. Let’s say, 10 gigs. Intra-region transfer, no. Outbound, yeah, let’s say also 10 gigs, just to get some numbers in here. And you can see here, we would have EC2 costs monthly of 72 and some storage costs of $50. </p>
<p>We have $87 total cost for our EC2 instance and bare in mind that we have a spikey workload. So these with these usage patterns are already calculated in here. We can now add this calculation to our estimate and see how much our website or our blog would cost us for 12 total months.</p>
<p>We could now save our estimate or add another service. For example, let’s say we also need a database, maybe like MySQL database. Notice that’s a huge instance here. Let’s go for a small one, m5.2xlarge. It’s also a big a one, but it’s just for numbers, right? On-Demand is fine. 20 gigs of storage.</p>
<p>Then we can see the database would cost us $592, plus a little bit of storage. We can also add this to our estimate and we can now see the total price here and so it goes on. You can add the services and tools that you need here and get a very accurate estimate for your total costs.</p>
<h1 id="AWS-Well-Architected-Framework"><a href="#AWS-Well-Architected-Framework" class="headerlink" title="AWS Well-Architected Framework"></a>AWS Well-Architected Framework</h1><p>AWS has developed a structure of best practices and strategies when building software in the cloud, called the AWS Well-Architected Framework. </p>
<p>Based on knowledge collected and gained by AWS solution architects and AWS users over the past years it is made to provide strategies and proven concepts for cloud systems.</p>
<p>The framework is based on five pillars that ensure a secure, reliable, efficient, and cost-effective system.</p>
<p><strong>The 5 Pillars of the AWS Well-Architected Framework - Overview:</strong></p>
<ul>
<li><p>Operational excellence</p>
</li>
<li><ul>
<li>aims at constant improvement and efficient managing of workloads, as well as gaining operational insights and continuous improvement of processes and procedures to support business value</li>
</ul>
</li>
<li><p>Security</p>
</li>
<li><ul>
<li>describes how to protect data, systems, and components using cloud technologies, how user rights and privileges are correctly managed, and how integrity and conformity of information is maintained</li>
</ul>
</li>
<li><p>Reliability</p>
</li>
<li><ul>
<li>focuses on the ability of a workload to perform correctly and as intended at the expected time. Including quickly recovery and prevention from failures</li>
</ul>
</li>
<li><p>Performances and efficiency</p>
</li>
<li><ul>
<li>Efficient allocation and right-sizing of computing resources by the system requirements and interception of demand changes</li>
</ul>
</li>
<li><p>Cost optimization</p>
</li>
<li><ul>
<li>Understanding and controlling expenses, avoiding unnecessary costs, and analyzing spend in detail</li>
</ul>
</li>
</ul>
<p>Furthermore, AWS provides the AWS Well-Architected Tool, which is based on the framework‘s principals. The tool helps to review the user workloads and compare them to the latest architectural best practices.</p>
<p>In general, the AWS Well-Architected Framework and the tool act as general guidelines that can be used without additional costs.</p>
<p>As a user, you will benefit from free architectural guidance, proven and tested strategies, and best practices directly from AWS for your workloads.</p>
<p>By making use of the AWS Well-Architected Framework and its tool, you can avoid pitfalls and failures by relying on proven strategies and professional practices for your own system, without any additional costs. The framework guarantees you improvements on security, reliability, efficiency, and on the financial end.</p>
<h1 id="Working-with-the-Pillars-of-the-AWS-Well-Architected-Framework"><a href="#Working-with-the-Pillars-of-the-AWS-Well-Architected-Framework" class="headerlink" title="Working with the Pillars of the AWS Well-Architected Framework"></a>Working with the Pillars of the AWS Well-Architected Framework</h1><p>Striving for excellence by applying as many best practices as possible into the workloads, with the aim to reduce human error, save time and resources by automation and continually improve processes.</p>
<p>Principles:</p>
<ol>
<li>Operations-as-Code</li>
</ol>
<p>The entire workload, i.e., the development of applications together with the infrastructure, should consist exclusively of code. This way, operational errors can be reduced, and the execution, as well as updates or changes, can be automated, which saves time and resources.</p>
<ol>
<li>Frequent but small and reversible changes</li>
</ol>
<p>The development and infrastructure should be designed for small and light updates that are made more frequently and should be easily reversible if necessary, without breaking anything.</p>
<ol>
<li>Evolve procedures alongside the workload</li>
</ol>
<p>As software development progresses, so should the associated processes. Regular and dedicated routines can help to find improvement opportunities, like finding processes that can be automated, and validate the effectiveness of the procedures.</p>
<ol>
<li>Failure prevention</li>
</ol>
<p>Identify sources of failures and remove the cause before issues reoccur by performing preventive exercises. Regular events and simulated exercises will increase awareness inside the team and improve reaction times.</p>
<ol>
<li>Learning from operational failures</li>
</ol>
<p>Every incident and failure is a lesson to analyze and learn from. Lessons and appropriate improvements should be well documented and shared throughout the whole company.</p>
<p>As customer needs and business demands can change rapidly, it is wise to design the development infrastructure to support agility and possible changes in advance. **<br>**</p>
<p>Additionally, keeping lessons on success and failure well documented and easily accessible helps maintain the best possible performance and reduce spending time on recurring decision-making.</p>
<p>That was the first pillar, operational excellence. The next one will be Security.</p>
<p>The goal of the security pillar is to provide the highest possible security on data, systems, and components.</p>
<ol>
<li>Implementation of a strong identity foundation</li>
</ol>
<p>Going by the principle of least privilege, users are granted only as much access as they need to fulfill a task. Appropriate authorization should be a major requirement for all resources in the cloud system.</p>
<ol>
<li>Traceability</li>
</ol>
<p>Real-time monitoring, alerting, and auditing actions and changes in the environment should be logged and made available for automatic investigation and action routines.</p>
<ol>
<li>Apply security at all levels</li>
</ol>
<p>Defensive strategies should be applied to all possible levels.</p>
<ol>
<li>Security by automation</li>
</ol>
<p>By using automated security mechanisms, architectures gain the ability to scale faster and more cost-effectively.</p>
<ol>
<li>Data protection</li>
</ol>
<p>Make the use of data encryption, tokenization, and access control mandatory.</p>
<ol>
<li>Access</li>
</ol>
<p>Reduce the risk of improper handling or modification of sensitive data by users by preventing direct access when not needed.</p>
<ol>
<li>Incident preparation</li>
</ol>
<p>Prepare incident management and investigation policy in advance to handle incidences when they occur.</p>
<p>When designing the system architecture, identity control, and access on multiple layers should be prioritized from the beginning to avoid major changes at a later state.</p>
<p>Recognize demand changes as well as disruptions at an early state to acquire resources in time and recover from failures automatically.</p>
<ol>
<li>Automatic recovery</li>
</ol>
<p>Identify potential outages by using Key Performance Indicators (KPIs) on the workload that will trigger the monitoring system. This allows you to take appropriate action to either prevent the outage or automatically begin remediation.</p>
<ol>
<li>Test recovery procedures</li>
</ol>
<p>Develop recovery strategies by testing different failure scenarios. Other than an On-Premise environment, the cloud allows simulating variously scaled scenarios.</p>
<ol>
<li>Horizontal scaling for better availability</li>
</ol>
<p>Prevent common points of failure by scaling the infrastructure horizontally, i.e., distributing requests across multiple resources rather than hoarding the entire workload on a single resource.</p>
<ol>
<li>Stop guessing capacity</li>
</ol>
<p>Use monitoring as a tool to detect demand and scale the environment accordingly by automated addition or removal.</p>
<ol>
<li>Manage changes in automation</li>
</ol>
<p>Changes to the infrastructure should only be made by automated and trackable actions.</p>
<p>Monitoring and logging is the fundament of a reliable system. By analyzing logged metric data and responding accordingly in time, failures can be detected beforehand and automatically repair themselves.</p>
<p>Efficient usage of computer resources to meet system requirements while demand and technological advances may change.**<br>**</p>
<ol>
<li>Make use of advanced technologies</li>
</ol>
<p>In times of cloud computing, many software solutions come as a service. Using software as a service instead of hosting, operating, and managing a tool provides more free time and resources for the development team.</p>
<ol>
<li>Global in minutes</li>
</ol>
<p>With AWS Regions, the workload can be deployed all around the world and at various scales, which allows the reduction of latency for customers.</p>
<ol>
<li>Serverless architectures</li>
</ol>
<p>Today, many traditional computing activities can be realized through serverless solutions that can be maintained without the need for a physical server, thus saving costs for operation, management, and operations.</p>
<ol>
<li>Experiment more</li>
</ol>
<p>Various tests can be performed with virtual resources to determine which service or resource and which type of configuration is best suited for individual requirements.</p>
<ol>
<li>Know the options - make the right choices</li>
</ol>
<p>Make sure to get to know about the service that aligns best with individual workload goals. For example, when making decisions, consider the appropriate database or storage concept that best suits the needs.</p>
<p>Use a data-driven approach to select high-performance designs. Collect data on all areas of your architecture, monitor deviations from expected performance, and then take action.</p>
<p>Deliver business value at the lowest possible price. Reduce upfront fixed costs and profit from controllable and small ongoing expenses.</p>
<ol>
<li>Implement Cloud Financial Management</li>
</ol>
<p>Build capabilities to manage and spread awareness of costs and expenses in the cloud environment.</p>
<ol>
<li>Adopt a consumption model</li>
</ol>
<p>Pay as you go. Analyze the actual business needs and match resources to current requirements.</p>
<ol>
<li>Measure overall efficiency</li>
</ol>
<p>Measure workload business performance and the costs associated with deployment. Use these metrics to determine the gains you make by increasing performance and reducing costs.</p>
<ol>
<li>Stop spending money on data center operations</li>
</ol>
<p>AWS takes over traditional data center tasks and does not charge extra to manage and update operating systems or applications with managed services. </p>
<ol>
<li>Analyze and attribute expenses</li>
</ol>
<p>AWS makes it easy to identify system usage and costs and transparently allocate IT costs to individual workload owners based on this data. This helps you measure return on sales (ROI) and enables workload owners to optimize resources and reduce costs.</p>
<p>Getting the best performance and value while spending as little as possible can be achieved by applying the whole AWS Well-Architected Framework correctly on the individual business and cloud environment. The most vital part is to spread awareness of expenses and correct resource usage across the team to get rid of inefficiency</p>
<h1 id="Understanding-the-Other-Departments"><a href="#Understanding-the-Other-Departments" class="headerlink" title="Understanding the Other Departments"></a>Understanding the Other Departments</h1><p>Every department and every job has its own language. When people from different areas meet, communication is often difficult because important terms are not familiar to the other side.</p>
<p>Take the example of IT and finance. The topics of these two areas are far apart.</p>
<p>But as we have already learned, it is important to establish communication between these areas and this is exactly what we want to help you with in this lesson.</p>
<p>We will introduce you to the most important terms from AWS but also from software development and finance.</p>
<p>AWS has hundreds of different services and you can probably fill a whole dictionary with terms. Here we would like to explain the most important terms that are also important for the Cloud Financial Management area.</p>
<h1 id="General-AWS-Terminology"><a href="#General-AWS-Terminology" class="headerlink" title="General AWS Terminology"></a>General AWS Terminology</h1><p>Account (AWS) - AWS services are housed within an Account. Accounts can be Master Payer accounts that contain billing data or Linked Accounts which do not. AWS Organizations and other services can be used to manage Accounts within AWS. Many AWS services can span Account boundaries.</p>
<p>RI -Reserved Instance - a commitment to use a cloud resource, usually of a specific type, location, and size, for some period of time, usually 1 or 3 years, in exchange for a discounted rate.</p>
<p>SP - Savings Plans are very similar to Reserved Instances but more flexible and can only be applied to compute usage.</p>
<p>AURI, PURI, NURI &#x2F; SP (SavingsPlans)</p>
<p>All Upfront Reserved instance, Partial Upfront Reserved Instance and No Upfront Reserved Instance. Some people use these acronyms when referring to reserved instances, in case you hear them.</p>
<p>EC2 (AWS) Elastic Compute Cloud AWS’ virtual computer cloud offering</p>
<p>AWS supports a variety of instance, Instance Type, Family, Generation, Size (AWS) - Instance refers to a specific EC2 virtual machine. Instance Families, designated by letter, an instance Generation designated by a number and optionally other letters, and instance sizes which follow a structure of nano, micro, small, medium, large, xlarge, 2xlarge, etc. The Instance type includes the entire ndesignation, such as m5a.4xlarge which would be an “m” family, 5th generation, “a” for AMD chipset, 4xlarge sized instance. </p>
<p>IAM - Identity and Access Management - is the way that AWS refer to their system of granting and governing permissions within their cloud platforms.</p>
<p>Tags are metadata attached to a specific ressource running in AWS. They are meant to provide contextual information about the resource. Tags can be created with the resource in most cases or added after the fact manually or systematically. Tags are useful for identifying the type of resource, the environment it supports, the owner, the cost center, etc. </p>
<p>Tags can be queried or accessed in a wide variety of ways and can be used to drive automation, divide costs, or for other important purposes. Most large cloud-using organizations will at some point establish governance policies around tag use and require specific tags be used on all resources.</p>
<p>Console</p>
<p>Web-based portal from where you can manage your accounts or access AWS services. </p>
<p>Convertible &#x2F; Standard</p>
<p>AWS terms referring to the ability to convert reserved instances for some resources to different specifications. Standard RIs cannot be converted or changed for their entire term. Convertibility reduces the discount offered by AWS.</p>
<p>Region </p>
<p>AWS has the concept of a Region, which is a physical location around the world where data centers are clustered. An AWS Region consists of multiple, isolated, and physically separate AZ’s within a geographic area. Regions are generally guaranteed to be more than a minimum distance from one another to satisfy disaster recovery requirements.</p>
<p>Availability Zones (AZ) are sub-units of a Region, there are typically multiple AZs per Region. AZs are made up of multiple physical data centers but can generally be thought of as being very closely situated from a network latency and performance perspective.</p>
<h1 id="Terminology-from-Software-Development-amp-Operations"><a href="#Terminology-from-Software-Development-amp-Operations" class="headerlink" title="Terminology from Software Development &amp; Operations"></a>Terminology from Software Development &amp; Operations</h1><p>You’ve probably heard this one here before: DevOps. DevOps is a set of practices that intends to break down traditional silos between developers and operators of computer systems, allowing combined teams to collaborate and deliver software in a more consistent, efficient and automated fashion.</p>
<p>Enterprise Architecture, or EA, groups are traditionally tasked with outlining the structure of the systems an enterprise will build and maintain to achieve its business goals. Like physical architects, they provide the blueprints for how the various systems should be put together, the “materials” or software concepts that should be used to build them, and how the end results should look.</p>
<p>Lift &amp; Shift. Lift &amp; Shift is a method of migration involving moving an application as currently architected and built from one environment (an on-premises data center) to another (usually a public cloud). Lift &amp; Shift migrations can usually be done more quickly as they often do not require substantial change to the application code or configuration.</p>
<p>However, because they do not modify applications to use cloud-native services, they tend to create situations where the cloud system is more expensive or difficult to run than the on-premises system had been. </p>
<p>Lift &amp; Shift migrations are typically used when time pressure to close a data center or other need outweighs the cost and quality issues. A remediation period for the environment should always be planned After a Lift &amp; Shift migration to address issues.</p>
<p>Workload is a generic name for an application or software system running on a computing or other platform.</p>
<p>In a traditional website, there might be a web server, an application server and a database server, each running on an individual hardware-based server, or virtual machine in my data center, each of those three elements of the application would be a workload running on that virtual server. </p>
<p>On-Premises (or On-Prem) is a term used to refer to company-owned or company-controlled data center space. Usually used to differentiate from public cloud environments where application migrations are targeting workloads.</p>
<p>Companies have an extensive On-premises infrastructure built over many years when they begin using the cloud, and there are often difficulties using systems, infrastructure or processes developed for the on-premises environment in the public cloud.</p>
<p>Rightsizing is a form of optimization where measurements are taken over time to match workloads to a virtual resource sized to run it efficiently with a minimum of waste. Rightsizing can be used as a technique to save costs but must always involve technology oversight as well.</p>
<p>Agile is a method of project management, used primarily for software development characterized by division of tasks to short phases of work (into sprints) and frequent assessment of priorities and plans. Generally, leads to development of products or software incrementally beginning with a minimum viable product and then continually enhancing it from a backlog of requirements.</p>
<h1 id="Terminology-from-the-Finance-amp-Accounting-World"><a href="#Terminology-from-the-Finance-amp-Accounting-World" class="headerlink" title="Terminology from the Finance &amp; Accounting World"></a>Terminology from the Finance &amp; Accounting World</h1><p>We are now getting to the last part of our terminology section, Now with terms from the finance and accounting world. And we’re starting with amortization. This means retiring a payment of capital gradually over time on a schedule which reflects the benefits the capital provides in each period. Like depreciation, amortization typically applies to the retirement of cash payments, where depreciation tends to apply to physical capital equipment. An upfront RI payment can be amortized over the usual lifetime (1 or 3 years) of the RI itself.</p>
<p>Variable Costs are costs that varies according to the business volume it supports. A company hosting websites would need to pay for more computers to host more websites, and so that cost per website is a variable cost.</p>
<p>Upfront Charges for Reserved instances or savings plans. Reserved instances or service reservations, in general, can typically be purchased with a full upfront payment (All Upfront), a partial upfront payment plus a reduced periodic charge (Partial-upfront), or with no upfront charge (No-Upfront). The upfront charge may be amortized over the life of the RI. Upfront charges might be treated as Prepaid Expenses on the Balance Sheet but check this with your accountants!</p>
<p>Opex or Operating Expenditure is a category of business expense made in a specific accounting period which provide benefits only in that accounting period. Purchasing on demand cloud services is considered an Operating Expenditure. Operating expenditures require no long-term tracking of depreciation or amortization but are subtracted from earnings in the period incurred.</p>
<p>ROI - Return on Investment is the amount of profit from an investment made, usually expressed as a percentage of the original total cost invested. In a cloud rightsizing business case, the ROI might be calculated as the savings in cloud expenditure expected less the engineering and other costs required to take the rightsizing action.</p>
<p>The Income Statement (sometimes referred to as a P&amp;L statement) is a statement showing the company’s net profit or loss over a period of time (a month, a quarter, a year, etc.) The income statement would show expenses and amortization incurred during the period, so in year two of a 3-year RI, the amortization for the second year would show up as an expense against earnings in the period covered.</p>
<p>NPV or Net Present Value. An assessment used to calculate the long-term profitability of a project made by adding together all the revenue it can be expected to achieve over its whole life and deducting all the costs involved, discounting both future costs and revenue at an appropriate rate. In a cloud business case, the net present value of all the cash flows of a no-upfront RI might be compared to the current cash value of the all upfront RI for determining which is better for the business.</p>
<p>Depreciation means retiring the cost of an asset gradually overtime on a schedule which reflects the provision of benefits. Often this reflects the decrease in value of an asset over time due to wear and tear or usefulness because of continued use in out periods.</p>
<p>EBITDA. Earnings Before Interest, Taxes, Depreciation, and Amortization. An assessment of the earnings expected when subtracting only the cost of goods sold from the revenue achieved. Tracking the prepaid expenses of a 3-year all-upfront Reserved Instance as a cash outlay that can be amortized over three years would affect EBITDA differently than if the resources were purchased using cash at on-demand rates. </p>
<p>Capitalization is the ability to treat an investment or outlay as a capital item which will be depreciated or amortized in future periods. Cost Allocation is in FinOps, the ability to identify and allocate costs to the appropriate cost categories in use. Ideally direct costs, amortized costs, and shared costs can be allocated to individual budgeting categories for a clear view of the entire cost of running my application or workload in the cloud.</p>
<p>Unit Economics is the ability to directly compare my overall cost to the overall business benefit l am creating on a per unit basis. For example, if I understand that the overall cost of running my website infrastructure is $5Mil per month and is able to support 10,000,000 paid hosted web pages, then I can track a Webpage&#x2F;$ metric of “2” which indicates how efficiently I run my service. Any future modifications to my cloud infrastructure can then be expressed in terms of the Webpage&#x2F;$ metric to determine if they are helping or hurting, and opportunities for cost savings can be expressed in terms of how they impact Webpages&#x2F;$.</p>
<p>Fixed Cost. A cost which does not change with changes in business volume. The cost of a data center building mortgage is a fixed cost in that it does not vary regardless of whether there it is supporting 1 web server or 1,000,000 web servers driving the company’s revenue.</p>
<p>Balance Sheet. A statement of financial position of the business on a specific date which indicates the value of all assets and liabilities as of that date, including the retained value of any undepreciated or unamortized capitalizable items. A company purchasing a 3-year RiI at the beginning of a year would show that RI with % of its original value on the Balance Sheet on the last day of that year.</p>
<p>Capex Capital Expenditure - the purchase of a capitalizable asset, such as a building or equipment meant to provide value over a long term and thus to be depreciated or amortized over that term. Purchasing a data center and using it over 30 years is considered a Capital Expenditure while paying to run a virtual server in the cloud for this month is not.</p>
<p>Congratulations! You passed a heavy theoretical part and learned about the most important terms and their meanings in the cloud and finance context!</p>
<h1 id="Bills-and-Cost-Drivers"><a href="#Bills-and-Cost-Drivers" class="headerlink" title="Bills and Cost Drivers"></a>Bills and Cost Drivers</h1><p>Welcome to the first live demo. As you can see here we are in the AWS Management Console. The most fundamental part is to get a good overview of the environment. Therefore you have to get a broad view of all the services, usage, and expenses from the past, and the present.</p>
<p>To set a base for our analysis, we first need to get some numbers. For this we will be using the billing dashboard which is also our main platform for gathering information on expenses. We will deal a lot with the billing and cost management dashboard alongside but the very first thing I like to investigate when I start with an unfamiliar account or environment is to check the last bills.</p>
<p>So to get there, you can either type bill here in the search field to get straight to the billing service or you can just go here over the menu to the billing dashboard. Here, you get a good first overview what is going on in your account. What are the most expensive services? How much have you spent so far? And what is the forecast? The forecast to the current month. But we will talk about this later.</p>
<p>So first thing here is the bill section here on the left side. I wanna get to overview from the last month, how much we spent last month and what were the biggest cost drivers. So as we are currently in January 2021, I go one month back to December 2020, here from the dropdown menu. And as you can see, I have a total amount, how much we spent last month.</p>
<p>I have like different information about taxes and payment summaries. And I can also get a CSV file from my whole bill or just printed as a PDF. So as you can see here we are in a master payer account, that means we’re using AWS Organizations which enables us to link other accounts to this account.</p>
<p>So all the costs that are caused by the under linked accounts are covered by this account that are collected here. You can see there are many accounts but let’s focus on this overview here. So what I wanna get here is the first overview. What are the most expensive services, or like what are the biggest cost drivers.</p>
<p>You can see we’re spending like small amounts, like compared to the total we’re spending small amounts on like different services. For example, here $34 for the API Gateway or 91 for CloudFront. As this is like just like a very very small percentage of our total amount, it would make no sense or like almost no sense to dive into deeper analysis here for CloudFront. It just would make no sense because like the amounts are here so small and this makes also like our whole analysis much easier because we know the only thing we have to focus is Elastic Compute Cloud because this is the biggest cost driver in this case. And when we go here, we can see, okay, we’re using like quite a lot of regions here.</p>
<p>So my first question would be, why do we need so many regions? And as you probably know, the regions in Asia are way more expensive than for example, the regions in Northern Virginia. So my first question would be, why do we have all these different regions? And as you can see this is like just my thinking process, how I would approach such an analysis or such an optimization process but we will talk about this later in detail. Let’s go through the next section.</p>
<h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p>AWS credits are applied to bills to help cover costs that are associated with eligible services. They are applied until they are exhausted or they expire. With credits AWS introduced a sort of reward system for particularly active users and developers. You can use them instead of spending money on certain services.</p>
<p>So you may ask yourself, “How do I get credits?” So, there are multiple ways. For example, developing and publishing a skill for Alexa or by attending webinars and events. As a startup, you can use the AWS Activate program or through the AWS Credit Program for Nonprofits.</p>
<p>In my case, let’s jump here to the credits section, I did a few AWS certifications, and for that I was rewarded by AWS with two $300 credit vouchers which is pretty cool. I used them here for my private account to play around with a few services that I was interested in and as you can see here, you can see a list of which these credits can be applied to. And there are almost all services that are available for AWS. And what’s quite funny is, the AWS cost explorer that can also cost money when you are using the API is not listed here. So, for the billing services and the cost services you can’t use these credits.</p>
<h1 id="Cost-Explorer"><a href="#Cost-Explorer" class="headerlink" title="Cost Explorer"></a>Cost Explorer</h1><p>In this section, we will take a look on how to use the AWS Cost Explorer, one of the most essential tools for our Cloud Financial Management within AWS. Moreover, the Cost Explorer is the very main tool for you to gather information and analyze all the costs and expenses in your environment. So let me introduce you to the tool itself.</p>
<p>To get there, you can go over to your Billing Dashboard. From here, go to the Cost Explorer, and just launch it. See, you’re getting guided to a whole new menu where you can start to analyze your costs. The Cost Explorer is a built-in tool that lets you get deeper insights into the costs and usage of your cloud environment.</p>
<p>With the help of the Cost Explorer, you will be able to identify trends, hunt down the biggest cost drivers and detect anomalies. And the best part, the Cost Explorer, in its base variation, does not cost anything. You can use it for free. However, take note that there can be additional costs when you use tools to request the API of the Cost Explorer.</p>
<p>Every API request costs about one cent, which could drive up your costs very fast when you fire up a lot of API requests. With the Cost Explorer, you can visualize your usage patterns over time and identify underlying cost drivers.</p>
<p>What we see here is the very first view that you get when you open the Cost Explorer. It is grouped by the different services that you use. You get a six-month overview, and the visualization type is bar, which means that the costs are stacked to each other in these kind of bars here. You can also change this to stacks or to lines, what doesn’t make too much sense in this case. So let’s stick to the bar view.</p>
<p>The legend here below shows you which service is illustrated by which color. In this example, we can see over the last six months our EC2 usage has increased quite a lot. If you scroll down a little bit you can see the detailed data table. Below the Cost Explorer here is the data table, which provides a deeper insight into the data seen in your chart. And you can also export this as a CSV file for further processing with a table tool like Excel if you wanna make forecasts or like further analysis. </p>
<p>You can also like, like I said we have the last six months here, you can also like scroll vertically to see all the services and cost drivers and you can scroll horizontally to see each month. It’s basically just a table.</p>
<p>You can choose the exact timeframe you want to see within Cost Explorer. By default, it’s on six months. So you can either select day by day that you wanna see or you can choose to auto select here to see for example like just last seven days, current month, three months, six months, one year, months to date, year to date, or three months forecast or 12 months forecast.</p>
<p>Let’s go for the last seven days here. So we can see how much we spend for the day different services day by day. We can also delete the group by filter here and get like monthly view. It makes no sense because we wanna see days. So you’re gonna select daily here and then we can see how much we spend day by day, if there’s, for example, an anomaly. We can see, oh wow, like way more than the other days. Why is that?</p>
<p>You can see we are here January 1st the first day of the month. And on the first day of the months, you usually have to pay for tags, reserved instances, and all that stuff that incurred like more costs than over the other days. And that could be the explanation for this anomally here.</p>
<p>And you may have noticed you can also go from a daily to an hourly view, but it is deactivated. I can’t select it. Why is that? The reason for that is super easy. You have to pay for the hourly view because you need like much more data records. So you have to enable it first. If you really wanna go in that much detail. I will show you how you can do this.</p>
<p>Hourly view, you have to go to preferences. And this is the point we are looking for, the hourly and resource level data. We can select this and save it. Here you can also see the cost that would occur. And we can see here one cent per 1000 usage records per month.</p>
<p>It depends how much EC2 instances because this just applies to EC2. It depends how much EC2 instances you’re running. Of course, the more instances you’re running the more usage records you are going to record and the more costs you incur, but 1 cent per 1000 records, in this case, it’s not that much. </p>
<p>So we wanna save it here, go back to the Cost Explorer and we can see, oh, the hourly view is available. We can only see last 14 days of usage was the hourly view enabled. In this case because I just enabled it, I won’t see anything because the data needs to be recorded first. So it will take a minimum of four days for Cost Explorer to collect enough data that the hourly view is available here.</p>
<p>About data grouping and filter. We started with the group function, with the grouped by functionality, you can segment your data based on a particular cost or usage. For example, you can choose a region to see in what region you spent the most money on which regions you spent money.</p>
<p>Let’s go with a bar so we can see, let’s take December. We spent the most money in Ireland followed by others because others mean we have like a lot of data here. Well, let’s go to December. We’ve used many different regions here and all the regions that we use that are not shown here are combined into the others bar. If you want to stripe your chart down to make it more granular, you should use the filter function located on the right.</p>
<p>Using the filter panel. You can refine your data set to include or exclude specific filtering dimensions and values. For example, let’s say we wanna only see costs that occurred in Frankfurt. We go to the filter section here search for the Frankfurt region, select it, applied it, then we can just see the costs that applied for the Frankfurt region. And we can use like many combinations here to dive deep within our costs.</p>
<p>For example, if we just wanna see the costs for our EC2 instances, just filter here for EC2, for our compute instances. Nothing else just compute instances.</p>
<p>So just our literal service and we filter it by instance type and then we can see what instances occurred the most. So for example, here, we have a G4DN which is like a graphical, a big graphical instance that is quite expensive in this case.</p>
<p>As you can see here on the right, there are quite a lot of filters that you can use and no worries, we won’t look at all of them, but some of them are quite interesting. Let’s for example talk about usage type, usage types are the unit that each service uses to measure the usage of a specific type of resource.</p>
<p>For example, if I wanna know how long my t2 micro instances we are running in the past, I would just type in here, t2 micro, oops micro, select a filter here. And I can see my t2 micro instances were running 370 hours in July and about 200 hours in August. And you can use this for many other services as well.</p>
<p>Usage type groups also is pretty cool, these are the kind of type of filters that collect a specific category of usage type filter and put it into one. For example, if I wanna know how long all my EC2 instances were running in the past, I would just search for EC2 running hours and I could see the amount all my EC2 instances were running over the last six months.</p>
<p>Another one that is quite useful if you wanna go in more detail is the filter for API operations. So let’s take S3 as an example, we can see here. We don’t have that much S3 costs here, just in December $188. But if I wanna know more about these costs, how these costs are structured, I can look for specific API operations that apply to S3.</p>
<p>For example, if I want to know how much my costs were for like reading files from my S3 three buckets, I would look up GetObject because this is the API operation for reading objects from S3. I apply this filter here and it can see the costs for GetObject from S3. That’s it, that’s how you’re gonna use API operations.</p>
<p>As you can imagine, there are a lot of API operations that are used within AWS and I can highly recommend to have a look at API reference documentation for each service because you can see here, this is like just for S3, all the API operations that you can use for S3. And just by looking at them, you can, for most of them, you can already like kind of guess what they’re doing. Like for example, here, create object, create bucket and you can like just look forward for the API operation that fits for your kind of analysis.</p>
<p>A little bit deeper into Cost Explorer and the advanced options that can also be quite useful. For example, show only untagged resources. This is a pretty important filter because we talk about tags already a lot and little spoiler here we will talk about them way more because this is such a super, super, super important topic. And if you set this filter here you will see all the resources that have no tags attached, super important to see that.</p>
<p>What is also quite cool is the show costs as here. The unblended filter here will be the best for the vast majority of AWS customers. This is the cost dataset presented you on the bill page like for the bills and for the invoices. And it’s also the default option for analyzing costs using AWS Cost Explorer or setting the custom budgets using AWS budgets.</p>
<p>The unblended costs represent your usage costs on the day that they are charged to you or in finance terms they represent your costs on a cash basis of accounting. For most of you this is the only data set that you will ever need. Okay?</p>
<p>Amortized costs are also quite interesting because the amortized cost is useful in cases in which it doesn’t make sense to view your costs on the day that they were charged or as many of finance owners say it’s useful to view costs on an actual basis rather than a cash basis.</p>
<p>This cost dataset is especially useful for those of you who have purchased AWS or reservations such as reserved instances or savings plans. Savings plans and reservations often have upfront or recurring monthly fees associated with them.</p>
<p>Recurrent fees for reservations are charged on the first day of the month that can lead to a spike on one day if you’re using unblended costs as your cost dataset. When you toggle over to amortized costs, these recurring costs, as well as any upfront costs, are distributed evenly across the month.</p>
<p>Armotised costs are a powerful tool if you seek to gain insight into the effective daily costs associated with your reservation portfolio, or when you are looking for an easy way to normalize costs and usage information when operating at scale.</p>
<p>And then there are also like, two more that can be quite helpful. And these are the net unblended costs and the net amortized costs. These are basically the same as the two that I just explained here but they also include discounts like the reserved instance volume discounts. Like these discounts are calculated into these costs.</p>
<h1 id="Reports"><a href="#Reports" class="headerlink" title="Reports"></a>Reports</h1><p>Let’s talk about reports. Reports can be super, super, super useful. For example, in this case, I build a specific view for EC2 instances running in Frankfurt, in the EU region on a monthly base, like on a base for December, in this case. And because I wanna use this view more often, I can save it as a report. And to do this, I just click here on save as. I give this one a title, EU-EC2. I save it, and that’s it.</p>
<p>By clicking here, on recent reports, I can see this report. Or I can just go under report overview and see all the reports that I have here. The reports with a lock here are default AWS reports, and the one without a lock are my own reports.</p>
<p>Some of the AWS reports can be pretty useful. For example, what I use quite a lot is the RI utilization and coverage, or the savings plan utilization and coverage, that basically shows you how effective or efficient your reservations are.</p>
<h1 id="Cost-and-Usage-Reports"><a href="#Cost-and-Usage-Reports" class="headerlink" title="Cost and Usage Reports"></a>Cost and Usage Reports</h1><p>The AWS Cost and Usage Reports or in short the CUR. The CUR is basically the most important thing to capture your AWS billing data. And the CUR is a pretty complex CSV file that stores all details about your cost and usage data of all AWS resources.</p>
<p>Enabling the CUR is super important because it’s the most granular and detailed mechanism to collect data for AWS costs and usage. It offers historical by-the-hour data that can offer clarity on trends and lead to a more accurate data-driven insight. And there’s no looking back. Until the CUR is enabled, you’re losing valuable data about your usage that is older than 12 months.</p>
<p>The CUR can get really big and in large corporations, it can easily get beyond five gigabyte and more with millions over millions of lines. So let’s see how to enable them.</p>
<p>When we are here in the AWS Management Console, we click here on the top menu on the billing dashboard and you can see here on the left menu for the Cost and Usage Report. By default it’s disabled so you need to enable it, enable it first and create a report. We give it a name. Let’s call it test. I would advise you to include the resource IDs because then, every resource get a unique resource ID.</p>
<p>You can enable the automatic refresh. Click Next and then you need to choose an S3 bucket where you put the file. This can either be existing an S3 or a new one. Then we set a path, a prefix pass task costs. You can select the time granularity here. Of course, the more granular the data are, the more data you’re going to produce.</p>
<p>We can create new report versions or we can override the existing ones and we can choose what kind of data integration we need.</p>
<p>And a little side note here maybe. You can export the files for Redshift or QuickSight usage. This will change the output format of the file to be readable for either Athena or Redshift and QuickSight. See, Athena is Parquet and Redshift or QuickSight is the CSV file that comes in a, in a zip file.</p>
<p>With Athena, Athena is a serverless service that allows you to analyze the data stored directly in Amazon S3 using standard SQL. And for that you need, as I just mentioned, the Parquet file While with Redshift and QuickSight, you can manage to see a SWI file as you would do it also like for example, for Excel.</p>
<p>Redshift is a so-called data warehouse service which you would use for querying big data sets with like multiple gigabytes or even up to petabytes. It can help you take wide insights of your own environment and for customers on a very large scale. And QuickSight is a business intelligence service that can combine data from literally any source into a dashboard. It helps you to visualize for any type of audience and it is much more visually driven than Athena or Redshift.</p>
<p>After you set up all these options here, you can click on Next and that’s basically it. You have now configured your Cost and Usage Report. But be aware that it may take up to 24 hours for the first report to be delivered. Also, expect some costs from S3 for storing the CUR data in your S3 bucket. But these costs are like very, very low. Maybe like, I don’t know, a few dollars per months or per year. Depends on how big your file gets of course.</p>
<h1 id="Budgets"><a href="#Budgets" class="headerlink" title="Budgets"></a>Budgets</h1><p>In this section, we will learn how to create a budget, to help manage running expenses. Budgets allow the user to get notified when costs or usage exceed a certain predefined amount. So let’s have look how to set up automatic notifications and actions with AWS Budgets.</p>
<p>So we get there by clicking here on the top menu on the billing dashboard. Here on the left, we have budgets, we click on it, and we can see that there is already a budget predefined here. I set this up in the past to get notified if my monthly budget threshold gets over $150 but let’s create a new budget.</p>
<p>So we can select four different kinds of budgets in this case or like in general. We have cost budgets based on actual costs, usage budgets based on usage like ours and budgets for reserved instances and savings plans. So let’s start with the cost budget.</p>
<p>So first we need to give it a name and we have to select a period in that we want to be notified. We will keep it here on a monthly period and we can also choose if this is a recurring budget or an expiring budget. Does that goes, for example, like just till April but we will stay with a recurrent budget.</p>
<p>So you can either choose if you want to set a fixed budget. Like for example, last month I had costs of $31. I can set this to $35 ‘cause this is quite close. And if I would like reach this threshold here I would get notified. You can also set a monthly budget planning.</p>
<p>For example, let’s say you have a snowboard rental and you know, there won’t be much users on your platform in the warmer month, like for example, in April till October. So you could set the budget here to like just 100 bucks. But you know, in the cooler months when there’s actually actually snow, you will have much, much more users. So you could set a budget here to 1000 bucks but let’s keep it simple and go with a fixed budget.</p>
<p>You have fear like also many other photos that you also know from the cost explorer demo. The last demo that I just showed you to get more information about the costs you had in the previous months. But let’s configure the thresholds.</p>
<p>You can define your budget thresholds and you can set it either to the actual costs like the cost that like actually occurred to a percentage, like an alert threshold for example, like 80%. So in this case, you would get an alert if 80% of the budget that we just defined in the last step, if we reach this threshold. That will be $28 in this case. And you could also set it based on forecast at cost but this is getting too complex.</p>
<p>Let’s keep it with the actual cost. So here, can you set up the notifications. I can like type in here, my email address and whenever I would reach the threshold I would get an email an alarm that I reached this threshold. And since October 2020, it is also possible to set different kinds of triggers for actions like budget actions. These are based either on identity and access management policies, service control policies or you can also target running instances like EC2 or RDS.</p>
<p>For example, you can choose to apply a custom denied EC2, run instance IAM policy to a user, to a group or to a role in your account once your monthly budget for EC2 has been exceeded. With the same budget threshold, you can configure a second action that targets specific EC2 instances, using a particular region. You can choose to execute actions automatically or make use of a workflow approval process before AWS Budgets execute a request on your behalf.</p>
<p>It’s possible to set up five budget thresholds with up to 10 actions for each threshold. IAM and SEP action type reset at the beginning of each budgeted period. Like in our case, monthly while actions target at a specific EC2 or RDS running instances will not reset.</p>
<p>So we’ve clicked here on the budget actions we activated it, and now we can choose an IAM role that allows budget actions to actually do something with the instances that we are going to define here. So let’s just take this open access role that have defined earlier and we can also choose what should happen.</p>
<p>We can say here if our threshold reaches the budget that we just defined, just stop all EC2 or RDS instances. This is of course, like quite radical step but if you are on a budget, well, you have to do what you have to do, right? So when you can choose if you want to stop EC2 or RDS instances, and you can also select a specific region where this should happen. And if I would click here on “Confirm budget,” the budget would be set. And if the threshold is reached, I would get an email and my EC2 instances would be stepped.</p>
<h1 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h1><p>The essence of a cost allocation strategy is the ability to tell how much is spent on which resource on which service. This type of visibility can be best achieved by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/introduction/">tagging</a> every single resource in your cloud. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> enables the user to put tags on every available resource. You can use tags for many things. But for this course, we’re just going to focus on how to use them for cost allocation. So let’s find out what tags actually are. </p>
<p>Tags provide the functionality to define metadata in the form of key and value pairs. These on the other hand are associated with the resources in a cloud account. Let’s have a look at this diagram. In this example, we’re looking at four resources. Don’t mind the details because it doesn’t really matter what kind of resources they are. These are just some key samples. So each of them has a tag, which goes by the key-value environment production. This one is true for every four of them. This means that all of these resources belongs to the production stage of our environment.</p>
<p>The next one distinguishes our resources between the frontend and backend, and basically tells us right away which resources host a front-end service and which hosts a back-end service. Typically, business tags such as cost center, business unit, or project are used to associate AWS costs with traditional financial reporting within an organization. However, a cost allocation report is not static and hence can include any tag. This allows customers to easily link costs to technical or security dimensions, such as specific applications, environments, or compliance programs.</p>
<p>With AWS Cost Explorer and Cost and Usage Report, AWS costs can even be viewed according to tags, providing even more insightful cost visualizations. AWS Cost and Usage report is otherwise known as AWS CUR. Just so you know what the Cost and Usage Report is, I wanted to drop in a short explanation. With the help of AWS Cost and Usage reports, you can track the monthly AWS costs and usage associated with your AWS account. The report includes items for each unique combination of product, usage type, and operation that is used in your AWS environment. It enables you to configure the AWS Cost and Usage report to show only the data that you want, using the AWS Cost and Usage API.</p>
<p>AWS Cost and Usage Reports contain the widest variety of cost and usage data. You can set up the CUR to collect billing data for any given period and push it into an Amazon S3 bucket to store it there for whenever you need it. You can get hourly, daily, or monthly reports. These contain the costs in detail and are sorted by product or resource. If tags are used properly, they are also listed in the report and can provide extra detail to your bill. A report is updated at least once per day and up to three times. They can be stored in an S3 bucket of your choice and be retrieved whenever needed, either manually or by using another service. </p>
<p>After you set up a cost and usage report, you receive the current month’s billing data and daily updates in the same Amazon S3 bucket. The data from the CUR forms the base for a detailed and complete cost analysis. It is often the main part for many business intelligence tools, like Athena and QuickSight, just to name a few. And CUR can also be reached by an API, which you can use for your custom scripts or individual needs.</p>
<p>So, in conclusion, with AWS CUR, you are able to store your report files in Amazon S3 buckets, update the report automatically, up to three times a day, make use of the AWS CUR API for automation or easier management through API calls, and use the CUR for in-depth analysis with business intelligence tools like QuickSight, Athena, and others. That’s about it for the AWS Cost and Usage Reports, so let’s continue. </p>
<p>Keeping cost allocation in mind, the best way to assign business-context details to specific resources is by using tags. Later on in the process, this enables you to carry out a more valuable analysis based on your cost data and facilitates company-specific decision making by a well-evaluated foundation of data. If you take bill analysis into consideration, tags can add business dimension and context to ease the allocation process. </p>
<p>Tags are used to identify which item or resource in your cloud is attributed to each of your business services. So you can always tell exactly which resources are used for which service in your company. Nevertheless, keep in mind that tags are only meaningful to their respective user or a customer. They literally do not have any semantic meaning. You can name them whatever you like and assign a value to them. However, when used correctly, they can help read and analyze your data and even automate your analysis with the right setup.</p>
<p>In AWS, you can manage tags in the service console or accessing the API through AWS CLI, although this limits you to only one resource at a time. If you want to add, edit, or delete tags on multiple resources, it is best to use a service, for example, the AWS Tag editor. Once you have tagged your resources, you can enable Cost Allocation Tags in the Billing and Cost Management sections. We will discuss how to tag and activate the Cost Allocation tags in a minute. One significant thing to note here is that tagging existing resources retroactively is pretty annoying. So make sure to tag your resources from the very beginning.</p>
<p>In the best case, policies can prohibit the deployment of new resources without the appropriate tags. But more about that later. If you want to analyze a cost report after the fact with unlabeled or poorly labeled resources, you will have a hard time understanding the exact usage of each resource, and will likely not be able to identify the exact costs and usage by resource. So, it is advised to start tagging resources as soon as possible and stay consistent with your tagging strategy.</p>
<p>Planning out a tagging strategy or a tagging standard is essential, and the best time to implement one is before a company launches its cloud resources. It’s best to keep tagging simple and easy to grasp. Don’t overdo it for the sake of it. After all, you want to gain visibility, not cause confusion. The best thing to do is to learn about predefined tags and choose the ones suitable for your business. It’s also advisable to adjust your tags to follow your KPIs once you determine them.</p>
<h1 id="Tagging-Best-Practices"><a href="#Tagging-Best-Practices" class="headerlink" title="Tagging Best Practices"></a>Tagging Best Practices</h1><p>We’ve picked a few best practice examples for you to apply for your business or organization. Let’s start with some common tags that are used by most organizations. Of course, these are just some ideas and you need to use tags that fit your business case. Some common examples include Cost Center or Business Unit tag, used to show where resource costs are allocated within the organization, and it also allows correct cost allocation within billing data.</p>
<p>Service&#x2F;Workload name tag. This shows which service the resource belongs to. Resource Owner tag. This is responsible for the resource. Simple Resource Name tag. This is something easier to read and to remember than the default tags. And Environment tag. It determines the cost difference between different environments. For example, dev, test&#x2F;stage, production. Check your cloud and see whether these tags can help you get started with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/tagging/">tagging</a>. Also make sure to check <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> pre-generated tags. They might save you some time.</p>
<p>Now let’s look at some tagging best practices. So, number one, align tags to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cost-management-tagging-1696/aws-generated-cost-allocation-tags/">cost allocation strategy</a>. Before you start tagging, you should think of a general cost management strategy. Think of tags that help you to track and allocate expenses and make those tags align with your strategy. Next, tag everything. Tag as many resources as possible so that no resource is left untagged. Make this a rule. In fact, you can roll out policies in your cloud environment that will forbid launching resources without tags.</p>
<p>Next, find a purpose for each tag. Think of a certain use case before adding a tag. Otherwise you will have a hard time justifying your tags and you risk running into a mess of baseless tags. That now leads me onto the next point. Limit the number of tags you adopt. Find redundancies and overlapping tags and simplify them. There’s no point in releasing multiple tags that cover the same subject. Look for tags that might logically overlap. See where you might merge them and reduce the number of your overall tags. And keep it manageable. Obviously, the more tags you have, the more tags you have to deal with. Keep the number as low as necessary, but the information value as high as possible.</p>
<p>Next, consistency is key. Use a consistent naming convention. This helps to keep an overview and eases further processing. Giving your tags less abstract names, and instead naming them with descriptive terms also makes them easier to read. Automate tag management. Make use of tools like the AWS tag editor to automate your tagging. Avoid wasting time on repetitive tasks and use automation as much as possible. Set up policies to forbid launching untagged resources. This is an easy way to ensure that no new resources are slipping into your environment without a tag.</p>
<p>And finally, audit and maintain your tags. Make it a habit to review tags from time to time and verify their purpose. Tag maintenance is essential and should involve everyone on the team. So make it a recurring task for everyone and have everyone keep their eyes open for suggestions for improvement.</p>
<h1 id="AWS-Generated-Cost-Allocation-Tags"><a href="#AWS-Generated-Cost-Allocation-Tags" class="headerlink" title="AWS-Generated Cost Allocation Tags"></a>AWS-Generated Cost Allocation Tags</h1><p>The default tags in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> are basically cost allocation tags. They can be activated in the billing section, which we will explain in a minute. Cost allocation tags are special tags that are used by Cost Explorer and other services for allocation and visualization. So they can be explicitly used and displayed in the various views of various services. So take a look at the screenshot. Here we can see the Cost allocation tags section in AWS Billing. We can tell that CostCenter and Name are active and enabled as cost allocation tags, while the others are inactive. These inactive tags were once set for resources that are not in use any longer.</p>
<p>Once the cost allocation tags are activated, the console detects all tag keys used and suggests those for activation as cost allocation tags as well. Otherwise, they won’t show up in Cost Explorer charts. The term User Defined means that a user created these tags and that they are therefore custom tags. AWS generated tags were automatically generated, such as createdBy, the createdBy Amazon WorkSpaces Tag. These were created and applied to support AWS resources for the purpose of cost allocation. They can only be view in the AWS Billing and Cost Management console and reports. They do not appear anywhere else in the AWS console, including the AWS Tag Editor.</p>
<p>Also, note that in this Cost allocation tags section, all the tag keys that are and were used in the account will be shown here. However, if you don’t enable the Cost Allocation Tags option, you will not be able to evaluate certain views in the Cost Explorer and other services. For example, if I want to know what costs have been incurred for all resources with a specific cost allocation tag, then I won’t be able to select or see them.</p>
<p>To prevent you from failing to find your resources with those cost allocation tags, I will show you how to enable this option. Let’s see how you can enable AWS generated Cost allocation tags. First, you need to log in to your Master account as an IAM user with the required permissions. Next, type Billing into the search field and go to the Billing console. Select Cost Allocation Tags on the left side menu. And then, simply click on Activate to enable the tags.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-IAM-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-IAM-24/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Introduction-to-IAM-24</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:53" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:53-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:42" itemprop="dateModified" datetime="2022-11-20T19:08:42-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-IAM-24/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-IAM-24/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-What-is-AWS-KMS-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-What-is-AWS-KMS-23/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-What-is-AWS-KMS-23</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:51" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:51-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:50" itemprop="dateModified" datetime="2022-11-20T19:08:50-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-What-is-AWS-KMS-23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-What-is-AWS-KMS-23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:50" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:50-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:20" itemprop="dateModified" datetime="2022-11-20T18:58:20-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Security-and-Compliance-CLF-C01-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Security and Compliance in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce some Security and Compliance services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#117;&#112;&#112;&#x6f;&#114;&#116;&#64;&#x63;&#108;&#x6f;&#x75;&#100;&#97;&#x63;&#97;&#x64;&#101;&#109;&#x79;&#46;&#99;&#111;&#x6d;">&#115;&#117;&#112;&#112;&#x6f;&#114;&#116;&#64;&#x63;&#108;&#x6f;&#x75;&#100;&#97;&#x63;&#97;&#x64;&#101;&#109;&#x79;&#46;&#99;&#111;&#x6d;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Security and Compliance services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to various Security and Compliance services in AWS and to introduce important security concepts, including:</p>
<ul>
<li>Finding compliance data with AWS Artifact;</li>
<li>Managing users, groups, and roles with AWS Identity and Access Management, or IAM; and</li>
<li>Evaluating the security of your AWS environment using AWS Trusted Advisor.</li>
</ul>
<p>We’ll also introduce the AWS Web Application Firewall, or WAF, which, along with other services such as the AWS Firewall Manager and AWS Shield, can help you create a comprehensive security solution for your web applications.</p>
<p>These objectives are covered by Domain 2 in the official AWS Certified Cloud Practitioner exam blueprint: Security and Compliance, which accounts for 25% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#115;&#x75;&#112;&#x70;&#x6f;&#114;&#116;&#x40;&#99;&#x6c;&#111;&#117;&#x64;&#97;&#x63;&#x61;&#x64;&#101;&#x6d;&#121;&#x2e;&#x63;&#x6f;&#109;">&#115;&#x75;&#112;&#x70;&#x6f;&#114;&#116;&#x40;&#99;&#x6c;&#111;&#117;&#x64;&#97;&#x63;&#x61;&#x64;&#101;&#x6d;&#121;&#x2e;&#x63;&#x6f;&#109;</a>. Thank you!</p>
<h1 id="Finding-Compliance-Data-With-AWS-Artifact"><a href="#Finding-Compliance-Data-With-AWS-Artifact" class="headerlink" title="Finding Compliance Data With AWS Artifact"></a>Finding Compliance Data With AWS Artifact</h1><p>Hello, and welcome to this lecture where I will be examining AWS Artifact, a free self-service portal that provides you with immediate access to AWS security and compliance reports. Within AWS Artifact, you also have the ability to view, download, accept, and terminate legal agreements between you and AWS at both the account and organization level.</p>
<p>So you may be asking yourself: why would I ever need to access the information in AWS Artifact? And as it turns out, there could be several reasons. For starters, you might be asked to provide evidence of the current or historical compliance of different AWS services used within your architecture as part of a required audit to ensure that your enterprise may continue to leverage the AWS cloud. And this audit could potentially extend out to include your suppliers as well. Or perhaps you just want to learn more about your responsibilities when it comes to complying with various regulatory standards such as Payment Card Industry, or PCI, or Service Organization Control, or SOC. After all, simply leveraging the AWS cloud does not guarantee that the systems you build within it will be fully secure or compliant. We’ll discuss this more in a moment.</p>
<p>AWS Artifact can be accessed directly from the AWS console by searching “Artifact.” From there, the AWS Artifact home page gives you options to view reports and view agreements, so let’s spend a little time discussing reports and agreements in more detail.</p>
<p>AWS Artifact Reports consist of AWS auditor-issued reports and include everything from ISO certifications to PCI and SOC reports.</p>
<p>These reports, known as audit artifacts, may be shared with auditors and regulators by creating IAM users with an associated identity-based policy that grants access only to the necessary reports. And these audit artifacts allow you to provide evidence of AWS security controls to ensure compliance with any applicable governance, regulations, or frameworks when architecting solutions in the AWS cloud. Now of course this is always done in accordance with the AWS Shared Responsibility Model, where AWS is responsible for the underlying security OF the cloud, but you remain responsible for your own systems’ and applications’ security IN the cloud. Now to learn more about the AWS Shared Responsibility Model, I encourage you to check out this resource. Consequently, the compliance reports provided within AWS Artifact pertain only to AWS and do not in any way certify the security or compliance of your own company, organization, or application. However, these audit artifacts can and should inform the security controls you choose to implement as part of your own cloud architecture and solution design.</p>
<p>In addition to security and compliance reports, AWS Artifact also allows you to view and execute legally binding agreements between you and AWS.</p>
<p>These agreements can be applied at the individual account level, or if you are signed in to the AWS console with the management account of an organization in AWS Organizations, you can also apply an agreement to all member accounts within your organization. One example of a commonly used agreement is the AWS Business Associate Addendum, or BAA, which governs your use of AWS services when storing personal health information, or PHI.</p>
<p>To accept an agreement, you must first accept the AWS Artifact non-disclosure agreement or NDA.</p>
<p>After you have accepted this NDA, then downloaded and reviewed the agreement, you may accept the agreement by checking a box acknowledging that you accept all of its relevant terms and conditions. Note that when accepting an agreement on behalf of all member accounts within an AWS Organization, you must also certify that you have the full power and authority to accept the agreement on behalf of every entity that either currently has, or may ever subsequently have, a member account within your organization at any point in the future.</p>
<p>So that’s how we can use AWS Artifact to not only view compliance reports and agreements but also to help ensure the solutions we architect in the AWS cloud remain secure and compliant with all necessary rules and regulations.</p>
<h1 id="What-is-Identity-and-Access-Management"><a href="#What-is-Identity-and-Access-Management" class="headerlink" title="What is Identity and Access Management?"></a>What is Identity and Access Management?</h1><p>Hello and welcome to this lecture where I shall provide an overview of what the Identity &amp; Access Management service is, and what IAM actually means.</p>
<p>Firstly I want to define what is meant by Identity &amp; Access Management and I shall break this down into two parts, starting with Identity Management. </p>
<p>Identities, such as AWS usernames are required to authenticate you to your AWS account, and this authentication process is managed in 2 stages.</p>
<ol>
<li>The first part of this process is to define who you are, effectively presenting your identity, so for example your AWS username. This identification is a unique value within IAM for your account, so this means IAM would prevent you from having 2 identical user accounts with the same name within the same AWS account.</li>
<li>The second part of the authentication process is to verify that you are who you say you are. This is achieved by supplying additional data, and when using our AWS usernames we can verify this by supplying a password</li>
</ol>
<p>Now, Access Management relates to authorization and access control. Authorization determines what an identity can access within your AWS account once it’s been authenticated to it. An example of this authorization would be the user’s list of permissions to access specific AWS resources, for example, they might have Full Access to EC2 or Read Only to RDS.</p>
<p>Access Control can be classed as the mechanism of accessing a secured resource. For example, using the following:</p>
<ul>
<li>Username and password (Authentication and Verification)</li>
<li>Multi-Factor Authentication (MFA, used as an additional verification step following a valid password)</li>
<li>Or Federated Access, which allows users external to AWS to access resources securely without having to supply AWS user credentials from a valid IAM user account. Instead, these credentials are supplied from identity providers. For more information on Identity Federation, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/">https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/</a></li>
</ul>
<p>So essentially IAM can be defined by its ability to manage, control, and govern authentication, authorization, and access control mechanisms of identities to your resources within your AWS Account.</p>
<p>Having an understanding of the different security controls from an authentication and authorization perspective can help you design the correct level of security for your infrastructure.</p>
<h1 id="IAM-Features"><a href="#IAM-Features" class="headerlink" title="IAM Features"></a>IAM Features</h1><p>It’s critical to understand how IAM works and what can be achieved via the service, but it’s even more important to know how to implement its features. Without IAM, there would be no way of maintaining security or control of who or what could access your resources and what they could do with them, both internally and externally. IAM provides the components to maintain this management of access, but it is only as strong and secure as you configure it. </p>
<p>The responsibility of implementing secure, robust, and tight security within your AWS account using IAM is yours, you are the owner of the AWS account. You must define how secure your access control procedures must be, how much you want to restrict users from accessing certain resources, how complex a password policy must be, and if users should be using Multi-factor authentication. All of this is and much more is down to you to architect and implement, and much of it will likely depend on your own security standards and policies within your Information Security Management System (ISMS).</p>
<p>In this lecture, I want to talk about some of the different features and components that AWS IAM uses to centrally manage and control security permissions for any identity requiring access to your AWS Account and its resources. </p>
<p>From with the AWS Management Console, the IAM service can be found under the ‘Security, Identity &amp; Compliance’ category, and when accessed it will take you to the IAM Dashboard. </p>
<p>The initial dashboard of the IAM Console will display the following information.</p>
<p>This is a URL link that you can send to users who you will need to gain access to your AWS Management Console. This sign-in link can be customized by clicking on the ‘customize’ button to make it easier to remember and read. If you have multiple AWS accounts, this customization would help to distinguish between your accounts</p>
<p>IAM Resources. This section provides a summary overview of your IAM resources using a simple count of the number of users, user groups, roles, customer-managed policies, and identity providers you have configured within IAM. </p>
<p>Best Practices. This is populated with a list of IAM security best practices that AWS recommends you implement with links on how to implement them. I strongly recommend you try to adopt these best practices at your earliest opportunity. Maintaining tight security is paramount with working within a cloud environment.</p>
<p>In addition to this dashboard, you will also see 2 categories on the left menu, Access Management and Access Reports each listing a number of components underneath. I want to start by reviewing each of these components under Access Management to give you an insight into what each are used for, starting with Users.</p>
<p>User objects are created to represent an identity, this could be a real person within your organization who requires access to operate and maintain your AWS environment, or it could be an identity that is used by an application to interact with your AWS resources programmatically.  Users are simply objects representing an identity which are used in the authentication process to your AWS account. Being unique values, every user has an Amazon Resource Name, an ARN which it can be referenced by. An example of a user’s ARN could be as shown.</p>
<p>When configuring your Users you can set them up for Multi-Factor Authentication. Configuring MFA allows for an additional level of verification to be applied, the user will have to enter a random 6 digit number from a linked MFA device after their usual password. MFA should be used for the AWS Account owner and any other users who have elevated privileges. </p>
<p>IAM USer Groups are objects much like user objects, however, they are not used in any authentication process, instead, they are used to authorize members of the group access to AWS resources.</p>
<p>So, the IAM USer Groups contain IAM Users, and these groups have IAM policies associated that will either allow or explicitly deny access to AWS resources. These policies are either pre-existing AWS Managed policies, customer-managed policies that are created by you, the customer, or in-line policies which are embedded explicitly to the group itself.</p>
<p>IAM Roles allow Users, other AWS services, and applications to adopt a set of temporary IAM permissions to access AWS resources. Roles essentially operate the same as User objects do in that it’s an identity with associated permissions to allow it access to different resources. The difference being however is that roles don’t define a single person, they are designed to be assumed by any identity or service that needs to temporarily acquire a set of permissions. Additionally, roles don’t have passwords associated with their identities, instead, like I just mentioned, roles are assumed as long as you have the correct permissions to assume it.</p>
<p>Policies used within IAM are written as JSON documents and these define what can and can’t be accessed. These policies can be attached to Users, User Groups, or Roles. </p>
<p>When working with policies, you can use Managed policies or In-line policies. Managed policies are viewed as a library of usable policies and come in 2 different flavors which can be applied to multiple Users, User Groups, and Roles:</p>
<ol>
<li>AWS Managed Policies: These are a list of predefined policies granting varied access to different AWS services</li>
<li>Customer Managed Policies: These are policies created and written by you as the customer</li>
</ol>
<p>Unlike Managed policies, Inline policies are not stored in a library, instead, they have to be written and explicitly embedded within a User, User Group, or Role, as a result, the same policy can’t easily be applied to another identity like Managed policies can.</p>
<p>If you are looking to provide federated access to your AWS resources, then you must add an identity provider. </p>
<p>Federated Access allows credentials external to AWS to be used as a means of authentication to your AWS resources. For example, you could establish a trust between your Active Directory Federation server on premises and your AWS account. Or, you could establish a trust between your AWS account and Google account. Either way, in these instances your ADFS server or Google account could be configured as Identity Providers allowing those authenticated by Active Directory or Google to access your resources in your AWS account. This prevents you from having to create individual IAM user accounts.</p>
<p>Under Account Settings, you can enforce a password policy, and it’s always best practice to do so. The Password policy should be used to enforce the minimum security requirements that need to be met for any password standards that your organization might have to adhere to. The password policy applies to all IAM users within your account, and as you can see here, you can be very specific by enabling&#x2F;disabling and configuring different controls.</p>
<p>The STS service is used to allow you to request temporary, limited-privilege credentials for both IAM users and federated users. The STS endpoints provide a list of Regions that are either activated or deactivated for STS. By default, all Regions are activated, however, it is recommended you deactivate the regions you do not intend to use. </p>
<p>By default STS uses the global endpoint of  <a target="_blank" rel="noopener" href="https://sts.amazonaws.com/">https://sts.amazonaws.com</a>, however, when using regional endpoints it can help to reduce latency.</p>
<p>So that has given a quick overview of the component covered by Access Management, I now want to look at the features that come under the category of Access Reports, starting with Access Analyzer</p>
<p>Access Analyzer. This is used to generate findings when a policy on a resource within your zone of trust allows access from outside your zone of trust. So this could be from a number of different sources, such as an IAM role allowing cross-account access, or a Bucket that allows a different account to upload objects into the bucket, basically any resource that allows external access to your resources will be flagged and highlighted to ensure you are aware of the access. This is a great tool to help reduce security risks and threats as you or your team may have unintentionally allowed access to a resource you shouldn’t have. Any issues are recorded by Access Analyzer as a finding allowing you to review the access </p>
<p>The credential report is a great tool that allows you to generate and download a *.csv file containing a list of all of your IAM users and their credentials. This provides a quick and easy way to review your accounts and the last time they were used, in addition to identifying when a user’s password was last changed and if they have Multi-Factor Authentication enabled.</p>
<p>If using AWS Organizations, it allows you to Select an organizational unit (OU) or account to view its service activity over the previous 365 days. By drilling down into the accounts in this section you can see which users have had activity, in addition to which AWS services they have accessed over a given time frame. </p>
<p>Service Control Policies. This links in with the previous component, ‘Organization Activity’, and lists any Service Control Policies that are applicable to the account and the number of identities affected. Service Control Policies, or SCPs, are different from identity-based policies which grant permissions to users, user groups, and roles as SCPs do not actually grant permission themselves. Instead, SCPs are used with AWS Organizations to implement and set a boundary of permissions for AWS accounts. </p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS, and EC2 via an identity-based policy in IAM. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2, despite having full access to S3. The SCP would prevent that service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<h1 id="Overview-of-the-User-Dashboard"><a href="#Overview-of-the-User-Dashboard" class="headerlink" title="Overview of the User Dashboard"></a>Overview of the User Dashboard</h1><p>Hello and welcome to this lecture where I’m going to review the dashboard of the user Console. In IAM, we can create user objects to represent an identity with long-term credentials, this could be a real person within your organization who requires access to operate and maintain your AWS environment, or it could be an account to be used by an application that may require permissions to access your AWS resources programmatically. Users are simply objects representing an identity which are used to authenticate to your AWS account with an associated set of permissions.</p>
<p>When you open the AWS IAM dashboard and select users, you will be presented with a screen similar to the following. This screen provides a summary of the key points of interest relating to your IAM users and it can quickly help you identify any potential security risks that you might encounter. As we can see, we have a number of different warnings on this screen in different colors. We have a green tick which symbolizes activity was measured within the last 90 days An amber exclamation mark showing activity was last measured between 91 and 365 days ago. And a red exclamation mark, which highlight anything older than 365 days ago. Anything either amber or red should be looked at and addressed as a priority to reduce potential security issues.</p>
<p>Let’s take a closer look at this table and its columns to understand exactly what it’s trying to show you. The complete list of field names are shown here, and you can toggle them on an off as you need. Let me go through each of them one-by-one to explain what they represent. So username. This column is self-explanatory, it provides the username of that user.</p>
<p>Path. If you create your users using the IAM API or the AWS Command Line Interface, the CLI, then you can specify a path structure for your users. This is useful when you have a large organization and you want to define a management structure to help you organize your users more effectively.</p>
<p>For example, you could add a path for a user Stuart of &#x2F;ContentTeam&#x2F;AWS&#x2F;UK&#x2F;Stuart. This would help to define where your users might be in accordance with your organization structure. In addition to this, you can also reference paths in your access policies, so for example you could allow all users in the &#x2F;ContentTeam&#x2F;AWS&#x2F;UK&#x2F; path to have full access to S3 with a simple policy like this.</p>
<p>Groups. This shows the different groups that the user belongs to. In the screenshot, you can see that the user Stuart belongs to the Admin group. Last activity. This column is really useful to show the last time your users logged into the AWS Management Console or if they accessed any of your AWS resources programmatically using their access keys. As we can see from my example, the user Alice hasn’t accessed anything for 467 days, as a result I can say with some confidence that this user can be removed and deleted from IAM. So it’s a great way to ensure that you don’t have any unused user accounts, and deleting unused accounts is an IAM best practice.</p>
<p>MFA. MFA stands for Multi-Factor Authentication, and so this will highlight any users that have been configured with MFA which adds an additional level of verification, enhancing security. The users in my example that show virtual mean that these users are using MFA on a virtual device, and in this example it’s the Google Authenticator app on a mobile phone. For those unfamiliar with MFA, let me give a brief explanation of what it is. Typically, when a user logs into the AWS Management Console, they will authenticate to your AWS account by providing their identification, typically their username, and then verify this identification, usually with a password. These two elements, the identification and verification, allow the user to authenticate. For many cases, the verification, the password, is sufficient enough to confirm the identity, the username. However, for users who have an elevated level of privileges, for example the users may have full access to a large number of AWS services, then you may want to, or be required to due to governance controls, implement an additional verification step within the authentication process. This adds another layer of security attached to the identity, and this is where Multi-Factor Authentication comes in, MFA. The name explains itself, it’s used to create an additional factor for authentication in addition to your existing methods, such as a password. Therefore creating a multi-factor level of authentication. So what is this additional level of security that MFA brings with AWS? MFA utilizes a random six-digit number that is only available for a very short time period before the number changes again which is generated by an MFA device. There is no additional charge for this level of authentication, however, you will need your own MFA device, which can be a physical token or a virtual device. AWS provides a summary of all supported devices here. Personally, I use Google Authenticator on my phone because it is simple and easy to set up and configure. Password age. This shows how long ago it was until the password of the user was changed. As a security best practice, your passwords should be changed regularly.</p>
<p>Console last sign-in. This column simply shows the last time a user signed in to the management console.</p>
<p>Access key ID. The access key ID will identify if it’s active or inactive and will display the key in use. Before I continue, let me just quickly explain a little more about Access Keys. Access Keys are used for programmatic access to your AWS resources, and they are comprised of two elements. An access key ID and a Secret Access Key ID The Access Key ID is made up of 20 random uppercase alphanumeric characters, such as the one shown here. The Secret Access Key ID is made up of 40 random upper and lowercase alphanumeric and non-alphanumeric characters, such as the one displayed on the screen. During the creation of a user who requires programmatic access, you are prompted to download and save these details and it’ll only be displayed once, and if you lose it, then you will have to delete the associated Access Key ID and recreate new keys for the user. It’s not possible to retrieve lost Secret Access Key IDs as AWS does not retain copies. These access keys must then be applied and associated with the application that you are using that requires the relevant access. For example, if you were using the AWS CLI to access AWS resources, you would first have to instruct the AWS CLI to use these Access Keys to authenticate you providing authorization. The method of performing this association varies based on the application and system that you are using. However, once this association has taken place it ensures that all API requests made to AWS are signed with this digital signature.</p>
<p>Active key age. The active key age refers to the age of the access keys that are associated with the user which, as I explained, are required for programmatic access. Access key last used. This column just shows the last time that the user used their Access Keys within your AWS account. As a security best practice, it’s a good idea to remove any access keys that haven’t been used for quite some time. This removes possible entry points for malicious users to gain access to your environment.</p>
<p>The ARN. The ARN, or Amazon Resource Name provides the unique identifier for referencing that particular user. For example, the one showne here for the user Stuart. Creation time. This is a simple time metric to show you how long ago the user was created Console access. The console access helps you to determine which users have access to the Management console. When creating a new user, you have the option to configure their access type as seen here. Each user can be configured to have Programmatic Access, or AWS Management Console Access, or both. If they have access to the Console, then this column will show as Enabled for the user and Disabled if the user does not have console access.</p>
<p>Signing certs. Finally, the last column of signing certs will show if the user has a Signing Certificate or X.509 Certificate associated with them for secure access to certain AWS product interfaces. For example, EC2 uses a Signing Certificate for access to its SOAP, Simple Object Access Protocol, and command line utilities. Finally, before we finish this lecture, you can also create and delete users as required, and search for any of your users via their username or access key from within the user dashboard.</p>
<h1 id="Creating-IAM-Users"><a href="#Creating-IAM-Users" class="headerlink" title="Creating IAM Users"></a>Creating IAM Users</h1><p>When creating a new user, you have the option to create it via the AWS Management Console or programmatically via the AWS CLI, Tools for Windows PowerShell, or using the IAM HTTP API. For this lecture, I should be using the AWS Management Console to demonstrate how to configure Users. User object creation is a simple process. Firstly, you will set the user details by creating a username, which can be up to 64 characters in length. Next you’ll select the AWS access type, either AWS Management Console access or programmatic access.</p>
<p>For programmatic access, an access key ID and secret access key ID will be issued to be used with the AWS CLI, SDKs or other development tools. If console access is required, you will need to define a console password for the user. Permission assignment through the use of policies can be attached to the user or inherited from a group that the user can be assigned to. And permission boundaries can also be applied to the user, controlling their maximum permission level. You can assign any tags to the user as you would with any other AWS resource. And then you must review and confirm the information that has been submitted before you create the user.</p>
<p>Once the user is created, you can download the security credentials via a CSV file. And that will contain the username, access keys required for programmatic access and the console login link. So let me now jump into the console to demonstrate how to create a new user. Okay, so I’ve just logged into my AWS Management Console. And the first thing I want to do is to go to IAM. And that can be found under the Security, Identity and Compliance category.</p>
<p>So if we select IAM, and that will take us straight to the IAM dashboard. And this is where we can start creating our users and groups and roles, et cetera, and anything else that we need to manage within IAM. So to create a new user, I need to go across to Users on the left here. And then from here, I can select Add users.</p>
<p>Now the first thing I need to do is to create the username of the user. So I’m going to call this user Patricia. And then we can select the access types. So we have the programmatic access here, all the AWS Management Console access here. So for this user, let me add both. So I want them to have programmatic access and also AWS Management Console access. So I’m going to select both. So for the AWS Management Console access, we need to enable a password so we can either have IAM auto-generated password or I can select my own. And for this demonstration, I’m going to add in my own password.</p>
<p>Now if you tick this option here, when the user signs in, they will be asked to generate their own password once they’ve used your initial password to login. And that’s a great idea, just to enhance security there. So if we click on Next, then we can assign permissions. And here we have a couple of options so we can add the user to an existing group. We can copy permissions from another user or attach existing policies directly to the user. So for best practice, I’m just going to add this user to a couple of different groups. So I’m gonna add them to the CloudAcademy group, and also to the RDSFullAccess.</p>
<p>So once you’ve selected the groups that you want the user to have, you can click on Next to go to tags. And this is an optional step. You can add any key value tags here for that user if you want to. Just gonna leave that blank for this demonstration. Then if we go to Next to Review, then we can review all the options that we’ve set. So we’ve given the username. We specify the access types. So you’ve got programmatic and AWS Management Console access. We’ve not set any permission boundaries. We’ve added the groups that we want the user to belong to and we haven’t applied any tags.</p>
<p>So now, we can go to Create User. Now we’ve successfully created the IAM user, but because we specified that we wanted programmatic access, we need to copy the access key ID and also the secret access key ID as well. If we download the CSV file of that user and take a look at that, we can see here that this CSV file shows the access key ID and also the secret access key ID, and also the console link as well to allow that user to login. So if we go back to the AWS Console, we can also email those login instructions to the user as well if we need to. Once you’ve taken a copy of the access key ID and the secret access key, then you can close this window. Remember, you will only be given one opportunity to take these details and download the CSV that contains that information, so make sure you do that. Then click on Close. And we can now see that that user, Patricia, has been set up as a user with the CloudAcademy and RDSFullAccess groups. So it’s very simple. It’s very quick. It’s very easy to set up a new user within IAM.</p>
<h1 id="Managing-IAM-Users"><a href="#Managing-IAM-Users" class="headerlink" title="Managing IAM Users"></a>Managing IAM Users</h1><p>Once you have created IAM users, you can view their details to configure additional security options or review permissions and change access. In this lecture, I want to cover these additional features. This will be easiest to explain via a demonstration, and I can explain each point as we go through, so let’s take a look. So in this demonstration, I just want to select a user and just to show you some of the different elements that you can change of that user once it has been created, so let’s take a look.</p>
<p>So I’m in the Identity and Access Management Dashboard at the moment and you can see I’m in the Users section. So let’s take a look at this user, Patricia. So if I select the user and we can have a look at some of the options that we can see about this user and some of the things that we can change, et cetera. So this is a summary screen of the user. We have the users ARN at the top here, and we can also see the creation time of that user. And then we have a number of different tabs.</p>
<p>So start with the permissions tab. We can see that this user is getting permissions from two policies at the moment here, the Amazon S3 Full Access policy, and also the Amazon RDS Full Access. And if we wanted to, we can just take a quick look at these groups. We can have a look at the policy summary, or we can take a look at the JSON as well. So that’s the policy and the JSON format. And then if we look at the policy summary, we can see here that this allows full access to S3 and S3 Object Lambda. Here we can set her permissions boundary. Currently there’s not one set, but if we wanted to, we can set one to control the maximum permissions that this user can have. And also there’s a feature here to generate policy based on CloudTrail events.</p>
<p>So what this will do, it will generate a policy looking at the user’s activity. And then based on what the user has been accessing, it can generate a policy based on what services this user has been accessing. Also at the top here, we can add an inline policy for this user. So if we’d done that, then that will be a policy that is embedded within the user object itself. So it’s not taken from a role, it’s not taken from a group. The policy is attached within the user.</p>
<p>Okay, if we take a look at the groups, we can just see a quick breakdown of any groups that the user belongs to, and the policies that are attached to them, which we covered just a moment ago in the summary. The tags is what you’d expect. If there’s any tags here for the user, then they would be listed, or if you wanted to add any tags, then you can do so here. So for example, we can add a key of location and say, UK, Save Changes. And then we can see this tag for this user. Under security credentials, we could see console link that this user can use and we can manage the user’s password. And if we want to change the password, we can simply click on manage, and we can either disable the console access or generate a new password, or ask the user to create a new password at the next sign-in. We also have here, the assigned MFA device, the multifactor authentication.</p>
<p>At the moment, it’s not assigned, but we can go ahead and set up MFA for this user. So let’s go ahead and do that quickly. So if we click on manage, we have a couple of options here, virtual MFA device, U2F security key, or another hardware MFA device. For this, I’m just going to use a virtual MFA device and I’ll use the Google Authenticator app on my phone to do this. If I click on continue. So, first of all, you need to make sure you have an app on your mobile phone or your computer. Like I say, I’m going to use the Google Authenticator app on my phone.</p>
<p>So what I need to do is to show the QR code, and now on my phone, I’m going to add this as a new entry in my Google Authenticator app, so I’m going to click on Scan QR code. And then we can see at the bottom there is added the user, Patricia. And then we add in that code, so 074720. And then what we need to do is to add the second code that comes in when it appears on the Google Authenticator app. So we’re just waiting for that to come around and then I can add in the second code and then it’ll be synchronized and configured. So, we can see it’s about to change, and now I can add in the next code 185887, and then I click on Assign MFA. And that’s it, so you have successfully assigned a virtual MFA device to that user. Click on Close, and there we can see here that there’s an assigned MFI device. We can see that this user also had programmatic access ‘cause there’s access keys that have been generated.</p>
<p>Now, if we wanted to, we can make this access key ID inactive. So if wanted to do that, simply click on Make inactive. And it’ll explained that once you’ve done this, you can’t then use these keys to form any programmatic access. Click on deactivate. And you can see here, the status is now inactive. So any access keys that were used before for this user will no longer be allowed to make any kind of requests. If we wanted to generate new access keys, simply click on Create access key. And again, you’ll have a new access key ID and a new secret access key. And if you wanted to, you can download the CSV file, so you don’t forget those keys. Click on Close.</p>
<p>Now, if we go back up to the top to Access Advisor, I just wanted to show you this quickly. So what this does, it will basically show you which services that this user can access based on their current permissions, and also the last time that these services were accessed. So if you scroll down here, we can see that this user has access to a whole different range of services. And it’ll show you which policies are actually granting these permissions.</p>
<p>So this access to EC2 in IAM is being granted through the RDS Full Access policy, and access to S3 is being granted through the Amazon S3 Full Access. So this is great to review to identify if there’s any users there that do have access to services that they probably shouldn’t do. So you can then modify the policies accordingly just to make sure that the users are only accessing what they are supposed to access. So that was a very quick demonstration of some of the key points that you can change within a user’s properties once you have created an IAM user.</p>
<h1 id="Managing-Multiple-Users-with-IAM-User-Groups"><a href="#Managing-Multiple-Users-with-IAM-User-Groups" class="headerlink" title="Managing Multiple Users with IAM User Groups"></a>Managing Multiple Users with IAM User Groups</h1><p>In this lecture, I want to talk about how IAM User Groups can be used to manage multiple users. IAM User Groups do not signify a single user and they can’t be referenced as a principal in any AWS access policy like a User or a Role can. However, they are used to authorize access for members of the group to AWS resources through the use of AWS Policies attached to the User Groups. So User Groups are objects that contain IAM Users, and these User Groups will have IAM policies associated that will allow or explicitly deny access to AWS resources.</p>
<p>These policies can be AWS Managed policies that can be selected from within IAM, customer-managed policies that are created by you, or in-line policies, which are written and embedded directly into the group. User Groups are a great user management feature and they are normally created to directly relate to a specific requirement or job role. For example, you could have a group called Developers, and then attach policies to that group that allow access to AWS resources required by your development team.</p>
<p>Any users that are then a member of that group will automatically inherit the permissions applied to the group. By applying permissions to a group instead of individual users, it makes it easy to modify permissions for multiple users at once, simplifying access management at scale. It’s a security best practice to apply permissions to User Groups and then associate users to that group than to associate policies to individual Users. This prevents you having to update permissions for each and every user.</p>
<p>For example, if you needed to change access for all the individual developers that had policies assigned to them directly, and this can be very time intensive and prone to human error, especially in an enterprise environment. If using groups and additional access is required for your Developer User Group, all you would need to do is to modify the permissions of the Developer Group and all your associated developers would inherit the new access.</p>
<p>Creating a group is very simple and is essentially a three-step process. You must give your group a meaningful name, add users to the group, attach permissions via the policies. Once you have created a user group, you can then review its configuration, edit the permissions and see other details such as the ARN of the user group. Let me show you via a quick demonstration on how to create an IAM group and then how to modify the permissions of the group once it’s been created.</p>
<p>Okay, so I’ve logged into my AWS Management Console and I’ve gone to the IAM dashboard. Now, from here, to access and create groups, under access management on the left, you can see user groups. So if you select that, and then will show you any groups that you currently have. And I only have one group, which is Admin. So to create a new group, you’ve gotta cross to the right-hand side here, click on create group. And the first thing you need to do is to give the group a name. So I’m going to call this MyS3andEC2Group. And then after that, you need to select the users that you want to be a part of the group.</p>
<p>So if you already have the IAM users there, you can add them at this stage. So let’s just go ahead and add in Stuart. And then at the bottom, you can then attach any policies that you want to be associated with the group. So if I type in S3, and it’ll pick up any policies that we have associated with S3. And I’m going to select this one here, this AmazonS3andEC2FullAccess policy. And if I click on the little plus sign here, it’ll give you a JSON view of the actual policy itself. So you can see exactly what’s happening.</p>
<p>So now I’ve selected that policy. If we just go down to create group at the bottom, and it’s as simple as that. So it’s very easy to create a group. You simple give it a name, specify the users if you need to at that stage and also add any permissions if you want to at that stage as well.</p>
<p>Now, once we’ve created our group, if we select it, we can see the user list here. Now, if you need to add any additional users, simply click on add users, select the users that you’d like and click on add users. And then they’ll be immediately added to the group. You can also look at the permissions. So here’s the policy that we added. Now, we can if we want to, add additional permissions by clicking on this button here. And you can either attach an existing policy or create an inline policy that’s directly embedded into the group.</p>
<p>So for example, if we go to attach policies, and we want RDS access as well for the people in this group, have a quick search and we’ll select this AWS managed policy for RDSFullAccess. Click on add permissions. And we can now see that this group has two permissions policies. And again, we can view the JSON details of those policies if we want to. And then we have the Amazon RDS policy here. So it’s very easy to set up groups and only literally takes just a few clicks.</p>
<p>So that’s how you create a new group, add users and also change the permissions as well. And also, just before we finish, if you want to delete the group, you can click on delete here. And we just need to type in the name of the group to confirm the deletion. So I’ll just go ahead and do that. Then click on delete. And that deletes the group. Finally, from a limitation perspective, your AWS account has a default maximum limit of 300 groups. To increase this you’ll need to contact AWS using the appropriate limit increase forms. Also, a user can only be associated with 10 groups, so bear this in mind when assigning permissions and each group can contain 10 different policies attached at once. Limitations on AWS services is fluctuating all of the time, so for the latest information on Group limitations, please see the following URL here.</p>
<h1 id="IAM-Roles"><a href="#IAM-Roles" class="headerlink" title="IAM Roles"></a>IAM Roles</h1><p>IAM Roles allow trusted Users, AWS services and applications to adopt a set of temporary IAM credentials to access your AWS resources. Roles act as identities, much like Users do, and have permissions assigned to them defining what resources the Roles can and can’t access. Unlike Users though, which represent a single identity, IAM Roles are designed to be assumed by multiple different entities as and when required. Like I say, Roles are used for temporary access to gain access to resources, and each time the role is assumed by a User, an AWS service or an application, a new set of credentials is dynamically created for the duration of that session. As a result, Roles do not have any long term credentials associated, so there is no password for console access, nor are there any access keys for programmatic access that are explicitly associated with the Role.</p>
<p>For every role there will be associated policies controlling access as to what can and can’t be accessed when the role is assumed. There will also be a Trust Relationship, and this Trust Relationship defined who or what can assume the role, for example a User, an AWS account, or an AWS Service. IAM Roles are generally used: if you need to grant temporary access for Users to AWS resources that they don’t normally require access to or you can use a Role to grant access for an IAM user in one account to access resources in another AWS account or perhaps an AWS service needs to access resources on your behalf or if an application requires access to resources you can use a Role instead of embedding credentials into the software itself. Or you might have federated users who require access to specific resources, perhaps those authenticated via Active Directory So, as a result, Roles can be assumed by the following: A user that’s in the same AWS account as the where the role has been created, a user that’s in a different AWS account than where the the role has been created, an AWS service, such as EC2, or an external federated users to your AWS account</p>
<h1 id="Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf"><a href="#Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf" class="headerlink" title="Using AWS Service Roles to Access AWS Resources on Your Behalf"></a>Using AWS Service Roles to Access AWS Resources on Your Behalf</h1><p>An AWS Service Role allows an AWS service to assume a role to access other AWS resources within your own account on your behalf. This is commonly used for EC2 instances, whereby you could create a role for an EC2 instance to assume to gain access to AWS resources on your behalf. Let’s look at an example of when you might use this.</p>
<p>Consider the following scenario. You have an EC2 instance running an application that requires access to Amazon S3 to Put and Get objects using the relevant API calls. To allow access to S3, a set of credentials could be stored on the EC2 instance within the application code allowing it to use those credentials to gain access to the relevant S3 Bucket for any Put or Get API requests. However, in this scenario, you would need to manage these credentials manually including the rotation of access keys, which is obviously an administrative burden and open to the possibility of being compromised by a malicious attacker.</p>
<p>To alleviate this issue, the EC2 instance could be assigned an IAM Role, which in turn would have the relevant permissions associated granting the EC2 instance and its application to access S3 to perform the Put and Get API calls using existing AWS managed or customer manager policies. EC2 instances can be assigned a role during its creation, or to a running instance. You can also replace a role that is already associated with an EC2 instance with a new role. From a security best practice perspective you should always associate a Role to an EC2 instance for accessing AWS resources instead of storing local credentials on the instance itself.</p>
<p>There is also another great advantage of using Service Roles. Let’s now imagine we have a fleet of EC2 instances all using the same application and performing the same task using the same role, but now consider that your existing application, which was used to perform Put and Get requests is now only required to perform Put requests only, and Get requests must be denied.</p>
<p>To make the change, all you’d to do is to alter the permissions assigned to the IAM Role and all EC2 instances associated with that Role would now have the correct permissions. If this same scenario happened by embedding credentials locally on the EC2 instance, then it would take a long time to replicate the change on every instance accurately.</p>
<p>When creating a Service Role, there’s a number of AWS Services that integrate with IAM that support roles. This is a screenshot at the time of writing this course showing the supported AWS services, but for the latest information, you should always check the AWS documentation found here. Before we move on from AWS Service Roles, I want to mention service-linked roles.</p>
<p>A number of different AWS services require roles to perform functions requiring very specific permissions, and in these instances AWS allows you to create service-linked Roles. These are often created the first time that you use a service. Service-linked Roles come pre-configured with the relevant AWS Managed policies, trusts and permissions allowing only that Service to carry out the required operations with other AWS resources that it needs to interact with. Some examples of these roles include AWS ServiceRoleForAmazonSSM.</p>
<p>So AWS Systems Manager uses this IAM service role to manage AWS resources on your behalf. AWS ServiceRoleForCloudTrail. So this service linked role is used for supporting the organization trail feature with AWS CloudTrail. And AWS ServiceRoleForCloudWatchEvents. CloudWatch uses this service-linked role to perform Amazon EC2 alarm actions.</p>
<p>So if we look closer at the AWSServiceRoleForAmazonSSM in IAM, we will find that the trusted identity to use this role is ssm.amazonaws.com, AWS Systems Manager. and with it having an AWS Managed policy already configured, we’re unable to edit and update this policy. This policy is specifically designed to provide access to AWS Resources managed or used by Amazon SSM, and this role is created when you configure SSM. So the difference between AWS Service and AWS Service-Linked Roles is that AWS Service roles allow you to apply your own customer managed or AWS Managed policies, whereas service-linked roles come pre-configured with a specific set of read-only AWS managed policies that can only be used by that particular service.</p>
<h1 id="Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users"><a href="#Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users" class="headerlink" title="Using IAM User Roles to Grant Temporary Access for Users"></a>Using IAM User Roles to Grant Temporary Access for Users</h1><p>There might be circumstances where you will need to grant temporary access to AWS resources for a particular user and the best way to do this would be to allow the user to assume a role with these new permissions. The alternative would be to either add a policy associated with the user and then remember to remove the policy again afterwards, which would likely lead to a security risk, and as we know, attaching policies to a specific user isn’t considered a security best practice, we should use user groups instead. However, if we added these permissions to a user group that the user belonged to, then all users that are a part of that same user group would receive those permissions, so that’s definitely a security risk.</p>
<p>So in this instance, creating a new role with the relevant permissions and allowing only specific users to assume that role, is the best approach to the problem. When a user assumes a role, it replaces all other permissions that the user has. You might think that the user keeps their current permissions and just inherits the additional permissions set out by the role, but that’s not the case. When you assume a role, your existing permissions are temporarily replaced.</p>
<p>I mentioned previously that every role has a trust relationship and so as a user, you can only assume roles where they have been added as a trusted entity. However, in addition to being trusted, the user also has to have the relevant permissions to assume the role as well and this is done via an access policy.</p>
<p>So let’s assume we have a role called RoleS3FullAccess with a permission policy attached as shown which grants full access to Amazon S3. And the trusted entity of this role was defined as the user specified in this ARN. So this means this role trusts the user, Stuart, to assume the role to gain full access to S3. However, Stuart’s permissions do not currently allow him to assume this role. For Stuart to be able to assume the RoleS3FullAcess role, Stuart will need to have the following policy associated with his user, where the text in red is the ARN of the role. When this policy is associated with Stuart, he then has permission to assume the role which is trusted to him as defined in the trusted entity section of the role allowing him to have full access to S3.</p>
<p>Let me now provide a demonstration on how you could create this example that we just ran through. Okay, so I’m in my AWS Management Console and I’m at the IAM dashboard. So in this demonstration, we’re going to create a role with a policy attached and then add an inline policy to the user, Stuart, to ensure that Stuart can assume the role.</p>
<p>So firstly, let’s create our role. So on the left here, we have roles and then over in the top right, we have Create Role. So because we want this role to assumed by a user, we have to pick the correct type of trusted entity. So it’s not going to be assumed by an AWS Service and it’s not to with Federation either, so the option we need here, is this one relating to an AWS account. Now although says another AWS account, this option is used to creating cross-account access, but you can also create roles to be assumed by users within your own account as well. And to do that, all we need to do in the account ID, is to enter our our account ID, so if I just add that in there, and you can also add a couple of additional options, such as requiring multifactor authentication, et cetera.</p>
<p>So once we’ve put in the account ID of where the users are that we want to assume this role, we can then click on next, permissions. And I want the policy that’s attached to this role to have full S3 access, so let’s have a quick search. And here we have it here, Amazon S3 full access, which is an AWS managed policy. Then click on next for tags. Add any tags in that you’d like for the role as an optional step. I’m just gonna leave it blank for this demonstration. And this is where we can give the role a name, so I’m going to call this RoleS3FullAccess. Give it a description if you need to and here we have the trusted entity, which is my own account and then all we need to do from there, is click on Create Role.</p>
<p>Okay, so if we search for that. RoleS3, here we have our role here. So if we go into the role, we can have a look at our trust relationships, so this is essentially saying that anyone within this account is trusted to assume this role and we can also edit this trust relationship as well. So this shows the policy that relates to that trust relationship. So this essentially allows anyone in this account, access to the role, as long as they have that assume role permissions that we’ll be looking at in just a moment. But I want to make this a bit more secure, so I’m going to change it from root to user, Stuart.</p>
<p>So now the trusted relationship for this role will only allow the user, Stuart, within this account to assume the role, given the correct permissions. So I’m gonna update that trust policy and now we can see the trusted entity has been changed. And if we take a look at the permissions, we can see that it has the policy attached there as well. So now we’ve got roles set up with the correct permissions, we now need to edit the user, Stuart with an inline policy to allow that user to assume the role. So I’ve just gone with IAM user of Stuart. If we click on add inline policy and go across to JSON.</p>
<p>Now, here I’m just going to paste in a policy that I’ve already created. So will allow the user, Stuart to use the secure token service AssumeRole action against the role that we just created and that’s the ARN in the role. So now Stuart will have access to assume that role. Let’s go to review policy. Give this a name. I’m just going to call this, AssumeRoleS3. Create policy. And there we have the inline policy that we just created, attached to the user, Stuart.</p>
<p>So now what I want to do, is to log in as the user, Stuart and show you how I can switch to this role. Okay, so I’ve now signed in as the user, Stuart and to switch roles, all I need to do, is to select on the top right up there, select Switch Role. And again, Switch Role. Type in the account. So I’ll just paste that in quickly and then type the role name, which we called RoleS3FullAccess and then click on Switch Role. And that’s it, we’ve now switched roles and if you look in the top right, we can see that it says, RoleS3FullAccess at this account number and that signifies that you swapped your permissions for that role that the user, Stuart has just assumed.</p>
<p>Once you finish what you need to do with the permissions given by the role, you can simply select the top right area again and say Switch Back. And that will revert you back to your original user with your original permissions. IAM user roles are often used to create a cross-account access role, allowing users in one AWS account to access resources in a different AWS account. For a full explanation on how to create cross-account roles, please see our existing course found here.</p>
<h1 id="Using-Roles-for-Federated-Access"><a href="#Using-Roles-for-Federated-Access" class="headerlink" title="Using Roles for Federated Access"></a>Using Roles for Federated Access</h1><p>In this lecture, I want to discuss how users who have been federated can access your resources using roles. When doing so you have two options. Firstly, Web Identity, this allows users federated by a specified external web identity or OpenID Connect provider, to assume this Role to perform actions in your account. And secondly, SAML 2.0 federation, this allows users that are federated with SAML 2.0, to assume this Role to perform actions in your account.</p>
<p>So let’s first look at Web Identity and a scenario where you might need to create a Role. You’ve just created a new mobile application that requires access to Amazon S3 to store media such as photos and videos from users worldwide. As a part of the operations of your application, it will require permissions to S3, to upload and download this media from tens of thousands of users. Embedding long-term credentials into your application code to do this goes against all security best practices. And so instead, you should design your application to request temporary credentials from authenticated users through web identity federation. </p>
<p>Before I go any further, let me explain what identity federation is. It’s basically a method of authentication where two different providers can establish a level of trust, allowing users to authenticate from one, which authorizes them to access resources in the other. During the federation process, one party would act as an Identity Provider, known as an IdP, and the other would be the service provider, an SP. The identity provider authenticates the user and the service provider controls access to their service or resources based on the IdP’s authentication.</p>
<p>You’ve probably all been to websites where it presents you with a login page, where you can either login using existing credentials, native that service, or you might have an option to authenticate using credentials from a third party provider, such as Facebook, Google, Twitter, or LinkedIn, et cetera. So going back to our example, our application run on AWS would be the service provider, and the Web Identity provider could be Google or Facebook, for example.</p>
<p>So when users authenticate to your app via web identity federation, they will receive an authentication token. This token is then exchanged for temporary security credentials in AWS, which can be mapped to your IAM Role, using the AssumeRoleWithWebIdentity API. This then allows the relevant access to Amazon S3 provided by the role, to carry out the operations needed by the application.</p>
<p>Generally, when working with mobile applications, the preferred and recommended method for managing access, would be via Amazon Cognito, to manage his federation process. For more information related to Amazon Cognito, please see our existing course here.</p>
<p>Let’s now take a look at SAML 2.0 federation. Whereas web identity federation is generally used for large, wide scale of access from unknown users, SAML 2.0 is generally used to authenticate your employees using existing directory services that you might already be using. SAML, which stands for Security Assertion Markup Language, is a standard that’s used to exchange authentication and authorization identities between different security domains, which uses security tokens containing assertions to pass information about a user between a SAML Identity Provider and a SAML consumer.</p>
<p>For example, you might already to be using Microsoft Active Directory to authenticate your employees to your internal network. And so you might not want to or need to create lots of users in IAM with their own set of credentials. Instead, it would be easier to allow them to federate their access through via SAML, integrating with your ADFS server. The benefits of this are twofold. It minimizes the amount of administration required within IAM and it allows for a single sign on solution.</p>
<p>As the vast majority of organizations today are using Microsoft Active Directory, using MSAD is an effective way of granting access to your AWS resources without going through the additional burden of creating potentially hundreds of IAM user accounts. Let’s take a high-level look at how active directory authentication mechanism is established. This example will assume the user within an organization requires API access to S3, EC2, and RDS. This scenario will also include the use of an AWS service called Security Token Service, STS. The Security Token Service allows you to gain temporary security credentials for federated users via IAM, associated with IAM roles and policies. Let’s look at this in more detail via a diagram.</p>
<p>A user within an internal organization initiates a request to authenticate against the Active Directory Federated Service, an ADFS server, via a web browser using a single sign on URL. If their authentication is successful by using their Active Directory credentials, SAML will then issue a successful authentication assertion back to the user’s client, requesting federated access. The SAML assertion is then sent to the AWS Security Token Service, to assume a role within IAM using the AssumeRoleWithSAML API. STS then responds to the user requesting federated access with temporary security credentials, with an assumed role and associated permissions, allowing S3, EC2, and RDS access as per our example, the user then has federated access to the necessary AWS services as per the role’s permissions.</p>
<p>This is a very simple overview of how federation is instigated from the user for API access to specific AWS services. Corporate identity federation is always authenticated internally first by Active Directory before AWS, when creating your role for users federating via SAML, you can specify if you want to provide programmatic access only, or programmatic and AWS management Console access, in addition to specific permissions to access other AWS resources.</p>
<h1 id="IAM-AWS-Policy-Types"><a href="#IAM-AWS-Policy-Types" class="headerlink" title="IAM AWS Policy Types"></a>IAM AWS Policy Types</h1><p>Hello and welcome to this lecture where I want to talk about four different policy types that you can expect to see when working with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">AWS IAM</a>. Identity-based policies. These policies can be attached to users, user groups or roles within IAM. Essentially, any entity that depicts an identity. Resource-based policies. Instead of being associated with an identity, these policies are attached in line to resources themselves. An example of a resource-based policy in IAM will be that of a role trust policy where the policy defines the principle that can assume the role.</p>
<p>Permission boundaries. These policies can be associated with a role or user, but they don’t actually grant permissions themselves, instead they define the maximum level of permissions that can be granted to an entity. Organization service control policies, SCPs. These are very similar to permission boundaries in the fact that they do not grant permissions. They define a boundary of maximum permissions. However, these service control policies are associated with an AWS account or organizational unit, an OU, when working with AWS organizations and govern the maximum permissions to the members of those accounts.</p>
<p>With all of these policy types, it’s easy to see why some people can get confused as to which type of policy is being used or should be used. So let me run through each of these policies in greater detail, starting with the identity-based policies.</p>
<p>As already highlighted, these policies can be attached to users, user groups, or roles and they control what permissions each of those entities have. Identity-based policies can either be managed or inline policies, but what does this mean? Well, managed policies are saved within the IAM library of policies and can be attached to any user, user group or role as and when required, and the same policy can be attached to multiple entities.</p>
<p>Managed policies also come in two different flavors, AWS managed policies and customer managed policies. AWS managed policies are policies that have been pre-configured by AWS and made available to you to help you manage some of the most common permissions that you may wish to assign. Some examples of AWS managed policies can be seen here. From the policy name, you can usually tell what access is being given, although you can expand upon each policy to see the JSON document that is associated.</p>
<p>Customer managed policies are those that you have created yourself, which can then be associated with a user, user group or role. You might want to create customer managed policies when the AWS managed policies do not meet your security requirements. For example, you might want to add additional granularity to the policy to restrict access at a specific API call level.</p>
<p>Let me now explain how inline policies contrast against managed policies. So inline policies are embedded directly into the entity, either the user, user group or role. The policy is not saved and stored in the IAM library policy, its only existence is within the associated entity. As a result, it can’t easily be replicated to other entities, it’s specific to that one user, user group or role, creating a one-to-one relationship. It’s not always best practice to use inline policies as they take a lot of administration to keep on top of and should only be used if absolutely necessary. For example, you might have some elevated permissions that you don’t want to have mistakenly given to someone else that they weren’t intended for.</p>
<p>Okay, let me now move onto resource-based policies. So resource-based policies are effectively inline policies that are associated with a resource instead of an identity. If you have been using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon S3</a> for any extended period of time, then you may have come across S3 bucket policies, which is a common resource-based policy where permissions to the bucket are defined at the resource level and defined which principle or principles can access the bucket based upon different actions.</p>
<p>When using roles within IAM, the role has a trust relationship policy, which is a resource-based policy. As a result, the permissions of the trust are embedded inline within the role itself. So in this instance, the resource is the role and the resource-based policy is the trust relationship. This example shows the resource-based policy of the trust relationship of one of my roles. In this policy, the user, Stuart is the principle and has the ability to assume the role. It is this parameter of principle which signifies the difference between an identity-based policy and a resource-based policy.</p>
<p>Identity-based policies don’t have that principle parameter as the policy is already associated within the identity. Resource-based policies must have the principle parameter to determine which identity that the policy permissions relate to. Next up, we have permission boundaries. So permission boundaries can only be associated with a user or role. It’s not possible to add a boundary to a group. They differ from both identity-based and resource-based policies in the fact that permission boundaries don’t grant permissions themselves. They act as a guide rail to limit the maximum level of permissions that the user or role can be given. The policy configured for the boundary can be an AWS managed or customer managed policy.</p>
<p>So let’s assume for example, that our user, Stuart has an identity-based policy associated with him that allows full access to S3 and full access to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-elastic-compute-cloud-ec2/amazon-ec2/">EC2</a> using the following AWS managed policies. However, if a permission boundary was also configured for Stuart using the following AWS managed policies, which only grants read only access to S3, but still full access to EC2, then the resulting access for the user, Stuart would be full access to EC2, but only read only to Amazon S3 as the maximum permission defining by the permissions boundary was limited to read only despite the user having an identity-based policy associated granting full S3 access.</p>
<p>So the last policy I want to reference are the SCPs, the service control policies. So service control policies are used by AWS organizations and attached to AWS accounts or organizational units, OUs, to define the maximum permissions allowed for the members of the associated account or OU. So in a way they act in a similar fashion to that of permission boundaries, but at the account level and affect all members of those accounts.</p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS and EC2 via an identity-based policy. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2 despite having full access to S3. The SCP would serve to prevent service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<p>So to be clear, an SCP does not grant access, they add a guide route to define what is allowed. You’ll still need to configure your identity-based or resource-based policies to identities, granting permission to carry out actions within your accounts. If you want to use service control policies to help you manage your security at an account level, then you need to ensure that you deploy AWS organizations using the enable all feature setting.</p>
<p>Within IAM, you have the ability to view any SCPs that are applicable to your AWS account, the policy statement itself, it’s ARN, the number of entities it affects and you can also review access activity to learn when a principal within the organization last accessed a service. However, if you wanted to update or edit the SCP, then you’d have to do that from within the AWS organization service itself. The SCP can’t be edited from within IAM. For more information relating to how to implement, manage and secure AWS organizations using SCPs, then please see our existing course here.</p>
<h1 id="Examining-the-JSON-Policy-Structure"><a href="#Examining-the-JSON-Policy-Structure" class="headerlink" title="Examining the JSON Policy Structure"></a>Examining the JSON Policy Structure</h1><p>In this lecture, I will be taking a deep look at the syntax and structure of IAM policies. For both identity-based and resource-based policies, the syntax is the same, but you might use slightly different parameters. If you’ve worked with IAM at all before, then I am sure you have come across an IAM policy. After all, it is the component that defines what an identity can or can’t access. But what do these policies actually look like? IAM policies are formatted as JSON documents, JavaScript Object Notation, and each policy will have at least one statement where the structure may look like this example.</p>
<p>Let me break the structure of this policy down to allow you to understand each element. Version. This specifies the policy language version and specifies the language syntax used, and at the time of writing this course, the current policy version is 2012-10-17. Statement. This defines the main element of the policy, which will includes other sub-elements, including the Sid, Effect, Action, Resource, and Condition. These elements will identify the level of access granted or denied and to which resource. A policy must contain at least one statement, but it can also contain an array of statements. For each statement block, it must be enclosed within curly braces, but if you use an array of statements, then you must enclose the entire array within square brackets. And we shall look at an example of this during this lecture.</p>
<p>Sid. This is the statement ID, and it’s an optional parameter that allows you to set a unique identifier within the statement. As you add more arrays within your policy, it can be a good idea to include a Sid for each one, allowing you to name them appropriately, making them more easily identifiable. For example, AllowGetObjectForS3. Without reading the rest of the statement, you can get a good idea of what the permissions in this statement allows.</p>
<p>Effect. This element can be set to either Allow or Deny, which either grant or restrict access for the actions defined in the statement. By default, all access to your resources are denied and so, therefore, if this is set to Allow, it replaces the default denied access. Similarly, if this was set to Deny, it would override any previous Allow. An explicit Deny in a policy will always take precedence over any Allow.</p>
<p>Principal. This parameter defines which principal the policy relates to. The Principal is only used for resource-based policies, for example, those policies attached to S3 Buckets. When using identity-based policies, this parameter is not required within the policy as the policy itself is associated with the principal and not a resource. As an alternative to the Principal parameter, this could be replaced with the NotPrincipal parameter, which would specify the user, role, or AWS account that is not allowed or denied access to the associated resource.</p>
<p>Action. This is the action that will either be allowed or denied, depending on the value entered for the Effect parameter. Actions are effectively API calls for different services. As a result, different actions are used for each service. For example the DeleteBucket action is available for S3 but not EC2, and likewise, the CreateKeyPair action is available for EC2 but not S3. The action is prefixed with the associated AWS service. This example defines two actions, CreateTrail and DeleteTrail, for the CloudTrail service. As another example, you can see this one here with the asterisk, and this acts as a wildcard which represents all actions for the CloudTrail service, essentially granting full access to the service.</p>
<p>Similarly, as we had with the Principal parameter, we could replace the Action parameter with the NotAction instead, and this could help you optimize your policy by creating a shorter version, listing just a limited set of actions that should not match instead of creating a longer policy listing all that actions that should. An example of using the NotAction is shown here. This policy essentially allows all actions for CloudTrail apart from the DeleteTrail API, and this is because the NotAction parameter is being used.</p>
<p>Resource. This element specifies the actual resource you wish the Action and Effect to be applied to. AWS uses identifiers known as ARNs, Amazon Resource Names, to specify specific resources. Typically, ARNs follow a syntax of, arn, partition, service, region, account-id, and then resource. So the Partition element, this relates to the partition that the resource is found in. For standard AWS regions, this would be AWS. Service. This reflects the specific AWS service, for example, S3 or EC2.</p>
<p>Region. This is the region where the resource is located. Some services do not need a region specified, so this can sometimes be left blank. Account-ID. This is your AWS Account ID, without hyphens. Again, some services do not need this information and so can be left blank. Resource. The value of this field will depend on the AWS service you are using. For example, if I were using the action, Action:s3:PutObject, then I could use the bucket name that I wanted those permissions to apply to. Again, we also have a NotResource parameter that can be used to explicitly match all other resources except those specified. For example, this policy will allow access to all S3 buckets other than those specified by the NotResource parameter.</p>
<p>Condition. This is an optional element that allows you to control when the permissions will be effective based upon set criteria. The element itself is made up of a condition and a key value pair and all elements of the condition must be met for the permissions to be active. Let’s take a look at an example condition. In the example, the IP address is the condition itself, which the key value pair will be effective against. The aws:SourceIp is the Key and the 10.10.0.0&#x2F;16 is the value element of the key. So effectively, what this is saying is, if the Source IP address of the user who is requesting access via the policy is within the 10.10.0.0&#x2F;16 network range, then implement the permissions in the policy statement.</p>
<p>Now that I’ve gone through the core parameters of an IAM policy, let’s take a look at a couple of examples to ensure we can understand the permissions that are being presented within the policy. As I mentioned previously, you can have multiple Sids within a statement, each granting a different level of access. The example below demonstrates this, and I have highlighted each a different color to show the separation.</p>
<p>So looking at this example policy, let’s determine what access is being granted. So in StatementBlock1, this allows any resource full access to CloudTrail on the condition that their source IP address is within the 10.10.0.0&#x2F;16 network range. StatementBlock2. This allows full access to all RDS databases using all API calls except for that of the rds:DeleteDBInstance due to the NotAction parameter being used instead of the Action being used. And in StatementBlock3, this allows the creation and deletion of S3 Buckets within the cloudacademy bucket on S3.</p>
<p>Let’s take a look at another example policy, this time, a resource-based policy, and in this instance, it’s from an S3 bucket. So in this policy, it allows the user, Stuart, as highlighted by the Principal parameter, to create and delete buckets in addition to deleting the bucket policy. Stuart can also delete and put objects within the bucket, and the bucket that this policy refers to is the bucket named ca-bucket-uk, as defined by the Resource parameter. However, there is a condition bound to this policy that states that Stuart can only do this if he was authenticated via Multi-Factor Authentication, MFA. That should now give you more of an understanding of how JSON IAM policies work and what they look like.</p>
<h1 id="Creating-an-AWS-IAM-Policy"><a href="#Creating-an-AWS-IAM-Policy" class="headerlink" title="Creating an AWS IAM Policy"></a>Creating an AWS IAM Policy</h1><p>Hello and welcome to this lecture where I will show you how to create your IAM policies. Let’s start by creating an identity-based, customer managed policy, and there are a number of ways to create a customer managed policy, these being, copy an existing AWS Managed Policy. So we can copy an existing policy and then edit that policy to ensure it meets the requirements we need, and this can save us some time.</p>
<p>We can use the Policy Generator, and this allows you to create a customer managed policy by selecting options from a series of dropdown boxes. And we can also create our own policy. So if you’re proficient in JSON and the syntax of IAM policy writing, then you can write your own policies from scratch or paste in a JSON policy from another source. So I now want to give you a quick demonstration on how to create a policy in each of these three ways. And the demonstration will include, how to create a customer managed policy by editing an existing AWS managed policy, how to create a customer managed policy using the Policy Generator, and how to create a customer managed policy from scratch. So let’s take a look.</p>
<p>Okay, so I’ve logged into my AWS Management Console, and I’m at the IAM dashboard, and I’ve gone into Policies. So from here, we can create a number of different IAM policies. And the first method I’m going to show you is how to copy and import an existing AWS managed policy to create your own, if you need to make a few changes to it. So let’s go ahead and do that now. So from here, we need to go across to Create Policy.</p>
<p>Now, from here, we have an option on the top right here called Import Managed Policy, and that’s what we want to do. I want to import an AWS managed policy and then make a couple of small changes to it. So if I select Import Managed Policy and then find the policy that I want to find. So let’s, for example, look at AmazonS3FullAccess, import that, and we can see here that it’s imported the data. And if we look at the JSON tab, we can actually see the JSON format of that policy, and from here, we can directly make changes.</p>
<p>So for this quick demonstration, I want to copy this existing policy, but instead of allowing any resource, I want to specify my own resource for this, which will mean instead of this policy being, allow full access to Amazon S3, it allow full access to a specific bucket on Amazon S3. So let me go ahead and make those changes now. So I’m gonna change the asterisks from Resource and put in my own specific ARN of a bucket. So now, I’ve changed the resource to my own ARN.</p>
<p>So now, all I need to do is click on Next, go to Tags, and I can add any optional tags if I want. I’m just gonna leave that blank for this demonstration. Go across to Review, and here I can give this new policy a name. So I can call it S3FullAccessToMyBucket. And description, this allows full S3 access to ca-bucket-uk. And it gives you a summary of the policy here. So we can see the service that it’s using, the access level, and also the resource. Once we’re happy with that, we can simply click on Create Policy. And we can see that that policy has now been created.</p>
<p>Now, if we have a look at that policy, just by clicking on it there, it’ll take us straight to it, and we can see the JSON version of the policy. So that’s a very quick and easy way if you want to save yourself some time by copying existing S3 managed policies that are already there. Now, I chose a fairly simple policy just for demonstration, but there are some quite complex policies that AWS already have that you might need to just tweak a few changes to so you can simply import those existing managed policies, make your changes, and then save it as a new customer managed policy.</p>
<p>Okay, so that’s the first method covered. Now, next, I want to show you how to create a policy using something called the AWS Policy Generator. Now, if you simply go to Google and type in the AWS Policy Generator, then it’ll come up straight away, and you’ll be brought to a page like this. Now, the actual URL of this Policy Generator, if you’d prefer to type it in, is awspolicygen.s3.amazonasw.com&#x2F;policygen.html. So this is a Policy Generator, and it allows you to easily create different types of AWS policies.</p>
<p>So if we look at this drop down list here, we can create an SQS policy, an S3 bucket policy, which, as we know, is a resource-based policy, a VPC endpoint policy, an IAM policy, and an SNS topic policy. We’re interested in the IAM policy. So if we select that, now, in step two, we can add our statements. Now, here, we have our effect, which can be allow or deny. So let’s say allow for this example. And then, we can select the service that we’re interested in. Let’s select Amazon S3 to keep it nice and simple. And now, we can select our actions.</p>
<p>Now, we can select individual actions here just through these tick boxes for any actions that we’re interested in. So I’ll just select a number of different ones there. And you can see here that it’s selected five actions. If you wanted all actions, you would simply tick this box. And then, you’d put in the ARN of the bucket. So let’s just put in that same bucket that we used in the previous example. And then, here, we can also add in any conditions that we’d like. So just through a series of dropdown boxes, you can specify any conditions that you’d like in there as well. And then, once you’re happy with your policy, simply select Add Statement. And here, it breaks down a summary as well. So it shows the effect, the action, the actions that we selected, the resource, and if there’s any conditions, which we didn’t specify any.</p>
<p>Now, if you wanted to, we can now add an additional statement. So for example, if we wanted to add some RDS elements in here as well, we can select AWS Service, select a load of actions, put in the ARN of your RDS database, and also add that statement to the same policy. Then, once you’re happy with your policy and the number of statements that you’ve added, simply click Generate Policy. And here, it shows you the JSON view of the policy that you would need based on your dropdown selection. So what you can do now is simply copy that, go back to IAM, go to your policies, Create Policy, go to the JSON tab, and simply paste it in. So it’s a very quick way of creating a policy through a series of dropdown boxes. And again, you can just progress through, adding any tags that you need to do, give this a name, ThisIsMyPolicy, and then click Create Policy. And again, you can see that this has been created. You can select that. And again, we can see the JSON statement there.</p>
<p>So now, we’ve looked at how to create a policy by copying an existing AWS managed policy. We’ve looked at how to use the AWS Policy Generator. Now, let’s just quickly review how to create a policy from scratch. So again, go back to Policies and then Create Policy. Now, you can either use this visual editor or go straight to JSON, and you can start typing out your policy in here as and how you need it, or you can use the visual editor, which is very similar to the Policy Generator we just looked at, where, again, you can choose a service, then the specific actions, say all S3 actions, then you can specify the resources, whether you want this, again, as an access point, a bucket, a job, an object. So you can say any object. Then, you can specify any conditions, if you need MFA or specific source IP, for example. And again, you can also add additional permissions here.</p>
<p>So almost like another statement. It’s very similar to the Policy Generator. But for me, personally, I think the Policy Generator is slightly easier to understand. And then, once you have your settings as you want them, and this has all been pre-filled from the options from the visual editor, and, again, click Tags, then Review, and then give this a name, MyPolicy, and then go to Create Policy. And if we take a look at the policy, we can just see its details again. And again, this shows the JSON view, et cetera. And if we wanted to edit the policy directly here, then we can. Click on Edit. Then, we can either edit through the visual editor, or we can directly edit the JSON view as well. So that’s just a couple of ways of creating your IAM policies in a few very simple steps.</p>
<h1 id="Policy-Evaluation-Logic"><a href="#Policy-Evaluation-Logic" class="headerlink" title="Policy Evaluation Logic"></a>Policy Evaluation Logic</h1><p>Every time someone tries to access a resource within AWS, the request is processed through a series of steps. One of which involves evaluating the level of permissions based upon the policies that are used. So let’s take a look at the whole process to understand how access is either granted or denied. And we can start with a simple four step process. So firstly authentication. We must first ensure that the principle sending the request is authenticated as a valid user.</p>
<p>Next, the context. Once authentication of the principle has been established, AWS then needs to determine the context of the request that is being asked, for example, what service or action is being requested. And this ensures that the relevant policies can be highlighted based on the request. We then have policy evaluation, and this is the part that we are interested in. Based on the request, there may be multiple policy types that need to be reviewed to determine the level of access, and I shall cover this in greater detail as we go through this lecture. And then finally, the result. AWS will determine if access is allowed or denied based upon the evaluation of all policies used.</p>
<p>So for this lecture, I want to focus purely on the third point, the policy evaluation and how that process is carried out. The rules for reviewing permissions across multiple policies in a single account is actually quite simple and can be summarized like this: by default, all access to a resource is denied. Access will only be allowed if an Allow has been specified within a policy associated with the principle. If a single Deny exists within any policy associated with the same principle against the same resource then that Deny will overrule any previous Allow that might exist for the same resource and action. So to reiterate, an explicit Deny will always take precedence over an Allow.</p>
<p>Now, there is an order in which policies are evaluated, and the following list of policies are shown in the order of evaluation. So firstly, we have any Organizational Service Control Policies. Then any Resource-based policies, then IAM permission boundaries, and then finally Identity-based policies. So let’s look at an example scenario. Let’s assume that the user, Stuart, is requesting to upload an object to the s3 bucket of ca-bucket-uk using the s3:PutObject API.</p>
<p>With this in mind, let’s assume we have the following policies in place to see what happens at each step of the evaluation. So firstly, the evaluation will review any organization SCPs in place, and here is our example SCP. So this SCP will simply deny all access to RDS. So there is no Deny in place that affects the s3:PutObject requested by Stuart so the evaluation continues. Next, the evaluation will identify any resource-based policies, and here we have a Bucket Policy associated with the ca-bucket-uk as shown.</p>
<p>Again, there is no Deny here for the request in question, so the evaluation continues. Next, we have IAM Permission Boundaries. And this IAM Permission Boundary Policy is set on the user, Stuart. So this policy sets out a maximum permission boundary of full access to s3. Remember, permission boundaries do not actually grant permissions, they set the maximum privilege level, as full access to s3 allowed, the evaluation continues.</p>
<p>Finally, we have the evaluation of any Identity-based Policies, and this policy is associated to the group that the user, Stuart, belongs to. So as we can see, this policy allows any s3 action to the ca-bucket-uk. As a result, this permits Stuart to upload an object using s3:PutObject to the s3 bucket of ca-bucket-uk. So the final decision upon the policy evaluation is to allow the request.</p>
<h1 id="What-is-AWS-Trusted-Advisor"><a href="#What-is-AWS-Trusted-Advisor" class="headerlink" title="What is AWS Trusted Advisor?"></a>What is AWS Trusted Advisor?</h1><p>Hello and welcome to this lecture where I am going to be looking at AWS Trusted Advisor, explaining what it is and the different components that make up this service. </p>
<p>Trusted Advisor plays an integral part in helping you to optimize your infrastructure across a number of key areas, allowing you to make decisions upon recommendations made by the service which follow and best practices that have been honed over the years by AWS.</p>
<p>The service itself can be found within the AWS Management Console under the Management &amp; Governance category, alongside services such as Amazon CloudWatch, Control Tower and Systems Manager. </p>
<p>The main function of Trusted Advisor is to recommend improvements across your AWS account to help optimize and streamline your environment based on these AWS best practices. These recommendations cover 5 distinct categories:</p>
<ol>
<li><strong>Cost optimization</strong> - Helps to identify ways in which you could optimize your resources to help you reduce costs by implementing features such as reserved capacity and removing unused capacity</li>
<li><strong>Performance</strong> - This reviews your resources to highlight any potential performance issues across your infrastructure, determining if you could take benefits from performance-enhancing capabilities such as provisioned throughput</li>
<li><strong>Security</strong> - This analyses your environment for any potential security weaknesses or vulnerabilities that could potentially lead to a breach.</li>
<li><strong>Fault Tolerance</strong> - This helps to suggest best practices to maintain service operations by increasing resiliency, should a fault or incident occur across your resources.</li>
<li><strong>Service Limit</strong> - This identifies and warns you when your resources reach 80% capacity of their service limit quota.</li>
</ol>
<p>Within each of these 5 categories, Trusted Advisor has a list of control points and checks to see how your account, resources and architecture is implemented to determine if you’re aligned with best practice. So it essentially acts as an automatic auditor across your account, which can save you money, increase the efficiency of your resources, maintain a tighter and more secure environment, help to ensure your resources remain operational should a failure occur and that you remain in line with your service limitations, allowing you to request an increase where possible.</p>
<p>Between the 5 different categories and at the time of writing this course, there are over 115 different checks. Please note, that the number of these checks are constantly changing, so for the most up to date figures, please review the following link: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>
<p>Although there are a lot of these checks that Trusted Advisor can perform, not all of them are freely available to anyone with an AWS account. The list of checks that you have access to is very dependent on the support agreement with hold with AWS.</p>
<p>The full power and potential of AWS Trusted Advisor is only available if you have a Business or Enterprise Support Plan with AWS. Without either of these plans then you will only have access to 6 core checks in the security category and all the Service Limits </p>
<p>The 6 checks within security are as follows:</p>
<ul>
<li>S3 Bucket permissions</li>
<li>Security Groups - Specific Ports Unrestricted</li>
<li>EBS Public Snapshots</li>
<li>RDS Public Snapshots</li>
<li>IAM Use</li>
<li>MFA on root account</li>
</ul>
<p>At the time of writing this course, here are the available service limit checks.</p>
<p>Now if you compare this to the full list of checks here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>
<p>….that are included with Business and Enterprise support plans, you will see that the full checklist can provide a huge wealth of valuable information to help you optimise your infrastructure. </p>
<p>In addition to these extra checks that these support plans offer, you will also get the additional benefit of being able to administer certain functions of Trusted Advisor, such as:</p>
<ul>
<li>being able to track the most recent changes to your AWS account by bringing them to the top of your AWS Trusted Advisor dashboard.</li>
<li>using the AWS Support API to retrieve and refresh trusted advisor results. </li>
<li>Also you’ll have the added advantage of having Amazon CloudWatch integration to detect and react to changes made to your Trusted Advisor checks</li>
</ul>
<p>There are also a number of features that everyone has access to, including those outside of the Enterprise and Business support plans, these being:</p>
<ul>
<li><p><strong>Trusted Advisor Notifications</strong> - This is an opt-in or opt-out feature which is completely free to everyone and can be configured within the preferences pane of the Trusted Advisor console. It tracks your resource check changes and cost saving estimates over the course of a week and it will then email up to 3 recipients, for billing, operations and security notifications with a report.</p>
</li>
<li><p><strong>Exclude Items</strong> - This allows you to select specific resources to be excluded from appearing in the console within a specific check. You may want to do this if you are not interested in the reporting for that particular resource and so you decide to exclude it. You can decide to include it again at any point if you do change your mind. This feature can make viewing and managing your checks easier by eliminating some resources within the console.</p>
</li>
<li><p><strong>Action Links</strong> - Many of the items identified within the Checks against resources have hyperlinks associated, these are known as Action Links which allow quick access to the resource in question allowing you to remediate the issue identified. For example, if you reached 80% of the number of VPC’s within a Region, the <strong>‘VPC’</strong> Service Limit Check would highlight this as an issue. The Action Link against the resource would lead you to an AWS Support Center page to create a case to increase the quantity of VPCs you’re allowed within a single region.</p>
</li>
<li><p><strong>Access Management</strong> - AWS Trusted Advisor is tightly integrated within Identity &amp; Access Management. You can grant different levels of access to Trusted Advisor, including Full Access, Read Only, or even restrict access down to specific Categories, Checks and Actions. For example, the following IAM policy allows access to AWS Trusted Advisor, but denies the user from performing a refresh and updating notification preferences.</p>
</li>
</ul>
<p>For a full list of IAM permissions using the trustedadvisor namespace please see the following AWS reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html">https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html</a></p>
<ul>
<li><strong>Refresh</strong> - The data within Trusted Advisor is automatically refreshed if the data is more than 24 hours old when you view it within the console. However, after any refresh, you can perform a manual refresh 5 minutes after the previous refresh. You can either choose to perform a refresh against individual checks or against all checks.</li>
</ul>
<p>Before I finish this lecture I just want to give a high level overview of how Trusted Advisor works in a few simple steps:</p>
<ul>
<li>Once you connect to AWS Trusted Advisor, the service will scan your infrastructure </li>
<li>It will then compare the state of your infrastructure against best practices defined within the 5 categories of Cost Optimization, Security, Performance, Fault Tolerance and service limits</li>
<li>The output of this scan will generate a number of recommendations of how your infrastructure could be optimised with a priority factor</li>
<li>This then allows you to optimize your resources based on the recommendations</li>
</ul>
<p>AWS Trusted Advisor uses a service-linked IAM role to access you resources, named <strong>AWSTrustedAdvisorServiceRolePolicy.</strong> This is a predefined role created by AWS and allows the services to call other services on your behalf. The policy summary of this role is as shown here and helps to define which AWS services that Trusted Advisor communicates with.</p>
<p>Please be aware that this list will change over time, so for an updated list please refer to the role within IAM to determine which services <strong>AWSTrustedAdvisorServiceRolePolicy</strong> has access to.</p>
<h1 id="AWS-Organizations"><a href="#AWS-Organizations" class="headerlink" title="AWS Organizations"></a>AWS Organizations</h1><p>Hello and welcome to this lecture where I will start this course by providing an overview of AWS Organizations as a foundation before focusing on the service control policies.</p>
<p>As businesses expand their footprint on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and utilize more services to build and deploy their applications, it will soon become apparent that the need for multiple AWS accounts is required to manage the environment and infrastructure effectively.</p>
<p>This multi-account strategy is beneficial for a number of reasons as your organization scales, and for example, multi-account strategies include cost optimization and billing, security and governance, management and control of workloads, resource grouping, and helping to define business units.</p>
<p>As you begin to expand with multiple accounts, it will become increasingly more difficult to manage them as separate entities. The more accounts you have, the more distributed your environment becomes and the associated security risks and exposures increase and multiply.</p>
<p>However, with AWS Organizations, it can provide a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization, which helps to maintain your AWS environment from a security, compliance, and account management perspective.</p>
<p>To understand how AWS Organizations operates, we first need to be aware of the hierarchy of the service’s components.</p>
<p>AWS Organizations uses the following components to help you manage your accounts: Organizations, Root, Organizational Units, Accounts, Service Control Policies.</p>
<p>An Organization is an element that serves to form a hierarchical structure of multiple AWS accounts. You could think of an organization as a family tree which provides a graphical view of your entire AWS account structure. At the very top of this Organization, there will be a Root container.</p>
<p>The Root object is simply a container that resides at the top of your Organization. All of your AWS accounts and Organizational units will then sit underneath this Root. Within any Organization, there will only be one single Root object.</p>
<p>Organizational Units (OUs) provide a means of categorizing your AWS Accounts. Again, like the Root, these are simply containers that allow you to group together specific AWS accounts. An organizational unit (or OU) can connect directly below the Root or even below another OU (which can be nested up to 5 times). This allows you to create a hierarchical structure as I mentioned previously.</p>
<p>Accounts. These are your AWS accounts that you use and create to be able to configure and provision AWS resources. Each of your AWS accounts has a 12 digit account number.</p>
<p>Service control policies, or SCPs, allow you to control what services and features are accessible from within an AWS account. These SCPs can either be associated with the Root, Organizational Units, or individual accounts. When an SCP is applied to any of these objects, its associated controls are fed down to all child objects. Think of it as a permission boundary that sets the maximum permission level for the objects that it is applied to.</p>
<p>Now we have an understanding of what AWS Organizations is exactly, what benefits can this bring to your AWS environment?</p>
<p>The primary benefit that this service brings is its ability to centrally manage multiple Accounts from a single AWS account, known as the master account. You can start by inviting your existing accounts to an Organization and then create new accounts directly from the Master Account.</p>
<p>Greater control of your AWS environment. Through the use of Service Control Policies attached to the Root, Organizational Units or individual accounts, administrators of the master account gain powerful control over which services and features—even down to specific API calls—that an IAM user within those accounts can use, regardless of the user’s identity-based or resource-based permissions. Consolidated Billing. The master account of your AWS Organization can be used to consolidate the billing and costs from all member AWS accounts. This allows for greater overall cost management across your individual AWS accounts. Categorization and grouping of accounts. By leveraging Organizational Units, you can segregate and group-specific AWS accounts together, applying different SCPs associated to each OU. For example, you may have a number of AWS accounts that must not have the ability to access any AWS Analytical services. In this case, you could place these accounts into a single OU and assign an SCP that denies this functionality.</p>
<h1 id="Implementing-AWS-Organizations"><a href="#Implementing-AWS-Organizations" class="headerlink" title="Implementing AWS Organizations"></a>Implementing AWS Organizations</h1><p>Hello and welcome to this lecture which will explain how to initially set up and configure AWS organizations. Setting up an organization is a very simple process that starts from a master AWS account. Your master account is a standard <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account that you have chosen to create the AWS organization. It’s best practice to use this AWS account solely as a master account, and not to use it to provision any other resources such as EC2 instances, et cetera. This allows you to restrict access to the master account at a greater level. The few users who need access to it, the better, and you need to do this because the master account carries certain administrative level capabilities such as being able to create additional AWS accounts within your organization, invite other accounts to join your organization, remove AWS accounts from your organization, and apply security features via policies to different levels within your organization.</p>
<p>Once you have selected your AWS account to be used as a master account, you can create an organization. From here, you have two choices when creating an organization type: enable all features or enable only consolidated billing. If you want to set up service control policies, then you need to select enable all features.</p>
<p>The second option allows you to control payments and manage costs centrally from that master account across all associated AWS accounts within the organization. When the organization is created, the master account can create organizational units for AWS account management as required. The master account can also invite other member AWS accounts to join the organization. During this invitational process, the account owner of these invited AWS accounts will receive an email requesting that their AWS account join the organization. Once the accounts have joined the organization, the master account can then move these accounts into the corresponding OUs that have been created and associate relevant service control policies with them.</p>
<p>Let me now show you via demonstration on how to create a new organization and invite an existing account to join it. Now I’m logged into my AWS management console in the AWS account that I want to be the master account, and the first thing I need to do is go to AWS organizations, which is under the management and governance category, and you can see, it’s just at the top here.</p>
<p>So if I go into organizations, and at the moment, I don’t have any organizations set up or created. So the first thing I need to do is click on create organization, and this gives you a quick, high-level screenshot just to explain what creating an organization does. So it provides single payer and centralized cost tracking, it lets you create and invite accounts, it allows you to apply policy-based controls, and it helps you simplify organization-wide management of AWS services.</p>
<p>Now, as I mentioned previously, there’s two options when you create your organization. You can only create it with all features enabled, which is what I just listed, or as you can see here, you can just create your organization to consolidate your billing features. With this demonstration, I’m going to create it with all features. So let’s go ahead and create our organization, and that’s effectively it. So it’s very easy to create your AWS organization to start with, and because this is a brand new organization, this is my master account, which is signified by this star here, and this is my account name, and my account ID.</p>
<p>So, to actually create the organization is very simple, but now I want to add another account as a member account, so let me go ahead and do that. So if I select add account, now I have two options here. I can invite an existing account or create a new account. Now I already have another AWS account, so I’m going to invite an existing account. Now I need to enter the email or account ID, so I’ll just paste in my account, and you can add any notes here, for example, please join my organization, and then you select invite.</p>
<p>Okay, now we can see that we have a request that’s been sent as an invitation. The status is currently open. So now the email address that was registered with this account will get an invitation and they must accept that invite into this organization. So let’s take a look and see if I got that email. So here we can see the email that’s been sent to the owner of that member account, and it says, Stuart would like to add your AWS account to their organization as a member account, and then it just gives some additional blurb about AWS organizations, but to accept the invitation, and to understand what features have been enabled, we need to click on this link here.</p>
<p>So if I select that link, and sign in to my account using my details and MFA code, then I can see that I have an invitation from AWS organizations. We can see the organization ID, the master account name, and the requested controls, which is enable all features. So here, I can either accept or decline and I’m going to accept. I just need to confirm the confirmation message about joining the organization.</p>
<p>Okay, now this member account is now a part of that organization. So if I go back to my master account now, I can see now that within my AWS organization of my master account, I have the CA demo account, which is the name of my other account, and we can see that it’s not a master because it hasn’t got the star whereas this account has the, this is the master account. So as you can see, it’s a very simple process to invite other accounts to your organization.</p>
<p>Now I also mentioned previously about organizing accounts and using organizational units. So if we select organize accounts, at the moment, we only have the root in here. So I can create the new organizational unit and assign each of these accounts into those. So, for example, let me create a new organizational unit called production.</p>
<p>Now I’m also going to create a second organizational unit called test. So let me create another one. At the moment, under root, we have our two accounts. So we have our master account and our member account here. Now I want to move my master account into the production organizational unit, just to make things a little more organized. So I can select the account, click on move, and then simply select where I want it to reside within the tree, and then click move, and we can see, it’s now been removed from the root location, and I want to do the same with the member account, but this time, I want to move that into the test OU. So now, if I click on production over here, this organizational unit, we can see the account that it has inside it, and again, if we go back to the root and click on test, we can see that we have the member account. So I just wanted to show you that quickly just to show you how you can easily and quickly organize your different AWS accounts.</p>
<p>Okay, and that’s the end of the demonstration.</p>
<h1 id="Securing-Your-Organizations-with-Service-Control-Policies"><a href="#Securing-Your-Organizations-with-Service-Control-Policies" class="headerlink" title="Securing Your Organizations with Service Control Policies"></a>Securing Your Organizations with Service Control Policies</h1><p>Hello and welcome to this lecture which will dive into Service Control Policies to understand how they can be used to secure your AWS Organization. SCPs are different from both identity-based and resource-based policies, which grant permissions to users, groups, and roles. However, SCPs do not actually grant permission themselves. Restrictions made within an SCP set a boundary of permissions for <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> accounts.</p>
<p>For example, let’s say a user within an AWS account had full access to S3, RDS, and EC2 via an identity-based policy. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2, despite having full access to S3. The SCP would serve to prevent that service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p>
<p>So to be clear, an SCP does not grant access. They add a guardrail to define what is allowed. You will still need to configure your identity-based or resource-based policies to identities, granting permission to carry out actions within your accounts. If you want to use Service Control Policies to help you manage your security at an account level, then you need to ensure that you deploy AWS Organizations using the enable all features setting.</p>
<p>Before you start using SCPs, you first need to enable them from the root account of your organization. However, you need to ensure that you have the following permissions. From within your AWS Organizations console, navigate to the Policies tab and select the Service Control Policies option that will show the status of Disabled. Then, select the option to enable the SCPs as shown. At this point, SCPs will now be enabled at the root level of your organization and you can now begin to use Service Control Policies within your organization.</p>
<p>I now want to perform a demonstration showing you how to create a Service Control Policy and attach it to an account. In this demonstration, I will have two accounts; my master account called Stuart and another account called CA-Demo. In the CA-Demo account, I have a user, Alice, who has an IAM policy attached that allows full access to S3. From within my master account, I will create a new Service Control Policy denying access to the Amazon S3 service. I will then attach this new SCP to the CA-Demo account. I will then log in as Alice and try to access Amazon S3 to see the result. Let’s take a look.</p>
<p>Okay, so to start this demonstration, I’m logged in as the user Alice in this CA-Demo account which is my member account to my organization. And as I said previously, Alice only has access to Amazon S3. So if we take a look at S3, we should be able to get into the service without any issues. So we can get into the service which is great and also create a bucket as well, so a bucket for Alice. And there we have it. So this user Alice is able to access S3 and also create buckets as well.</p>
<p>Now what I want to do is swap back over to my master account of my AWS Organization, create a Service Control Policy to block access to Amazon S3 and then apply it to this account. So let’s do that next. Okay, so I’m now back in my master account and what I want to do is to create a new Service Control Policy. So from the AWS Organizations dashboard, if I click on Policies, now I already have my Service Control Policies enabled, if yours says disabled, simply select it and then you’ll have the option to enable it.</p>
<p>Once your Service Control Policies are enabled, if you select them, now we can create a policy. We have a default policy here which is FullAWSAccess which allows access to every operation and this is the default Service Control Policy that’s applied to the root of my AWS Organization. However, what I want to do is deny access to one of those services. So if I select create policy, I’ll just call this Deny S3, give it a description of deny access to S3, and then down here is where we can build our policy. So there’s different options.</p>
<p>Firstly, the statement and then the resource and any conditions. So to start with, we can see that over here our policy is in a deny state. Now if we wanted to allow access to a service, we’ll change that to allow. But I’m gonna leave that as deny and over here I wanna select my service that I want to deny so let me scroll down to S3, if I can just find it in the list, there we go, and I want to prevent all actions of S3. Now as we can see, it’s now updated this policy with the action of all actions to S3, but we don’t have the resource yet. So let’s now scroll down to add resource. So our service is S3 and our resource type would be all resources. So if I select add resource, we can now see that the policy has been updated again. So at the moment, it’s denying any action in S3 for all resources.</p>
<p>Now if I wanted to add a condition, then I could add any conditions in here with the condition key and qualifier, et cetera. For this demonstration, I’m not going to add any conditions.</p>
<p>Now I just need to create the policy. Okay, so that’s our policy created. Now although the policy has been created, it’s not actually attached to any organization unit or account thus yet. So let me go back to my accounts. Now if you remember from a previous demonstration, we had our member account, the CA-Demo account, within the Test OU. So I’m going to select that account, go across the Service Control Policies, and then we can see that we have this policy here that we just created and I want to attach that to this specific account. So if I select attach, we can now see that this account has two Service Control Policies, the FullAWSAccess which is filtered down from the root which allows access to all S3 resources, but we also have a Service Control Policy here that denies access to one of those services and the deny will always overrule an allow.</p>
<p>So now we’ve attached that Service Control Policy to the CA-Demo account which is the account that Alice is a part of. Let me now log back in as Alice into this account to see if I can now access S3 or if the Service Control Policy has been applied. So let’s take a look.</p>
<p>Okay, so I’m back in my CA-Demo account which is the member account. I’m logged in as Alice. So now if I go to S3, let’s see what happens. We have an error of access denied and we can’t see any buckets. So it looks as though that Service Control Policy has taken effect because I can’t access S3 at all. So let me see if I can create a bucket, if I just add in any name, and try to create, again we get an error of access denied. So we can see that the Service Control Policy has in fact now blocked access for Alice despite her having IAM permissions allowing her full access to S3. You would have noticed that in my list of SCPs from the demonstration I just carried out there was a pre-existing SCP there called FullAWSAccess and this was associated with the root object of my Organization, so how does the inheritance of SCPs work? Well, let’s take a look at an example.</p>
<p>Let’s suppose we had the following AWS Organization layout, with the FullAWSAccess SCP at the root. The objects within an Organization follow a parent-child relationship, with the Root being the parent to all other child objects. As you can see at the root level, we have an SCP that allows full access to AWS. I now want to establish what SCPs each of the five AWS accounts, highlighted in orange, is governed by. So looking at Account 1, looking from the root, we have the FullAWSAccess SCP. The next level down to Account 1 is the Dev OU. Now, this has four SCPs associated. However, these are number 2, 3, 5 and 6. So at this stage, only the services and features within these SCPs are allowed at this level.</p>
<p>Now if we go down further to Account #1, we have another OU, the Test OU, which again is governed by a list of SCPs, 3, 5 and 6. We can see at this level of the tree, SCP 2 has been dropped so this OU is now restricted to SCP 3, 5 and 6. Therefore, Account 1 which is a child of the Test OU is restricted and governed by the details set out in these three SCPs. Using this methodology, we can now look at the other accounts. So Account #2 is governed by SCP 2, 3, 5 and 6. Account #3 is governed by 1 and 6. And Accounts 4 and 5 are governed by SCP 4.</p>
<p>Let me now look at a different example, this time I’m going to remove the default SCP of FullAWSAccess at the root and replace it with custom SCPs as shown here. Now I’m going to assume in this scenario that every SCP shown has different service restrictions. So as you can see, at the root level this time we have four SCPs, number 1 through to 4, so let’s see how this affects the results this time for each account. So Account #1, looking from the root, we have SCPs 1 through to 4, the next level down to Account #1 is the Dev OU. Now, this also has four SCPs associated. However, these are number 2, 3, 5 and 6. As a result, this Dev OU is only controlled by SCPs 2 and 3. SCPs 5 and 6 are discarded as they are not a part of the parent relationship with the Root object, and 5 and 6 do not exist at any parent level.</p>
<p>Now if we go down further to Account 1, we have another OU, the Test OU, which again is governed by a list of SCPs, 3, 5 and 6. We already know that 5 and 6 are not allowed as they are not in the parent chain. However, SCP number 3 is. So SCP 3 exists from the root downwards in this OU. Therefore, Account #1 is restricted and governed by the details set out in SCP 3. Using this methodology, we can now look at the other accounts. So Account #2 is governed by SCP 2 and 3. Account #3 is governed by SCP 1. And Account #4 and #5 are governed by SCP 4.</p>
<p>Finally, before I end this lecture, please be aware of some of the characteristics of Service Control Policies. SCPs do not affect resource-based policies. They only affect principals managed by your accounts in your organization. SCPs affect all users and roles, in addition to the root user. However, the root user will still be able to change its own password including MFA settings, manage root access keys, and manage x.509 keys for the root user. If you disable SCPs in your organization, all SCPs are deleted and removed. Re-enabling SCPs again in the same organization will revert to the default SCP allowing FullAWSAccess. The following elements are not affected by SCPs: any actions performed by the master account, SCPs do not affect service-linked roles, and managing Amazon CloudFront keys.</p>
<p>That now brings me to the end of this lecture and to the end of this course. You should now have a greater understanding of how Service Control Policies can be used within your AWS Organization to help you centrally control the different levels of access between multiple accounts. If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="An-Overview-of-AWS-WAF"><a href="#An-Overview-of-AWS-WAF" class="headerlink" title="An Overview of AWS WAF"></a>An Overview of AWS WAF</h1><p>Hello, and welcome to this lecture where I shall give an introduction to the AWS WAF service. If you are delivering any kind of web content, either through CloudFront Distributions, Amazon API Gateway REST APIs, Application Load Balancers, or via AWS AppSync GraphQL APIs, then I would recommend you implement the AWS Web Application Firewall service as an additional layer of security.</p>
<p>Without using a web application firewall you could be exposing your websites and web apps to potentially harmful and malicious traffic, which could lead to security risks within your environment. This could have a significant detrimental impact on your business from both a financial and reputation perspective. The AWS Web Application Firewall is a service that helps to prevent websites or web applications from being maliciously attacked by common web attack patterns. Many of which are outlined in the OWASP top 10 list, such as SQL injection and cross-site scripting.</p>
<p>In addition to your own custom criteria, such as perhaps filtering request based on source IP address or country of origin. OWASP, the Open Web Application Security Project is a not-for-profit organization that is dedicated to helping others improve security and software. They provide a top 10 list of the most critical security vulnerabilities and risks surrounding application architecture. For the <a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">latest OWASP top 10 list</a>, please visit the following <a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">link</a>.</p>
<p>If you can implement a WAF within your architecture to mitigate against some of these vulnerabilities, then that’s a huge asset to your web application architecture and a great relief to the security officers within your organization. If you then compare the implementation and administration time needed to deploy AWS WAF to a standard WAF solution, then it’s by far quicker. Further, AWS WAF is far simpler and easy to manage as well. And there are currently two versions of AWS WAF. AWS WAF classic, and AWS WAF, sometimes referred to as new AWS WAF.</p>
<p>Now you should only use AWS WAF classic if you created AWS WAF resources prior to November, 2019. So in this course, I shall be focusing on new AWS WAF. The AWS WAF service interacts with Amazon API Gateway, Amazon CloudFront for its distributions, Application Load Balances, and AWS AppSync, ensuring only filtered web requests that meet specific conditions are forwarded to be processed by these services. WAF works with these other services to filter both HTTP and HTTPS requests by distinguishing between legitimate and harmful inbound requests that will then either be allowed or blocked.</p>
<p>Let’s take a closer look at what the WAF service is composed of to allow you to understand how it works. So there are a number of essential components relating to WAF. These being Web ACLs, Rules and Rule Groups. Let me explain each of these individually to see what part they play within the service, starting with the main building block of the configuration, the Web ACL. So Web Access Control lists, or web ACL, are the main building block of the WAF service. And it’s used as the component that is associated with one of the supported resources to determine which web requests are considered safe and which ones are not.</p>
<p>At the time of writing this course, the supported resources include Amazon CloudFront Distributions, Amazon API Gateway REST APIs, Application Load Balances, and AWS AppSync GraphQL APIs. The Web ACL contains rules, which contains specific controls and criteria checks that assess each web request to determine whether it should be allowed or blocked. Also for each Web ACL, there is a default action that traffic should take if the criteria set out in the rules are not met by the incoming request, and the options for this are either allow or block.</p>
<p>Rules. Each rule contains statements and actions, which focus on specific criteria that the web request will be inspected against. If the inspected request matches the criteria set out in the statement, then that is considered a match. The result of this match can then follow an action of allow, block, or count. Allow means the request is forwarded onto the resource. Block means the request is dropped and a response is sent back to the requester, informing them that the request was denied, and count simply counts the number of matching requests.</p>
<p>Rule Groups. A Rule Group is essentially a collection of rules that you can apply to different Web ACLs as you need to. AWS WAF also comes pre-configured with a number of AWS manageable groups that have been built and designed to protect your resources against some common attack patterns. In addition to this, you can also get access to other Rule Groups created and sold by other vendors on the AWS marketplace. So from a logical architecture perspective, let’s assume that WAF has been associated with a CloudFront Distribution with its origin pointing to S3. The network diagram would look as follows at a high level.</p>
<p>So firstly, a user would initiate a request to the web content being served by the CloudFront Distribution. Next, although logically AWS WAF sits in front of CloudFront, the request will be received by the CloudFront Distribution first. Then it’s immediately forwarded to your associated WAF Web ACL. The AWS WAF Web ACL associated with the distribution would filter the incoming web traffic using the Rules or Rule Groups. So before it’s even traversed your CloudFront environment and network, you have the ability to detect, analyze, and either allow or block the incoming request.</p>
<p>If the criteria highlighted by the Rules deemed that the request should be blocked, then the traffic would be stopped and prevented from progressing further. The requester would then be informed the request was denied. If the traffic met the criteria, allowing the traffic to pass, then the request would be forwarded to CloudFront. CloudFront would then serve the content as required. You might already have other security detection mechanisms within your organization that operate deeper within your infrastructure, perhaps at the web server layer, to mitigate against some of the same risks that WAF does.</p>
<p>And so you may be thinking, why should I implement WAF if I have this existing solution, which is working okay? If you have existing detection systems within your infrastructure, then that’s great. However, the closer they are logically implemented to your web application, then the greater the risk of additional vulnerabilities occurring towards the edge of your infrastructure. It’s best to mitigate vulnerability risks as close to the perimeter of your network environment as possible. By doing so, reduces the chances of other infrastructure and systems being compromised.</p>
<h1 id="AWS-Firewall-Manager-and-Prerequisites"><a href="#AWS-Firewall-Manager-and-Prerequisites" class="headerlink" title="AWS Firewall Manager and Prerequisites"></a>AWS Firewall Manager and Prerequisites</h1><p>Hello, and welcome to this lecture, where I should provide an overview of AWS Firewall Manager, so, you can understand what the service is used for. The core function of AWS Firewall Manager is to help you simplify the management of being able to provide security protection to a range of different resources, between multiple AWS accounts. It’s the fact that it works across multiple account infrastructure, that gives this service a lot of power from a security perspective. So, it’s a great tool to become familiar with, if you are responsible for security across more than one AWS account.</p>
<p>Once your configured security policies to govern the protections that you require for your resources, AWS Firewall Manager, will then automatically apply this protection in addition to managing this protection for any newly creative resources, that match your configuration across any of your accounts that it has responsibility for. So, once it’s set up, the management and protection efforts are simplified dramatically, across your entire organization.</p>
<p>The current AWS services and resources that Firewall Manager provides protection for and integrate with, include the following; <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/protecting-web-apps-aws-waf-shield-firewall-manager/introduction/">AWS WAF, AWS Shield Advanced, AWS Network Firewall</a>, VPC Security Groups and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/amazon-route-53/">Amazon Route 53</a> Resolver DNS Firewall. In addition to these resources that are protected, Firewall Manager is also closely integrated with AWS Organizations. In fact, running AWS Organizations is a prerequisite of using Firewall Manager. For those I’m familiar with AWS Organizations, it’s a service which provides a means of centrally managing and categorizing multiple AWS accounts that you own, bringing them together into a single organization.</p>
<p>Let’s look at the prerequisites of Firewall Manager in a little more detail, to allow you to begin using the service. So, the first step is to decide which AWS account will be used as your Firewall Manager Administrator account. And this account will be used to essentially manage your security policies. Next, you must ensure that this account is a part of an AWS Organization. However, the that it joins must be configured with all features enabled, and not just consolidated billing.</p>
<p>When your account has successfully joined an AWS Organization, you must then configure AWS Firewall Manager within that account, as the Firewall Manager Administrator Account. And this administrator account is used to create a manager security policies. To delegate your account as the administrator, open the Firewall Manager Console, select, get started and enter the account number of your AWS account. Once you’ve added your AWS account to an AWS Organization and designated the Firewall Manager administrative account, you’ll see confirmation ticked on the Firewall Manager dashboard as seen to reflect that you have met these prerequisites.</p>
<p>Next, you must enable AWS config for your account, and for any other account in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/securing-aws-organizations-with-service-control-policies-scps/introduction/">AWS Organization</a> that you want to manage resource security for. And it must be enabled for each region in that account, in which the resources reside. If you don’t want to enable AWS conflict for all resources in each of your accounts, then you must ensure that you enable the following depending on which resources you want Firewall Manager to secure. The next step is optional, depending on if you are looking to apply security policies for all Network Firewalls and DNS Firewalls.</p>
<p>Then you must enable sharing with AWS Organizations in AWS Resource Access Manager. By doing so, it allows you to deploy security policies to these resource types, using Firewall Manager across your accounts in your organization. To complete this configuration, you must open the settings page in the AWS Resource Access Manager Console, and then from here, select, enable sharing with AWS Organizations, and then select, safe settings.</p>
<p>The final step allows Firewall Manager to manage resources in regions, that might be disabled by default. So, you must enable these regions before you can create and managed resources within them. These regions must being enabled in the AWS management account, for your AWS Organization, in addition to the AWS account designated as your Firewall Administrator account. Enabling a region is a simple process. From within the AWS Management Console, navigate to the top right corner and select your account, and then select my account, scroll down to regions section and select, enable in the action column, for the regions that you would like to enable. Once you’ve completed these initial steps you are ready to begin configuring AWS Firewall Manager and its policies.</p>
<h1 id="What-is-AWS-Shield"><a href="#What-is-AWS-Shield" class="headerlink" title="What is AWS Shield?"></a>What is AWS Shield?</h1><p>Hello and welcome to this section of the course focusing on the third and final service, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Shield. AWS Shield is closely related to both AWS WAF and also the AWS Firewall Manager. So what is it used for? Well, AWS Shield has been designed to help protect your infrastructure against distributed denial of service attacks, commonly known as DDoS. These attacks are very common, and the attack itself targets a host which might be running a website or web application, and it receives a huge number of requests simultaneously sent maliciously by an attacker from multiple distributed sources. This increase and flood of traffic aims to prevent legitimate requests getting through to host and being processed, while at the same time severely hindering the performance of the application or website. So much so in fact, that users often think the site is down. </p>
<p>There are a number of different types of DDoS takes that can take place, for example, a SYN Flood. In a Syn Flood attack, a large number of connections are made to the host under attack. The host will then respond accordingly with an SYN&#x2F;ACK packet, at which point the client sending the original connection request would normally respond with another SYN, completing the three-way handshake to allow communications to begin. However, this final SYN packet is not sent to the host, and this leaves a huge number of open connections on the host, resulting in diminished resources available to process legitimate requests. DNS Query Flood. By using multiple DNS queries an attacker can drain the resources against a DNS server, such as Route 53 in AWS. HTTP flood and cache-busting attacks. These attacks operate at layer seven, the application layer. And during an HTTP flood attack an attacker sends a large amount of HTTP requests, which may include POST and GET requests to a host, consuming the resources available. Cache-busting attacks are similar to HTTP floods, however, by using the HTTP request query string they are able to force content to be retrieved from the originating server, rather than from an edge location, which impacts the performance of the source servers available resources unnecessarily. AWS Shield itself is available at two different levels of features, AWS Shield Standard and AWS Shield Advanced, and AWS shield advanced has a lot more power and protection on offer than standard. AWS Shield Standard is free to everyone, well, at least anyone who has an AWS account, and it offers DDoS protection against some of the more common layer three, the network layer, and layer four, transport layer, DDoS attacks. This protection is integrated with both CloudFront and Route 53. </p>
<p>AWS Shield advanced offers a greater level of protection for DDoS attacks across a wider scope of AWS services for an additional cost. This advanced level offers protection against your web applications running on EC2, CloudFront, ELB and also Route 53. In addition to these additional resource types being protected, there are enhanced levels of DDoS protection offered compared to that of Standard. And you will also have access to a 24-by-seven specialized DDoS response team at AWS, known as DRT. With these additional features, the advanced level also provides an enhanced monitoring capability allowing you to view real-time metrics of any attacks against your resources. Whereas the Standard version of Shield offered protection against layer three and layer four, Advanced also offers protection against layer seven, application, attacks. Another great advantage is the fact that you also get cost protection as a part of the plan, whereby your resources may scale suddenly and unexpectedly to cope with the rise in traffic. From a cost perspective, if your decide to go with AWS Shield Advanced then you also get AWS WAF included in the same price, and this price is currently $3,000 a month, plus data transfer fees. As you can see from this image, there are a significant amount of advantages with the Advanced version of AWS Shield over Standard. That now brings me to the end of this lecture.</p>
<h1 id="Amazon-Inspector-Basic-Features"><a href="#Amazon-Inspector-Basic-Features" class="headerlink" title="Amazon Inspector Basic Features"></a>Amazon Inspector Basic Features</h1><p>You need to be responsible for the security of applications, processes, and tools that run on EC2 instances. Amazon Inspector lets you analyze your deployed EC2 instances and helps you identify potential security issues. Some of the basic features include a knowledge base with hundreds of rules that are mapped to common security compliance standards and vulnerability definitions. These rules are regularly updated by AWS security experts. You can install an agent in the operating system of an Amazon EC2 instance to monitor behavior like network, file system, and process activity.</p>
<p>You can also automate vulnerability assessments to make security testing of EC2 instances a regular part of your cloud operations. As a result, Amazon Inspector gives you a prioritized list of findings. A sample list of findings is shown. Amazon inspector is all about protecting the security of your EC2 instances.</p>
<h1 id="Amazon-Guard-Duty-Basic-Features"><a href="#Amazon-Guard-Duty-Basic-Features" class="headerlink" title="Amazon Guard Duty Basic Features"></a>Amazon Guard Duty Basic Features</h1><p>Amazon GuardDuty is an intelligent threat detection service that provides you with an accurate way to consistently monitor and protect your AWS accounts and workloads for suspicious activity. We’re talking about intelligent threat identification for your accounts, data, and workflows. It uses trained machine learning models to identify suspicious user and resource behaviors. It also learns from your environment to eliminate false positive identifications.</p>
<p>Amazon GuardDuty is able to analyze CloudTrail logs, VPC flow logs, and DNS query logs to identify issues worth looking into. An interesting item about GuardDuty is that sample findings help you analyze the type of results that GuardDuty delivers. When you generate sample findings, GuardDuty populates your current findings list with one sample finding of each type.</p>
<p>Don’t forget, GuardDuty is able to display its results to AWS Security Hub. Generating sample findings will allow you to verify AWS Security Hub’s functionality sooner more than later. Let’s take a look at some sample results from the GuardDuty screens. As a result, Amazon GuardDuty gives you a listing of findings classified under three categories, low, medium, and high severity.</p>
<p>In this screen of the AWS console, low severity findings are marked in blue with a small circle next to the finding. Medium severity findings are marked by GuardDuty in orange with a small square next to the finding. High severity findings are marked by GuardDuty in red with a small triangle next to the finding. Also notice how, on the top right, you have a findings summary showing the total for each of the severity categories.</p>
<h1 id="Amazon-Macie-Basic-Features"><a href="#Amazon-Macie-Basic-Features" class="headerlink" title="Amazon Macie Basic Features"></a>Amazon Macie Basic Features</h1><p>Amazon Macie uses machine learning to do its work and helps you discover and analyze sensitive data stored in Amazon S3 buckets, including personal identifiable information, or PII, such as names, addresses, credit card numbers, API Keys, and access credentials among many others. Macie scans S3 buckets and recognizes critical private information. It also automatically tracks changes to buckets and only evaluates new or modified objects in future scans. That way, it doesn’t have to review objects that have not changed and makes the discovery job significantly more efficient and scalable. You can run one-time or automated data discovery and display the results to AWS Security Hub.</p>
<p>Amazon Macie provides a list of findings where the severity and finding type are clearly displayed. In this case, we created a bucket called academy-ca-macie and uploaded a file with disabled user keys, an RDS SQL Query, a credit card list in CSV format, and a few other files. Notice the severity as medium or high. Also, notice the finding type for the S3 objects include Personal, Financial, and Credentials. It also points to the resource that is affected. Finally, it shows when the object was last scanned. In the future, unless there is a change, these objects will not be re-evaluated.</p>
<h1 id="2Finding-Compliance-Data-With-AWS-Artifact"><a href="#2Finding-Compliance-Data-With-AWS-Artifact" class="headerlink" title="2Finding Compliance Data With AWS Artifact"></a>2<strong>Finding Compliance Data With AWS Artifact</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/resource/aws-shared-responsibility-model">AWS Shared Responsibility Model</a></p>
<h1 id="3What-is-Identity-and-Access-Management"><a href="#3What-is-Identity-and-Access-Management" class="headerlink" title="3What is Identity and Access Management?"></a>3<strong>What is Identity and Access Management?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-aws-identity-federation-simplify-access-scale-1549/">Identity Federation course</a></p>
<h1 id="4IAM-Features"><a href="#4IAM-Features" class="headerlink" title="4IAM Features"></a>4<strong>IAM Features</strong></h1><p><a target="_blank" rel="noopener" href="https://sts.amazonaws.com/">STS Global Endpoint</a></p>
<h1 id="8Managing-Multiple-Users-with-IAM-User-Groups"><a href="#8Managing-Multiple-Users-with-IAM-User-Groups" class="headerlink" title="8Managing Multiple Users with IAM User Groups"></a>8<strong>Managing Multiple Users with IAM User Groups</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-quotas.html">Group limitations</a></p>
<h1 id="10Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf"><a href="#10Using-AWS-Service-Roles-to-Access-AWS-Resources-on-Your-Behalf" class="headerlink" title="10Using AWS Service Roles to Access AWS Resources on Your Behalf"></a>10<strong>Using AWS Service Roles to Access AWS Resources on Your Behalf</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html">AWS services that work with IAM</a></p>
<h1 id="11Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users"><a href="#11Using-IAM-User-Roles-to-Grant-Temporary-Access-for-Users" class="headerlink" title="11Using IAM User Roles to Grant Temporary Access for Users"></a>11<strong>Using IAM User Roles to Grant Temporary Access for Users</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/implementing-cross-account-access-using-iam/">Implementing cross-account roles using IAM</a></p>
<h1 id="12Using-Roles-for-Federated-Access"><a href="#12Using-Roles-for-Federated-Access" class="headerlink" title="12Using Roles for Federated Access"></a>12<strong>Using Roles for Federated Access</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-cognito-manage-authentication-authorization-mobile-web-apps-1560/">Using Amazon Cognito to Manage Authentication &amp; Authorization to your Mobile and Web Apps</a></p>
<h1 id="17What-is-AWS-Trusted-Advisor"><a href="#17What-is-AWS-Trusted-Advisor" class="headerlink" title="17What is AWS Trusted Advisor?"></a>17<strong>What is AWS Trusted Advisor?</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">Best Practice Checklist</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html">Full list of IAM permissions</a></p>
<h1 id="21An-Overview-of-AWS-WAF"><a href="#21An-Overview-of-AWS-WAF" class="headerlink" title="21An Overview of AWS WAF"></a>21<strong>An Overview of AWS WAF</strong></h1><p><a target="_blank" rel="noopener" href="https://owasp.org/www-project-top-ten/">OWASP Top Ten List</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:48" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:48-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:24" itemprop="dateModified" datetime="2022-11-20T19:08:24-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-AWS-Shared-Responsibility-Model-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:47" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:47-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:34" itemprop="dateModified" datetime="2022-11-20T19:08:34-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-CloudWatch-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/52/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/52/">52</a><span class="page-number current">53</span><a class="page-number" href="/page/54/">54</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/54/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
