<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/126/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/126/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30/" class="post-title-link" itemprop="url">AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:32" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:32-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:50" itemprop="dateModified" datetime="2022-11-19T22:53:50-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-How-to-Use-KMS-Key-Encryption-to-Protect-Your-Data-30/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to this course focused entirely on how AWS KMS, the Key Management Service, can be used to encrypt your data within AWS. You will learn the basic concepts of the service through to how to manage more complex components such as configuring Key policies. </p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the details shown on the screen. Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#x40;&#99;&#x6c;&#111;&#x75;&#x64;&#x61;&#x63;&#x61;&#x64;&#x65;&#109;&#x79;&#x2e;&#99;&#x6f;&#109;">&#x73;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#x40;&#99;&#x6c;&#111;&#x75;&#x64;&#x61;&#x63;&#x61;&#x64;&#x65;&#109;&#x79;&#x2e;&#99;&#x6f;&#109;</a>, where one of our Cloud Experts will reply to your question. </p>
<p>As this course focuses on data encryption, it’s ideally suited to those in the following roles: Cloud Security Engineers, Cloud Security Architects, Cloud Administrators, and Cloud Support and Operations. The KMS service is also heavily featured within the AWS Specialty Certification, so this could be advantageous to those who are studying for this certification. </p>
<p>The course consists of a number of theory lectures and practical demonstrations. As a result, the course is compiled as follows: </p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/what-kms/">What is KMS</a>? This lecture provides a high-level overview of encryption itself before explaining what the KMS service is, and what it is used for. </li>
<li>Key <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">Components of KMS</a>. To understand how KMS works, you need to be aware of the different components that make up the service, and this lecture explains each element. </li>
<li>Understanding <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">Permissions and Key Policies</a>. KMS is a powerful service, and so understanding how to control access is critical. This lecture focuses on how to grant access to specific keys. </li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/key-management/">Key Management</a>. This lecture looks at some of the security best practices in understanding how to maintain your Key infrastructure. </li>
<li>And the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/course-summary/">Course Summary</a>. This final lecture provides a high-level summary of the key points taken from each of the previous lectures.</li>
</ul>
<p>By the end of this course, you will able to define how the Key encryption process works, explain the differences between the different key types, create and modify Key policies, understand how to rotate, delete, and reinstate keys, and define how to import your own Key material. </p>
<p>To gain the most from this course, you should have a basic understanding and awareness of the following: AWS CloudTrail and AWS IAM, specifically relating to the understanding of policies. </p>
<p>Throughout this course, I will reference a number of URL links which will help and direct you to related information on specific topics. To make these links easily accessible to you, I have included them at the top of the Transcripts section within the lecture that they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#x73;&#x75;&#112;&#112;&#111;&#x72;&#x74;&#64;&#99;&#x6c;&#x6f;&#x75;&#x64;&#97;&#x63;&#x61;&#100;&#101;&#x6d;&#x79;&#46;&#x63;&#x6f;&#x6d;">&#x73;&#x75;&#112;&#112;&#111;&#x72;&#x74;&#64;&#99;&#x6c;&#x6f;&#x75;&#x64;&#97;&#x63;&#x61;&#100;&#101;&#x6d;&#x79;&#46;&#x63;&#x6f;&#x6d;</a>. </p>
<p>That brings me to the end of this lecture. Coming up next, I will introduce the KMS service by looking at it from a fundamental level and answering the question of: What exactly is KMS?</p>
<h1 id="What-is-KMS"><a href="#What-is-KMS" class="headerlink" title="What is KMS?"></a>What is KMS?</h1><p>Hello and welcome to this lecture where I shall explain what the KMS service is and the function that it provides to enhance your data security. </p>
<p>Before we dive into KMS itself, I want to first provide a high-level overview of encryption to help you understand which cryptography method AWS KMS uses. Unencrypted data can be read and seen by anyone who has access to it and data stored at rest or sent between two locations in transit is known as plain text or clear text data. The data is plain to see and could be seen and understood by any recipient. There is no problem with this as long as the data is not sensitive in any way and doesn’t need to be restricted. However, on the other hand, if you have data that is sensitive and you need to ensure that the contents of that data is only viewable by a particular recipient or recipients, then you need to add a level of encryption to that data. But what is data encryption? </p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Data encryption</a> is the mechanism in which information is altered, rendering the plain text data unreadable through the use of mathematical algorithms and encryption keys. When encrypted, the original plain text is now known as cipher text which is unreadable. To decrypt the data, an encryption key is required to revert the cipher text back into a readable format of plain text. A key is simply a string of characters used in conjunction with the encryption algorithm and the longer the key the more robust the encryption. This encryption involving keys can be categorized by either being symmetric cryptography or asymmetric cryptography. </p>
<p>AWS KMS only uses symmetric cryptography so let’s take a look at what this is and what this means. With symmetric encryption, a single key is used to both encrypt and also decrypt the data. So for example if someone was using a symmetric encryption method, they would encrypt the data with a key and then when that same person needed to access that data, they would use the same key that they used to encrypt the data to decrypt the data. However, if the encrypted data was being read by a different person, that person would need to be issued the same key. Remember, the same key is needed to decrypt the data that was used to encrypt it. As a result, this key must be sent securely between the two parties and here it exposes a weakness in this method. If the key is intercepted by anyone during that transmission, then that third party could easily decrypt any data associated with that key. </p>
<p>AWS KMS resolves this issue by acting as a central repository, governing and storing the keys required and only issues the decryption keys to those who have <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">sufficient permissions</a> to do so. Some common symmetric cryptography algorithms that are used are AES which is Advanced Encryption Standard, DES, Digital Encryption Standard, Triple DES and Blowfish. </p>
<p>Now let’s compare this to asymmetric encryption which involves two separate keys. One is used to encrypt the data and a separate key is used to decrypt the data. These keys are created both at the same time and are linked through a mathematical algorithm. One key is considered the private key and should be kept by a single party and should never be shared with anyone else. The other key is considered the public key and this key can be given and shared with anyone. Unlike with the symmetric encryption, this public key does not have to be sent over secure transmission. It doesn’t matter who has access to this public key as without the private key, any data encrypted with it cannot be accessed. Both the private and public key is required to decrypt the data when asymmetric encryption is being used. So how does it work? </p>
<p>If another party wanted to send you an encrypted message or data, they would encrypt the message using your own public key which can be made freely available to them or anyone. It’s public for a reason. The message is then sent to you where you will use your own private key which has that mathematical relationship with your public key to decrypt the data. This allows you to send encrypted data to anyone without the risk of exposing your private key, resolving the issue highlighted with symmetric encryption. </p>
<p>The advantage that symmetric has over asymmetric is the speed of encryption and decryption. Symmetric is a lot faster from a performance perspective. However, it does carry an additional risk as highlighted. Some common examples of asymmetric cryptography algorithms are RSA, Diffie-Hellman, and Digital Signature Algorithm. So now we know that AWS KSM uses symmetric cryptography for its encryption. Let me explain a little more about the service itself. </p>
<p>The Key Management Service is a managed service used to store and generate encryption keys that can be used by other AWS services and applications to encrypt your data. For example, S3 may use the KMS service to enable S3 to offer and perform server-side encryption using KMS generated keys known as SSE-KMS. There are different types of keys used within KMS which perform different roles and functions. I will go into detail on these key types in the next lecture.</p>
<p>Due to the nature of this service, the contents contain highly sensitive data, the key is to decrypt your private data. As a result, administrators at AWS do not have access to your keys within KMS and they cannot recover your keys for you should you delete them. AWS simply administers the underlying operating system and application. All administrative actions performed by Amazon on the underlying system require dual authentication by two Amazon administrators to make sure the action is correct and will not cause any issues with the service. As AWS has no access to your keys, it’s our responsibility as the customer and users of the KMS service to administer our own encryption keys and administer and restrict how those keys are deployed and used within our own environment against the data that we want to protect. It is important to understand that the KMS service is for encryption at rest only which can include for example S3 Object Storage, RDS, EMR and EBS Encryption to name a few. KMS does not perform encryption for data in transit or in motion. If you want to encrypt data while in transit, then you would need to use a different method such as SSL. However, if your data was encrypted at rest using KMS, then when it was sent from one source to another, that data would be a cipher text which could only be converted to plain text with the corresponding key. </p>
<p>Another important aspect of encryption at rest is whether it is done server-side by the server or client-side by the end user. Examples of server-side encryption are back end servers that encrypt the data as it arrives transparent to the end user such as the example I gave earlier with SSE-KMS. The overhead of performing the encryption and managing the keys is handled by the server, in this case S3, not by the client-side application or the end user. Client-side encryption is quite different. Client-side encryption requires the user to interact with the data to make the data encrypted and the overhead of encryption process is on the client rather than the server. </p>
<p>When working with encrypted data, compliance and regulations are often tightly integrated. As a result, KMS works seamlessly with AWS CloudTrail to audit and track how your encryption keys are being used and by whom in addition to other metadata captured by the APIs used such as the source IP address, et cetera. The CloudTrail logs that are stored in S3 record KMS API calls such as Decrypt, Encrypt, GenerateDataKey and GetKeyPolicy among others as shown on the screen. </p>
<p>When architecting your environment with regards to data encryption, you need to be aware that AWS KMS is not a multi-region service like IAM is for example. It is region specific. Therefore, if you are working in a multi-region system with multi-region failover, you need to establish a Key Management Service in each region that you want to encrypt data. </p>
<p>Now we have a base understanding that KMS generates and provides a secure central repository of encryption keys to allow you to encrypt your data at rest or integrating with numerous AWS services. In the next lecture, I want to expand the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">components that makeup the service</a>.</p>
<h1 id="Components-of-KMS"><a href="#Components-of-KMS" class="headerlink" title="Components of KMS"></a>Components of KMS</h1><p>Hello, and welcome to this lecture where I’m going to drill down into the different elements and components that make up the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">KMS service</a> to allow you to understand how it operates and functions. This section will consist of customer master keys, data keys, or data encryption keys, key policies and grants. </p>
<p>Let me start off by explaining the CMK, the customer master key. This is the main key type within KMS. This key can encrypt data up to 4 kilobytes in size, however it is typically used in relation to your data encryption keys, DEKs. The CMK can generate, encrypt and decrypt these DEKs, which are then used outside of the KMS service by other AWS services to perform encryption against your data.</p>
<p>It’s important to understand that there are two types of customer master keys. Firstly, those which are managed and created by you and I, as customers of AWS, which can either be created by using KMS or by importing key material from existing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/key-management/">key management applications</a> into a new CMK. And secondly, those that are managed and created by AWS themselves. The CMKs which are managed by AWS, are used by other AWS services that have the ability to interact with KMS directly to perform encryption against data. For example, Amazon S3, in particular, SSE KMS, these AWS managed keys can only be used by the corresponding AWS service that created them within a particular region. As remember, AWS KMS is a regional service. </p>
<p>These CMKs that are used by the services, are generally created the first time you implement encryption using that particular service. CMKs that are generated and created by us, rather than AWS, provide the ability to implement greater flexibility, such as being able to manage the key, including rotation, governing access and key policy configuration, along with being able to both enable and disable the key when it is no longer required. Now there is a big difference between disabling a CMK and deleting a CMK, which can have significant effect on being able to access your data, governed by the CMK that encrypts it. I shall discuss more on these differences in an upcoming lecture, where I’ll talk about key management. </p>
<p>Like the AWS managed CMKs, AWS services can be configured to use your own customer CMKs, too. Depending on the service and your own internal security policies, the choice is yours as to which CMK the supported services uses to encrypt your data. Any CMKs created within KMS are protected by FIPS, validated cryptography modules. As I mentioned in the previous few slides, data encryption keys, or data keys, are created by the CMK and are used to encrypt your data of any size. When a request to generate a key is issued, the CMK specified in the request will create a plain text data encryption key and an encrypted version of the same data encryption key. Both of these keys are then used to complete the encryption process. As a part of this process, your plain text data is encrypted with the plain text data key using an encryption algorithm. Once encrypted, the plain text data is deleted from memory and the encrypted data key is stored alongside the encrypted data. If anyone gains access to the encrypted data, they will not be able to decrypt it, even if they have access to the encrypted key, as this key was encrypted by the CMK, which remains within the KMS service. This process of having one key encrypted by another key is known as envelope encryption. The only way you would be able to decrypt the object is if you have the relevant decrypt permission for that CMK that the data keys are associated to. </p>
<p>Let me explain the process of how these data keys are used with the CMK outside of KMS, such as S3. And I shall stick with the example of SSE KMS, server side encryption, using KMS keys.</p>
<h3 id="Start-of-SSE-KMS-Sketch-Demonstration"><a href="#Start-of-SSE-KMS-Sketch-Demonstration" class="headerlink" title="Start of SSE-KMS Sketch Demonstration"></a><em><strong>Start of SSE-KMS Sketch Demonstration</strong></em></h3><p>Hello, and welcome to this Cloud Academy sketch video. I’m going to be talking about S3, and in particular, server side encryption with KMS, which is the key management service. So I’m going to show you how the process works from both an encryption point of view and also decryption. Let’s start by creating our user over here. And let’s call our user Bob, keep it nice and simple. </p>
<p>Now, Bob has a document that contains sensitive information. So when he stores it on his S3 bucket, he wants to make sure it’s encrypted, and he’s going to select server side encryption using KMS. So S3 will handle the encryption process for him using keys that are generated by the KMS service. So when he uploads it to his S3 bucket, he specifies that he wants to use SSE KMS. At this point S3 realizes that it needs to invoke services from the KMS service. So it calls upon KMS to generate some data keys for us. So it then sends a request over to KMS. At this point, KMS uses the customer master key, the CMK, to generate two keys. Now the first key is just a plain text data key, and the second key that’s generated is the same key but an encrypted version of that key. </p>
<p>Now both of these keys are then sent back to S3. So S3 receives both those keys, both the plain text key and the encrypted data key. Now S3 has all the keys it needs to perform the encryption process. So S3 will take the object uploaded by Bob, which is his document. It will then combine this with the plain text data key, and it will perform an encryption algorithm, and then that will generate an encrypted version of Bob’s document. So now his document is encrypted. And S3 will store and associate the encrypted data key alongside this object. So these two objects are then stored on S3. Meanwhile, the plain text data key is then deleted from memory. So that’s how encryption works using SSE KMS. </p>
<p>So just to recap. The end user or client will upload the object to S3, specifying that SSE KMS should be used. S3 will then contact KMS, and using the specified CMK, it will generate two data keys, a plain text data key and an encrypted version of that data key. Both of these keys are then sent back to S3, at which point S3 can then encrypt the object that was uploaded, using the plain text data key, to generate an encrypted version of your object. And then S3 will associate and store the encrypted data key alongside your encrypted object. </p>
<p>Okay, so let’s now look at how the decryption process works. So if we just get rid of some of this detail. So Bob will request the object from S3. At this point, S3 knows that the object is encrypted and it has the associated encrypted data key. So what it does, it sends that associated encrypted data key over to KMS, and it asks KMS to generate a plain text data key. So what it’ll do, it’ll use the same CMK plus the encrypted data key. And this will generate a plain text version of that data key. Now just this single plain text data key is then returned to S3. At this point, S3 can then access the encrypted object and use the plain text data key that it just got back from KMS, perform an encryption algorithm again to decrypt the object. And this will generate a plain text version of the object, at which point this can then be returned to Bob. </p>
<p>So just to recap. The user will request access to the encrypted object. S3 will then send the associated encrypted data key to KMS, to generate a plain text version of that encrypted data key, using the associated CMK. This plain text data key is then sent back to S3. The plain text data key is then used to decrypt the encrypted object, generating a plain text version of the object, which can then be returned to the user. So that’s how the encryption process works for S3, SSE KMS. Thank you.</p>
<h3 id="End-of-SSE-KMS-Sketch-Demonstration"><a href="#End-of-SSE-KMS-Sketch-Demonstration" class="headerlink" title="End of SSE-KMS Sketch Demonstration"></a><em><strong>End of SSE-KMS Sketch Demonstration</strong></em></h3><p>The key policy is a security feature within KMS that allows you to define who can use and access a particular key within KMS. These policies are tied to the CMKs, making these resource-based policies, and different key policies can be created for different CMKs. These permissions are defined within this key policy document, which is JSON-based, much like IAM policies are. I will cover key policies in far greater depth in the next lecture, when <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">I discuss permissions </a>and access control. </p>
<p>Grants are another method of controlling access and use of the CMKs held within KMS. Again, they are a resource-based policy, but they allow you to delegate a subset of your own access to a CMK for principals, such as another AWS service within your AWS account. The benefit of this is that there is less risk of someone altering the access control permissions for that CMK. If anyone has the KMS put key policy permission, then they could simply replace the key policy with a different one. Using grants eliminates this possibility, as a grant is created and applied to the CMK for each principle requiring access. Again, coming up in the following lecture, I shall dive deeper into grants and how they work. </p>
<p>That now brings me to the end of this lecture, which leads me nicely onto permissions and access control within KMS. And I shall look at key policies and grants in greater detail.</p>
<h1 id="Understanding-Permissions-amp-Key-Policies"><a href="#Understanding-Permissions-amp-Key-Policies" class="headerlink" title="Understanding Permissions &amp; Key Policies"></a>Understanding Permissions &amp; Key Policies</h1><p>Hello and welcome to this lecture where I will be diving deeper on how to secure access to your KMS keys and associated levels of permission. </p>
<p>With many services that you use within AWS, you can control access using IAM policies whether that is against a user, group, role or even a federated user. The point is access control for most services can be completely controlled and governed by using IAM alone. For KMS however, this is not the case. In all cases, to manage access to your CMKs, you must use a key policy. Without a key policy associated to your CMK, users will not be able to use it. </p>
<p>Permissions to allow you to access and use your CMK can’t be given and generated using IAM alone. As a result, there are a number of different ways in which you can control access to your keys in KMS, just by using key policies, using key policies with IAM, and using key policies with grants. Let me run through each of these options starting with key policies. </p>
<p>As I briefly mentioned <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">in the previous lecture</a>, key policies are resource based policies which are tied to your CMK. And if you want a principal to be able to access your CMK, then a key policy must be in place to do so. The key policy document itself is JSON based much like IAM policies are and the document syntax is much like other IAM policies. They contain elements such as resource, action, effect, principal and optionally conditions. As a result, they typically look like the following. </p>
<p>During the creation of a CMK, whether you create it programmatically or if you create it through the AWS Management Console, either way KMS will create a default key policy for you to allow principals to use the CMK in question as remember a key policy is required for all CMKs. Within both of these default key policies, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">KMS</a> will configure the root of the AWS account full access to the CMK, by doing so ensures that the CMK will never become unusable as it’s not possible to delete the root account. If full access of the CMK was given to another user and then that user was deleted from IAM, it would not be possible to manage the CMK unless you contacted AWS support to regain the control required. Also, by allowing the root account full access to the CMK performs another important and useful function. It allows access to be given to the CMK by normal IAM policies for users and roles, etc. That’s why it’s not possible to allow users access to a CMK without a key policy. Without the root account having full access in the key policy, IAM can’t be used to manage access for other users. </p>
<p>Within the key policy, this section would look as follows. The resource section shows an asterisk which essentially means this CMK that the key policy is being applied to. When you create your CMK through the Management Console, then you have the chance to configure different permission sets. These include key administrators of the CMK and users which are allowed access to the CMK. </p>
<p>You’ll be asked to firstly define the key administrators. These can either be users or roles that you have setup and configured within IAM. These principals can only administer the CMK and not use it to perform any encryption function that may be needed within the CMK. The administrative actions that are allowed by the key admins are shown on screen. During the selection of key admins, you can also specify if you would like them to be able to delete the key via a checkpoint option. An important point to bear in mind is that although these key administrators do not have access to use the CMK, they do have access to update the associated key policy. As a result, if they wanted to, they could give themselves access to use the CMK. </p>
<p>Next you’ll be asked which users you want to allow to use the CMK. And by use, it’s essentially asking which users should be allowed to perform any encryption using the CMK. In addition to these permissions, the users associated will also be able to use grants to delegate a subset of their own permissions to another service which integrates with KMS. For example, if you are attaching an encrypted EBS volume to an EC2 instance, then through the use of grants, you as the key user of the CMK which encrypted the EBS volume implicitly give permission to EC2 to be able to attach that encrypted EBS volume. Within the key policy, the grant permissions look as follows. For every user or role selected as a user of the key will be displayed within the principal section. As you can see from this example, the only user currently selected for this key is Cloud Academy. The permissions given to use the key for any user selected are as follows. </p>
<p>As the key policy uses the same syntax as IAM policies, you can also restrict access to the CMK in the same way that you can restrict access to resources via the Effect element. Instead of using the Effect Allow, you can instead restrict and deny a particular user access to a CMK by specifying the Effect as deny. As mentioned, we can also use key policies in conjunction with IAM policies, but only if you have the following entry within the key policy allowing the root full KMS access to the CMK, by doing so enables you to centralize your permissions administration from within IAM as you would likely be doing for many other AWS services. This would mean you can configure your IAM policies to allow users, groups and roles to perform the encryption and decryption process, for example using the KMS Encrypt and KMS Decrypt permissions. </p>
<p>Using the resource component within the policy, you can also specify which CMKs the user, group or role can use to perform the encryption and decryption process. In this example, we can see that the policy will allow the identity associated with the policy to use two different CMKs to encrypt and decrypt data. The first CMK is within the US-East-1 region and the second CMK is within the EU-West-2 region. </p>
<p>Finally, you can assign permissions using grants alongside key policies. I’ve mentioned grants a couple of times already, but in essence, they allow you to delegate your permissions to another AWS principal within your AWS account. Much like the key policy, grants are another resource based method of access control to the CMKs. The grants themselves need to be created using the AWS KMS APIs. It’s not possible to create them using the AWS Management Console and these grants are then attached to the CMK, much like key policies are. Within the key policy of the CMK, you will see the Sid which allows the users of the CMK to also perform three grant actions, kms:CreateGrant, kms:ListGrants and kms:RevokeGrant. </p>
<p>When issuing the CreateGrant API, a number of different parameters are also issued such as the CMK identifier. The grantee principal to gain the permissions and the required level of operations that the grantee can perform include the following on screen. Once the grant is active, the principal or grantee can then adopt the permissions programmatically based on the level of access provided within the grant. Also, after the grant has been created, a GrantToken and a GrantID is issued. When a grant is created, there may be a delay in being able to use the permissions and this is due to the fact that eventual consistency has to take place. To get around this, you can use the GrantToken with some APIs which would allow the grantee to perform the operation specified within the grant with immediate effect without having to wait for eventual consistency to complete.</p>
<p>I gave an example earlier relating to EC2 and EBS whereby another AWS service can use grants to gain access to and use a CMK. I will now provide a demonstration of how the grantee of a grant can be an IAM user to show how grant permissions work. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration 1"></a>Start of demonstration 1</h3><p>Okay so for this demonstration, we’re going to use two different users, one user that has access to user CMK and create grants and another user that has no IAM policies at all and isn’t explicitly allowed access within the CMK policy either. So let’s go ahead and take a look at the CMK we’re going to be using first. Go down to encryption keys and we’re going to be using this demo key. If we have a look at the policy of this CMK, scroll down and take a look, then we can see that the users within this policy are listed here and we have a user called Alice and Alice can also create grants as well as we can see down here and here’s the permission CreateGrant. So we’re going to be using the user Alice. </p>
<p>So let’s now create an entirely new user with no permissions at all. We’ll call this user Bob. We’ll give them programmatic access because we’ll be using the AWS CLI for this demonstration. We won’t create any permissions or attach them to any groups. And as we could see here, the user has no permissions. Create user and we have their access key here and their secret access key. </p>
<p>So what I’m going to do now I’m going to swap across to my terminal and I’m going to set them up a profile on my AWS CLI. Okay so what I need to do is type in aws configure –profile of Bob and this will ask us for the access key ID so we can copy and paste that in and the same for the secret access key ID. It will then ask us for the default region name. Our CMK is in us-east-1 so it’ll accept that as the default and also the default output format. So now we have two users, Alice and Bob, and Alice is already setup as a profile on the CLI and we’ve just setup Bob as well. As we know, Bob doesn’t have any permissions within the key policy of the CMK and he doesn’t have any permissions within IAM either. </p>
<p>So let’s just do a quick test with Bob. We’ll try and encrypt something using the CMK ID and it will fail so let’s just take a look at that. So if I type in aws kms encrypt using plaintext and we’ll encrypt CloudAcademy using the key-id alias of DemoKey which is the name of my CMK using the profile of Bob and there we can see an error has occurred access denied. That’s because Bob doesn’t have any encrypt permissions. So if we do the same but this time with Alice, so if we change the profile to Alice, then we can see that it has encrypted the file using this CiphertextBlob here. So that file is now encrypted and Alice can do it because she is listed as a user within the key policy for that CMK. </p>
<p>Okay, let me just clear the screen. So now what I want to do is use grants to allow Bob access to encrypt and decrypt data and this would be issued by the user Alice. So the command to do this is as follows so it’s aws kms create-grant using the key-id and we need the ARN of our key here so if we just go across, select our CMK, we have our ARN here so paste that in. We’ll then need to add the grantee-principal and this will be the person who is actually going to gain this access and again we need the ARN of the user. I’ll go ahead and find Bob, copy his ARN, paste that in. Then we need to specify the operations which is the actual access that’s going to be given so we’re want to grant encrypt and decrypt access and this is going to be issued by the profile of Alice. So Alice is using grants to allow Bob to give access of encrypt and decrypt which is access that Alice has. And we could see that that now works. </p>
<p>We now have a GrantToken and also a GrantId as well. And we can use this information to then perform encryption and decryption straight away using the user Bob. So let me now show you how that works. So now we have the GrantToken and the GrantId. What we can do is encrypt or decrypt data. So let’s go ahead and try and encrypt a new file with Bob’s profile. So to do this we type in aws kms encrypt –plaintext and “Friday”. We add the key-id. We have the key-id up here so I’ll just copy and paste that. And then we need to issue the grant-tokens and here I need to add in the GrantToken that was generated which is this block of text so I’ll paste that in. And then finally because we want to encrypt with Bob, we’ll need to add Bob’s profile at the end. As we can see, we have successfully encrypted that file of Friday. So whereas before Bob was unable to encrypt, we have now used the GrantToken which was generated by Alice to allow Bob access to encrypt data. </p>
<p>So just to run through what we’ve done in this demonstration, we had Alice who is a user within the key policy of a CMK. We then created a new user Bob who had no IAM policies attached and he wasn’t listed within the key policy. We then tested that Bob wasn’t able to encrypt anything. We then confirmed that Alice could. We then created a grant using Alice’s profile allowing access to Bob for encrypt and decrypt and there are other permissions there as well such as generate data keys, etc. We just allowed encrypt and decrypt. We then used the GrantToken that was generated from the create-grant to allow Bob to encrypt a file using the new permissions that he gained through the grant. So once you understand the methodology of how you can grant these additional permissions, it’s a fairly simple process to go ahead and allow other principals to use the CMK based on the permissions that you have. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration 1"></a>End of demonstration 1</h3><p>I now want to show you how to create a CMK using the Management Console from scratch and what the final key policy looks like and how key administrators, users and grants all fit in. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration 2"></a>Start of demonstration 2</h3><p>Okay so I’ve just logged in to my AWS account and first of all I’ll need to go to Identity and Access Management which is where KMS can be found. And if we go down the left-hand side on the menus down to encryption keys and this is essentially KMS. This is the KMS function within IAM. Now to create a new CMK, we go up to the blue button here that says create key and we need to add an alias and a description so I’m just going to call this DemoKey and description This is a demo. If we click on advanced options, here we can see different key material origins and I’ll talk more about the difference between KMS and external origins in the following lecture. So for this demonstration, I’m just going to select the key material origin of KMS and this will allow KMS to generate the key material. If we click on next step, here we can add tags. For this demonstration, I’m just going to leave them as blank. </p>
<p>And now it’s asking us to define the key admins, so essentially who should be the administrators of these keys. And remember as I explained earlier, the key administrators aren’t actually able to use the CMK to encrypt and decrypt data. So let me just pick a couple of users here, go down to the next step, and now it’ll ask me to define the users of the CMK. So these are the identities that can actually perform encryption and decryption using this CMK. So again I’ll just pick a couple of users, click on next step, and now we can look at the key policy that’s being created based on the parameters that I have selected. </p>
<p>So if we look at the different sections here, firstly this Sid Enable IAM User Permissions so what we have here is an Allow for the root account or KMS access to this particular CMK. So as I discussed earlier, this entry within the policy ensures that you can use IAM policies to govern access to this CMK. If we scroll down, the next section shows the key administrators for this CMK and here are the two identities that I selected. And if we look at the actions, we can see a number of different KMS actions, but there is no kms:Encrypt and no kms:Decrypt because the key administrators aren’t actually able to use the key for encryption. We scroll down to the next section of the policy and this section defines the actual users of the policy so whoever is listed here under the principal section can actually use this CMK to encrypt data and decrypt data. And if we look at the actions, we can see here kms:Encrypt and kms:Decrypt and GenerateDataKeys, etc. Whereas you’ll find that these permissions aren’t actually within the administrator list. As you can see, there’s no kms:Encrypt. So that’s the users of the CMK. And finally, this section relates to grants. So this also allows whoever has been added to the user section above, which are these two accounts here, it allows them to create grants and generate grants for the CMK. And then if we click on finish, that’s it. </p>
<p>The CMK is created and we can see here that the status is enabled. And if we click on the DemoKey, it would just bring up some different information. So we have the ARN of the key itself. We can see that the status is enabled. And if we scroll down, we can see the key administrators. And there’s a checkbox here that allows the key administrators to delete this key. If you don’t want a key deleted, then you can simply uncheck that box. Scroll down further, we can see the users of the key as well. Any tags that you added would also be there. And that’s it, it’s as simple as that. So it’s a very simple method to create your CMK, give it a name, add any tags that you need, add the key administrators, add the users, and then select finish, and then you will have your CMK. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration 2"></a>End of demonstration 2</h3><p>The overall access of understanding who has access to a CMK in KMS can be a little confusing as there are three potential ways of gaining access and using a CMK, through the key policy, with IAM policies and also Grants. And determining the correct level of access means you need to understand how they all work together. So let’s look at a simple example to ensure we understand some key points. In this scenario, we have three CMKs and four users. </p>
<ul>
<li>CMK-A key policy provides access to the root account which as we know also enables IAM policies to be used in conjunction with the key policy. </li>
<li>CMK-B key policy allows access to Bob and Charlie. </li>
<li>CMK-C key policy provides access to the root account. Access is explicitly denied for Bob, Charlie and David, but full access is given to Alice. </li>
<li>Alice’s IAM policy allows all KMS actions to CMK-A and CMK-B. </li>
<li>Bob has no IAM policy</li>
<li>Charlie’s IAM policy allows KMS encrypt access to CMK-A </li>
<li>and David’s IAM policy allows all KMS actions to CMK-B and CMK-C.</li>
</ul>
<p>So let’s now look at each of these user’s access and what they have access to, starting with Alice. </p>
<p>Alice’s access to CMK-A is successful as her IAM policy allows all KMS actions against CMK-A and CMK-A allows for IAM policies to be used. Her access to CMK-B provides no access as the key policy for this CMK does not allow for IAM policies to be used. And her access to CMK-C is successful as the key policy allows her access despite her having no IAM policy relating to permissions. </p>
<p>Now let’s take a look at Bob. His access to CMK is denied as there are no explicit entries in the key policy for Bob’s access and he has no IAM policy. His access to CMK-B is successful as the key policy allows him access despite him having no IAM policy relating to permissions and access is denied to CMK-C due to explicit deny actions within the key policy and an explicit deny will always overrule any other allow. </p>
<p>Now let’s look at Charlie’s access. For CMK-A, he has encrypt access only which is given through his IAM policy and IAM policy permissions are allowed. For CMK-B, access is also successful as the key policy allows him access. His IAM policy permissions are irrelevant as the CMK does not allow for IAM policies to be used. And his access to CMK-C is denied due to the explicit deny actions within the key policy and an explicit deny will overrule any other allow. </p>
<p>And finally David’s access. He has no access to CMK-A as neither the key policy or his IAM policy provides permissions. He has no access to CMK-B as the key policy for this CMK does not allow for IAM policies to be used and access is also denied to CMK-C due to explicit deny actions within the key policy. </p>
<p>That now brings me to the end of this lecture covering access control permissions for KMS. Coming up next, I shall be looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/key-management/">key management within KMS</a>.</p>
<h1 id="Key-Management"><a href="#Key-Management" class="headerlink" title="Key Management"></a>Key Management</h1><p>Hello and welcome to this lecture where I shall be explaining how to manage your keys within KMS. I will look at rotation of CMKs, How to import key material from an existing key management system outside of AWS, and the deletion of CMKs. So let me start off by looking at how rotation works within your CMKs. </p>
<p>Firstly, why do we need to rotate keys? The simple answer is for security best practice reasons. Largely, the longer the same key is left in place, the more data is encrypted with that key, and if that key is breached, then the wider the blast area of data is at risk. In addition to this, the longer the key is active, the probability of it being breached increases. </p>
<p>The simplest method of rotating keys is to use automatic key rotation, whereby KMS will automatically rotate your keys every 365 days. But what does that mean, exactly? When automatic key rotation is enabled, many of the details of the CMK remain the same, such as the CMK-ID and the ARN, along with any <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">associated permissions and policies</a>. The only thing that changes is the backing key of the CMK. This backing key is the fundamental cryptographic element that is used when the encryption process is taking place. During the rotation, older backing keys are retained to decrypt data that was encrypted prior to this rotation. </p>
<p>One important point to bear in mind is that should a breach of the CMK occur, rotating the key would not remove the threat of that breach. Consider the following scenario. You have encrypted 100 objects which are stored on S3, using SSE-KMS with a customer-managed CMK. It has been made apparent that a breach of your cryptographic material, of your CMK, has occurred, and you decide to rotate the key. As we now know, rotating the key simply creates a new backing key, and retains the older key to allow you to continue to decrypt existing data, encrypted by the original backing key. So by rotating the key, doesn’t prevent the malicious user from decrypting the 100 objects. However, by rotating the key, it does stop them from decrypting any future encryptions of objects, as it now has a new cryptographic material of the backing key. </p>
<p>There are a couple of points to bear in mind when it comes to automatic key rotation. Firstly, Automatic key rotation is not possible with imported key material, and secondly, the key rotation happens every 365 days and there is no way to alter that time frame. If these two points are an issue, then the only solution is to perform a manual key rotation, which would give you greater flexibility and control of how and when you perform rotations of your keys. If your CMK is the state of disabled or pending deletion, then KMS will not perform a key rotation. If the key is re-enabled, or if the deletion process is canceled, then KMS will assess the age of the backing key, and if that key is older than 365 days, the automatic key rotation process will rotate the key immediately. </p>
<p>One final point on automatic key rotation is that it’s not possible to manage the key rotation for any AWS managed CMKs. These are by default rotated every 1095 days, which is essentially three years. Let me now look at manual key rotation, and how this differs from automatic key rotation.</p>
<p>The process of manual key rotation requires a new CMK to be created, which is different from the automatic process. By doing so, a new CMK-ID is created along with a backing key. As a result, if you have any applications that reference the CMK-ID of the old key, you will need to update them and point them to the new CMK-ID. To make things easier in this process, you could use alias names for your keys, and then simply update your alias target to point to the new CMK-ID. To do this, you can use the update alias API, as shown here. </p>
<p>Once you have performed a manual key rotation, you should keep any CMKs that were used to encrypt data before it, as KMS will use the same CMK that it did to encrypt it. So, for example, let’s say you had a document A encrypted with CMK-A, using the backing key of that CMK to perform the cryptographic mechanism. You then performed a manual key rotation to comply with your own internal security policies. In doing so, a new CMK was created, CMK-B, and this CMK then encrypted document B. You then deleted or disabled CMK-A. When Alice tries to retrieve document A in plain text format, the operation fails. This is because <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">KMS</a> knows which CMK was used to encrypt the object, and it must use the same CMK to decrypt it. However, if it is disabled or deleted, then this is not possible. To resolve this issue, the older CMKs must remain active. </p>
<p>When I was just talking about key rotation, I mentioned that it’s not possible to perform automatic key rotation of imported key material, but what is this key material? Key material is essentially the backing key, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">component</a> that completes and implements the encryption and decryption process on behalf of the CMK itself. When customer-managed CMKs are generated and created within KMS, the key material is automatically created for the CMK. However, it is possible to create CMKs without any key material at all. This is done by selecting the External option for the Key Material Origin, under the Advanced Options of the Create Alias and Description page of the CMK creation within the management console. By doing so, you can still create a CMK which will have its own ARN and CMK-ID. However the actual key material for the CMK can then be imported from your own key infrastructure that you may be using with your own on-premise material that you have already been using. When using your own key material, it becomes tied to that CMK, and no other key material can be used for that CMK. This is why it’s not possible to enable automatic key rotation, as only this single backing key or key material can ever be used for that CMK. </p>
<p>The process for importing your own material follows four key points. The first is, as I have already mentioned, creating your CMK with no key material generated by KMS by selecting External for your key origin. Following this, you will then be asked to download a wrapping key, which is also referred to as a public key, and an import token. This wrapping key is to allow you to upload your key material in an encrypted form. When you import your key material, you do not want this to be in plain text format, So AWS KMS provides a means of encrypting it with this public&#x2F;wrapping key. During this step, you will be asked to select an encryption algorithm. The options for this are as shown. AWS recommends that you select option A that’s shown if possible. If not, then to use option B, and lastly, C. The wrapping key then uses this method of encryption to encrypt your key material before it’s uploaded. The import token that is downloaded is used as a part of the import process when uploading your encrypted key material, to ensure that the process completes correctly. Both the wrapping key and the import token is only active for 24 hours once you have downloaded it, so you need to ensure that you complete the process within this timeframe, otherwise you will need to complete this step again, and download a new key and import token. </p>
<p>Once you have downloaded the wrapping key and import token, you are then ready to encrypt your key material that you’ve extracted from your own key management system or hardware security module. One point regarding this step is that the key material must be in a binary format to allow you to use the wrapping key. For detailed information on the commands to carry out this step, you can visit the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-encrypt-key-material.html">link</a>. The final step is to actually import your key material that is now encrypted into KMS and to associate it with your currently empty CMK. This step can be performed programmatically, or from within the AWS Management Console, whereby you need to select your CMK, and select to import the key material. At this point, you would then be prompted for the location of the key material, along with the location of the import token. Optionally, you also have the option of setting an expiration of the key material being imported. If you do set an expiry, CMK would cease to operate when this expires. To activate the CMK again, you would need to re-upload the key material, following steps two to four. </p>
<p>There are some considerations of using your own key material that differ from key material generated by KMS, which is largely down to durability and availability. The key material created by KMS for customer-managed CMKs receives a far higher durability and availability than that of your own key material that has been imported. For example, it’s not possible to set an expiration time with key material generated by KMS, but you can for your own imported material. When this expires, the key material is deleted, but the CMK and metadata of that CMK is retained. Also, if a region-wide failure occurred where your keys were being used and stored, you would need to ensure that you had the key material to import back into the CMK, as KMS would not be able to retain this information. </p>
<p>I now want to talk about the process involved with deleting CMKs. When you no longer have a need or requirement for a particular CMK, you may want to delete it for security best practices and general housekeeping of your key infrastructure. Deleting a key, of course, can have significant impact against your operations and data if there are services that are still using it without your knowledge. To help in this scenario, KMS enforces a scheduled deletion process, which can range from seven to 30 days. When a key is scheduled for deletion, the CMK is taken out of action and put in a state of Pending deletion. When a key is in this state, it can’t be used to perform any encryption or decryption actions, neither can the backing keys for the CMK be rotated. You could also analyze AWS CloudTrail event logs to look for events relating to the use of your CMK, such as an encrypt action against the CMK-ID. </p>
<p>AWS also recommends that you set up a Cloudwatch alarm to identify if anyone tries to use this key to perform an encryption or decryption request. This will help you identify if it is, in fact, still in use, allowing you to cancel the deletion. Details on how to configure this can be found using the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-creating-cloudwatch-alarm.html">link</a> on screen. If you are not confident that your CMK is no longer in use, or that it should be deleted, then you can simply disable the CMK. Again, this will change the key to a state of disabled and prevent it from being used, and, again, prevent the backing key from being rotated. If you are using a CMK which has your own key material imported, then you can delete just the key material from the CMK, as well as having the option of deleting the entire CMK. </p>
<p>I’m now going to perform a quick demonstration to show you how to schedule a deletion of a CMK, and how you can disable and re-enable CMKs via the AWS Management Console. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so for this demonstration, I’m going to show you how to schedule a deletion of a CMK, and then also how to disable and re-enable a CMK. So, again, if we go to IAM. Go down to Encryption keys, which is essentially where KMS lives, and as you remember from the demonstration earlier, where we created the DemoKey, so now I’m going to schedule a deletion for this key. </p>
<p>So if I select the key, and go across to Key actions, and then down to Schedule key deletion, now here you can enter a waiting period between seven and 30 days, so the minimum is seven and the maximum is 30 days. So this will give you a buffer to ensure that you do actually want to delete this key, and give you enough time to ascertain and see if it’s still in use. So, let’s just put in seven days there, go down to Schedule deletion. And we can see here, that the Status is now changed to Pending Deletion. So it won’t be possible to use this CMK anymore, and it won’t be rotated either. </p>
<p>If you change your mind, and you don’t want this CMK deleted, you can just select it again, go to Key actions, and Cancel key deletion. You’ll see at this stage that it puts the key in this state of Disabled, so what you need to do, again, is to select the key, go to Key actions, and simply Enable the key and then it will be fully operational again, and you can use that CMK to encrypt and decrypt data. </p>
<p>You may have noticed in the Key actions there, let me just select the key once more, to Disable the key, there’s an option there, so we can just go ahead and Disable that, and you can see that the Status has changed, and, again, if you need to re-enable it, just simply click on Enable. So to disable and enable the key is very simple, simply selecting the CMK and then just selecting the appropriate option from the Key actions box. And that’s it, so that’s how you can schedule a deletion for your CMK and also disable and enable the key as required.</p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Hello and welcome to this final lecture in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">this course</a> where I shall provide a summary of the key points taken from each of the previous lectures. </p>
<p>I started this course by explaining <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/what-kms/">what exactly KMS was</a> and a high-level overview of Symmetric and Asymmetric cryptography. In this lecture, we learned that unencrypted data is known as plaintext or cleartext, and data encryption is the mechanism in which information is altered, rendering the plaintext data unreadable, which is then known as ciphertext. To decrypt encrypted data, an encryption key is required to revert the ciphertext back into a readable format of plaintext. A key is simply a string of characters used in conjunction with the encryption algorithm. Symmetric cryptography uses a single key, which is used to both encrypt and also decrypt the data. Asymmetric cryptography uses two separate keys. One is used to encrypt the data, and another separate key is used to decrypt the data. One key is considered the private key and the other is considered the public key. KMS is used to store and generate encryption keys that can be used by other AWS services and applications to encrypt your data. AWS administrators do not have access to your keys within KMS and they cannot recover your keys for you should you delete them. KMS is used for encryption at rest only, and KMS works seamlessly with AWS CloudTrail to audit and track how your encryption keys are being used and by whom.</p>
<p>Following this lecture, I then moved on to talking about the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/components-kms/">different components of KMS</a>, and the main points from this lecture were as follows. Customer Master Keys, CMKs, are the main key type within KMS. CMKs can generate, encrypt, and decrypt data encryption keys known as DEKs. Data encryption keys are used outside of the KMS service by other AWS services to perform encryption. And there are two types of Customer Master Keys, customer managed and AWS managed. Customer managed CMKs provide greater flexibility, such as being able to manage the key, including rotation, Key policy configuration, and enable and disable the key. Any CMKs created within KMS are protected by FIPS, validated cryptographic modules. Data encryption keys are created by the CMK and are used to encrypt your data of any size. The CMK can create a plaintext data encryption key and an encrypted version of the data encryption key, and both of these keys are then used to complete the encryption process. Plaintext data is encrypted with the plaintext data key and plaintext data keys are deleted from memory after use. Envelope encryption is the process of having one key encrypted by another key, and Key policies are a security feature that allows you to define who can use and access a particular key within KMS. Key policies are tied to CMKs as a resource-based policy, and Grants allow you to delegate a subset of your own access to a CMK for principals. </p>
<p>Next I covered and explained the different methods of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/understanding-permissions-key-policies/">implementing permissions against your CMKs</a>, and throughout this lecture, I explained that access to use CMKs is not possible through the use of IAM policies alone. To manage access to your CMKs, you must use a Key policy. And there are three different ways of granting access to the use of a CMK, Key policies, Key policies with IAM policies, and Key policies with Grants. Looking at Key policies, there are resource-based policies which are tied to your CMK, they are JSON-based, much like IAM policies are, the syntax follows the same as IAM policies. A Key policy is required for all CMKs, and KMS will configure the root user of the AWS account full access to the CMK which allows access to be given to the CMK by normal IAM policies for users and roles. Without the root account having full access in the Key policy, IAM can’t be used to manage access for other users. Key Administrators and Users can also be configured within the Key policy. And Key Administrators can only administer the CMK and not use it to perform any encryption functions. Users of the CMK define which users should be allowed to perform any encryption using this CMK, and users are also able to use Grants. Using Key policies with IAM policies. Key policies can only be used with IAM policies if you have the following entry within the Key policy allowing the root full KMS access to the CMK. Using Key policies with Grants, you can also assign permissions using Grants alongside Key policies. Grants allow you to delegate your permissions to another AWS principal within your account, Grants need to be created using the AWS KMS APIs, Grants are attached to a CMK, much like Key policies are, and to create a Grant, you use the Create-Grant API. When creating the Grant, you need to specify the CMK identifier, the grantee principal to gain the permissions, and the required level of operations that the grantee can perform. Permissions can then be adopted programmatically by the grantee, and Grants generate a GrantToken and a GrantID. And GrantTokens allow the GrantID to perform the operations with immediate effect without having to wait for eventual consistency to complete. </p>
<p>The final lecture focused on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/key-management/">how to manage your keys and their lifecycle</a>. This lecture highlighted the following points. Key rotation follows security best practices, and the simplest method of rotating CMKs in KMS is to use this automatic key rotation. And the automatic key rotation rotates keys every 365 days. During the automatic key rotation, the only part that changes of the CMK is the backing key, and the backing key is the cryptographic element that is used when the encryption process is taking place. All existing backing keys are retained during and after rotation, and rotating keys does not remove the threat of a security breach. Automatic key rotation is not possible with imported key material. CMKs in the state of disabled or pending deletion will not be rotated, and it’s not possible to manage the key rotation for any AWS-managed CMKs. These are rotated every 1,095 days, or three years. Manual key rotation is the process of replacing the current CMK with a new CMK. For any applications that reference the CMK-ID of the oldie key, you will then need to update and point them to the new CMK-ID following a manual rotation or update the Alias name of the key with the new CMK-ID Target. Key material is the backing key, the component that completes and implements the encryption and decryption process on behalf of the CMK itself. It is possible to create CMKs without any key material at all, allowing you to import your own, and the process for importing your own key material follows four main points. Creating your CMK with no key material, download a wrapping key, which is also referred to as a public key, and an import token, and these will remain active for 24 hours, encrypt your key material using the wrapping key, and then import your key material that is now encrypted into KMS and associate it with your currently empty CMK. Deleting a key can have significant impact against your data if there are services that are still using it without your knowledge, and KMS enforces a scheduled deletion process, which can range from seven to 30 days to help manage and identify if the key is still in use. Keys scheduled for deletion can’t be used to perform encryption or decryption actions, neither can the backing keys for the CMK be rotated. By analyzing AWS CloudTrail event logs, you can look for events relating to the use of the CMK, such as an encrypt action against the CMK-ID. AWS also recommends that you set up a CloudWatch alarm to identify anyone trying to use this key to perform any encryption or decryption requests. And you should consider disabling a key instead of deleting a key, as this will still take the key out of operation, however, you can reinstate it again at any point should you need it. </p>
<p>That now brings me to the end of this lecture and to the end of this course. You should now have a greater understanding of the AWS Key Management Service, and through the use of different keys, how it can provide a level of encryption of your data across a range of AWS services. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:&#115;&#x75;&#x70;&#x70;&#x6f;&#114;&#116;&#x40;&#99;&#108;&#111;&#117;&#100;&#x61;&#99;&#97;&#100;&#x65;&#109;&#x79;&#46;&#99;&#x6f;&#x6d;">&#115;&#x75;&#x70;&#x70;&#x6f;&#114;&#116;&#x40;&#99;&#108;&#111;&#117;&#100;&#x61;&#99;&#97;&#100;&#x65;&#109;&#x79;&#46;&#99;&#x6f;&#x6d;</a>. Your feedback is greatly appreciated. </p>
<p>Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="3Components-of-KMS"><a href="#3Components-of-KMS" class="headerlink" title="3Components of KMS"></a>3<strong>Components of KMS</strong></h1><p><a target="_blank" rel="noopener" href="https://csrc.nist.gov/projects/cryptographic-module-validation-program/Certificate/3139">FIPS 140-2 Validated Cryptographic Modules</a></p>
<h1 id="5Key-Management"><a href="#5Key-Management" class="headerlink" title="5Key Management"></a>5<strong>Key Management</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/kms/latest/developerguide/importing-keys-encrypt-key-material.html">Encrypting Imported Key Material</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/kms/latest/developerguide/deleting-keys-creating-cloudwatch-alarm.html">Creating an Amazon CloudWatch Alarm to Detect Usage of a Customer Master Key that is Pending Deletion</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring-Metrics-and-Logging-29/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring-Metrics-and-Logging-29/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring,-Metrics-and-Logging-29</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:30" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:30-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:54:26" itemprop="dateModified" datetime="2022-11-19T22:54:26-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring-Metrics-and-Logging-29/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Advanced-Techniques-for-AWS-Monitoring-Metrics-and-Logging-29/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to CloudAcademy.com’s Advanced AWS Monitoring Metrics and Logging course. This course is intended for DevOps engineers that want to get the most out of monitoring metrics and logging and, in general, keeping track of the state of their cloud.</p>
<p>So in this lecture, we’ll be going through an introduction of the course, which will include a definition of the three terms that make up the title of the course. We’ll also go over the intent of the course and what you’re trying to learn while watching this video series. We’ll go over the scope of the content, so everything that’ll be covered beyond just the intent of what you should learn, the benefits of advanced systems running for monitoring metrics and logging on AWS, the different lectures in the course that you should expect to see, and then a brief summary of the intent of the course in a final mission statement.</p>
<p>So without further ado, let’s get into it. So first, let’s define the term “monitoring”. When we think of monitoring, we should be thinking of how we can observe and check the progress or quality of something over a period of time, and keep it under a systematic review. Now in Amazon Web Services, this means verification that your cloud works, and I’ve included icons for three of the main services that people should be familiar with for doing this. You may not have used all of them before, but you’ll become more familiar with them over this course as we talk about how we might do these things.</p>
<p>That’s the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-route53-dns/dns-private-hosted-zones-virtual-private-cloud-1/">Route 53</a> icon in the top-left there. We can use Route 53 health checks, where we ping a certain DNS endpoint and do a holistic check to make sure that we get a 200 error code, or just a general heartbeat or response back.</p>
<p>That’s the CloudWatch icon up there in the top-right, which can include CloudWatch Events, Metrics, and Logs, and as well as Alarms.</p>
<p>Then, we have the ELB icon there. There are ELB health checks for when we’re thinking about doing auto-scaling, and making sure that individual instances behind a load balancer are healthy.</p>
<p>So we should also look at metrics. When we think about defining metrics, we have a standard or a system of measurement. So in AWS, this means quantifying cloud behavior and state. So standard or system of measurement here means that we have a specific thing that we’re measuring whenever we think about a metric. It’s a little bit different than monitoring, where we are more looking for a binary “is it online or not” over a course of time. Whereas, a metric we’re looking to quantify something using CloudWatch, which is that icon there. So quantifying our cloud behavior and state could include things like reading the amount of traffic that comes across EC2 instances or an Elastic Load Balancer, reading the amount of provisioned throughput or consumed throughput for DynamoDB tables. Any number of things that we can put numbers to and are nicely graphed over time, that’s what you should be thinking about for metrics.</p>
<p>So we also use metrics as input for determining CloudWatch Alarms, which is that icon in the bottom-right there, where that red presumably would be a threshold that I cross. That’s one of the reasons that I would be using metrics, is to see whenever I cross certain thresholds and create certain behaviors.</p>
<p>So moving on to logging, this one might surprise people, recording of performance events or day-to-day activities. So in Amazon Web Services, this means a time sequence of system occurrences, which is very generic. It’s not like you might think, where most people coming from a non-cloud background or from a cloud that doesn’t have sophisticated value-added services tools to help us manage our logging, might think of logging as dumping out files that represent things that happened during your application code. Which, that’s a useful abstraction for maybe a single desktop computer, but it’s extremely difficult to handle something like that when thinking of log files as just individual files. Not just logging in general, log events, performance events, or day-to-day activities. It’s very difficult to manage that level of complexity in the cloud, so that’s why you’re watching this course.</p>
<p>So intent of this course is enable actionable understanding of your cloud, which is a very generic statement. But what that means is that we go from thinking about logs as afterthoughts, or things that we might use for debugging whenever things go wrong, and move forward towards this after, where monitoring metrics and logging is a first-class design task that delivers massive business value. So what does that mean? Well, it means that when we implement correctly monitoring metrics and logging, as a DevOps engineer, one of the primary things that you should be praised for is the level of sophistication for your monitoring metrics and logging system. By making it a very easy system to extract value from, answer questions about the operational metrics of a system, and ensure high availability, ensure good software delivery practices. So monitoring metrics and logging is very important, and as we move towards this value-added thinking around monitoring metrics and logging, rather than afterthoughts, we’ll be better DevOps engineers.</p>
<p>So the scope of our content is that we fundamentally rethink the log. So as we eluded to earlier when we defined the log, we need to in this course fundamentally rethink the log away from a set of files, maybe, that you might run on your virtual machine or bare metal if you’re coming from a data center that you actually have.</p>
<p>We have to think about logs and metrics, and how they offer value. So one of the statements that I made on an earlier slide was that we are going to turn these things that are typically afterthoughts into value-added systems. So we have to cover how we can do something like that and extract that value. So we’ll learn the skills to extract the insight from the logs. When I talk about value, typically the type of value that we’re thinking of when we talk about using logs to get values insight, so we have three levels of completeness of information or data. So we have data, which is just that might be individual lines in your log. We have information, which is a slightly higher level of abstraction where I can speak an English sentence and explain what the data means. Then, insight, which is where I take away some critical thing that I didn’t know before, or I’ve learned something new from that kind of information or data. So we want to be able to use logs to extract new insights that we’ve never thought of or seen before, and deliver value that way.</p>
<p>So we want to learn practical methods for handling logs. If we’re going to be messing with all these logs and these metrics, which are really kind of just a subset of logs, then we need practical methods for handling these things and handling complexity. Because as you know in a highly dynamic cloud environment where we’ve got lots of distribution and lots of moving parts, sometimes the challenge can just be managing complexity.</p>
<p>We’ll design some automation around log event streams. So you should know ways, in addition to having human eyes derive insight from logs, the insight that you can receive from logs if they’re structured can also derive automation. So we’ll get into that a little bit later. But there are a number of places where you can read or sift through logs, and depending on what you see, do automation actions. So we’re going to get logging superpowers, effectively.</p>
<p>Monitoring and metrics are also in the title of the course. But I like to think about those as subsets of logging in general, which is the more generic thing where monitoring is a little bit more binary in the context of the cloud, where we’re monitoring for uptime. Metrics are a little bit more oriented towards quantitative goals.</p>
<p>So what are the benefits of some of the advanced systems that we’ll be building? Advanced logging helps manage systems in a number of different ways. Logging techniques should scale the business. We should yield immediately the convenient insights that we were talking about earlier, and we will reduce the ongoing DevOps effort of managing our cloud.</p>
<p>Finally, when we look at the lectures in our course, we have events everywhere handling distribution, try the ELK stack and ChatOps with Slack. So what that gets us is events everywhere. We’ll be talking about how to rethink the log in general, that it’s changing our brain around the paradigm shift that is going from thinking about logs as a file-oriented system into this events-driven system. Handling distribution we’ll get into a little bit around the nature of the systems that we’ll use to manage the complexity around delivering these insights and this additional value from logging systems. Try the ELK stack will be a show-and-tell, where I walk through a very, very common, the most common actually, logging extraction and insight extraction tool on Amazon. The ELK stack is now offered as a service from Amazon, and ChatOps with Slack. So ChatOps eludes to when you present automation with your system. Where if system events occur, rather than emailing you or phone-calling you, since most people spend a lot of their time in chat and chats are effectively an event stream as well, we can insert log notices into our chat system. I’ve picked Slack, which is an enterprise chat system that got really popular in 2014 and 2015, where we can do easy API-driven development to insert insight into our group chats.</p>
<p>So when we think about doing these different lectures, we should be thinking about all of these different graphs and insights that we might be deriving. We will mostly see some of these in the ELK stack, but start thinking about your logs as a tool to derive insight and do analytics, which is what we have all these graphs on here on the side.</p>
<p>So in summary, this course will teach you how to extract value from observing your AWS systems. It’s very generic-speaking, but effectively it means we first rethink the way that we are handling logs in general, and start packaging them and utilizing them in a different way. Then, design appropriate technical systems to handle the new format of the logs that we’re going to deal with in the best way possible, and make them usable and consumable by other systems as best possible, and usable and consumable by both technical and sometimes even nontechnical users. Then, we’ll go over some practical examples for how we can actually utilize these things so you can envision implementing these in your own systems or your own company.</p>
<p>So next up, we’ll be doing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/events-everywhere/">Events Everywhere</a>, which is the video course that’ll teach us how to rethink how we do log systems.</p>
<h1 id="Events-Everywhere"><a href="#Events-Everywhere" class="headerlink" title="Events Everywhere"></a>Events Everywhere</h1><p>Welcome back to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/introduction-27/">Advanced AWS Monitoring Metrics and Logging</a> on CloudAcademy.com. In this lecture, we’ll be talking about how events are everywhere and how this relates to logging.</p>
<p>First of all, monitoring metrics and logging, they’re all events. We’ll talk about how we can use metrics and how they’re slightly different than normal logs. We can talk about how logging is everything and the value that it holds. We’ll go over event sources and consumers, so different things that are creating events and different things that are reading or seeing these events come across and acting on them, and how streams solve a lot of the problems associated with managing these logs. Finally, after going through all of the logic in the previous slides, we’ll talk about why we’re saying “death to the log file” as we eluded to in the introduction lecture.</p>
<p>So again, events are everywhere. Monitoring metrics and logging, they’re all events. So let that sink in for a moment. Events are, “Hello. Boom. Something happened. Event.” Monitoring is a typically binary, “Oh, holistic check. Is my system online?” This can be a DNS ping or HTTP check or something. Metrics are typically a quantifiable, time-sequenced thing, so metrics of usage, metrics of traffic, metrics of throughput. Logging is a little bit more generic and is the parent category to both of these, in that it can be unstructured or structured. So every health check, metrics collection ping, line of log content is an event, or a thing that happened.</p>
<p>You can imagine that there can be a timestamp associated with all three categories of these things, so we just think about it as recording an event or a thing that happened. Like normal analytics, what happens has value, so there’s a logical jump here. If we think about what most businesses, or at least small businesses or companies that haven’t undergone technology modernization efforts, most businesses think only about analytics from a marketing or even a physical operations perspective perhaps on a factory floor.</p>
<p>These business analysis, business intelligence, business analytics tools, these are a sophisticated ecosystem, but we’re just seeing log analytics catch on. So if we think about events as different things that occur and we have analytics over events in the marketing space, we should also be able to do analytics over the logging space when we have log events.</p>
<p>When we think about metrics, we should think about this kind of graph where they’re entirely quantitative and our metrics are best used for time-series algorithms and reaction. Because metrics are inherently quantified already, they have some sort of number associated with them, the metric that we like to think about, we can make decisions based on what we see.</p>
<p>So here we actually look at a DynamoDB consumed write capacity units throughput table, and this is a metric in the CloudWatch Console. This metric tells me that I just consumed about a quarter of a million requests in five minutes after a period of essentially no requests overtime. So metrics are useful because they let you do things like detect when something like that happens. Where I go from almost no requests coming across my system or making writes to my DynamoDB table, and suddenly one minute there was 0.25 million requests, and the previous 5-minute period there was over 200,000 requests. We want to be able to figure out when these kind of spikes happen, and we do that using metrics in the CloudWatch Metrics Console.</p>
<p>So logging is everything. Logging, the more generic field of things that we’re talking about, where metrics are technically just log events with a well-quantified field to them. They still have timestamps. So business logic is loggable and maybe quantifiable, the quantifiable part being the metrics, and we can think about logs as append-only series of events in a flexible semi-or-unstructured format that can be quantified. That’s a little bit of a doozy of a sentence to think about. But append-only, meaning, if you think about a log file, when you concatenate to the end of the log file you append to each new line that you come out with. Log files are just a file representation of a stream, like we’re looking at here in the bottom-right. So we have an ordering where the top or left, it depends on the orientation of your log, in a file it’d be the top of the file, is the oldest event. The next record that you write to the log will always be guaranteed to be the newest record at the time. It’s also append-only is very important here, when we think about it. We’re not modifying 0 through 11 in that log event right there in the bottom-right. We are creating number 12, and that append-only property is very useful to us, as we’ll see a little bit later here.</p>
<p>They’re also best served with structure. That’s my little joke there. I have a small JSON object that I’ve typed into a text editor. But what this tells us is that a metric is actually one of the more structured log types. It has an associated quantified piece to it. But all logs should be served with structure and consistent fields. So rather than just logging or printing text out to the console whenever you’re trying to do your debugging, it’s more helpful if you return the kind of error or the nature of the error, or any kind of details or parameters that were provided to a method when you log out an error.</p>
<p>When we think about logging, we should be thinking about how to move from free text, or unstructured format, to more structured formats since that’s what computers and people will be able to do analysis over.</p>
<p>Note that I don’t mention the storage medium here, that stream on the bottom-right there. It doesn’t say that it’s stored to disk, that log sequence. It doesn’t say that it’s stored to disk. It doesn’t say that it’s in a database. It doesn’t say that it’s in memory. It doesn’t really matter, because logs can be transported or represented in a number of different ways. So don’t think about files when you think about logs. Think about these sequences of events that may or may not have quantitative properties on them and should be structured if you’re doing things correctly.</p>
<p>Talking about event sources and consumer, first of all, data availability is good. This is an age-old mantra of anybody that’s ever done business intelligence or analytics from a data warehousing or data-like perspective that generally making more data available to more different parties is a good thing. Because we get better insights, better integration, and more actionable insights as we diversify the way that we can consume these things.</p>
<p>Metrics and log events are data. Right? So particularly if we think about from the previous slide, where I had a “Best”: “Served” “With”: “Structure” JSON object, if my logs are in JSON format there are very sophisticated and well-built out tools to help us do analysis over things like JSON objects. Because they’re effectively a serialized representation of objects in memory, once we have our logs into JSON or some representation like that, then we can use them as first-class citizens or primary data sources.</p>
<p>So events are good, because we’re saying if data availability and having more data about our business that’s relevant is clean, if that kind of data availability is good and metrics and log events are data, then we should be looking towards our metrics and log event data as good, or sources of value for the business.</p>
<p>One example where somebody created an actionable system that actually delivers value just using logs, this is a metric. Which in my mind, it’s a subset of the log and that’s how you should think about it as well. Auto Scaling actually uses this method. So if we look at, we have an Auto Scaling group here on the left, we have a group of instances. If all of these instances are publishing metrics, which they are, to Amazon CloudWatch and we have a thresholding algorithm on them, which are our CloudWatch alarms. If you’ve ever done an Auto Scaling threshold alarm, that’s one in the same. Then, we have logic that once we emit to the alarm and say, “Oh, I flagged a specific pattern in the log data, and I’ve come up with some analysis and some actionable thing that I need to do based on what I’ve seen,” then we trigger the Auto Scaling and command something like an auto scaling system to scale up and down. So this is actually effectively how you would accomplish auto-scaling.</p>
<p>Beyond AWS, you know Amazon is very sophisticated and they have a very straightforward infrastructure¬-driven requirement to use a form of log to deliver value, we can also think about delivering value via a normal SaaS company by using log data to make self-managing systems. So let’s trace through the user journey here. In the bottom-left we have a user or an end user, customer. The user can be a human being in the case of a web application, or it could be another software system in the case of more of a systems-oriented or a service-oriented architecture. They interact with your service, which I’ve represented as a collection of EC2 instances here, but it could be any number of complex things or stacks.</p>
<p>We produce business logs from the different components of our service, and we emit them over to CloudWatch Logs. We can pick them up from CloudWatch Logs using Amazon Elasticsearch Service, or ELK, which we’ll get into a little bit later in the course. Based on thresholds coming out of that system, we can create custom alarms, notify an SNS topic, and have different actions be taken based on that SNS topic. We can write the SNS topic into a support database using a Lambda perhaps, or another system watching for this, and have customer support read out of a support database. We can page engineers based on different things that we see come out of the logs. We can also, if we have sophisticated enough logs that provide us with enough information, sometimes perform actions that solve the problem completely autonomously, which would be a healing Lambda automaton. It could also be EC2 instances as well. But we can implement self-healing logic if we have business logic logs that can flag things like increased error rates, or something like that.</p>
<p>So beyond just metrics that you’re thinking about for throughput, we can also do more sophisticated logic like detecting patterns in the actual text or enumerated inputs inside of business logs, and create fairly simply, even though there’s a lot of boxes and arrows here. There’s lots of different value-added services that Amazon provides to us, such that we can string together one of these self-managing SaaS systems without creating our own instances or software simply by stringing together a couple different Amazon Web Services managed services.</p>
<p>So we have a complexity management problem already, just looking at that other slide over there. We think about strings as time-sequenced event buffers. We already talked about streams, and we already talked about events. Events are these different things that pop up in our logs, or they’re the individual things that happen. Streams are entire sequences of these events. Streams also work as buffers, because if we have a stream that is the data intermediary to carry between two systems that might want to share log data, the streams will allow us to do a number of different things. They’ll allow us to natively support log event style, so that one’s clear. If we think about logs as a sequence of events and streams are simply time sequences of events, then this is a natural thing to want to start streaming our logs.</p>
<p>We also get a unified transport to other services from a stream. So if we think about what Kinesis is primarily used for, if you’re familiar with the rest of the Amazon platform, it’s primarily used for moving data in the correct order from one place to another. If we think about firehoses, if you’ve ever heard of that before, data firehoses, those are just streams as well. The firehose typically just means the velocity with which they come through. Then, three and four, they allow different consumption and production rates. If we think about if we have a system that is creating things, events very quickly or in a burst-y matter, and then another system that slowly DQs things in a constant rate, we can use a stream to buffer between those two systems as we have the spiky production and consistent consumption. Or even the other way around, where we might have constant addition to the stream, and then a spiky read off of the stream to do analysis and perhaps a spark, or write to a database.</p>
<p>It also allows us to decouple producer and consumer systems. So rather than having to keep a registry of all of the DNS addresses of all other micro-services in a complex system, we could have our log systems simply write to a stream that never changes location or DNS address. Write to that stream, and not concern itself with the consumers that are picking these things up.</p>
<p>We could also potentially have multiple producers writing to the same stream, and multiple consumers reading off of the same stream. So we can have a many-to-many relationship in which the producer only has to be aware of one thing, where the stream is, and the consumer only has to be aware of one thing. Even if we had 10 producers and 10 consumers all producing to and reading from the same stream, the only thing that any of those services need to be aware of is the stream location, so we have a nice decoupling there.</p>
<p>Streams are great, and we talked about why they’re great. We need to realize that logs aren’t just files that you open and scan when something breaks. They’re a primary data transport mechanism, that is we can put the data that come out of our log streams and do a number of different things on them, even replicate databases. They’re a primary first-class citizen for data, and log events are fundamental design building blocks.</p>
<p>So if we looked at my self-healing system, the primary thing that it was operating over were logs. All of my business logic was centered around the log, and even in Auto Scaling the primary thing that Auto Scaling does is use the design of the log system as the fundamental building block. It just reads out of this log stream, these time-sequenced events of load, and then make decisions for when to scale out. Hopefully, you’ve learned a thing or two about why streams and events are everywhere in a logging problem. Next up, we’ll be talking about how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/handling-distribution/">handle distribution</a>, that is how to handle the distributed nature of log producers and log consumers, and in general how to write a distributed system that handles logs in the cloud.</p>
<h1 id="Handling-Distribution"><a href="#Handling-Distribution" class="headerlink" title="Handling Distribution"></a>Handling Distribution</h1><p>Welcome back to Advanced <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/introduction-27/">Amazon Web Services Monitoring Metrics and Logging</a> on CloudAcademy.com. In this lecture, we’ll be talking about how we handle distribution and the distributed nature of stacks that we need to log and monitor.</p>
<p>First, we’ll talk about how we need to stream everything to make this goal of handling distributed event sources work correctly. We need to understand the log group structure. We need to talk about how we perform unification and distribution of our streams and log events into multiple consumers. We need to talk about different kinds of stream syncs that we can use when we’re talking about creating our log streams and how they can be useful to syncing data or transactions into individual syncs. We’ll talk a little bit about how you might perform an archival and backup solution, so how we might achieve logically the long-term auditability requirements that you may have rather than using the logs as an in-flight or real-time log streaming. And then we’ll talk briefly about aggregation with the ELK stack and how that is logically achieved.</p>
<p>So, we’re talking about handling distribution. We are talking about how we want to stream it all. So when we’re talking about streaming in an Amazon Web Services cloud, now we say we need to stream it all and that’s because streams are helpful transport mechanism as we learned in the other lesson. But when we’re thinking about streaming logs specifically in Amazon Web Services, we should be thinking of Kinesis CloudWatch Log Streams and how they are your friend and we’re going to use them copiously as we set up these logging systems.</p>
<p>So first of all, we need to log everything, truly everything to CloudWatch Logs first and use this as a source of truth. So it’s a common temptation to log to disk because it’s easy and convenient and it’s nice and easy or log to standard out even. If we’re logging to standard out, we should be using a CloudWatch Logs daemon to take each of the lines that we’re logging to standard out and pick them up and emit them to CloudWatch Logs which Amazon provides a tool that lets us reroute anything that’s sent to standard out or to disk to CloudWatch Logs which runs as a daemon on Amazon Linux, Ubuntu, or their CentOS distributions. So, rather than just using a plain file, we should be thinking about using a daemon like that and submitting to CloudWatch Logs first. And if we’re using anything like a Lambda or something, we will natively support that CloudWatch Logs.</p>
<p>So all processing should be done stream with CloudWatch. This is an important notion that the top of our log stream funnel should be starting with CloudWatch just because in Amazon we have the ability to do long-term retention if we’re streaming into CloudWatch. But we also have the ability to read off of it like an inflight stream that might be more transient or temporary. So you get the best of both worlds when we’re thinking about using CloudWatch Logs because we can get long-term persistence as well as event ordering and event buffering through the streaming behavior.</p>
<p>So, when we’re thinking about how logs work and we’re CloudWatch and how log model works, this actually works the same model, this diagram that we’re looking at. This is actually how Apache Kafka works and CloudWatch is in some ways similar to Kafka in that it has similar partitioning and such. So let’s take a look.</p>
<p>So we have… CloudWatch as a service encompasses this entire diagram. But these two inner rectangular shapes that we have, we see one log group and another log group on the left and right here. Log groups are the logical grouping of our logs. So, Amazon Lambda already creates log groups based on the Lambda name and names spaces them under the AWS Lambda. You can also create log groups by putting a log group when you’re running an easy two instance if you name the group along the same vein. So for instance, I might decide to use one singular log group for each process that I might be running inside of an auto-scaling group. So if I’m running two processes at a time, two primary processes at a time on each instance in an auto-scaling group, I might use two log groups, two log groups that are used from each system.</p>
<p>So, log groups are great but they’re not the unit that you read from. They’re just the logical unit of abstraction. When we’re thinking about actually pulling data off in those time sequenced recordings like we were talking about in the other lecture, only the log streams which are belonging to these log groups are guaranteed to be in order. So there’s no absolute ordering of events inside of a log group at all. There is absolute ordering inside of a log stream. So, that’s a key distinction because we might have multiple parallel streams being produced at the same time but not have any way to globally order them if we don’t create a system on top of it. So, if you think about CloudWatch Logs and the model that we have, we have there are many CloudWatch has many log groups. Log group has many streams. Stream has many events. And inside of each stream, the events are guaranteed to be in order as you can see here.</p>
<p>So, when we’re thinking about having these multiple logical streams, it’s important to remember that if we’re trying to configure a useful streaming application for logs where there may be systems that need to consume multiple streams or there may be systems that need to consume streams in a different format, than they’re used otherwise, then we have these two different operations that we can perform which are our primary method of working with log streaming system design. So, on the left here, we have unification where we have the capability to unify two streams into a single stream and publish them to a consumer who may be considered, who may be concerned with the correctly global ordered streams, that is stream C, that is the unification or interleaving of A and B. We may be not concerned with the global ordering. We might just be concerned with the mixing two topics together. So those could be API and database logs. This is actually how you might merge two streams within a log group into stream C. So presumably, we could imagine if we stream into Kinesis from two different streams inside of the same log group, then that stream C there might be the Kinesis or the unified log which represents the merged events from all streams inside of a group. And if we have a consumer that is interested in reading all events from that entire group, then consumer A will be happy with stream C.</p>
<p>We also have this ability to do a transformation with a map or a filter or both at the same time, filter map or a map filter. So, if we see we have this stream before and we run through some sort of logical process and emit into a stream after. In Amazon, this is typically achieved by reading out of a Kinesis Stream running a funk door on a Lambda on shards of the events coming out, shards being a frame of multiple events coming out at once as we optimize for network throughput. So for instance we wouldn’t want to, if we’re streaming a million records every hour, we wouldn’t necessarily want to read out of the stream one record at a time. We might want to chunk 10 at a time, that would be a shard.</p>
<p>So these map or filter operations here, we can submit or we can have a Lambda read out of the upstream stream, execute some sort of mapping or filtering logic and put it into a second stream. When we combine these two methods, this unification and this distribution through a map or filter, this transformation, we combine these two and realize that we can chain them together. We can design systems of arbitrary complexity because we can also have multiple consumers per stream, right? So key concepts, merge and alter.</p>
<p>But there’s this third concept where we have stream syncs and we can sync the output of a stream into multiple different places here rather than showing a fanout of any streaming, typically fanout happens where have an end consumer rather than fanning out into two individual streams. So if you see here, potentially, this is an example that we could use where we have an application database that’s receiving application-style reads and writes in a transaction log. Now that transaction log, we can actually use a stream as our transport mechanism. So the transaction log only needs to be aware of the stream location. So, app database only knows where to submit its stream. And then our ElasticSearch, Free-Text search, our Hadoop, Ad-hoc analytics, our Redshift for analysis and business intelligence SQL, our S3 and then subsequently Glacier backup, our replica application database even since the transaction log represents a changed stream that we can use to recreate a replica or anything else, we only need to know where the transaction log and that where is that data that we’re curious about exists. We actually don’t need to know where the application database lives. That way if we have any of the databases go down either upstream or downstream, the only piece that needs to have a consistent address that needs to be addressed is that transaction log.</p>
<p>So log streams are great for replication if we see the app database versus the replica app database. The log is actually the ideal method for replicating any kind of state across two different databases simply because that time ordered sequence of events, if you replay it on the other database, you get an exact copy and you can also use the transaction to recreate the secondary database at any point in time since you can only partially play forward if you want if you restore the database.</p>
<p>So, common question is if we’re not looking at logs as files anymore, that we go and peruse through when we need to do some sort of lookup in the past to see what went wrong, how do we do archival backup and auditability when we’re talking about this brave new world of log events and streaming? Well, rather than just looking up files and doing direct S3, simply look at the S3 objects as another sync. So, again, we can have our database creating a transaction log stream, maybe publishing to Kinesis, and we can have a Lambda reading off of Kinesis and writing an S3 object for each shard. So we have logs that are divided by time and we have many objects over time. We can also then still have the other consumers read from the same transaction log stream. So the archival and auditability actually is no longer a special case at all. It’s a primary case where it’s just another sync. It’s just another reader off of the same stream.</p>
<p>Once we write into the S3 objects, we could set life cycle rules on the S3 bucket object and you can go and look up the documentation in Amazon Web Services if you so choose. But a life cycle rule effectively tells S3 to change the storage class of data after a certain amount of time. Storage class changes can include altering to this AWS Glacier which is storage on magnetic tape which is at the time of this recording of this video it’s seven-tenths of a cent in US dollars in the primary regions, the US East and US West versus three cents for the S3 storage. So it’s four times cheaper to store in Glacier, so we might imagine that after a certain period of time, it would be advantageous for us to simply life cycle rule into Glacier.</p>
<p>But the important piece here is that the S3 object writing is the part that finishes dealing with the streams and then we batch after that. That portion there is not a special case anymore in this archival and backup in auditability when you’re using streams. It’s just another consumer. So, it’s a very clean design and consistent. If you really want files, you should do it this way.</p>
<p>So, if you think about how we do event sources and consumers, we can also realize that we can make the ELK stack another consumer. So the ELK stack stands for Elasticsearch, Logstash, and Kibana. Now Elasticsearch is a free text search database and arbitrary query analysis, noSQL engine. It’s typically used for free text search which lends itself really well to doing kind of lookups or forensic work on your cloud system because you can do a free text search for the error message or a reference code or something and see every time that a reference code or error messages appear in the entire history of your log stream, so E is great in ELK.</p>
<p>L stands for Logstash and it’s our indexing mechanism. It is the way that data is flumed into Elasticsearch typically. Since that part is pretty much abstracted away by CloudWatch Logs and Lambda, we don’t really think about that too much when we’re rolling our own ELK stack solution on AWS but that’s what the L stands for.</p>
<p>K is Kibana. So the E in ELK for that search engine in Elasticsearch service, that’s just an API-driven database that speaks JSON as its wire protocol and it uses HTTP. The K there stands for Kibana. Kibana is a system that uses, it’s just a graphical interface and an indexing system and some prebuilt logic and prebuilt indexes logics on the cluster. So it runs on the Elasticsearch cluster itself and it’s a GUI that lets you do things like create graphs, do log analysis, and general business logic on the index logs inside of the E portion, that Elasticsearch portion. So K, Kibana is what adds the GUI and the credibility value.</p>
<p>So, if we look at this entire flow chart here, it’s the same thing that we’ve been seeing with those white boxes early on in the slides, only this has some exact services named to it. So, the Elasticsearch, yet again, is just another sync. So we have our database or presumably, this would be something like a DynamoDB or even a SQL database if you wanted to implement your own transaction log scraper. In this case, I’ve used the Dynamo logo. DynamoDB can submit its changes to this stream, change system, so there’s DynamoDB Streams is something that is supported now. In effect you are turning on what looks almost exactly like a Kinesis Stream that is populated with all of the events from a change stream off the database. So it’s almost exactly like a transaction log.</p>
<p>You can have a Lambda poll on that transaction log or that change log so we can have a Lambda reading off the change stream. Then you could emit… Then whenever you have these changes occurring, you can have the CloudWatch log streams that are created by Lambda by default, in this case, we can have those logs automatically indexed by CloudWatch. That’s a default thing if we set the role policy correct to allow Lambdas to create the logs. CloudWatch, we can also configure to stream into another Lambda. So we can stream into this Lambda off of our CloudWatch Logs and have the Lambda insert into the Elasticsearch service in the correct way for Kibana to operate efficiently. That is actually a console action that is handled for you.</p>
<p>And then we have a fast, searchable, united graphical logs UI which is excellent for Ops management. So this one of the more common syncs that people think of because it uses logs for the same utility that people are used to thinking about using them for, for debugging. This is just the extremely sophisticated way to debug an entire cloud because you don’t need to submit only the API Lambda logs or the database transaction logs. You don’t need to submit only those things to the Elasticsearch service. We can also submit logs from anywhere else in the cloud and still have them aggregated in a centralized place. So not only do we get unification of the multiple streams inside of log groups, but we can also aggregate across log groups and then create pretty charts and different analytics and metrics based not only on hard numeric metrics that CloudWatch natively support. But also, we can create arbitrary metrics off of a query that might be derived from working over the JSON representation of different logs.</p>
<p>So next we’re going to do a little hands-on demonstration and try the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/elk-stack/">ELK stack</a> out by creating one inside the console.</p>
<h1 id="ELK-Stack"><a href="#ELK-Stack" class="headerlink" title="ELK Stack"></a>ELK Stack</h1><p>Welcome back to CloudAcademy’s course on Advanced <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/introduction-27/">Amazon Web Services Monitoring Metrics and Logging</a>. In this lecture, we’re going to be trying out the ELK stack which was one of our logs stream sinks that we had talked about. It’s a log aggregation service that let’s us aggregate across multiple CloudWatch log streams, any kind of events that we want to index into the system. It integrates nicely with the Amazon Web Services stack with the introduction recently of the AWS Elasticsearch service which runs the E in that service and also the K. And the Lambda ends up being our L for the indexing of the log.</p>
<p>Without further delay, let’s get started. So just a reminder of what we’re trying to build. If we look really quickly at this flow chart that I’m working with here, I’m using a flow chart software that we can take peeks at so we can make these diagrams. But effectively, I’ve already got some of this built out here. We’re going to be simulating a little API, but the main important piece here is that we’ll be emitting events into CloudWatch logs, pulling logs out, feeding them into a Lambda which then subsequently inserts them to this Elasticsearch service that’s actually running the E and the K in our ELK stack. Then we’ll go through this log inspector cycle here where we try out that fast, searchable unified graphical logs UI.</p>
<p>So I need to navigate over to my CloudWatch console first and we need to first see what these log events might look like. I’m actually going to use AWS Lambda to create some logs just because Lambda integrates with CloudWatch logs really nicely. It’s important to note that anything that can submit logs into CloudWatch logs will work for this part. This just happens to be lightweight, easy to test thing during a screen share.</p>
<p>Here we’re looking at some function code inside of a Lambda. It doesn’t really matter. The idea here though is that I’m running a test event where I’ve got some logs coming out of this output. Those might be a little small on your screen, but these logs should be visible now if we go look into CloudWatch logs. So I’m actually going to open these logs up. If we look at the raw log stream here, we can see that we have these function-loaded portions here and we can see that we have some events coming through. That’s all well and good.</p>
<p>Let’s go back to the Lambda console and rather than echoing, I’m now going to configure my test event and run a ping. We should see it come back with Pong here and if I go back to my log streams… …we can see that last execution that I did for the echo and eventually that pong will show up. There’s a little bit of latency there. So I won’t bore us to death waiting for it.</p>
<p>Here we’re navigating inside of the CloudWatch log group. These are interface here. We can navigate into individual streams which we were talking about in our previous lecture. Now in general this is helpful, but it’s not optimal because I might want also to be able to search for events at the log group level instead of just the log stream which as you can see all I can do is search by stream prefix here and I can only search by text once I navigate into this user interface for the individual streams.</p>
<p>Without further ado, let’s set up an ELK stacks. So you’d want to navigate over to the Elasticsearch service somehow if you’re looking in the all AWS Services, it’s alphabetical. So we could just click up here. And once we click on our Elasticsearch services, we’ll navigate to this tab. I’ve already set this up so we can accelerate the pace of this video, but I’ll show you what it takes to set one of these domains up since it takes about 10 minutes usually.</p>
<p>So this is exactly how I set up that other domain. I just hit yes. All of these default values work. I’m just selecting size and the count of the instances that will join that Elasticsearch cluster. I’ll hit next. I would use open access to the domain for this demonstration just so we can easily see what’s going on. I hit next and then I would hit confirm and create. I actually did this step already to accelerate the demonstration here. So we actually have a complete cluster already here.</p>
<p>So if I click on that Kibana 4 user interface, we should see a loading screen like this. So for the first time that you do that load, it will take a while and you’ll see that little loading screen. We need to configure an index pattern. So we have index contains…  we want index contains time event. We’re actually not going to do this until I go and create some more log events that will be submitted into my system. Again, I’m going to run another test with my ping and my pong. May be able to see more events going up here. Not yet. Okay. So even though I didn’t get any events showing up from that one yet, what we can do is move back to the log groups. I actually want CloudAcademyDynamoLambda which I’ll copy from from up here. I can use that identifier to search for it. Then click on the radio button here and stream into Elasticsearch service.</p>
<p>So I only have one cluster left, I can create a new role to allow the Lambda that I’m pushing into to publish into Elasticsearch. So I’m granting it that ability to post into my cluster. So I hit allow. I want to move forward. Realize that my log format will be coming through in the AWS Lambda format. We can see that we have some sample events coming up here. Then after I’ve done all of my configurations, this filter pattern should be familiar if you were watching very carefully here for what my log formats look like on this system, and then I can start streaming.</p>
<p>So what this is doing is it’s setting up this portion here where we have this arrow going into a Lambda and this arrow going into the Elasticsearch service. So we just set that up. We’ll wait just a second to allow that to activate. We’re provided with the link to Kibana 4 but we already have it open. Another best thing that I can do now is to configure test events to actually run these operations and we’re actually going to demonstrate what happens if we have a broken Dynamo configuration here. So I can do something like setting the received event here. I’m actually going to use a slightly different formatting scheme here, and allow my event to be logged in JSON formatting like that. So if I just log my event, then I can save and test. I should see operation ping and message “Hello World!” since I just added the console.log of my JSON and now I can see any invocation that comes into this Lambda in my log output. Now this log output will also show up if look at my log stream over here. So I’ve navigated back to my CloudWatch logs group and I can see that I’ve already got the JSON file or JSON line showing up inside here.</p>
<p>Now when we configured our ELK stack streaming service here when we did that. I’m going to reload so that we can see if those indices start showing up here and what we can do here is go directly to our cluster. We can run a search and see that we started having some events showing up here. So we’ve got some Kibana 4 indices. We’ve got some data showing up in the cluster now at the search engine end point. And if we scroll around here. We can see that my index that it set up is CloudWatchLogs- with the date that it occurs, with the type of the log stream names. Given that, we now know how we need to configure our index setting.</p>
<p>So I’m actually going to set this to cwl for CloudWatch Logs and set it to…use a time stamp field name of @timestamp. So this is the correct configuration for when we’re doing CloudWatch log segregation in Kibana. So once I hit create, I should see all of my metadata start showing up since we already acquired some events from running before, and that’s actually sufficient for us to begin using the discovery module here where I can see I start seeing all of my events that came through. Now, this is not particularly useful until we actually start creating some more logs. So let’s create an error and see what happens.</p>
<p>So I’m going to create an error by…if we can see my default when I switch cases if I select an operation that is unknown then I can get it to throw an error. So let’s do that. Going to set it up for an operation of many question marks, hit save and test, run that a couple times. We can see some very angry execution results with JSON representations of an error as well as the input value itself.</p>
<p>So if we think about this, this could be if somebody fuzzes your API and sends you some input that you don’t understand. Perhaps your API doesn’t support unicode. Rather than an intentional error like I’ve created here, this could easily be a 500 error or a 404 or some other problem that might organically occur where you want your API if this was a stream that’s was being logged to standard out even on an EC2 instance. Then we would also want that to show up on this end.</p>
<p>What happens if I just search? We can see a whole bunch more events because I ran the test a couple more times. We can see my older event that I ran that ping up here and then I sent some more events where I have question marks. We can see the incoming event itself and we can see some error message events. So, say for instance, I want to know how many times there were unrecognized operations. Then I can see that there were three events during which there were unrecognized operations. We can also see the frequency with which they occurred. They occurred three times very frequently there. So this is excellent if you’re trying to find specific values. Now you could also use this if you’re trying to debug specific customers.</p>
<p>So again, this is contrived, but say we also set a customer ID field and set it one through zero, and search for customer ID. So we can see that we have this customer ID string and once I do the search for it after I allowed some time for those events to propagate, we can see that whenever I want my customer ID to show up, we can search for just the customer ID and see these events show up. I can also go back and expand by different properties, see the messages that are coming through etc, etc. So this is what a log aggregation system look like.</p>
<p>Now, we can also do our log analytics. If I want to visualize, for instance, an area chart from a new search, if I want my X-axis to be a histogram or a date histogram on time stamp over an automatic interval and add a sub-aggregation where I could split the area and I want to go by terms inside of the field for say on event or it can do an application. We could see the different frequencies of terms as we look inside of a system. So I could see where different numbers or terms showed up, and I can actually further refine this and say “Okay, I only want to see a histogram of these events.”</p>
<p>So we can do all kinds of things like graphing on different fields and just generally perform magic. You can also add visualizations to different dashboards. You can set up any number of different things that you want based on these sub-aggregations. And of course, we have this excellent capability of doing searches.</p>
<p>There are commercial products that you can use that do the similar behaviors here, but we’re a big fan of open source and being able to do value-added automation on top of our system. So we can see this big, nice log stream here. We have the original streams and sources that it came from, the AWS account associated with it, and we can do free text search. So again, we can search for specific error types if we so please.</p>
<p>So this kind of thing is very helpful for if you’re trying to do a debugging session if a customer’s complaining. Then, we of course want to be able to see what the customer’s talking about by, for instance, searching for the error code that was dumped onto the page if they’re on the phone and you’re trying to do some kind of support with them. You can use this for any number of things. It’s very helpful and it was very easy because all I had to do was create one of these domains, run through, create some sort of events so that they show up in CloudWatch logs, go to the log group, and check that streaming box and send it to Elasticsearch granting the role to the Lambda to allow the post. So that’s it for ELK stack demonstration.</p>
<p>I hope to see you soon on the next lecture in which we’ll be doing a little bit of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/slack-chatops/">chatOps</a> in which we do another trivial solution where I make something pop up in my slack channel whenever a certain event occurs in my Amazon account.</p>
<h1 id="Slack-ChatOps"><a href="#Slack-ChatOps" class="headerlink" title="Slack ChatOps"></a>Slack ChatOps</h1><p>Welcome back to Cloud Academy’s course on Advanced <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/introduction-27/">Amazon Web Services Monitoring Metrics and Logging</a>. Today, we’re going to look at a demo of a ChatOps system, which is a system whereby we post operational messages or alerts into a chat system. I’ll be using Slack today, since I like their API. It’s nice and easy, and I already have a domain that I could use. So without further ado, let’s get started.</p>
<p>So the first thing that we need to realize is that we can reuse from the previous demo the same kind of logic that we were using to degenerate these unhandled errors here. So I’m going to continue using that same Lambda, simply because it’s already pre-integrated with CloudWatch, and I don’t have to do any additional work to get this thing to generate logs and submit them to CloudWatch. I wouldn’t have to install the daemon like I do on an EC2 instance.</p>
<p>So looking at the function, all we need to do is give it an operation value that does not meet any of these cases, and we’ll get this uncaught exception, which is what I did. I gave it this name “Fatal Operation”. So how do we get the logs from this thing to show up in a Slack chat whenever there’s an error or something? Well, it’s relatively simple. I have to create a Lambda function that will post into Slack. So one way that I can do that is by creating one of these Slack integrations. So if I go to my custom integrations, I could configure an incoming Webhook. All you have to do is click Yes, and click Add Configuration. I have this set up to post into AWS, and then I copy my Slack Webhook URL, which I’m not going to show you because then you could post into my account. Then, once we realize that I have an endpoint that I can post into Slack with, I need to create a Lambda function that will actually let me post into Slack. So let’s do “SlackChatOpsDemo2”, then enter some code in that will handle the stream <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-techniques-for-aws-monitoring-metrics-and-logging/events-everywhere/">events</a> that come out of CloudWatch.</p>
<p>So a couple things going on here, really only three. I receive the event, I un-Base64 the event, and I G-unZip the event. So CloudWatch Logs is going to try to save bandwidth by transporting all the events pre-GZipped and Base64-ed so they compress well. So all I’m doing here is decompressing and then parsing and re-stringifying in a nicer format for me to post into Slack. When I post into Slack, literally all it is is adding some marked down-style block code formatting, joining it with new lines, and then running an HTTPS request against my Slack, Hook, host and path.</p>
<p>So I need to then assign a role to the Lambda, which will allow it to both create new logs and read from log streams. So I’m going to create a new role policy and manually edit this one. There’s no out-of-the-box role for us to use for this kind of ChatOps system. But I can just give Lambda full access to logs for now, allow it to have access to those full sets for the logs, then up my timeout to about five seconds so we don’t have Slack issues, and create the function.</p>
<p>So the next thing I need to do is start streaming that data from a log group which corresponds to my Cloud Academy Dynamo Lambda, which is where I’m generating my error, so my Cloud Academy Dynamo Lambda. Stream that to AWS Lambda. If I can actually find my demonstration Lambda, that Lambda that we just created that does the G-unZipping and Base64 decoding, we want to strip it over there. Since our event generator again is this Cloud Academy Dynamo Lambda, we want to use the AWS Lambda format.</p>
<p>So we should now be streaming from our Dynamo Lambda. We’ll automatically create log lines that go into CloudWatch Logs at the top of the stream. So this is an inserting or publishing function into a stream that is our log stream. Then, use this other Lambda, this Slack ChatOps Demo 2 as my consumer. So I set that subscription up when I went over and checked this box, and started subscribing to that log stream with this other Lambda. So now we should expect to be able to run a test, have the “Fatal Operation” fail, check the actual logs, see that we have some fatal operations, which we’ll then subsequently post into Slack. So you can see that I have Slack showing up here. This is the un-Base64, un-GZipped message.</p>
<p>Again, we went from Lambda, which generates log event lines, sends them into CloudWatch Logs here. We then went to the CloudWatch Logs Group user interface, and went to our subscriptions and added a subscription filter to Lambda. This Lambda was the recipient Lambda, which has these events coming in. These events correspond to our CloudWatch Logs event data. The way that those are formatted and sent to us to save bandwidth are Base64 and GZip, so we had to undo those steps. We had to decode the Base64 and then G-unZip, and then simply publish to Slack via the API endpoint and, voila, we have our ChatOps system. We can see something terrible happened, and we have fatal operations, and now our entire team that’s on our chat system should be able to see a message like this.</p>
<p>So hopefully, you enjoyed seeing the practical way to implement a very simple ChatOps system using totally serverless technologies, as well as CloudWatch log streams, and treating logs as a first-class citizen for insight and automation. ChatOps, which is one of these things, is a very simple way for us to alert people on our team when certain log events that are scary or frequent, or whatever other metric we want to us, publish into Slack and notify the entire team.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:28" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:28-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:03:56" itemprop="dateModified" datetime="2022-11-19T23:03:56-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Monitor-Amazon-CloudWatch-Security-Logs-for-failed-SSH-attempts-28/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/" class="post-title-link" itemprop="url">AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:27" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:27-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:54:50" itemprop="dateModified" datetime="2022-11-19T22:54:50-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-2-of-2-27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Resources-Referenced"><a href="#Resources-Referenced" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/">How to implement &amp; Enable Logging Across AWS Services (Part 1 of 2)</a></p>
<h2 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to the second part of this two-part series of courses which have been designed to help you understand how <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> performs logging for a number of key services and how to use this data captured by the logs to resolve instance and identify security threats. If you haven’t already taken part one of the series, then you can use the link on the screen.</p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy, specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the detail shown on the screen. Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#117;&#112;&#x70;&#111;&#114;&#116;&#x40;&#99;&#108;&#111;&#117;&#x64;&#x61;&#x63;&#x61;&#100;&#101;&#109;&#x79;&#46;&#99;&#x6f;&#x6d;">&#115;&#117;&#112;&#x70;&#111;&#114;&#116;&#x40;&#99;&#108;&#111;&#117;&#x64;&#x61;&#x63;&#x61;&#100;&#101;&#109;&#x79;&#46;&#99;&#x6f;&#x6d;</a> where one of our cloud experts will reply to your question. </p>
<p>The focus of this two-part series is to understand the logging process and how to monitor this data to your organization’s benefit from both an operational and security perspective. As a result, those who have the following or similar roles would benefit from this content: cloud security engineers, cloud security architects, cloud administrators, cloud support and operations, and compliance managers. </p>
<p>As this is part two in the series, the content will continue the theme of logging across AWS services by explaining the following: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/cloudfront-access-logs/">CloudFront Logs</a>. Here I’ll look at how to log the requests from each user requesting access to your website and distribution. Next, I look at VPC Flow Logs. And this lecture focuses on how to log the network data, traversing your network interface cards within your VPC. Next, I focus on AWS Config Logging, and here I look at how AWS Config provides a timeline of changes against your AWS resources. And then lastly, I look at filtering and searching of log data. And within this lecture, I look at how to use Amazon Athena to query logs being stored on S3. </p>
<p>For information, part one of this series dived into the following: the benefits of logging, and in this lecture I focused on the core principle of why logging is important. I also looked at CloudWatch Logs, and within that lecture I explained how to implement logging using CloudWatch Logs and the associated agent. I also touched on CloudTrail logging, and CloudTrail records all API calls so here I explained how you can use these logs and how they are constructed. I then looked at the monitoring of those CloudTrail Logs, and here I looked at how you can use CloudWatch to monitor CloudTrail events. And finally in part one, I looked at S3 Access Logs, where this lecture focuses on the logging capabilities of S3 buckets. </p>
<p>The objectives of this series is to enable you to understand when and why you should enable logging of key services, how to configure logging to enhance incident resolution and security analysis, and you’ll understand how to extract specific data from logging data sets. This is an advanced level course series, and so you should be familiar with the following services and understand the individual use cases and feature sets. Throughout this series, I will reference a number of URL links which will help and direct you to related information on specific topics. To make these links easily available to you, I have included them at the top of the transcript within the lecture the they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#115;&#x75;&#x70;&#112;&#x6f;&#114;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#x75;&#x64;&#x61;&#x63;&#97;&#x64;&#101;&#x6d;&#x79;&#46;&#99;&#111;&#x6d;">&#115;&#x75;&#x70;&#112;&#x6f;&#114;&#x74;&#x40;&#x63;&#x6c;&#x6f;&#x75;&#x64;&#x61;&#x63;&#97;&#x64;&#101;&#x6d;&#x79;&#46;&#99;&#111;&#x6d;</a>.</p>
<h1 id="CloudFront-Access-Logs"><a href="#CloudFront-Access-Logs" class="headerlink" title="CloudFront Access Logs"></a>CloudFront Access Logs</h1><h2 id="Resources-Referenced-1"><a href="#Resources-Referenced-1" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">Web distribution log file format</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">RTMP distribution log file format</a></p>
<h2 id="Transcript-1"><a href="#Transcript-1" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello, and welcome to this lecture focusing on the access logs generated by Amazon CloudFront. Amazon CloudFront is AWS’s content delivery network that speeds up distribution of your static and dynamic content through its worldwide network of edge locations. When you use a request content that you’re hosting through Amazon CloudFront, the request is routed to the closest edge location which provides it the lowest latency to deliver the best performance. When CloudFront access logs are enabled you can record the request from each user requesting access to your website and distribution. As with S3 access logs, these logs are also stored on Amazon S3 for durable and persistent storage. There are no charges for enabling logging itself, however, as the logs are stored in S3 you will be stored for the storage used by S3. </p>
<p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/introduction/">logging</a> process takes place at the edge location and on a per-distribution basis, meaning that there will not be data written to a log that belongs to more than one distribution. For example, distribution a, b, c, will be saved in a different log to that of distribution d, e, f. When multiple edge locations are used for the same distribution, a single log file is generated for that distribution and all edge locations write to the single file. </p>
<p>The log files capture data over a period of time and depending on the amount of requests that are received by Amazon CloudFront for that distribution will depend on the amount of log fils that are generated. It’s important to know that these log files are not created or written to on S3. S3 is simply where they are delivered to once the log file is full. Amazon CloudFront retains these logs until they are ready to be delivered to S3. Again, depending on the size of these log files this delivery can take between one and 24 hours. </p>
<p>When these log files are delivered they use a standard naming convention as follows. So let’s say for example you had the following settings. The bucket name was access-logs, the prefix was web-app-a, and you had the following distribution ID. Then your name and convention for the log would look something like this. Let me now show you a very simple demonstration on how to enable log in for your CloudFront distribution. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>So setting up access logs for your CloudFront distributions is very simple and easy to do. So let’s go into CloudFront. I’ll just select an existing distribution here, and then if you click on distribution settings and under the general tab you select edit, and then if we scroll down these settings here you’ll see a section where it starts referring to logging. And at the moment I have logging off. So to enable logging I simply click on on and then I select the bucket in S3 where I want the access logs to reside, so I’m going to select CloudFront Access Logs, which is an existing bucket I have set up for this. Now here I can add a log prefix if I want to, if I’ve got different distributions, etc. I’m just going to leave that as blank for this demonstration. And here we can have cooking logging on or off, which will log all cookie data within the request, and it’s as simple as that. And then once you’re happy with that you just click on yes to confirm your changes. And now any access requests that go via your CloudFront distribution will be logged via S3. And that’s it. </p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>To perform the demonstration that I just completed and to access the logs when they are stored, you will need specific permissions to the S3 bucket designated for logging. To enable the log in for your distribution, the user account activating that feature must have full control on the ACL for the S3 bucket, along with the S3 GetBucketAcl and S3 PutBucketAcl. The reason for this is that during the configuration process, CloudFront will use your credentials to add the AWS data-feeds account to the ACL with full control access. This is an account used by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> which will write the data to the log file and deliver it to your designated S3 login bucket. Therefore, if you’re trying to enable the login feature for your distribution and it’s failing, then you should check your access to ensure you have the required permissions. </p>
<p>Depending on the delivery type of your CloudFront distribution, either WEB or RTMP, the log output will vary. The number of fields within the log files differ between the two types. Web distributions have a total of 26 different fill types for each entry within the log, whereas the RTMP distributions only have 13. I won’t go through every single field explaining their purpose and use, however, I want to highlight a few points of interest starting with the web delivery type. These logs contain information which allow you to identify the following. The date and timestamp of the request of the user and which edge location received this request, source metadata of the requester including IP address details, HTTP access method of the request, such as PUT, DELETE, or GET, etc., the HTTP status codes of the request such as 200, the distribution domain name relating to the request, and the encryption and protocol data used in request such as SSL, V3, or AES256-SHA. For full information on each field and options please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">link</a>. </p>
<p>Now looking at the RTMP delivery type, the points of interest are as follows. Again, a timestamp of the request of the user and which edge location received this request, the source IP address of the requester, the event being carried out by the requester such as play, pause, or stop, and the URL of the page where your SWF file is linked to. Again, for full information on field data captured within RTMP logs you can view the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">here</a>. </p>
<p>One final feature of logging with CloudFront is cooking logging. If you enable this within your distribution, then CloudFront will include all cookie information with your CloudFront access log data. This is only recommended if your origin of your distribution points to anything other than S3 such as an EC2 instance as S3 does not process cookie data. </p>
<p>That now brings me to the end of this lecture covering AWS CloudFront logs. Coming up next I shall be looking at the logs generated at the network level within your VPC with the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC flow logs</a>.</p>
<h1 id="VPC-Flow-Logs"><a href="#VPC-Flow-Logs" class="headerlink" title="VPC Flow Logs"></a>VPC Flow Logs</h1><h2 id="Transcript-2"><a href="#Transcript-2" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture covering VPC Flow Logs. Within your VPC, you could potentially have hundreds or even thousands of resources all communicating between different subnets both public and private and also between different VPCs through VPC peering connections. VPC Flow Logs allows you to capture IP traffic information that flows between your network interfaces of your resources within your VPC. This data is useful for a number of reasons, largely to help you resolve incidents with network communication and traffic flow in addition to being used for security purposes to help spot traffic reaching a destination that should be prohibited. </p>
<p>Unlike S3 access logs and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/cloudfront-access-logs/">CloudFront access logs</a>, the log data generated by VPC Flow Logs is not stored in S3. Instead, the log data captured is sent to CloudWatch logs. Before creating your VPC Flow Logs, you should be aware of some of the limitations which might prevent you from implementing or configuring them. If you are running a VPC peered connection, then you’ll only be able to see flow logs of peered VPCs that are within the same account. Or if you are still running resources within the EC2-Classic environment, then unfortunately you are not able to retrieve information from their interfaces. And once a VPC Flow Log has been created, it cannot be changed. To alter the VPC Flow Log configuration, you need to delete it and then recreate a new one. </p>
<p>In addition to this, the following traffic is not monitored and captured by the logs. DHCP traffic within the VPC, traffic from instances destined for the Amazon DNS Server. However, if you decide to use and implement your own DNS Server within your environment, then the traffic to this will be logged and recorded within the VPC Flow Log. Any traffic destined to the IP address for the VPC default router and traffic to and from the following addresses, 169.254.169.254 which is used for gathering instance metadata, and 169.254.169.123 which is used for the Amazon Time Sync Service. Traffic relating to an Amazon Windows activation license from a Windows instance and finally the traffic between a network load balancer interface and an endpoint network interface. All other traffic both ingress and egress can be captured at a network IP level. </p>
<p>You can set up and create a flow log against three separate resources. These being a network interface on one of your instances, a subnet within your VPC, and your VPC itself. Obviously for option two and three, this will contain a number of different resources. As a result, data is captured for all network interfaces either within the subnet or the VPC respectively. I mentioned earlier that this data is then sent to CloudWatch logs via a CloudWatch log group. For every network interface that publishes data to the CloudWatch log group, it will use a different log stream. And within each of these streams, there will be the flow log event data that shows the content of the log entries. Each of these logs captures data during a window of approximately 10 to 15 minutes. </p>
<p>To enable your flow log data to be pushed to a CloudWatch log group, an IAM role is required for permissions to do so. This role is selected during the setup configuration of the VPC Flow Log. If your role does not have the required permissions, then your log data will not be delivered to the CloudWatch group. At a minimum, the following permissions must be associated to the role. In addition to this, you will also need to ensure that the VPC Flow Log service can assume that IAM role to perform the delivery of logs to CloudWatch. This can be achieved with the following permissions. </p>
<p>While on the topic of permissions, I want to also show you the required permissions for someone to review and access the VPC Flow Logs or indeed be able to create one in the first place. The following three EC2 permissions allows you to create, delete, and describe flow logs. These being ec2:CreateFlowLogs, ec2:DeleteFlowLogs, and ec2:DescribeFlowLogs. The logs:GetLogData permissions is used to enable you to list log events from a data stream. If you wanted to create flow logs, then you need to also grant the use of the IAM permission of iam:passrole which allows the service to assume the role mentioned previously to create these flow logs on your behalf. </p>
<p>Let me now show you how to create a flow log for an interface on an instance, a subnet, and lastly the VPC itself. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay so firstly I’m going to set up a VPC Flow Log for the running instance that we’ve used in a previous demonstration which was for the logging server. So what I need to do is go down to our network interfaces under network and security and select the ENI of the logging server. As you can see, it’s this bottom instance here. So if I select that interface, if I just drag this up a little bit, and we have three tabs here, details, flow logs, and tags. If we select the flow logs tab of this interface, we can see that there’s no flow log created as yet. </p>
<p>What we need to do is click on create flow log. Now we can select the filter for this flow log to only log either accepted requests or rejected requests so I’m going to select all so it gets accepted and rejected. We now need to select our role and I created a role earlier and I called that Flow-Logs-Role so that has the required permissions to push data to CloudWatch logs. And here we have the ARN of the role. The destination log group for CloudWatch, I set up a log group prior to this demonstration and I’ve just called this Flow-Logs. And then click on create flow log. And that’s it, it’s as simple as that. So now you can see for this eni interface here, we now have a flow log created. It gives it a flow log ID. Shows the filter which we have ALL here. Their destination log group. The ARN of the role and it’s currently active. So now any traffic going in and out of that interface on that EC2 instance will be captured and the data will be sent to the flow logs log group in CloudWatch. And let’s take a look at how you set up flow logs for a subnet. </p>
<p>So let’s go across to our VPC service. I have a couple of VPCs here and we’ll use our logging VPC. So if we go down to our subnets, and let’s select the public subnet for our logging VPC, now again we have the tabs for this subnet. We have the summary, route table, network ACL, etc, and we also again have the flow logs tab. Very simple process again. Click on create flow log. The same filters. Select the same role and the same log group. And then simply create flow log. And that’s now having the flow logs enabled on this particular subnet so all traffic going in and out of this subnet will be captured and sent to the flow logs log group. </p>
<p>And for the VPC, it’s very similar. So you simply select your VPC so we have our logging VPC here, again we have our flow logs tab. Create flow log. Select the role and the destination log group of flow logs and then create flow log and that’s it. So it’s very easy to set up your flow logs for your EC2 network interface clouds or your subnet or your entire VPC. And that’s it. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>Let’s now take a look at a record within one of these flow logs. When you access the logs, you will find each entry has the following syntax. These entries are defined as follows. Version, which is the version of the flow log itself. Account-id, this is your AWS account ID. Interface-id, this is the interface ID of which the log stream data applies to. Source address, this is the IP source address. Destination address, this is the IP destination address. Source port, this is the source port being used for the traffic. And the destination port is the destination port being used for the traffic. The protocol, this defines the protocol number being used for the traffic. Packets, this shows the total number of packets sent during the capture. Bytes, again this shows the total number of bytes sent during the capture. Start and end shows the timestamp of when the capture window started and finished. Action, this shows if the traffic was accepted or rejected by security groups and network access control lists. And the log-status shows the status of the logging through three different codes. OK, where data is being received by CloudWatch logs. NoData, this means there was no traffic to capture during the capture window. And SkipData, where some data within the log was captured due to an error. </p>
<p>One of the key fields from an incident response and troubleshooting perspective is the action field. For example, if you are troubleshooting an issue of traffic not being received by a particular resource, then you could check the VPC Flow Logs to see if the traffic is getting blocked at the subnet level by a network ACL. This will then allow you to review your entries within the NACL to make the changes that’s necessary from a security perspective. </p>
<h1 id="AWS-Config-Logging"><a href="#AWS-Config-Logging" class="headerlink" title="AWS Config Logging"></a>AWS Config Logging</h1><h2 id="Transcript-3"><a href="#Transcript-3" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture regarding AWS Config. AWS Config is a great security and compliance tool that integrates well with many other <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS services</a>. As you may or may not know, AWS Config can perform the following functions. It can capture resource changes, so any change to a resource supported by Config can be recorded, which will record what changed along with other useful metadata or held within a file known as the Configuration Item, the CI. It can act as a resource inventory. AWS Config can discover supportive resources running within your environment, allowing you to see data about that resource type. It can store configuration history for individual resources. The service will record and hold all existing changes that have happened against the resource, providing a useful history record of changes. It can provide a snapshot in time of current resource configurations where an entire snapshot of all supported resources within a region can be captured that will detail their current configurations with all related metadata. It can enable notifications of when a change has occurred on a resource. So SNS is used with AWS Config to capture a configuration’s stream of changes enabling you to process and analyze the changes to resources. It can provide information on who made the change and when through AWS CloudTrail Integration. AWS CloudTrail is used AWS Config to help you identify who made the change and when and with which API. It can enforce rules that checks the compliancy of your resource against specific controls. Predefined and custom rules can be configured within AWS Config, allowing you to check resources compliance against these rules. It can perform security analysis within your AWS environment. A number of security resources can be recorded and when this is coupled with rules relating to security such as encryption checks, this can become a powerful analysis tool. And finally it can provide relationship connectivity information between resources. The AWS Management Console provides a great relationship allowing you to quickly see and identify which resources are related to any other resource. For example, when looking at an EBS volume, you’ll be able to see which EC2 instance it is connected to. </p>
<p>From a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/introduction/">logging</a> perspective, the ability to store configuration history is highly valuable. So let me look into this a little further. The configuration history uses Configuration Items, CIs, to collate and produce a history of changes to a particular resource. This allows you to see the complete set of changes made to a resource over a set period of time. The information can be accessed either programmatically though the AWS CLI using the following command, or you can also specify the resource type. So for example, if you wanted to look at the configuration history for a subnet you could enter the following into the CLI. Or you could also access this history via the AWS Management Console. Additionally, AWS Config also sends a configuration history file for each resource type to an S3 bucket that is selected during the setup of AWS Config. This configuration file is typically delivered every six hours and it contains all CI changes for all resources of a particular type. For example, there’ll be one configuration history file covering six hours for all RDS DB instant changes in one region. A Configuration Item, or CI as it’s known, is a key component of AWS Config. It is comprised of a JSON file that holds the configuration information, relationship information, and other metadata as point in time snapshot view of a supported resource. All the information that AWS Config can record for resource is captured within the CI.</p>
<p> A CI is created every time a supported resource has a change made to its configuration in any way. In addition to recording the details of the affected resource, AWS Config will also record CIs for any directly related resources to ensure the change did not affect those resources, too. For example, if there was a rule change to a security group, perhaps additional rules were added with new ports. AWS Config will record all CI information for that resource. But it also gathers CI information for any instances that were part of that security group. These will then be sent to the configuration stream. As so much data is gathered within these CIs, it’s important we look at these in further detail. </p>
<p>So for every CI generated there will be five different sections. Firstly, metadata. This essentially contains details about the configuration item itself. So within this metadata we have both a version ID and a configuration ID, which uniquely identifies the CI. In addition to this, other information includes an MD5Hash that allows you to compare other CIs already recorded against the same resource as well as ensuring there are no duplications. And then we have the time of the capture and a state ID, which puts the CIs for a particular resource into an order of sequence. </p>
<p>Attributes, this holds common attribute information against the actual resource. Within this section we also have a resource ID and any key-value tags that are associated to the resource. The resource type is also listed. For example, if this was a CI for an EC2 instance, the resource types listed could be the network interface or the EIP for that EC2 instance. The Amazon Resource Name, the ARN, for the resource would also see shown along with the availability zone that the resource belonged to. Bear in mind that for services and resources that are not fixed to a particular availability zone, such as IAM, then this section would not be applicable. Lastly, the time that the resource was created would also be given. </p>
<p>Relationships, this holds information for any connected relationships that the resource may have. So within this section it would show a clear description of any relationship to other resources that this resource had. For example, if the CI was from EC2 instance the relationship instance may show the connection to a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC</a> along with a subnet that the EC2 instance resides in. </p>
<p>Current configuration. This will display the same information that would be generated if you were to perform or describe or list API call made by the AWS CLI. AWS Config uses the same API call to get the same information. So depending on the API call and resource, different information will be returned, which is resource specific. </p>
<p>Related events, and this relates to AWS CloudTrail. This will display the AWS CloudTrail event ID that is related to the change that triggered the creation of this CI. There is a new CI made for every change made against a resource. As a result, a different CloudTrail event ID will be created. This allows you to deep dive into who or what and when made the change that triggered the CI. A great feature allowing for some great analysis to be taken, specifically when this affects security resources. </p>
<p>As you can see, the configuration item is a fundamental aspect of AWS Config when recording changes and data made to a supported resource. The configuration history file is stored on the S3 bucket that was selected at the time of configuration and is used to store all the configuration history files that are generated for each resource type, which happens every six hours. If you have multiple AWS accounts you may want to aggregate your configuration history files into the same S3 bucket for your primary account. However, you’ll need to grant write access for this service principle, config.amazonaws.com, and your secondary accounts with write access to the S3 bucket in your primary account. Let me now demonstrate how to use the AWS Management Console to view configuration details and history within my account. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so I’ve just logged into my AWS account, and I’m going to go up to services and you can see the AWS Config is under management tools. So if we select config, and this is the first splash screen you’ll be presented with if you don’t have AWS Config started as yet. So, let’s get started by clicking on the get started button. And then there’s three steps to setting AWS Config up. </p>
<p>So the first step is configuring the settings. And this is where we essentially start the configuration recorder, by identifying what resources we’d like included in our AWS Config configuration. So starting from the top, we can either select all resources within this region, remember, AWS Config is region specific. So we can choose to record all the resources supported within this region, and currently I’m in the London region. And we can choose to include global services such as IAM, etc. But for this demonstration we’ll just choose to record all resources in this region. </p>
<p>Here you can select specific types if you’re required. So if I unticked this we can then go down to this dropdown box and select specific resource types that we’d like set up for this configuration. So you can be quite specific. Selecting the load balancer or different elements of IAM and again different elements of RDS, etc. So there’s a list that it can go through if you just want to record a specific resource type. But like I said for this demonstration I’m just going to select all resources. </p>
<p>Next we need to define our S3 bucket. And this is where our configuration history and snapshot files are stored. So we can ask AWS Config to just create a new bucket for us and it will prefix it with this bucket name here or we can choose an existing bucket from my account, by selecting the dropdown list or we can select a bucket from another account. So if you had multiple accounts you can send the configuration history files and snapshot files for all those accounts to a single primary AWS account in a single bucket. So for this example, though, for this demonstration I’m just going to ask AWS Config to create a new bucket for us. Config bucket Cloud Academy. </p>
<p>Now moving down to the SNS topic. Now this is the configuration stream. So for any events that AWS Config picks up it will send it to this SNS topic for the configuration stream. Now we can create a new topic, and again, AWS Config will just give us topic name here or we can select to choose another topic from your account or again from another account. For this demonstration we’ll just ask AWS Config to create the topic name for us, and I’ll just add in Cloud Academy. </p>
<p>Now finally we have the AWS Config role, and this is where we spoke about the different permissions. And this is required to allow AWS Config to send the configuration history file and snapshots to S3 as well as having access to send events to the SNS topic. I’m not forgetting enabling AWS Config to be able to kind of go out and poll your resources as a describe or list API call to get the details of those resources. And again, for this demonstration, I’m just going to ask AWS Config to create a role for us. I’ll just add Cloud Academy on the end. So that’s settings. So this screen is essentially your configuration recorder. By setting this up you’re asking AWS Config to start recording. We have set up the S3 bucket to allow our configuration history files and our snapshots files to be captured. And we have created our configuration stream by configuring an SNS topic. And we have also allowed AWS Config to have the necessary permissions to carry out those functions. So once that’s all set up we’ll click on next. </p>
<p>And here we have a set of predefined AWS managed config roles. This is a number of templates that you can select to check the compliance of your resources against these rules. So for now I’m just going to click on skip. And here we’re on the final stage which is review. So we can see here the resource types we’ve selected, which are all resources, and we’ve not chosen to include global resources. We have our S3 bucket set up for our history files and snapshots and we have our SNS topic for our configuration stream configured, and we have the permissions given by the AWS Config role. And that’s essentially the elements to setting up AWS Config. So once you’re happy with that and once you’re done with that you can click on confirm, and you can see that it’s now setting up AWS Config for this region. And that’s it. It’s set up and it’s running. </p>
<p>So whatever create, deletes, or changes I make to support the resource types within this region AWS Config will not begin to monitor that and record it and log it within the configuration stream and also within the configuration history files. Now what I’ve done, I’ve already set this up in another region, and I’ve made a few changes. So what I’ll do now, I’ll swap to that region, and we can take a look at some of the other relevants such as the configuration history file, the timeline of events, etc., and look at how the relationships are set up within this dashboard. So let me swap over to another region which is island. </p>
<p>So as you can see, I’ve just swapped to the Ireland region, and we don’t have the option of setting up AWS Config because I’ve already set it up and only have it running once within the region. So let’s take a look at what we have here. Straight away here we can see that I’ve had existing rules. I had a rule name of S3 bucket versioning enabled. So I wanted to check if my S3 buckets had versioning enabled. So this is one of the config rules. So if I just open that up I can see that a number of these buckets do not have versioning enabled. And they have been marked as noncompliant. I can still use these buckets. I can still write to the buckets. I can still delete objects from those buckets. It’s just notified me that these buckets are noncompliant with this specific config rule, which is to check if, it says it checks whether versioning is enabled for your S3 buckets. </p>
<p>Now let’s have a look at some of the other resources that we have. So I can filter on specific resource type that I want to take a look at. So let me take a look at my VPC. Let me look up everything to do with my VPC. So I can see here that I have two VPCs, and let’s take a look at one of them. And this has now taken us to the timeline of events of these VPCs. So on the 15th of March at 11:25 there was a resource discovery. So that’s probably when I activated AWS Config within this region and it went out and done a discovery of all resources. And then at 11:32 there was a change on this resource item. So lets take a look at the rest of the page. So under the configuration details we have a number of details here. We have the Amazon Resource Name, the resource type, the ID, the CIDR block of the VPC. So there’s different configuration details that we can see for different items. </p>
<p>Now we can also see the tag name here, which is CA demo and further down we can look at the relationships. So this will show other resources that have a direct relationship with this VPC. So we can see we have a couple of network ACLs there. We have an instant gateway, we’ve got some root tables, security groups, and some subnets. So each of these resource types has a direct relationship with this VPC, and if I wanted to I can select straight from here to click on that network ACL and it will take me to the details for that ACL. And again, we have some resource type information here, the ARN, and the ID, etc. Again, if we click on the relationships we should see the VPC that we just came from. So let’s go back to our VPC. So as you can see it’s very easy to look at the relationships between different resources and navigate between them, and it’s kind of grouped in a logical manner to allow you to quickly get between the different resource types that you’d like to. </p>
<p>Now we can see up here that on the 15th of March 11:23 there was a change. Now if we go down to our changes here, we can have a look at what that change was. We can see it’s defined by two different categories, a configuration change or a relationship change. We can see that by the number that this change was to do with relationships. And it looks as though a new subnet was added to this VPC. And then finally, we have our CloudTrail events. And this will capture any API calls that made changes to resources. However, it will only capture these for the previous seven days. So because this change was made on the 15th of March and it is now the 30th of March, so if we look at the CloudTrail events there’s nothing there, they have expired. So what I’ll do, I’ll go and make a couple of changes. I’ll create a new subnet within this VPC, so I’ll pause the video, wait a few minutes, and then I’ll start it again and we can analyze the CloudTrail event of that new change. </p>
<p>Okay, so I’ve gone off and I’ve created a new subnet within this VPC so we can see that the 30th of March, which is today, that there was a change that occurred and two new events. So let’s go ahead and take a look at the change. So we can see that there is a new subnet. This is the subnet that I just created. And if we look at the CloudTrail events, we can see that we have the creation of the new subnet there. Now if we click on the actual CloudTrail event itself it will take us to CloudTrail and we can look at additional information. So now we’re looking at the details of the API call itself, and we can see a number of different information here. We can see the user that created it, the source IP address, the time, the events source, and also the region as well, along with any affected resources as well. </p>
<p>So here we can drill down into exactly what time it occurred. As you can see up here the date and time and by who and what event actually was created. So as you can see, clicking on that CloudTrail event you can get additional information to kind of help you with resolving instance and looking at potential security breaches to try and gather more information as to identifying who is doing what and when. So now let’s take a look at the configuration history, which is stored in S3. So let’s go across to S3. So if I go to the bucket that I use for the ireland region, and then navigate through the folders to the correct date and time of today, you can see the configuration history folder. And then if I download one of these files and then open that, we can see here that that configuration history file contained the subnet creation that we just created for our VPC, and it shows you all the different information about it, the availability zone, the CIDR block used, etc., etc. And there’s the subnet ID. And it also highlights the relationships to other resources for that new subnet such as any network ACLs that may be associated. So that’s the configuration history file that’s stored in S3 in a JSON format. That brings us to the end of this demo. So we look to how to set up AWS Config for a region. We then looked at the details of a specific resource type. We looked at the VPC and how the timeline of events work. We looked at a couple of changes and also the CloudTrail event logs as well. And then finally we looked at one of the configuration history files as a JSON document. That now brings me to the end of this lecture covering the AWS Config service and the config history login data.</p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="Filter-and-Query-data-with-Amazon-Athena"><a href="#Filter-and-Query-data-with-Amazon-Athena" class="headerlink" title="Filter and Query data with Amazon Athena"></a>Filter and Query data with Amazon Athena</h1><h2 id="Transcript-4"><a href="#Transcript-4" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture. We will learn how to analyze and search for specific data across log files being stored in S3. If your log data is being stored in S3, such as your CloudTrail logs, then you can use Amazon Athena to query that data within S3 to search for specific entries. The following demonstration has been created by Jeremy Cook, one of our AWS expert trainers here at Cloud Academy. In this demonstration, Jeremy will walk through the steps required to set up Athena to allow you to query CloudTrail log data.</p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>In this demo we’ll walk through the steps required to set up Athena to allow us to query CloudTrail log data. This demo will involve configuring CloudTrail, S3, EC2, and the Athena services. The end result will allow us to perform SQL queries against CloudTrail data stored in an S3 bucket. This type of setup will aid your DevOps and SecOps experience when building on top of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS platform</a>. </p>
<p>Let’s get started. Within the AWS console, select the Athena service. The first step involves us creating a new Athena database, which will host our custom data table created later in this demo. This task can be accomplished either through an Athena provided wizard or manually through the query editor using an appropriate SQL create statement. We’ll perform this by using SQL statements. But for learning purposes, I will show you where the wizard resides within the Athena console. Let’s see where this is located. Clicking the catalog manager, menu item in the Athena menu bar, takes us into the catalog manager. Initially, we have only the default Athena database to work with. We can click the Add Table action. This opens the Add Table Wizard. The first step requires us to either choose an existing database or create a new database. In this case, I’m going with the create new database option. I need to provide the name for the new database. In this case, I’m going with New DB. And then I need to provide the name for the new table. I’ll go with New Table. If I were to complete this wizard, I would additionally need to specify the S3 bucket location of our input set. However, I’m now going to cancel out of this wizard and perform the same process manually using SQL statements within the query editor. </p>
<p>Clicking back on the query editor menu item, we’re taken back into the query editor. Clicking within the editor itself clears the area. I’m now going to switch over to my visual code editor that has some pre baked SQL queries. I’m taking a copy here of a create database statement. Flipping back to the query editor, I paste the create database statement into the editor pane. As you can see, this particular create database statement will create as a new database named CloudTrail DB1. The create database statement, in this example, specifies an S3 bucket location. This bucket will be used to store the new database’s catalog. Let’s go and create this S3 bucket now, opening the S3 console in a separate browser tab. Clicking the Create Bucket button. In the resulting create bucket window, we paste in our bucket name that our create database statement references. We then hit the Create button at the bottom of the window. This creates our S3 bucket that will store the database catalog. </p>
<p>Okay, great. Our bucket has been created successfully as shown here. Next, we’ll go back to the Athena query editor. The create database statement can be configured to execute with metadata that may be relevant to your cause. In this example, I’m simply going to set the creator as myself, the company to be Cloud Academy, and created to the current year. With all this in place, we can now go ahead and execute the create database statement. We do so by clicking the Run Query button. Let’s do this now. We now wait for the query to complete. As we can now see, the create database statement has executed successfully, as per the query successful response in the result section. Additionally, we can see that we have our new database now displaying in the left hand side menu. We’ll now create our first table within our new database. </p>
<p>Let’s flip over to Visual Code and take a copy of a pre built table creation statement. Back within the query editor, we paste in the table creation statement. Here you can see that our new table will be named CloudTrail logs. The create table statement specifies all the relevant column attributes that CloudTrail tracks per order record. Next, we highlight the serializer-deserializer, or SERDE in short form, that is used. In this case, we’re using the Amazon Provided CloudTrail SERDE. Finally, I draw your attention to the S3 bucket location that needs to be provided. This particular S3 URL represents the location where our raw CloudTrail logs will reside once configured. Let’s go ahead and create this bucket. Switching back over into the S3 console, click the Create Bucket button. In the resulting Create Bucket window, we paste in our bucket name that our create table statement will reference. In our case, we’ll create our new bucket with the name CA CloudTrail Logs Demo. We then hit the Create button at the bottom of the window. This creates our S3 bucket that CloudTrail will be next to configure to push logs into, and for which Athena will scan from when executing our SQL queries. </p>
<p>Okay, great. Our bucket has been successfully created, as can be seen here. Switching back into the query editor window, we now paste in the S3 bucket name we just created. Okay, everything looks ready. Let’s now click on the Run Query button. And again, our query has executed successfully. And in this case, our new CloudTrail logs table has been created. On the left hand side, we see our newly created table listed. Clicking on the preview icon to the right of the table name executes the sample query now shown in the editor pane. This query will perform a Select All across the table, but limited to the first 10 rows. Since our CloudTrail bucket has yet to be populated, it’s expected that the query will return an empty result set as it does. Next, if we expand the table name itself, we see the column names and types that define it. Finally, clicking on the table properties icon, we are presented with a view of all the respected table properties associated with our new table. Important properties include, table name, database name, S3 bucket location, and serialization library. Let’s now go and establish a new CloudTrail trail and configure it to push its logs into our S3 CloudTrail bucket. </p>
<p>Under services, select the CloudTrail service. Once in the CloudTrail console, click the Create Trail button. Give the new trail a name. Here we’re gong to call ours CA CloudTrail Logs Demo. Leave all defaults as is until we get to the storage location section. Disable the create new S3 bucket option, and instead, select the name of the S3 CloudTrail bucket that we built earlier. Next, under Advanced, disable the enable log file validation option. This is unnecessary for this demo. Finally, click the Create button at the bottom of the screen. If all goes well, we should see fairly quickly our new trail has been provisioned successfully, as we do now. </p>
<p>Let’s switch over into the S3 console, and check to see if our newly created trail is publishing events into our bucket. Clicking on our CloudTrail configured bucket and drilling down into the lowest folder, we can see that we are indeed receiving logs from CloudTrail. This is great. Let’s go back into the Athena console and perform a couple of queries against this data. Clicking on the preview icon in the right of our CloudTrail table, kicks off the sample query for us again. And we’re now successfully seeing some early results coming through. Next, we’ll flip across to the Visual Code and copy a pre configured SQL select statement. Back within the query editor, we paste in the select statement. Before we execute, let’s click the Format Query button and have the editor reformat the query for us. This is a great feature that aids the readability of any SQL statements that we craft by hand. </p>
<p>Okay, running the formatted query still returns just four rows of data. This implies we’re still waiting for more CloudTrail logs to be delivered into our S3 bucket. Okay, now that we have all of the individual parts wired up successfully, let’s try out the following scenario. We’ll create a new example only security group within the EC2 service. The security group itself won’t be attached to anything. We’re creating it only to generate and capture the associated API calls within CloudTrail, for which, we will eventually query for within Athena. We’ll add in some inbound rules on this new security group. Performing these actions will generate CloudTrail data that will be published into our CloudTrail S3 bucket. The end result being that we should be able to query and discover these actions within Athena. Right, let’s start by heading over to the EC2 console. Click into the Security Group section, and then click the Create Security Group button. Give the security group a name. Here we name ours DemoSG. Don’t worry about setting the VPC. In the inbound rule section we’ll add a couple of rules. Clicking the Add Rule button, we add the first rule, allowing incoming traffic from source IP address 1.1.1.1&#x2F;32 and to port 1000. Add a second rule. This time to allow incoming traffic from source IP address 2.2.2.2&#x2F;32 and to port 2000, and then click the Create button. Next, we need to take a copy of the security group ID for the security group we just created. We’ll use this within our Athena query. </p>
<p>Jump back into the Athena query editor and update our like clause referencing the security group ID we just copied. This now tells Athena to search for all records who your request parameters attribute contains the security group ID we pasted into the like clause. Okay, let’s now execute this query and see if we get any results. As you can see, no results have come back, likely due to relatency involved in CloudTrail receiving, processing, and saving out to the S3 bucket. Let’s try again at approximately five minutes time. Okay, running the query again now provides us with results. As you can see, there are six rows in our output. Scrolling across the fourth row until we see the request parameters column. Here we can see two of the inbound rules we attached to the security group earlier. The first inbound rule allows incoming traffic to port 1000 from source IP address of 1.1.1.1&#x2F;32. And the second inbound rule allowing incoming traffic to port 2000 from source IP address of 2.2.2.2&#x2F;32. </p>
<p>Okay, let’s now expand our query by adding an additional clause. This time we’ll filter out all events except for the authorize security group ingress event. Running this query now gives us back just the one row, as expected. Okay, let’s now take a quick look at some of the other useful features within the Athena console. Each query that you author in the editing will be saved and replayed at a later stage. So I’ll save our current query. Click the Save As button. Give the saved query a name and description. In this example, we’ll call ours Important SG Query for both name and description. Click the Save button, and our query is saved and accessible in the saved queries area. Clicking on the Saved Queries top menu item shows us all of the previously saved queries, including our just saved Important SG Query at the bottom of the list. If we click on the query, it will be recalled back into the editor pane as can now be seen. </p>
<p>Next, let’s look at the history feature. This allows us to examine all past executed queries. Here we can see the most recent query at the top of the list. This was our last query that we ran, where we added the extra and the clause to filter on the event name column. If we click this query, once again, it’s recalled to the editor pane. But additionally, it also shows us the results that were returned at the time the query was actually executed. Bonus points. Going back into the history feature, I’ll now highlight a couple of the other important attributes for each captured query. Firstly, each query has a state associated with it. Here our query succeeded. If it hadn’t, it would track as an error. Next, there is a time the query took to run captured in seconds. This is useful for performance tuning and troubleshooting. Then there is the amount of data scanned recorded in kilobytes. This is useful to understand how much each query is going to cost you. Finally, there is a download results link that allows you to get a local copy of the results. Clicking the link for this row downloads the results locally. We’ll now use our local terminal to output the contents of the file to the screen. </p>
<p>Again, we can see that the details of the two inbound rules we attached to the security group in question. The first inbound allowed incoming traffic to port 1000 from source IP address of 1.1.1.1&#x2F;32. And the second inbound allowed incoming traffic to port 2000 from source IP address of 2.2.2.2&#x2F;32. This concludes the demo. But before we finish, let’s quickly go through the process of doing some clean up within Athena. Firstly, we’ll drop our CloudTrail table. Back within the query editor we type the statement, drop table CloudTrail Logs. Running this query will drop our table, allowing us to then drop the database. Next, clear the editor and type the statement, drop database CloudTrail DB1. And execute it. This will drop our custom Athena database. Don’t forget to delete the CloudTrail trail and remove the S3 buckets as used in this demo.</p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>That now brings me to the end of this lecture. Coming up <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/course-summary/">next</a>, I will summarize the key points taken from the previous lectures of this course.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><h2 id="Transcript-5"><a href="#Transcript-5" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this final lecture within <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/introduction/">this course</a>. In this lecture, I want to summarize and highlight the key points from the previous lectures. </p>
<p>I started by talking about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/cloudfront-access-logs/">Amazon CloudFront Access Logs</a>. During this lecture, we learned the following points. Amazon CloudFront is AWS’s content delivery network that speeds up distribution of your static and dynamic content through its worldwide network of edge locations. When a user requests content via CloudFront, the request is routed to the closest edge location providing the lowest latency. CloudFront Access Logs can record the request from each user requesting access to your website and distribution. And the logs are stored on S3 for durable and persistent storage. The logging process for CloudFront takes place at the edge location and on a per distribution basis. And the log files capture data over a period of time and are dependent on the amount of requests received. CloudFront retains the logs until they are ready to be delivered on S3. And to enable the logging for your distribution, you must have FULL_CONTROL on the ACL for the S3 Bucket, along with the s3:GetBucketAcl permission and the s3:PutBucketAcl permission. And log output varies on the distribution type, whether this be Web or RTMP. And by enabling cookie logging, it will include all cookie information with your CloudFront Access Log data. </p>
<p>Following CloudFront, I then looked at network logging at the VPC level through the use of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/vpc-flow-logs/">VPC Flow Logs</a>. In this lecture, I explained that VPC Flow Logs allow you to capture IP traffic information that flows between your network interfaces and your resources within your VPC. The log data generated by VPC Flow Logs is then sent to CloudWatch Logs. And if you are running VPC peered connections, then you’ll only be able to see Flow Logs of peered VPCs in the same account. Flow Logs are not available for EC2-Classic environments. And once a VPC Flow Log has been created, it can’t be changed. The following traffic is not monitored and captured by the logs: DHCP traffic within the VPC, traffic from instances destined for the Amazon DNS Server, any traffic destined to the IP address of the VPC default router, traffic to and from 169.254.169.254 and 169.254.169.123, traffic relating to an Amazon Windows activation license, and traffic between a Network Load Balancer Network Interface and an Endpoint Network Interface. Flow Logs can be set up for a network interface on an instance, your subnet, or your entire VPC. And each interface that sends data to CloudWatch will do so in its own stream. Specific permissions are needed to allow VPC Flow Logs to push data to CloudWatch, as well as permissions to assume the role with the required permissions, these being CreateLogGroup, CreateLogStream, PutLogEvents, DescribeLogGroups, and DescribeLogStreams. And the log files have the following syntax. </p>
<p>Next, I reviewed how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/aws-config-logging/">AWS Config</a> uses configuration history to display all changes within your environment. During this lecture, we learned that AWS Config is a great security and compliance tool that integrates well with many other AWS services. The Configuration History uses Configuration Items, CIs, to collate and produce a history of changes to a particular resource. AWS Config sends a Configuration History file for each resource type to an S3 Bucket that is selected during the setup of AWS Config. And this configuration file is typically delivered every six hours. A Configuration Item is comprised of a JSON file that holds the configuration information, relationship information, and other metadata as a point in time snapshot view of a supported resource. A CI is created every time a supported resource has a change made to its configuration. And a CI consists of the following sections: metadata, attributes, relationships, current configuration, and related events. And it’s important to note that you can aggregate Configuration History files from multiple accounts into one S3 Bucket. </p>
<p>The final lecture focused on how to retrieve data from your logs that are being stored on Amazon S3, and this was shown via a demonstration. </p>
<p>That now brings me to the end of this lecture and to the end of part two of this two-part course series. If you have now viewed both parts, you should now have a deeper understanding of some of the logging capabilities that AWS provides across a number of key services. You will also have an insight into how these logs are constructed and how to search for specific information that you might need to use in your day-to-day operations. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. </p>
<p>Thank you for your time, and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="1Introduction"><a href="#1Introduction" class="headerlink" title="1Introduction"></a>1<strong>Introduction</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/">How to implement &amp; Enable Logging Across AWS Services (Part 1 of 2)</a></p>
<h1 id="2CloudFront-Access-Logs"><a href="#2CloudFront-Access-Logs" class="headerlink" title="2CloudFront Access Logs"></a>2<strong>CloudFront Access Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#BasicDistributionFileFormat">Web distribution log file format</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#StreamingDistributionLogFileFormat">RTMP distribution log file format</a></p>
<h1 id="3VPC-Flow-Logs"><a href="#3VPC-Flow-Logs" class="headerlink" title="3VPC Flow Logs"></a>3<strong>VPC Flow Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/whats-new/2018/08/amazon-vpc-flow-logs-can-now-be-delivered-to-s3/">Flow Logs delivery to S3</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/" class="post-title-link" itemprop="url">AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:25" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:25-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:54:14" itemprop="dateModified" datetime="2022-11-19T22:54:14-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-How-to-Implement-Enable-Logging-Across-AWS-Services-Part-1-of-2-26/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to part one of this two part series of courses which has been designed to help you understand how <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> performs logging for a number of key services and how to use this data captured by the logs to resolve incidents and identify security threats. </p>
<p>Before we start I would like to introduce myself. My name is Stuart Scott. I am one of the trainers here at Cloud Academy specializing in AWS Amazon Web Services. Feel free to connect with me with any questions using the details shown on screen. Alternatively you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> where one of our cloud experts will reply to your question.</p>
<p>The focus of this two part series is to understand the login process and how to monitor this data to your organization’s benefit from both an operational and security perspective. As a result those who have the following or similar roles would benefit from this content: Cloud Security Engineer, Cloud Security Architect, Cloud Administrators, Cloud Support and Operations and Compliance Managers. </p>
<p>Part one of this series is constructed of the following lectures. The Benefits of Logging. This lecture focuses on the core principle of why logging is important. CloudWatch Logs. Here we’ll look at how to implement logging using CloudWatch Logs and the associated agent. Next I’ll look at CloudTrail Logging and CloudTrail records all API calls. So here I explain how you can use these logs and how they are constructed. Next we’ll be monitoring CloudTrail Logs, and here I look at how you can use CloudWatch to monitor CloudTrail events, and then finally in this course we look at S3 Access Logs, and this lecture focuses on the logging capabilities within S3 buckets. </p>
<p>Part two of this series will continue the theme of logging across AWS services by explaining the following. CloudFront Logs, and here we’ll look at how to log the request from each user requesting access to your website and distribution. Next I look at VPC Flow Logs, and this lecture focuses on how to look at the network data traversing your network interface cards within your VPC. Next is AWS Config Logging, and here I’ll look at how AWS Config provides a timeline of changes against your AWS resources. And finally in Part two, I look at Filtering and Searching of Log Data, and this lecture looks at how to use Amazon Athena to query logs being stored on S3. </p>
<p>The objectives of this series is to enable you to understand why and when you should enable logging of key services, how to configure logging to enhance incident resolution and security analysis, and you’ll understand how to extract specific data from logging data sets. </p>
<p>This is an advanced level course series, and so you should be familiar with the following services and understand the individual use cases and feature sets. Throughout this series I will reference a number of URL links which will help and direct you to related information on specific topics. To makes these links easily available to you I’ve included them at the top of the transcript within the lecture they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#x73;&#x75;&#112;&#112;&#111;&#x72;&#116;&#x40;&#x63;&#x6c;&#x6f;&#117;&#100;&#x61;&#99;&#97;&#x64;&#x65;&#109;&#x79;&#46;&#99;&#111;&#x6d;">&#x73;&#x75;&#112;&#112;&#111;&#x72;&#116;&#x40;&#x63;&#x6c;&#x6f;&#117;&#100;&#x61;&#99;&#97;&#x64;&#x65;&#109;&#x79;&#46;&#99;&#111;&#x6d;</a>. </p>
<p>That brings me to the end of this lecture. Coming up next I want to start off by looking at the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/benefits-logging/">benefits that logging brings</a> to your operational environment.</p>
<h1 id="The-Benefits-of-Logging"><a href="#The-Benefits-of-Logging" class="headerlink" title="The Benefits of Logging"></a>The Benefits of Logging</h1><p>Hello, and welcome to this short lecture, where I want to discuss a few of the different benefits that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> can bring you and your infrastructure. For some, many people consider logging an afterthought, something that is implemented after it’s too late. This is often the case where an incident or breach of security has occurred that resulted in a delay of resolution and safeguarding of your environment. In hindsight of these situations, logging would have been a great idea to have had running and implemented in the first place to rectify the event quickly and efficiently, or even prevent it from happening in the first place.</p>
<p>So how can logging help?</p>
<p>Generally, logs are created by services and applications which contain a huge amount of information, which is recorded and retained on persistent storage, to be reviewed and analyzed at any time that it might be needed. Some logs can be monitored in real time, allowing automatic responses to be carried out, depending on the data contents of the log. From an auditing perspective, these logs are invaluable. They often contain vast amounts of metadata, including date stamps, source information such as IP address or usernames, and this is especially true when you’re looking at CloudTail logs. These logs can be used to help you achieve specific compliance certifications that require evidence of traceable and auditable actions that have been carried out. </p>
<p>Being able to resolve an incident as quickly as possible is paramount within your organization. Whether it’s a priority one, two, or three, being able to gain as much insight into what happened just before and just after the incident can significantly reduce your time to resolution. Using logs to ascertain the state of your environment before and after and even during the incident provides clarity and enables you to detect where the incident occurred, allowing you to pinpoint your efforts in a specific area. Quicker resolution results in a better customer experience for your organization. </p>
<p>By monitoring the data within your logs, you’re able to quickly identify potential issues that you want to be made aware of as soon as they occur. By combining this monitoring of logs with thresholds and alerts, you are able to receive automatic notifications of potential issues, threats, and incidents, prior to them becoming a production issue. By logging what’s happening within your applications, network, and other cloud infrastructure, you are able to build a baseline of performance and establish what’s routine and what isn’t. By having this baseline, you are able to identify threats and anomalies easier through the use of third party tools and management services. </p>
<p>To have a thorough understanding of what’s happening within your infrastructure provides a huge benefit to your operational teams. Having an inside look of how your infrastructure is performing and communicating helps achieve the previous benefits that I’ve already discussed, and having more data about how your environment is running far outweighs the disadvantage of not having enough information, especially when it really matters to your business in the case of incidents and security breaches. </p>
<p>That now brings me to the end of this lecture. There are many more reasons as to why you should be capturing data that can be logged. But I just wanted to provide a few key points to you. </p>
<h1 id="CloudWatch-Logging-Agent"><a href="#CloudWatch-Logging-Agent" class="headerlink" title="CloudWatch Logging Agent"></a>CloudWatch Logging Agent</h1><p>Hello and welcome to this lecture where I shall explain what CloudWatch Logs are, how they work and how they are configured. As we know CloudWatch is a monitoring service that is used to collate and collect metrics on resources running on your AWS account allowing you to monitor their performance and respond to alerts that meet to find thresholds. In addition to this, Amazon CloudWatch is a powerful tool that allows you to collect logs of your applications and a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. </p>
<p>When data is fed into Cloudwatch Logs you are able to monitor the logstream in real time and set up metric filters to search for specific events that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. Let me explain the different components of this feature to give you a better understanding of how it all fits together. Starting with the Unified CloudWatch Agent. </p>
<p>With the installation of the Unified CloudWatch Agent you are able to collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. Interestingly this metric data is in addition to the D4EZ2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. The agent can be installed on a number of different operating systems and at the time of writing this course the operating systems versions supported are as follows. </p>
<p>To install the agent and configure it requires a number of different steps. To install the agent on your EC2 instances you need to perform the following, firstly you need to create a role and attach it to the instance with permissions allowing CloudWatch to collect data from the instances in addition to interacting with AWS systems manager SSM. You then need to download and install the agent onto the EC2 instance. And lastly configure and start the CloudWatch Agent. The most efficient way of completing this installation and configuration is with the use of the EC2 systems manager service known as SSM. You will need to create two roles. One role will be used to install the agent and also to send the additional metrics gathered to CloudWatch. The other role is used to communicate with the parameter store within SSM, to store a configuration file of the agent which then can be shared with other EC2 instances. </p>
<p>From a security perspective it’s best that only one of your EC2 instances has this permission to write to the parameter store. Once the agent configuration file is stored on SSM there is no need for other EC2 instances to do the same. In fact once the write has been completed, this role should be detached from the EC2 instance and the other role applied which simply allows the agent to send data to CloudWatch. </p>
<p>The role with the additional permissions for SSM needs to be configured as follows when creating a role. The options, ‘select the type of trusted identity’ needs to be ‘AWS service’. The option to ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. The ‘Attach Permissions Policies’ needs to be ‘CloudWatch Agent Admin Policy’ and the ‘Amazon EC2 role for SSM’. </p>
<p>The role that is simply used to install the agent and send data back to CloudWatch needs the following configuration, the ‘select type of trusted identity’ needs to be ‘AWS service’. The option ‘choose the service that will use this role’ needs to be ‘EC2 Allows EC2 instances to call AWS services on your behalf’. And finally under the ‘Attach Permissions Policies’ it needs to be ‘CloudWatch Agent Server Polic’y and ‘Amazon EC2 Role for SSM’. </p>
<p>Once your roles are created you can then associate the role shown in orange here, which I shall call ‘CloudWatch Agent Admin Role’ to your EC2 instance that will store the configuration file in the parameter store. </p>
<p>From the EC2 instance with additional permissions that will be saved in the configuration file with in the parameter store of SSM, you must then install the agent which can be done using systems manager or it can be downloaded from an S3 public link either for Linux or Windows. However as mentioned earlier I will explain how to do this via SSM in particular the run command function. As a prerequisite to the CloudWatch Agent installation you’ll need to verify that your EC2 instance has access to the internet to communicate with SSM and CloudWatch endpoints. In addition to this you must also have the SSM agent installed. For some AMI’s as stated on the screen the agent may already be installed. For more information on how to install or update your SSM agent on your EC2 instance please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">link</a>. Let me now perform a very quick demonstration to show you how to install the actual CloudWatch agent on an Amazon Linux EC2 instance using the SSM role command. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration 1"></a>Start of demonstration 1</h3><p>Okay so I’m within my AWS management console in the EC2 dashboard and as you can see I have an EC2 instance here named Logging Server and I have it in a VPC that has internet access. I have the SSM agent installed because it’s based on one of the latest Linux AMI’s comes by default with this, and I’ve also attached the CloudWatch Agent Admin Role which I discussed in the previous section so I’ve met all the prerequisites to install the CloudWatch Agent. </p>
<p>Now what I want to do is use the EC2 systems manager to install the CloudWatch Agent itself. So if I load up SSM which now has it’s own console and on the left hand side if I go down to the run command and click on run command to create a new command. What I want to select for the command document is the AWS configure AWS package. So if you just select the entry and then scroll down. And then next I need to select which EC2 instance that I want as the target. So which instance I’m going to install this package on and here we can see the Logging Server so I would just highlight that. And as you can see it’s added our EC2 instance already up there. </p>
<p>Now we come down to the command parameters and the action I want to perform is an install. The name of the package is Amazon CloudWatch Agent and I want to install the latest version. You can add some other comments here and some timeouts, for this demonstration I’m just going to leave those default. If you wanted to perform this same installation via the AWS CLI then you can click on the AWS command line interface command and then based on the above parameters you can simply cut and paste this command and run that command. But for this demonstration I’m going to use the run command within SSM. </p>
<p>So if I click on run we can see here that the command ID was successfully sent. It’s currently in progress and that will just take a moment to process and install the agent, then we should get a return of successful. So if I go back to the main dashboard of the run command we can see here that it was a success. This is the package that we just run using using this command ID, so now the agent is successfully installed. </p>
<p>As you can see here I have a previous command that failed so I just want to show you that worst on here. So if we select that command and go to view details we can drill down to understand why this failed. And if we scroll down to the bottom you can see here that it failed. Select the instance ID and view the output. Now step one, we can dive deeper to see why this failed. Now if we scroll down to the error itself and here it said it failed to retrieve the manifest and it could not find the latest version of this particular package. Now what happened here was I didn’t enter the correct package name. What I entered was CloudWatch Agent instead as we performed in the demonstration just now. The correct name is Amazon CloudWatch Agent. So it couldn’t find the package that was available and that was the reason why it failed. </p>
<p>However, going back to one that was successful, we can see here, we can drill down into this just to make sure everything was okay. Again here, success and we can view the output of this as well like we done with the failed one. And we can see here that it was all installed. It found all the files and it went through and processed everything okay. And that’s it, that’s how you install the CloudWatch Agent using the SSM run command. </p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration 1"></a>End of demonstration 1</h3><p>On your first instance you’ll need to create the CloudWatch Agent configuration file. Without doing so, you will not be able to start the agent. This file stores configuration parameters that specify which metrics and logs to capture on the instance which are then sent to CloudWatch. It can be created manually or by using a wizard. If you create it manually then you have a much wider scope for capturing elements that are not included within the wizard. Let me show you via another demonstration how to configure the agent using the wizard. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration 2"></a>Start of demonstration 2</h3><p>So I’m now on my Logging Server instance where we installed the agent and now need to configure it. So to configure it we’ll run this command here and this will launch the Amazon CloudWatch Agent configuration wizard. And it just goes through a number of questions before it completes so let’s take a look. So the first question it asks which operating system your using Linux or Windows, take the default choice of Linux. And if we’re going to try and collect logs on EC2 instances or On-Premises So it’s EC2 for us. </p>
<p>We then have a question if we want to monitor any host metrics such as CPU, memory, etc, I’m going to set a default of yes. And if we want to monitor CPU metrics per core and here additional CloudWatch charges may apply. For this demonstration I’m going to say no. We then have a question if we want to add any EC2 dimensions such as the instance ID or the instance type into our metrics if the information is available. Say yes. Then we have a question about how often CloudWatch will collect these metrics. Whether that’s one second, 10 seconds 30 seconds or 60. I’ll accept the default of every minute, 60 seconds and then asks which default metric config we want whether that’s basic, standard, advanced or none. I’ll accept the default of basic for this demonstration. It then just shows you an example of that configuration that you just selected. If you’re happy with that you can say one for yes or two for no. And then select a different default metrics config so I will just select yes for this demonstration, I’m happy with that. </p>
<p>Now it asks the question if we have an existing CloudWatch log agent configuration file that we want to import. At the moment we don’t but what we’re trying to do is get in the position of creating one to then import into the parameter store of SSM. So at this moment our default choice is no which is correct. It then asks if we want to monitor any log files on this EC2 instance. I’m going to say no just for this demonstration because all we are trying to do is configure the CloudWatch agent at the moment. Our next question asks if we want to store the config in the SSM parameter store, and here yes we do cause we want to upload this file to the parameter store to allow all other EC2 instances to connect to the SSM parameter store to download the configuration file to prevent us from doing this on each and every instance. So I’m going to say yes which is number one that we do want to store the config in the SSM parameter store. </p>
<p>It then asks what default name you want to give this configuration file. And it just gives you a little message there saying you should use the prefix of Amazon CloudWatch hyphen if you’re using any of the AWS managed policies. So I would go with their suggested name of Amazon CloudWatch Linux and you shouldn’t run into any issues there. It then asks you which region you want to store the config in the parameter store in and it gives a default choice. I’m just going accept that default. It then asks a question about which credentials should be used to send the config to the parameter store. I’m going accept the default credentials there which should have access and we now have a message that it successfully put the config to the parameter store Amazon CloudWatch-Linux. And that’s it, that’s the end of the wizard so it’s a very simple and quick and easy wizard. And now your CloudWatch agent configuration file is configured and it’s been uploaded to the parameter store so now any other EC2 instances can click to that parameter store and simply download it and have the agent running very quickly and easily. </p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration 2"></a>End of demonstration 2</h3><p>Now the configuration file is configured and successfully copied to the SSM parameter store I simply now need to start the agent and again I will use the systems manager service to complete this file with the run command. Let’s take a look. </p>
<h3 id="Start-of-demonstration-3"><a href="#Start-of-demonstration-3" class="headerlink" title="Start of demonstration 3"></a>Start of demonstration 3</h3><p>Okay so the final stage is to start the agent and again I’m going do this from the AWS systems manager so I’m at the systems manager dashboard. Again I’m going use the run command so so it’s over on the left hand side here. Click on run command. Then click on the orange run command button and this will allow us to start a new command for the command document. What we need to look for is the following, which is Amazon CloudWatch Manage Agent. And here we have the command document here. So I’m going to select that. </p>
<p>Scroll down, we then need to select our target instance and we have our Logging Server here. Now the command parameters, for the action we want to select configure rather than stop because we’re going to select the configuration file that we uploaded to the parameter store first using the EC2 mode rather than On-Premises. The optional configuration source which is SSM which is where we stored the configuration file so we’ll leave that as SSM. Now in the optional configuration location we need to enter the name of the file that we stored it as. And if you can remember that was Amazon CloudWatch-Linux. So we’ll paste that in. </p>
<p>Now under optional restart we want that as yes because it will then start the agent once it’s pulled the information from SSM. If we scroll down to the bottom simply click on run. And now we have it, the command ID was successfully sent. If we go back to our dashboard we can see that it was a success. And here the document name was the CloudWatch Manage Agent. So the CloudWatch Agent will now be running on that EC2 instance, the Logging Server. And it’s as simple as that. Now we have our logs configured and log data of our EC2 instance is being sent to CloudWatch along with the additional metric information. It’s now possible to search for specific entries within the logs for points of interest. </p>
<h3 id="End-of-demonstration-3"><a href="#End-of-demonstration-3" class="headerlink" title="End of demonstration 3"></a>End of demonstration 3</h3><p>That now brings me to the end of this lecture on CloudWatch logs where I explained how CloudWatch can be used to centralize login for more EC2 instances or applications running on your instances by determining logs past configured by the CloudWatch Agent and log groups within CloudWatch. Coming up next I should be talking about CloudTrail logs.</p>
<h1 id="CloudTrail-Logging"><a href="#CloudTrail-Logging" class="headerlink" title="CloudTrail Logging"></a>CloudTrail Logging</h1><p>Hello, and welcome to this lecture focusing on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging</a> capabilities and configuration of AWS CloudTrail. You should already be familiar with what CloudTrail is and what it does. However, to quickly summarize: It’s a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS Command Line Interface: CLI, from within the AWS Management Console, Or even from a request made by another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service. For example, when auto-scaling automatically sends an API request to launch or terminate an instance. These API requests are all recorded by CloudTrail. When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the new log file. </p>
<p>CloudTrail also records and associates other identifying metadata with all events. For example, the identity of the caller, which can be the user or the account that made the API call, the timestamp of when the request was initiated, and the source IP address. The logs generated are the output of the CloudTrail service and they hold all of the information relating to the API calls that have been captured. So as a result, it’s important to know what you can do with these logs in order to maximize the benefit of the data they contain. </p>
<p>So, what is a log file and what does it look like? Log files are written in a JSON format. Much like access policies within IAM and S3. Every time an API is captured, it’s associated with an event and written to a log. And new logs are created approximately every five minutes or so, but they are not delivered to a nominated S3 bucket for persistent storage for approximately 15 minutes after the API was called. So if you expect to see the log file for an API called seven minutes ago, then you may not see the log as expected for potentially another eight minutes. The log files are held by the CloudTrail service until final processing has been completed. Only then will it be delivered to S3, and optionally, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a>, depending on your configuration of the trail. </p>
<p>When an event reflecting an API call is written to a log, a number of attributes are also written to the same event capturing key data about that call. As you can see from this example. Without going through every attribute here, I just want to point out some of the more interesting ones. These being eventName. This refers to the name of the actual API that was called. EventSource. This refers to the service as to which the API called was made against. EventTime. This was the time that the call was made. SourceIPAddress. This displays the source IP address of the requester who made the API call. This is a great piece of information when trying isolate an attacker from a security perspective. UserAgent. This is the agent method that the request was made through. Example values of these are: Signin.amazonaws.com and this is what we have in our example and it simply means that user made this request from within the AWS management console. Console.amazonaws.com, this is the same as the previous, however, if this was displayed, it would mean that the request was made by the root user of the account, and lambda.amazonaws.com, this is fairly obvious, this would reflect that the request was made with AWS lambda. UserIdentity. This contains a larger set of attributes that provides information on the identity that made the API request. Once events have been written to the logs and then delivered and saved to S3, they are given a standard name and format, as shown. </p>
<p>The first three elements of this naming structure are self-explanatory. The AccountID, Name of the Service delivering the log, CloudTrail, and the region that it came from. The next part relates to the date and time. The year, months, and days. The T indicates the next part is the time reflecting hour and minutes. The Z simply means that the time is in UTC. The UniqueString value is a random 16 alphanumeric character string that is simply used by CloudTrail as a unique file identifier to ensure that it doesn’t get overwritten with the same name of another file. Currently, the FileNameFormat is defaulted to json.gz which is a compressed GZ version of a JSON text file. While we are looking at structures, let me also talk about the bucket structure way your logs are stored. </p>
<p>You may feel that the logs are all stored in one folder within your S3 bucket. However, there is a lengthy but very useful folder structure as follows: Firstly, you have your dedicated S3 BucketName that you selected during the creation of your Trail. Next, is the prefix that is also configured during Trail creation and is used to help you organize a folder structure for your logs corresponding to different Trails. Following this, is a fixed folder name of AWSLogs. Followed by the originating AWS account ID. Then another fixed folder name of CloudTrail indicating which service has delivered the logs. And after that, the RegionName of where the log file originated from. This is useful for when you have Trails that apply to multiple regions. The last three folders show the year, month and day that the log file was delivered. As you can see, although there are multiple folders underneath your nominated S3 bucket, it does provide an easy navigation method when looking for a specific log file.</p>
<p>This folder structure comes into even greater use if you have multiple AWS accounts delivering logs to the same S3 bucket. Some organizations may be using more than one AWS account, and having CloudTrail logs stored in different S3 buckets across multiple accounts can be inconvenient in certain circumstances and require additional administration to manage. Thankfully, AWS offers the ability to aggregate CloudTrail logs for multiple accounts into a single S3 bucket belonging to one of these accounts. This is why there is an accountID folder within your S3 bucket. Please note that you are unable to aggregate CloudTrail logs for multiple AWS accounts into CloudWatch logs that belongs to a single AWS account. </p>
<p>So to have all your logs from your accounts delivered to just one S3 bucket is a fairly simple process with the end result allowing you to essentially manage all your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/monitoring-cloudtrail-cloudwatch/">CloudTrail logs</a>. Let’s take a look at how this solution is configured. Firstly, you need to enable CloudTrail by creating a Trial in the AWS account that you want all log files to be delivered to. Permissions need to be applied to the destination S3 bucket allowing cross account access for CloudTrail. And once permissions have been applied to your policy, you need to edit the bucket policy and add an additional line for each AWS account requiring access. Then you need to create a new Trial in your other AWS accounts and select to use an existing S3 bucket for the log files. When prompted, add the bucket name used in step one and when alerted, accept the warning that you want to use a bucket from a different AWS account. An important point to make here when configuring the bucket selection is to ensure that you use the same prefix as the one you used when you configured the bucket in the first step. That is unless you intend to edit the bucket policy to allow CloudTrail to write to the location of a new prefix you wish to use. When you have configured your Trail, click create and your new Trail will now deliver it’s log files to the S3 bucket in your AWS account used in the first step. Again, this is a great solution that allows you to essentially manage all of your CloudTrail logs in one single account and S3 bucket. However, there may be uses such as system administrators who manage the other AWS accounts where the logs have come from that might need access to data within these logs. So how would they gain access to the S3 bucket to allow them only to access their CloudTrail logs that originated from their AWS account? </p>
<p>It could be done quite easily by configuring a few elements within IAM. Firstly, in the master account, IAM Roles would need to be created for each of the other AWS accounts requiring read access. Secondly, a policy would need to be assigned to those Roles allowing access to the relevant AWS account logs only. Lastly, users within the requesting AWS accounts would need to be able to assume this Role to gain read access for their CloudTrail logs. The easiest way to show you how to configure the permissions required is by a demonstration while I shall perform the following steps: I shall create a new Role. Apply a policy to this Role to only allow access for AWS account B’s folder in S3. Show the Trust Relationship between AWS account A and B. I will then create a new IAM user in account B. And create a Policy and apply the sts:AssumeRole permissions to this user allowing them to assume the new Role we created in account A. So let’s take a look at how and where we apply these permissions. </p>
<h3 id="Start-of-demonstration"><a href="#Start-of-demonstration" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so, as I just said the first thing we need to do is create a new Role in our primary account. So, if we go across to IAM, which is under Security, Identity and Compliance and then once that’s loaded, we need to go across to Roles and then Create New Role. So let’s give this Role a name. I’ll call it ‘Cross Account CloudTrail’ Click on Next Step. We then need to select a Role type, and what we want to do is select Role for Coss-Account access because we will allow users in another AWS account to access the log files in this primary AWS account. And this will set up the trust relationship between this account and then my secondary account. So, for that we will select this top option of providing access between AWS accounts you own. Then next, I’ll need to enter the secondary account ID that I want to create the trust relationship with. So I’ll just enter that number. </p>
<p>Okay, then after you have entered your account ID click on next step. And now we need to attach a Policy to this Role. Prior to this demo, I set up my own Policy and this allows cross-account access to read only from my secondary account to the bucket on this primary account. But I’ll explain this Policy in a few moments and I’ll show exactly what it contains. And from here click on next step and this is just a review of the Role. So we have the Role name, the ARN, the Amazon Resource name, the trusted entities. So this is the secondary account ID that I entered, and then the actual Policy and then the link that we can give to users in the secondary account to allow them to switch Roles. So, create Role. And there we go, the cross-account CloudTrail Role that we just created. So, let’s take a look at this.</p>
<p>Firstly, I’ll show the trust relationships. So, because we added a cross-account Role access and then we entered the secondary AWS account ID, we can see that this account is trusted by our primary account and that allows entities in this account to assume this Role. Now, I mentioned earlier that I previously set up a Policy with permissions in. So let’s take a look at that Policy. I named it Cross-Account read only for CloudTrail. So if I show the Policy, as you can see it. I made a very small Policy. Very simple. Now we have an effect of allow which will allow any S3:Get and any S3:List command, so essentially, read only access on this resource here specified by this line. Now this resource links to the bucket and folder where CloudTrail logs are delivered for our secondary account as you can see here. So, essentially, what this Policy does is allow read only access to any folders within the secondary account’s CloudTrail log folders. So this account won’t be able to access any other accounts CloudTrail logs, which is important. So, if we come out of this. So let’s just have a quick recap of what we’ve achieved so far.</p>
<p>So, so far, what we’ve done: We’ve created a Role in our primary account for our secondary account access. And we’ve also assigned an access Policy to this Role in order for the secondary AWS account to access the relevant folder in S3. So now what we need to do is assign a user in the secondary account and then apply the permissions to that user to enable them to assume the new Role in the primary account. So let’s go ahead and do that. </p>
<p>Okay, so I’ve now logged into the secondary account where I’ll need to create a new user and assign the correct permissions. So, to start with, I’m going to set up a permission Policy to assign to the user. So if I go down to Security, Identity and Compliance and select IAM. And then go across to Policies, and from here I want to create a new Policy. And I am going to create my own Policy, so I’m going to select the bottom option. I’m going to call this AssumeRoleforCloudTrail And description will be Assume role in primary AWS account. And for the Policy document, I’m going to paste in a Policy that I’ve already created. As you can see, it’s only a very small Policy again. And we have an allow effect that allows the Assume Role action from the security token service against the following resource, and this resource links back to a Role on our primary account where we created the Role cross-Account CloudTrail. </p>
<p>So this Policy will allow the user to assume this Role in the primary account. So let’s go ahead and create that Policy. Let’s validate it first. And then create. Now what we need to do is to assign a user to use that Policy. Now, I created a new user earlier prior to this demo. So, let’s just find our new Policy that we just created. And here it is at the bottom, AssumeRoleforCloudTrail. And I’m going to attach a user. And I’ve called our user CloudTrailUser1. And then attach Policy. And there we go. </p>
<p>So we now have one user attached to this Policy. So that’s all the actions and steps necessary to allow a user in a secondary account to access CloudTrail log files that have been delivered to an S3 bucket in a primary account. And it would do this by using the permission Policy that we just applied to that user to access the Role in the primary account. And that Role has a Policy attached that allows S3 read access to it’s own CloudTrail logs. </p>
<h3 id="End-of-demonstration"><a href="#End-of-demonstration" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><p>CloudTrail allows you to enable a feature called Log File Integrity Validation. Which simply allows you to verify that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. This is typically used for security and forensic investigations where by the integrity of the log files are critical to confirm that they have not been tampered with in any way. </p>
<p>When a log file is delivered to an S3 bucket a hash is created for it by CloudTrail. A hash file is a set of characters that are unique that are created from a data source. In this case, the log file. The hashing algorithms used by CloudTrail are SHA-256. In addition for a hash for every log file created, </p>
<p>CloudTrail creates a new file every hour, called a digest file, which is used to help verify your log files have not changed. The digest file contains details of all the logs delivered within the last hour along with a hash for each of them. These files are stored in the same bucket as the key pair. When it comes to verifying the integrity of your log files, the public key of the same key pair is used to programmatically check that the logs have not been tampered with in any way. Verification of the log files can be achieved via a programmatic access and not via the console. Using the AWS CLI, this can be checked by issuing the following command. The folder structure for the digest is very similar to the CloudTrail logs, as you can see. But the digest files are clearly distinguishable by the CloudTrail digest folder. </p>
<p>That has now taken me to the end of this lecture. Coming up next, I’ll explain how you can use CloudTrail and CloudWatch together as a monitoring solution.</p>
<h1 id="Monitoring-CloudTrail-with-CloudWatch"><a href="#Monitoring-CloudTrail-with-CloudWatch" class="headerlink" title="Monitoring CloudTrail with CloudWatch"></a>Monitoring CloudTrail with CloudWatch</h1><p>Hello and welcome to this lecture where we will look at how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudtrail-logging/">AWS CloudTrail</a> interacts with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/cloudwatch-logging-agent/">AWS CloudWatch</a> and SNS to create a monitoring solution. In addition to S3, the logs from CloudTrail can be sent to CloudWatch Logs, which allows metrics and thresholds to be configured, which in turn, can utilize SNS notifications for specific events relating to API activity. CloudWatch allows for any event created by CloudTrail to be monitored. This enables a whole host of security monitoring checks to be utilized. A great example of this is to be notified when certain API calls requesting significant changes to your security groups or network access control lists within your VPC. Other examples of these checks that are common within organizations are API calls relating to it starting, stopping, rebooting, and terminating EC2 instances. If instances are being created that shouldn’t be, then your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> cost could rise dramatically and quickly. Also, if instances are being rebooted or stopped, this could have a severe impact to your services if they are not configured in a high availability and resilient solution. Changes to security policies within IAM and S3. If changes are being made to your policies that shouldn’t be, access can be inadvertently removed for authorized users and access granted to unauthorized users, having a massive impact on operational services. Even a minor change to a policy can pave the way for an untrusted user to exploit the error. Failed login attempts to the Management Console. Monitoring failed attempts here can help to prevent unauthorized access at your environment’s front door. API calls that result in failed authorization. Not only does CloudTrail track successful API calls whereby the correct authorization was met by the authenticated identify, but it also tracks unsuccessful API requests, too, which would likely be due to the permissions applied. Special attention should be given to these unsuccessful attempts, as this could be a malicious user trying to gain access. However, it could also be a legitimate user trying to access a resource they should have access to for their role, but the incorrect permissions had been applied with their associated IAM policy. </p>
<p>To configure CloudTrail to use CloudWatch, you must first create a trail. Once your trail has been created, you can then configure it to use an existing CloudWatch Log group or have CloudTrail create a new one. Having CloudTrail create a new one for you is recommended if it is your first time doing this, as CloudTrail will take care of all of the necessary roles, permissions, and polices required. You may be wondering why roles and policy are required, so let me give you a high-level overview of the simple process that takes place when sending CloudTrail logs to CloudWatch. When a log file is created by CloudTrail, it is sent to your selected S3 bucket and your chosen CloudWatch Log group, assuming your trail has been configured for this feature. To allow CloudTrail to deliver these logs to CloudWatch, CloudTrail must have the correct permissions and these are gained by assuming a role with the relevant permissions needed to run two CloudWatch APIs. The first being CreateLogStream, and this enables CloudTrail to create a CloudWatch Logs log stream in the log group, and PutLogEvents, and this allows CloudTrail to deliver CloudTrail events to the CloudWatch Logs log stream. CloudWatch then delivers logs to the CloudWatch Logs. </p>
<p>When using the AWS Management Console, you can have the CloudTrail create this role for you, along with the correct policy. By default, the role is called CloudTrail_CloudWatchLogs_Role. For those that are curious, the policy for this role looks as shown. It’s important to point out that CloudWatch Log events have a size limitation of 256 kilobytes on the events that they can process. Therefore, any events that are larger than 256 kilobytes will not be sent to CloudWatch by CloudTrail. </p>
<p>Now that you have your logs with the associated events being sent to CloudWatch, you must then configure CloudWatch to perform analysis of your CloudTrail events within the log files. This is done by configuring and adding metric filters to the log within CloudWatch. These metric filters allow you to search and count a specific value or term within your events in your log file, which then allows for customizable thresholds to be applied against them. When creating these metric filters, you must create a filter pattern which determines what exactly you want CloudWatch to monitor and extract from your files. These filter patterns are usually fully customizable strings but as a result, a very specific pattern syntax is required. So, if you’re creating these for the first time, you must understand the correct syntax. </p>
<p>Just to reiterate what we have spoken about so far, I want to provide a demonstration on how to edit an existing trail to configure it to send logs to CloudWatch Logs. I will then configure a metric filter with the associated metric pattern, and finally, I will set up an SNS alert to notify me when a particular threshold is met. So, let’s take a look. </p>
<h3 id="Start-of-demonstration-1"><a href="#Start-of-demonstration-1" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so what I need to start with is going into CloudTrail to edit an existing trail to enable CloudWatch Logs. So, if I go down to Management Tools and click on CloudTrail and then across to Trails, as you can currently see, under CloudWatch Logs log group, there’s no log group selected. So, if we go into the trail and then scroll down to CloudWatch Logs, click on Configure, and there we can get CloudTrail to automatically set up this group and it’ll create the necessary roles and permissions, etc. So, let’s call this CloudTrail&#x2F;Demo and then click on Continue. So, we’ve given it a name and here it just gives a message to say that for CloudTrail to deliver events and logs to CloudWatch Logs, it needs to assume a role with permissions to run two API calls, which are these two here. And if we go down into the details, you can see that the IAM role that it’s going to use is the CloudTrail_CloudWatchLogs_Role and we’ll ask it to create a new policy. And here’s the policy document. </p>
<p>So, go down to Allow, and then if we scroll down to our CloudWatch Logs section, you can now see that we have a log group created in CloudWatch called CloudTrail&#x2F;Demo. So, if we now go across to CloudWatch and if we click on Logs on the left-hand side here, we can see that we have our log group that was just created by CloudTrail, and it’s CloudTrail&#x2F;Demo. Now, if we go into our log group and select it, you’ll see this log stream, which is the incoming stream of events being sent from CloudTrail. Now, as we’ve only just started, there’s only a few events coming in here, so you might want to wait a few minutes before setting up your metric filters to give you more of a test pattern to search on. So, what I might do is just leave it a couple of minutes for some more events to start streaming in before we set up our metric filters here, just so we have something to search on. </p>
<p>Okay, so I’ve left it a few minutes, so let’s go back into the log group and you can see we’ve now got a couple of streams, and if we go into these, we can see there’s a lot more events. So, if we go back a couple of pages, back to our log group, now we need to create our metric filters to allow us to define what we want to search on within our logs. So, if we select the tick next to our log group and then go up to Create Metric Filter, and here within the metric filter, we need to define a filter pattern. Now, as explained earlier, filter pattern will define what we’re actually searching for within our logs. So, for this example, I’ll keep it fairly simple. I’m going to search for any API call that’s been made from my machine, so from my IP address. So, for that, I need to enter the following command, ( $.sourceIPAddress equals 2.218.11.188, which is my IP address. And now we can test to make sure that that filter pattern’s okay using this Test Pattern box here, and what that does, that’ll run this test filter on some log data we see from this log here and the output of that log is in this box here. So, all we need to do is click on Test Pattern and we can see at the bottom here that it found 47 matches out of 50 events in the sample log. So, we know that the syntax is okay for this filter pattern, so I’m going to go ahead and assign this metric. </p>
<p>And we can see up here that we’ve got our filter name and our filter pattern, and I’m going to create a new name space for this metric and I’ll call it Demo, and the metric name will be IPAddress. And then what we need to do is click on Create Filter. Now, as you can see, our filter has been created and we have the details in this screen here. Now, what we can do at this point is create an SNS alarm so it could be notified if a certain threshold was met. So, let’s go ahead and do that. </p>
<p>So, the first thing that we need to do is add a name, so I’m going to call this SourceIPAddress and description will be Too many calls from my IP. Now, I’m going to set this to be 30. So, whenever my IP address is used as a source IP address that is greater or equal to 30 times for one consecutive period over five minutes, then I want it to set to a state of an alarm. And I want to be notified, so I’m going to enter a new list, give this a new topic, SourceIPAddressAlarm, and I want that to be sent to myself. So, as we can already see, with the current data it’s got that it has already breached the alarm, but it has gone back down below, so we’ll see how this goes and we’ll create the alarm. And this is a message just to say that I need to subscribe to that AWS notification, and I can do that in just a few moments. So, if we go across to our Alarms, we can see that we have our source IP address alarm in the state of OK. </p>
<p>So, at the minute, it’s currently below the 30 threshold. As soon as it goes above that, it will alarm and I will get a notification. Now, over the past few minutes, I’ve just been having some activity within the Management Console, and as we can now see, we do have an alarm on our alert. We can see that it just crossed the threshold, and so, I’ve received an email notification to say that it is now in a state of alarm. And if we take a quick look at that email, we can see here that it was crossed with a data point of 33 and the threshold was 30. So, that is how you set up CloudTrail to use CloudWatch with the inclusion of SNS to create alarms against API activity.</p>
<h3 id="End-of-demonstration-1"><a href="#End-of-demonstration-1" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="S3-Access-Logs"><a href="#S3-Access-Logs" class="headerlink" title="S3 Access Logs"></a>S3 Access Logs</h1><p>Hello, and welcome to this lecture, where I shall be looking at Amazon S3 access logs, what they are, and what they contain. As you may have guessed from the name, Amazon S3 access logs collate data based on who has been accessing a particular S3 bucket, and these logs record information, such as the source bucket that was accessed, a timestamp of the event, the identity requesting access to the object in the bucket, and the action that they were performing with the object. </p>
<p>By default, when you create a new bucket, access logging is not enabled. However, should you have a requirement to understand who is accessing your S3 buckets, then this logging can quickly and easily be enabled. The configuration of this process is based upon a source bucket and a target bucket. The source bucket is the bucket in which you want to log access requests for. The target bucket is the bucket in which the access logs will be delivered to. It’s best practice to use different buckets for both the source and the target for ease of management. When configuring your buckets for logging, you need to be aware that the source and target buckets need to be in the same region. </p>
<p>To allow S3 to write logs to this target bucket, it will of course require specific permissions. These permissions allow write access for the Log Delivery group, which is a pre-defined Amazon S3 group, which is used to deliver log files to your target buckets. If the configuration of your access logging is configured using the management console, then the setup process automatically adds the Log Delivery group to the ACL of the target bucket, allowing the relevant access. However, if you were to configure the access logging using the command line, then you would need to manually configure these permissions. </p>
<p>To set up the access logs using the console is a very simple process. Firstly, you select the S3 bucket that you would like to capture access logs for, select the properties tab, select server access logging, choose Enable Logging. From the dropdown, select your target bucket, and this is the bucket in which the logs will be delivered and saved to. Enter a prefix for your log files if required, and click save. If you then look at the ACL permissions for that bucket, you’ll notice that the log delivery group has automatically been given write access for that bucket. If you wanted to enable logging on the bucket programmatically, then you can do so using the S3 API or the AWS SDKs. When doing so, you need to configure the write access for the Log Delivery Group on the target bucket as an additional action. More information on how to perform these steps can be found using the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/enable-logging-programming.html">link</a>. </p>
<p>When viewing your logs within the target bucket, you’ll notice that each entry is made up of a number of different parameters. Let me show an example of a log access file to show you the data that makes up the entry. </p>
<h3 id="Start-of-demonstration-2"><a href="#Start-of-demonstration-2" class="headerlink" title="Start of demonstration"></a>Start of demonstration</h3><p>Okay, so I’ve just opened up one of the S3 access logs that I have, just a very small snippet. And we can see at the top here, we’ve got a couple of entries, so let’s just run through this top entry, just some of the key bits of information that make up the access log. This first entry here, and that’s the canonical ID of the owner of the source bucket. Next we have the AWS bucket itself that was accessed along with the date and time as well. Next we have some information from the requester, which is their source IP address. This hyphen means that the user was unauthenticated. If it was a user within IAM then we’d see their user ID there. This set of characters is generated by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> just as a unique ID for this request. Next we have the action that was carried out, which is a get object request against the object within the bucket which is this image file here. And again, we can see that here and also we have this response code of 200. Also we have some information here with regards to the amount of bytes sent and the object size, and also some timings in milliseconds as well for that request. We also have the referrer here of cloudacademy.com where the request initially came from, and then finally just some information regarding to the requester’s application software. So that’s a very quick summary and example of what an entry looks like within your AWS S3 access logs.</p>
<h3 id="End-of-demonstration-2"><a href="#End-of-demonstration-2" class="headerlink" title="End of demonstration"></a>End of demonstration</h3><h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Hello, and welcome to this final lecture within Part One of this two-part series relating to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/introduction/">logging in AWS</a>. In this lecture, I want to summarize and highlight the key points from the previous lectures.</p>
<p>I started off by talking about the benefits of logging to your organization. Within this lecture, we learnt that logging allows you to rectify incidents quicker and more efficiently, or even prevent the incident from happening in the first place. Also logs created by services and applications contain a huge amount of information which is then recorded and retained for later use. Some logs can be monitored in real-time allowing automatic responses to be carried out depending on the data contents of the log, and logs are invaluable from an auditing perspective as they contain vast amounts of metadata. Logs can also be used to help achieve compliance. Using logs to ascertain the state of your environment before and after and even during an incident enables you to detect where the incident occurred. And by combining the monitoring of logs with thresholds and alerts, you can configure automatic notifications of potential issues, threats and incidents prior to them becoming a production issue. Using logs, you can establish a baseline of performance allowing you to determine anomalies easier through the use of various third-party tools and management services. and finally, having more data about your environment and how its running far outweighs the disadvantages of not having enough information. </p>
<p>Following this lecture, I explained how CloudWatch Logs were configured and used. During this lecture, the following points were made. Amazon CloudWatch is a powerful tool that allows you to collect logs of your applications and a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. You are able to monitor the log stream in real-time and set up metric filters to search for specific events that you need to be alerted on or respond to. And CloudWatch Logs acts as a central repository for real-time monitoring of log data. The unified CloudWatfch agent allows you to collect logs and additional metric data from your EC2 instances and well as from on-premise servers. And to install the agent on your EC2 instances you need to create two roles. One role will be used to install the agent and also to send the additional metrics gathered to CloudWatch, and the other role is used to store a configuration information file in the parameter store within SSM. You then need to download and install the agent onto the EC2 instances using SSM and the Run Command and finally, configure and start the CloudWatch agent using a Wizard or the manual configuration. </p>
<p>I then looked at a different service, which was AWS CloudTrail, which records and tracks all API requests made within your AWS account. Within this lecture, I explain the following. When an API request is initiated, AWS CloudTrail captures a request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the log file and the logs generated are the output of the CloudTrail service. Log files are written in JSON, Javascript Object Notation format, much like access policies within IAM and S3. New logs are created approximately every five minutes but they are not delivered to the nominated S3 bucket for persistent storage for approximately 15 minutes after the API was called. The log files are held by the CloudTrail service until final processing has been completed. Log files are delivered to S3 and optionally CloudWatch Logs as well. And any logs that are delivered to S3 are given a standard naming convention of the following. AWS offers the ability to aggregate CloudTrail logs from multiple accounts into a single S3 bucket belonging to one of these accounts. But do be aware you are unable to aggregate CloudTrail Logs from multiple AWS accounts into CloudWatch Logs that belongs to a single AWS Account, and CloudTrails allows you to enable a feature called “Log file integrity validation,” which allows you to verity that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. And finally, when a log file is delivered to your S3 bucket, a hash is created for it by CloudTrail. </p>
<p>At this point, we had looked at both CloudWatch and CloudTrail. So the following lecture looked at how you could use CloudWatch to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/monitoring-cloudtrail-cloudwatch/">monitor CloudTrail Logs</a>. And here we learnt that the logs from CloudTrail can be sent to CloudWatch Logs allowing metrics and thresholds to be configured which, in turn, can utilize SNS notifications for specific events relating to API utility. CloudWatch allows for any event created by CloudTrail to be monitored and you can then configure your new and existing trails to use an existing CloudWatch Log Group, or have CloudTrail create a new one. To allow CloudTrail to deliver logs to CloudWatch, CloudTrail must have the following permissions given via role, and that’s the CreateLogStream permission and PutLogEvents. CloudWatch Log Events have a size limitation of 256KB on the events that they process and adding CloudWatch metric filters allows you to perform analysis of your CloudTrail events within the log files. And finally, metric filters allow you to search and count a specific value or term within your events in your log file. </p>
<p>The final lecture in Part 1 of this course series looked at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-1-2/s3-access-logs/">S3 Access Logs</a>. Within this lecture, the key points were as follows. Amazon S3 Access Logs collate data based on who has been accessing a particular S3 Bucket. And by default, when you create a new bucket, access logging is not enabled. S3 Access Logs are based upon a source bucket and a target bucket. The source bucket is the bucket in which you want to log access request for, and the target bucket is the bucket in which the access logs will be delivered to. It’s best practice to use different buckets for both the source and the target bucket for ease of management. And remember, the source and target buckets needs to be in the same region. During the configuration of access logs using the Management Console, permissions for right access for the log delivery group which is a predefined Amazon S3 group which is used to deliver log files to your target buckets. And if you wanted to enable logging on the bucket programmatically, then you can do so using the S3 API or the AWS SDKs. When doing so, you need to configure the write access for the Log Delivery Groups on the Target bucket as an additional action. </p>
<p>That now brings me to the end of this lecture and to the end of Part 1 of this course series. If you are ready to dive deeper into further AWS services relating to logging, including CloudFront Access Logs, VPC Flow Logs, AWS Config Logging, and how to filter data using Amazon Athena, then head over to Part Two, which can be found using the link on screen. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. </p>
<p>Thank you for your time, and good luck with your continued learning of cloud computing. Thank you.</p>
<p>URLs referenced during this course can be found below:</p>
<h1 id="3CloudWatch-Logging-Agent"><a href="#3CloudWatch-Logging-Agent" class="headerlink" title="3CloudWatch Logging Agent"></a>3<strong>CloudWatch Logging Agent</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">Metrics collected by the CloudWatch Agent</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent Linux</a></p>
<p><a target="_blank" rel="noopener" href="https://s3.amazonaws.com/amazoncloudwatch-agent/windows/amd64/latest/AmazonCloudWatchAgent.zip">Download the CloudWatch Agent for Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.htm">How to install&#x2F;update your SSM Agent</a></p>
<h1 id="6S3-Access-Logs"><a href="#6S3-Access-Logs" class="headerlink" title="6S3 Access Logs"></a>6<strong>S3 Access Logs</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/enable-logging-programming.html">Enabling Logging Programmatically</a></p>
<h1 id="7Course-Summary"><a href="#7Course-Summary" class="headerlink" title="7Course Summary"></a>7<strong>Course Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/how-implement-enable-logging-across-aws-services-part-2-2/">How to implement and enable logging across AWS services - Part 2 of 2</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Building-CloudWatch-Dashboards-25</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:23" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:23-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:57:40" itemprop="dateModified" datetime="2022-11-19T22:57:40-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Building-CloudWatch-Dashboards-25/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, my name is Will Meadows! Welcome to this course on Building CloudWatch dashboards. This course is geared towards helping you understand the value in building your own dashboards within CloudWatch, to give you unparalleled visibility into your architecture and dedicated systems.</p>
<p>If you have any questions about anything I cover in this lecture please let me know at <a href="mailto:will.meadows@cloudacademy.com">will.meadows@cloudacademy.com</a></p>
<p>Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> and one of our cloud experts will reply to your question, concern, or comment. </p>
<p>This lecture is perfect for someone who is looking to gain insight on how their infrastructure is performing through the use of easy to understand graphics, metrics, and tables.</p>
<p>After watching this course you will be able to create your own CloudWatch dashboard to monitor the items that are important to you, understand how CloudWatch dashboards can be shared across accounts, and understand the cost structure of CloudWatch dashboards and the limitations of the service.</p>
<p>This lecture will cover high-level topics related to CloudWatch and many other AWS services. Having a basic understanding of the most common AWS services (such as EC2, S3, and RDS) as well as a moderate level of experience with CloudWatch is recommended.</p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>Please note that, at the time of writing this content, all course information was accurate. AWS implements hundreds of updates every month as part of its ongoing drive to innovate and enhance its services.</p>
<p>As a result, minor discrepancies may appear in the course content over time. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. So, if you notice any information that is outdated, please contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. This will allow us to update the course during its next release cycle.</p>
<h1 id="CloudWatch-Dashboards"><a href="#CloudWatch-Dashboards" class="headerlink" title="CloudWatch Dashboards"></a>CloudWatch Dashboards</h1><p>CloudWatch dashboards are a fantastic way to visualize your data within AWS without having to dig into the nitty-gritty of each individual service.it allows you to quickly display key information at a glance giving you the ability to make decisions about your workload And your processes. These dashboards are created from individual widgets that You can combine together to create graphs and provide detailed information quickly about the topics you desire. even allows you to run queries within these widgets to display even more detailed and specific information. </p>
<p>CloudWatch also has automatic dashboards which are created for you by the service itself. These automatic dashboards work on a service by service basis and pick out some of the key components that you might be interested in. </p>
<p>For example, if you have any ec2 instance already running, there is probably an automatic dashboard that has been created to monitor your ec2 workloads.</p>
<p>I recommend you take a look at some of these automatically created dashboards as they give you a really good understanding of what is available with this service and the types of metrics you can harness to build the perfect dashboard for yourself.</p>
<p>There are two ways that you can create a dashboard. You can either do so visually through the editor or you can create dashboards programmatically and even use them inside cloud formation templates.</p>
<p>Both methods allow you to pick from many different media types called widgets. There are currently 8 flavors of these widgets and they are as follows:</p>
<ol>
<li>Line charts - A line chart is a type of chart which displays information as a series of data points connected by straight line segments. It is a basic type of chart common in many fields.</li>
<li>Stacked area chart -This type of chart compares the totals of many different subjects within the same graph</li>
<li>Number Widget - Allow you to instantly see the value for a certain metric that you’re particularly interested in - this could be as simple as displaying the current number of online instances.</li>
<li>Bar Charts - compares values of multiple types of data within the same graph.</li>
<li>Pie charts - Proportional data in direct relationship to other information fitted within a circle.</li>
<li>Text widget - which is free text with markdown formatting allowing you to add useful information to your dashboards as you see fit</li>
<li>Log tables - which explore results from log insights. Logs Insights enables you to interactively search and analyze your log data in Amazon CloudWatch</li>
<li>And finally, we have alarm statuses: in case you have an alarm set up that you’d like to know immediately if something is going wrong right on this dashboard</li>
</ol>
<p> One extremely cool feature of CloudWatch dashboards is they allow you to perform math on the metrics you want to display. So if you wanted to see how a graphed metric looked when applying normalization techniques or filters to your data you have the power to do so. </p>
<p>Additionally when working with dashboards are also allowed to aggregate data across multiple sources, like an auto scaling group for example, so if you were interested in seeing how the CPU load was handling overtime across your entire fleet you could create a dashboard that would display that. </p>
<p>Well by now you might be curious how to actually create your own dashboards. It is actually fairly painless to create dashboards with the visual dashboard creation tools provided by AWS within the CloudWatch console.</p>
<p>Creating dashboards in the editor is as simple as drag and dropping and adding new widgets onto a blank canvas. the editor allows you to pick any of the previously mentioned different types of media widgets and place them where you please. Pieces are rearrangeable and can be placed with as much finite controls as you desire. all widgets have a stretchable window view that you can position into specific sizes.</p>
<p>Dashboards can also be written as code giving you programmatic access to all the same information and tools. This means you can also put these code snippets inside cloud formation templates for easy dashboard creation on new accounts or projects. Creating these codified dashboards however is not as easy as it may sound at first. There is a lot of work that goes into testing and making sure your creation functions well.</p>
<p>Your dashboard code is written as a string in JSON formatting and can include anywhere between 0 to 100 separate widget objects. You have to specifically note down the x,y location of your widgets as well as the width and height of each element. That can be a little tedious to set up for the first time, but if you already have a functional blueprint, you can modify that fairly easily.</p>
<p>Here is an example of what your dashboards will look like when written out in code. This structure contains one metric widget ( this is the number widget displaying a metric ) and one text widget. The metric widget follows the CPU utilization of one instance with the id: i-012345.</p>
<p>And the text widget is just a simple title showing off the capabilities of the widget.</p>
<p>When you’re building your charts and after you have them completed you have the ability to add annotations to your graphs. This is Helpful for displaying when a certain event has taken place in the past which could help give other members of your team insight and exposure to certain peaks and valleys in your information. Just like writing good code requires comments it’s especially important to make sure your graphs and charts also have that advantage.</p>
<p>You can have both horizontal and vertical annotations in your graphs - each having their own purpose. For example, horizontal annotations can denote reasonable top and bottom bounds for a service’s CPU load while vertical annotations are great for noting when a specific event happened in the past.</p>
<p>You also have the ability to link to other dashboards within your own systems or even across accounts. These dashboards don’t have to be in the same region either. This is a very powerful tool that helps to centralize operations teams, DevOps, and other service owners who all need to have visibility into the status of your applications.</p>
<p>In order to allow cross-account and cross-region access, you need to enable it within the CloudWatch settings for your account as well as each of the accounts you wish to connect to. You can then link your accounts together, to share CloudWatch data between. These settings can also be activated within the AWS SDK and CLI.</p>
<p>Now, this sharing capability is not all or nothing, you have a few options:</p>
<ul>
<li>Share a single dashboard and designate specific email addresses and passwords of the people who can view the dashboard.</li>
<li>Share a single dashboard publicly, so that anyone who has the link can view the dashboard.</li>
<li>Share all the CloudWatch dashboards in your account and specify a third-party single sign-on (SSO) provider for dashboard access. All users who are members of this SSO provider’s list can access the dashboards in the account. To enable this, you integrate the SSO provider with Amazon Cognito.</li>
</ul>
<p>CloudWatch Dashboards allow you to have up to three dashboards - each containing up to 50 metrics at no charge. This is more than enough for anyone just practicing or having a few applications they want to monitor. For any more than that however, you will be charged $3 per month per new dashboard you wish to create. </p>
<p>For an enterprise company, that is not too much to spend. However If you are a solo developer or a small shop just starting off - those little 3 dollar charges can add up if you create dashboards willy nilly. So make sure you use your resources appropriately when building dashboards for your services.</p>
<p>Some Dashboard best practices:</p>
<ul>
<li>Use larger graphs for the most important display metrics - this may seem like a fairly obvious thing to do, but it’s important to keep in mind that humans are visual creatures. If you want them to pay attention to something, make it big and obvious.</li>
<li>Layout your dashboards and graph for the average minimum display resolution of your users. - this can help make sure that all relevant data is on screen at one time. This prevents users from missing key information that might be off-screen, which in the case of time-sensitive issues or events could be catastrophic. Most screens these days are able to handle 1920 by 1080 fairly well, however if you know your support staff all look at things on their phones, maybe you can design your dashboards around that instead.</li>
<li>Display time zones within your graphs for time-based data and if multiple operators are expected to be using the same dashboard simultaneously keep the time zone in UTC. This allows people to know at a glance, when an event took place. Its also important during an emergency that all users are working on the same premise in regards to the time the event happened, having to calculate differences in time zones can be frustrating when your customers’ satisfaction and your business is on the line.</li>
<li>Default your time interval and datapoint period to whatever is the most common use case.</li>
<li>Avoid plotting too many datapoints within your graphs. Having too much data can slow the dashboard loading time and might reduce the visibility of anomalies.</li>
<li>Annotate your graphs with the relevant alarm thresholds for your services. - this allows your users to understand at a glance if one of your services is about to go over its SLA times or when something terrible is about to happen. Having alarms is great, but never triggering them because you knew something was wrong ahead of time is way better.</li>
<li>Don’t assume your users will know what each metric means - be aggressive with tagging and having descriptions right in the dashboard using text widgets.</li>
</ul>
<h1 id="Wrap-Up"><a href="#Wrap-Up" class="headerlink" title="Wrap Up"></a>Wrap Up</h1><p>One of the biggest troubles when working with cloud architectures is understanding what’s going on behind the scenes. The technology is no longer built with physical devices that you can get your hands on so pulling information out of the system is paramount to success. </p>
<p>CloudWatch dashboards gives you visibility into what your resources are doing. being able to effectively use CloudWatch dashboards will give you such incredible insight into your architectures that not doing so might even be considered detrimental. </p>
<p>And it’s almost inexcusable not to at least give it a try, the creation system with the visual Wizard allows you to drag and drop these widget elements into a live dashboard. Allowing you to mix and match and change the sizes of anything that you desire. And once you understand what’s important to you and your architectures you can make repeatable code that you can inject right into your cloud formation templates to automatically create these highly visual and extremely potent dashboards for every system you set up.</p>
<p>And heck if you don’t even want to get your hands dirty, CloudWatch automatically sets up dashboards for you that can help you track some of the high-level systems within your services. You can go in and modify these automatically created dashboards to make them your own if you’re not comfortable starting from scratch.</p>
<p>I guess the big thing I really want you to take away from this is that CloudWatch dashboards are so easy to use. That I want you to just go give it a try, and you might be surprised what kind of insights you’ll gain by having all your information at the tips of your fingers. </p>
<p>Well, that brings us to the end of this course. My name is Will Meadows and I’d like to thank you for spending your time here learning about CloudWatch Dashboards. If you have any feedback, positive or negative, please contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated, thank you!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24/" class="post-title-link" itemprop="url">AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:21" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:21-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:56:58" itemprop="dateModified" datetime="2022-11-19T22:56:58-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-An-Overview-of-Amazon-CloudWatch-24/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course where I shall be taking a high-level look at Amazon CloudWatch and some of its features and components which enable you to monitor the health and performance of your environment to ensure it’s operating within its expected thresholds.</p>
<p>Before we start I’d like to introduce myself, my name is Stuart Scott, and I am the AWS content and security lead here at Cloud Academy. Feel free to connect with me to ask any questions using the details shown on the screen, alternatively you can always get in touch with us here at Cloud Academy by sending an e-mail to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> where one of our Cloud experts will reply to your question.</p>
<p>This course is ideally suited to those who are unfamiliar with Amazon CloudWatch and are looking to find out more about the service at an introductory level. </p>
<p>The objective of this course is to introduce you to Amazon CloudWatch, explaining what the service is used for and some of the features that it provides.</p>
<p>As a prerequisite to this course, you should have a very basic understanding of AWS. This is an introductory course and so it will cover the basics of Amazon CloudWatch</p>
<p>Feedback on our courses here at Cloud Academy is valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>Please note that, at the time of writing this content, all course information was accurate. AWS implements hundreds of updates every month as part of its ongoing drive to innovate and enhance its services.</p>
<p>As a result, minor discrepancies may appear in the course content over time. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. </p>
<p>So, if you notice any information that is outdated, please contact <a href="mailto:&#115;&#x75;&#112;&#112;&#x6f;&#114;&#x74;&#x40;&#99;&#x6c;&#x6f;&#x75;&#x64;&#x61;&#99;&#x61;&#x64;&#101;&#x6d;&#121;&#46;&#99;&#111;&#109;">&#115;&#x75;&#112;&#112;&#x6f;&#114;&#x74;&#x40;&#99;&#x6c;&#x6f;&#x75;&#x64;&#x61;&#99;&#x61;&#x64;&#101;&#x6d;&#121;&#46;&#99;&#111;&#109;</a>. This will allow us to update the course during its next release cycle.</p>
<p>Thank you! </p>
<h1 id="What-is-Amazon-CloudWatch"><a href="#What-is-Amazon-CloudWatch" class="headerlink" title="What is Amazon CloudWatch?"></a>What is Amazon CloudWatch?</h1><p>Hello and welcome to this lecture which will provide you with a high-level overview of what Amazon CloudWatch is and does.</p>
<p>Amazon CloudWatch is a global service that has been designed to be your window into the health and operational performance of your applications and infrastructure. It’s able to collate and present meaningful operational data from your resources allowing you to monitor and review their performance. This gives you the opportunity to take advantage of the insights that CloudWatch presents, which in turn can trigger automated responses or provide you with the opportunity and time to make manual operational changes and decisions to optimize your infrastructure if required. </p>
<p>Understanding the health and performance of your environment is one of the fundamental operations you can do to help you minimize incidents, outages and errors. As a result Amazon CloudWatch is heavily used by those in an operational role and site reliability engineers. </p>
<p>There are a wide range of components to Amazon CloudWatch, making this an extremely powerful service. Let me now run through at a high level some of these features and what they allow you to do, including CloudWatch Dashboards, CloudWatch Metrics and Anomaly Detection, CloudWatch Alarms, CloudWatch EventBridge, CloudWatch Logs, CloudWatch Insights.</p>
<p>Using the AWS Management console, the AWS CLI, or the PutDashboard API, you can build and customize a page using different visual widgets displaying metrics and alarms relating to your resources to form a unified view. These dashboards can then be viewed from within the AWS Management Console.</p>
<p>Here is an example of the different types of widgets you can select to build your dashboard.</p>
<p>The resources within your customized dashboard can be from multiple different regions making this a very useful feature. Being able to build your own views, you can quickly and easily design and configure different dashboards to represent the data that you need to see from a business and operational perspective. For example, you might need to view all performance metrics and alarms from resources relating to a particular project, or a specific customer. Or you might want to create a different dashboard for a specific region or application deployment. The key point is that they are fully customizable to be designed how YOU want to represent your data.  </p>
<p>For more information of selecting the right chart type to visualize data, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/">https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/</a></p>
<p>Once you have built your Dashboards, you can easily share them with other users, even those who may not have access to your AWS account. This allows you to share the findings gathered by CloudWatch with those who may find the results interesting and beneficial to their day-to-day operational role, but don’t necessarily require the need to access your AWS account.</p>
<p>Metrics are a key component and fundamental to the success of Amazon CloudWatch, they enable you to monitor a specific element of an application or resource over a period of time while tracking these data points. For example, the number of DiskReads or DiskWrites on an EC2 instance, these are just 2 metrics relating to EC2 that you can monitor. Different services will offer different metrics, for example, there is no DiskReads for Amazon S3 as it’s not a compute service, and so instead metrics relevant to the service are available, such as NumberOfObjects, which tracks the number of objects in a specified bucket.</p>
<p>By default when working with Amazon CloudWatch, everyone has access to a free set of Metrics, and for EC2, these are collated over a time period of 5 minutes. However, for a small fee, you can enable detailed monitoring which will allow you to gain a deeper insight by collating data across the metrics every minute. In addition to detailed monitoring, you can also create your own custom metrics for your applications, using any time-series data points that you need, but be aware that when you create a metric they are regional, meaning that any metrics created in 1 region will not be available in another.</p>
<p>CloudWatch metrics also allow you to enable a feature known as anomaly detection. This allows CloudWatch to implement machine learning algorithms against your metric data to help detect any activity that sits outside of the normal baseline parameters that are generally expected. Advance warning of this can help you detect an issue long before it becomes a production problem.</p>
<p>Amazon CloudWatch Alarms tightly integrate with Metrics that I just discussed and they allow you to implement automatic actions based on specific thresholds that you can configure relating to each metric.</p>
<p>For example, you could set an alarm to activate an auto scaling operation, such as provisioning another instance if your CPUUtilization of an EC2 instance peaked at 75% for more than 5 minutes. You could also configure an alarm to send a message to an SNS Topic when the same instance drops back below the 75% threshold, causing it to come out of an ‘alarm’ state notifying engineers of the change. </p>
<p>For more information on SNS, please see our existing course here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/">https://cloudacademy.com/course/using-sqs-sns-ses/</a></p>
<p>Speaking of Alarm states, there are 3 different states for any alarm associated with a metric, these being OK – The metric is within the defined configured threshold, ALARM – The metric has exceeded the thresholds set, and INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.</p>
<p>CloudWatch alarms are also easily integrated with your dashboards as well, allowing you to quickly and easily visualize the status of each alarm. When an alarm is triggered into a state of ALARM, it will turn red on your dashboard, giving a very obvious indication.</p>
<p>CloudWatch EventBridge is a feature that has evolved from an existing feature called Amazon Events. So if you have any prior experience working with CloudWatch Events then this will be fairly familiar to you.  </p>
<p>CloudWatch EventBridge provides a means of connecting your own applications to a variety of different targets, typically AWS services, to allow you to implement a level of real-time monitoring, allowing you to respond to events that occur in your application as they happen.  </p>
<p>But what is an event? Basically, an event is anything that causes a change to your environment or application.</p>
<p>The big benefit of using CloudWatch EventBridge is that it offers you the opportunity to implement a level of event-driven architecture in a real-time decoupled environment. </p>
<p>EventBridge establishes a connection between your applications and specified targets to allow a data stream of events to be sent. Currently, there is a wide range of targets that can be used as a destination for events as you can see here.</p>
<p>For the latest list of targets, please see the relevant documentation here: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html</a></p>
<p>Let me provide a quick level overview of some of the elements of this feature, and these include Rules, Targets, and Event Buses.</p>
<p>So starting with Rules. A rule acts as a filter for incoming streams of event traffic and then routes these events to the appropriate target defined within the rule. The rule itself can route traffic to multiple targets, however the target must be in the same region. </p>
<p>Next, we have Targets. We saw a list of these just a few moments ago, so targets and where the events are sent by the Rules, such as AWS Lambda, SQS, Kinesis or SNS. All events received by the target are done os in a JSON format</p>
<p>Now finally, Event Buses. An Event Bus is the component that actually receives the Event from your applications and your rules are associated with a specific event bus. CloudWatch EventBridge uses a default Event bus that is used to receive events from AWS services, however, you are able to create your own Event Bus to capture events from your own applications. </p>
<p>CloudWatch Logs gives you a centralized location to house all of your logs from different AWS services that provide logs as an output, such as CloudTrail, EC2, VPC Flow logs, etc, in addition to your own applications.</p>
<p>When log data is fed into Cloudwatch Logs you can utilize CloudWatch Log Insights to monitor the logstream in real time and configure filters to search for specific entries and actions that you need to be alerted on or respond to. This allows CloudWatch Logs to act as a central repository for real-time monitoring of log data. </p>
<p>An added advantage of CloudWatch logs comes with the installation of the Unified CloudWatch Agent, which can collect logs and additional metric data from EC2 instances as well from on-premise services running either a Linux or Windows operating system. This metric data is in addition to the default EC2 metrics that CloudWatch automatically configures for you. The list of these additional metrics collected by the agent can be found at this link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">here</a>. </p>
<p>There are now 3 different types of insights within CloudWatch, there are Log Insights, Container Insights, and Lambda Insights.</p>
<p>But what exactly are insights? Well as the name suggests, they provide the ability to get more information from the data that CloudWatch is collecting. So let’s look at each of these at a high level to understand the role that they perform, starting with Log Insights.</p>
<p>This is a feature that can analyze your logs that are captured by CloudWatch Logs at scale in seconds using interactive queries delivering visualizations that can be represented as bar, line, pie, or stacked area charts. The versatility of this feature allows you to work with any log file formats that AWS services or your applications might be using.</p>
<p>Using a flexible approach, you can use Log insights to filter your log data to retrieve specific data allowing you to gather insights that you are interested in. Also using the visual capabilities of the feature, it can display them in a visual way.</p>
<p>Much like Log insights, Container Insights allow you to collate and group different metric data from different container services and applications within AWS, for example, the Amazon Elastic Kubernetes Service, (EKS) and the Elastic Container Service (ECS). </p>
<p>In addition to the standard metrics collected for these services by CloudWatch, Container Insights also allows you to capture and monitor diagnostic data giving you additional insights into how to resolve issues that arise within your container architecture. This monitoring and insight data can be analyzed at the cluster, node, pod, and task level making it a valuable tool to help you understand your container applications and services.</p>
<p>As you may have guessed by now, this feature provides you the opportunity to gain a deeper understanding of your applications using AWS Lambda. Working on the principles as we have seen with the previous 2 insight features, it gathers and aggregates system and diagnostic metrics related to AWS Lambda to help you monitor and troubleshoot your serverless applications.</p>
<p>To enable Lambda Insights, you need to enable the feature per Lambda function that you create within Monitoring Tools section of your function:</p>
<p>This ensures that a CloudWatch extension is enabled for your function allowing it to collate system-level metrics which are recorded every time the function is invoked.</p>
<h1 id="CloudWatch-infrastructure-monitoring"><a href="#CloudWatch-infrastructure-monitoring" class="headerlink" title="CloudWatch - infrastructure monitoring"></a>CloudWatch - infrastructure monitoring</h1><p>Hi and welcome to our seventh lecture. In this lecture, we will take a look at Cloud Watch and explore its main features. We will also work with one of the best features of Cloud Watch, the custom metrics, where we can upload any data we want and monitor it from AWS.</p>
<p>First, we need to go to the Cloud Watch dashboard. Here, we can see the two alarms that we created in the last lecture. The low CPU utilization alarm is sending emails to the Cloud Motors admins topic.</p>
<p>We don’t want that, as this information is not critical for us. So let’s remove this notification and adjust the alarm, because after a while, we noticed that it was removing instances too soon. In the same way, let’s modify the high CPU utilization alarm, because it was also adding instances to auto scaling too soon. If you explore enough, you will notice that we could use the same alarm to trigger the increase and decrease policies that we specified in auto scaling. Let’s check the metrics. Here, we can see a list of all available metrics that we can query. We can see graphics, combine graphics with multiple metrics, or set alarms for the metrics that we see here. Depending on what you have in your environment, you could have more or less metrics to view.</p>
<p>If we select one metric, we can see a graphic with the metrics value over the time. We can also combine the current graphic with more than one metric. That way, we could, for instance, see what happens with the instance IO when the CPU utilization is high, or how the RDS database reacts when there is too much network traffic at the instances on the auto scaling group, or how many requests are being processed by ELB during the working hours. There are many possible choices. It’s up to you to combine the data and explore the possibilities. You could select the metrics that you want, build a graphic and share it. You just need to click on Copy URL and share the link.</p>
<p>You can select the exact time frame that you want to see on the graphic. But besides AWS services, Cloud Watch can also monitor how much you’re going to pay for the current month. You can also set alarms for that. You just need to go to the North Virginia region and view the billing metrics. Here you could, for instance, create a billing alarm in case you have a defined budget for your project. Since we want to save money, let’s create an alarm that will send an email to the one who’s responsible for the bills. Whenever the estimated cost is more than zero, it will trigger the alarm. In other words, we don’t want to pay anything. To really have billing alarms, you would need to first go on the billing and cost management console, click on preferences, and then select the receive billing alerts. I can’t do this since I’m not using the route account, and my IAM user doesn’t have permissions for that. Now, we are done.</p>
<p>For me, the best feature of Cloud Watch is the custom metrics that we can send and monitor as we do with any other metric.</p>
<p>I’ve written a simple script that’s available in the GitHub repository in the <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/cloudmotors/tree/master/scripts">scripts folder</a>. Before running, you need to have the AWS command line interface configured on your machine. Since I have it already, I will skip that part and show you the script.</p>
<p>You just need to change the end point variable and allow ping on the ELS’ security group to allow it to work properly. The ping will get the latency and curl will count the time that the welcome page is taking to load. I’m running this script as we speak, and it’s sending data already.</p>
<p>Let’s check the data. And here we have our custom metrics. And we can compare them on the same graphic over a time frame as we already saw. We can also export the graphic URL as any other graphic. I strongly recommend you take some time and explore everything that Cloud Watch can provide.</p>
<p>Lectures:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/what-to-study-and-course-introduction/">AWS SysOps Administrator Roles and Responsibilities</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/initial-application-setup-cloudformation/">Initial Application Setup</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/troubleshooting-and-diagnosing-failpoints/">Troubleshooting</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/managing-instances-through-scripts-with-aws/">Managing instances</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/deploying-elb-as/">Deploying an Elastic Load Balancer</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/complete-the-elastic-infrastructure-with-aws/">Complete the Elastic infrastructure</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/cloudwatch-infrastructure-monitoring-with-aws/">Infrastructure monitoring with CloudWatch</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/standards-deployment-aws/">Industry standards</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/application-security-in-aws/">Application Security</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/aws-shared-responsibility-model-1/">The AWS Shared Responsibility Model</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-sysops-administrator/elastic-beanstalk/">Elastic Beanstalk</a></li>
</ul>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello and welcome to the final lecture in this course where I just want to quickly summarize what we have covered. </p>
<p>So we now know what Amazon CloudWatch is, it’s a global service designed to be your window into the health and operational performance of your applications and infrastructure. It comes complete with many different features that allow us to capture and log essential data relating to our resources and applications. </p>
<p>By creating CloudWatch Dashboards you can build and customize different visual widgets to display metrics and alarms relating to your resources to form a unified view. This provides a very quick and easy method of viewing the status of your infrastructure based on metrics you have defined.</p>
<p>CloudWatch Metrics are key to the service, they enable you to monitor a specific element of an application or resource over a period of time while tracking these data points. This can help to ensure that you are not undersizing your resources, or in fact, oversizing your resource with too much spare capacity. Different services offer different metrics, allowing you to monitor the most important performance factors for each service.</p>
<p>CloudWatch Alarms enable you to implement automatic responses and actions based on custom thresholds defined for your metrics. Alarms can be in any 1 of 3 different states, either </p>
<ul>
<li>OK – The metric is within the defined configured threshold</li>
<li>ALARM – The metric has exceeded the thresholds set </li>
<li>INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state</li>
</ul>
<p>By using CloudWatch EventBridge provides a means of connecting your own applications to different targets to allow you to implement a level of real-time monitoring, allowing you to respond to events that occur in your application as they happen. This offers you the opportunity to implement a level of event-driven architecture in a real-time decoupled environment. </p>
<p>CloudWatch Logs centralize your logs from different AWS services and applications allowing you to monitor them in real time and filter for specific entries and actions. They can also capture log data from your on-premise infrastructure through the use of a logging agent.</p>
<p>There are 3 different variants of CloudWatch Insights, these being:</p>
<ul>
<li>Log Insights</li>
<li>Container Insights</li>
<li>Lambda Insights</li>
</ul>
<p>And they all provide the ability to collate and display more information from the data that CloudWatch is collecting. Log insights can analyze your logs at scale using interactive queries. Container insights allow you to collate and group metric data from container services such as EKS and ECS, and Lambda insights provides you the opportunity to gain a deeper understanding of your applications using AWS Lambda.</p>
<p>That now brings me to the end of this lecture and to the end of this course, and so you should now have a greater understanding of Amazon CloudWatch and the different features and options it can provide.</p>
<p>Feedback on our courses here at Cloud Academy is valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="2What-is-Amazon-CloudWatch"><a href="#2What-is-Amazon-CloudWatch" class="headerlink" title="2What is Amazon CloudWatch?"></a>2<strong>What is Amazon CloudWatch?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/data-visualization-how-to-convey-your-data-1112/course-introduction/">Data Visualization: How to Convey your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-sqs-sns-ses/introduction-sqs-sns-ses/">Using SQS, SNS and SES in a Decoupled and Distributed Environment</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/metrics-collected-by-CloudWatch-agent.html">List of metrics</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eventbridge-targets.html">List of targets</a></p>
<h1 id="3CloudWatch-infrastructure-monitoring"><a href="#3CloudWatch-infrastructure-monitoring" class="headerlink" title="3CloudWatch - infrastructure monitoring"></a>3<strong>CloudWatch - infrastructure monitoring</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/cloudmotors/tree/master/scripts">Scripts Folder</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:19" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:19-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:02" itemprop="dateModified" datetime="2022-11-19T22:53:02-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Lecture-Transcript"><a href="#Lecture-Transcript" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello, and welcome to this course, which has been designed to give you an overview in the introduction to the Amazon Macie service. I will explain what the service is and does, and how you can use it within your own environment to enhance your security level, especially when it comes to identifying the potential exposure of sensitive data, such as PII, or secret keys stored on Amazon S3. This is critical when it comes to maintaining your specific compliance programs, such as GDPR. </p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy, specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the details shown on screen, alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#112;&#x70;&#111;&#114;&#116;&#64;&#99;&#x6c;&#x6f;&#x75;&#100;&#x61;&#x63;&#97;&#100;&#101;&#x6d;&#x79;&#46;&#x63;&#x6f;&#x6d;">&#x73;&#x75;&#112;&#x70;&#111;&#114;&#116;&#64;&#99;&#x6c;&#x6f;&#x75;&#100;&#x61;&#x63;&#97;&#100;&#101;&#x6d;&#x79;&#46;&#x63;&#x6f;&#x6d;</a>, where one of our Cloud experts will reply to your question. </p>
<p>The content of this course is centered around security and compliance. As a result, this course is beneficial to those who are in the roles of, or similar to, Cloud security architects, compliance managers, Cloud administrators, and Cloud support and operation engineers. </p>
<p>This course is made up of the following lectures to explain the service and how it operates:</p>
<ul>
<li>What is Amazon Macie? Within this lecture you will understand exactly what the service is and the benefits that it provides. </li>
<li>Enabling and associating Macie with S3. There are specific requirements that must be configured before you enable this service. This lecture looks at those requirements and how to fulfill them. In addition to this, I’ll also show you how to associate your Amazon S3 buckets with Amazon Macie. </li>
<li>Alerts. In this lecture, I focus on the different types of alerts that Amazon Macie generates to allow you to resolve and rectify any issues identified. </li>
<li>Dashboard. Here I look at the different metrics that are available to help you understand the data that Amazon Macie has captured, monitored, and identified. </li>
<li>Users. This lecture looks at how Amazon Macie categorizes users and how to gain statistics on individuals for further analysis. </li>
<li>Research. In this lecture explain how you can perform deeper analysis of the data recorded by Macie using queries. </li>
<li>Classifying and protecting data. A key component of Amazon Macie is the classification of your data and this lecture looks at how that process works. </li>
<li>Multiple AWS accounts with Amazon Macie. This lecture is a demonstration on how to configure one AWS account as a master, and one as a member account with an Amazon Macie to consolidate and manage your results centrally. </li>
<li>And finally, the course summary. This lecture will highlight the key points taken from each of the previous lectures.</li>
</ul>
<p>The objectives of this course are, to provide an understanding and awareness of what Amazon Macie is, and what it’s used for. Also, to provide an explanation of each configured component of the service, to allow you to gain maximum benefit from Macie’s capabilities. You’ll understand how the service can provide a customizable approach to maintaining compliance. You’ll also understand how through automation and machine learning, Amazon Macie detects and categorizes S3 content to detect potential security threats and exposures. </p>
<p>As a prerequisite of this course, you should have an understanding and awareness of the following, Amazon S3, and AWS CloudTrail, including how to setup a trail in CloudTrail. </p>
<p>Throughout this course, I will reference a number of different URL links, which will help and direct you to related information on specific topics. To make these links easily accessible to you, I have included them at the top of the transcript, within the lecture that they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#115;&#x75;&#112;&#x70;&#x6f;&#x72;&#116;&#64;&#99;&#108;&#111;&#117;&#x64;&#97;&#99;&#x61;&#100;&#101;&#x6d;&#x79;&#46;&#x63;&#x6f;&#x6d;">&#115;&#x75;&#112;&#x70;&#x6f;&#x72;&#116;&#64;&#99;&#108;&#111;&#117;&#x64;&#97;&#99;&#x61;&#100;&#101;&#x6d;&#x79;&#46;&#x63;&#x6f;&#x6d;</a>. </p>
<p>That brings me to the end of this lecture. Coming up next, I start off by answering the question, what is Amazon Macie?</p>
<h1 id="What-is-Amazon-Macie"><a href="#What-is-Amazon-Macie" class="headerlink" title="What is Amazon Macie?"></a>What is Amazon Macie?</h1><h2 id="Resources-Referenced"><a href="#Resources-Referenced" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/webinars/establishing-privacy-program-gdpr-compliance-beyond-62/">GDPR Compliance Webinar</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">AWS Regional Product Service Table</a></p>
<h2 id="Lecture-Transcript-1"><a href="#Lecture-Transcript-1" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture, which will answer the question, what is Amazon Macie? </p>
<p>Amazon Macie was introduced in August of 2017 as a powerful security and compliance enabling service which sits within the security identity and compliance category of the AWS management consult. The main function of the service is to provide an automatic method of detecting, identifying, and also classifying data that you are storing within your AWS account. Macie currently supports Amazon S3 storage. However, additional support for other storage systems will be developed and added over time. The service is backed by machine learning, allowing your data to be actively reviewed as different actions are taken within your AWS account. Machine learning can spot access patterns and user behavior by analyzing cloud trail event data to alert against any unusual or irregular activity. Any findings made by Amazon Macie are presented within a dashboard which can trigger alerts, allowing you to quickly resolve any potential threat of exposure or compromise of your data. </p>
<p>There are a number of key features that are offered by Amazon Macie during its detection and classification process. These can be summarized as follows. Amazon Macie will automatically and continuously monitor and detect new data that is stored in Amazon S3. Using the abilities of machine learning and artificial intelligence, this service has the ability to familiarize over time, access patterns to data. Amazon Macie also uses natural language processing methods to help classify and interpret different data types and content. NLP uses principles from computer science and computational linguistics to look at the interactions between computers and the human language. In particular, how to program computers to understand and decipher language data. The service can automatically assign business values to data that is assessed in the form of a risk score. This enables Amazon Macie to order findings on a priority basis, enabling you to focus on the most critical alerts first. In addition to this, Amazon Macie also has the added benefit of being able to monitor and discover security changes governing your data. As well as identify specific security-centric data such as access keys held within an S3 bucket. </p>
<p>This protective and proactive security monitoring enables Amazon Macie to identify critical, sensitive, and security focused data such as API keys, secret keys, in addition to PII and PHI data. It can detect changes and alterations to existing security policies and access control lists which effect data within your S3 buckets. It will also alert against unusual user behavior and maintain compliance requirements as required. </p>
<p>Over the past few months, you will have likely heard about numerous occurrences whereby huge quantities of PII data being stored in the cloud, have been exposed unnecessarily. Many of these instances can be attributed to a lack of understanding of key security controls offered by Amazon S3 by those storing data within the service, in addition to simple human error. Also, the sensitivity of the data may not of been understood before being stored. Understanding your data and its business value is essential. Therefore, having a managed service which provides in essence, a double-check, against your sensitive business data is invaluable. </p>
<p>For example, checking to ensure you are not allowing sensitive data to be accessible via the internet, which will almost certainly have adverse negative effects. There are a wide variety of compliance programs that need to be adhered to and ensuring you maintain your compliance is crucial to your business. For example, from a general data protection regulation, GDPR perspective, you are required to keep any personal information of EU citizens protected and secured at all times with adequate protection. If you inadvertently expose data of EU citizens, you could be faced with significant financial penalties, which can total 4% of your annual global turnover or up to 20 million euros, whichever is greater. So maintaining compliance and having the available tools and services to help you enable this, is fundamental for businesses storing data in the cloud. </p>
<p>If you would like to learn more about GDPR, you can listen to our existing webinar <a target="_blank" rel="noopener" href="https://cloudacademy.com/webinars/establishing-privacy-program-gdpr-compliance-beyond-62/">here</a>, entitled Establishing a Privacy Program GDPR Compliance and Beyond. Currently, Amazon Macie is not available in all regions of AWS, so I recommend you check the AWS regional product service table found <a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">here</a> before relying on a service to work with your S3 data, which is also a regional service.</p>
<h1 id="Enabling-and-Associating-Amazon-Macie-with-Amazon-S3"><a href="#Enabling-and-Associating-Amazon-Macie-with-Amazon-S3" class="headerlink" title="Enabling and Associating Amazon Macie with Amazon S3"></a>Enabling and Associating Amazon Macie with Amazon S3</h1><h2 id="Resources-Referenced-1"><a href="#Resources-Referenced-1" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=MacieServiceRolesMaster&templateURL=https://s3.amazonaws.com/us-east-1.macie-redirection/cfntemplates/MacieServiceRolesMaster.template">US East (Virginia) CloudFormation Template</a></p>
<p><a target="_blank" rel="noopener" href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=MacieServiceRolesMaster&templateURL=https://s3-us-west-2.amazonaws.com/us-west-2.macie-redirection/cfntemplates/MacieServiceRolesMaster.template">US West (Oregon) CloudFormation Template</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-setting-up.html#macie-setting-up-enable">CloudFormation Templates for all regions available</a></p>
<p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-automation-how-to-use-cloudformation/">How to use Cloudformation for Automation</a></p>
<p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-aws-cloudformation/">Advanced use of AWS CloudFormation</a></p>
<p>Lab: <a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/deploy-wordpress-cloudformation-17/">Deploy Wordpress using CloudFormation</a></p>
<p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/">AWS Cloudtrail: An Introduction</a></p>
<p>Lab: <a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/monitoring-aws-cloudtrail-events-amazon-cloudwatch-76/">Monitoring AWS CloudTrail Events with Amazon CloudWatch</a></p>
<h2 id="Lecture-Transcript-2"><a href="#Lecture-Transcript-2" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture. Now we have an understanding of what Amazon Macie is, let me now explain how to enable it so you can associate it to your Amazon S3 data to understand any potential security issues. </p>
<p>As I mentioned previously, the services located under the security identity and compliance category within the AWS Management Console. When you first go into the service for the first time you’ll be presented with a splash screen similar to the following. From here you simply click on get started. Now at this point Amazon Macie will check your AWS account for specific requirements that are needed before you can go ahead and fully enable Amazon Macie. These requirements are to check the existence of IAM roles, specifically the AWSMacieServiceCustomerSetupRole and to check if AWS CloudTrail is enabled within your AWS account. Both of these are prerequisites to being able to enable Amazon Macie. If these components are not configured within your AWS account you will see the screen like this. As you can see, there are two red check marks against each of these requirements. </p>
<p>To resolve the first issue of the IAM roles, you will need to launch a preconfigured AWS CloudFormation Stack that has been created by AWS that will automatically set up and configure the roles that are needed. Dependent on your region that you’re using will depend on the CloudFormation Stack used. Currently there are templates for US East Virginia and US West Oregon. The URL links to these stacks can be found in the transcript of this lecture along with the URL source of these templates, allowing you to find more regional stacks as and when they are released. </p>
<p>Before I continue I just want to quickly mention that if you’d like additional information on AWS CloudFormation, then you can view our existing content here. We have a couple of courses, how to use CloudFormation for AWS Automation and Advanced Use of AWS CloudFormation, and we also have a lab Deploying Wordpress using AWS CloudFormation. </p>
<p>Okay, back to our CloudFormation Stacks. Regardless of which one you use dependent on your region, the process is very simple. So don’t worry if you’re not familiar with CloudFormation. When you click on one of the links for the stacks it will open up CloudFormation within your AWS account, providing you are logged into your account already. The data and information will already be prefilled and all you need to do is to accept the defaults. On the select template page you will see that it has already preconfigured the relevant template under the specify an Amazon S3 template URL heading. To proceed to the next screen press next. At the specified details screen leave the default stack name of MacieServiceRolesMaster and click next. On the options screen leave all settings as default and click next. On the final review screen you will need to acknowledge the message that CloudFormation might create IAM resources with custom names via checkbox. Once you have done so click create. At this point the CloudFormation Stack will be created and will generate the required resources defined by the stack, which includes the necessary IAM roles and policies that Amazon Macie requires. </p>
<p>The next requirement needed by Amazon Macie is the enablement of AWS CloudTrail. If you are not familiar with CloudTrail then you can see our existing content of the service here. We have a course, AWS CloudTrail: An Introduction and a lab, Monitoring AWS CloudTrail events with Amazon CloudWatch. The following demonstration will explain and show how to create and enable a new trail within CloudTrail to fulfill this second requirement. </p>
<p>Okay, so I’m logged into my AWS account and CloudTrail is under management tools here. So if we just select CloudTrail, and that will take us to the dashboard. And now from the dashboard on the left-hand side we can then create a trail here, which is what we need to do. So let’s create a new trail. We’ll call this trail name Macie-demo, and we can apply this trail to all regions. And just leave that as a default yes. Under management events, we want to be notified of all read&#x2F;write events. </p>
<p>If we scroll down to data events here you can see S3 and lambda. We’re only interested in S3 so let’s select all S3 buckets in your account. And by default it selected all read and write actions. If we wanted just to do specific buckets then we could add the bucket here, but we’ve said we will select all three S3 buckets in our account. Under storage location this is where it’ll store the CloudTrail logs, and we can create a new S3 bucket for this. So let’s give this a name of CloudTrail logs. And if we go down to advanced, here we can add a log file prefix if we wanted to, but there’s no need to for this demonstration. We can crypt our log files if we wanted to, and we can select a KMS key. For this demonstration I’m just going to leave that as a default of no. </p>
<p>Log file validation determines if the log file has been tampered with, so you can either have that on or off, and also here you can select to have an SNS notification for every long file delivery. And again, I’m going to set the default for no. So once that’s been selected you can click on create. As I should have expected, this bucket order exists. So I’m going to add cloud academy to the end of that because it needs to be a unique bucket name, as we know. That will then go ahead and create the trail. And here we have the new trail and the status is running. And that’s it. Now that we have both requirements completed for Amazon Macie we will now be able to enable the service. As you can see I now have both requirements fulfilled indicated by tick. There is one final element to address and that is the permissions checkbox that you can see at the bottom of the page. This simply asks you to acknowledge that through the enablement of Amazon Macie you are happy that the service will have permission to analyze you AWS CloudTrail logs and events. It’s worth noting that you can always disable Amazon Macie at any point, and this will in turn stop the monitoring and analysis of your logs. When you select the tick box you will then be able to enable Macie. </p>
<p>Once Macie is enabled you will be taken to the console of the service, which will look something like this. From here our next step is to associate our Amazon S3 buckets that we want Amazon Macie to monitor. The best way to show you how to do this is via a quick demonstration. </p>
<p>Okay, so I’m back in my AWS account, and I’ve just opened up Amazon Macie. From here what I need to do is on the left-hand side go down to integrations. And then you can see at the top here accounts and services. What I need to select is services. Now I need to select the account I want to integrate other services with Amazon Macie, and this is my AWS account. And at the moment the only viable option is Amazon S3. Overtime there will be more and more storage services added to this section. So all I need to do is click on add. And now here it’s asking me which S3 buckets I want Macie to monitor. So I’ve created a bucket down here called macie-demo-cloudacademy. So I just want Amazon Macie to monitor this bucket here. And I also want Macie to classify all the data within this bucket. And by classifying all the content within that bucket, as I add more and more content to this bucket then Amazon S3 will automatically detect and classify all that data within it to notify me of any potential issues. Once I’ve selected that I then click on review and save. </p>
<p>There’s a couple of tick boxes that you need to select here, the first one to say that you understand that S3 object-level logging is enabled for all buckets and that you understand that choosing to classify all objects in the selected S3 bucket can significantly affect the content classification costs. And then you click on save. And that’s it. Now your bucket will be classified by Amazon Macie, and it will be monitored as well. So it’s a very simple quick and easy process. We now have our Amazon Macie account enabled and our Amazon S3 bucket’s being monitored, which brings me to the end of this lecture. </p>
<p>Over the next few lectures I’ll be explaining the different elements of Amazon Macie console to help us understand how we configure and utilize it effectively as a compliance and security tool, starting with alerts.</p>
<h1 id="Alerts"><a href="#Alerts" class="headerlink" title="Alerts"></a>Alerts</h1><h2 id="Resources-Referenced-2"><a href="#Resources-Referenced-2" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-research.html#macie-query">Constructing Queries in Amazon Macie</a></p>
<h2 id="Lecture-Transcript-3"><a href="#Lecture-Transcript-3" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture on Amazon Macie alerts. As Amazon Macie is a security analysis and compliance tool, we can expect it to identify and notify us of any potential issues that it finds, and this action is performed by Macie Alerts. By default, the service is pre-configured with a wide range of alerts based on security best practices and the sensitivity of data that the service will check against. Depending on your organization and the industry that you’re in, it can affect what you deem as sensitive information, and so there may not be an alert which fulfills your requirements. Thankfully, Amazon Macie provides the functionality of being able to create custom alerts which you can configure which Macie will then use against your data. This is very useful when it comes to maintaining and control and compliance of your data sets. </p>
<p>Alerts exist as two different types, these being basic and predictive. </p>
<p>Basic alerts. The basic alert consist of both pre-built alerts that come with Amazon Macie and custom alerts. These customized alerts are designed and built by you, which relate to specific security checks. The pre-defined alerts and checks defined by Macie have been categorized as follows. </p>
<ul>
<li>Anonymized access. This is where users are trying to gain access to your AWS account and its resources while trying to mask and hide their own identity. </li>
<li>Config compliance. This focuses on settings and policies that can relate to compliance issues within your environment such as a change to a CloudTrail login policy. </li>
<li>Credential loss covers scenarios where access control data may have been compromised. </li>
<li>Data compliance. This checks your data for identification of security content, such as access keys, credential data, or PII and PHI information. </li>
<li>File hosting. This check relates to compromised instances where malware may be detected. </li>
<li>Identity enumeration. This check helps to detect a potential attack or weakness in access control and credentials by identifying a rise in API calls or access attempts across your account. </li>
<li>Information loss. This looks at unusual behavior and irregular activity of access to information classed as sensitive. </li>
<li>Location anomaly. Checks for access requests where the source of the request is from an unexpected and unusual location from outside the normal operations based on history. </li>
<li>Open permissions. This check is very useful as it checks your permissions to sensitive data to see if they are overly permissive and potentially allow for unnecessary data exposure. </li>
<li>Privilege escalation. These checks focus on all access attempts, which will result in a privileged level of access control which could potentially cause further damage and harm to your environment. </li>
<li>Ransomware. This focuses on compromised infrastructure where ransomware has been detected. </li>
<li>Service disruption. This checks if a disruption to your environment is likely when making your own internal configuration alterations to your infrastructure. For example, the alteration of a security group that will result in the denial of access of previously used communications. </li>
<li>Suspicious access. Checks for unusual and risky access being requested by anonymous users where source data is being masked, such as their IP address, etc. For example, a malicious user masking their connection across a number of compromised hosts.</li>
</ul>
<p>It’s worth noting that these alerts provided by Amazon Macie cannot be altered or modified in any way. </p>
<p>Predictive alerts. Predictive alerts look at the behavior of your AWS account to automatically identify activities that sit outside the realms of normal operations. Over time, Macie forms a baseline understanding of day-to-day operations and learns what is normal and what is unusual behavior through analysis of your CloudTrail logs and events using machine learning. For example, this could be a sudden spike in user access of a particular S3 bucket that may normally only be accessed very rarely. When this happens, Amazon Macie will send a predictive alert informing you of the anomaly. The same principle applies to predictive alerts as with basic in that they can’t be edited or changed in any way. </p>
<p>Let me now explain the different points of interest on the alert itself so you know what to expect and what information comes with an alert as and when you receive one. This image shows an alert within my Amazon Macie console. This is the view of an alert that appears in the alerts section of the console for Macie, and it gives a high-level overview of information relating to that event that triggered the alert. The top half displays the severity, which is set to low, the name of the alert, which is Change to Cloudtrail logging policy, the alert type and category, these being basic and config compliance, and the bottom half gives some additional detail, such as when the alert was triggered, the identity that triggered the alert, and in which region, along with the number of results captured in the alert and how many times it’s been viewed. </p>
<p>To drill down into this further, you can click on the alert and it will display a detailed finding. This view is broken down into two parts, the alert summary and the alert details. The summary provides additional information allowing you to respond to the alert appropriately with the findings given. The description of the alert provides a deeper level of understanding of why the alert has been generated. A breakdown of results is also displayed, and as this relates to a CloudTrail action rather than an S3 action, it displays the API calls related and captured in the event. If it was related to S3 data, then it would list the S3 buckets and objects affected by the alert. The alert details section allows you to gain even more information. By clicking on the type icon, a whole host of additional information is displayed. I won’t go through all the data, but as an example, the following are just some elements of data captured for this alert. As you can see, you can obtain a substantial amount of information and useful data from these alerts to ascertain if there is a viable threat. If you analyze the alert and identify that this is in fact a legitimate security incident, then you can make the necessary changes to your infrastructure to ensure this doesn’t happen again. For example, restrict the user permissions or revoke the user entirely. However, you may decide that this activity is a normal operation and is expected of this user. In this situation, you have the ability to whitelist the user for this alert. By doing so, Amazon Macie will no longer register an alert for Change to Cloudtrail logging policy for this particular user. As you can see, you can also archive the alert for future reference or edit the alert. As this alert was predefined by Amazon Macie, you are unable to edit it. If it was a custom alert, then you could edit it as required. </p>
<p>Before I move on to show you how to create a custom alert, I just want to explain the different level of severity associated with these alerts and what these severities generally mean. There are five different severities for an alert, these being:</p>
<ul>
<li>Informational. If you receive this alert, it doesn’t indicate a threat or any risk to your current operations. These alerts are used to provide information to allow you to make informed decisions and changes to your infrastructure if you deem it necessary. </li>
<li>Low. This is the lowest level of a potential security threat that could compromise your data from a confidentiality, integrity, and availability perspective. Action isn’t required immediately, but it should be investigated and resolved in future changes and fixes. </li>
<li>Medium. This indicates the next level up for a threat that could impact the CIA of your data, and action should be taken against this prior to any low severities. </li>
<li>High. If an alert is set to high, this requires immediate action and attention, as there is a very high chance that your data can be compromised from a CIA perspective. </li>
<li>Critical. This is very similar to high. However, the main difference is that it is likely that your data and services have already been compromised from a CIA standpoint.</li>
</ul>
<p>Let me now move on to show you how you can add your own customized alert via a quick demonstration. In this demonstration, I will show you how to create a custom basic alert using a very simple query. </p>
<p>Okay, so I’m on the dashboard of my Amazon Macie account, and what I need to do to create my own alert is go down to Settings and then scroll down and select Basic alerts at the bottom. Now, this shows all the basic alerts that have been created, and as you can see, many of them have been created by Macie. But if I want to add my own alert, I can click on the green button here, click on Add New, and it’ll ask me for a number of different types of information. So we can give it an alert title. I’ll just call it This is a test alert. Same for a description. </p>
<p>Next I’ll need to select a category, and these are all the different categories for basic alerts. For this one, I’m going to select Anonymized Access. Now, this next section is the query. Now, this is where you enter the query information that defines what the alert is, the information that it’s searching for within the data that’s been collected by Macie through CloudTrail logs and also through S3 data. How to construct these queries is outside of the scope of this course. However, you can find more information on these queries in the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-research.html#macie-query">link</a> on the screen. </p>
<p>So I’m going to add in my query here, and what this will do, this will search for every time the GetBucketPolicy API is called by the user identity type of IAMUser. So this alert will not be triggered if an AWS service called the GetBucketPolicy, for example. It’ll only be alerted if an IAMUser requested the GetBucketPolicy. Next we have the index, and this is the source data that it’ll search upon, either CloudTrail data, S3 bucket properties, and S3 objects. For this particular query, I want it to search through the CloudTrail data. Then you can have the minimum number of matches. For this demonstration, I’m going to have it as one. For the severity, we can have informational, low, medium, high, or critical. I’m just going to leave this as informational. And then at the bottom here, we have enabled Yes - active. And all I need to do at this point is then click on Save. </p>
<p>And that’s now added our alert to this list of basic alerts, and we can filter and order these alerts. So if I click on the Created by, we can see here our alert, This is a test alert, and it’s a custom alert. Now, if we want to test that just to make sure it’s picking up data, we can go across to the magnifying class and click on Research current query. This takes us to the Research section, and as you can see, currently there’s a total of 13 matched results based on that query. But I’ll be covering more on this research feature in another lecture in later in this course. </p>
<p>So if you go back and find our alert again, then what we can do, as well, we can delete this alert or edit it, and if we want to edit it, we can just change the details here. And that’s how you create your own basic alert.</p>
<h1 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h1><h2 id="Lecture-Transcript-4"><a href="#Lecture-Transcript-4" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture where I am going to be looking at the Amazon Macie dashboard. The Amazon Macie dashboard is the central hub of information that is collated, monitored, and classified through Amazon CloudTrail logs and any services associated to Macie, such as Amazon S3. The dashboard is accessed via the Amazon Macie console, which will look something like this. </p>
<p>Let’s start by looking at the four metric boxes at the top of the page and what they mean, starting with critical assets. This metric defines as a percentage how many of your assets have been identified as high-risk, which is anything with a risk value of eight, nine, or 10. These values are assigned to your assets in two ways, either by data classification made by Macie monitoring S3 data, or from specific API calls detected in your CloudTrail logs that have been marked with a set risk value. If the risk value has been classified based on S3 data, then the following categories are used to define its risk. The content type. Examples of this are plaintext, document, or source code. File extensions. Examples are .bat, .dmg, .sql. Themes. Examples of themes include financial keywords and social security keywords. And finally, regex configurations, which are regular expressions used to search for data patterns. More on these classification types and the methods behind it will be covered in the classifying and protecting data lecture later in this course. If the risk relates to an API call relating to your AWS infrastructure in some way, then the CloudTrail events in CloudTrail errors risk management will determine their value. Again, more on this will be discussed in that same lecture. </p>
<p>Next is the total event occurrences metric. This relates to your Amazon CloudTrail logs and calculates the number of API calls that Amazon Macie has monitored as a part of the security analysis of your infrastructure. The total user sessions metric is a count of user sessions which Macie has processed. A user session is defined by a five-minute aggregate of CloudTrail data. This metric provides its count from when Amazon Macie was first enabled in your AWS account. Finally, the total users metric, which shows the number of users that have been identified by CloudTrail data, which are then are then categorized into Platinum, Gold, Silver, or Bronze, depending on which API calls those users have been requesting and initiating, will relate to their perceived risk level, Platinum being high-risk, Bronze being low-risk. More on Amazon Macie users will be covered in the next lecture.</p>
<p>Let me now move on to the bottom half of the dashboard. So the bottom half of the dashboard screen is used to present a number of different views of graphs, charts, and statistics that Amazon Macie has detected and monitored through its sources. They are accessed via the following icons, and these icons represent the following. S3 objects for selected time range, S3 objects, S3 objects by PII, S3 objects by ACL, high-risk CloudTrail events and associated users, high-risk CloudTrail errors and associated users, activity location, CloudTrail events, activity ISPs, and CloudTrail user identity types. Let’s have a quick look at what each of these filter against starting with:</p>
<p> S3 objects for selected time range. This metric includes a slider bar which defines the objects that are visualized in the graph. The value of the slider goes between one to 10 and represents the minimum risk value of the object to be included. For example, if the slider is set to five, all objects represented in the graph will have a risk value of five or greater. The graph also shows when the object was last modified using the time ranges of zero to six months ago and beyond six months from the date Macie was enabled. </p>
<p>S3 objects. This metric shows your monitored S3 objects grouped together by Amazon Macie themes. The complete list of themes can be found under Settings &gt; Themes from within the Macie console. For each theme identified in this view, a percentage will be shown which indicates how much of your total objects are categorized to that particular theme. In addition to this, it’ll also provide a count number of the objects within that theme. More on themes will be discussed in the later lecture Classifying and Protecting Data. </p>
<p>S3 objects by PII, personally identifiable information. The third metric relating to S3 is split into two sections, S3 objects by PII priority and S3 objects by PII types. S3 objects by PII priority displays the objects that have been classified by Amazon Macie as having PII-related data associated with them, such as names, email addresses, credit card numbers, et cetera. These objects are then split out into different priority levels ranging between none, low, moderate, and high, depending on the quantity of PII data detected. Again, each of these priority classifications also have a percentage showing how much of your total objects are categorized within that PII priority value as well as a total count of objects in that priority. S3 objects by PII types metric displays the PII objects by their type classification. For example, here you can see the type of IPV4 and name where Amazon Macie has detected this data within my S3 objects. Again, the percentage and count values also exist per PII type. </p>
<p>S3 objects by ACL, access control list. The final metric relating to S3 displays three graphs relating to your access control list. The first graph is S3 objects by ACL URIs, uniform resource identifiers, which is used to define the object’s location. This shows how many URIs appear in your S3 access control list that your objects are associated to. The usual percentage and count statistics are also applied. Next we have S3 objects by ACL display names, which simply shows the different ACL display names that are associated to your objects along with the percentage of objects relating to each ACL name and its count of objects. Lastly, S3 objects by ACL permissions. This metric looks at the different level of access control list permissions, such as full control, read, write, et cetera. In this example, we can see that within the current set of ACLs used and associated to objects, the only permission given is full control to all objects. </p>
<p>High-risk CloudTrail events and associated users. This is the first of the metrics that relate to AWS CloudTrail. Much like the first S3 metric discussed earlier, this also provides a slider representing risk values from one to 10, which will affect the results displayed in two different charts. The first chart relates to the top 20 high-risk CloudTrail events detected in the last 60 days. These are events that have been captured through analysis of CloudTrail logs where Amazon Macie has classified specific API calls at a certain risk level. These APIs are then visualized on the bar chart displayed. The chart has date depicted along the x-axis and the API count on the y-axis. There is also a key provided for API identification, and you can also view this data as either a daily count or weekly. The second chart relates to the different users detected within the CloudTrail logs, and they’re presented in the same format as the previous graph, again showing the count, date, and key for easy identification of users. </p>
<p>High-risk CloudTrail errors and associated users. This focuses on any AWS CloudTrail errors detected, and those are errors resulting from API actions detected in the CloudTrail logs. Similarly to the previous metric, two charts are used again. The first relates to the top 20 high-risk CloudTrail errors detected in the last 60 days, and these errors are then visualized on the bar chart display. The second chart, again represented in the same way, displays the CloudTrail users who made the API calls that resulted in these errors. </p>
<p>Activity location. This represents a global map showing the locations of activity of actions that Amazon Macie is monitoring and analyzing. The map is interactive, allowing you to zoom in and out and also change the date range from the past 15, 30, or 90 days or the past year. </p>
<p>CloudTrail events. This metric identifies all of the CloudTrail events that Amazon Macie is monitoring and capturing. As with the other metrics, there is a count, and this count represents the number of user sessions for the API that was recorded. The percentage represents the occurrence of that API in the total list of events. </p>
<p>Activity ISPs. This metric simply records the ISPs that have been used when actions have been monitored in CloudTrail. </p>
<p>CloudTrail user identity types. This final metric indicates which user type has been used when calling APIs based on your CloudTrail logs. As you can see from this image, the majority of events recorded have been made by an account that belongs to a service indicated by the AWS Service user type. </p>
<p>That has now brought me to the end of this lecture covering the AWS dashboard. Coming up next, I will be looking at Amazon Macie users.</p>
<h1 id="Users"><a href="#Users" class="headerlink" title="Users"></a>Users</h1><h2 id="Lecture-Transcript-5"><a href="#Lecture-Transcript-5" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture covering users. I want to define how Amazon Macie classifies users and the different user types available that I briefly mentioned in the previous lecture when looking at the CloudTrail User identity types within the dashboard. Let me first start out by talking about the users section within the console. You may recognize the platinum, gold, silver, and bronze categories which are always present on the dashboard. But this users screen provides additional information surrounding this metric. This screen essentially shows the different users that have been identified via CloudTrail data when linking to API calls. Depending on the history of those users will dictate how Amazon Macie classifies them between these four category ratings which ultimately reflect the level of perceived risk that those users pose. Let me define the differences between these categories a bit further. </p>
<p>Platinum. A user or role placed within this category is considered to make regular high risk API calls such as DeleteFlowLogs or UpdateTrail. These can often be contributed to users with administrative privileges, such as the root account. Due to the amount of control and elevated permissions these users pose, you should monitor their activity for any signs of compromise. </p>
<p>Gold. Users or roles categorized as gold will have been identified through their use of calling and making API requests relating to infrastructure changes, such as CreateRouteTable or RequestSpotinstances. Again, due to the permission level of these power users, they should also be monitored to ensure that they are not compromised in any way. </p>
<p>Silver. As we progress down the track, users or roles in the silver category are identified as performing medium level risk API calls. Remember, these risk levels are managed by Amazon Macie and automatically assigned these risk levels to the API calls. As an example, a medium level risk API could be DescribeRouteTables or ListObjects. </p>
<p>Bronze. Users or roles in this category provide the lowest level of risk due to history of their API call usage, which may include API calls such as DescribeSubnets and DescribeHosts. </p>
<p>These categories give you a very quick visualization as to how many users are operating with potentially over-elevated permissions. This allows you to drill down into these users further to ascertain exactly what tasks they are performing. On the main page of the user screen, you’ll see a list of users that Amazon Macie has identified. These users are categorized under the DLP, data loss prevention value column. It also provides a high level overview of the most recent activity, total events, errors, and unique IP addresses that have been attributed to that particular user. However, should you wish to investigate a particular user and gain further insight into events surrounding them, then you can click on their name within the user column. This will then provide you with a condensed version of the dashboard discussed in the previous lecture, with metrics and data that only relate to that user in question.</p>
<p>In this example, you may recognize the dashboard icons at the top where we have views relating to high-risk CloudTrail events, high-risk CloudTrail errors, activity location, CloudTrail events, activity ISPs, and CloudTrail user identity types. Again, these metrics in each view will only relate to events and data associated with that specific user. The last point I want to discuss in this lecture is how Amazon Macie defines user identity types, which is used in the main dashboard and the condensed dashboard for each specific user. </p>
<p>Amazon Macie identifies user identity types from the CloudTrail logs that it monitors and analyzes. Specifically using the user identity element from the events in the logs. The different user types are defined as follows. </p>
<ul>
<li>Root. This means that the request was initiated by the root account of your AWS account. </li>
<li>IAM user. Here the request was made by an IAM user. </li>
<li>Assumed role. This identity type defines that the request was made by credentials that were temporarily assumed having been obtained by calling the assumed role API for the security token service. </li>
<li>Federated user. Here the request, again, was made with temporary credentials but this time from calling the AWS STS GetFederationToken API. </li>
<li>AWS account. This simply defines that the request of the API was made by a different AWS account. </li>
<li>AWS service. Here the request was made by an AWS service rather than a particular user.</li>
</ul>
<p>That now brings me to the end of this lecture. Coming up next I will be discussing the research functionality of Amazon Macie.</p>
<h1 id="Research"><a href="#Research" class="headerlink" title="Research"></a>Research</h1><h2 id="Resources-Referenced-3"><a href="#Resources-Referenced-3" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://lucene.apache.org/core/2_9_4/queryparsersyntax.html">Apache Lucene Query Parser Syntax</a></p>
<h2 id="Lecture-Transcript-6"><a href="#Lecture-Transcript-6" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello, and welcome to this lecture what looks at the research feature offered by Amazon Macie. This is a very useful function providing having an awareness and understanding of how to create queries using Apache Lucerne. How to construct queries is outside the limitations of this course. However, for further information you can visit the Apache Lucerne Query Parser syntax page. </p>
<p>This research function allows you to create your own queries against all of the data Amazon Macie has collected and monitored for AWS CloudTrail and Amazon S3. By doing so it enhances the flexibility of the service by providing a way of enabling deep dive analysis of your data that relates to your specific requirements within your business. Using the query parser you can build and construct your own queries to return the exact results that you need. </p>
<p>Underneath the query parser are a number of options that provide additional filters for your results. The first option allows you to limit the data source that Amazon Macie uses to perform your query. The options included here are CloudTrail data, S3 bucket properties and S3 objects. There are also two other filters that allow you to restrict the number of results found along with the date range filter. The number of results filters here include top 10, top 50, top 100 and top 500. The date range include seven days, 30 days, 90 days, 365 days, all, and a custom time frame. Amazon research is in fact closely tied with most of the other sections we have already discussed and look at. For example, the dashboard and alerts of Amazon Macie. When looking at the information represented in your dashboard graphs and visual representations of statistical information you will find that the graphs and images are interactive. If you were to click on the data or magnifying glass, by doing so you will often be redirected to the research feature. </p>
<p>For example, if I were to select CloudTrail events in the dashboard and then select the magnifying glass next to one of these events. Let’s say, get bucket policy, I am redirected to the research feature which will automatically fill out the query parser, allowing me to investigate the data further. As you can see, there are 707 results matched which we already knew from the dashboard page with the statistics provided there. However, what we could do now is perform additional analysis by drilling down in this information further. If I only wanted to look at IAM users that perform this API call rather than all user identity types which as we know know includes the root account IM users assumed roles of federated users, et cetera, et cetera then I can add some additional commands to the query parser which only currently states event name, error code key, get bucket policy. By adding the following commands after the current content, this query will then only display results where the get bucket policy API is used by IAM users only. </p>
<p>The results of this query are then displayed as follows. This query now only shows a total of nine matched results. This was a very simple example of how you can use the query parser to get further detailed analysis of your data that’s specific to the results that you need to investigate instant security threats and compliance requirements further. As you become more and more familiar with Amazon Macie and the components and elements that are of particular interest to you you are able to save your queries as a favorite to save you having to repeatedly type them into the query parser, using the following icon. </p>
<p>You can also create your queries and have them saved as a custom alert. This ensures that whenever criteria that matches your query appears it will appear within your alert screen to allow it to quickly identify and take the appropriate action as necessary. Again, this level of customization allows you to filter and direct your search for specific elements, allowing you to achieve different levels of compliance regulation. The following icon allows you to save your query as one of these custom alerts. </p>
<p>That now brings me to the end of this lecture. Coming up next I’ll be looking at how Amazon Macie classifies and protects your monitored data.</p>
<h1 id="Classifying-amp-Protecting-Data"><a href="#Classifying-amp-Protecting-Data" class="headerlink" title="Classifying &amp; Protecting Data"></a>Classifying &amp; Protecting Data</h1><h2 id="Resources-Referenced-4"><a href="#Resources-Referenced-4" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://www.regexbuddy.com/regex.html">Regular Expressions (Regex)</a></p>
<h2 id="Lecture-Transcript-7"><a href="#Lecture-Transcript-7" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture where I’ll be explaining how Amazon Macie makes its decisions on data classification through AWS CloudTrail logs and Amazon S3 actions. </p>
<p>Data being stored on Amazon S3 within your AWS account is classified by Macie which determines its level of business sensitivity and criticality. Every data object within your Amazon S3 buckets automatically receives a perceived level of risk based on this classification process. The data values depicted within the dashboard, discussed earlier, are all driven from this classification and risk assessment. So what are these categories of classification that Amazon Macie uses? </p>
<p>There are four categories for classification, which can be found under the settings menu within the Amazon Macie console. These being content type, file extensions, themes, and regex. The classifications within these categories can not be ordered or modified in any way. Neither can you add additional entries within each of these classifications. </p>
<p>Content type. The content type classification allows Macie to detect the type of file that is being stored on S3. For example, a binary file, a document, or source code object. Amazon Macie will then embed an identifier in the header of the file for classification. If you look at the different content types available, you will notice that there is a long list of types and every entry has the following fields. Name, description, classification, risk, and enabled. The first two fields are obvious. The classification field actually specifies the content type of that type of file. For example, an Adobe Illustrator file is classified as a document and a WireShark packet capture is classified as binary. The risk is a value between 1 to 10 and defines the business risk value of that type of content. Respectively, Adobe Illustrator files have a risk value of one and the WireShark files have a risk value of six. Finally, you can choose to have the content type entry enabled or disabled. If it’s enabled and active, the value will read yes. If it’s disabled the value will read no. This setting can be changed by selecting the entry and making the change. This is the only value that you can change on the content types. </p>
<p>File extensions. The file extension classification looks at the file extension of the object to ascertain its risk value. The same field types are used with file extensions as is for content type we just discussed. </p>
<p>Themes. Themes operate differently to both content type and file extensions in the fact that they assess the object based upon a series of key words that are detected within the actual object itself. Depending on these key words and their combinations will determine the risk level assigned to the object. The field types for themes are theme title, minimum keyword combinations, risk, and enabled. Examples of these titles are ‘American Express Credit Card Keywords’ or ‘Audit Keywords’, allowing you to determine the types of words that are being assessed. You can click on any of these entries to look at the actual words that are being scanned for. In the case of ‘Audit Keywords’, these are audit, risk assessment, security, and evaluation. The minimum keyword combinations is a numerical value showing how many of these keywords must be present in the object to dictate the risk level. </p>
<p>Regex. Like themes, regex or regular expression classifies content based on the actual content within the object. These regular expressions contain a text string for describing a specific search pattern allowing Amazon Macie to look for specific data within the content to calculate its risk. If you would like to learn and understand more about regex, then you can look at the link <a target="_blank" rel="noopener" href="https://www.regexbuddy.com/regex.html">here</a>. For each object stored on S3, Amazon Macie will assign a content type, file extension, theme, and regex value before defining its final risk value. This is determined by the highest value that was detected in each of these categories. For example, you may have an Excel document containing UK Passport numbers and the risk values may be classified as follows with a content type, file extension, and theme all with a risk value of one and a regex value of five. This would give a result of five as that was the highest value obtained for that data object. </p>
<p>During Amazon Macie’s process to classify data, it also performs automatic PII classification. This uses a list of predefined metrics relating to PII which include the following and are assigned a low, moderate, or high rating which is dependent on the quantities found within the object. The PII data searched includes full names, mailing addresses, email addresses, credit card numbers, IP addresses, driver license IDs, national identification numbers, and birth dates. </p>
<p>Amazon Macie uses two methods for protecting your data using AWS CloudTrail and artificial intelligence and machine learning to assess and review historical patterns of access. Using this historical data provided by CloudTrail, Macie can detect if there is unusual behavior occurring within your account that could potentially lead to your data being compromised. These methods include the use of AWS CloudTrail events and AWS CloudTrail errors. Both of these can be accessed by the settings menu within the Macie console, along with the data classification categories. </p>
<p>CloudTrail events provides a list of CloudTrail events along with their associated risk value of the API. The fields include name, description, classification, risk, and enabled. The classification field relates to the resource that a particular API is actionable against. It’s also possible to search the events by name. As you can see in the image, I’ve searched for get, which as returned all events with get in the title. The image shows just three of the results returned. As expected, an API starting with get is likely to have a higher risk value then an API starting with list, as a get is generally a request to retrieve data which could indicate an intrusion of some kind depending on the API in question. These three get events have a rating of eight, eight, and nine, which again is marked out of 10. </p>
<p>CloudTrail errors. This looks at the different errors that are generated and reported within CloudTrail. If you perform an action within AWS and receive an error back, it is generally because you did not have permission or access to the resource, you were using the wrong credentials, or some other kind of invalid request. These are all common errors that can occur when someone is trying to access or perform a function or action against something that they shouldn’t be. So from a security awareness and assessment point of view, these are crucial. As a result, the minimum risk value to these errors is five, with some reaching a risk value of 10, the highest possible value. </p>
<p>That now brings me to the end of this lecture covering the classification and protection of data within Amazon Macie.</p>
<h1 id="Multiple-Accounts-with-Amazon-Macie"><a href="#Multiple-Accounts-with-Amazon-Macie" class="headerlink" title="Multiple Accounts with Amazon Macie"></a>Multiple Accounts with Amazon Macie</h1><h2 id="Resources-Referenced-5"><a href="#Resources-Referenced-5" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-integration.html#macie-integration-member">CloudFormation Templates</a></p>
<h2 id="Lecture-Transcript-8"><a href="#Lecture-Transcript-8" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this very quick lecture where I want to demonstrate how you can use a single AWS Master account to gather data for multiple AWS accounts that you may have, to gain a full understanding of your business risk value based on everything that I’ve discussed so far. To do this, I shall perform a quick demonstration to explain how to complete this process. </p>
<p>OK so, to carry out this demonstration, I’m going to need two AWS accounts. I’m going to need a Master account, and also a second account that’ll be used as a Member account. And what I’ll do, I’ll add the Member account to the Master account, then all the dashboard findings will be sent through to the Master account rather than having to view Macie across all your different AWS accounts. </p>
<p>So I’ve logged into one of my AWS accounts, and this is going to be my Member account. Now the first step I need to do, is run a cloud formation stack. Now this cloud formation stack is provided by AWS, and it is different from the cloud formation stack that you first use to enable Macie, because that was to enable Macie as your Master account, whereas here, we want to enable Macie as a Member account. These stacks can be found on the following webpage, and I’ll also put a link to them within the transcripts as well. </p>
<p>So I’m going to run the cloud formation template for the Virginia region. Now again it’s very simple on this select template screen. All I need to do is go across to Next. I’ll leave the stack name as MacieServiceRolesMembers, and here I need to enter the Master account. So my Master account number is as follows, once I’ve put that Master account number in, I can then click on Next. Now here I can change a number of options if I need to, but I’m just going to leave all these as the default, then click on Next. And then at the Review screen, we just need to acknowledge the fact that AWS cloud formation might create IAM resources with custom names. So that’s just a simple checkbox, and then you scroll down, and go across to Create. </p>
<p>This will then go ahead and run the cloud formation stack. Just refresh the page there. We can see here the stack name is the MacieServiceRolesMembers stack, and it’s currently in progress. OK, now that’s complete, what I’ll do, I’ll go across to my other AWS account, which will be my Master account. I then open up Amazon Macie and I’ll add this Member account into my Macie console on my Master. </p>
<p>OK, so I’m now in my Master account, so if I go across to Amazon Macie. And then on the left hand side, if I go down to Integrations. And on the Accounts page here, we can see currently that we don’t have any Member AWS accounts. So, to add the account we click on the Plus across this side, enter the Account ID. Now that’s the Account ID of the Member account where we’d just run the account formation script. Then click on Add Accounts. And then we can see that it’s now been approved. So if we click on Close, and we can check our Member AWS account section here, we can now see that, that account is now a Member account, and up here is our Master account, which is the account that I’m currently logged into. So if you have multiple AWS accounts, you’ll have one as your Master account, where you set it up and enable it using the cloud formation script that we discussed earlier in this course, and if you need to add Member accounts, then you need to use a different cloud formation script, which will configure that Macie account as a Member account. And once you have those Member accounts configured, you then go to your Master account and simply add them here. </p>
<p>That brings me to the end of this lecture. Coming up next will be a summary of all the key points taken from the previous lectures.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><h2 id="Lecture-Transcript-9"><a href="#Lecture-Transcript-9" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello, and welcome to this final lecture within this course covering Amazon Macie. Hopefully you should now have a better understanding of Amazon Macie, and how it can be used to enhance your security level within your AWS account. Specifically across your data being stored in Amazon S3. The abilities of being able to automatically classify data and its potential risk value, and being able to identify security loopholes and potential exposures with your business critical data is invaluable. Amazon Macie provides a means of ensuring you have a way of enabling compliance on different levels. For example, it can be used as a service to help you enable compliance regulations to be met by GDPR. Through the use of alerts, metrics, deep analysis, best practices and customization, you can use Amazon Macie to meet stringent compliance needs within your business. </p>
<p>I now want to review and highlight some of the key points taken from each of the lectures within this course. I started by focusing on what Amazon Macie was and in this lecture we learnt that the main function of this service is to provide an automatic method of detecting, identifying, and also classifying data that you are storing within your AWS account. It’s backed by machine learning to detect access patterns and look for unusual or irregular activity. Findings are presented within a dashboard which can trigger alerts, and Amazon Macie, automatically and continuously monitors data in S3. Natural language processing methods are used to help classify data types and content. Objects are assigned a risk value based on data classification. Amazon Macie can monitor and discover security changes governing your data, and it can detect sensitive and security focused data, such as API keys, secret access keys, in addition to PII and PHI data. It can also detect changes to security policies and access control lists, and alert against unusual user behavior . This all helps you to maintain compliance requirements as needed. </p>
<p>Following this lecture, I looked at how to enable the service and associate your S3 data. In this lecture I explained that your AWS account needs to meet two requirements before you can enable Macie. You need to check the existence of IAM roles, specifically the AWS Macie service customer setup role. And to check that AWS CloudTrail is enabled within your AWS account. Point one here can be implemented through the use of CloudFormation templates provided by AWS. And point two simply requires that you create a trail within CloudTrail. When both requirements are met, you can then enable the service. You can then associate your Amazon S3 buckets with Macie via the integrations menu in the console. During this particular lecture, I provided a demonstration on how to do this. </p>
<p>Next I focused my attention on the different types of alerts generated and that are available with Amazon Macie. By default, Macie is pre-configured with a wide range of alerts based on security best practices and the sensitivity of data that the service will check against. Macie offers the ability to create custom alerts. These alerts exist as two different types. Basic, which consist of prebuilt alerts that come with Amazon Macie, and also custom alerts. Predictive alerts look at the behavior of your AWS account to automatically identify activities that sit outside the realms of normal operations. Alerts displayed in the console show summarized details, however these details can be expanded by clicking on the alert itself. The alert summary shows information allowing you to respond to the alert appropriately with the findings given. The alert detail section offers a whole host of additional information retrieved by CloudTrail events. It’s possible to whitelist users for specific alerts that are identified. The severity of an alert can either be informational, low, medium, high, or critical. I also provided a demonstration on how to create your own alerts. </p>
<p>Following this lecture, I discussed the Amazon Macie dashboard. In this lecture, we learnt that the Amazon Macie dashboard is the central hub of information that is collated, monitored and classified through Amazon CloudTrail logs and any services associated to Macie, such as Amazon S3. The dashboard has four metric boxes at the top of the page. Critical assets, this metric defines as a percentage how many of your assets have been identified as high risk, which is anything with a risk value of eight, nine, or 10. Total event occurrences metric. This relates to your Amazon CloudTrail logs and calculates number of API calls that Amazon Macie has monitored as a part of the security analysis of your infrastructure. Total user sessions is a count of user sessions which Amazon Macie has processed. And total users shows the number of users that have been identified by CloudTrail data. The bottom of the dashboard is used to present a number of different views in graphs, charts and statistics of monitored data. These being S3 objects for the selected time range. This displays the S3 objects within a time range at a minimum risk level. S3 objects, this metric shows your monitored S3 objects grouped together by Amazon Macie themes. S3 objects by PII. This shows PII data grouped by priority and type. S3 objects by ACL. This groups S3 objects by their ACL URIs, display names, and permission levels. High-risk CloudTrail events and associated users. This metric relates to the top 20 high-risk CloudTrail events detected in the last 60 days. High-risk CloudTrail errors and associated users. This displays the errors resulting from API actions detected in the CloudTrail logs. Activity location, this represents the global map showing the locations of activity of actions that Amazon Macie is monitoring and analyzing. CloudTrail events, this identifies all CloudTrail events monitored by Macie. Activity ISPs, this records the ISPs that have been used by users. And finally, CloudTrail user identity types. This groups users detected by their identity type, such as an IAM user. </p>
<p>Once I had reviewed the Macie dashboard, I looked at the users section. In this lecture I covered the following points. Users are grouped by platinum, gold, silver and bronze. Which represents their perceived level of risk based on their history of API calls. Platinum poses the highest risk, and bronze the lowest risk. Additional data can be generated by selecting the user, which will present you with a condensed version of the dashboard displaying metrics only relating to that particular user. User identity types are defined from CloudTrail logs that it monitors and analyzes via the user identity element. User identity types include, root, which means the request was initiated by AWS root account. IAM user, the request was made my an IAM user. Assumed role, defines that the request was made by credentials that were temporarily assumed by the assumed role API. Federated user, the request again was made with temporary credentials, but using the STS GetFederation Token API. AWS account, the request was made by a different AWS account. And AWS service, the request was made by an AWS service. </p>
<p>Following this lecture, I then focused on the research feature of Amazon Macie. Here I covered the following points. The research function allows you to create your own queries against all of the data that Amazon Macie has collected and monitored via AWS CloudTrail and Amazon S3. It enables you to perform deep dive analysis of your data that relates to your specific requirements within your business using the query parser. You can filter the results based on CloudTrail data, S3 bucket properties, and S3 objects. You can also filter on the number of results found, along with a date range filter. Research is integrated with all elements of Macie, for example the dashboard and alerts. An example of an entry on the query parser is as follows. Which will only display the results with a get bucket policy API call is used by IAM users only. It’s possible to save your favorite queries within a favorites list, and you can also create your queries and have them saved as a custom alert. </p>
<p>Following this lecture, I then moved my focus on to explain how Amazon Macie classifies your data to assign its risk value. In this lecture, I explained that every data object within the Amazon S3 bucket automatically receives a perceived level of risk based on a classification process. There are four classification categories. Content type allows Macie to detect the type of file that is being stored on S3. For example, a binary file, a document, or source code object. File extensions. This looks at the file extension of the object to ascertain its risk value. Themes. This assesses the object based on a series of key words that are detected within the actual object itself. And Regex, regular expression. This classifies content based on content within the object using a text string for describing a specific search pattern. Each S3 object has a risk value for each category. The object’s final risk value is given by the highest value received between the four categories. Amazon Macie also performs automatic PII classification using a list of predefined metrics. Amazon Macie uses AI and machine learning to assess and review historical CloudTrail data access patterns using CloudTrail events and CloudTrail errors. CloudTrail events provide a list of CloudTrail events along with the associated risk value of the API. CloudTrail errors looks at the different errors that are generated and recorded within CloudTrail. </p>
<p>The final lecture was a demonstration where I showed you how to use a single AWS master account to gather data for multiple AWS accounts. </p>
<p>That now brings me to the end of this lecture, and to the end of this course. You should now be able to effectively use Amazon Macie to help you protect your data, and to meet and maintain governance and compliance regulations within your environment. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:&#x73;&#x75;&#112;&#x70;&#111;&#x72;&#x74;&#x40;&#99;&#108;&#x6f;&#x75;&#x64;&#97;&#x63;&#97;&#x64;&#101;&#109;&#x79;&#x2e;&#x63;&#x6f;&#x6d;">&#x73;&#x75;&#112;&#x70;&#111;&#x72;&#x74;&#x40;&#99;&#108;&#x6f;&#x75;&#x64;&#97;&#x63;&#97;&#x64;&#101;&#109;&#x79;&#x2e;&#x63;&#x6f;&#x6d;</a>. You feedback is greatly appreciated. Thank you for your time, and good luck with your continued learning of cloud computing. </p>
<p>Thank you.</p>
<h1 id="2What-is-Amazon-Macie"><a href="#2What-is-Amazon-Macie" class="headerlink" title="2What is Amazon Macie?"></a>2<strong>What is Amazon Macie?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/webinars/establishing-privacy-program-gdpr-compliance-beyond-62/">GDPR Compliance Webinar</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">AWS Regional Product Service Table</a></p>
<h1 id="3Enabling-and-Associating-Amazon-Macie-with-Amazon-S3"><a href="#3Enabling-and-Associating-Amazon-Macie-with-Amazon-S3" class="headerlink" title="3Enabling and Associating Amazon Macie with Amazon S3"></a>3<strong>Enabling and Associating Amazon Macie with Amazon S3</strong></h1><p><a target="_blank" rel="noopener" href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=MacieServiceRolesMaster&templateURL=https://s3.amazonaws.com/us-east-1.macie-redirection/cfntemplates/MacieSer">US East (Virginia) CloudFormation Template</a></p>
<p><a target="_blank" rel="noopener" href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=MacieServiceRolesMaster&templateURL=https://s3-us-west-2.amazonaws.com/us-west-2.macie-redirection/cfntemplate">US West (Oregon) CloudFormation Template</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-setting-up.html#macie-setting-up-enable">CloudFormation Templates for all regions available</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-automation-how-to-use-cloudformation/">Course: How to use Cloudformation for Automation</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-aws-cloudformation/">Course: Advanced use of AWS CloudFormation</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/deploy-wordpress-cloudformation-17/">Lab: Deploy Wordpress using CloudFormation</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/">Course: AWS Cloudtrail: An Introduction</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/monitoring-aws-cloudtrail-events-amazon-cloudwatch-76/">Lab: Monitoring AWS CloudTrail Events with Amazon CloudWatch</a></p>
<h1 id="4Alerts"><a href="#4Alerts" class="headerlink" title="4Alerts"></a>4<strong>Alerts</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-research.html#macie-query">Constructing Queries in Amazon Macie</a></p>
<h1 id="7Research"><a href="#7Research" class="headerlink" title="7Research"></a>7<strong>Research</strong></h1><p><a target="_blank" rel="noopener" href="https://lucene.apache.org/core/2_9_4/queryparsersyntax.html">Apache Lucene Query Parser Syntax</a></p>
<h1 id="8Classifying-amp-Protecting-Data"><a href="#8Classifying-amp-Protecting-Data" class="headerlink" title="8Classifying &amp; Protecting Data"></a>8<strong>Classifying &amp; Protecting Data</strong></h1><p><a target="_blank" rel="noopener" href="https://www.regexbuddy.com/regex.html">Regular Expressions (Regex)</a></p>
<h1 id="9Multiple-Accounts-with-Amazon-Macie"><a href="#9Multiple-Accounts-with-Amazon-Macie" class="headerlink" title="9Multiple Accounts with Amazon Macie"></a>9<strong>Multiple Accounts with Amazon Macie</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-integration.html#macie-integration-member">CloudFormation Templates</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:18" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:18-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:02:20" itemprop="dateModified" datetime="2022-11-19T23:02:20-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:15" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:15-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:38" itemprop="dateModified" datetime="2022-11-19T22:53:38-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course covering the AWS security service, Amazon GuardDuty, which was announced during AWS re:Invent 2017. This course will explain what the service is and guide you through its features and configuration.</p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy, specializing in AWS <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon web services</a>. Feel free to connect with me with any questions, using the details shown on the screen, or tentatively, you can always get in touch with us, here, at Cloud Academy, by sending an email to <a href="mailto:support@CloudAcademy.com">support@CloudAcademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>As a pre-requisite to this course, you should have a basic understanding of the fundamentals of AWS, along with an awareness of different security measures and mechanisms that are offered by different AWS services, such as within IAM, specifically IAM Policies.</p>
<p>This course has been designed for those who are in a role of a security consultant or specialist, security analyst, security auditor, Cloud architect, or Cloud operational support analyst. This would also be valuable to anyone looking to learn more about AWS Security and threat detection within AWS.</p>
<p>This course has been designed to lead someone who is new to Amazon GuardDuty through to becoming someone who has a sound understanding of the service. The lectures have therefore been constructed as follows:</p>
<ul>
<li>What is Amazon GuardDuty? This lecture focuses on explaining what the service is and the function that it provides.</li>
<li>Components and configuration: This looks at the different components and elements that make up a service. This lecture also includes a demonstration on how to configure a service.</li>
<li>Managing multiple accounts: If you have multiple accounts, then this lecture will explain how you can configure Amazon GuardDuty to work across all your AWS accounts that you have.</li>
<li>Managing permissions: As with any service, you need to ensure you have the correct permissions configured for both the service with the service-linked role, and also your operational staff who will manage the service. This lecture looks at the different permissions required.</li>
<li>Understanding Amazon GuardDuty findings: Here, we’ll look at how to view the findings generated by Amazon GuardDuty and the different components of these findings to help you remediate any issues.</li>
<li>Benefits to the enterprise: This lecture focuses on how Amazon GuardDuty can be of benefit to your business.</li>
<li>Costing: Understanding that cost is important when using the features of a new service. This lecture examines those costs and provides an example.</li>
<li>Partner offerings: A number of different third parties offer services that seamlessly interact with GuardDuty. So, here, I will look at a couple of these examples.</li>
<li>Summary: Finally, I provide a summary lecture, which will highlight the main points from each lecture.</li>
</ul>
<p>There are a number of key objectives to this course. These being: to understand what Amazon GuardDuty offers as a service, you’ll understand how to manage and configure the service for single and multiple accounts, you’ll understand how to implement the correct permissions to both enable and manage the service, you’ll have an awareness on how to manage and resolve findings generated, and you’ll be able to explain how Amazon GuardDuty can play an important role within your organization.</p>
<p>Feedback on our courses, here at Cloud Academy, are valuable to both us, as trainers, and any students that can take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you can contact us at <a href="mailto:support@CloudAcademy.com.">support@CloudAcademy.com.</a></p>
<p>That brings us to the end of this lecture. Coming up next, I’ll be explaining exactly <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/what-is-aws-amazon-guardduty-1/">what Amazon GuardDuty is</a>.</p>
<h1 id="What-is-AWS-Amazon-GuardDuty"><a href="#What-is-AWS-Amazon-GuardDuty" class="headerlink" title="What is AWS Amazon GuardDuty?"></a>What is AWS Amazon GuardDuty?</h1><p>Hello and welcome to this lecture where I want to provide an introduction to the service, explaining what it is, what it does, and the problem that it solves.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> still treats security as its number one priority across its public cloud. They know that without adequate security techniques, mechanisms, and measures in place to safeguard and protect their customers and their data, their customers will not have the confidence to use their services. Cloud security can still be seen as one of the main reasons that companies are slow to adopt cloud technology from a public cloud provider such as AWS. Much of this can be attributed to the lack of cybersecurity skills within an organization. Not having the knowledge and ability to confidently implement a high level of security within the cloud can be damaging to an organization.</p>
<p>Security is an ongoing development process. As technology changes, so do threats and risks against that technology. With this comes a need for newer, more advanced and powerful tools to protect against these threats, and AWS is at the forefront of this development.</p>
<p>Prior to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a>, there were 10 other services that sat within the security, identity, and compliance category of the AWS Management Console, making this service the 11th. Each security service has a very specific function and benefit that it provides to assist and help customers control, manage, and operate a secure and safe environment within the cloud. The services within this category already cover a wide scope of features and security mechanisms, so how does this new service differ from the rest that already exist?</p>
<p>Amazon GuardDuty is a regional-based intelligent threat detection service, the first of its kind offered by AWS, which allows users to monitor their AWS account for unusual and unexpected behavior by analyzing AWS CloudTrail event logs, VPC flow logs, and DNS logs. It then uses the data from logs and assesses them against multiple security and threat detection feeds, looking for anomalies and known malicious sources, such as IP addresses and URLs.</p>
<p>The service itself is powered by machine learning, and this allows the service to continuously evolve by learning and understanding operational behavior within your infrastructure. Amazon GuardDuty then uses this data to look for erroneous patterns within your AWS account that could indicate potential threats to your environment. These threats could be behavioral based, where a resource has been compromised by an account or credential exposure, unexpected API calls that sit outside security best practices, or even communications from suspicious sources.</p>
<p>Using different threat detection feeds, some generated from public sources and some by AWS, Amazon GuardDuty provides automatic and continuous security analysis for safeguarding your entire AWS environment. Any findings generated by the service are presented and issued with a priority level that enables you to investigate the issue further to ensure that your environment is not compromised and exposed unnecessarily. Amazon GuardDuty is very simple to activate within your account, and unlike other more traditional threat detection mechanisms, there is no need to install any agents or software on your resources, meaning that this is a very scalable and flexible security tool to have enabled.</p>
<p>With this in mind, it’s also possible to link your AWS accounts together to perform a threat detection layer across all of your accounts. In addition to this, the service itself operates entirely on AWS infrastructure, providing zero impact of the performance of your own existing resources within your account. Threat detection is key in the defense against a security breach. Having the ability to respond to a potential threat as it is detected significantly reduces the chances of a breach. Cyber criminals are using more advanced techniques to infiltrate networks and hosts using zero-day threats, and Amazon GuardDuty is the latest service to help defend against these attacks.</p>
<p>That now brings me to the end of this lecture. Coming up next, I will be discussing the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/components-and-configuration-1/">components</a> of the service and how it fits together.</p>
<h1 id="Components-and-Configuration"><a href="#Components-and-Configuration" class="headerlink" title="Components and Configuration"></a>Components and Configuration</h1><h2 id="Resources-mentioned-in-this-lecture"><a href="#Resources-mentioned-in-this-lecture" class="headerlink" title="Resources mentioned in this lecture:"></a>Resources mentioned in this lecture:</h2><p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail: An Introduction</a></p>
<p>AWS Resource: <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types.html#actual-types">Amazon GuardDuty Finding Types</a></p>
<h2 id="Lecture-Transcript"><a href="#Lecture-Transcript" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture. Now we have an understanding of what the service is, I want to talk about the components that make up Amazon GuardDuty and its configurable options.</p>
<p>Firstly, let me look at the data sources that the service uses to perform its analysis. As mentioned in the previous lecture, the service uses three data sets to monitor your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account for threats, these being AWS CloudTrail Event Logs. These logs are generated from the output of the CloudTrail service in a JSON format and they hold all of the information and data relating to API calls that have been captured within your account. If you’d like further information on AWS CloudTrail, then please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">here</a>, which covers the service in detail.</p>
<p>VPC Flow Logs, these logs capture and store network traffic information flown into an out of your network interfaces from instances within your VPC. They are often used to troubleshoot networking issues for instances and can be used as a security tool by monitoring what traffic is reaching your instance.</p>
<p>DNS Query Logs, these logs contain queries that DNS resolvers forward to Amazon Route 53 and they can include information such as the domain and subdomain that was requested, a timestamp of the request, the DNS record type, and the DNS response code. If you’re not currently running these logs or have them configured within your account, then you need not worry. When you enable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a>, it will automatically make these logs available to the services itself for analysis. However, it doesn’t manage these logs in anyway. If you want management of these logs, then you’ll need to configure this through the relevant service dashboard or API.</p>
<p>Amazon GuardDuty also incorporates machine learning. This allows the service to learn and adapt to what classes as unusual behavior within your account over time to then highlight it as a potential threat. This is an ongoing process. The more you work with your account, the more Amazon GuardDuty will be able to distinguish between intended operational changes and that of malicious or unusual behavior that sits outside of your normal parameters of operation. In addition to the analysis that is undertaken against the various logs that I’ve already discussed. </p>
<p>It’s also possible to upload your own list of trusted IPs and threat list. Any IP information that is added to the trusted IP list is white listed. This simply means that GuardDuty will not generate findings based on these IP addresses. They are trusted and known IP addresses. It’s worth mentioning, but you can only have one active trusted IP listed any time. You can also include your own list of threats as well based on IP information. This list will contain a known list of malicious IP address or networks, but you want to ensure Amazon GuardDuty generates findings for if any traffic is detected with this information. Unlike the trusted IP limit list of one, you are allowed six threat list to be simultaneously active within guardduty at any one time. To add either a trusted IP list or threat list is a very simple process. You simply need to provide three bits of information from within the Amazon GuardDuty dashboard. Firstly, you need to give your list a name, then provide the URL of whether source list is located on S3, and then the format of that list.</p>
<p>The main functions of Amazon GuardDuty is of course to detect any potential threats within your environment. When a threat is found, it is labeled as a finding within the GuardDuty dashboard, allowing you to take appropriate actions against them to resolve any security vulnerability that might exist. The content of the finding itself contains a lot of useful information and is essentially broken down into five parts. The finding summary, the resource affected, the action, actor, and additional information. Let me just run through each of these elements so you have a brief understanding of the complete finding.</p>
<p>The summary of the finding contains some key data, including the finding type, the severity, the region, the account ID, resource ID, time of detection, and depending on the threat detected, it can also contain information on which threat list was used to detect the finding. Much of this data is self-explanatory. However, I want to explain more about the finding type as this is a very useful piece of data when trying to ascertain what the finding relates to.</p>
<p>The finding type is a single phrase composed of concatenated data, which follows a specific syntax that defines a potential threat event that is detected. For example, the finding type of unauthorized access, EC2 &#x2F; SSH Brute Force shows that an EC2 instance has been involved with an SSH brute force attack. This makes it very easy to quickly identify the nature of the threat from the summary screen. As I said, this finding type is defined by a set syntax, which is as follows.</p>
<p>Using our previous example, we an see the finding type with the above syntax. As you can see, not every finding type will contain all elements of the four syntax. For a complete list of the available finding types and what they define, see the AWS documentation <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types.html#actual-types">here</a>.</p>
<p>Resource affected, this section is dedicated to providing information and by the affected resource in the threat that has been detected. Depending on the finding type, the information in this section and all remain sections can vary.</p>
<p>Action. The action section provides information relating to the action that was carried out, that resulted in the threat being detected.</p>
<p>Actor. The actor section contains data relating to the source of the detected threat, such as geographical information, IP address, port and domain.</p>
<p>Additional information. Finally, the additional information section may contain information such as which threat list was used to detect the threat and if the activity was considered to be unusual compared to historical data. Each finding is associated with the severity level and score. The score value will affect the severity. As of writing this course, the severity is set as follows. High is labeled is seven to 8.9, medium is four to 6.9, and low is 0.1 to 3.9. These different severity levels allow you to quickly identify which findings you should act upon first as a priority.</p>
<p>Any finding that is marked as high should be investigated immediately as it assumes that a security breach has occurred and the resource in question has been compromised which poses a significant threat to your infrastructure. Remediation of this finding should be your first priority before the threat extends further within your account with additional malicious activity.</p>
<p>A medium severity will genuinely indicate that GuardDuty had to take the suspicious activity within your account against a specific resource. Although not as critical as high, this level of finding should still be investigated as soon as possible as it could be the beginnings of a greater threat and could soon escalate to a high if left unchecked.</p>
<p>Low severity findings do not require you to take any immediate action. These are threats that were detected and blocked before any issues arose within your environment. However, it is still worth looking at any low findings to see if there are improvement that can be made to prevent it from happening again.</p>
<p>I now want to perform a quick demonstration introducing you to the service itself. I’ll show you how to enable the service, configure trusted IP and threat list, and I’ll walk you through each of the screens within the Amazon GuardDuty dashboard. So let’s get started.</p>
<p>Okay, so I’m signed into my AWS management console, and what I want to do first is go to Amazon GuardDuty and we can find that down under Security, Identity, &amp; Compliance. And if this is the first time that you’re using the service, you’ll be presented with this splash screen, and it just enables you to get started and gives you a quick overview of exactly what it is. So, let’s get started.</p>
<p>Firstly, we need to enable the service. And like I say, it’s a very simple process. All we need to do is to click enable guard duty, And when we do this, as it says here, when you enable GuardDuty, you grant GuardDuty permissions to analyze AWS CloudTrail Logs, VPC Flow Logs, and DNS query logs to generate security findings. And this will set up a service role permission which has this access here, but I’ll go more into permissions in a later lecture to explain these service roles et cetera.</p>
<p>So if I click on Enable GuardDuty. It’s as simple as that. So we now have GuardDuty running within this account within this region. And at the moment, we don’t have any findings which is absolutely fine. I don’t really have many resources running in this account. It’s just a test account. So if I just take you through the dashboard and each of the screens and we can just take a look and get a feel for the service.</p>
<p>So on the left-hand side, you can see we have three different headings. We have Findings, Settings, and Free trial. So let’s start at Findings. In the screen, it will show any findings that have been found by Amazon GuardDuty and it will list under here as a finding and when it was last seen, etc. Like I say, at the moment, we’ve only just enabled the service. I don’t really have any resources running, so there is no findings at the moment. If we go into Archived, this will show a list of archived findings that you’ve had, again, with the same columns.</p>
<p>If we go down to General under Settings, here it talks about the permissions of the service role that GuardDuty uses to monitor your resources. And like I say, I’ll dive deeper into permissions in a later lecture, but this is where you can just see the service role permissions that it has and the trust relationships as well. On the CloudWatch events, GuardDuty does support CloudWatch and if you click on this link here, it will tell you how to configure this.</p>
<p>Under Sample findings, this is quite useful. So, sample findings help you visualize and analyze the finding types that GuardDuty generates. And if we want to list a number of sample findings for us to kind of get familiar with the console and the different findings that it has. Then we can simply click on Generate sample findings. And now if we go back to our finding settings under Current, we can see that GuardDuty has enabled a number of different sample findings for us, and they’re indicated here as samples. Don’t worry, these aren’t real findings in your account, these are all samples, and we can see that we have 33 samples.</p>
<p>If we look along these icons here, we can see that one of them has a high severity and 32 of them have a medium severity. And if you click on each of these, you can filter on those to identify exactly where the finding is. And if you want to remove the filters, just simply click on the X here. So if we keep going through these settings on the left. So that’s Generate sample findings. Underneath this, it gives you two settings, one to enable you to suspend GuardDuty and another to disable GuardDuty.</p>
<p>When you suspend GuardDuty, it stops monitoring your AWS environment, and it won’t generate any new findings at all. But any existing findings you have will remain in the console. If however you disable GuardDuty, then again monitoring will stop and it won’t generate new findings and you’ll also lose all your existing findings as well. If you want a copy of those findings before you disable it, then it’s best to export that data before you do so.</p>
<p>Next, if we go down to Lists, now this is where we can perform list management of your trusted IP lists and also your threat list as well. Okay, so let’s add a trusted IP list and a threat list whilst we’re here. So if I click on Add a trusted IP list, specify the list name, let’s just call this TrustedIP, enter the location, and I’ve called it TrustedIP.txt. So that’s my bucket. And if I specify the format just as plain text. And you need to click on I agree to accept and agree to the GuardDuty service terms. And then click on Add list. There we’ve added the trusted IP list.</p>
<p>If we move down to a Threat list, we can do the same. So let’s just call this ThreatList in the same bucket, and I’ve called it ThreatList.txt. Specify the format, plain text again. I agree. Add list. Then we got it. So now we have our trusted IP list and also our threat list that GuardDuty will use. And if you had a number of threat list, then you can select different ones to be active, etc. So it’s very simple to add your trusted IP list and your threat list as you can see.</p>
<p>If we go down to Accounts on the left-hand side, this is where you can set up and manage multiple accounts. So you can use a master GuardDuty account. And then if you have other AWS accounts, you can add them as member accounts, and then view all the findings from those member accounts in this one master account So it makes management a lot easier, and I’ll talk about this in a further lecture when I talk about multiple accounts. So I’ll go into that a lot deeper then.</p>
<p>Now if we look at Free trial, we look at the details on the Free trial. When you first enable Amazon GuardDuty, you get 30 days free of the service. And using the screen here, you can see how many events have been monitored by CloudTrail logs and how many bytes we’re using through VPC Flow logs and DNS logs. And it’ll give you an estimate in cost on how much that would actually cost once you’re outside of your 30-day free trial. So it gives you a good estimation on understanding how much this service would cost you over a typical month. So like I say, when you first enabled the service, you get 30 days free, and you can use this screen to kind of get an estimation on how much it would cost you going forward, which I think is a really good idea.</p>
<p>And then finally, on the left-hand side, we have Partners, and this will take you off to a list of Amazon GuardDuty partners. That’s essentially it for the console. Like I say, it’s a very simple service. There’s not much to it at all.</p>
<p>Before I finish the demonstration, if we go into one of these findings, pick this one for example, we can see here that we have a brief description of what the finding is. We can see that an instance with an unusual type was launched by a IAM principal, has a severity, and tell us the region it was in, how many. The account ID that it came under and any resource information, and also the treat list name as well. Again, remember these are just samples. It tells us about the resource effective and the access keys and IDs and usernames, what action was used, like the API call and the service name, and also some geographic information as well about where that was initiated from. So when you select the finding, you can get quite a lot of information from this to help you resolve the issue. And I have a later lecture coming up that dives deeper into analyzing findings as well. So I’ll talk more about these findings in that lecture. Let’s click on Close. And there you have it.</p>
<p>So, it’s very simple to enable GuardDuty. It’s imply a one click and it’s enabled for that region, and you just have a few menus on the left-hand side that are very self-intuitive.</p>
<p>That now brings me to the end of this lecture. Coming up next, I’m going to be talking about more on how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/managing-multiple-accounts-1/">manage multiple accounts</a>, so you can miss out of the dashboard. So I’ll explain how to share the findings from all your member AWS accounts and push it to one master GuardDuty account.</p>
<h1 id="Managing-Multiple-Accounts"><a href="#Managing-Multiple-Accounts" class="headerlink" title="Managing Multiple Accounts"></a>Managing Multiple Accounts</h1><p>Hello and welcome to this lecture where I’m going to explain how it’s possible to link multiple <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> accounts when using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a> for centralized management.</p>
<p>Many organizations have multiple AWS accounts for many different reasons. When using GuardDuty, it’s possible to have one of these accounts act as a master account, and then all other AWS accounts act as members. In this scenario, all the findings from the member accounts are configured to send a copy of the results to the dashboard of the master account. This is ideal for your security teams who manage multiple AWS accounts, as it allows them to view all findings in a central location, instead of having to view the findings from within each AWS account.</p>
<p>It’s important to point out that Trusted IP lists and threat lists within the member accounts are not used within the master account. The member accounts each have their own lists, which can be configured by the users within those member accounts. However, the master account does give you added control and administrative functions, such as having the ability of being able to suspend Amazon GuardDuty within its own account and other member accounts. If you wanted to disable the service, then you must first remove the member accounts from the master account, as the master account is not allowed to disable GuardDuty on its member accounts.</p>
<p>The operation of Amazon GuardDuty with the member accounts remains the same. The users can still perform the same functions from within the dashboard, and it’s still possible to view and review the findings, upload trusted IP threat lists, as well as suspend or disable the service on that member account.</p>
<p>To set up your AWS accounts in a master and membership <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/components-and-configuration-1/">configuration</a> for GuardDuty is a simple three stage process. You must first add an AWS account from within the master account, send an invitation to the member account, and then accept the invitation from within the member account. This process prevents a single account from simply adding member accounts without authorization. The member account has to accept the invitation before sharing its findings with the master account.</p>
<p>To show you how to create the link between accounts, I will now demonstrate these three steps by adding a member account to a master account.</p>
<p>Okay, so I’m at the dashboard of the Amazon GuardDuty, and what I need to do is go down on the left hand side to Accounts. Now, I’m going to have this account as my master account, and I want to invite a second AWS account that I have to be a member account. So, that member account’s findings and details will be fed through to this master account. So, the first thing I need to do is click on Add Accounts, and I need to enter the account ID of the member account, and also the associated email address of that account. Now, if you have maybe 10, 20, 30 AWS accounts, or even more, then you might find it a little laborious to add each individual account, so what you can do, you can upload a CSV file with all that information in with all the account IDs and the associated email addresses. As I only have the single account, I’m going to add, I can put that account number in, and also the associated email address. Click on Add, and at the bottom here, we can see these are the accounts to be added, and then click on next, and we see here that member accounts share the findings with you, and members must first accept your invitation.</p>
<p>So, if we go down to under the Status column and click on Invite. Now, if you wanted to, you can enter a personal message here that will go to owner of the secondary account, and I’m just going to say “Please accept.” At this point, we just need to click on Send Invitation. And that has now sent an email to the owner of the account that we added. And we can see the status is now saying “Pending.” So, if I go across to my email and take a look at the email that we’ve received.</p>
<p>Now, this is the email that I’ve received. Now, the title states that there’s action requested, and it says “The master account wants to become the AWS GuardDuty administrator for this secondary AWS account.” And we can see down here where it says the following notes were provided with this invitation, and it says “Please accept.” So, that’s that custom message that I added. Now, to view the invitation, we can click this link here, and it takes me straight to this screen, and it says you have a membership invitation, but we must enable GuardDuty before we can accept invitations. So, we’ll need to enable GuardDuty on this account first.</p>
<p>GuardDuty’s now enabled, and it states that “The following AWS accounts have requested permission <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/managing-permissions-1/">permission</a> to view and manage GuardDuty items on your behalf. You can accept only one invitation.” So, this is our master account here, and I want to accept, and then I’ll go across to Accept Invitation. And that’s it. So, this is now the member account, and it will push its findings across to the master account. So now, if I go down to Accounts on this member account, I can see that this is a member account, and that pushes its findings to this master account, so I’ve accepted the invitation there.</p>
<p>So now, if I go across to the master account. So, I’m back in my dashboard for my master account, and now if I go down to Accounts, I can see here that I have a member account added, and the status is now monitored. And that’s it. So, it’s a very simple process to achieve. So, from your account that you want to be the master, you then invite any member accounts. Those member accounts then simply accept that invitation, and from that point onwards, your master account will then monitor the status and findings of all your member accounts.</p>
<h1 id="Managing-Permissions"><a href="#Managing-Permissions" class="headerlink" title="Managing Permissions"></a>Managing Permissions</h1><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_managing_access.html">AWS Policies for Access Management</a></p>
<p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Identity &amp; Access Management</a> </p>
<h2 id="Lecture-Transcript-1"><a href="#Lecture-Transcript-1" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture where I want to talk about the different permissions required and used when using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a>, both from a user perspective and from the service itself.</p>
<p>This lecture will primarily focus on permissions to perform the following functions. How to access the Amazon GuardDuty Dashboard. How to enable the Amazon GuardDuty within a region and to manage your Trusted IP and Threat Lists. Before a user even begins to use Amazon GuardDuty specific permission are required to access the dashboard and enable the service. For example, if a user is trying to access the dashboard to enable GaurdDuty on your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account but receive an error that prevents them from doing so then it’s most likely related to their permissions.</p>
<p>If the user selects Amazon GuardDuty from the homepage of the AWS Management console the following error message may appear preventing them from access to the GuardDuty dashboard. This error indicates that user doesn’t have the relevant permissions to access the GaurdDuty service. They would need to speak to their administrator to ask them to revise their permissions for GuardDuty. However, if additional permissions were then given to that user by allowing all actions within GuardDuty using the following policy, they will then be able to access the dashboard without an issue. This does not mean however that the user has all the permissions they need to initially enable the service within the region. If the user attempts to enable the service with the above permissions alone they will receive the following error.</p>
<p>To enable the Amazon GuardDuty service a user will need specific IAM permissions that allows them to create a service-linked role that allows GuardDuty to retrieve information about some of your resources. So although the user will have full access to GuardDuty actions the user will still need these additional permissions relating to IAM and service-linked roles.</p>
<p>AWS has created an AWS management policy with the relevant permissions to allow you to enable GuardDuty within a region. The name of this policy is Amazon GuardDuty Full Access. This policy essentially allows full access to Amazon GuardDuty actions with the added permissions of being able to create the service-linked role as shown in this policy. One point to be made aware of with this Amazon GuardDuty Full Access is that it doesn’t allow you to upload a Trusted IP or Threat List as this again, requires different IAM permissions. If you do try to add a list with the Full Access policy you will receive the following error.</p>
<p>To allow a user to be able to manage your Trusted IP and Threat lists you will need to add the following permissions to the user group or role. Do remember to replace the AWS account number with your own account. With all this in mind, AWS have provided a policy that you can create as a custom policy which allows you genuine full access to Amazon GuardDuty which will allow you access to the dashboard to enable the service in all regions and perform operations within the service including updating and managing Trusted IP and Threat Lists. And this policy is as shown, however you must remember to replace the AWS account number with your own. While I am on the topic of AWS management policies you may also notice that there is another AWS management policy entitled Amazon GuardDuty Read Only Access. As expected this policy provides the user to have read-only permissions to Amazon GuardDuty allowing them to review findings.</p>
<p>Earlier I mentioned the fact that GuardDuty uses the service-linked role during the enablement of the service. This role, AWS Service Role for Amazon GuardDuty contains the following permissions. This is used to enable GuardDuty to have read-only visibility of your EC2 instances should a finding become known relating to one of these resources. If there is a finding against your EC2 instance GuardDuty can use these permissions to retrieve metadata about the resource to present in the finding it generates to help resolve the security issue and threat. This service-linked role also has an associated trust relationship which allows Amazon GuardDuty to adopt this role. Again, when this service is enable against a region these permissions are granted to Amazon GuardDuty automatically.</p>
<p>As with other AWS services you can be very specific with what actions a user, group or role can perform with Amazon GuardDuty by creating custom IAM policies. For further information and details on how to create custom IAM policies please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">here</a>.</p>
<p>That now brings me to the end of this lecture. Coming up next I shall be diving into <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/understanding-amazon-guardduty-findings-1/">GuardDuty findings</a> in more detail.</p>
<h1 id="Understanding-Amazon-GuardDuty-Findings"><a href="#Understanding-Amazon-GuardDuty-Findings" class="headerlink" title="Understanding Amazon GuardDuty Findings"></a>Understanding Amazon GuardDuty Findings</h1><p>Hello, and welcome to this lecture. I’m going to take a deeper look at the findings generated by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a>, and how to look into the details further to help you remediate the issues you find.</p>
<p>In an earlier lecture, I briefly provided an overview of the findings section, however, in this lecture I want to look at these findings in a bit more detail and also the findings area of the dashboard to show you some additional features that it has. To do this, I feel it would best to demonstrate this form within the management console, so let’s take a look.</p>
<p>Okay, so I’m at the dashboard of Amazon GuardDuty. And, as you can see, I have a number of sample findings, which is what we went through earlier in a previous demonstration. So, let’s take more of a look around this section of the findings dashboard.</p>
<p>On the far left-hand side of this table you can we have a number of check boxes, and we can select individual findings. Now, what we can do with these selected findings, is we can go to actions at the top here, and either archive those findings or export them. So let’s just run through each of those. If we click on archive, it says our findings have now been archived. So, if we go over to the left-hand side and select archive, we can see those findings that we archived there. Now you might do this for ease of management, or for general housekeeping, just keepin’ a record of the findings that you’ve had. And if you want to select all findings in a table, you simply click on the top check box, there. Pretty standard stuff. I’m going to move those findings back into current.</p>
<p>Okay, and the other option we had under actions was export. And this will export the JSON data for all of those findings. And you simply click on download and it will export the JSON file. If you need to refresh your findings list, there’s the refresh button at the top, here, simply click on that and it’ll refresh your findings.</p>
<p>Over on the right-hand side, at the top, we have indicators that let you know how many findings are of a particular severity. So, at the moment, I have zero findings that are low severity, 32 findings that are medium and 1 finding that is high. That is a very quick way to have a look at your dashboard to see your most critical findings, and you can simply click on each of these to filter on these findings. So, that’s the high, and there’s all your medium, and obviously, we don’t have any low. You can add additional filters to your findings. So, for example, if we looked at this top finding, here, that would bring up all the details for this finding. Now, if there’s any hyperlinks on this screen, as you can see here with the plus or minus or the actual resource id, here or the instance, if you click on the plus, then that will add that as a filter. As you can see, here, in this row we have a filter row with severity of medium. Now I can select, exclude that, or currently, it’s included.</p>
<p>So, if we go back into that same finding and I want to filter on this account id only, I can click on the plus, and again add it into the filter. And, one more, if I filter on resource type of instance. And now that’s filtered all the findings that have a severity of medium, with this account id, and this resource type. Now you can see that it now only showing 19 of the total 33 findings. If you like, you can save this as a set filter. So, I’m just going to call this, instance. If I remove all my filters now, I can then revert back to that filter at anytime by clicking on the dropdown box, clicking on the name and it will bring up the filter. So, you can set up many different filters if you have different accounts, especially if you’re filtering for member to master accounts or you can filter on resource types, or severities. Anything that has a hyperlink within the details of that finding, you can add a filter against. For example, again here, an action type of DNS_REQUEST, so if I go ahead and add that, we now have only eight findings of the 33 that match this filter.</p>
<p>If you wanted to exclude any of these filters, then just simply hover over it and click on exclude. So now a filter on all findings with a medium severity, under that account id that do not include a resource type of instance. You can either include or exclude filters as you see fit. Let’s just clear those filters, just by clicking on this x, here.</p>
<p>So, let’s now take a closer look at a couple of these sample findings. Let’s take a look at this one here, let’s do a Bitcoin. When you click on a finding, it will open up this additional window which gives you lots more information about the finding. At the very top, here, we have the finding type, which just gives us a breakdown of what the finding’s relating to. And then, from here, we can see that this EC2 instance is querying a domain name that is associated with Bitcoin-related activity.</p>
<p>Now, unless you’re using this EC2 instance to mine for Bitcoin, then it’s pretty fair to say that this instance has been compromised in some way or another. So, you could either take a look at that instance and see if it has any software on it that it shouldn’t do, any kind of malware or anything like that. Failing that, it’s probably best to terminate that instance and then set up a new EC2 instance. And then, when you’ve done that, just make sure you’re applying best practices to harden that instance against any kind of security threats using the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> security best practices.</p>
<p>So, looking more at this window, here, we can the Severity of the finding, and it’s currently set as Medium, the Account ID that this finding is associated with, and that’s helpful if you have <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/managing-multiple-accounts-1/">multiple accounts</a> feeding through to a master account. We can see when this finding was last seen, two days ago. The Region it was in, and also the Resource ID that is affected by this, and how many times this finding’s come up. And if we scroll down to the Resource affected, this gives us a bit more information on the resource itself. So we can see the role of this resource was a TARGET, Resource type is set as an Instance and it gives us the Instance ID. And we can click on that instance id and it will take us into the EC2 management console direct to that instance. As this is just a sample, made-up instance id, it won’t lead me anywhere if I click on it. It will take me to the EC2 console, but it’ll explain that it can’t find the instance.</p>
<p>If we go down to Action, this gives us the Action type of the threat which was a DNS_REQUEST, and as we know, the instance was drawn to a query, a domain name that’s related to Bitcoin mining. And then down to Actor, this gives us the actual domain, and then in this particular finding there wasn’t really much in the Additional information section. So, in this instance, I would probably terminate the instance and then launch it again. And then just ensure that we harden that instance with security best practices.</p>
<p>Just while I’m here, these icons at the top just simply tell you where this additional window will appear on your screen. So we can have it at the bottom, to the side, or just full screen. I tend to have it to the side, just makes it a little bit easier to read. And then when you’re done with the information, just click on close.</p>
<p>Let’s take a look at another example.Let’s take a look at this one, here. This time the finding type relates to a Behavior threat, to do with an EC2 instance and Network Port Unusual. This is stating that the EC2 instance is communicating with a remote host on an unusual server port 22. So what this is telling us is, generally during the history of this instance, communicating on port 22 to this remote host hasn’t happened before so it’s an unusual behavior. It’s flagged it as a potential threat. So, we can take a look again, it’s given it a severity, you can see the account id, the region etc. This time we have a threat list name, so it’s found it due to one of the configured threat lists. If you go down to the Resource affected, again we can see that the resource is the TARGET, and again it give the instance id as well. Under the Action section, we can see that it’s an OUTBOUND network connection that’s being made.</p>
<p>We can see some more information, here, about the remote host, it give the IP address, the Port that it’s actually using and some information relating to the city and the country. This time under Additional information it does give us a bit more, it gives us threat list name, the unusual port, which is 22, and the protocol, UDP. So, it does depend on the finding type as to what level of information you get under these additional section such as Actor, Action and Additional information. So, again, it looks like your instance is being compromised. You want to check out the security groups and ACLs, and also the EC2 instance itself. Again, a good way if you have an EC2 instance that’s been compromised, is just terminate it and launch a new instance, and harden that instance according to best practices.</p>
<p>So, it’s very self-intuitive. The findings themselves do provide some good information as to what has been affected, what kind of threat it is, and some information to help you remediate the problem as well. It’s a good idea to get familiar with these sample findings as the more familiar you become with these, the easier it will be to help you remediate real issues when you generate your own findings with your production environment.</p>
<p>That brings me to the end of this demonstration and the end of this lecture. Coming up next, I’m going to be talking about how you can use Amazon GuardDuty as a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/benefits-to-the-enterprise-1/">benefit to your enterprise environment</a>.</p>
<h1 id="Benefits-to-the-Enterprise"><a href="#Benefits-to-the-Enterprise" class="headerlink" title="Benefits to the Enterprise"></a>Benefits to the Enterprise</h1><p>Hello, and welcome to this lecture, where I shall be focusing on a number of benefits that this service provides to the enterprise.</p>
<p>By now, you may already have a number of ways of how this service could benefit your own organization, as this is a very powerful and useful security service to have at your disposal. Any service that is able to offer assistance into the protection of your data within the public cloud is very valuable. All too often, we hear about organizations that are being probed and hacked within the cloud environment, with millions of records being stolen, containing sensitive, personally identifiable information. This is one of the reasons that cloud adoption is stalled, due to security risks and an awareness of how to secure the environment correctly.</p>
<p>The main benefit of the service is that it is simply an intelligent threat detection service, which performs continual and automatic analysis and threat detection within your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account by analyzing your cloud trail logs, DNS query logs, and VPC flow logs. Essentially, you now have an active service monitoring and detecting anomalies throughout your environment with the addition of having it powered by machine learning and utilizing multiple threat detection feeds, looking for any communications with un-trusted and malicious sources.</p>
<p>Whether you are running 10 instances or 10,000 instances, the service does not impose any performance issues against your resources, and will provide the same high level of detection used within huge global enterprise deployments as it does to small-scale, single availability zone deployments. Security is crucial, and having these security resources available from day one is not something a small organization would have had in a traditional datacenter deployment of a solution.</p>
<p>The technology, skill set, and resource to implement a threat detection system in a traditional environment will be very costly and unlikely to make it into the forefront of priority in most cases. Regardless of the size of your AWS account and the resources within it, you will be able to use the power or full force of this intelligent threat detection service for a minimal cost.</p>
<p>As I explained in an earlier lecture, it’s possible to aggregate the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/understanding-amazon-guardduty-findings-1/">findings</a> of all your AWS accounts into a single master account. This simplifies the management for your security team by allowing them to monitor any findings through a single console. Any efficiencies such as this that can be taken advantage of saves time and reduces risk of something being missed. As with any monitoring solution, being able to assess your entire infrastructure through a single pane of glass effect pays dividends in its productivity as a service, and makes management that much easier.</p>
<p>With many security detection and vulnerability solutions out there today, either an agent or other software is often required to be installed onto the server that you want to monitor and detect potential security threats for. With Amazon GuardDuty, this is not required. All threat detection is performed without the need to monitor the incidents with additional software or agents.</p>
<p>One of the many great things about Amazon GuardDuty is that it comes with no upfront costs at all. You only pay for the processing of your log files, which I’ll come onto in a later lecture of this course. Traditionally, to install and configure a scalable, intelligent threat detection solution, would require a considerable amount of capital expenditure. With Amazon GuardDuty, you simply click enable, and it starts working straightaway without any upfront costs.</p>
<p>Having a powerful, intelligent threat detection system such as <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a> is one thing, but being able to automate responses to findings to help remediate potential security loopholes is another. You can use Amazon CloudWatch event rules and targets in conjunction with AWS Lambda to help you automate a response to a particular finding. With the ability to trigger automated responses based on GuardDuty findings, you are able to quickly and easily lock down a particular resource or restrict permissions that could stop an attack. For example, if you had a resource that was the target of a brute force SSH attack, you could set an automatic response to block SSH. For more information on AWS Lambda and Amazon CloudWatch, please see our content library for labs and courses on these services.</p>
<p>There are many features and reasons as to why this service will be beneficial to your business, many of which could save you a lot of money, should malicious activity occur within your environment. That now brings me to the end of this lecture. Coming up next, I want to talk about how much this service <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/costing-1/">costs to run</a>.</p>
<h1 id="Costing"><a href="#Costing" class="headerlink" title="Costing"></a>Costing</h1><h2 id="AWS-Resource"><a href="#AWS-Resource" class="headerlink" title="AWS Resource"></a>AWS Resource</h2><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/pricing/">Amazon GuardDuty Pricing</a></p>
<h2 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello, and welcome to this very short lecture just to explain how much <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a> is likely to cost you, and how the pricing structure works.</p>
<p>Essentially, the pricing for this service is broken down into two parts. CloudTrail Event Analysis, and VPC Flow Log and DNS Log Analysis. CloudTrail Event Analysis is charged at per one million events per month, whereas the VPC Flow Logs and DNS Logs are charged at per gig of log analyzed per month.</p>
<p>The cost for each depends on which region you are running the service in. For a full listing of each region, visit the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> pricing page of the service found <a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/pricing/">here</a>. When you first enable Amazon GuardDuty, you are able to use the service free for the first 30 days. In addition to this, you are able to see how much Amazon GuardDuty would have cost you for those 30 days, to help you estimate your ongoing payment should you continue to use the service. As you can see from this image within my own account for testing, there are minimal events and log data. As such, after a week, it wouldn’t have cost me anything as yet. I’ve not reached one million events or one gig of data. However, you can see from here that should you be running this is your Enterprise account, as opposed to my personal test account, where you may have thousands of resources, it would allow you to estimate the ongoing costs associated with GuardDuty.</p>
<p>Before I finish this lecture, I just want to provide a costing example, based on running Amazon GuardDuty from within the London EU region, which currently states pricing as follows. Please note, for the latest pricing information, please refer to the AWS documentation. With this in mind, let’s presume we have the following data per month: 55 million CloudTrail Events, 3,000 Gigabytes of VPC Flow Logs, and 2,000 Gigabytes of DNS Query Logs. Using the charges from the table listed for the London region, we can calculate the costing as follows. The CloudTrail Events would work out at $242, and the VPC Flow Logs and DNS Query Logs will total 2,350, giving a total of $2,592 for that month.</p>
<p>The charges for this service are very simple to understand and estimate. You may be thinking, is this worth the cost? But at the same time, you should also be thinking of the cost against not using the service, and suffering the effects of a security breach. Not only will this have a financial impact on your organization in the event you have to shut systems down to remediate the issue, but there is also a significant cost of reputation to your business. All of this needs to be taken into consideration when looking at the cost of security.</p>
<p>That now brings me to the end of this lecture. Coming up next, I shall be looking at some of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/partner-offerings-1/">partner offerings</a> that are available, that work in conjunction with Amazon GuardDuty.</p>
<h1 id="Partner-Offerings"><a href="#Partner-Offerings" class="headerlink" title="Partner Offerings"></a>Partner Offerings</h1><h2 id="Resources-1"><a href="#Resources-1" class="headerlink" title="Resources"></a>Resources</h2><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/marketplace/pp/B0764JH55Q?ref=_ptnr_AL_Website_CloudInsightEssentials_AWS_Marketplace">Alert Logic Cloud Insight Essentials (US)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.crowdstrike.com/products/falcon-intelligence/">Crowdstrike</a></p>
<p><a target="_blank" rel="noopener" href="https://www.trendmicro.com/aws/guardduty/">TrendMicro</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/resources/partners/">Full Partner List</a></p>
<h2 id="Transcript-1"><a href="#Transcript-1" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture. Well, I just want to briefly highlight some of the partner offerings that integrate with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a> and how to find out more information about them.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> work with many approved vendors to seamlessly interact their services with existing tools and products on the market today. Many of these vendors focus on monitoring and security. And as a result, there are many partners that provide services that can interact with Amazon GuardDuty to help you get even more from the service. Many organizations will probably be familiar with some of the partners and would likely already be using some of their products within their current infrastructure and environment. I just want to highlight a few of these partners currently committed with the Amazon GuardDuty. So you are aware that there are additional benefits to be had from a security standpoint.</p>
<p>Starting with Alert Logic Cloud Insight Essentials for AWS. Alert Logic offer a product that allows you to gain additional insight into your Amazon GuardDuty findings. Again without any agent required on your resources. Cloud Insight Essentials is designed to help you respond to your GuardDuty findings faster by providing further intelligence about the threat, in addition to providing more information about how to remediate the issue with specific actions. It also adds a feature that allows you to produce reports to analyze trends with your AWS account from a threat detection perspective. For further information on this offering and what it can do please visit the <a target="_blank" rel="noopener" href="https://aws.amazon.com/marketplace/pp/B0764JH55Q?ref=_ptnr_AL_Website_CloudInsightEssentials_AWS_Marketplace">link</a> on the screen.</p>
<p>CrowdStrike is another partner of Amazon GuardDuty. However, they integrate their technology and threat intelligence feeds which are used within CrowdStrike Falcon to Amazon GuardDuty. GuardDuty can then pull data and information from CrowdStrike which uses AI and machine learning to provide protection and block against cyber security threats. For more information on CrowdStrike and CrowdStrike Falcon please visit the <a target="_blank" rel="noopener" href="https://www.crowdstrike.com/products/falcon-intelligence/">link</a> on-screen.</p>
<p>I’ll take a look at one more example. This time with Trend Micro. Trend Micro have an existing product called Deep Security which uses an agent that can be installed on EC2 or ECS deployments to help protect against an array of threats such as anti-malware and adds features such as intrusion prevention. Using automation through CloudWatch and Lambda triggers can be used to invoke deep security and engage its rich features to analyze and detect any issues that may have occurred on the resource. More information on how these two products can work together can be found <a target="_blank" rel="noopener" href="https://www.trendmicro.com/aws/guardduty/">here</a>.</p>
<p>These were just a few of the partners from the Amazon GuardDuty partners list which can be found <a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/resources/partners/">here</a>.</p>
<p>You may find that you are already using services from these partners. If you are then you may find you already have additional security features available to you when and if you enable Amazon GuardDuty.</p>
<p>That brings me to the end of this lecture. Coming up next, I will be providing a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/course-summary/">summary</a> of the key points taken from the lectures throughout this course.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Hello and welcome to this final lecture of the course. We want to summarize the key points from each of the lectures.</p>
<p>I started the course by explaining what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/what-is-aws-amazon-guardduty-1/">Amazon GuardDuty is and what it does</a>, and I explained that Amazon GuardDuty is a regional based intelligent threat detection service. It allows users to monitor their AWS Account for unual and unexpected behavior by analyzing AWS CloudTrail event logs, VPC flow logs, and DNS logs. The logs that were assessed against multiple security and threat detection feeds, looking for anomalies and known malicious sources. The service itself is powered by machine learning, and Amazon GuardDuty provides automatic and continuous security analysis for safeguarding your entire AWS environment. Findings are presented with a priority level that enables you to investigate the issue further, and the service does not require any agents or software on your resources. It’s also possible to link your AWS accounts together to perform a threat detection layer across all of your accounts. And this service has zero impact of the performance of your existing resources.</p>
<p>Following this, I then discussed the different component and the elements of the service itself. Within this lecture, we learned that there are three data sources that Amazon GuardDuty uses to perform its analysis: AWS CloudTrail event logs, VPC flow logs, and DNS query logs. Machine learning is interwoven with Amazon GuardDuty, allowing it to learn and adapt to what it classes as unusual behavior within your account at the time to then highlight it as a potential threat. List management allows you to upload your own list with trusted IPs and threat list, and any IP information that is add to the trusted IP list is whitelisted. Threat lists contain a known list of malicious IP addresses on networks that you want to ensure guide you to generate findings for if any traffic it detected with this information. GuardDuty findings are listing within the GuardDuty dashboard and allow you to take the appropriate actions against them to resolve any security vulnerabilities that may exist. The content of the finding itself can be broken down into five parts: the finding summary, the resource affected, the action, actor, and additional information. Each finding is associated with a severity level and score. The score value will affect the severity. I also gave a demonstration that introduced you to the service portion and how to enable the service, configure trusted IP and threat lists, and an overview of the different options within the Amazon GuardDuty dashboard.</p>
<p>Next, I spoke about how to link multiple AWS accounts for Amazon GuardDuty, allowing for centralized management. In a multi-account scenario, one account can act as a master account and then all others can act as members. Findings from member accounts send a copy of the results to the dashboard of the master account. And this allows you to view all accounts in a central location. Trusted IP lists and threat lists within the master account are not used within the member accounts, and the master account has additional control and administrative functions such as having the ability of being able to suspend Amazon GuardDuty within its own account and other member accounts. To set up your AWS accounts in a master-member configuration it’s a simple three-stage process. Add an AWS account from within the master account, send an invitation to the member account, and then accept the invitation from within the member account. Ie then performed a simple demonstration showing you how to carry out these steps.</p>
<p>Next, I focused on how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/managing-permissions-1/">manage permissions with Amazon GuardDuty</a> and here I covered the following points. To enable the Amazon GuardDuty service, the user will need some specific IAM permissions that allows them to create a service-linked role that allows GuardDuty to retrieve information about some of your resources. AWS has created an AWS management policy to allow you to enable GuardDuty within a region called Amazon GuardDuty full access. This policy allows full access to Amazon GuardDuty plus the ability to create a service-linked role. For full permissions to Amazon GuardDuty, you can create a custom policy which allows a full access to Amazon GuardDuty, the ability to enable the service, and permissions to update and manage trusted IP and threat lists. AWS offers another management policy entitled Amazon GuardDuty read-only access, and this provides read-only permissions to GuardDuty’s findings.</p>
<p>Following this section covering permissions, I then provided a demonstration where I looked at GuardDuty findings in greater detail.</p>
<p>Next, I looked at how Amazon GuardDuty can be used to bring benefit to the enterprise which included that it’s an intelligent threat detection service, it provides high-level security, regardless of deployment size, it has centralized management, there are no agents required, there are no upfront costs, and you can perform automation of remediation.</p>
<p>Next, I focused on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/costing-1/">costing</a> of the service which offers a very simple charging method. Pricing for this service is broken down into two parts: CloudTrail event analysis and VPC flow log and DNS log analysis. CloudTrail event analysis is charged at one million events per month, and VPC flow logs and DNS logs are charged at per gig of log analyzed per month. And the cost varies depending on which region you have GuardDuty in. When you first enable Amazon GuardDuty, you are able to use the service free for the first 30 days. As an example of cost, the following table shows you the cost for the London, EU region. And the charges for this service are very simple to understand and estimate.</p>
<p>Finally, I looked at a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/partner-offerings-1/">partners</a> that seamlessly interact the services with Amazon GuardDuty. Some of these included Alert Logic Cloud Insight Essentials for AWS, CrowdStrike, and Trend Micro. For more information on these partners and others, the full listing can be found here. That now brings me to the end of this lecture and to the end of the course.</p>
<h1 id="3Components-and-Configuration"><a href="#3Components-and-Configuration" class="headerlink" title="3Components and Configuration"></a>3<strong>Components and Configuration</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail: An Introduction</a></p>
<p><a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types.html#actual-types">Amazon GuardDuty Finding Types</a></p>
<h1 id="5Managing-Permissions"><a href="#5Managing-Permissions" class="headerlink" title="5Managing Permissions"></a>5<strong>Managing Permissions</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_managing_access.html">AWS Policies for Access Management</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Identity &amp; Access Management</a></p>
<h1 id="8Costing"><a href="#8Costing" class="headerlink" title="8Costing"></a>8<strong>Costing</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/pricing/">Amazon GuardDuty Pricing</a></p>
<h1 id="9Partner-Offerings"><a href="#9Partner-Offerings" class="headerlink" title="9Partner Offerings"></a>9<strong>Partner Offerings</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/marketplace/pp/B0764JH55Q?ref=_ptnr_AL_Website_CloudInsightEssentials_AWS_Marketplace">Alert Logic Cloud Insight Essentials (US)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.crowdstrike.com/products/falcon-intelligence/">Crowdstrike</a></p>
<p><a target="_blank" rel="noopener" href="https://www.trendmicro.com/aws/guardduty/">TrendMicro</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/resources/partners/">Full Partner List</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/125/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/125/">125</a><span class="page-number current">126</span><a class="page-number" href="/page/127/">127</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/127/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
