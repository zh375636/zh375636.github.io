<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/23/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/23/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/CKAD-Introduction-to-Kubernetes-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/CKAD-Introduction-to-Kubernetes-2/" class="post-title-link" itemprop="url">CKAD-Introduction-to-Kubernetes-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:31:25 / Modified: 00:31:26" itemprop="dateCreated datePublished" datetime="2022-11-19T00:31:25-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/CKAD-Introduction-to-Kubernetes-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/CKAD-Introduction-to-Kubernetes-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/CKAD-Certified-Kubernetes-Application-Developer-CKAD-Exam-Preparation-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/CKAD-Certified-Kubernetes-Application-Developer-CKAD-Exam-Preparation-1/" class="post-title-link" itemprop="url">CKAD-Certified-Kubernetes-Application-Developer-CKAD-Exam-Preparation-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:31:23 / Modified: 00:31:24" itemprop="dateCreated datePublished" datetime="2022-11-19T00:31:23-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/CKAD-Certified-Kubernetes-Application-Developer-CKAD-Exam-Preparation-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/CKAD-Certified-Kubernetes-Application-Developer-CKAD-Exam-Preparation-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Cert-Prep-Google-Professional-Cloud-Architect-26/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Cert-Prep-Google-Professional-Cloud-Architect-26/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Cert-Prep-Google-Professional-Cloud-Architect-26</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:14:31 / Modified: 18:17:54" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:31-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Cert-Prep-Google-Professional-Cloud-Architect-26/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Cert-Prep-Google-Professional-Cloud-Architect-26/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Cert-Prep-Google-Professional-Cloud-Architect.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Recommended-Reading-for-Google-Professional-Cloud-Architect-Exam-25/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Recommended-Reading-for-Google-Professional-Cloud-Architect-Exam-25/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Recommended-Reading-for-Google-Professional-Cloud-Architect-Exam-25</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:14:29 / Modified: 18:31:10" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:29-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Recommended-Reading-for-Google-Professional-Cloud-Architect-Exam-25/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Recommended-Reading-for-Google-Professional-Cloud-Architect-Exam-25/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Before taking the Google Professional Cloud Architect exam, I recommend reading the documentation at the following links:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/blog/topics/cost-management/principles-of-cloud-cost-optimization">Cloud cost optimization</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/private-catalog">Private Catalog</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/vpc-service-controls">VPC Service Controls</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/functions/docs/testing/test-basics">Testing Basics</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/solutions/hybrid-and-multi-cloud-network-topologies">Hybrid and multi-cloud network topologies</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/solutions/tcp-optimization-for-network-performance-in-gcp-and-hybrid">TCP optimization for network performance in Google Cloud and hybrid scenarios</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.google.com/solutions/continuous-delivery-spinnaker-kubernetes-engine">Continuous delivery pipelines with Spinnaker and Google Kubernetes Engine</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Google-Cloud-Scaling-Applications-Challenge-24/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Google-Cloud-Scaling-Applications-Challenge-24/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Google-Cloud-Scaling-Applications-Challenge-24</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:14:28 / Modified: 00:14:30" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:28-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Google-Cloud-Scaling-Applications-Challenge-24/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Google-Cloud-Scaling-Applications-Challenge-24/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Google-Cloud-Networking-Challenge-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Google-Cloud-Networking-Challenge-23/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Google-Cloud-Networking-Challenge-23</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:14:26 / Modified: 00:14:28" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:26-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Google-Cloud-Networking-Challenge-23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Google-Cloud-Networking-Challenge-23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:14:25 / Modified: 18:13:28" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:25-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Google-Professional-Cloud-Architect-Case-Studies-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to “Google Professional Cloud Architect Case Studies”. My name is Daniel Mease and I’ll be taking you through this course. I am a trainer at Cloud Academy with over 20 years of software and web development experience.</p>
<p>This course is intended for:</p>
<ul>
<li>Anyone planning to take the Professional Cloud Architect Exam</li>
</ul>
<p>I am going to cover the 4 case studies presented in the exam guide:</p>
<ul>
<li>EHR Healthcare</li>
<li>Helicopter Racing League</li>
<li>Mountkirk Games</li>
<li>TerramEarth</li>
</ul>
<p>Feedback on our courses is valuable, both to us as trainers and our future students. If you have any criticisms or suggestions for improvement, we would greatly appreciate it if you would share those with us.</p>
<p>Please note that, when this video was recorded, all course information was accurate. Google is constantly updating its exams and services as part of its ongoing drive to innovate. As a result, over time, minor discrepancies may appear in the course content. Here at Cloud Academy, we strive to keep our content up to date in order to provide the best training available. </p>
<p>So, if you notice any information that is outdated, please contact <a href="mailto:&#115;&#117;&#x70;&#x70;&#111;&#x72;&#116;&#x40;&#x63;&#108;&#111;&#x75;&#100;&#97;&#x63;&#97;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;">&#115;&#117;&#x70;&#x70;&#111;&#x72;&#116;&#x40;&#x63;&#108;&#111;&#x75;&#100;&#97;&#x63;&#97;&#x64;&#x65;&#109;&#121;&#46;&#x63;&#x6f;&#x6d;</a>. This will allow us to make all necessary fixes to the course during its next release cycle.</p>
<h1 id="Preparing-for-the-Exam"><a href="#Preparing-for-the-Exam" class="headerlink" title="Preparing for the Exam"></a>Preparing for the Exam</h1><p>To help you prepare for the Professional Cloud Architect Certification Exam, Google has provided an official study guide.</p>
<p>The study guide contains every topic that is to be covered by the exam. So theoretically, if you are familiar with everything on this page, you should be able to pass the exam without any issues. At the top of this exam guide, you will find some case studies. The whole point of this course is to explain what these case studies are, why they are important, and how to use them to prepare for the exam. So first, what are these case studies? </p>
<p>When you first read them, they might seem a little confusing. Normally a “case study” is a detailed description of a real world event. So an example might be something along the lines of “Company X migrated from an on-premises data center to the cloud and saved a million dollars”. The case study would then describe in detail everything that was running before the migration, including costs. And then it would break down those migration costs, as well as the updated costs after the migration. The basic idea is to make a claim and then offer proof of that claim, using a real world example. </p>
<p>The case studies linked in the study guide are different. Each case study describes a fictitious business. Also there is no explicit “event” described either. Instead, each case study presents a problem. Maybe the company wants to be able to save money, or maybe they want to be able to scale faster. Something like that. It also will describe the existing environment, along with any business and technical requirements. So, you can think of them as theoretical scenarios.</p>
<p>So, what is the point of including these case studies in the study guide?  </p>
<p>Well that is explained in the study guide itself:</p>
<p>“During the exam for the Cloud Architect Certification, some of the questions may refer you to a case study that describes a fictitious business and solution concept. These case studies are intended to provide additional context to help you choose your answer(s). Review the case studies that may be used in the exam.”</p>
<p>So these are basically mini-scenarios that will be referenced by one or more exam questions. Essentially they are a way for Google to ask complicated questions, without requiring several pages of text for each question. Now if you come into the exam already familiar with these scenarios, then the questions can be much shorter and they don’t have to spell out the entire background.</p>
<p>So then, how do I use these case studies to prepare for the exam? There are a few things you need to do. </p>
<p>First, you should try to memorize (or at least get very familiar) with all of them. Yes, I realize memorizing fictional scenarios is probably not something you want to do. However, memorizing these scenarios will save you precious time. Every time I have taken a Google exam, they always included a link to the appropriate case study. However, reading through the whole case study takes time. That means you will have less time to think and choose an answer. If you can go into the exam already knowing the scenarios, you can save yourself a significant amount of time. That means you will be less likely to run out of time and less likely to have to leave some answers blank.</p>
<p>Second, instead of just memorizing the case studies, you should also be able to identify key pieces of information within. For example, if a case study says that the company is going to be dealing with customers’ medical records. That means that any solution will need to be very secure, because it’s going to be handling sensitive health information. You need to make sure that any solution you pick can support encryption and be HIPAA compliant. Things like that. Your answers will be very dependent upon the requirements in each case study, so it is critical that you understand what the requirements are.</p>
<p>In the following lessons I am going to go through each case study, one at a time. I am going to read through each section, and then I’ll point out any key terms that you should take note of. Now, you need to be aware that simply watching these videos will not be sufficient. You should plan to read through the case studies yourself several times. Now it does not have to be word-for-word, but memorize the details. Memorize the key requirements. That way, when a question asks you to pick a “compute” solution for one of the companies, you can pick the most appropriate answer.</p>
<p>Some of you might be thinking that you might just watch this course several times in a row. I would actually discourage you from doing that. Google can modify its exams at any time. This also means that they can modify those case studies at any time. There is a chance that one or more case studies will have been modified since I recorded this video. It would actually be best for you to read the latest version of the case studies yourself. Google does not notify people when it makes any changes. So, I will try to keep this video updated, but I can’t guarantee 100% accuracy.</p>
<p>Also please note that I cannot tell you what questions will be asked, nor can I tell you which of the case studies will be used. Your exam might refer to all four, or maybe only one. Everyone’s exam experience will be different. So use these videos as a guide, but be prepared to do some studying on your own as well.</p>
<h1 id="EHR-Healthcare"><a href="#EHR-Healthcare" class="headerlink" title="EHR Healthcare"></a>EHR Healthcare</h1><p>In this lesson, we are going to dive into the case study for a fictional company called “EHR Healthcare”. I am going to read each section, and then I’ll point out key requirements and topics for study. Alright, so let’s start with the company overview:</p>
<p>“EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi-national medical offices, hospitals, and insurance providers.”</p>
<p>Now there are a few phrases that jump out at me when I read this. First of all, I see that this company will be storing “health records”. When I hear that I think about needing to store people’s private medical information. This means you need to think very carefully about security, encryption and compliance. Of course you should always be careful with customer’s data, but medical information is in a class of it’s own. There are all kinds of laws regulating this stuff. </p>
<p>That means you should understand how to properly set up IAM roles to prevent your employees from accessing your customer’s data. You need to be familiar with picking the right services to store and transmit this information securely. Know how to detect if private information is being accidentally written out to your logs. On your exam, you could get questions about any of these topics.</p>
<p>Now the next thing that I see is “multinational”. If your customers are going to be all over the world, that implies you probably need to support multiple regions. So a single instance running in a single zone is not going to work. You will need multiple instances across multiple zones in multiple regions. Also, this suggests that you might have to think about different jurisdictions and laws in various countries. For example, the GDPR in the EU (which is the General Data Protection Regulation). So start thinking about all the different services and options you would need to support customers all over the globe. Next, let’s go through the solution concept:</p>
<p>“Due to rapid changes in the healthcare and insurance industry, EHR Healthcare’s business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google Cloud has been chosen to replace their current colocation facilities.”</p>
<p>Ok, so the first thing I notice here is “rapid changes”. This implies agility and flexibility will be very important. So to me this means that you will generally want to avoid custom solutions. Your answers should probably stick to the built-in services and things that are easy to change. So if the question involved VMs, you’d want to stick with a Google base image instead of trying to roll your own. If the question was about storage, you’d be better off using Cloud SQL instead of trying to maintain your own MySQL server. Things like that.</p>
<p>Also, I see that things have been growing “exponentially” and that things need to be able to “scale”. So for any questions involving this case study, you are going to want to make sure that your solutions are scalable. Now for compute, that means using things like managed instance groups. Or for GKE, you would use a cluster autoscaler. For storage, you need to pick solutions that won’t run out of space and can automatically handle higher levels of I&#x2F;O. All of your answers need to be able to automatically scale with demand and avoid common bottlenecks.</p>
<p>I did notice that they do have a disaster recovery plan, and so you may be asked how to migrate this over to GCP. You might also get questions about dealing with lost data, or how to deal with a major outage. You should be familiar with Google best practices for disaster recovery. I see here that they are currently using continuous deployment, so be aware of how to set up CI&#x2F;CD on Google. Know the common services involved, and how to maintain and troubleshoot issues. Alright, next let’s go through the existing technical environment:</p>
<p>“EHR’s software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire. Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB). EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time. Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.”</p>
<p>So we see they currently have multiple co-locations. As I mentioned before, this implies that any solution would need to be able to support multiple regions. So if you get a question about storage solutions, you might need to think about things like data replication. It looks like they are using containers on Kubernetes, so you could get some questions about GKE. You should be familiar with migrating applications from one cluster to another. They also previously mentioned scalability so understand how to automatically scale containers using GKE. Basically, you could get any number of Kubernetes questions on the exam.</p>
<p>It also looks like you could get any number of database questions as well. They currently are using both SQL and noSQL databases. So you should be familiar with how to migrate data over into the various offerings. You want to be able to know how to pick the right type of database depending on the data. I also notice Redis is mentioned as well, so be prepared to get questions about caching. When should data be stored temporarily vs. permanently? How do you set up a Redis cache on GCP? Etc.</p>
<p>Now this says that the APIs on-prem will NOT be migrated. So they might NOT ask you anything about building APIs, but you probably DO need to know how to connect your on-prem environment to GCP. That means you need to be familiar with things like Cloud VPN and Cloud Interconnect. Basically, any way in which your GCP services can securely access your on-prem APIs. They might even ask you how to handle DNS routing between the two.</p>
<p>The company is currently using Active Directory, so you need to be familiar with syncing to AD and using LDAP. They might even ask you to replace Active Directory with Google Cloud Identity. Also, the company is currently using monitoring and alerting, and it looks like the alerts are not effective. So you will be expected to understand how to set up effective monitoring and alerting in GCP. Next, let’s go through the business requirements:</p>
<ul>
<li>“On-board new insurance providers as quickly as possible </li>
<li>Provide a minimum 99.9% availability for all customer-facing systems</li>
<li>Provide centralized visibility and proactive action on system performance and usage Increase ability to provide insights into healthcare trends</li>
<li>Reduce latency to all customers</li>
<li>Maintain regulatory compliance</li>
<li>Decrease infrastructure administration costs</li>
<li>Make predictions and generate reports on industry trends based on provider data”</li>
</ul>
<p>So here, on-boarding quickly means that your solutions should not need a lot of setup. You don’t have to manually provision resources every time you add a new provider. The entire system needs to be highly available, so understand how to achieve that. Be familiar with services that automatically provide 99.9% availability. If a VM dies, another should automatically replace it. If a region goes down, requests should automatically be re-routed. Things like that. </p>
<p>You could also be asked about system performance and usage, so know how to track and monitor that. And know how to set up alerts, so if your performance dips or if there is a usage spike, you can be notified. It looks like they want to track insights and identify trends. So I would imagine this could involve logging, creating dashboards, and maybe even Big Data. You might be asked about using Bigtable here, so I’d be familiar with that.</p>
<p>Low latency is important, so understand how to reduce latency to your customers spread out all over the world. This sounds like supporting multiple regions. You might get questions on Cloud CDN or load balancers. Anything that might impact latency. You need to be aware about how to handle regulatory compliance. We’ve already covered this. There could be questions that require you to pick a solution based on cost. Here it talks about administration costs, so this I assume means to automate as much as possible. You don’t want to pick solutions that require an employee to manually build and maintain them. Managed services are your friend. </p>
<p>This last part talks about making predictions and generating reports. So that could imply either AI or ML questions. It definitely seems to indicate Big Data at the very least. So know how to collect data, store data, and generate custom reports. Let’s go through the technical requirements:</p>
<ul>
<li>“Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers</li>
<li>Provide a consistent way to manage customer-facing applications that are container-based</li>
<li>Provide a secure and high-performance connection between on-premises systems and Google Cloud</li>
<li>Provide consistent logging, log retention, monitoring, and alerting capabilities</li>
<li>Maintain and manage multiple container-based environments</li>
<li>Dynamically scale and provision new environments</li>
<li>Create interfaces to ingest and process data from new providers”</li>
</ul>
<p>So, we see the need to establish and maintain a connection between GCP and the on-prem. That implies VPNs, Cloud Interconnect, routing, DNS, all that and more. We are reminded that containers are being used. I think we covered that. Again, we see the connection to on-prem needs to be both secure and fast. So you should understand the different options and be able to pick the best one based upon security and speed. We know that there is going to be logging, monitoring and alerting.</p>
<p>Here we see there will be multiple container-based environments, so you might need to deal with multiple GKE clusters. Either in different regions, or for different environments such as testing vs. staging vs. production. You might also be asked about dynamically provisioning and scaling environments. Or even creating APIs. So finally, let’s read the executive statement:</p>
<p>“Our on-premises strategy has worked for years. But it has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.”</p>
<p>They are mostly just repeating themselves here. I again see they mention multiple environments. I see there may be questions about dealing with outages. There could be questions about fixing a misconfigured system. Scaling should already take care of inadequate capacity and spikes in traffic. And we already mentioned monitoring. And again, things need to be scalable, resilient and we should support multiple environments. So I think we covered everything here. You can see that this case study covers a lot of ground. But that should be it for the EHR Healthcare case study.</p>
<h1 id="Helicopter-Racing-League"><a href="#Helicopter-Racing-League" class="headerlink" title="Helicopter Racing League"></a>Helicopter Racing League</h1><p>In this lesson, I am going to walk you through the case study for a fictional company called “Helicopter Racing League”.</p>
<p>Let’s start with the company overview:</p>
<p>“Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.”</p>
<p>So what are the key terms that I see here? Well first, I notice that this is a “global sports league”. Now that implies that our services need to support multiple regions. A global audience will require using global services. Things like CDNs and load balancers. Next I see that this will be a paid service. So there might be some questions about collecting money. Possibly storing credit card information. You should be familiar with storing and encrypting sensitive data. Also this means users are going to need to log in. So you could get some questions about user authentication and authorization as well.</p>
<p>And again here, we see that we need to support customers all over the world. And we see that there will be live telemetry captured. So possibly you might get some questions about internet-of-things. I don’t think this says if they are real helicopters or just like drones. But either way, if you are sticking sensors on the helicopters and capturing data from them in real time, that could definitely suggest an IoT question or two.</p>
<p>We also see here that we are going to use this live data to make predictions. So that sounds like you could get some AI &amp; machine learning questions as well. This is starting to sound like it is going to require some very compute intensive loads. Next, let’s read the solution concept:</p>
<p>“HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.”</p>
<p>Well, that confirms that AI and ML are potential topics for questions. You should make sure to be familiar with the main offerings available there. This says they want to migrate their existing services, so make sure you know how to handle migration in AI and ML as well.</p>
<p>Here I see the phrase “emerging regions”. Now that is interesting. So not only will the customers be all over the world, but they are going to be outside of your typical areas. That sounds like your services will need to have many different regions; not just the US and Eurape. This definitely sounds like you are going to have to make heavy use of CDNs because your viewers might not have access to reliable, high speed internet. So I could see getting some questions that ask you to design a streaming solution based around these constraints.</p>
<p>Here we can see that the content is going to be both real-time and recorded. So the recorded content can be cached, but the real-time videos cannot. So I would start thinking about how you can stream content to viewers in more remote areas. Let’s move on the existing technical environment:</p>
<p>“HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows: </p>
<p>Existing content is stored in an object storage service on their existing public cloud provider. Video encoding and transcoding is performed on VMs created for each job. Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.”</p>
<p>So it seems like they are doing a lot of video editing and recording. Now, it doesn’t sound like that is going to happen in the cloud. However, they probably want to store all this footage in the cloud. Video can take up a lot of space. So you might get questions about storing massive amounts of data or uploading huge amounts of data. They also might ask you how to store all this in a cost effective way. You will want to know how to search it and retrieve it when it’s needed. Those sort of things.</p>
<p>We can see that we are potentially going have to deal with both encoding and transcoding. Now encoding is very compute heavy, but it can be done offline. So this sounds like a good use case for Spot VMs. Spot VMs would give you a bunch of temporary VMs for a cheaper price. Transcoding has to happen in real-time, so Spot VMs would not work for that. Instead you might have to use standard on-demand VMs for that on race day. Or you could look into using the transcoder API. Also, both encoding and transcoding video can be accelerated by using GPUs, so you might get a question about provisioning a VM with a GPU. So I would say that you should start thinking about how to accomplish the two tasks on GCP, and understand the different requirements of each.</p>
<p>Now having “truck mounted mobile data centers” is really quite interesting. I’m not sure exactly what the implications are. It’s possible that this might be an extra challenge to connect them to your GCP environment. You won’t be able to make the connection permanent, so I’m guessing you can’t use Interconnect. You might be stuck using a VPN connection. Also if you were to set up some firewall rules, you might not be able to hard code any IP addresses for white lists. This definitely has some potentially interesting implications. It’s probably worth sitting down and taking some time to think about how a mobile data center would be different from a standard on-prem environment.</p>
<p>Here we see they are currently using an object storage service, so you definitely should be familiar with Cloud Storage. Since we are talking about a lot of storage space being needed, I could also see you getting some questions on object storage classes or object lifecycle management. Again here we see that you could get asked about using VMs to do the encoding and transcoding. And here we see they are using Tensorflow for machine learning. You might be asked how they could best migrate this over to GCP. I would say there definitely is the possibility of getting some ML and AI questions. Ok, so let’s go through the business requirements:</p>
<p>“HRL’s owners want to expand their predictive capabilities and reduce latency for their viewers in emerging markets. Their requirements are:</p>
<ul>
<li>Support ability to expose the predictive models to partners</li>
<li>Increase predictive capabilities during and before races:<ul>
<li>Race results</li>
<li>Mechanical failures</li>
<li>Crowd sentiment</li>
</ul>
</li>
<li>Increase telemetry and create additional insights</li>
<li>Measure fan engagement with new predictions</li>
<li>Enhance global availability and quality of the broadcasts</li>
<li>Increase the number of concurrent viewers</li>
<li>Minimize operational complexity</li>
<li>Ensure compliance with regulations</li>
<li>Create a merchandising revenue stream”</li>
</ul>
<p>So, we already covered AI &amp; ML. Latency could be an issue for some of the customers. But we already talked about that. This is interesting, so not only will we be dealing with Tensorflow models, but they might also want to share their models with partners. So you might want to brush up on how to do that. Now here we see that the predictions need to happen in real time. Predictions are made both before a race and during a race. So ask yourself, do you know how to do streaming predictions? How can you minimize latency to achieve real-time predictions?</p>
<p>We already covered telemetry and insights. And we need to measure engagement and make predictions. We covered that. We mentioned global availability. Quality is going to be a challenge. So ask yourself how do you maintain high quality streaming to all of your customers? In addition to having a high quality stream you also have to worry about scaling those streams up. So we are talking about a lot of bandwidth here. You are going to need some mechanism to autoscale up and down as required. You don’t want to bump up against some upper limit by having too many viewers at once.</p>
<p>Here is something new. We need to minimize operations complexity. So you want to avoid custom solutions. Try to stick to using GCP services. Also, things should automatically scale up and down as needed. You are not going to want to require a lot of manual intervention for dealing with problems. Managed services will be key. Since this is a paid service and you are dealing with a global audience, compliance could be an issue. You are going to have to deal with a lot of different local laws. You also will be handling money so there are regulations about that as well. Basically you need to be able to do audits and verify that the company is in compliance.</p>
<p>And finally there is going to be some sort of merchandising stream. Now, I’m not sure if this means they will be selling t-shirts or if they are just going to be showing ads. Or maybe both. But you should take some time to think about the impact that these things could have on the system architecture. Ok, let’s go through the technical requirements:</p>
<ul>
<li>“Maintain or increase prediction throughput and accuracy</li>
<li>Reduce viewer latency</li>
<li>Increase transcoding performance</li>
<li>Create real-time analytics of viewer consumption patterns and engagement</li>
<li>Create a data mart to enable processing of large volumes of race data”</li>
</ul>
<p>We already talked about AI &amp; ML. I already mentioned latency. We covered transcoding. And real-time analytics. Oh, here is something new. You should know what a data mart is and how to set one up. There is going to be a large amount of data, both video footage and telemetry data. So that sounds like there could be some Big Data questions in there as well. Finally, let’s go through the executive statement:</p>
<p>“Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.”</p>
<p>So yes, we talked about high quality video streaming. And that we need to do predictions. And of course these predictions need to be in real-time. Ok, this is new. Some of our prediction models need to be able to process data from all the races over the season. So not every prediction model is just going to work with current race data. Make sure you understand how to handle that. And that’s it. I think we have covered the Helicopter Racing League case study pretty thoroughly.</p>
<h1 id="Mountkirk-Games"><a href="#Mountkirk-Games" class="headerlink" title="Mountkirk Games"></a>Mountkirk Games</h1><p>In this lesson, I am going to walk you through the case study for a fictional company called “Mountkirk Games”. Let’s read the company overview:</p>
<p>“Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms after successfully migrating their on-premises environments to Google Cloud. Their most recent endeavor is to create a retro-style first-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-specific digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players across every active arena.”</p>
<p>Alright, so it appears they are going to be making online games for mobile platforms. So you might want to brush up on Google services that are designed for supporting mobile devices. I can also see that there are going to be hundreds of simultaneous players, so they are going to require infrastructure for dealing with a lot of different players at once. This is a video game so it has to be real-time and it has to be as low of latency as possible.</p>
<p>This geo-specific arena concept is interesting. So I assume they mean that all the players in Germany would be playing together, and all the players in Japan would be playing together. So expect some questions about detecting user location. And this of course implies you are going to be supporting multiple regions and zones across your services.</p>
<p>So it sounds like these arenas are going to be kept separate. But you also need to be able to access data from each and use that to create the global leaderboard. So maybe each arena is going to be a separate project, but you are going to have to access data from a separate project. So I could see some IAM permission questions popping up here. If the leaderboard is in real-time, you can’t rely on exporting the data to a bucket and then importing it later on. So maybe this will involve some APIs as well. Let’s read the solution concept:</p>
<p>“Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game’s backend on Google Kubernetes Engine so they can scale rapidly and use Google’s global load balancer to route players to the closest regional game arenas. In order to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.”</p>
<p>Ok, so again we see there could be a lot of simultaneous players. Now, this sounds like we need things to be very scalable. You don’t want to launch a new video game and no one can play it because you can’t handle the influx of new customers. Also, it appears that they are going to start using Google Kubernetes Engine. So I could think of quite a few questions about that. How to migrate a cluster. How to autoscale a cluster. How to create a new cluster. You should be familiar with all of that.</p>
<p>Oh, it looks like you could get questions on load balancers as well. You might want all players to enter at the same point, and then forward them to the appropriate regional arena. So think about how you would do that. And they also specifically call out that they plan to use Cloud Spanner. That makes sense given the requirements. So make sure you read up on Cloud Spanner. You should understand the difference between it and say Cloud SQL. You are going to have to use it to support multiple regions, so make sure you understand that. And you might even want to try to think about the types of data that should be stored in Cloud Spanner. And think about the types of data that should NOT be stored in Spanner. Alright, next let’s go through the technical environment:</p>
<p>“The existing environment was recently migrated to Google Cloud, and five games came across using lift-and-shift virtual machine migrations, with a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions and network policies. Legacy games with low traffic have been consolidated into a single project. There are also separate environments for development and testing.”</p>
<p>So I see mention of lift-and-shift VMs. You might be asked how to do that, so just be aware. You should also probably think about how you auto-scale these. Both up and down. You also should know that lift-and-shift usually doesn’t take advantage of any GCP-specific features, so you might want to start thinking about what it would take to optimize these. Maybe at some point, they want to containerize these VMs and run them on Kubernetes. So think about what that would take.</p>
<p>Here they explicitly say that each game is running in its own project. And they also use folders. So you should be familiar with the resource hierarchy of organization, folders, and projects. You need to understand permissions and policy inheritance. You should be prepared in case you get a question on how Project A can access something in Project B. And I see there could be potential questions on network policies as well. It also looks like you need to support multiple environments per game. So you want to understand how to set those up and maintain them. Your developers might need full control over development. They might have limited permissions in testing. And they probably have no permissions or at least very limited permissions for production. The business requirements are:</p>
<ul>
<li>“Support multiple gaming platforms</li>
<li>Support multiple regions</li>
<li>Support rapid iteration of game features</li>
<li>Minimize latency</li>
<li>Optimize for dynamic scaling</li>
<li>Use managed services and pooled resources</li>
<li>Minimize costs”</li>
</ul>
<p>So we already have seen multiple platforms. This could imply mobile phones, tablets, and computers. But it could also include gaming consoles as well. I’m not sure. It definitely seems like you need to think about juggling a huge array of different devices. Now, we already covered multiple regions. Rapid iteration is new. Software updates are going to need to be pushed out pretty frequently. Bug fixes are going to be very common, especially in games. And they are going to want to add new game features as well. So to me, this means you are going to have to think about things like Continuous Integration&#x2F;Continuous Deployment. With all these rapid changes, versioning is going to be critical. So you might get questions on Cloud Source Repository, Container Registry, and Artifact Registry. Basically, any solution you suggest has to be able to handle a constant amount of small change.</p>
<p>We already covered scaling. And we see managed services are going to be important. Pooled resources are going to be important. Definitely, you want to understand load balancers. And you are probably going to get at least one question about optimizing for cost. Autoscaling is great for this. You will use less resources when you can. And basically, you just want to understand cost differences between similar services. So, for example, you should realize that Cloud Spanner is very powerful, but it is also very expensive. For every option, you want to know the associated cost. Now let’s read the technical requirements:</p>
<ul>
<li>“Dynamically scale based on game activity</li>
<li>Publish scoring data on a near real–time global leaderboard</li>
<li>Store game activity logs in structured files for future analysis</li>
<li>Use GPU processing to render graphics server-side for multi-platform support</li>
<li>Support eventual migration of legacy games to this new platform”</li>
</ul>
<p>We already covered scale. We covered the leaderboards. Ok, here we see the potential for there being a lot of logs. That means you need to know how to store logs, how to organize logs, and how to search logs. I also notice that logs will be stored in structured files. So to me, that sounds like they are hinting about BigQuery. I would say you should be familiar with building a data warehouse and writing queries. Understand all the best practices about BigQuery. </p>
<p>Now they are specifically calling out GPUs here. So you might get some questions on that. You are going to want to know how to launch a GPU VM. And since they are planning on using GKE, you also want to know how to create node pools that are equipped with GPUs as well. And it looks like they want to migrate some of their older games, so you could get questions about containerizing VMs. On to the executive summary:</p>
<p>“Our last game was the first time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games using cloud-native design principles. Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top priority, although cost management is the next most important challenge. As with our first cloud-based game, we have grown to expect the cloud to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug fixes and new functionality.”</p>
<p>So this makes it look like they want to analyze player behavior. Make sure you think about collecting telemetry data and then using that for deriving insights about the players. I’ve already covered potential migration scenarios. Gaming platforms beyond mobile heavily implies gaming consoles to me. Latency and cost will be key requirements to optimize for. Ok here, advanced analytics capabilities heavily suggests that Big Data is going to be very important. So make sure you brush up on Big Query and any other Google product with the word “data” in it. Datprep, Dataflow, Data Studio. You could get questions on any of these. And we covered “rapidly iterate” already, so I think that covers the Mountkirk Games case study pretty well.</p>
<h1 id="TerramEarth"><a href="#TerramEarth" class="headerlink" title="TerramEarth"></a>TerramEarth</h1><p>In this lesson, I am going to walk you through the case study for a fictional company called “TerramEarth”. Let’s start with the company overview:</p>
<p>“TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their customers more productive.”</p>
<p>Ok, so nothing here is really jumping out at me. This seems a little bit too vague to identify any keywords. Let’s continue on to the solution concept:</p>
<p>“There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors during operation. A small subset of critical data is transmitted from the vehicles in real-time to facilitate fleet management. The rest of the sensor data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes of data per day.”</p>
<p>Ok, this section seems a lot more useful. I see that they will be collecting telemetry data from the vehicles. So this sounds like there could be some Internet of Things questions involved. It also looks like there could be some Big Data questions as well. We see many different sensors per vehicle and there are 2 million vehicles. So there could be a lot of data flowing in and being processed. You need to start thinking about how you would collect all this data, how you would store it, and then how you would process and query it. This case study sounds like it’s going to be very data-heavy.</p>
<p>So it looks like some of the data is going to be uploaded in real-time and then the rest is going to be batch uploaded at the end of the day. So you need to consider how you would potentially handle both scenarios. Now, here it says here that there is 200-500 MB per day. So if you take 200 MB * 2M &#x3D; 400 million megabytes. That’s 40 petabytes a day? So I can foresee questions on storing and archiving huge amounts of data. You are probably going to be asked about creating and running data pipelines. You should be comfortable with cleaning up and transforming data. Things like that. Let’s move on to technical environment:</p>
<p>“TerramEarth’s vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory and logistics management systems. The private data centers have multiple network interconnects configured to Google Cloud. The web frontend for dealers and customers is running in Google Cloud and allows access to stock management and analytics.”</p>
<p>Data aggregation and analysis is exactly what I would expect, based upon the previous sections. I see that clients are going to be from all over the world, so that means your services need to be running in multiple regions. This “multiple network interconnects” is interesting. You are not going to be dealing with a single on-prem environment. There will be multiple. So start thinking about the implications of that. This stock management and analytics implies that we are going to be doing lots of things with the data we derive. It’s not just going to be used for generating a few charts. This might end up getting fed into a Cloud SQL database or maybe a machine learning model. I would be expecting questions about any of the Google data tools. Now the business requirements are:</p>
<ul>
<li>“Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.</li>
<li>Decrease cloud operational costs and adapt to seasonality.</li>
<li>Increase speed and reliability of development workflow.</li>
<li>Allow remote developers to be productive without compromising code or data security.</li>
<li>Create a flexible and scalable platform for developers to create custom API services for dealers and partners”</li>
</ul>
<p>Predict and detect suggests you need to be ready for AI &amp; ML questions. “Rapidly ship parts for just-in-time repairs” suggests the use of real-time streaming predictions. It looks like they want to focus on decreasing costs. So think about things like: How can I store a lot of data for cheap? How can I save money on all the data processing? How can I scale down usage when we are in the off-season? It looks like they want more speed and reliability. That implies scalability. Think about redundancy as well. Maybe even a CI&#x2F;CD pipeline.</p>
<p>They have remote developers. So that means they are going to need to use either VPNs or maybe they are going to use Zero trust instead. So should know the tradeoffs between VPN and the Identity aware proxy. Now security is very important. So you also want to think about IAM. You want to think about keys. And you want to think about secrets. Again, they want flexibility and scalability. They need custom APIs, so know how to create those. And you should also learn the best practices for setting up APIs as well. For technical requirements we have the following:</p>
<ul>
<li>“Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting operations </li>
<li>Modernize all CI&#x2F;CD pipelines to allow developers to deploy container-based workloads in highly scalable environments</li>
<li>Allow developers to run experiments without compromising security and governance requirements</li>
<li>Create a self-service portal for internal and partner developers to create new projects </li>
<li>Request resources for data analytics jobs, and centrally manage access to the API endpoints</li>
<li>Use cloud-native solutions for keys and secrets management and optimize for identity-based access</li>
<li>Improve and standardize tools necessary for application and network monitoring and troubleshooting”</li>
</ul>
<p>Again, we see that you might get questions about setting up APIs. They say they want a gradual migration, so I’m thinking this might involve creating microservices. We get confirmation of CI&#x2F;CD pipelines here, so make sure you are familiar with all the services involved with those. They mention they are going to be using containers, so you want to know which compute resources support those. Containers usually imply Kubernetes, but not always. This is interesting. They want to be able to run experiments. Now I know App Engine makes this pretty easy, but there are other ways of doing that as well. You want to think about how you can roll out different versions at the same time. Maybe have half of your users on one, half on the other. And then you want to measure the results of that.</p>
<p>Security and governance will be important. It looks like they will need employees to be able to create new projects, so you want to understand how to set up permissions correctly to allow that. And I see data analytics jobs and API endpoints mentioned once again. They specifically mention key and secret-based management. So that confirms what I thought before. Expect questions about generating and storing keys. Know how to use Google Secret Manager. And this mention about identity-based management confirms my suspicion that you might be asked a question or two about Zero trust systems.</p>
<p>I see there is also the potential for questions on monitoring and troubleshooting. So, you want to know how to set up monitoring on GCP. You want to know how to enable logging and know where and how to search through the logs. Finally, let’s read through the executive statement:</p>
<p>“Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle downtimes. After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online fleet management services to our customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to the cloud.”</p>
<p>Ok, so they say we need to improve the operations of online fleet management. That sounds like more IoT stuff. As we saw before, they want to share their data. So think about the different ways to do that. Is it through exposing an API? Are you writing out files to a public bucket? Maybe you need to create some service accounts to access BigQuery tables. This part here about increasing autonomous operations translates to AI &amp; ML services, in my mind. And this last part means you might get some questions about migration. Alright, I think that covers just about everything in the TerramEarth case study.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Congratulations! You made it through the course.</p>
<p>Since this course was recorded, Google might have made one or more changes to the case studies. You should not assume that the case studies I presented here are the latest versions. Make sure to visit the Google Professional Cloud Architect study guide to review the latest copies of the use cases. </p>
<p>I highly suggest you read through them several times in order to get very familiar with each. Make sure you identify the main business and technical requirements. You also want to start thinking about which products and services might be the best fit for each company. If you go into the exam with this knowledge, it’s going to save you a good deal of time and it will help ensure that you can choose the best answer for any of the questions.</p>
<p>Well, that’s all I have for you today. Remember to give this course a rating, and if you have any questions or comments, please let us know. Thanks for watching, and make sure to check out our many other courses on Cloud Academy!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:14:24 / Modified: 00:14:26" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:24-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Connect-to-Google-Compute-Engine-GCE-Linux-VM-Instances-Using-SSH-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:14:22 / Modified: 18:08:32" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:22-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Managing-Your-Google-Cloud-Infrastructure-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to “Managing Your Google Cloud Infrastructure”. I’m Guy Hummel and I’ll be showing you how to keep your cloud systems running well.</p>
<p>This course is about how to maintain your cloud infrastructure after you have implemented it. Although you can set up Google Cloud to automate many operations tasks, you will still need to monitor, test, manage, and troubleshoot it over time to make sure your systems are running properly.</p>
<p>To get the most from this course, you should know the fundamentals of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, such as how to create virtual machine instances and use Cloud Storage. If you need a refresher, then you can take our Google Cloud Platform: Fundamentals course.</p>
<p>You should also have experience with performing operations tasks, especially working with Linux. It’s also helpful to have some programming experience, although just knowing the basics of a typical programming language should be enough.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>We’ll start by going through all of the components of the Operations suite to monitor, log, report errors, debug, and trace your applications.</p>
<p>Then I’ll show you how to test your infrastructure to see how it performs under difficult conditions, including heavy load, instance failures, and cyber attacks.</p>
<p>After that, you’ll see how to get data into Cloud Storage and then how to keep it under control with lifecycle management. You’ll also learn how to optimize your Cloud SQL and Cloud CDN configurations.</p>
<p>Finally, we’ll wrap up with how to troubleshoot instance startup failures, SSH errors, and network issues.</p>
<p>If you’re ready to learn how to tame your Google Cloud infrastructure, then let’s get started.</p>
<h1 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h1><p>After you’ve implemented your infrastructure in Google Cloud Platform, the first thing you’ll want to do is set up a monitoring system that will alert you when there are major problems. The easiest way to do this is to use Cloud Operations (formerly known as Stackdriver), which is Google’s powerful monitoring, logging, and debugging tool.</p>
<p>To get to it, select “Monitoring” from the console menu. The first time you bring up Monitoring in a project, it’ll take a while to set up. I’ll fast-forward.</p>
<p>Here’s where you’ll find the instructions for installing the Monitoring Agent (and the Logging Agent). You don’t need to install the agent to be able to use Monitoring, but if you do install it, you can get more information about an instance, such as about the third-party software running on it. We don’t need to install it yet, so we’ll leave that until later.</p>
<p>Suppose you want to monitor a web server and get notified if it goes down. First, you need to create an Uptime Check. </p>
<p>For the title, let’s call it “Example”. Click “Next”. Since we want to check if a web server is up, leave the Protocol as HTTP and the Resource Type as URL.</p>
<p>For the hostname, I’m going to put in the IP address of an instance I have that’s running a web server. Leave “Check Frequency” set to 1 minute. And click “Next”. We can leave the Response Validation with the defaults, so click “Next” again.</p>
<p>This is where we set up an alert for when the uptime check fails. First, we specify how long a failure has to last before it’ll trigger an alert. Let’s leave it set to 1 minute. We should also tell it how to send alert notifications. Click the dropdown menu, and then click “Manage Notification Channels”. You can be alerted by email, text message, or a variety of other options, such as Slack. We’ll get it to send an email when the web server is down. Click “Add New”, and enter your email address and your name.</p>
<p>Okay, now close this tab in your browser, and go back to the previous one. Click Refresh and select your email. Now click the “Test” button. Since the web server at that address is up, it came back right away.</p>
<p>Now I’m going to stop Apache on the instance that’s running the web server and test it again. This time, the connection failed, as expected. Click the Create button.</p>
<p>To see the results of the uptime check, go to the menu on the left and select Uptime Checks. It’ll take a while before it runs for the first time, so don’t worry if you don’t see anything in the dashboard right away. I’ll skip ahead to when the uptime check has run. OK, now you can see it’s showing that the web server’s down. After a little while, it’ll send a notification email. Here’s what it looks like.</p>
<p>Now I’ll start Apache up again and see if the alert policy sees it. I’ll just skip ahead a few minutes. Yes, it sees that the web server is up now.</p>
<p>If you want to see data graphically, then click on Dashboards. It provides default dashboards for many Google services. To create your own, click Create Dashboard. I’ll call it “Example Dashboard”.</p>
<p>Now click the “Add Chart” button. In this search field, type “URL”. There’s the resource type we need. It’s called “Uptime Check URL”. It gives us a few different metrics to choose from. The obvious one to choose is “Check passed”, but to make things more interesting, let’s choose “Request latency”. Click Save and the graph will be added to your dashboard.</p>
<p>This graph shows the network latency between each region and the web server. You can see when the web server was down, but it gives us more information than that. This network latency data from different locations around the world can be quite helpful, especially if some of your users are reporting slow performance.</p>
<p>Note that you’ll need to refresh this page to see the latest data. You can turn on auto-refresh if you want.</p>
<p>Suppose you’d like to get more information about the instance where the web server is running, such as the CPU load. Let’s create another chart.In the search field, type “cpu load”. Let’s select the 1-minute version. For the resource type, select “VM instance”. Notice that you can even monitor Amazon EC2 instances.</p>
<p>You’ll notice that the chart is blank. That’s because we need to install the Monitoring Agent to get CPU data.</p>
<p>Go back to the Overview, and then click this link to bring up the installation instructions. The instructions are different depending on which Linux distribution you’re running on your instance. I’m running Debian, so I need to follow these instructions. I’ll fast-forward to the point after I’ve run all of the install commands.</p>
<p>While we’re here, let’s install the logging agent too. You can find a link to the documentation in the GitHub repository I created for this course. The link to the GitHub repository is at the bottom of the Overview tab below this video.</p>
<p>Installing the logging agent will prepare this instance for the next lesson when we use Cloud Logging. I’ll fast-forward again.</p>
<p>Now let’s go back and see what happened to our chart. Okay, now there’s a line showing the load average, so it worked.</p>
<p>If you’ve been following along using your own account, then you should go back and delete the monitoring you set up. First, go to the Alerting page from the menu, and then delete the policy. You have to do that before it will let you delete the uptime check. Okay, now go to “Uptime Checks” and delete the one you created. Finally, go to Dashboards, click on the one you created, and delete it.</p>
<p>That’s it for this lesson.</p>
<h1 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h1><p>Looking at real-time monitoring is great, but there will be many times when you’ll want to look at what happened in the past. In other words, you need logs.</p>
<p>For example, suppose I wanted to see when a VM instance was shut down. Compute Engine, like almost every other Google Cloud Platform service, writes to the Cloud Audit Logs. These logs keep track of who did what, where, and when.</p>
<p>There are three types of audit logs. Admin Activity logs track any actions that modify a resource. This includes everything from shutting down VMs to modifying permissions.</p>
<p>System Event logs track Google’s actions on Compute Engine resources. Some examples are maintenance of the underlying host and reclaiming a preemptible instance.</p>
<p>Data Access logs are pretty self-explanatory. They track data requests. Note that this also includes read requests on configurations and metadata. Since these logs can grow very quickly, they’re disabled by default. One exception is BigQuery Data Access logs, which are not only enabled by default, but it’s not even possible to disable them. Fortunately, you won’t get charged for them, though.</p>
<p>In the console, select “Logging”.</p>
<p>There are lots of options for filtering what you see here. You can look at the logs for your VM instances, firewall rules, projects, and many other components. You can even send logs from other cloud platforms like AWS to here. You just need to install the logging agent on any system that you want to get logs from.</p>
<p>This is a great way to centralize all of your logs. Not only does centralizing your logs make it easier to search for issues, but it can also help with security and compliance, because the logs aren’t easy to edit from a compromised node.</p>
<p>In this case, we need to look at the VM instance logs. You can choose a specific instance or all instances. I only have one instance right now, called instance-1. Since we installed the logging agent on instance-1 in the last lesson, there are already some log entries for it. </p>
<p>Here you can choose which logs you want from the instance, such as the Apache access and error logs. I could set it to “syslog” since that’s where the shutdown message will be, but I’ll just leave it at “All logs” because sometimes you might not know which log to look in.</p>
<p>You can also filter by log level, and for example, only look at critical entries. I’ll leave it at “Any log level”.</p>
<p>Finally, you can change how far back it will look for log entries. I’ll change it to the last 24 hours.</p>
<p>OK, now I’ll search for any entries that contain the word “shutdown” so I can see if this instance was shut down in the last 24 hours.</p>
<p>If you need to do really serious log analysis, then you can export the logs to BigQuery, which is Google’s data warehouse and analytics service. Before you can do that, you need to have the right permissions to export the logs. If you are the project owner then, of course, you have permission. If you’re not, then the “Create Sink” button will be greyed out, and you’ll have to ask a project owner to give you the Logs Configuration Writer role.</p>
<p>First, click the “Create Sink” button. A sink is a place where you want to send your data. Give your sink a name, such as “example-sink”. Under “Sink Service”, you have quite a few options, such as BigQuery, Cloud Storage, or a Custom destination. We’ll choose BigQuery. </p>
<p>Under “Sink Destination”, you have to choose a BigQuery dataset to receive the logs. If you don’t have one already, then click “Create new BigQuery dataset”. Give it a name, such as “example_dataset”. Note that I used an underscore instead of a dash because dashes are not allowed in BigQuery dataset names. Now click the “Create Sink” button.</p>
<p>It says the sink was created, so let’s jump over to BigQuery and see what’s there. Hmmm. It created our example dataset, but it doesn’t contain any tables, which means it doesn’t have any data. That’s weird, right? Well, it’s because when you set up a sink, it only starts exporting log entries that were made after the sink was created.</p>
<p>OK, then let’s generate some more log entries and see if they get exported. I’ll restart the VM, which will generate lots of log entries. Okay, I’ve restarted it. Now if we go back to the Logging page, do we see the new messages? Yes, we do.</p>
<p>Now let’s go back to BigQuery and see if the data’s there. Yes, there are two tables there now. Click on the syslog table. Now click the “Query Table” button. To do a search in BigQuery, you need to use SQL statements, so let’s write a simple one just to verify that the log entries are there.</p>
<p>Thankfully, it already gave me the skeleton of a SQL statement. I just need to fill in what I’m selecting. I’ll put in an asterisk to select everything, but I’ll restrict it by using a WHERE clause with the column name “textPayload” (which is the column that contains the text in the log entry)…”LIKE ‘%shutdown%’”. The percent signs are wildcards, so this SQL statement says to find any log entries that have the word “shutdown” in them somewhere.</p>
<p>Now we click the “Run” button…and it returns the matching log entries. If we scroll to the right, then we can see the textPayload field and it does indeed contain the word “shutdown” in each of the entries.</p>
<p>Of course, we did exactly the same search on the Logging page and it was way easier, so why would we want to go through all of this hassle of exporting to BigQuery and writing SQL statements? Well, because sometimes you may need to search through a huge number of log entries and need to do complicated queries. BigQuery is lightning fast when searching through big data, and if you build a complex infrastructure in Google Cloud Platform, then the volume of log data it will generate will easily qualify as big data.</p>
<p>Since we don’t want our example sink to keep exporting logs to BigQuery and incurring storage charges, let’s delete what we’ve created. On the Logging page, click on “Logs Router” in the left-hand menu, then select the sink and delete it.</p>
<p>We should also delete the BigQuery dataset, so go back to the BigQuery page, select the dataset, and click “Delete Dataset”. It wants you to be sure that you actually want to delete the dataset, so you have to type the dataset name before it will delete it.</p>
<p>One concern that you or your company may have is how to ensure the integrity of your logs. Many hackers try to cover their tracks by modifying or deleting log entries. There are a number of steps you can take to make it more difficult to do that.</p>
<p>First, apply the principle of least privilege. That is, give users the lowest level of privilege they need to perform their tasks. In this case, only give the owner role for projects and log buckets to people who absolutely need it.</p>
<p>Second, track changes by implementing object versioning on the log buckets. The Cloud Storage service automatically encrypts all data before it is written to the log buckets, but you can increase security by forcing a new version to be saved whenever an object in a log bucket is changed. Unfortunately, this won’t prevent an owner from deleting an incriminating object, which is why you need to keep tight control on which users are given the owner role.</p>
<p>Third, you could add more protection by requiring two people to inspect the logs. You could copy the logs to another project with a different owner using either a cron job or the Cloud Storage Transfer Service. Of course, this still won’t prevent an owner in the first project from deleting the original bucket before the copy occurs or from disabling the original logging.</p>
<p>So the bottom line is that a person with the owner role can get around just about anything you put in place, but you can make it nearly impossible for someone without the owner role to change the logs without you knowing about it.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Error-Reporting-and-Debugging"><a href="#Error-Reporting-and-Debugging" class="headerlink" title="Error Reporting and Debugging"></a>Error Reporting and Debugging</h1><p>So far, we’ve been looking at alerts and log messages from system software. Now it’s time to look at how to get error information from your applications. That’s where the Cloud Error Reporting service comes in. To show you how this works, I’m going to install Google’s Hello World application in App Engine and then get it to generate an error.</p>
<p>Normally, you’d use your own workstation for the development environment, but to simplify this demo, I’m going to use Cloud Shell. The nice thing about Cloud Shell is that it already has all of the packages installed that you need. When you do want to write and test Java code on your own workstation, remember that you need to install the Google Cloud SDK, the Java SE 11 Development Kit, Git, and Maven 3.5 or greater on your system.</p>
<p>First, open Cloud Shell. Next, get a copy of the Hello World application with this “git clone” command. Then go into the directory where the app is.</p>
<p>Now use the local development server to make sure the app works. To see if it’s working, click the “Web Preview” icon, and select “Preview on port 8080”. You should see a “Hello world!” message. OK, it’s working, so let’s stop the development server and upload the application to App Engine. You can stop the development server with a Ctrl-C.</p>
<p>Now use the “gcloud app deploy” command to upload it to App Engine. If you’re doing this yourself, then it may look slightly different than mine because I’ve already configured App Engine. If this is your first time deploying to App Engine, then it will likely ask you to choose a region. OK, it’s done deploying.</p>
<p>There are a couple of ways to test it. If you’re not using Cloud Shell, then you could do a “gcloud app browse”, which is pretty handy. Since we are using Cloud Shell, we’ll have to go to this URL. There’s “Hello World!” again.</p>
<p>Now, in order to see an error on the Error Reporting page, we need to generate an error. Let’s edit the code and mess something up. I’m going to add a line that I know will cause a problem. This’ll throw an exception because you can’t divide a number by zero.</p>
<p>Now run “gcloud app deploy” again. When it’s done, bring the app up in your browser again. This time it gives you a big error message, which is actually what we want, for once.</p>
<p>Let’s see if Error Reporting picked it up. The Google Cloud Console shows errors on the main dashboard, so you don’t have to go to the Error Reporting page to see them. I’ll refresh the page. There it is. Click on “Go to Error Reporting” to see what it shows. If you click on the error, you’ll see more details, including the stack trace. You can even click on the line where the error occurred and it will take you into your source code in the Debugger. However, in this case, the line it’s showing at the top of the stack trace is not in our code. We can click on this line, though, which is in our code, and it should take us there.</p>
<p>The Debugger is a great tool that you can use whether an error occurred or not. Let’s put a more subtle problem in the code and see how we can use the Debugger to figure out what’s wrong.</p>
<p>Suppose we want to check the operating system running our app, and if it’s Ubuntu, then we’ll print “Ubuntu rocks!”</p>
<p>First, we have to fix the bug that we introduced previously, so we’ll remove that line. We have to go back to the editor to do that. Now I’ll add the new code. Even if you’re not familiar with Java, this is pretty straightforward. It gets the name of the operating system, then it checks to see whether it’s equal to Ubuntu or not, and if it is, it says, “Ubuntu rocks!”, and if it isn’t, it says, “Hello world!”.</p>
<p>Now we’ll upload it to App Engine again. OK, now we’ll refresh the webpage. And it says “Hello world!” again, not “Ubuntu rocks!” That might be because the underlying operating system isn’t Ubuntu, but let’s go back to the Debugger and see if that’s the reason.</p>
<p>You’ll notice that this is still the old version of the file. First, refresh the browser. It’s still showing the old version. To get to the new version, you have to click on this drop-down menu and select the right one. The latest version should say 100% at the end. Sometimes you have to tell it where the source code is. Find “App Engine” in the list, and click the “Select source” button.</p>
<p>Now find the file. You’ll see some text on the right-hand side that says to click a line number to take a snapshot of the variables and call stack. It also points out that taking a snapshot does not stop the running application, which is good to know.</p>
<p>Click in the left-hand gutter on the line just after the “osname” variable is set. Now that the snapshot point is set, we can refresh the webpage and trigger the snapshot. If we go back to the Debugger tab, you’ll see that it’s showing the variables and call stack on the right-hand side. There’s “osname”. It’s set to “Linux”, not anything more specific. I guess it doesn’t know the specific distribution of Linux that’s running, so let’s change our code to check for Linux instead.</p>
<p>And deploy the new version. Now refresh the webpage. It worked!</p>
<p>Let’s go back to the main Error Reporting page and I’ll show you a couple of other things. First, if you’re sitting on this page watching for errors in real-time, then you should click the “AUTO RELOAD” button, which will refresh the page every 5 seconds. If you don’t want to hang around here and just want to get an email when an error occurs, then click the “Turn on notifications” button.</p>
<p>Alright, that’s it for this lesson.</p>
<h1 id="Tracing-and-Profiling"><a href="#Tracing-and-Profiling" class="headerlink" title="Tracing and Profiling"></a>Tracing and Profiling</h1><p>In the last lesson, we looked at how to debug errors in your application, but what do you do if your application is working properly but performing too slowly? That’s what Cloud Trace and Cloud Profiler are used for. Cloud Trace shows you the latency of each application request. That is, it tells you how long each request takes.</p>
<p>The Trace List is probably where you will spend most of your time. It shows you all of the traces over a specific period of time in this cool graph. It is set to “1 hour” right now, but we can change that to give a longer view. Each one of these dots is a trace of an individual request to the application. If you click on one of the dots, it brings up two more panes underneath. The Waterfall View shows what happened during the request. The first bar shows the total end-to-end time, which was 215 milliseconds in this case. The bars underneath show the time it took to complete calls performed when handling the request. In this case, we have one bar for an HTTP GET request.</p>
<p>Of course, this timeline would be a lot more useful if we were running a more complex application with multiple calls so you could see which ones were taking the most time. Each of those calls would have a bar on this chart. The Hello World application is about the simplest application possible, so you’ll just have to use your imagination here.</p>
<p>Analysis reports show you the latency distribution for your application and also attempt to identify performance bottlenecks, which is a great feature. You have to have at least 100 traces before you can run a report, though.</p>
<p>If you’re running your applications in App Engine, then it’ll automatically capture and submit traces, but if you want to trace code that’s running outside of App Engine, then you’ll have to either add instrumentation code to your applications using the Trace SDK or submit traces through the API.</p>
<p>Cloud Trace shows you which requests take the longest to run. Once you’ve determined which requests might need to be optimized, you can use Cloud Profiler to see which parts of the code for those requests are using the most CPU and memory.</p>
<p>To use Cloud Profiler, you have to add instrumentation code to your application even if it’s running in App Engine. Google has provided a sample application called shakesapp that includes this instrumentation. It’s written in the Go language. Here’s what it looks like in Cloud Profiler. This is called a flame graph, and it can be a bit confusing until you know how it works.</p>
<p>Since CPU time is selected, the bars represent the CPU time taken by each function. I ran the application seven times, so these results show the average of those seven runs. The first bar is for the entire application, which took about 13 seconds of CPU time, on average.</p>
<p>The bars underneath are color-coded according to the package they’re in. Most of these functions are part of the standard libraries for the Go language. The ones that are part of the actual application, shakesapp, are dark green in this graph. The first one just calls the second one, so the second bar is the one that matters. It calls a Go language function called MatchString. This single function takes up 58% of the CPU time for this application, so we might want to see if there’s a more efficient way to perform this operation.</p>
<p>Okay, that’s it for the Cloud Operations suite. Before we go, you might want to delete your application, so it doesn’t incur any more charges. Go to App Engine and then go to Settings. Click “Disable application”. It will ask you to type the app’s ID before you can click “DISABLE”. This doesn’t delete the application, but it does stop it from serving requests. To start the application up again, you can just click “Enable application”.</p>
<p>If you want to permanently delete the application, then you’ll have to delete the project it’s associated with, which you can do in the “IAM &amp; Admin” page. Be aware that if you delete a project, you will never be able to use that project ID again. That is, you won’t be able to create a new project with the same ID.</p>
<p>That’s it for this lesson.</p>
<h1 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h1><p>So far, we’ve been talking about monitoring and debugging your applications in production, but you’ll also need to test your application and infrastructure to see how it will perform under different conditions.</p>
<p>There are at least three types of tests you should run: load tests, where you stress your application with a heavy load. Resilience tests where you see what happens when various infrastructure components fail and vulnerability tests where you see if your application can withstand hacker attacks.</p>
<p>Ideally you should run load tests before you put your application into production. Your test should be designed to simulate real world traffic as closely as possible. You should test at the maximum load you expect to encounter which can admitingly be difficult to predict for some applications but hopefully you’ll have a reasonably good idea of how much traffic you’re likely to get. You should also measure how your Google cloud costs increase as the number of users increases.</p>
<p>If you’re expecting a wide variation in how much traffic you get then you should also test how your application performs when traffic suddenly increases.</p>
<p>Resilience testing is similar to disaster recovery testing because you’re testing what happens when infrastructure fails but the difference is that in resilience testing you’re expecting your application to keep running with little or no downtime.</p>
<p>One common testing scenario is to terminate a random instance within an autoscaling instance group. Netflix created software called Chaos Monkey that automates this sort of testing. If your application in the autoscaling instance group is stateless, then it should be able to survive this sort of failure without any noticeable impact on users.</p>
<p>Since cyber attacks are extremely common these days, your organization should put processes in place to test the security of your applications. Here are a few important ones:</p>
<p>First, ideally your software development team should have a peer review process with developers checking each other’s code for security flaws. Second, you should integrate a static code analysis tool such as HP Fortify into your continuous integration continuous deployment pipeline to automate security checking.</p>
<p>Third, at least once a year you should run penetration tests on your applications and infrastructure to see if they’re vulnerable. You can either do this yourself or contract a third party to do it. Other cloud providers typically require that you request permission before you perform penetration testing on your cloud infrastructure. Surprisingly Google does not require that you contact them.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a> also provides a useful tool called the Web Security Scanner. This service connects to the base URL of your application and follows all of the links in it while scanning for vulnerabilities, such as cross-site-scripting, mixed content, and outdated libraries. It can scan applications hosted in App Engine, Compute Engine, and Google Kubernetes Engine.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Storage-Management"><a href="#Storage-Management" class="headerlink" title="Storage Management"></a>Storage Management</h1><p>Once you’ve set up your Cloud Storage buckets and applied the right security settings to them, you’ll still need to manage them on an ongoing basis.</p>
<p>One of your first tasks will likely be to get data into your Cloud Storage buckets. If you need to upload data from an on-premise location, then you have three options:</p>
<ol>
<li>The Cloud Storage console</li>
<li>gsutil, or</li>
<li>Offline media import &#x2F; export</li>
</ol>
<p>The easiest way is to click on a bucket in the Cloud Storage console and then click “Upload Files” or “Upload Folder”. You can even view the uploaded file from the console if it’s the type of file that a web browser knows how to display.</p>
<p>The second way is to use the “gsutil” command. For example, to upload a folder called “example-folder” from your desktop, you would use “gsutil cp -r Desktop&#x2F;example-folder” and then put “gs:&#x2F;&#x2F;” and the name of the bucket. Now, if we go back to the Cloud Storage console, we can see the uploaded folder.</p>
<p>If you have a slow or expensive Internet connection, then you may want to use the third option, which is to ship your data on offline media. One way is to send hard disks, storage arrays, tapes, or other media to a third-party provider, such as Iron Mountain, and let them upload your data. </p>
<p>Another way is to use a Transfer Appliance supplied by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a>. Here’s how it works. You submit a request for a Transfer Appliance, which Google then ships to you. When you receive it, you install it in your data center and transfer your data to it. Then you ship it back to Google so they can upload it for you.</p>
<p>There are still more steps to perform, though, because the data on the Transfer Appliance is encrypted. First, Google uploads your encrypted data to a staging bucket in Cloud Storage. Next, you have to launch and configure what’s called a rehydrator instance. Then you run a rehydrator job on the instance, which will decrypt the data and copy it to a Cloud Storage bucket of your choosing. Finally, you delete the instance and send Google a request to erase the data from the Transfer Appliance and the staging bucket.</p>
<p>If you need to transfer data from another cloud provider, then you can use the Google Cloud Storage Transfer Service. I’ll show you how it works.</p>
<p>Click “Transfer” in the left-hand menu. Then click the “Create transfer” button. Under “Select source”, there are three options: you can transfer from another Google Cloud Storage bucket, an Amazon S3 bucket, or from a URL. I’m going to transfer from another Cloud Storage bucket to simulate transferring from another cloud provider because it works the same way.</p>
<p>Click the Browse button and select the bucket. Then say which files to include in the transfer by specifying their prefix, such as “example”. You can also exclude files in the same way. Another option is to specify that you only want files modified recently, say in the last 24 hours. Of course, if you want to copy the entire contents of the bucket, then you don’t need to put in any filters.</p>
<p>Now choose the destination bucket. There are also some options for how it handles overwrites and deletions. By default, an object will only be overwritten when the source version is different from the destination version. Also by default, no objects will be deleted from either the source or the destination. I’ll just leave it with the default settings.</p>
<p>Then you specify when you want the transfer to happen. You can either run it now or schedule it to run at a particular time every day. I’ll just run it now. The transfer is going to take a little while, so I’ll fast forward until it’s done. It’s a bit tedious to transfer files manually like this, which is why scheduled transfers are so nice. For example, you could have it run automatically every day to check for new files in the source bucket and transfer them to the destination bucket. Alternatively, you could do this with a cron job that runs the gsutil command, but it’s much cleaner to do it this way.</p>
<p>If you have any experience with managing data, then you know that data just keeps growing and growing over time, and if you don’t implement something to keep that growth under control, then you will either run out of storage space or have runaway costs. Since Google Cloud has nearly unlimited storage resources, that means you could easily have runaway costs.</p>
<p>To prevent that, you can use object lifecycle management. There are two ways to control costs using lifecycle management. Based on age, creation time, or number of newer versions, either:</p>
<ul>
<li>Delete objects, or</li>
<li>Move objects to a cheaper storage class, such as Nearline or Coldline Storage</li>
</ul>
<p>You can manage object lifecycle policies through the Cloud Storage console, the “gsutil” command, or the Google API Client Libraries. I’ll show you how to do it with gsutil.</p>
<p>First, you need to create a lifecycle config file that contains the rules you want to set. Here’s an example in JSON format:</p>
<p>{</p>
<p>“lifecycle”: {</p>
<p> “rule”: [</p>
<p> {</p>
<p>  “action”: {“type”: “Delete”},</p>
<p>  “condition”: {</p>
<p>   “age”: 365,</p>
<p>   “isLive”: true</p>
<p>  }</p>
<p> },</p>
<p> {</p>
<p>  “action”: {“type”: “Delete”},</p>
<p>  “condition”: {</p>
<p>   “isLive”: false,</p>
<p>   “numNewerVersions”: 3</p>
<p>  }</p>
<p> }</p>
<p>]</p>
<p>}</p>
<p>}</p>
<p>The first rule says to delete any live object older than 365 days. This might seem like a short time to live, but it’s not as draconian as it looks because it won’t completely delete the object if you’ve enabled versioning on the bucket. It will archive it instead. However, if you don’t have versioning enabled, then a Delete action will completely delete objects matching the condition and there will be no way to get them back. This is why you should test your lifecycle rules on test data before applying them to production data.</p>
<p>The “isLive” parameter only matters if you’ve turned on versioning. If an object is live, it means that it’s the most current version of that object. If it’s not live, then it is one of the archived versions of that object.</p>
<p>Let’s see if versioning is enabled on this bucket. I’ll use the gsutil command to check. It says it’s suspended. What the heck does that mean? It means versioning is disabled. I don’t know why they say suspended instead of disabled.</p>
<p>To enable versioning, we just need to change “get” to “set on”. I have a file in the ca-example bucket called “examplefile” [Show that]. Now, I’ll upload a different version of “examplefile” and see if it archives the old version. OK, it’s uploaded, now I’ll run the “gsutil ls -la” command on that file. Yes, there are two versions of it now and they have different dates and sizes. Note that if you do an “ls -l” without the ‘a’ flag, then it won’t show the different versions, so make sure you include the ‘a’ flag.</p>
<p>OK, let’s get back to the lifecycle policy. The second rule says to delete any object that has at least 3 newer versions of itself, including the live version. Unlike the first rule, this one really will delete the object because when you delete an archived object, it gets deleted forever. Although this rule has an explicit condition that the object must not be live, you don’t actually need to put in that condition because if an object has three newer versions, then it can’t be live. It has to be an archived version.</p>
<p>So, looking at the big picture for these two rules, there are two possible scenarios, depending on whether versioning is enabled or not. If versioning is enabled, then the first rule will archive any object that is more than one year old, and the second rule will delete any object that has at least three newer versions of itself. If versioning is not enabled, then the first rule will delete any object that is more than one year old, and the second rule will not do anything.</p>
<p>Suppose that instead of deleting objects older than one year, you’d like to send them to Nearline Storage, which is significantly cheaper, and send objects in Nearline Storage that are older than 3 years to Coldline Storage, which is even less expensive.</p>
<p>Here’s a lifecycle config file that will implement this policy.</p>
<p>{</p>
<p>“lifecycle”: {</p>
<p> “rule”: [</p>
<p> {</p>
<p>  “action”: {</p>
<p>   “type”: “SetStorageClass”,</p>
<p>   “storageClass”: “NEARLINE”</p>
<p>  },</p>
<p>  “condition”: {</p>
<p>   “age”: 365,</p>
<p>   “matchesStorageClass”: [“MULTI_REGIONAL”]</p>
<p>  }</p>
<p> },</p>
<p> {</p>
<p>  “action”: {</p>
<p>   “type”: “SetStorageClass”,</p>
<p>   “storageClass”: “COLDLINE”</p>
<p>  },</p>
<p>  “condition”: {</p>
<p>   “age”: 1095,</p>
<p>   “matchesStorageClass”: [“NEARLINE”]</p>
<p>  }</p>
<p> }</p>
<p>]</p>
<p>}</p>
<p>}</p>
<p>The first rule moves objects that are older than one year from Multi_Regional Storage to Nearline Storage. </p>
<p>The second rule says that if an object is in Nearline Storage and it is at least 1,095 days old (which is 3 years), then it should be moved to Coldline Storage.</p>
<p>To apply this lifecycle policy, you type “gsutil lifecycle set”, then the name of the config file, which is “lc2.json” in this case, and then the URL for the bucket, which is “gs:&#x2F;&#x2F;ca-example” in this case. Again, make sure you apply a lifecycle policy to test data before you put it into production or you risk losing valuable data.</p>
<p>Once you’ve set up a lifecycle policy, you can monitor what it’s doing in two ways:</p>
<ul>
<li>Expiration time metadata</li>
<li>Access logs</li>
</ul>
<p>To see the metadata for an object, use “gsutil ls -La” on the object. I’ll just show you the first dozen or so lines of the output because it prints all of the access control list information, which we’re not interested in right now. The output from this command may or may not contain expiration time metadata (and it doesn’t for this file), but the lifecycle policy should add that metadata when it knows the date and time that an object will be deleted.</p>
<p>Bear in mind that updates to your lifecycle configuration may take up to 24 hours to go into effect. Not only will it take up to 24 hours before your new rules kick in, but your old rules may still be active for up to 24 hours, so if you discover a mistake in your rules, that bug can still be active for another 24 hours after you fix it, which is another reason why testing your rules on test data first is so important.</p>
<p>Expiration time metadata is useful to see when your lifecycle policy is planning to delete an object, but it won’t show any of the other potential operations, such as moving an object to another storage class. If you want to see all of the operations that your lifecycle policy has actually performed, then you can look at the logs.</p>
<p>If you haven’t already set up access logs for your bucket, then here are the commands you need to use. First create a bucket to hold the logs. Remember to change “ca-example-logs” to your own log bucket name.</p>
<p>gsutil mb gs:&#x2F;&#x2F;ca-example-logs</p>
<p>Then you have to give Google Cloud Storage WRITE permission so it can put logs in this bucket.</p>
<p>gsutil acl ch -g <a href="mailto:&#99;&#108;&#111;&#117;&#100;&#x2d;&#115;&#x74;&#x6f;&#x72;&#97;&#103;&#101;&#45;&#97;&#x6e;&#97;&#108;&#121;&#x74;&#105;&#x63;&#115;&#64;&#103;&#x6f;&#111;&#x67;&#108;&#x65;&#x2e;&#x63;&#111;&#x6d;">&#99;&#108;&#111;&#117;&#100;&#x2d;&#115;&#x74;&#x6f;&#x72;&#97;&#103;&#101;&#45;&#97;&#x6e;&#97;&#108;&#121;&#x74;&#105;&#x63;&#115;&#64;&#103;&#x6f;&#111;&#x67;&#108;&#x65;&#x2e;&#x63;&#111;&#x6d;</a>:W gs:&#x2F;&#x2F;ca-example-logs</p>
<p>Next, you can set the default object ACL to, for example, “project-private”. You don’t have to do this, but it’s a good idea for security purposes to keep your logs private.</p>
<p>gsutil defacl set project-private gs:&#x2F;&#x2F;ca-example-logs</p>
<p>And finally, you enable logging with the “gsutil logging set on” command.</p>
<p>gsutil logging set on -b gs:&#x2F;&#x2F;ca-example-logs gs:&#x2F;&#x2F;ca-example</p>
<p>Once you have logging set up, then you can go into your logging bucket in the console and the access logs will show up there after the lifecycle policy has made changes.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Cloud-SQL-Configuration"><a href="#Cloud-SQL-Configuration" class="headerlink" title="Cloud SQL Configuration"></a>Cloud SQL Configuration</h1><p>Suppose you’ve been running a Cloud SQL for MySQL instance for a while, and it’s starting to run out of storage space. There are a few ways you could deal with this. If the database contains lots of old data that you can delete to free up space, that might be a good option. If the extra usage is caused by a temporary spike, and the data will be deleted soon anyway, then you might want to leave the storage capacity at its current level. You have to be careful, though, because if the instance runs out of space, it could go offline.</p>
<p>But you also have to be careful about increasing the instance’s storage capacity because once you increase it, the change is permanent, and you can never decrease it. So you could be paying extra money every month for space that you don’t need. In most cases, though, you’ll want to increase the storage space to avoid problems.</p>
<p>The first way to increase the capacity is very simple. You can just edit the instance’s configuration and change the storage capacity setting. However, if you think you’ll need to increase the storage space again in the future, then you might want to consider enabling the “Automatic storage increase” setting. Note that this option is enabled by default now, so you’d only have to turn it on if you disabled it when you created the instance.</p>
<p>You should also consider setting the “Automatic storage increase limit.” Since you can never decrease the storage capacity of an instance, you may want to prevent a runaway increase by setting a limit. By default, it’s set to 0, which means there’s no limit other than the maximum that’s available for the machine type of the instance.</p>
<p>While we’re on the subject of defaults, there’s another setting you may want to consider changing. Every few months, Google applies updates to each Cloud SQL instance. This requires a reboot that typically takes only a few minutes, but you’ll probably want to set a maintenance window for when Google will do the update. You can specify a 1-hour window on any day of the week. If you don’t set a window, then by default, Google can perform the update at any time.</p>
<p>Amazingly, you can change almost all of the configuration settings for a Cloud SQL instance after you’ve created it, even the machine type and the zone! The instance will go offline for a few minutes when you change the machine type or the zone, though.</p>
<p>The only settings you can’t change after you’ve created an instance are the instance ID, the region, the MySQL version (which can be set to 5.6, 5.7, or 8.0), and the storage type (that is, either SSD or HDD). Also, once you’ve configured a private IP address for an instance, you can’t remove it.</p>
<p>Another issue you might run into is called replication lag. To achieve high availability for a Cloud SQL instance, you need to create a failover replica. With this configuration, every write operation on the primary database is also made on the replica. Normally, there’s only a slight delay before the update is performed on the replica, but there are times when the replica can fall behind, and there’s a significant lag before updates are made.</p>
<p>Although the replica will not lose any of the updates, a significant lag can cause a failover to take longer if the primary fails. If the replication lag becomes too high, then Google’s SLA will no longer be valid, so it’s important to address this issue.</p>
<p>To make sure you’ll know when replication lag occurs, you can set up an alert. You just need to configure Stackdriver to monitor the seconds_behind_master metric.</p>
<p>When replication lag is caused by a temporary spike in database updates, the replica will eventually catch up, and you don’t need to take any action. If the lag is continually high, then you could try recreating the replica. If that doesn’t work, then you may need to add RAM and disk to the replica. If that still doesn’t solve the problem, then you’ll likely have to shard your database into multiple instances to spread out the load.</p>
<p>And that’s it for Cloud SQL configuration.</p>
<h1 id="Cloud-CDN-Configuration"><a href="#Cloud-CDN-Configuration" class="headerlink" title="Cloud CDN Configuration"></a>Cloud CDN Configuration</h1><p>As you probably know, Cloud CDN is a service that caches your web content in Google’s delivery network. Then when a user goes to your website, they retrieve your content from the nearest CDN location rather than from your web server.</p>
<p>Cloud CDN is a great service for serving content to your users faster, but it does complicate things, so you may need to change some settings to optimize the cache. Here’s the first problem. When you update your content, how do you make sure the cache gets updated, too, so your users get the latest content? This might sound like a simple problem, but there are several different approaches to dealing with it.</p>
<p>First, you need to set an appropriate expiration time. If you have content that changes frequently, then you should set a short expiration time so users won’t get stale content from the cache. For example, stock prices change rapidly, so you’d set a very short expiration time for them.</p>
<p>You wouldn’t want to set a short expiration time for everything, though. For example, if you set a short expiration time for your company’s logo, then your users would frequently get cache misses, and the logo would have to be retrieved from the web server, which would defeat the purpose of using a CDN. And if your company changes its logo at some point, it wouldn’t be a big deal that it would take a while before your customers see the new one.</p>
<p>Another approach for dealing with content that changes infrequently is to use versioned URLs. The idea is that if you change the name of a piece of content, then it won’t be in the cache, so users will always get the latest version.</p>
<p>Here are three different ways to use versioning. You could add a query string with a version number in it. You could add a version number to the filename. Or you could add a version number in the path.</p>
<p>You might be wondering why you can’t just remove stale cache entries directly rather than relying on expiration times and versioning. Well, you can. It’s called invalidation, but you should only use it as a last resort because <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google</a> charges for invalidations and also enforces rate limits on how many invalidations you can do at a time.</p>
<p>Now that we’ve covered ways to prevent cache entries from becoming stale, we should also cover the opposite problem—how to reduce the number of cache misses. As I mentioned earlier, if the content that users need is often not in the cache, then response times will be slower. Aside from setting the expiration time appropriately, another way of improving the cache hit ratio is to use custom cache keys.</p>
<p>By default, Cloud CDN creates each cache key using the full URL, but this can be inefficient. For example, many sites serve up the same content regardless of whether the URL contains http or https. If you cache a separate copy of each piece of content for each protocol, then there would be a lot of cache misses. By using a custom cache key that doesn’t include the protocol, you’d only have one copy of each piece of content in the cache for both protocols, so you’d have way fewer cache misses.</p>
<p>Why? Suppose you have a web page that hasn’t been accessed for a while, so the cached version of that page has expired and is no longer in the cache. Then suppose a user browses the http version of that page. There’ll be a cache miss, and the page will be retrieved from the web server into the cache. When another user browses the https version of that page, there will be a cache miss again if the cache keys are based on the full URL. But if you use a custom key without the protocol, then the request for the https version of the page will result in a cache hit.</p>
<p>Similarly, you can create custom cache keys that leave out the host. If you have multiple copies of your website on different hosts, then it would make sense to only have one copy of your content in the cache.</p>
<p>Dealing with the query string can be a bit more complicated. If the content should always be the same for a URL regardless of what’s in the query string, then that’s easy. You can just create a custom cache key that leaves out the query string. But if certain parts of the query string will result in different content being retrieved, then you need to specify which parts of the query string to include in the cache key.</p>
<p>Note that you can leave out any combination of protocol, host, and query string when you create your custom cache keys.</p>
<p>And that’s it for Cloud CDN configuration.</p>
<h1 id="Instance-Startup-Failures"><a href="#Instance-Startup-Failures" class="headerlink" title="Instance Startup Failures"></a>Instance Startup Failures</h1><p>What can you do if your VM instance fails to boot up completely? You can’t use SSH because the SSH server isn’t running yet. If you’re running the VM on your desktop, then you could look at the console. But how do you do that for a <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud</a> instance? Luckily, there’s a solution. You’d look at the serial port.</p>
<p>By default, you can see the output of the serial port by clicking on the instance, and then at the bottom of the page, you can click the view serial port button. This might be enough information to help you troubleshoot your problem, but it many cases, you’ll need to interact with the VM to see what’s going on. You’ll notice that there’s a button called connect to serial port, but it’s grayed out. How frustrating. To enable interactive access, you need to add meta data to the instance. This isn’t a terribly user friendly way of enabling a feature, but it’s actually not too difficult.</p>
<p>First you have to decide whether you want to enable interactive access for an individual instance or for an entire project. If you enable it on individual instances, then you’ll have to enable it manually for every instance. For convenience, you might want to enable it for an entire project, but there is a higher security risk enabling serial port access for all of your instances because there is currently no way to restrict access by IP address. So hackers could try to break in to any of your VMs through the serial port. It wouldn’t be easy though, because they’d need to know the correct SSH key, username, project ID, zone, and instance name.</p>
<p>To enable interactive access to an individual instance, you can use this gcloud command gcloud compute instances add dash meta data. Now put in the instance name, which is instance dash one in my case, and then dash dash meta data equals serial dash port dash enable equals one.</p>
<p>Now when I refresh the page, the connect to serial port button lights up. If I click on it, then it brings up another window where I can interact with the serial console.</p>
<p>By the way, if you’re connecting to a Windows instance, then you’ll need to go into the drop down menu and select port two.</p>
<p>If the serial port output showed that you have a problem with the file system on your boot disk, then you can attempt to fix it by attaching the disk to another instance.</p>
<p>First, delete the instance, but be sure to include the keep disks option. Notice that it still gives me a warning about deleting disks, even though I used the keep disks option. That’s normal.</p>
<p>Then create a new instance. I’ll call it debug dash instance.</p>
<p>Now attach the disk that we saved from the original instance. Notice that by default the name of a boot disk is the same as the name of the instance, instance dash one in this case. You can also add the device name flag so it will be obvious which device corresponds to this disk, which will be helpful in a later step.</p>
<p>Then SSH into the new instance.</p>
<p>Now you need to find out the device name for the debug disk. Look in the dev disk by ID directory.</p>
<p>Remember when I mentioned that naming the disk device would be helpful? You can see that the debug disk is SDB. The file system is on the first partition, or part one. So the device name we need to use is SDB One. Now you can run an fs-check on it.</p>
<p>Of course, I’m doing this on a good disk, so fs-check doesn’t see any problems. But if this disk had come from an instance that couldn’t boot properly, then there’s a good chance that an fs-check would find lots of problems.</p>
<p>Let’s pretend that fs-check had to clean the file system and it was successful. After that, you should verify that it will mount properly.</p>
<p>You should also check that it has a colonel file. It does, but before you celebrate, you should check one more thing, that the disk has a valid master boot record.</p>
<p>It printed out information about the file system, so this disk is good to go. Now you would create a new instance and use this disk as its boot disk.</p>
<p>That took a bit of work, but it was relatively straightforward. For a tougher challenge, try the next lesson where we tackle SSH errors.</p>
<h1 id="SSH-Errors"><a href="#SSH-Errors" class="headerlink" title="SSH Errors"></a>SSH Errors</h1><p>We’ve covered what to do when your instance won’t boot at all, but what if it boots, at least partway, but you can’t connect to it using SSH?</p>
<p>When you can’t connect with SSH, then a feeling of helplessness can set in. But, don’t worry, there are quite a few things you can do to resolve the issue.</p>
<p>Before we get into how to troubleshoot the “Connection failed” error you see here, let’s go over how to deal with the “Permission denied” error. This error can occur for obvious reasons, such as not using the right flags on your ssh command when you try to connect, but there are also some potential issues that are specific to Google Cloud Platform.</p>
<p>To understand why, you need to know the different ways you can configure SSH connections on GCP instances. First up is “OS Login”, which is the preferred method. It lets you control access by using IAM roles. The advantage of using IAM roles is that they’re the standard way to control access to all other resources on GCP, too. OS Login can also manage your public SSH keys for you, and it even supports two-factor authentication.</p>
<p>If you can’t use the OS Login method for some reason, then you can add public SSH keys to metadata. One easy way to do it is to add project-wide SSH keys by putting them in a project’s metadata. Any user who has an SSH key in a project’s metadata can connect to any instance in that project with the exception of instances that specifically block project-wide SSH keys. The other way is to add SSH keys to the metadata of each instance. This gives you more fine-grained control of who can access individual instances, but it takes a lot more effort to manage.</p>
<p>Okay, so how might these different configurations lead to a “Permission denied” error? There are several different ways. For example, OS Login disables SSH keys in metadata, so if you try to connect to an OS Login-enabled VM using a key in metadata, you’ll get an error. The opposite configuration also generates an error. That is, if you try to use a key stored in an OS Login profile to connect to a VM that doesn’t have OS Login enabled, then it won’t work. Another scenario is if you try to use a project-wide SSH key to connect to a VM that has project-wide keys disabled.</p>
<p>All right, now let’s get back to the “Connection failed” error. Here’s how to troubleshoot it.First, check your firewall rules. By default, your network will contain a firewall rule that allows SSH traffic, but you should make sure that the rule wasn’t deleted or modified. The easiest way to check is to go into the Networking section of the Google Cloud console and click on Firewall Rules. There should be a rule called “default-allow-ssh” that allows traffic on tcp port 22. The source should be all zeroes (meaning it will allow SSH requests from any IP address) and the target should say “Apply to all” targets.</p>
<p>If you don’t have this rule, then you can easily create it. Click “Create Firewall Rule”. You don’t have to name it “default-allow-ssh”, but you probably should, just so it follows the convention of network name (that is “default”), allow, and protocol (that is, “ssh”). Change the priority to 65534. Leave “Direction of traffic” as “Ingress” and “Action on match” as “Allow”. Change “Targets” to “All instances in the network”. Leave the “Source filter” as “IP ranges”. In the “Source IP ranges” field, put 0.0.0.0&#x2F;0. And put “tcp:22” in the “Protocols and ports” field. Now click the “Create” button and that’s it. Let’s try connecting with SSH again and see if that fixed it. Great, it did.</p>
<p>If you didn’t have a problem with the firewall rules, but you still can’t get in with SSH, then try connecting to port 22 manually and see if the SSH server responds. Use the “nc” command on the external IP address of your instance and then specify port 22 . If you see the SSH banner, then you know that the network connection is ok and the SSH server is running. If you don’t see the banner, then it could be a problem with either the network connection or the SSH server.</p>
<p>Next, try accessing the serial console, which I showed in the previous lesson. Look at the serial port output to see if that will tell you why you can’t connect using SSH. </p>
<p>The next thing to check is if there’s a problem with your account. Try connecting with another username, like this.</p>
<p>You can put in whatever username you want and the gcloud tool will update the project’s metadata to add the new user and allow SSH access. I’m going to call it “newuser”.</p>
<p>If that didn’t work and your instance boots from a persistent disk (which is the case by default), then you can detach the persistent disk and attach it to a new instance. Of course, this will take down your existing instance, so if the instance is serving production users properly and you don’t want to cause an outage, then skip this procedure and go to the next one.</p>
<p>First, delete the instance and be sure to include the –keep-disks option.</p>
<p>Then create a new instance (I’m going to create it with the same name as the original one) and attach the disk that we saved from the original instance. Also add the “auto-delete&#x3D;no” option so the disk isn’t automatically deleted when you delete this instance.</p>
<p>Now SSH into the new instance. That worked. If your instance is serving production users properly and you don’t want to cause an outage, then you can follow this procedure instead of the one I just did.</p>
<p>First, create an isolated network that only allows SSH connections. This is because you’re going to clone your production instance and you don’t want the clone to interfere with your production services.</p>
<p>Now add a firewall rule to allow SSH connections to the network. Then create a snapshot of the boot disk. Now you can create a new disk with the snapshot you just created. Then create a new instance in the new network. Now attach the cloned disk.</p>
<p>By the way, Google recommends that you don’t give this instance an external IP address, but that makes it much more difficult to connect to it. Considering that the instance is sitting in a network that blocks everything except SSH requests, it shouldn’t cause any disruption to your production services even though it has an external IP address.</p>
<p>Now SSH into this instance. Although the cloned disk is attached to this instance, it isn’t mounted anywhere, so you’ll have to do that manually.</p>
<p>First create a mount point. Then see what the device name of the disk is. The boot disk is disk 0, so the extra disk we attached has to be disk 1. Then mount the filesystem.</p>
<p>Now you can finally try to debug why you can’t connect to the original instance using SSH. You can look through the logs, for instance.</p>
<p>That was a pretty complicated process, but that’s the sort of thing you have to do if you don’t want to disrupt your production service while you’re debugging it. Here’s a summary of what I just showed you. First, create an isolated network that doesn’t allow any connections. Then add a firewall rule to allow SSH connections. Next, create a snapshot of the boot disk. After that, you can create a new disk from the snapshot. Then create a new instance in the new network, and attach the cloned disk to it. Finally, SSH into the instance, and mount the disk so you can inspect it.</p>
<p>If you still can’t find the reason why your instance won’t accept SSH connections and you are able to restart the instance at some point, then you can use a startup script to gather information. If you’re not sure what to put in the startup script, then you can use one provided by Google.</p>
<p>The script will run the next time the instance boots, so when you’re ready, you can run <code>gcloud compute instances reset instance-1</code>. The script sends its output to the serial port, so look there for the debugging info.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Network-Traffic-Dropping"><a href="#Network-Traffic-Dropping" class="headerlink" title="Network Traffic Dropping"></a>Network Traffic Dropping</h1><p>Another problem you might run into is network traffic dropping between an instance and other systems. The first thing to check is the firewall rules for the network the instance is in. The default network has firewall rules that allow http, https and a few other protocols. If any of those rules were deleted, that could be the reason for your network issues.</p>
<p>If instead of putting the instance in the default network, you put it in another network that you created, then make sure you created the appropriate firewall rules. When you create a new network, it doesn’t come with any firewall rules. So you have to create some to allow any traffic.</p>
<p>Another potential reason for network traffic dropping, is the idle connection timeout. Idle TCP connections are disconnected after ten minutes. If your instance initiates or accepts long lived connections, then you should adjust the TCP keep alive settings. This will prevent connections from being dropped by the idle timeout.</p>
<p>Since the timeout is ten minutes, you need to set the TCP keep alive to something less than ten minutes so connections will never be idle for that long. You need to set the keep alive on either the instance or the external client, depending on which side initiates connections.</p>
<p>Be aware that you should only set a TCP keep alive if you are having problems with connection timeouts. If you are not having that sort of problem, then setting a TCP keep alive will just increase network traffic for no good reason.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to manage your Google Cloud infrastructure. Now, you know how to monitor and debug using stack driver, test your infrastructure under challenging conditions, manage your storage while keeping costs under control, and troubleshoot issues with instance and networks.</p>
<p>To learn more about <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know on the Cloud Academy Community Forums. Thanks, and keep on learning.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Optimizing-Google-BigQuery-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-19 00:14:21 / Modified: 18:01:08" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:21-04:00">2022-11-19</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Optimizing-Google-BigQuery-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to the “Optimizing Google BigQuery” course. I’m Guy Hummel and I’ll be showing you how to get the most from this big data service.</p>
<p>BigQuery is an incredibly fast, secure, and surprisingly inexpensive data warehouse, but there are ways to make it even faster, cheaper, and more secure.</p>
<p>To get the most from this course, if you don’t already have experience with BigQuery, then please take my Introduction to BigQuery course first.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>We’ll start with how you can reduce the amount of data that’s processed by your queries, which will also lower your costs. Then we’ll go into more depth on partitioned tables, which is one method for reducing processing requirements.</p>
<p>Next, we’ll talk about ways to speed up your queries, including denormalizing your data structures and using nested and repeated fields.</p>
<p>Finally, we’ll wrap up with how to use roles and authorized views for access control.</p>
<p>If you’re ready to learn how to get the most out of BigQuery, then let’s get started.</p>
<h1 id="Reducing-the-Amount-of-Data-Processed"><a href="#Reducing-the-Amount-of-Data-Processed" class="headerlink" title="Reducing the Amount of Data Processed"></a>Reducing the Amount of Data Processed</h1><p>BigQuery is relatively inexpensive to use. But if you process large amounts of data, your costs can add up quickly. At first, it looks like streaming and storage would be your highest costs. That would be true if you didn’t run many queries, but most organizations use BigQuery to run lots of queries. After all, query is even part of the name, so that must be what people use it for, right?</p>
<p>Although the streaming and storage costs are higher than the query costs on a per gigabyte basis, you only get charged for streaming once and you only get charged for storage once a month. With queries, on the other hand, you get charged every time you run a query, which can be hundreds, or even thousands, of times a month, so reducing how much data is processed by your queries is usually the best way to reduce your costs. Another benefit is that it often speeds up your queries too.</p>
<p>To see how to optimize our queries, let’s use some stock exchange data that Google makes available on Cloud Storage. First, create a dataset where you can put the tables. Click the down arrow next to your project name and select “Create new dataset”. Let’s call it “examples”.</p>
<p>Now create a table in the examples dataset. Change the Location to Google Cloud Storage. Here’s the path to the first stock exchange file.</p>
<p>There will be quite a bit of text to enter in this course, especially when we start writing queries, so if you don’t want to type everything yourself, you can copy and paste from a <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/optimizing-bigquery/">readme file</a> I put on Github. It’s at this URL.</p>
<p>Now call the Destination Table “gbpusd_201401”. For the Schema, click “Edit as Text” and put this in. Make sure you get rid of the text that was in this box already. Click “Create Table”…and it takes a little while to upload, so I’ll fast forward. I’m going to fast forward quite a few times throughout the course, so if some operations seem to take longer when you run them, it’s probably because of that.</p>
<p>Then go through the same process for the second file.</p>
<p>OK, now we can run a simple query to get a baseline before we try to optimize it. Click “Compose Query”. Then, before you do anything else, click on “Show Options” and uncheck the “Use Legacy SQL” box. I don’t recommend using Legacy SQL because you have to learn some non-standard syntax to use it. Now click “Hide Options” so it doesn’t clutter up the page. OK, click on the first table and then click “Query Table”. Let’s select star so we see how much data gets processed when we read all of the data. Let’s also get rid of the “LIMIT 1000” to make sure it’s reading the whole table.</p>
<p>Before you run it, see how much data it says it’s going to process. Click “Open Validator”, which is the green checkmark. It says it will process 46.4 meg. Now click “Run Query”. It warns us that we’ll be billed for all of the data in the table, even if we use a LIMIT clause. We actually want to read all the data in the table this time, so ignore the warning. Click “Run query”…and after a few seconds, it comes back and confirms that it processed 46.4 meg.</p>
<p>There are two basic approaches to reducing how much data is processed by a query. You can reduce the number of rows that are scanned and you can reduce the number of columns that are scanned.</p>
<p>One obvious way to try to reduce the number of rows processed is by using a LIMIT clause, but that warning message we just got said that we’d be billed for all the data in the table even if we use a LIMIT clause. Let’s try it to see if that’s true.</p>
<p>Add a “LIMIT 10” clause to the end of the query.</p>
<p>It still says this query will process 46.4 meg, but let’s run it anyway. And the warning message comes up again. Click “Run query”…it takes a few seconds, and it says that it processed the whole 46.4 meg again. It did run slightly faster, but that’s probably not because of the LIMIT clause. The processing time varies, even for the exact same query.</p>
<p>The bottom line is that we can’t reduce the number of rows scanned by using a LIMIT clause, so we’ll have to look at other approaches. I’ll show you how to do that later, but first let’s try limiting the number of columns scanned.</p>
<p>The obvious way to do that is to only select specific columns instead of doing a SELECT star. For example, on this table, instead of selecting all 5 columns, let’s just select 2 of them. Replace the star with “time, bid”.</p>
<p>That only processed 19.5 meg, so it actually worked. You probably also noticed that it didn’t give us a warning message this time. That’s because it doesn’t give you a warning when you select specific columns instead of selecting star.</p>
<p>Let’s go back to trying to reduce the number of rows processed. Have a look at the data by going to the Preview tab. Do you see a pattern? The data is sorted in ascending order by the time column. Maybe we could try to use the BETWEEN operator to only select rows that are between two dates. You can copy and paste this query from the Github file.</p>
<p>Since the validator accurately predicts how much data will be processed by a query, we don’t even have to run this to know that it won’t reduce the number of rows scanned because it says it will process 19.5 meg, which is how much it processed without the BETWEEN operator. It makes sense, when you think about it, because there’s no way for BigQuery to know that all of the data is properly sorted, so it still has to read the entire table.</p>
<p>Is there anything we can do to reduce the number of rows processed? Yes, there are a couple of ways, but they’re not easy. The first way is to put your data in separate tables. That’s kind of a brute force way of limiting your queries. It still scans the whole table, but since you only put some of your data in the table, that automatically limits how much data is processed.</p>
<p>The two tables we uploaded are actually split that way because there’s one for each month. The first is for January 2014 and the second is for February 2014. So all of the queries we’ve run so far have only been on the January data. If, instead, all of the data for 2014 were in one file, then we would have processed about 12 times as much data on every query.</p>
<p>In a normal relational database, breaking your data into tables like this is not recommended. Instead, you would use an index. But BigQuery doesn’t support indexes, so you can’t do that.</p>
<p>If you break your data into multiple tables, then how can you run queries across the tables when you need to? For example, what if you needed to run a query across all of the 2014 data? Would you have to write a long query with a UNION of all 12 tables? No, because fortunately BigQuery supports using wildcards in your table references.</p>
<p>Here’s how you would run a query across the two stock market tables. Let’s select the earliest time and the latest time from the tables. To query both tables, you just need to put in a star as the last character of the table name, which will match both 1 and 2. The wildcard has to be the last character of the table name.</p>
<p>When you run the query…you’ll see that it took the mintime from the first table because it’s in January and it took the maxtime from the second table because it’s in February.</p>
<p>You’ll recall that I mentioned that there are two ways to reduce the number of rows processed. I’ll tell you how to use the other method in the next lesson.</p>
<h1 id="Partitioned-Tables"><a href="#Partitioned-Tables" class="headerlink" title="Partitioned Tables"></a>Partitioned Tables</h1><p>In the last lesson, I showed you how you could break your data into tables to limit how much data your queries process. But that seems kind of awkward, doesn’t it? There are, in fact, quite a few disadvantages to this approach. First, you have to get your data into all of these tables somehow, probably by writing a program. Second, the more tables there are in your query, the worse your query performance is. And third, you can’t reference more than 1,000 tables in a single query. You might think you’re not likely to ever hit that limit, but it’s not as unlikely as it sounds. If you collect a lot of data every day, then you may want to have a separate table for every day. Then if you had 3 years worth of data, that would be more than 1,000 tables.</p>
<p>BigQuery has a solution for this called partitioned tables. The way it works is you put all of your data in one table, but in a separate partition for every day. Then you can limit your queries to particular days and it won’t scan the rest of the table.</p>
<p>Of course, you still have the challenge of how to divide your data into the right partitions, just like you would with dividing your data into separate files with the other approach. If you’re starting fresh and want to stream new data into a table every day, then it’s very easy. You can create an ingestion-time partitioned table, and when you stream or upload data to it, BigQuery will automatically put it in the partition for that day. It also creates a pseudo-column called _PARTITIONTIME that contains the date for each data record.</p>
<p>It wouldn’t be very useful to upload our existing data to an ingestion-time partitioned table, though, because all of the data would go into today’s partition regardless of what date was in each record.</p>
<p>The solution is to create a time-unit column-partitioned table. You just need to tell BigQuery which column contains the date, and it will put each data record into the right partition.</p>
<p>We can copy the data from our existing table and put it into a new partitioned table in a single command. You can copy it from the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/optimizing-bigquery/">repository</a>.</p>
<p>We use “bq query” because we’re going to query the existing table to get the data. We set the “use_legacy_sql” flag to “false” so it’ll use standard SQL. Then we use the “replace” flag, which isn’t strictly necessary, but if you need to run the command again, then it will overwrite the destination table instead of appending to it. Then we tell it what to call the destination table. I used the same name as the original table except with a ‘p’ for ‘partition’ at the end. To make this a partitioned table, we use the time_partitioning_field flag and set it to the column that contains the date, which is called “time” in the original table. Finally, we run a “SELECT *” to get all of the data from the original table.</p>
<p>By the way, if you wanted to partition the data based on a different unit of time than a day, you’d need to add the “time_partitioning_type”, which can be set to day, hour, month, or year. If you don’t set it, then it defaults to “day”.</p>
<p>I’m going to run the command from Cloud Shell, which is right here. I need to authorize it.</p>
<p>Alright, it’s done. You’ll notice that I recorded this demo after Google changed BigQuery’s user interface, so that’s why it looks different.Now let’s have a look at the partitioned table it created. It looks exactly the same as the original table. The only indication that there’s anything different about it is this note up here saying that it’s a partitioned table.</p>
<p>Let’s say you wanted to retrieve records only from January 9th and 10th. Here’s the SQL to do that.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT time, bid</span><br><span class="line">FROM examples.gbpusd_201401p</span><br><span class="line">WHERE time</span><br><span class="line"> BETWEEN TIMESTAMP(&#x27;2014-01-09&#x27;)</span><br><span class="line"> AND TIMESTAMP(&#x27;2014-01-10&#x27;)</span><br><span class="line">ORDER BY time ASC</span><br></pre></td></tr></table></figure>

<p>It’s basically the same as a query we tried to run earlier except that it’s querying the partitioned table instead of the original one. What a difference that makes. It says it will only process 1.3 meg. When we tried to do it with the original table, it said it would process 19.5 meg. Let’s run it to make sure it works. Yes, it only processed 1.3 meg.</p>
<p>Before we move on, I should mention another really useful feature of partitioned tables. If you have more data being added to a table every day, you may want to delete the oldest data at some point. You can configure a partitioned table to do this automatically by setting a partition expiration time. For example, you could say that each partition will be deleted after it’s 12 months old. This would ensure that you only have the latest 12 months worth of data in your table. You can also set an expiration time on an entire table if you want.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Denormalized-Data-Structures"><a href="#Denormalized-Data-Structures" class="headerlink" title="Denormalized Data Structures"></a>Denormalized Data Structures</h1><p>Now you know how to reduce the amount of data BigQuery processes, which will reduce your costs, but it won’t necessarily reduce how long it takes your queries to run. There are different strategies for doing that, but we might have to break some database structure rules.</p>
<p>One of the core principles of good schema design is normalization, which is basically to remove redundancy from data. Some of you are probably saying, “That’s a dramatic oversimplification of the concept, and removing redundancy is more of a result of normalizing data.” You’re right, of course, but for our purposes here, the lack of redundancy is what matters. I should mention, though, that not only does normalization reduce the amount of space taken up by the data, but it also improves data integrity, because if the same information is stored in multiple places, you could end up with inconsistencies between the copies.</p>
<p>Having said all that, in BigQuery, you may want to do the unthinkable and denormalize your data and add redundancy into it. Why would you do that? Well, because it can make your queries much faster, and because the downsides of having redundant data are not a problem for what we’re doing. First, data integrity is not an issue because BigQuery isn’t a transaction processing system. We’re just using it for reporting, not for recording transactions, so we’re not going to create inconsistencies in the data, since we’re not going to modify the data at all.</p>
<p>Second, the extra cost associated with storing and querying extra copies of data is often outweighed by the much faster query times. Why does denormalization make queries so much faster? Because when you query a normalized database, you will often have to join multiple tables together, but joining tables is a very time-consuming operation. It’s not very noticeable when the tables are small, but as the tables get larger, the time required to join them increases exponentially.</p>
<p>This performance comparison done by Google shows just how much of a difference it can make. The slope of this line gets so steep that it would take a ridiculously long time to join tables that have billions of rows.</p>
<p>Let’s go through an example of how to denormalize a data structure. We’ll use some MusicBrainz data, which is a freely available encyclopedia of music. Here are three data files to import into tables.</p>
<p>Create a new table in the examples dataset. Change the Location to Google Cloud Storage and paste the Data File URL. Then change the File format to JSON. Call the table “artist”. Under the Schema, click “Edit as Text”, then paste in the contents of the schema file…and create the table.</p>
<p>Now do the same for the other two tables. I’ll show the process, but I’ll speed it up so it all happens in 15 seconds, so hang on.</p>
<p>OK, here’s how these three tables are organized. They’re normalized, so there isn’t any redundant data. Now we’re going to take columns from these three tables and combine them into one table. We’re going to join these tables and save the results so that when we need to run queries in the future, they don’t have to go through the join step first, which will save a lot of time. We’re doing an inner join on the three tables using the artist. The resulting table will have lots of duplicate information, such as multiple copies of an artist’s name and multiple copies of a recording’s name.</p>
<p>Click Show Options and set the Destination Table to “recording_by_artist”. While you’re here, make sure the “Use Legacy SQL” box is not checked. Alright, now run the query, which should take about 30 or 40 seconds.</p>
<p>Since this query took a long time, it says, “We can notify you when long-running queries complete.” That could be helpful in the future, so click “Enable notifications”. Then you have to click Allow.</p>
<p>Let’s see how much extra space it takes up. The recording table takes up about 1.5 gig…and the other two tables…bring the total up to about 1.7 gig. The denormalized table takes up about 2.3GB, so it’s about 35% bigger, but the speed gains would be worth it if we were dealing with larger tables.</p>
<p>Let’s get rid of the Destination Table option so we don’t accidentally overwrite it. This table only has about 18 million rows, so the speed gain won’t be very noticeable. One benefit is much simpler queries, though. Compare this…with this.</p>
<p>Both queries do exactly the same thing.</p>
<p>You can see why this table takes up more space. The name “Red Elvises” is repeated many times. In the original tables, it only had the artist’s ID for each recording, and the artist’s name was only listed once in the artist table.</p>
<p>By the way, you probably wouldn’t use the web interface to denormalize data unless it’s a one-time conversion. If you need to denormalize data on a regular basis, then you would want to automate the process using Cloud Dataflow or something similar.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Nested-Repeated-Fields"><a href="#Nested-Repeated-Fields" class="headerlink" title="Nested Repeated Fields"></a>Nested Repeated Fields</h1><p>Denormalizing data might seem like a pretty clunky way to increase query performance. Is there a way to improve performance without having to add lots of redundant data? Yes. BigQuery supports something called “nested repeated fields”. This is a way to combine data into one table without redundancy.</p>
<p>Google has provided a very small data file to demonstrate this concept. First, open it in your browser and then right-click and do a Save As.</p>
<p>Then upload it into a new table. Change the file format to JSON. Call it “persons_data”. Now open the schema file…and copy the contents here. It’s pretty hard to understand the schema by looking at this, but it will be easier to see once you’ve created the table.</p>
<p>Now have a look at the schema. Some of the fields have a dark gray background, like phoneNumber. And under phoneNumber, you’ll see that areaCode and number look like subfields.</p>
<p>It gets more interesting with the children and citiesLived fields. Not only do they have nested fields under them, but they are also repeated fields, so each person can have multiple children and citiesLived.</p>
<p>Let’s go to the Preview to see what the data looks like. This is pretty different from a normal record because it takes up two rows and there are blanks at the beginning of the second row. One thing that can be confusing is that it looks like the child Jane lived in Seattle in 1995 and the child John lived in Stockholm in 2005 because in a normal record, all of the pieces of data on one row make up the entire record. But with repeated records, that’s not the case.</p>
<p>It’s easier to see if you look at the next record. First of all, either Mike Jones has a really hard time deciding where he wants to live or there’s something wrong with this data. Anyway, here it’s obvious that the children fields are not directly related to the citiesLived fields. Another way to see this is to look at the JSON representation of this record. Click on the JSON button. It’s not as easy to read as the table view, but it makes it very obvious how the record is organized.</p>
<p>To refer to a nested field, you can use the dot notation that it shows in the schema. For example, to get a list of all of the people in the table and their phone numbers, you would run this.</p>
<p>You refer to the phone number as phoneNumber.number. You don’t need the “LIMIT 1000” because there are only 3 records in this table. It doesn’t hurt to leave it there, but I prefer to keep unnecessary clutter out of my queries.</p>
<p>If you want to refer to a nested repeated field, though, then it takes a bit more work. Let’s say you want to see who has lived in Austin. It’s giving us an error because it doesn’t recognize the “place” field. That’s because it’s a nested repeated field, so you have to use the UNNEST function. First, put in a comma after the table name. Then unnest citiesLived. You’ll notice that the error went away because now that citiesLived is unnested, we can refer to the field as just “place” instead of citiesLived.place. Let’s also select place, so we can verify that the query is working properly.</p>
<p>It came back with the correct results because Anna Karenina and Mike Jones both lived in Austin, but John Doe didn’t.</p>
<p>What happens if you don’t specify the WHERE clause? Will it print all the places for each person?</p>
<p>Yes, it does, although it repeats the person’s name beside every city, unlike what we saw in the Preview tab. The records also come up in random order, because we didn’t specify an ORDER BY clause.</p>
<p>What if you do a SELECT *? Will it look like what we saw in the Preview tab?</p>
<p>Yes, it looks like the Preview tab…or does it? There are 9 rows in the result, but there are only 3 in the table. What’s going on? It’s because we left UNNEST(citiesLived) in the query. Now it’s very clear what the UNNEST does. It creates two new columns with the contents of the nested fields. And since our query says to select from the table and from UNNEST(citiesLived), it’s actually doing a JOIN. That’s what the comma between the two means. In fact, you could replace the comma with JOIN and it would work the same. There are 9 citiesLived records in total, one for each city that each person has lived in, which is why there are now 9 records in the result.</p>
<p>OK, so if we remove UNNEST(citiesLived), will it look like the Preview tab?</p>
<p>Ignore the warning. Yes, it does, although the records are in random order, of course.</p>
<p>In this example, we uploaded a JSON file that already contained nested repeated data. If you want to do this with your own data and it’s not already in that format, then you’ll have to create it yourself. There are many ways of doing this, such as using Cloud Dataflow, but that’s outside the scope of this course.</p>
<p>If you find that you frequently need to unnest the same repeated fields in your queries, then you might want to create a view.</p>
<p>Here’s a really simple example. Open the query where you selected the person’s name and place, but didn’t use a WHERE clause.</p>
<p>Then click Save View. For the table name, use cities_by_person. You’ll notice that the icon to the left of the name is different from the other ones. That’s because a view isn’t a table, even though it asked us to put in a table name when we created it. If you click on the view, you’ll see that the schema looks like a table’s schema, but if you click on Details, you’ll see that it’s quite a bit different.</p>
<p>In fact, it doesn’t store any data. It just stores the query. So when you use a view in your queries, it actually runs the query again. So why would you want to use a view if it just runs the query again anyway? Well, it can make your queries less complicated. Here, I’ll show you.</p>
<p>First, replace the table name (persons_data) with the name of the view (cities_by_person). Now you can remove UNNEST(citiesLived). And remove the comma too. Let’s put in a WHERE clause to make it a useful query. Let’s search for all of the people who have lived in Stockholm.</p>
<p>OK, I realize that it probably wasn’t worth the effort of creating the view, and changing our queries to use the view, all to make the queries a little simpler, but if you were regularly running queries with multiple levels of UNNESTs, then it might be handy to create a view.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Access-Control"><a href="#Access-Control" class="headerlink" title="Access Control"></a>Access Control</h1><p>Data can be one of the most precious resources an organization owns. So it’s important to keep tight control of not only who’s allowed to read your data, but also who’s allowed to modify or delete it. That might seem pretty basic, but doing this with BigQuery is more complicated than it sounds.</p>
<p>BigQuery provides several layers of access control. The top layer is primitive roles, which act at the project level. They’re administered through IAM, the Identity and Access Management system.</p>
<p>There are three primitive roles: owner, editor, and viewer. When you add a member to a project, you can assign them one of these roles, which will apply to all Google Cloud Platform services, not just BigQuery. A viewer can view all datasets and run jobs (such as queries). Editors have viewer permissions, but can also modify or delete all tables. They can’t modify datasets, but they can create new datasets. An owner has editor permissions, but can also delete all datasets and see all jobs for all users in the project.</p>
<p>Primitive roles are fine as long as you have these very simple access control requirements:</p>
<ul>
<li>First, each user can have the same permissions for all GCP resources in a project, not just BigQuery. For example, if a user has the Editor role, then they’ll be an Editor not only for BigQuery, but also for Google Cloud Engine instances or any other resources in the project. If you only have BigQuery enabled in this project, then this isn’t an issue.</li>
<li>Second, each user can have the same permissions for all datasets in a project. For example, if each team has owner permissions for their team’s datasets, then they’ll also have owner permissions for other team’s datasets in this project.</li>
<li>Third, you don’t need to separate data access permissions from job-running permissions. For example, if a user has permission to view data in a project, then they can also run queries on that data.</li>
</ul>
<p>If all of these requirements are true for a project, then primitive roles are a good solution, but otherwise, you’ll need to use predefined roles.</p>
<p>There are six predefined roles. The dataViewer, dataEditor, and dataOwner roles are essentially the same as the primitive roles except for two things: First, you can assign these roles to users for individual datasets, and second, they don’t give users permission to run jobs or queries. Those permissions can be granted through the user and jobUser roles. A jobUser can only start jobs and cancel jobs. A user, on the other hand, can perform a variety of other tasks, such as creating datasets. The admin role gives all permissions.</p>
<p>Here’s how you can use predefined roles to give more fine-grained access than primitive roles in each of these situations:</p>
<ul>
<li>To give a different level of access to BigQuery than to other GCP resources in a project, use BigQuery roles, such as BigQuery Data Editor. Then also use predefined roles for any other GCP resources they need to access, such as App Engine Admin.</li>
<li>To give a user or group a different level of permissions for a dataset in a project, click the down arrow next to the dataset and select “Share dataset”. Then add a user or group by putting in their email address. Then select one of the three roles (owner, editor, or viewer).</li>
<li>To give job-running permissions without giving data access permissions, select either BigQuery User or BigQuery Job User. In most cases, you should give BigQuery User because it lets the user list datasets and tables as well as create new datasets. Job Users can only run jobs.</li>
</ul>
<p>You might be wondering why you would want to separate data access permissions from job-running permissions because it would seem like most users would need both. That’s true in many cases, so remember that you have to assign both types of roles to users in order for them to look at the data and run queries (unless you’re using primitive roles). But there are situations when you might want to give only one or the other. For example, if you have an application that monitors the size of your tables, then you might want to assign only the BigQuery Data Viewer role to its service account. That way, even if the application developers accidentally make a change to the program that would cause a query to run, it will be disallowed. Why would that matter? Because queries incur a cost and a program could potentially run up some hefty charges if there’s a bug.</p>
<p>So far, I’ve only shown you how to set permissions at the project and dataset levels. There’s a way to set permissions at the table level and even to particular data within a table, but it’s a much more complicated process. The only way to do it is to use something called an authorized view.</p>
<p>We already created a view in the last lesson. What’s different about an authorized view is that it allows users to access the results of a query without giving them access to the tables that were queried. So, for example, if you didn’t want a particular group of users to have access to certain columns in a table, you could run a query that didn’t include those columns, and then save the results as an authorized view for that group.</p>
<p>To make this work, you need to perform four steps:</p>
<ul>
<li>First, create a separate dataset to store the view. You need to do this because if you were to put the view in the same dataset as the original tables, then the group would be able to access the tables too and not just the view.</li>
<li>Second, create the view in the new dataset.</li>
<li>Third, give the group read access to the dataset containing the view.</li>
<li>Fourth, authorize the view to access the source dataset.</li>
</ul>
<p>This assumes that you’ve already given the group permission to run queries in the project.</p>
<p>Let’s say you wanted to give a group called “team1” access to only the name, age, and gender fields in the persons_data table. Before I start, I should mention that if you don’t currently have permission to assign roles to other users, then you won’t be able to do all of the steps I’m about to show you.</p>
<p>OK, first create the new dataset. Call it “shared_views”.</p>
<p>Then, run a query to select the name, age, and gender fields from the table.</p>
<p>Now click “Save View”. Change the dataset to “shared_views” and call the table “persons_view”.</p>
<p>Then click the down arrow to the right of shared_views and select “Share dataset”. In the menu on the left, select “Group by e-mail” since we’re giving permission to a group, not a user. Then I’ll fill in the email address of the team1 group. If you have a group you can assign, then use that one. Leave the permission on “Can view” and click Add. You might want to uncheck “Notify people via email” if you’re testing this with an actual group of people. And click Save Changes.</p>
<p>Now that team1 has read access to the view, the only thing left to do is to give the view read access to the persons_data table. We need to do this because the view takes on the permissions of the person using it, and since team1 doesn’t have access to the persons_data table, they’d get an error if they tried to use this view.</p>
<p>Now we’re finally at the point where we’re going to turn this into an authorized view. In the menu next to the examples dataset, select “Share dataset”. In the menu at the left, select “Authorized View”. Then click “Select View”. Change the dataset to “shared_views” and put “persons_view” for the table. Click OK, click Add, and save the changes.</p>
<p>Now users in team1 will be able to run queries on this view, even though they don’t themselves have access to the persons_data table.</p>
<p>Before we go, you’ll probably want to delete everything you’ve created in this course. Fortunately, that will be very easy. Click the menu next to examples and select “Delete dataset”. Type in “examples” to confirm that you want to delete the dataset and all of the tables in it. Now do the same thing for the shared_views dataset.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to optimize BigQuery. Now you know how to reduce the amount of data processed by queries, speed up your queries through denormalization, and use access controls on projects, datasets, and tables.</p>
<p>To learn more about BigQuery, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know in the Comments tab below this video. Thanks and keep on learning!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/22/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><span class="page-number current">23</span><a class="page-number" href="/page/24/">24</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/24/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
