<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/55/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/55/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Hang's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Storage-CLF-C01-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:31" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:31-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:59:02" itemprop="dateModified" datetime="2022-11-20T18:59:02-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Storage in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various Storage services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Storage services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Storage services in AWS, including:</p>
<ul>
<li>The Amazon Simple Storage Service, known as S3;</li>
<li>Amazon Elastic Block Store, or EBS; and the</li>
<li>Amazon Elastic File System, or EFS.</li>
</ul>
<p>We’ll also discuss AWS services that can assist with large-scale data storage, migration, and transfer both into and out of AWS using the AWS Snow family, as well as hybrid cloud storage services and on-premises data backup solutions using AWS Storage Gateway.</p>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="Overview-of-Amazon-S3"><a href="#Overview-of-Amazon-S3" class="headerlink" title="Overview of Amazon S3"></a>Overview of Amazon S3</h1><p>Hello and welcome to this lecture where I will introduce the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon Simple Storage Service</a>, commonly known as S3. Amazon S3 is probably the most heavily used storage service that is provided by AWS simply down to the fact that it can be a great fit for many different use cases, as well as integrating with many different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. Amazon S3 is a fully managed, object-based storage service that is highly available, highly durable, very cost-effective, and widely accessible.</p>
<p>The service itself is promoted as having unlimited storage capabilities making Amazon S3 extremely scalable, far more scalable than your own on-premise storage solution could ever be. There are, however, limitations on the individual size of a single file that it can support. The smallest file size that it supports is zero bytes and the largest file size is five terabytes. Although there is this size limitation on a maximum file size, it’s one that many of us will not perceive as an ongoing inhibitor in the majority of use cases.</p>
<p>The service operates an object storage service which means each object uploaded does not conform to a data structure hierarchy like a file system would, instead its architecture exists across a flat address space and is referenced by a unique URL. Now, if you compare this to file storage, where your data is stored as separate files within a series of directories forming a data structure hierarchy much like your own files are on your own laptop or computer then S3 is very different in comparison. S3 is a regional service and so when uploading data you as the customer are required to specify the regional location for that data to be placed in.</p>
<p>By specifying your region for your data Amazon S3 will then store and duplicate your uploaded data multiple times across multiple availability zones within that region to increase both its durability and availability. For more information on regional availability zones and other AWS global infrastructure components, please see the following blog <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-global-infrastructure/">here</a>. Objects stored in S3 have a durability of ninety-nine point nine nine nine nine nine nine nine nine nine percent, known as eleven nines of durability and so the likelihood of losing data is extremely rare and this is down to the fact that S3 stores multiple copies of the same data in different availability zones. The availability of S3 data objects is dependent on the storage class used and this can range from 99.5% to 99.99%.</p>
<p>The difference between availability and durability is this: when looking at availability AWS ensures that the uptime of Amazon S3 is between 99.5% to 99.99%, depending on the storage class, to enable you to access your stored data. The durability percentage refers to the probability of maintaining your data without it being lost through corruption, degradation of data, or other unknown potential damaging effects. When uploading objects to Amazon S3, a specific structure is used to locate your data in the flat address space.</p>
<p>To store objects in S3, you first need to define and create a bucket. You can think of a bucket as a container for your data. This bucket name must be completely unique, not just within the region you specify, but globally against all other S3 buckets that exist, of which there are many millions. And this is because of the flat address space, you simply can’t have a duplicate name. Once you have created your bucket you can then begin to upload your data within it. By default your account can have up to a hundred buckets, but this is a soft limit and a request to increase this can be made with AWS. Any object uploaded to your buckets are given a unique object key to identify it.</p>
<p>In addition to your bucket, you can if required create folders within the bucket to aid with categorization of your objects for easier data management. Although folders can provide additional management from a data organization point of view, I want to reiterate that Amazon S3 is not a file system and many features of Amazon S3 work at the bucket level and not a specific folder level and so the unique object key for every object contains the bucket, any folders that are present, and also the name of the file itself. Let me now provide a quick overview via a demonstration of the Amazon S3 console and I’ll show you how to create a bucket within the service and upload an object to that bucket and then show you the unique object key of that object. Okay so I’m currently logged into my AWS management console and I can find amazon S3 under the storage category which is down here. So if I select S3 and this has taken me to the S3 dashboard.</p>
<p>Now up here we have our buckets and this list that we have here are a list of buckets that I have already created in my account so this is the bucket name which is the unique bucket identifier. And over here we have the region in which that bucket exists in, so we have some in London some in Ireland some over in US as well, and also, the date created. Over here we have access whether these buckets can be accessed by the public or not. I’m not going to dive too deep into access and security the buckets at this stage, as this is just more of an introduction to give you an overview of the console, but we do have other courses that focus on security.</p>
<p>Now, if I go into one of these buckets, for example, this one here cloudacademyaudio, we can see that I have created a folder in here called Stuart. There’s no other objects, it’s just a folder we can see that by this little icon here, so there’s no actual objects in here, this is just a folder that I created just to help me manage and categorize any objects that I do upload. If I select that folder, I can see I have two more folders here. If I go into this one, for example, I can see that I have an object I have a PNG file. So this is an object that I have uploaded to S3 and we can see that it’s in the courses folder under Stuart under the cloudacademyaudio bucket.</p>
<p>Now, if I select this object, I can get some information about it. I can open it and download it etc. We can see the last time it was modified, the storage class that it belongs to, and I’ll be talking more about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/storage-classes/">storage classes</a> in the next lecture, if there’s any encryption at rest activated, the file size, and here is the unique identifier of the key for this object. So we can see that the key comprises of any folders within the bucket and then also the object name at the end. And this here is the unique identifier of this object on S3, so it gives it a URL.</p>
<p>Now if I want to open this I can simply click on open and as we can see, it’s just an image file, or I can download the object if I want to. So I just wanted to show you there how you can use folders within your bucket and also what the object key looks like as well. So now if I go back to the console, the main dashboard where all my buckets are, I want to show you how to create a bucket quickly. It’s very simple simply click on create bucket, then we need to give it a unique bucket name.</p>
<p>Now remember, this has to be a globally unique name, so if I type in stubucketdemo and then I can select a region that I want this bucket to be in, I’m just going to select the London region, and if I want to, I can copy settings from an existing bucket, but it’s going to go through the different screens to show you the options quickly that you can have when you’re creating a bucket. Click on Next. Here’s some management, we have some management options such as versioning and server access logging. Versioning keeps all versions of an object in the same bucket and server access logging logs requests for access to your bucket. You can also use key value pair tags, you can activate object level logging which will record any API activity with CloudTrail associated with your objects and you can also encrypt your objects as well. I’m just gonna leave all those options as default for this demonstration. Click on next.</p>
<p>Here we can set different permissions, I’m just gonna leave the default block all public access so this will prevent anyone from outside of my VPC accessing any data within my bucket. Click on next. We just have a review of the settings that we selected and then all you need to do is click on create bucket. So if I scroll down to my bucket that I just created, which was stubucketdemo. If I select that, we can see here that I’ve got no folders and no objects. So if you want to create a folder, you simply click on create folder, just give it a name. If you want to add any encryption you can do so here. I’m just going to use none as a default. Now, I can either add an object directly under this bucket or I can add it into that folder. Let me just add it directly under the bucket name of stubucketdemo.</p>
<p>So to upload an object you simply click on upload, add files, select your object that you’d like to upload or objects, click on next, you know we have some permissions here as to who can read or write to the object, and as we can see here, we have the block public access setting turned on for this bucket. Click on next. Now here we have our storage classes and depending on what storage class we select it will affect the durability, the availability, and also the cost of your object being stored. Now, I’m gonna go deeper into the different storage classes in the next lecture so I won’t go over this too deep now. For the sake of this demonstration I’m just going to select the standard storage class.</p>
<p>Then we have a review page and then simply upload. And then we have my object that’s been uploaded to my bucket. Again, if I select it, I can see the object key and also the unique object URL as well. So that’s just a very quick demonstration to show you what the S3 console looks like, how to create buckets, how to create folders, and also upload objects as well, just so hopefully you can piece things together a little bit easier if you’ve not used Amazon S3 before.</p>
<h1 id="Storage-Classes"><a href="#Storage-Classes" class="headerlink" title="Storage Classes"></a>Storage Classes</h1><p>Hello and welcome to this lecture covering <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon S3</a> storage classes. As we just saw in the demonstration, I had an option to select which storage class I wanted my uploaded object to reside in. Amazon S3 offers these different storage classes to allow you to select a class based on performance features and costs and it’s down to you to select the storage class that you require for the data. The storage classes available are as follows S3 Standard, S3 Intelligent Tiering, S3 Standard Infrequent Access, S3 One Zone Infrequent Access, S3 Glacier, and S3 Glacier Deep Archive.</p>
<p>S3 Standard. This storage class is considered a general-purpose storage class. It is ideal for a range of use cases where you need high throughput with low latency with the added ability of being able to access your data frequently. By copying data to multiple availability zones, S3 Standard offers eleven nines of durability across multiple availability zones, meaning the OData remains protected against a single availability zone failure. It also offers a 99.99% availability across the year, which is the highest availability that S3 offers. From a security standpoint this storage class also has the added support of SSL, Secure Sockets Layer, for encrypting data in transit in addition to encryption options for when the data is at rest. With management features such as lifecycle rules, objects in S3 Standard can automatically be moved to another storage class. For those unfamiliar with life cycle rules, they provide an automatic method of managing the life of your data while it is being stored on Amazon S3. By adding a life cycle wall to a bucket you are able to configure and set specific criteria that can automatically move your data from one class to another or delete it from Amazon S3 altogether. You may want to do this as a cost saving exercise by moving data to a cheaper storage class after a set period of time.</p>
<p>S3 Intelligent Tiering. This storage class is ideal for those circumstances where the frequency of access to the object is unknown. Effectively, we have unpredictable data access patterns and so by using this storage class, it can help to optimize your storage costs. Depending on your data access patterns of objects in the Intelligent Tiering Class, S3 will move your objects between two different tiers, these being frequent and infrequent access. Now, these classes are a part of the Intelligent Tiering Class itself and are separate from the existing storage classes I listed earlier. When the objects are moved to Intelligent Tiering, they are placed within the frequent access tier, which is the more expensive of the two tiers. If an object is not accessed for 30 days then AWS will automatically move the object to the cheaper tier known as the infrequent access tier. Once that same object is accessed again, it will automatically be moved back to the frequent tier. Much like S3 Standard, S3 Intelligent Tiering also offers 11 nines of durability across multiple availability zones offering protection against the loss of a single AZ. However, its availability isn’t quite as high as S3 Standard as it set at 99.9%. This storage class also has the added support of SSL for encrypting data in transit in addition to encryption options for when the data is at rest. S3 Intelligent Tiering also supports the lifecycle rules and matches the same performance throughput and low latency as S3 Standard.</p>
<p>S3 Standard infrequent access. This can be seen as the equivalent to the infrequent tier from the Intelligent Tiering class as it is designed for data that does not need to be accessed as frequently as data within the Standard tier, and yet still offers high throughput and low latency access, much like S3 Standard does. As with all other S3 storage classes, it carries that 11 9s durability across multiple AZs, again by copying your objects to multiple availability zones within a single region to protect against AZ outages. It shares the same availability as Intelligent Tiering of 99.9 percent. As a result, this storage class comes at a cheaper cost than S3 Standard. Common security features such as SSL for encryption in transit and data at rest encryption is supported as well as management controls such as lifecycle rules to automatically move objects to an alternate storage class based on your requirements.</p>
<p>S3 One Zone Infrequent Access. By now you can probably assume what this storage class comprises of based off of the previous classes that I’ve already discussed. However, again, being an infrequent storage class it is designed for objects that are unlikely to be accessed frequently. It also carries the same throughput and low latency. However, the durability, although remaining at eleven nines only exists across a single availability zone. As the name implies to this class it is one zone, as in one availability zone. So the objects will be copied multiple times to different storage locations within the same availability zone instead of across multiple availability zones. This results in a 20% storage cost reduction when compared to S3 Standard. One Zone IA does, however, offer the lowest level of availability which is currently 99.5 percent and this is down to the fact that your data is being stored in a single availability zone. Should the AZ storing your data become unavailable then you will lose access to your data or even worse it may become completely lost should the AZ be destroyed in a catastrophic event. Again, life cycle rules and encryption mechanisms are in place to protect your data both in transit and at rest.</p>
<p>S3 Glacier. The next two storage classes are associated with S3 Glacier which is used for archival data. Firstly let me explain more about S3 Glacier, as it can be accessed separately from the Amazon S3 service but closely interacts with it S3 Glacier storage classes directly interact with the Amazon S3 lifecycle rules discussed previously. However, the fundamental difference with the Amazon Glacier storage classes come at a fraction of the cost when it comes to storing the same amount of data than the S3 storage classes. So what’s the catch? Well, it doesn’t provide you the same features as Amazon S3 but more importantly, it doesn’t provide you instant access to your data.</p>
<p>So what do Amazon Glacier classes offer exactly? Well, they offer an extremely low-cost long term durable storage solution which is often referred to as cold storage, ideally suited for long term backup and archival requirements. It’s capable of storing the same data types as Amazon S3, effectively any object, however, like I just mentioned it doesn’t provide instant access to your data. In addition to this, there are other fundamental differences which makes this service fit for purpose for other use cases. The service itself has 11 nines of durability making this just as durable as Amazon S3. Again this is achieved by replicating the data across multiple different availability zones within a single region but it provides the storage at a considerably lower cost compared to that of Amazon S3. And this is because retrieval of data stored in Glacier is not an instant access retrieval process. When retrieving your data it can take up to several hours to gain access to it depending on certain criteria. The data structure within Glacier is centered around vaults and Archives. Buckets and folders are not used. They are purely used for S3.</p>
<p>A Glacier vault simply acts as a container for Glacier archives. These vaults are regional and as such during the creation of these vaults, you are asked to supply the region in which they will reside. Within these vaults, we then have our data which is stored as an archive and these archives can be any object similar to S3. Thankfully you can have unlimited archives within your Glacier vaults, so from a capacity perspective, it follows the same rule as S3. Effectively you have access to an unlimited quantity of storage for your archives and vaults. Now whereas Amazon S3 provided a nice graphical user interface to view, manage, and retrieve your data within buckets and folders, Amazon Glacier does not offer this service.</p>
<p>The Glacier dashboard within AWS management console allows you to create your vaults, set data retrieval policies, and event notifications. When it comes to moving data into S3 Glacier for the first time it’s effectively a two-step process. Firstly, you need to create your vaults as your container for your archives and this could be completed using the Glacier console. Secondly, you need to move your data into the Glacier vault using the available API or SDKs. As you may be thinking, there’s also another method of moving your data into Glacier and this is by using the S3 lifecycle rules that I discussed earlier. When it comes to retrieving your archives, which is your data, you will again have to use some form of code to do so, either the APIs, SDKs or the AWS CLI. Either way, you must first create an archival retrieval job, then request access to all or part of that archive.</p>
<p>Now you have more of an understanding of S3 Glacier, let me review the two S3 Glacier storage classes. Firstly, S3 Glacier. This is the default Standard storage class within S3 Glacier offering a highly secure using in transit and at rest encryption low-cost and durable storage solution. The durability matches that of other S3 storage classes, being 11 9s across multiple availability zones, and the availability of S3 Glacier is 99.9%. It’s simple to add data to this storage class using the S3 put APIs is in addition to S3 lifecycle rules. However, it does offer a variety of retrieval options depending on how urgently you need the data back, each offering a different price point. These being expedited, Standard, and bulk.</p>
<p>Expedited. This is used when you have an urgent requirement to retrieve your data but the request has to be less than 250 megabytes. The data is then made available to you in one to five minutes and this is the most expensive retrieval option of the three.</p>
<p>Standard. This can be used to retrieve any of your archives no matter their size but your data will be available in three to five hours, so much longer than the expedited option and this is the second most expensive of the three options.</p>
<p>And finally, bulk. This option is used to retrieve petabytes of data at a time, however, this typically takes between five and twelve hours to complete. This is the cheapest of the retrieval options so it really depends on how much data and how quickly you need it, as the retrieval speed and cost to be made by your retrieval option.</p>
<p>S3 Glacier Deep Archive. Out of all the storage classes offered by S3, Glacier Deep Archive is the cheapest and again being a Glacier class, it focuses on long-term storage. This is an ideal storage class for circumstances that require specific data retention regulations and compliance with minimal access, such as those within the financial or health sector where data records might need to be legally retained for seven years or even longer. The durability and availability matches that of S3 Glacier with eleven 9s durability across multiple AZss with 99.9% availability.</p>
<p>Adding data into deep archive follows the same processes as S3 Glacier, using S3 put APIs in addition to S3 lifecycle rules. Deep Archive, however, does not offer multiple retrieval options. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> states that the retrieval of the data will be within 12 hours or less. To summarize some of the common features between the storage classes, this table clearly shows how they differ. As you can see, the main difference of the classes is the durability and availability percentages, in addition to the pricing.</p>
<p>So when selecting your class for your data you really need to be asking yourself the following questions: how critical is my data? Does it require the highest level of durability? How reproducible is the data? Can it be easily created again if need be? and how often is the data likely to be accessed? For detailed information on Amazon S3 pricing covering all storage classes discussed please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/introduction/">Understanding and Optimizing Costs with AWS Storage Services</a>.</p>
<h1 id="EC2-Instance-Storage"><a href="#EC2-Instance-Storage" class="headerlink" title="EC2 Instance Storage"></a>EC2 Instance Storage</h1><p>Hello, and welcome to this lecture covering EC2 Instance Level Storage. Which is referred to as an instance store volume. The first point to make about EC2 instance store volumes, is that the volumes physically reside on the same host that provides your EC2 instance itself, acting as local disc drives, allowing you to store data locally to that instance. Up until now within this course we have discussed persistent storage options. But instance store volumes provide ephemeral storage for you EC2 instances. </p>
<p>Ephemeral storage means that the block level storage that it provides offers no means of persistency. Any data stored on these volumes is considered temporary. With this in mind, it is not recommended to store critical or valuable data on these ephemeral instance store volumes, as it could be lost, should an event occur. By an event, let me explain under what conditions that your data would be lost, should it be stored on one of these volumes. </p>
<p>If your instance is either stopped or terminated, then any data that you have stored on that instance store volume associated with this instance will be deleted without any means of data recovery. However, if your instance was simply rebooted, your data would remain intact. Although, you can control when your instances are stopped or terminated, giving you the opportunity to either back-up the data or move it to another persistent volume store, such as the elastic block store service. Sometimes this control is not always possible. Let’s consider you had critical data stored on an ephemeral instance store volume and then the underlying host that provided your EC2 instance and storage failed. You had no warning that this failure was going to occur, and as a result of this failure, the instance was stopped or terminated. Now all of your data on these volumes is lost. When a stop and start, or termination occurs, all the blocks on the storage volume are reset, essentially wiping data. So, you might be thinking, why use these volumes? What use do they have if there is a chance that you are going to lose data? They do, in fact, have a number of benefits. </p>
<p>From a cost perspective, the storage used is included in the price of the EC2 instance. So, you don’t have an additional spend on storage cost. The I&#x2F;O speed on these volumes can far exceed those provided by the alternative instance block storage, EBS for example. When using store optimized instance families, such as the I3 Family, it’s potentially possible to reach 3.3 million random read IOPS, and 1.4 million write IOPS. With speeds like this, it makes it ideal to handle the high demands of no SQL databases. However, any persistent data required would need to be replicate or copied to a persistent data store in this scenario. Instance store volumes are generally used for data that is frequently changing; that doesn’t need to be retained, as such, they are great to be used as a cache or buffer. They are also commonly used for service within a load balancing group, where data is replicated across the fleet such as a web server pool. </p>
<p>Not all instance types support instance store volumes. So, if you do have a need where these instance store volumes would work for your use case, then be sure to check the latest AWS documentation to ascertain if the instance type you’re looking to use supports the volume. The size of your volumes, however, will increase as you increase the EC2 instance size. </p>
<p>From a security stance, instance store volumes don’t offer any additional security features. As to be honest, they are not separate service like the previous storage options I have already explained. They are simply storage volumes attached to the same host on the EC2 instance, and they are provided as a part of the EC2 service. So, they effectively have the same security mechanisms provided by EC2. This can be IAM policies dictating which instances can and can’t be launched, and what action you can perform on the EC2 instance, itself. If you have data that needs to remain persistent, or that needs to be accessed and shared by others, then EC2 instance store volumes are not recommended. If you need to use block level storage and want a quick and easy method to maintain persistency, then there is another block level service that is recommended. This being the elastic block store service.</p>
<h1 id="Overview-of-EBS"><a href="#Overview-of-EBS" class="headerlink" title="Overview of EBS"></a>Overview of EBS</h1><p>In this lecture, I shall be talking about the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-block-store-ebs-1060/course-introduction/">Amazon Elastic Block Store</a> service, known as EBS, which provides storage to your EC2 instances via EBS volumes, which offer different benefits to that of instance store volumes used with some EC2 instances.</p>
<p>EBS provides persistent and durable block level storage. As a result, EBS volumes offer far more flexibility with regards to managing the data when compared to data stored on instance store volumes. EBS volumes can be attached to your EC2 instances, and are primarily used for data that is rapidly changing that might require a specific Input&#x2F;Output operations Per Second rate, also known as IOPS.</p>
<p>EBS volumes are independent of the EC2 instance, meaning that they exist as two different resources. They are logically attached to the instance instead of directly attached like instance store volumes. From a connectivity perspective, only a single EBS volume can only ever be attached to a single EC2 instance. However, multiple EBS volumes can be attached to a single instance.</p>
<p>Due to the EBS ability to enforce persistence of data, it doesn’t matter if your instances are intentionally or unintentionally stopped, restarted, or even terminated, the data will remain intact when configured to do so. EBS also offers the ability to provide point in time backups of the entire volume as and when you need to. These backups are known as snapshots and you can manually invoke a snapshot of your volume at any time, or use Amazon CloudWatch events to perform an automated schedule of backups to be taken at a specific date or time that can be recurring.</p>
<p>The snapshots themselves are then stored on Amazon S3 and so are very durable and reliable. They are also incremental, meaning that each snapshot will only copy data that has changed since the previous snapshot was taken. Once you have a snapshot of an EBS volume, you can then create a new volume from that snapshot. So, if for any reason you lost access to your EBS volume through or incident or disaster, you can recreate the data volume from an existing snapshot and then attach that volume to a new EC2 instance. To add additional flexibility and resilience, it is possible to copy a snapshot from one region to another.</p>
<p>Looking at the subject of high availability and resiliency, your EBS volumes are, by default, created with reliability in mind. Every write to a EBS volume is replicated multiple times within the same availability zone of your region to help prevent the complete loss of data. This means that your EBS volume itself is only available in a single availability zone. As a result, should your availability zone fail, you will lose access to your EBS volume. Should this occur, you can simply recreate the volume from your previous snapshot and attach it to another instance in another availability zone.</p>
<p>There are two types of EBS volumes available. Each have their own characteristics. These being SSD backed storage, solid state drive, and HDD backed storage, hard disk drive. This allows you to optimize your storage to fit your requirements from a cost to performance perspective.</p>
<p>SSD backed storage is better suited for scenarios that work with smaller blocks. Such as databases using transactional workloads. Or often as boot volumes for your EC2 instances. Whereas HDD backed volumes are designed for better workloads that require a higher rate of throughput, such as processing big data and logging information. So, essentially working with larger blocks of data.</p>
<p>These volume types can be broken down even further. Looking at the following table we can see how different volumes can be used for both SSD and HDD volumes types.</p>
<p>You can see that depending on the use case for your EBS volume you can select the most appropriate type. Each of these volumes also offer different performance factors which include: Volume size, Max IOPS per volume, Max throughput per volume, Max IOPS per instance, Max Throughput per instance, and Dominant performance.</p>
<p>The performance of volumes change frequently and so for the latest information on these volume types, please refer to the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">here</a>.</p>
<p>For those who are not familiar with provisioned IOPS (input&#x2F;output operations per second) volumes, they deliver enhanced predictable performance for applications requiring I&#x2F;O intensive workloads. When working with these volumes you also have the ability to specify at IOPS rate during the creation of a new EBS volume, and when the volume is attached to an EBS-optimized instance, EBS will deliver the IOPS defined and required within 10%, 99.9% of the time throughout the year.</p>
<p>The throughput optimized HDD volumes are designed for frequently accessed data and are ideally suited to work well with large data sets requiring throughput-intensive workloads, such as data streaming, big data, and log processing. These volumes will deliver the expected throughput 99% of the time over a given year, and an important point to make is that these volumes can’t be used as boot volumes for your instances.</p>
<p>The cold HDD volumes offer the lowest cost compared to all other EBS volumes types. They are suited for workloads that are large in size and accessed infrequently. They will deliver the expected throughput 99% of the time over a given year, and again, it is not possible to use these as boot volumes for your EC2 instances.</p>
<p>One great feature of EBS is its ability to enhance the security of your data, both at rest and when in transit, through data encryption. This is especially useful when you have sensitive data, such as personally identifiable information, stored in your EBS volume. And in this case, you may be required to have some form of encryption from a regulatory or governance perspective. EBS offers a very simple encryption mechanism. Simple in the fact that you don’t have to worry about managing the data keys to perform the encryption process yourself. It’s all managed and implemented by EBS. All you are required to do is to select if you want the volume encrypted or not during its creation via a checkbox.</p>
<p>The encryption process uses the AES-256 encryption algorithm and provides its encryption process by interacting with another AWS service, the key management service, known as KMS. KMS uses customer master keys, CMKs, enabling the encryption of data across a range of AWS services, such as EBS in this instance.</p>
<p>To learn more about the Key Management Service, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Any snapshot taken from an encrypted volume will also be encrypted, and also any volume created from this encrypted snapshot will also be encrypted. You should also be aware that this encryption option is only available on selected instance types.</p>
<p>One final point to make on EBS encryption is that you can create a default region setting that ensures that all EBS volumes created will be encrypted by default.</p>
<p>For a detailed overview of exactly how this encryption process works, please take a look at the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/how-to-encrypt-an-ebs-volume-the-new-amazon-ebs-encryption/">blog post</a>.</p>
<p>As EBS volumes are separate to EC2 instances, you can create an EBS volume in a couple of different ways from within the management console. During the creation of a new instance and attach it at the time of launch, or from within the EC2 dashboard of the AWS management console as a standalone volume ready to be attached to an instance when required. When creating an EBS volume during an EC2 instance launch, at step four of creating that instance, you are presented with the storage configuration options. Here you can either create a new blank volume or create it from an existing snapshot. You can also specify the size and the volume type, which we discussed previously. Importantly, you can decide what happens to the volume when the instance terminates. You can either have the volume to be deleted with the termination of the EC2 instance, or retain the volume, allowing you to maintain the data and attach it to another EC2 instance. Lastly, you also have the option of encrypting the data if required.</p>
<p>You can also create the EBS volume as a standalone volume. By selecting the volume option under EBS from within the EC2 dashboard of the management console, you can create a new EBS volume where you’ll be presented with the following screen.</p>
<p>Here you will have many of the same options. However, you can specify which availability zone that the volume will exist in, allowing you to attach it to any EC2 instance within that same availability zone. As you might remember, EBS volumes can only be attached to EC2 instances that exist within the same availability zone.</p>
<p>EBS volumes also offer the additional flexibility of being able to resize them elastically should the requirement arise. Perhaps you’re running out of disk space and need to scale up your volume. This can be achieved by modifying the volume within the console or via the AWS CLI. You can also perform the same resize of the volume by creating a snapshot of your existing volume, and then creating a new volume from that snapshot with an increased capacity size.</p>
<p>As we can see, EBS offers a number of benefits over EC2 instance store volumes. But EBS is not well suited for all storage requirements. For example, if you only needed temporary storage or multi-instance storage access, then EBS is not recommended, as EBS volumes can only be accessed by one instance at a time. Also, if you needed very high durability and availability of data storage, then you would be better suited to use Amazon S3 or EFS, the elastic file system.</p>
<p>That now brings me to the end of this lecture and to the end of this introductory course and you should now have a greater understanding of the Amazon Elastic Block Store service and how it can be used as a storage option for your EC2 instances.</p>
<p>If you have any feedback, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="What-is-the-Amazon-Elastic-File-System"><a href="#What-is-the-Amazon-Elastic-File-System" class="headerlink" title="What is the Amazon Elastic File System?"></a>What is the Amazon Elastic File System?</h1><p>Hello and welcome to this lecture where I will explain what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-file-system-1137/course-introduction/">Amazon EFS</a> service is and how it fits into the storage ecosystem. Let me start by taking a step back and looking at where the EFS service fits in within the world of AWS storage. Firstly, I want to look at the array of AWS storage offerings and compare a few of them. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> has more storage solutions than I’m going to discuss in this course and I will probably continue to add more in the future. But I’m just going to focus on three different services. The reason I’ve selected these three is that, at first glance, they may seem similar and many people can be unsure which of these solutions to choose from to fit their current storage requirements. </p>
<p>Amazon Simple Storage Service or S3 is an object storage solution. Object storage stores everything as a single object, not in small chunks or blocks. With this type of storage, you upload a file and if the file changes to replace it, the entire file will be replaced. This type of storage is best for situations where files are written once and then accessed many times. It’s not optimal for situations that require both heavy read and write access at the same time. So Amazon S3 is usually used for storage of large files such as video files, images, static websites, and backup archives. For example, Netflix uses S3 for their data streaming service. They upload large movie files once and then subscribers access and play the movies many, many times. </p>
<p>The next service is the Amazon Elastic Block Store or EBS, and it’s block-level storage. Files are not stored as single objects. They’re stored in small chunks of blocks so that only the portion of the file that is changed will be updated. This type of storage is optimized for low latency access and when fast, concurrent read and write operations are needed. EBS provides persistent block storage volumes for use with a single EC2 instance. As described, EBS is persistent, meaning that even if you stop or terminate an EC2 instance that’s using EBS, the data on the EBS volume remains intact. You should use this type of storage like a computer hard drive where you store operating system files, applications and other files you wish to obtain for use with your EC2 instance.</p>
<p>Amazon Elastic File System, or EFS, is considered file-level storage and is also optimized for low latency access, but unlike EBS, it supports access by multiple EC2 instances at once. It appears to users like a file manager interface and uses standard file system semantics such as locking files, renaming files, updating files and uses a hierarchy structure. This is just like what we’re used to on standard premise-based systems. This type of storage allows you to store files that are accessible to network resources. </p>
<p>Before diving deep on EFS, let me discuss how people are traditionally used to accessing network files and resources. In traditional premises-based networks, users access files by browsing network resources that connect to a server, perhaps via a mapped drive that has been configured for them, and once they connect, they will see a tree view of available folders and files. This functionality is generally provided by various local area network systems such as file servers or storage area network, a SAN, or network-attached storage, a NAS. </p>
<p>Now let’s move on from the traditional premises-based solutions and talk about cloud-based solutions, specifically within AWS and the Amazon Elastic File System service. EFS provides simple, scalable file storage for use with Amazon EC2 instances. Much like traditional file servers, or a SAN or a NAS, Amazon EFS provides the ability for users to browse cloud network resources. EC2 instances can be figured to access Amazon EFS instances using configured mount points. Now, mount points can be created in multiple availability zones that attach to multiple EC2 instances. So, much like your traditional land servers, EC2 instances are connected to a network file system, Amazon EFS. So from a user standpoint, the result is the same. The user accesses network resources just as they always have done except for now, it’s done using cloud resources. </p>
<p>EFS is a fully managed, highly available and durable service that allows you to create shared file systems that can easily scale to petabytes in size with low latency access. EFS has been designed to maintain a high level of throughput in addition to low latency access response, and these performance factors make EFS a desirable storage solution for a wide variety of workloads, and use cases and can meet the demands of tens, hundreds or even thousands of EC2 instances concurrently. Being a managed service, there is no need for you to provision any file servers to manage the storage elements or provide any maintenance of those servers. This makes it a very simple option to provide file-level storage within your environment. It uses standard operating system APIs, so any application that is designed to work with standard operating system APIs will work with EFS. It supports both NFS versions 4.1 and 4.0, and uses standard file system semantics such as strong consistency and file locking. It’s replicated across availability zones in a single region making EFS a highly reliable storage service. </p>
<p>As the file system can be accessed by multiple instances, it makes it a very good storage option for applications that scale across multiple instances allowing for parallel access of data. The EFS file system is also regional, and so any application deployments that span across multiple availability zones can all access the same file systems providing a level of high availability of your application storage layer. At the time of writing this course, EFS is not currently available within all regions. For a list of supported regions, please visit the following link: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region">https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region</a>. </p>
<p>That now brings me to the end of this lecture.</p>
<h1 id="Storage-Classes-and-Performance-Options"><a href="#Storage-Classes-and-Performance-Options" class="headerlink" title="Storage Classes and Performance Options"></a>Storage Classes and Performance Options</h1><p>Hello and welcome to this lecture, where I will be discussing the different storage class options that EFS provides in addition to how you can alter and configure certain performance factors depending on your use case of EFS. Now, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/introduction/">Amazon EFS</a> offers two different storage classes to you, which each offer different levels of performance and cost, these being Standard and Infrequent Access, known as IA. The Standard storage class is the default storage used when using EFS. However, Infrequent Access is generally used if you’re storing data on EFS that is rarely accessed. By selecting this class, it offers a cost reduction on your storage. </p>
<p>The result of the cheaper storage means that there is an increased first-byte latency impact when both reading and writing data in this class when compared to that of Standard. The costs are also managed slightly differently. When using IA, you are charged for the amount of storage space used, which is cheaper than that compared to Standard. However, with IA, you are also charged for each read and write you make to the storage class. This helps to ensure that you only use this storage class for data that is not accessed very frequently, for example, data that might be required for auditing purposes or historical analysis. </p>
<p>With the Standard storage class, you are only charged on the amount of storage space used per month. Both storage classes are available in all regions where EFS is supported. And importantly, they both provide the same level of availability and durability. If you are familiar with S3, then you may also be familiar with S3 lifecycle policies for data management. Within EFS, a similar feature exists known as EFS lifecycle management. When enabled, EFS will automatically move data between the Standard storage class and the IA storage class. This process occurs when a file has not been read or written to for a set period of days, which is configurable, and your options for this period range include 14, 30, 60, or 90 days. </p>
<p>Depending on your selection, EFS will move the data to the IA storage class to save on cost once that period has been met. However, as soon as that same file is accessed again, the timer is reset, and it is moved back to the Standard storage class. Again, if it has not been accessed for a further period, it will then be moved back to IA. Every time a file is accessed, its lifecycle management timer is reset. The only exceptions to data not being moved to the IA storage class is for any files that are below 128K in size and any metadata of your files, which will all remain in the Standard storage class. </p>
<p>If your EFS file system was created after February 13th, 2019, then the life cycle management feature can be switched on or off. Let me now take a look at the different performance modes that EFS offers. AWS are where the EFS can be used for a number of different use cases and workloads, and as such, each use case might require a change of performance from a throughput, IOPS, and latency point of view. As a result, AWS has introduced two different performance modes that can be defined during the creation of your EFS file system. These being General Purpose, and Max I&#x2F;O.</p>
<p>Now, General Purpose is a default performance mode and is typically used for most use cases. For example, home directories and general file-sharing environments. It offers an all-round performance and low latency file operation, and there is a limitation of this mode allowing only up to 7,000 file system operations per second to your EFS file system. If, however, you have a huge scale architecture, where your EFS file system is likely to be used by many thousands of EC2 instances concurrently, and will exceed 7,000 operations per second, then you’ll need to consider Max I&#x2F;O. Now, this mode offers virtually unlimited amounts of throughput and IOPS. The downside is, however, that your file operation latency will take a negative hit over that of General Purpose. </p>
<p>The best way to determine which performance option that you need is to run tests alongside your application. If your application sits comfortably within the limit of 7,000 operations per second, then General Purpose will be best suited, with the added plus point of lower latency. However, if your testing confirms 7,000 operations per second may be reached or exceeded, then select Max I&#x2F;O.</p>
<p>When using the General Purpose mode of operations, EFS provides a CloudWatch metric percent I&#x2F;O limit, which will allow you to view operations per second as a percentage of the top 7,000 limit. This allows you to make the decision to migrate and move to the Max I&#x2F;O file system, should your operations be reaching that limit. </p>
<p>In addition to the two performance modes, EFS also provides two different throughput modes, and throughput is measured by the rate of mebibytes. The two modes offered are Bursting Throughput and Provisioned Throughput. Data throughput patterns on file systems generally go through periods of relatively low activity with occasional spikes in burst usage, and EFS provisions throughput capacity to help manage this random activity of high peaks.</p>
<p>With the Bursting Throughput mode, which is the default mode, the amount of throughput scales as your file system grows. So the more you store, the more throughput is available to you. The default throughput available is capable of bursting to 100 mebibytes per second, however, with the standard storage class, this can burst to 100 mebibytes per second per tebibyte of storage used within the file system.</p>
<p>So, for example, presume you have five tebibytes of storage within your EFS file system. Your burst capacity could reach 500 mebibytes per second. The duration of throughput bursting is reflected by the size of the file system itself. Through the use of credits, which are accumulated during periods of low activity, operating below the baseline rate of throughput set at 50 mebibytes per tebibyte of storage used, which determines how long EFS can burst for. Every file system can reach its baseline throughput 100% of the time. By accumulating, getting credits, your file system can then burst above your baseline limit. The number of credits will dictate how long this throughput can be maintained for, and the number of burst credits for your file system can be viewed by monitoring the CloudWatch metric of BurstCreditBalance. </p>
<p>If you are finding that you’re running out of burst credits too often, then you might need to consider using the Provisioned Throughput mode. Provisioned Throughput allows you to burst above your allocated allowance, which is based upon your file system size. So if your file system was relatively small but the use case for your file system required a high throughput rate, then the default bursting throughput options may not be able to process your request quick enough. In this instance, you would need to use provisioned throughput. However, this option does incur additional charges, and you’ll pay additional costs for any bursting above the default option of bursting throughput. That brings me to the end of this lecture, now I want to shift my focus on creating and connecting to an EFS file system from a Linux based instance.</p>
<h1 id="What-is-the-Snow-Family"><a href="#What-is-the-Snow-Family" class="headerlink" title="What is the Snow Family?"></a>What is the Snow Family?</h1><p>In this lecture I want to answer 2 simple questions:</p>
<ol>
<li>What is the snow family </li>
<li>and what does it consist of?</li>
</ol>
<p>So firstly, what is it? The snow family consists of a range of physical hardware devices that are all designed to enable you to transfer data into <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> from the edge or beyond the Cloud, such as your Data Center, but they can also be used to transfer data out of AWS too, for example, from Amazon S3 back to your Data Centre. </p>
<p>It’s unusual when working with the cloud to be talking about physical devices or components, normally your interactions and operations with AWS generally happen programmatically via a browser or command line interface. The snow family is different, instead, you will be sent a piece of hardware packed with storage and compute capabilities to perform the required data transfer outside of AWS, and when complete, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">device</a> is then sent back to AWS for processing and the data uploaded to Amazon S3.</p>
<p>You can perform data transfers from as little as a few terabytes using an AWS snowcone all the way up to a staggering 100 petabytes using a single AWS snowmobile, and I’ll be talking more about these different snow family members shortly. Now of course when we are talking about migrating and transferring data at this magnitude, using traditional network connectivity is sometimes simply not feasible from a time perspective. For example, let’s assume you needed to transfer just 1petabye of data over a 1gbps using Direct Connect it would take 104 Days, 5 Hours 59 Minutes, 59.25 Seconds, not forgetting the cost of the data transfer fees too! </p>
<p>In addition to these devices packing some serious storage capacity for data transfer, some of them also come fitted with compute power, allowing you to run usable EC2 instances that have been designed for the snow family enabling your applications to run operations in often remote and difficult to reach environments, even without having a data center in sight, and when working with a lack of persistent networking connectivity or power. For example, the snowcone comes with the ability to add battery packs increasing their versatility. The enablement of running EC2 instances makes it possible to use these devices at the edge to process and analyze data much closer to the source.   </p>
<p>So let’s now take a look at what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/introduction/">snow family</a> consists of to get a better understanding of what these devices are.</p>
<p>As you can see from this table, both from a physical and capacity perspective, the snowcone is the smallest followed by the snowball and finally the snowmobile. You may also notice that the snowball comes in 3 choices, compute optimized, compute optimized with GPU, and storage optimized, each offering a different use case, however, each of these 3 offerings all come in the same size device. </p>
<h1 id="Which-Snow-Device-Do-I-Need"><a href="#Which-Snow-Device-Do-I-Need" class="headerlink" title="Which Snow Device Do I Need?"></a>Which Snow Device Do I Need?</h1><p>In this lecture, I will be looking at different scenarios to help you decide which <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/introduction/">snow device</a> to use and when.</p>
<p>So we have the following snow devices AWS Snowcone, AWS Snowball, and AWS Snowmobile. </p>
<p>The AWS Snowcone is the smallest of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/what-is-the-snow-family/">snow family</a>, this has been designed to be lightweight, easily portable, allowing you to easily use the device pretty much anywhere and under any conditions due to the ruggedness of the casing, and the added advantage of being able to run on battery should a persistent mains connection not be available. It can easily fit into a standard backpack, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> have even demonstrated that the snowcone can be attached to a drone emphasising its portability and versatility. Packed with 8TB of storage and an EC2 instance, this device is perfect for taking your computing needs way beyond the cloud and your Data Centre allowing you to capture, process, and analyze data, perhaps via IoT sensors, which can then be shipped back to AWS for data transfer, or you could even use AWS DataSync to transfer the data on-line over your traditional network connectivity. </p>
<p>For those unaware of AWS DataSync, it’s a service that allows you to easily and securely transfer data from your Snowcone or your on-premise data center, to AWS storage services. It can also be used to manage data transfer between 2 different AWS storage services too, so it’s a great service to help you migrate, manage, replace and move data between different storage locations.  </p>
<p>This is essentially the elder sibling of the Snowcone, it’s bigger in size and it contains a greater amount of storage and compute power. This brings a new set of use cases for this device, it’s primarily used for large scale data transfer operations, up to 80TB at a time, both in and out of AWS. The devices themselves can be rack mounted in your data centre, and if need be clustered in groups of 5-10 devices. Unlike Snowcones, they can’t be powered by battery expansion packs, and they are not as portable, for example, you can’t stick a snowball in a backpack and walk up a mountain, or strap it to a drone! </p>
<p>The Storage optimized snowball is targeted for data migrations and transfers with its storage being compatible with both S3 object storage and EBS volumes. The Compute optimized snowball is a great option if you need to handle compute intensive edge computing workloads in disconnected environments. From a storage perspective, it also comes with 42 TB of usable HDD capacity which comes compatible with EBS volumes and S3 object storage. The Compute Optimized with GPU option is used to accelerate AI, HPC, and graphics, which is great when working with video analysis and graphic intensive use cases. </p>
<p>Both the Snowcone and Snowball can be used for many of the same use cases which I will reference in just a moment, so if that’s the case, when would you use the snowcone over the snowball and vise versa?</p>
<p>You would use the snowcone if you:</p>
<ul>
<li>Needed a portable device that you could easily carry to difficult to reach locations and situations</li>
<li>Only needed a maximum of 8TB storage</li>
<li>If you needed the ability to perform on-line data transfer using AWS DataSync, preventing you the need to send the Snowcone back to AWS for an off-line data transfer</li>
<li>If you didn’t have a consistent power support and you needed the support of a battery pack</li>
</ul>
<p>Alternatively, you would use the Snowball device if you: </p>
<ul>
<li>Didn’t need to provide mobility to the snow device and it could remain in one location for a set period of time</li>
<li>Needed to transfer data of up to 80TB</li>
<li>Needed the ability to run enhanced graphics processing by using the Compute Optimized with GPU option</li>
<li>Had a requirement to transfer data using S3 API’s</li>
<li>Required the use of usable SSD Storage </li>
<li>Needed to optimized network ports that could reach speeds of up to 100Gbit, as Snowcones only have network port speeds of 10Gbit</li>
<li>Needed to cluster your snowballs. Clustering allows you to order between 5-10 snowball devices, acting as a single pool of resources. This allows you to gain a larger storage capacity, and also enhance the level of durability of the data should a snowball fail.  Clustering is only an option if you are looking to simply perform local compute and storage workloads without transferring any data back to AWS.</li>
<li>Needed to rack mount your devices providing the opportunity to implement temporary installations of both compute and storage</li>
<li>Required the snow device to be HIPAA compliant</li>
</ul>
<p>Ok, so that should provide a better understanding of how the Snowcone and Snowball differ. Let me now run through a couple of scenarios of when you might use these devices in the real world.</p>
<p>Both the snowcone and Snowballs are perfectly suited to provide a level of portable edge computing allowing you to collect data from wireless sensors or networked resources, for example in locations such as industrial warehouses or manufacturing plants, where you might need to collect environmental metric data. By collecting and gathering data it can then be transferred to AWS offline, or if using the snowcone it can be transferred on-line using AWS DataSync, which can then be analyzed at scale using other AWS services.</p>
<p>With storage capabilities of up to 80TB of usable HDD storage from a single snowball, it easily allows you to provide a means of securely storing and transferring a large amount of data into AWS, and you can run multiple snowballs in parallel allowing you to transfer petabytes of data if required. Being of rugged design and portable, the devices can be used in remote locations, such as mining and oil sectors, or even in the travel industry, fitted to trucks, trains, and boats, providing a mechanism of easily collecting data and then transferring it back to AWS.</p>
<p>Another common use case is from within the media and entertainment industry, the Snowcone and snowball can be used as a way to aggregate data from multiple sources before shipping it back to AWS for transfer into Amazon S3. You might get video and audio data from multiple feeds, especially if you are working in the film industry, this data can then be aggregated to your snowball device and shipped back to AWS for further processing and editing from your wider production team.</p>
<p>So we have covered Snowball and Snowcone, but let’s now turn the attention to the AWS Snowmobile, what is the primary use case for this? Well, it’s quite simple, the AWS Snowmobile is used to transfer MASSIVE amounts of data from a single location, up to 100PB per snowmobile which arrives on a truck as a ruggedized shipping container. When you are talking about data transfer of this scale you are normally looking at migrating entire data centers to a new location, or migrating entire storage libraries or repositories, and so AWS snowmobile is a great solution to help you with this when you need it done quickly, securely and cost-efficiently. You can run multiple snowmobiles in parallel which will allow you to transfer Exabytes of data! Generally, you would use AWS snowmobiles if you needed to transfer more than 10petabytes of data, anything less than this then you might want to consider using multiple snowball devices. </p>
<h1 id="Key-Features-of-the-Snow-Family"><a href="#Key-Features-of-the-Snow-Family" class="headerlink" title="Key Features of the Snow Family"></a>Key Features of the Snow Family</h1><p>The snowcone and the snowball are similar in their size and construction with regards to the casing compared to that of the snowmobile which is in a class of its own, I mean it comes on a Truck! </p>
<p>So let me talk about the Snowcone and Snowball for a moment. These two hardware appliances share some key features between them that I want to highlight. </p>
<p>When transferring data to these devices it is automatically encrypted to protect the data. This encryption is backed by keys generated by the Key Management Service (KMS). To enhance the security of the device, the encryption keys are not stored on the device during transit. If you would like to learn more about the Key Management Service, then you can see our existing course here. </p>
<p>Following on from encryption, another security feature that these devices have in common is that their enclosure is Anti-tamper in addition to specific verification checks on the boot environment when the device is first switched on. These measurements and checks help to validate the integrity of the data to ensure that it has not been interfered with at all during transit.</p>
<p>In addition to their enclosure being secure by design, it is also highly rugged and able to withstand the harshest of environments, for example, the snowcone can operate in conditions of -32ºC&#x2F;-25.6ºF to 63ºC&#x2F;145.4ºF. It’s all windproof, dustproof and water-resistant! The enclosures are designed to withstand operational vibrations and shockproof should you drop the device.</p>
<p>Each of the snow devices uses an E Ink shipping label which is pre-loaded with the delivery details entered on the associated job. When the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">snow device</a> leaves AWS premises it can be tracked via SNS, text messaging and the AWS Management Console. When the device is prepared to be returned to AWS premises, the E Ink automatically updates with the appropriate location</p>
<p>As these devices are packed full of data, it’s essential that the data is deleted once the transfer has been completed and the snow device is no longer required. As a result, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> carries out a secure erase which meets the National Institute of Standards and Technology, more commonly known as NIST for the sanitization of the media and storage. </p>
<p>Before I finish this lecture I just want to point out a couple of features related to the snowmobile. </p>
<p>Each snowmobile is sent with a connector rack allowing you to connect it to the backbone network of your own data center, as a result, this rack comes with up to 2 kilometers of network cabling.</p>
<p>The Snowmobile is designed to operate within ambient temperature up to 85F (29.4C). If the temperature exceeds this then an additional auxiliary chiller unit can be supplied by AWS following a site assessment survey. If there isn’t sufficient power to feed the snowmobile at the data center then AWS can also send a separate generator to power the snowmobile, however, this requires the same space required to home a snowmobile.</p>
<p>As with the Snowcone and snowball, the snowmobile also encrypts data backed by the Key Management Service. Also, the snowmobile is only ever operated by AWS personnel and can also be escorted by an additional security vehicle during the transit of the container to and from premises, in addition to having GPS tracking available. As added security, the container is also protected via 24&#x2F;7 video surveillance systems and alarms.</p>
<h1 id="AWS-OpsHub"><a href="#AWS-OpsHub" class="headerlink" title="AWS OpsHub"></a>AWS OpsHub</h1><p>This is going to be a very quick review of AWS OpsHub, which has been designed with the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/key-features-of-the-snow-family/">AWS Snow family</a> of services in mind. It provides a graphical user interface to help you manage your snow family of devices. It doesn’t come as a service as you would normally see in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Management Console, instead, it’s an application that can be downloaded onto a Mac OS or Windows client. </p>
<p>When you receive a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">snow device</a> from the snow family, you can download the AWS OpsHub management software which can then be used to unlock your device allowing you to configure and manage it and it’s operations. As a result, within a very short space of time, you are able to begin your data transfer through simple drag and drop operations. Previously, you were required to configure and manage the device by using the AWS CLI or REST APIs, this approach of using AWS OpsHub makes it a lot more intuitive and easier to do.</p>
<p>It can also be used to configure fleets of clustered snow devices if you have requested more than one. It also comes with a dashboard which provides an overview and summary screen displaying metrics relating to your snow device and these metrics relate to both storage and compute resources. Integrating with AWS Systems Manager, it can also easily help you with the automation of different tasks.</p>
<p>So in summary, it’s a very useful tool allowing you to quickly and easily manage your snow device upon arrival using a simple GUI.</p>
<h1 id="2Overview-of-Amazon-S3"><a href="#2Overview-of-Amazon-S3" class="headerlink" title="2Overview of Amazon S3"></a>2<strong>Overview of Amazon S3</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-global-infrastructure/">AWS global infrastructure components</a></p>
<h1 id="5Overview-of-EBS"><a href="#5Overview-of-EBS" class="headerlink" title="5Overview of EBS"></a>5<strong>Overview of EBS</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/how-to-encrypt-an-ebs-volume-the-new-amazon-ebs-encryption/">Blog post: How to Encrypt an EBS Volume</a></p>
<h1 id="6What-is-the-Amazon-Elastic-File-System"><a href="#6What-is-the-Amazon-Elastic-File-System" class="headerlink" title="6What is the Amazon Elastic File System?"></a>6<strong>What is the Amazon Elastic File System?</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region">List of supported regions</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:29" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:29-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:36" itemprop="dateModified" datetime="2022-11-20T19:01:36-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Knowledge-Check-Compute-CLF-C01.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:27" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:27-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:02:06" itemprop="dateModified" datetime="2022-11-20T19:02:06-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Linux-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Linux-6/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Linux-6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:26" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:26-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:02:26" itemprop="dateModified" datetime="2022-11-20T19:02:26-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Linux-6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Linux-6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Compute-CLF-C01-5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:25" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:25-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:30" itemprop="dateModified" datetime="2022-11-20T18:58:30-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Compute-CLF-C01-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Compute in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various compute services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#x75;&#112;&#x70;&#111;&#114;&#116;&#64;&#99;&#108;&#111;&#x75;&#x64;&#97;&#x63;&#97;&#100;&#101;&#x6d;&#121;&#x2e;&#99;&#x6f;&#109;">&#x73;&#x75;&#112;&#x70;&#111;&#114;&#116;&#64;&#99;&#108;&#111;&#x75;&#x64;&#97;&#x63;&#97;&#100;&#101;&#x6d;&#121;&#x2e;&#99;&#x6f;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Compute services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Compute services in AWS, including:</p>
<ul>
<li>Elastic Compute Cloud, or EC2;</li>
<li>The Elastic Container Service, also known as ECS;</li>
<li>The Elastic Container Registry, or ECR;</li>
<li>The Elastic Container Service for Kubernetes, known as EKS;</li>
<li>AWS Elastic Beanstalk;</li>
<li>AWS Lambda;</li>
<li>AWS Batch; and</li>
<li>Amazon Lightsail.</li>
</ul>
<p>We’ll also introduce the concept of Elastic Load Balancing, along with the different types of load balancers you can provision within the AWS Cloud. And finally we’ll discuss EC2 Auto Scaling and see how it works together with Elastic Load Balancing to help you build robust, highly available web applications.</p>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="What-is-Compute-in-AWS"><a href="#What-is-Compute-in-AWS" class="headerlink" title="What is Compute in AWS?"></a>What is Compute in AWS?</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/products/compute/">Cloud Compute Index</a></p>
<p><strong>Transcript</strong></p>
<p>Hello, and welcome to this very short lecture where we are going to answer the question, what is <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">Compute in AWS</a>? Before we begin to explore Compute services, resources and features, we must first understand what is meant by the term Compute. So what is it? </p>
<p>Put simply, Compute resources can be considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions. So essentially Compute is closely related to common server components, which many of you will already be familiar with, such as CPUs and RAM. With that in mind, a physical server within a data center would be considered a Compute resource, as it may have multiple CPUs and many gigs of RAM to process instructions given by the operating system and applications. </p>
<p>Within AWS, there are a number of different services and features that offer Compute power to provide different functions. Some of these services provide Compute, which can comprise of utilizing hundreds of EC2 instances, or virtual servers, which may be used continuously for months or even years, processing millions upon millions of instructions. On the other end of this scale, you may only utilize a hew hundred milliseconds of Compute resource to execute just a few lines of code within AWS Lambda before relinquishing that Compute power. AWS Lambda is a serverless Compute resource in AWS, and I’ll cover more on this service later in this course. Compute resources can be consumed in different quantities, for different lengths of time across a range of categories, offering a wide scope of performance and benefit options. So it will really depend on your requirements as to which Compute resource you use within AWS. </p>
<p>In this course, we’ll discuss them all, allowing you to decide which is best for your implementation. As a quick high level reference, AWS offers a Cloud Compute Index, which can be found using the <a target="_blank" rel="noopener" href="https://aws.amazon.com/products/compute/">link</a> onscreen. And this shows different examples and scenarios of where you might use different Compute deployment units. That brings me to the end of this very short lecture. Now we are aware of what Compute is, let’s start by looking at some of the services offered by AWS that provide this Compute resource, starting with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/ec2-elastic-compute-cloud/">Elastic Cloud Compute, EC2</a>.</p>
<h1 id="EC2-Elastic-Compute-Cloud"><a href="#EC2-Elastic-Compute-Cloud" class="headerlink" title="EC2 - Elastic Compute Cloud"></a>EC2 - Elastic Compute Cloud</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Blog Post about Shared Responsibility Model and Security Groups</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/create-your-first-amazon-ec2-instance-1/">Lab: Create your first Amazon EC2 Instance (Linux)</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/">Lab: Ceate your first Amazon EC2 Instance (Windows)</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture where I will explain what the EC2 service is and does, and how to configure an EC2 instance, so let’s get started. As EC2 is one of the most common <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> services used within AWS. I will discuss this service in greater detail over the other services that we’ll cover in this course. </p>
<p>EC2 is arguably the first compute service that you will encounter when working with AWS. It allows you to deploy virtual servers within your AWS environment and most people will require an EC2 instance within their environment as a part of at least one of their solutions. There are a number of elements in creating your EC2 instance, which I want to break down and explain. This will hopefully help to define how the service works and answer a number of questions that you may have. The EC2 service can be broken down into the following components. Amazon machine images, AMIs, instant types, instance purchasing options, tenancy, user data, storage options and security. Let’s look at each of these individually. </p>
<p>The first point I want to cover are AMIs, Amazon Machine Images. These are essentially templates of pre-configured EC2 instances which allow you to quickly launch a new EC2 instance based on the configuration within the AMI. This prevents you from having to install an operating system or any other common applications that you might need to install on a number of other EC2 instances. From a high level perspective an AMI is an image baseline that will include an operating system and applications along with any custom configuration. AWS provides a large number of AMIs covering different operating systems from Linux to Red Hat to Microsoft Windows among others. when configuring your EC2 instance, selecting your AMI is the first configuration choice you’ll need to make. You can also create your own AMI images to help you speed up your own deployment. For example you would start with selecting an AWS AMI, let’s say a Linux server. And then once it is up and running you may then need to install a number of your own custom applications and make specific configuration changes. Now if you needed another server to perform the same functionality, you could go through the same process of selecting a Linux AWS AMI, and again manually installing the applications and making your configurations. Or once you have made those changes on the first instance you can then simply create a brand new AMI or template of that instance with all the applications installed and configurations already made. Then if you need another instance of the same configuration all you would need to do is to select your custom AMI as the base image for your instance and it will launch with Linux server, your custom applications already installed and any configurations already made. As you can see this has many benefits and certainly comes in useful when implementing auto scaling. </p>
<p>In addition to both AWS managed and your custom managed AMIs, you could also select an AMI from the AWS marketplace. The AWS marketplace is essentially an online store that allows you to purchase AMIs from trusted vendors like Cisco, Citrix, Alert Logic et cetera. These vendor AMIs may have specific applications and configurations already made, such as instances that are optimized with built-in security and monitoring tools or contained database migration systems. Lastly community AMIs also exists which are a repository of AMIs that have been created and shared by other AWS members. </p>
<p>Let’s now take a look at instance types, once you have selected your AMI from any of the different sources already discussed, you must then select an instance type. An instance type simply defines the size of the instance based on a number of different parameters, these being ECUs. This defines a number of EC2 compute units for instance, vCPUs this is the number of virtual CPUs on the instance. Physical processor, this is the process speed used on the instance. Clock speed, it’s clock speed in gigahertz. Memory, the amount of memory associated. Instance storage this is the capacity of the local instance store volumes available. EBS optimized available, this defines if the instance supports EBS optimized storage or not. Network performance, this shows the performance level of rate of data transfer. IPV6 support, this simply indicates if the instance type supports IPV6. Process architecture this shows the architected type of the processor. AES-NI, this stands for advanced encryption standard new instructions and it shows if the instance supports it for enhanced data protection. AVX this indicates if the instance supports AVX which is advanced vector extensions, which are primary used for applications focused on audio and video, scientific calculations and 3D modeling analysis. And finally Turbo which shows if the instance supports intel turbo boost and AMD turbo core technologies. The key parameters here to primarily be aware of for general usage of an EC2 instance, could be summarized as vCPUs and memory, instant storage and network performance. But obviously this really depends on your actual usage and application. Having this flexibility of variant instances allows you to select the most appropriate size or power of an instance that you need for optimal performance within your applications. These different instance types are categorized into different family types that offer distinct performance benefits, which again helps you to select the most appropriate instance for your needs. Within each of these instance families you will have a range of instant types with varied CPU, memory, storage and network performance et cetera. </p>
<p>These instance families can be summarized as follows. Micro instances, these instances have a low cost against them due to the minimal amount of CPU and memory power that they offer. These are ideal for very low throughput use cases such as low traffic websites. General-purpose, instance types within this family have a balanced mix of CPU memory and storage making them ideal for small to medium databases, tests and development servers and back-end servers. Compute optimized, as the name implies instance types within this family have a greater focus on compute. They have the highest performing processes installed allowing them to be used for high-performance front end servers, web servers, high-performance science and engineering applications and video encoding and batch processing. GPU, GPU stands for graphics processing unit. And so the instances within this family are optimized for graphic intensive applications. FPGA, this family of instances allows you to customize field programmable gate arrays. To create application specific hardware accelerations when used with applications that use massively parallel processing power such as genomics and financial computing. Memory optimized, this family include instance types that are primarily used for large-scale enterprise class in-memory applications, such as performing real time processing of unstructured data. They are also ideal for enterprise applications such as Microsoft SharePoint. These instances of the lowest cost per gigabyte of RAM against all other instance families. Storage optimized, as expected these are optimized for enhanced storage. Instances in this family use SSD backed instant storage for low latency and very high I&#x2F;O, input&#x2F;output performance, including very high IOPS which is input&#x2F;output operations per second. And these are great for analytic workloads and no SQL databases. Data file systems and log processing applications. </p>
<p>Instance purchasing options. You can purchase EC2 instances through a variety of different payment plans. These have been designed to help you save cost by selecting the most appropriate option for your deployment. The different EC2 payment options are as follows, on-demand instances, reserved instances, scheduled instances, spot instances and on-demand capacity reservations. It’s good to be aware of these different options as well having an understanding of these can help you save a considerable amount of money depending on your use case. Let me run through each option to help explain. Starting with on-demand instance</p>
<p>These are EC2 instances that you can launch at any time and have it provisioned and available to you within minutes. You can use this instance for a shorter time or for as long as you need before terminating the instance. These instances have a flat rate and is determined on the instance type selected and is paid by the second. On-demand instances are typically used for short term uses where workloads can be irregular and where workload can be interrupted. Many users of AWS use on-demand instances within their testing and development environments. And when you stop or terminate your on-demand instance you’ll stop paying for the compute resource. </p>
<p>Reserved instances allow you to purchase a discount for an instance type with set criteria for a set period of time in return for a reduced cost compared to on-demand instances. This reduction can be as much as 75%. These reservations against instances must be purchased in either one or three year time frames. Further reductions can be achieved with reserved instances depending on which payment methods you select. There are three options available to you, firstly all upfront. The complete payment for the one or three year reservation is paid. And this offers the largest discount and no further payment is required regardless of the number of hours the instance is used. Partial upfront, here a smaller upfront payment is made and then a discount is applied to all remaining hourse during the term. And finally no upfront, no upfront or partial payments are made and the smallest discount of the three models is applied to all remaining hours in the term. Reserved instances are used for long-term predictable workloads allowing you to make full use of the cost savings to be had when using compute resources offered by EC2. </p>
<p>Scheduled instances, these are similar to reserved instances and the fact that you pay for the reservations of an instance on a recurring schedule, either daily, weekly or monthly. For example you might have a weekly task that is scheduled that performs some kind of bulk processing for a number of hours at the same time every week. With scheduled instances you could set up a scheduled instance to run during that set timeframe once a week. And this prevents you for having to use the on-demand instances which would incur a higher price. You should note that when using scheduled instances but even if you didn’t use the instance you would still be charged. This allows you to provision instances for scheduled workloads that are not continuously running. Which is where a reserved instance would be the preferred choice. </p>
<p>Spot instances allows you to bid for unused EC2 compute resources, however your resource is not guaranteed for a fixed period of time. To you to spot instance you must bid higher than the current spot price which is set by AWS. And this spot price fluctuates depending on supply and demand of the unused resource. If your bid price for an instance type is higher than the spot price, then you’ll purchase that instance. But as soon as your bid price becomes lower than the fluctuating spot price, you will be issued a two-minute warning before the instance automatically terminates and is removed from your AWS environment. The bonus for spot instances is that you can bid for large EC2 instances at a very low cost point saving a huge amount on cost. Due to the nature of how the instances can be suddenly removed from your environment, spot instances are only useful for processing data and applications that can be suddenly interrupted. Such as batch jobs and background processing of data. </p>
<p>Capacity reservations allows you to reserve capacity for your EC2 instances based on different attributes. Such as instance type, platform and tenancy et cetera. Within a particular availability zone for any period of time. This ensures that you always have the available number of instances you require within a specific availability zone immediately. This capacity reservation could also be used in conjunction with your reserved instances discount providing you additional savings. </p>
<p>Let me now talk to you about EC2 tenancy and this relates to what underlying host your EC2 instance will reside on. So essentially the physical server within an AWS data center. Again there are different options available to you with pros and cons to each. Shared tenancy, this option will launch your EC2 instance on any available host with the specified resources required for your selected instance type. Regardless of which other customers and users also have EC2 instances running on the same host, hence the share tenancy name. AWS implement advanced security mechanisms to prevent one EC2 instance from accessing another on the same host. How the security is applied and operated is out of scope of this course and it is maintained by AWS themselves. Dedicated tenancy, this includes both dedicated instances and dedicated hosts. Dedicated instances are hosted on hardware that no other customer can access. It can only be accessed by your own AWS account. You may be required to launch your instances as a dedicated instance due to internal security policies or external compliance controls. Dedicated instances do incur additional charges due to the fact you are preventing other customers from running EC2 instances on the same hardware and so there will likely be unused capacity remaining. However the hardware might be shared by other resources you have running in your own account. Dedicated host, a dedicated host is effectively the same as dedicated instances. However they offer additional visibility and control, how you can place your instances on the physical host. They also allow you to use your existing licenses, such as PA-VM license or Windows Server licenses et cetera. Using dedicated hosts give you the ability to use the same host for a number of instances that you want to launch and align with any compliance and regulatory requirements. If you don’t need to address any compliance or security issues that require dedicated tenancy, then I recommend using shared tenancy to reduce your overall costs. </p>
<p>User data, during the configuration of your EC2 instance there is a section called user data. Which allows you to enter commands that will run during the first boot cycle of the instance. This is a great way to automatically perform functions upon boot, such as to pull down any additional software you want installing from any software repositories you may have. You could also download and get the latest OS updates during boot. For example you could enter yum update dash y, for a Linux instance which will then update its own software automatically at the time of boot. Storage options, as a part the configuration when setting up an EC2 instance, you are asked to select and configure your storage requirements. </p>
<p>Selecting storage for your EC2 instance will depend on the instance selected, what you intend to use the instance for and how critical the data is. Storage for EC2 can be classified between two distinct categories, persistent storage and ephemeral storage. Ephemeral meaning temporary. Persistent storage is available by attaching elastic block storage EBS volumes. And a ephemeral storage is created by some EC2 instances themselves using a local storage on the underlying host known as instance back storage. Let’s look at each of these storage options in greater depth. EBS volumes are separate devices from the EC2 instance itself. And so it’s not physically attached like ephemeral storage is. EBS volumes are considered network attached storage devices which are then logically attached to the EC2 instance via the AWS network. This principle is not dissimilar to attaching an external hard disk to your home laptop or PC. With the external hard disk represent your EBS volume and your PC represents your EC2 instance. The data on EBS volumes are automatically replicated to other EBS volumes within the same availability zone for resiliency which is managed by AWS. You can disconnect an EBS volume from your EC2 instance and the data will remain intact. Allowing you to reattach it to another EC2 instance if required. You can also implement encryption on these volumes if needed and take backup snapshots of all the data on the volume to S3. EBS volumes can be created in different sizes again with different performance capabilities depending on your requirements. </p>
<p>Ephemeral storage or instance backed storage is the storage that is physically attached to the underlying host on which the EC2 instance resides on. Looking back at our previous example, this would be similar to your own laptop or PC’s hard disk. There is a difference here though, with AWS EC2 instances as soon as the instance is stopped or terminated all saved data on a ephemeral storage is lost. If you reboot your instance then the data will remain but not if you stop it. Therefore if you have data that you need to retain it is not recommended that you use instance backed storage for this data. Instead use EBS volumes for persistent data storage. Unlike EBS volumes you are unable to detach ephemeral instance store volumes from the instance. </p>
<p>Security, security is fundamental with any AWS deployment. As so I just want to highlight a couple of points relating specifically to EC2 security. Firstly and during creation of your EC2 instance you will be asked to select a security group for your instance. A security group is essentially an instance level firewall allowing you to restrict both ingress and egress traffic by specifying what traffic allowed to communicate with it. You can restrict this communication by source ports and protocols for both inbound and outbound communication. Your instances are then associated with this security group. More information on security groups can be found in my blog post, found here covering instance level security. At the very end of your EC2 instance creation, you will need to select an existing key pair or create and download a new one. But what is a key pair and what is it used for? A key pair, as the name implies, is made up of two components, a public key and a private key. The function of key pairs is to encrypt the login information for Linux and Windows EC2 instances. And then decrypt the same information allowing you to authenticate onto the instance. The public key encrypts data such as the username and password. For Windows instances, the private key is used to decrypt this data allowing you to gain access to the login credentials including the password. For Linux instances the private key is used to remotely connect onto the instance via SSH. The public key is held and kept by AWS, and the private key is your responsibility to keep and ensure that it is not lost or compromised. So going back to when you create your EC2 instance and a new key pair. You’re given the opportunity to download the key pair, once you have done this you must keep that file safe until you’re ready to log on to the associated EC2 instance. It’s worth noting that you can use the same key pair on multiple instances to save you managing multiple private keys. Do bear in mind however should the private key become compromised access could be gained to all the instances where that key pair was used. Once you have authenticated to the EC2 instance the first time, you can set up additional less privileged access controls such as local windows accounts allowing other users to connect and authenticate to or even utilize Microsoft Active Directory. One final point regarding security on your EC2 instance it is your responsibility to maintain and install the latest OS and security patches released by the OS vendor as dictated within the AWS shared responsibility model. More information on this can be found in this blog post. </p>
<p>We have now covered the main elements of the EC2 service that should hopefully allow you to get started by creating your first EC2 instance and selecting the most appropriate configuration for your needs. But to reiterate what we have covered and make it all fit together, I will demonstrate how to create a new EC2 instance from within the console, quickly highlighting the elements we have discussed as I go through. </p>
<p>Okay so I’m logged into my AWS management console, and to start with we need to go to EC2 which is under the compute category. Now this take us to the EC2 console and from here we can simply select launch instance. Now this is the first stage of the configuration where we have to select our Amazon Machine Image, AMI. And here are a number of AWS AMIs that they supply, covering Windows, Red Hat, Linux et cetera. On the left hand side there’s just a few other options if you’ve created any AMIs yourself that would be stored here. I mentioned the AWS marketplace earlier and here you can see lots of different AMIs from other suppliers such as Trend Micro, Juniper Networks, Barracuda et cetera. And also the community AMI as well. So let’s get started with the Quick Start and look at some of the AWS supplied AMIs and I’m just going to launch this Amazon Linux box. So I’ll select that as my AMI, now we get to choose instance type. And we can filter up here with different types of instance types, general purpose, computer optimized et cetera. Just going to leave it as all, and then down here we can see the different families and the types, the vCPUs, memory et cetera that each of these has. So I’m going to leave it on the T2 micro general purpose instance. </p>
<p>So I’m going to select next configure instance details. Now I have a number of options here, the number of instances that we want to launch. I’m just going to keep it as one. If we want to launch this as a spot instance then we can select this box here to do so. But I’m just going to create it as an on-demand instance. We can select VPC that we want to run it in and we’ll have different VPCs there. You can then select the subnet if you’d like. And if you’d like to auto-assign a public IP address. We can assign a role to an EC2 instance if we need to and we can also control the shutdown behavior. So when we shut down all EC2 instance, do you want to terminate that instance or just stop. I’m going to leave that as a default or stop. There’s a couple of other controls you can put in here, enable termination protection. And what that will do, that will prevent you from terminating your instance until you uncheck this box. And you can also enable detail cloud watch monitoring if you need to. With regards to tenancy we discussed this earlier rather we shared or one of the dedicated instance or on a dedicated host, I’m going to leave that as shared. I’ll leave all the other options as default and then I’ll click on next to go to storage. This shows the current storage that comes with the AMI. We consider there’s 8 gig in size and it’s a general-purpose SSD drive. It’s a tick box here to delete the volume on termination and we can see that it’s not encrypted. If we wanted to add a new volume, we can add an EBS volume here. Again we can specify the size, eight gig or if you want to add it to 30 for example. And then we can also again delete on termination and we have the option to encrypt the EBS volume if we wanted to it’s by selecting the default AWS EBS encryption key. So now we have two drives, one of them is the root volume and an additional drive which is an EBS volume. </p>
<p>Click on next add tags, here we can add a key value pair tag to this instance. So for example a key of name and a value of my instance. And we can add additional tags as well as you can see here we can add up to 50 tags if we wanted to. So we can add a project that it belongs to or the cost et cetera, any tags that make it more usable to you. Once you’ve finished adding your tags click on configure security group. And this is where we control what can and can’t access your instance. You can create a new secure group or select an existing security group that you might already have. If we create a new security group and call it My Security Group, then you can add a description. And here are the rules for the security group. So at the moment we have SSH using TCP across port 22 and can you have the source as a custom IP address range or a single IP address. So you might just want a specific subnet in your VPC to talk to this instance or you can have anywhere, or you can have just your own IP address. Let’s put in a custom IP address range of 10.0.1.0&#x2F;24. And again you can add a description in there if you want to. You can add a new rule, for example HTTP traffic. And again you can add your source, cite anywhere then once you’re happy with your security groups click on review and launch. And this just provides a summary of all the configuration options that you’ve made up to this point. If you need to edit any other details you can just click on the right hand side here, edit the instance type the security groups et cetera. Once you’re happy with all of your information click on launch. And this is where you can select or create a new key pair to connect your instance. </p>
<p>So let’s go ahead and create a new key pair, call it My Instance, and now I need to download this key pair. Which will download the private part, once that key pair is downloaded I can then click on launch instance. Now if we go back to our dashboard by clicking on view instances. We can see here that it’s trying to launch at the minute, the status is pending and it shouldn’t take too long for that to become active. And here we go we can now see it’s up and running. </p>
<p>Before I finish this demonstration I just want to point out one last point relating to status checks that we can see from within our EC2 dashboard. These status checks are used to check the health and status of your EC2 instance and understanding what kind of faults could trigger these checks to fail. Kinda help you troubleshoot issues with your EC2 resources. There are two types of status checks. System status checks and instance status checks. If the system status check fails then it is likely to be an issue with the underlying host rather than a configuration issue with your EC2 instance. Common issues that trigger system status checks to fail are loss of power, loss of network connectivity and hardware and software issues on the underlying host. Basically a system status check failure is out of our control as the fault lies with components that AWS are responsible for. The best way to resolve this will be to stop the instance and restart. This is likely to cause the instance to launch on another physical host resolving the problem. Do not reboot the instance as this will cause the instance to continue running on the same physical server. Instance status checks, these differ from system status checks as if this fails then it would likely require your input to help them resolve the issue. This check looks at the EC2 instance itself, rather than focusing on the underlying hosts. Common issues that trigger these checks to fail are incorrect network configuration, corrupted file systems, exhausted memory or incompatible kernel. These faults will require you to troubleshoot and resolve the issue, for example changing the network configuration. If you’d like some hands-on experience with EC2, then we do offer two labs in which you can practice creating your own EC2 instances for both Linux and Windows. </p>
<p>That now brings me to the end of this lecture on EC2, like I mentioned previously this is going to be the longest and most in-depth lecture, simply due to how much of a key compute service it is in a wide range of use cases.</p>
<h1 id="ECS-Elastic-Container-Service"><a href="#ECS-Elastic-Container-Service" class="headerlink" title="ECS - Elastic Container Service"></a>ECS - Elastic Container Service</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/">Introduction to Docker</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/basics-of-using-containers-in-production/">Basics of using Containers in Production</a></p>
<p><strong>Transcript</strong></p>
<p>Hello, and welcome to this short lecture which will provide a high-level overview of the Amazon EC2 Container Service, commonly known as Amazon ECS. This service allows you to run Docker-enabled applications packaged as containers across a cluster of EC2 instances without requiring you to manage a complex and administratively heavy cluster management system. The burden of managing your own cluster management system is abstracted with the Amazon ECS service by passing that responsibility over to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a>, specifically though the use of AWS Fargate. </p>
<p>If you’re new to some of these terms such as Docker, containers, and AWS Fargate then let me quickly, in a single sentence, define what they are to help you understand this service a little easier. AWS Fargate is an engine used to enable ECS to run containers without having to manage and provision instances and clusters for containers. Docker is piece of software that allows you to automate the installation and distribution of applications inside Linux Containers. So what are containers? A Container holds everything that an application requires to enable it to run from within it’s isolated container package. This may include system libraries, code, system tools, run time, etcetera. But it does not include an operating system like a virtual machine does, and so reduces overhead of the actual container itself. </p>
<p>Containers are decoupled from the underlying operating system, making Container applications very portable, lightweight, flexible, and scalable across a cloud environment. This ensures that the application will always run as expected regardless of it’s deployment location. With this in mind, if you are already using Docker, or have existing containerized applications packaged locally, then these will work seamlessly on Amazon ECS. For more information on Docker and Containers, please see our existing content found here. Let’s now take a deeper look at the EC2 Container Service and some of the additional functions that it provides. </p>
<p>As I mentioned before, EC2 Container Service removes the need for you to manage your own cluster management system thanks to its interactions with AWS Fargate. You don’t even have to specify which instance type to use. This can be very time consuming and requires a lot of overhead to continue to monitor and maintain and scale. With Amazon ECS there is no need to install any management software for your cluster, neither is there a need to install any monitoring software either. All of this, and more, is taken care of by the service, allowing you to focus on building great applications and deploying them across your scalable cluster. </p>
<p>When launching your ECS cluster you have the option of two different deployment models: a Fargate launch and an EC2 launch. The Fargate launch requires far less configuration and simply requires you to specify the CPU and memory required, define the networking and IAM policies in addition to you having to package your applications into containers. However, with an EC2 launch you have a far greater scope of customization and configurable parameters. For example, you are responsible for patching and scaling your instances, and you can specify which instance types you used, and how many containers should be in a cluster. </p>
<p>There are use cases for both modes. You may need more granularity and control with some of your clusters due to security and compliance controls. Monitoring is taken care of through the use of AWS CloudWatch, which will monitor metrics against your containers and your cluster. Those of you who have used CloudWatch before will be aware you can easily create alarms based off of these metrics providing you notification of when specific events occur such as your cluster size scaling up or down. An Amazon ECS cluster is comprised of a collection of EC2 instances. As such, some of the functionality and features that we’ve already discussed in this course can be used with these instances. For example Security Groups to implement instance level securely at a port and protocol level, along with Elastic Load Balancing and Auto Scaling. Although these EC2 instances form a cluster, they still operate in much the same way as a single EC2 instance. So again, for example, should you need to connect to one of your instances itself, you could still use the same familiar methods such as initiating an SSH connection. </p>
<p>The clusters themselves act as a resource pool, aggregating resources such as CPU and memory. The cluster is dynamically scalable, meaning you can start your cluster as a single small instance, but it can dynamically scale to thousands of larger instances. Multiple instance types can be used within the cluster if required. Although the cluster is dynamically scalable, it’s important to point out that it can only scale within a single region. Amazon ECS is region-specific, so it can span multiple availability zones, but it cannot span multiple regions. With ECS you can schedule your containers to be deployed across your cluster based on different requirements, such as resources requirements or specific availability requirements, through the use of multiple availability zones. The instances within the Amazon ECS cluster also have a Docker daemon and an ECS agent installed. These agents communicate with each other allowing Amazon ECS commands to be translated into Docker commands.</p>
<h1 id="ECR-Elastic-Container-Registry"><a href="#ECR-Elastic-Container-Registry" class="headerlink" title="ECR - Elastic Container Registry"></a>ECR - Elastic Container Registry</h1><h3 id="Resources-referenced-within-this-lecture"><a href="#Resources-referenced-within-this-lecture" class="headerlink" title="Resources referenced within this lecture:"></a><strong>Resources referenced within this lecture:</strong></h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Overview of AWS Identity &amp; Access Managment (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Docker Push</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">Docker Pull</a></p>
<h3 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a><strong>Transcript</strong></h3><p>Hello and welcome to this lecture covering the Elastic Container Registry service, known as ECR. This service links closely with the previous service discussed, the EC2 Container Service, as it provides a secure location to store and manage your docker images that can be distributed and deployed across your applications. </p>
<p>This is a fully managed service, and as a result, you do not need to provision any infrastructure to allow you to create this registry of docker images. This is all provisioned and managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a>. This service is primarily used by developers, allowing them to push, pull, and manage their library of docker images in a central and secure location. </p>
<p>To understand the service better, let’s look at some components used. These being, registry, authorization token, repository, repository policy, and image. Let’s take a look at the registry first. The ECR registry is the object that allows you to host and store your docker images in, as well as create image repositories. Within your AWS account, you will be provided with a default registry. When your registry is created, then by default, the URL for the registry is as follows:</p>
<p><a target="_blank" rel="noopener" href="https://aws_account_id.dkr.ecr.region.amazonaws.com/">https://aws_account_id.dkr.ecr.region.amazonaws.com</a></p>
<p>where you’ll need to replace the red text with your own information that is applicable to your account or medium. Your account will have both read and write access by default to any images you create within the registry and any repositories. Access to your registry and images can be controlled via IAM policies in addition to repository policies as well, to enforce tighter and stricter security controls. As the docker command line interface doesn’t support the different AWS authentication methods that are used, then before your docker client can access your registry, It needs to be authenticated as an AWS user, which will then allow your client to both push and pull images. And this is done by using an authorization token. To begin the authorization process to allow your docker client to communicate with the default registry, you can run the get login command using the AWS CLI, as shown:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws ecr get-login --region region --no-include-email</span><br></pre></td></tr></table></figure>

<p>where the red text should be replaced with your own region. This will then produce an output response, which will be a docker login command.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login -u AWS -p password https://aws_account_id.dkr.ecr.region.amazonaws.com</span><br></pre></td></tr></table></figure>

<p>You must then copy this command and paste it into your docker terminal which will then authenticate your client and associate a docker CLI to your default registry. This process produces an authorization token that can be used within the registry for 12 hours, at which point, you will need to re-authenticate by following the same process. The repository are objects within your registry that allow you to group together and secure different docker images. You can create multiple repositories with the registry, allowing you to organize and manage your docker images into different categories. </p>
<p>Using policies from both IAM and repository policies, you can assign permissions to each repository allowing specific users to perform certain actions, such as performing a push or pull IP line. As I just mentioned, you can control access to your repository and images using both IAM policies and repository policies. There are a number of different IAM managed policies to help you control access to ECR, these being the three shown on the screen.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AmazonEC2ContainerRegistryFullAccess</span><br><span class="line">AmazonEC2ContainerRegistryPowerUser</span><br><span class="line">AmazonEC2ContainerRegistryReadOnly</span><br></pre></td></tr></table></figure>

<p>For more information on IAM and policies, please refer to our system course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">here</a>, which covers IAM and policy creation and management. Repository policies are resource-based policies, which means you need to ensure you add a principle to the policy to determine who has access and what permissions they have. It’s important to be aware of that for an AWS user to gain access to the registry, they will require access to the ecr get authorization token API call. Once they have this access, repository policies can control what actions those users can perform on each of the repositories. These resource-based policies are created within ECR itself and within each other repositories that you have. Once you have configured your registry, repositories, and security controls, and authenticated your docker client with ECR, you can then begin storing your docker images in the required repositories, ready to then pull down again as and when required. </p>
<p>To push an image into ECR, you can use the docker push command, and to retrieve and image you can use the docker pull command. For more information on how to perform both a push and a pull of images, please see the following links.</p>
<p><strong>Docker Push</strong>: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html</a></p>
<p><strong>Docker Pull</strong>: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html</a></p>
<p>That now brings me to the end of this lecture covering the Elastic Container Registry service. Coming up in the next lecture, I shall be looking at the Amazon <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/eks-elastic-container-service-kubernetes/">Elastic Container Service for Kubernetes</a>, known as EKS.</p>
<h1 id="EKS-Elastic-Container-Service-for-Kubernetes"><a href="#EKS-Elastic-Container-Service-for-Kubernetes" class="headerlink" title="EKS - Elastic Container Service for Kubernetes"></a>EKS - Elastic Container Service for Kubernetes</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-kubernetes/">Introduction to Kubernetes</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Install Kubectl</a></p>
<p>IAM Authenticator:</p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/aws-iam-authenticator">Linux</a></p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator">MacOS</a></p>
<p>- <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/windows/amd64/aws-iam-authenticator.exe">Windows</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">Configuration map to joing the Worker Node to the EKS Cluster</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">Introduction to EKS</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture covering the Elastic Container Service for Kubernetes, more commonly known as EKS.</p>
<p>Firstly, for those unfamiliar with Kubernetes let me briefly explain what it is at a high level.  Kubernetes is an open-source container orchestration tool designed to automate, deploy, scale, and operate containerized applications. It is designed to grow from tens, thousands, or even millions of containers. Kubernetes is also container-runtime agnostic, which means you can actually use Kubernetes to run rocket and docker containers.</p>
<p>So back to EKS, with EKS, AWS provides a managed service allowing you to run Kubernetes across your AWS infrastructure without having to take care of provisioning and running the Kubernetes management infrastructure in what’s referred to as the control plane. You, the AWS account owner, only need to provision and maintain the worker nodes.</p>
<p>What is a control plane and what are worker nodes?</p>
<p>Kubernetes Control Plane:</p>
<p>There are a number of different components that make up the control plane and these include a number of different APIs, the kubelet processes and the Kubernetes Master, and these dictate how kubernetes and your clusters communicate with each other.  The control plane itself is run across master nodes.</p>
<p>The control plane schedules containers onto nodes. The term scheduling does not refer to time in this context. Scheduling, in this case, refers to the decision process of placing containers onto nodes in accordance with their declared, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> requirements.  The Control Plane also tracks the state of all kubernetes objects by continually monitoring the objects. So in EKS, AWS is responsible for provisioning, scaling and managing the control plane and they do this by utilising multiple availability zones for additional resilience.</p>
<p>Worker nodes:</p>
<p>Kubernetes clusters are composed of nodes and the term cluster refers to the aggregate of all of the nodes.  A node is a worker machine in Kubernetes and runs as an on-demand EC2 instance and includes software to run containers managed by the Kubernetes control plane.  For each node created, a specific AMI is used which also ensures docker and kubelet in addition to the AWS IAM authenticator is installed for security controls. These nodes are what us as the customer are responsible for managing within EKS.  Once the worker nodes are provisioned they can then connect to EKS using an endpoint.</p>
<p>For more information on Kubernetes, please see our existing course ‘Introduction to Kubernetes’ here</p>
<p>Let me provide a brief overview of what’s required to start using the EKS service.</p>
<ol>
<li><p>Create an EKS Service Role: Before you begin working with EKS you need to configure and create an IAM service-role that allows EKS to provision and configure specific resources.  This role only needs to be created once and can be used for all other EKS clusters created going forward. The role needs to have the following permissions policies attached to the role: AmazonEKSServicePolicy and AmazonEKSClusterPolicy</p>
</li>
<li><p>Create an EKS Cluster VPC: Using AWS CloudFormation you need to create a and run a CloudFormation stack based on the following template: <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-vpc-sample.yaml</a> which will configure a new VPC for you to use with EKS</p>
</li>
<li><p>Install kubectl and the AWS-IAM-Authenticator: Kubectl is a command line utility for Kubernetes and can be installed following the details supplied here The IAM-Authenticator is required to authenticate with the EKS cluster.  Depending on your client OS (Linux, MacOS or Windows) it can be downloaded from here:</p>
</li>
<li><p>Create your EKS Cluster: Using the EKS console you can now create your EKS cluster using the details and information from the VPC created in step 1 and 2</p>
</li>
<li><p>Configure kubectl for EKS: Using the update-kubeconfig command via the AWS CLI you need to create a kubeconfig file for your EKS cluster</p>
</li>
<li><p>Provision and configure Worker Nodes: Once your EKS cluster shows an ‘Active’ status you can launch your worker nodes using CloudFormation based on the following template: <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml</a></p>
</li>
<li><p>Configure the Worker Node to join the EKS Cluster: Using a configuration map downloaded here:</p>
</li>
</ol>
<p>curl -O <a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml</a></p>
<p>You must edit it and Replace the &lt;ARN of instance role (not instance profile)&gt; with the NodeInstanceRole value from step 6</p>
<p>Your EKS Cluster and worker nodes are now configured ready for your to deploy your applications with Kubernetes.</p>
<p>For more information on EKS, please see our existing course ‘Introduction to EKS’ which will cover these points and more in greater detail <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">https://cloudacademy.com/course/introduction-to-aws-eks/</a></p>
<h1 id="AWS-Elastic-Beanstalk"><a href="#AWS-Elastic-Beanstalk" class="headerlink" title="AWS Elastic Beanstalk"></a>AWS Elastic Beanstalk</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/deploy-php-application-using-elastic-beanstalk-26/">Lab: Deploy a PHP Application using AWS Elastic Beanstalk</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/run-controlled-deploy-aws-elastic-beanstalk-43/">Lab: Run a controlled deploy with AWS Elastic Beanstalk</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture on AWS Elastic Beanstalk. AWS Elastic Beanstalk is an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">AWS</a> managed service that allows you to upload the code of your web application, along with the environment configurations, which will then allow Elastic Beanstalk to automatically provision and deploy the appropriate and necessary resources required within AWS to make the web application operational. These resources can include other AWS services and features, such as EC2, Auto Scaling, application health-monitoring, and Elastic Load Balancing, in addition to capacity provisioning. This automation and simplification makes it an ideal service for engineers who may not have the familiarity or the necessary skills within AWS to deploy, provision, monitor, and scale the correct environment themselves to run the developed applications. Instead, this responsibility is passed on to AWS Elastic Beanstalk to deploy the correct infrastructure to run the uploaded code. This provides a simple, effective, and quick solution to deploying your web application. </p>
<p>Once the application is up and running, you can continue to support and maintain the environment as you would with a custom built environment. You can additionally perform some of the maintenance tasks from the Elastic Beanstalk dashboard itself. Elastic Beanstalk is able to operate with a variety of different platforms and programming languages, making it a very flexible service for your DevOps teams. Currently at the time of writing this course, Elastic Beanstalk is compatible with the following. One important point to note is that the service itself is free to use. There is no cost associated with Elastic Beanstalk, however, any resources that are created on your application’s behalf, such as EC2 instances, you will be charged for as per the standard pricing policies at the time of deployment. </p>
<p>So now we know at a high level what AWS Elastic Beanstalk is and does, let me run through some of its core components that creates the service. The application version. An application version is a very specific reference to a section of deployable code. The application version will point typically to S3, simple storage service to where the deployable code may reside. </p>
<p>The environment. An environment refers to an application version that has been deployed on AWS resources. These resources are configured and provisioned by AWS Elastic Beanstalk. At this stage, the application is deployed as a solution and becomes operational within your environment. The environment is comprised of all the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code. </p>
<p>Environment configurations. An environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave. The environment tier. This component reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. If the application manages and handles HTTP requests, then the app will be run in a web server environment. If the application does not process HTTP requests, and instead perhaps pulls data from an SQS queue, then it would run in a worker environment. I shall cover more on the differences between the web server and work environment shortly. </p>
<p>The configuration template. This is the template that provides the baseline for creating a new, unique environment configuration. Platform. The platform is a culmination of components in which you can build your application upon using Elastic Beanstalk. These comprise of the operating system of the instance, the programming language, the server type, web or application, and components of Elastic Beanstalk itself, and as a whole can be defined as a platform. Applications. Within Elastic Beanstalk, an application is a collection of different elements, such as environments, environment configurations, and application versions. In fact, you can have multiple application versions held within a single application. You can deploy your application across one of two different environment tiers, either the web server tier or the worker tier. </p>
<p>These tiers are configured differently depending on the use case of your application. The web server environment is typically used for standard web applications that operate and serve requests over HTTP port 80. This tier will typically use service and features such as Route 53, Elastic Load Balancing, Auto Scaling, EC2, and Security Groups. The worker environment is slightly different and are used by applications that will have a back-end processing task that will interact with AWS SQS, the Simple Queue Service. This tier typically uses the following AWS resources in this environment, an SQS Queue, an IAM Service Role, Auto Scaling, and EC2. </p>
<p>Now you are aware of some of the terminology and components, we can look at how AWS Elastic Beanstalk operates a very simple workflow process for your application deployment and ongoing management in what can be defined in four simple steps. Firstly, you create an application. Next, you must upload your application version of the application to Elastic Beanstalk, along with some additional configuration information regarding the application itself. This creates the environment configuration. The environment is then created by Elastic Beanstalk with the appropriate resources to run your code. Any management of your application can then take place, such as deploying new versions of your application. If the management of your applications have altered the environment configuration, then your environment will automatically be updated to reflect the new code should additional resources be required. For further information and to get some hands-on experience with AWS Elastic Beanstalk to deploy an application, take a look at our two labs which will guide you through the steps and processes we have discussed. </p>
<p>That now brings us to the end of this lecture. Coming up next, I will introduce you to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/aws-lambda/">AWS Lambda</a>.</p>
<h1 id="AWS-Lambda"><a href="#AWS-Lambda" class="headerlink" title="AWS Lambda"></a>AWS Lambda</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html#supported-event-source-s3">AWS Lambda Event Sources</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/">Understanding AWS Lambda to Run and Scale your Code</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/introduction-aws-lambda-22/">Lab: Introduction to AWS Lambda</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/aws-lambda-s3-events-55/">Lab: Process Amazon S3 events with AWS Lambda</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/automating-ebs-snapshots-lambda-and-cloudwatch-events-45/">Lab: Automating EBS snapshots with AWS Lambda</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this lecture where we shall take an introductory look at AWS Lambda. AWS Lambda is a serverless <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> service which has been designed to allow you to run your application code without having to manage and provision your own EC2 instances. This saves you having to maintain and administer an additional layer of technical responsibility within your solution. Instead, that responsibility is passed over to AWS to manage for you. </p>
<p>Essentially, serverless means that you do not need to worry about provisioning and managing your own compute resource to run your own code, instead this is managed and provisioned by AWS. AWS will start, scout, maintain, and stop the compute resources as required, which can be just as short as just a few milliseconds. Although it’s named serverless, it does of course require service, or at least compute power, to carry out your code requests, but because the AWS user does not need to be concerned with what’s managing this compute power, or where it’s provisioned from, it’s considered serverless from the user perspective. </p>
<p>If you don’t have to spend time operating, managing, patching, and securing an EC2 instance, then you have more time to focus on the code of your application and its business logic, while at the same time, optimizing costs. With AWS Lambda, you only ever have to pay for the compute power when Lambda is in use via Lambda functions. And I shall explain more on these later. </p>
<p>AWS Lambda charges compute power per 100 milliseconds of use only when your code is running, in addition to the number of times your code runs. With sub-second metering, AWS Lambda offers a truly cost optimized solution for your serverless environment. So how does it work? Well there are essentially four steps to its operation. </p>
<p>Firstly, AWS Lambda needs to be aware of your code that you need run so you can either upload this code to AWS Lambda, or write it within the code editor that Lambda provides. Currently, AWS Lambda supports Notebook.js, JavaScript, Python, Java, Java 8 compatible, C#, .NET Core, Go, and also Ruby. It’s worth mentioning that the code that you write or upload can also include other libraries. Once your code is within Lambda, you need to configure Lambda functions to execute your code upon specific triggers from supported event sources, such as S3. As an example, a Lambda function can be triggered when an S3 event occurs, such as an object being uploaded to an S3 bucket. Once the specific trigger is initiated during the normal operations of AWS, AWS Lambda will run your code, as per your Lambda function, using only the required compute power as defined. Later in this course I’ll cover more on when and how this compute power is specified. AWS records the compute time in milliseconds and the quantity of Lambda functions run to ascertain the cost of the service. </p>
<p>For an AWS Lambda application to operate, it requires a number of different elements. The following form the key constructs of a Lambda application. Lambda function. The Lambda function is compiled of your own code that you want Lambda to invoke as per defined triggers. Event source. Event sources are AWS services that can be used to trigger your Lambda functions, or put another way, they produce the events that your Lambda function essentially responds to by invoking it. For a comprehensive list of these event sources, please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html#supported-event-source-s3">link</a> on the screen. Trigger. The Trigger is essentially an operation from an event source that causes the function to invoke. So essentially triggering that function. For example, an Amazon S3 put request could be used as a trigger. Downstream Resources. These are the resources that are required during the execution of your Lambda function. For example, your function might call upon accessing a specific SNS topic, or a particular SQS queue. So they are not used as the source of the trigger, but instead they are the resources to be used to execute the code within the function upon invocation.</p>
<p>Log streams. In an effort to help you identify issues and troubleshoot issues with your Lambda function, you can add logging statements to help you identify if your code is operating as expected into a log stream. These log streams will essentially be a sequence of events that all come from the same function and recorded in CloudWatch. In addition to log streams, Lambda also sends common metrics of your functions to CloudWatch for monitoring and alerting. At a high level, the configuration steps for creating a Lambda function via the AWS Management Console could consist of selecting a blueprint, and AWS Lambda provides a large number of common blueprint templates which are preconfigured Lambda functions. To save time on your own code, you can select one of these blueprints and then customize it as necessary. An example of one of these blueprints could be the S3 get object, which is an Amazon S3 trigger that retrieves metadata for the object that is being updated. You then need to configure your triggers, and as I just explained, the trigger is an operation from an event source that causes the function to invoke and in my previous statement, I suggested an S3 put request. And then you need to finish configuring your function. And this section requires you to either upload your code or edit it in-line and it also requires you to define the required resources, the maximum execution timeout, the IAM Role, and Handler Name. </p>
<p>A key benefit of using AWS Lambda is that it is a highly scalable serverless service, coupled with fantastic cost optimization compared to EC2 as you are only charged for Compute power while the code is running and for the number of functions called. For more information on AWS Lambda and how to configure it in detail, can be found in our following course. For your own hands on experience with AWS Lambda, please take a look at our labs which will guide you through how to create your first Lambda function. </p>
<p>That now brings me to the end of this lecture covering AWS Lambda. Coming up next, I shall be discussing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/aws-batch-cf/">AWS Batch</a>.</p>
<h1 id="AWS-Batch"><a href="#AWS-Batch" class="headerlink" title="AWS Batch"></a>AWS Batch</h1><p>Hello, and welcome to this lecture where I’ll provide a high level overview of AWS Batch. As the name suggests, this service is used to manage and run Batch computing workloads within AWS. Before we go any further, I just want to quickly clarify what Batch computing is. </p>
<p>Batch computing is primarily used in specialist use cases which require a vast amount of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> power across a cluster of compute resources to complete batch processing executing a series of jobs or tasks. Outside of a cloud environment, it can be very difficult to maintain and manage a batch computing system. It requires specific software and requires the ability to consume the resources required, which can be very costly. However, with AWS Batch, many of these constraints, administration activities and maintenance tasks are removed. You can seamlessly create a cluster of compute resources which is highly scalable, taking advantage of the elasticity if AWS, coping with any level of batch processing while optimizing the distribution of the workloads. All provisioning, monitoring, maintenance and management of the clusters themselves is taken care of by AWS, meaning there is no software to be installed by yourself. </p>
<p>There are effectively five components that make up AWS Batch service which will help you to start using the service, these being: Jobs. A job is classed as a unit of work that is to be run by AWS Batch. For example, this can be a Linux executable file, an application within an ECS cluster or a shell script. The jobs themselves run on EC2 instances as a containerized application. Each job can at any one time be in a number of different states, for example, submitted, pending, running, failed, among others. Job definitions. These define specific parameters for the jobs themselves. They dictate how the job will run and with what configuration. Some examples of these may be how many vCPUs to use for the container, which data volume should be used, which IAM role should be used, allowing access for AWS Batch to communicate with other AWS services, and mount points.</p>
<p>Job queues. Jobs that are scheduled are placed into a job queue until they run. It’s also possible to have multiple queues with different priorities if needed. One queue could be used for on-demand EC2 instances, and another queue could be used for the spot instances. Both on-demand and spot instances are supported by AWS Batch, allowing you to optimize cost, and AWS Batch can even bid on your behalf for those spot instances. </p>
<p>Job scheduling. The Job Scheduler takes control of when a job should be run and from which Compute Environment. Typically it will operate on a first-in-first-out basis, and it will look at the different job queues that you have configured, ensuring that higher priority queues are run first, assuming all dependencies of that job have been met. </p>
<p>Compute Environments. These are the environments containing the compute resources to carry out the job. The environment can be defined as managed or unmanaged. A managed environment means that the service itself will handle provisioning, scaling and termination of your Compute instances based on the configuration parameters that you would enter regarding the instance type, purchase method, such as on-demand or spot. This environment is then created as an Amazon ECS Cluster. Unmanaged environments are provisioned, managed and maintained by you, which gives greater customization. However, it does require greater administration and maintenance and also requires you to create the necessary Amazon ECS Cluster that the managed environment would have done on your behalf. </p>
<p>If you have a requirement to run multiple jobs in parallel using Batch computing, for example, to analyze financial risk models, perform media transcoding or engineering simulations, then AWS Batch would be a perfect solution.</p>
<h1 id="Amazon-Lightsail"><a href="#Amazon-Lightsail" class="headerlink" title="Amazon Lightsail"></a>Amazon Lightsail</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://lightsail.aws.amazon.com/ls/webapp/home/resources">Amazon Lightsail dashboard</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this short lecture covering Amazon Lightsail. This is another <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/introduction-to-aws-compute-fundamentals/">compute</a> service that in some respect closely resembles EC2 out of all the other compute resources we have covered so far. Amazon Lightsail is essentially a virtual private server, A VPS, backed by AWS infrastructure, much like an EC2 instance but without as many configurable steps throughout its creation. </p>
<p>It has been designed to simple, quick, and very easy to use at a low cost point for small-scale use cases by small business or for single users. With its simplicity and small-scale use, it’s commonly used to host simple websites, small applications, and blogs. You can run multiple Lightsail instances together, allowing them to communicate. And it’s even possible if required to connect it to other AWS resources and to your existing VPC, running within AWS via a peering connection. </p>
<p>To deploy a Lightsail instance, it’s easy to do all from a single page with just a few configuration options. Amazon Lightsail can be accessed either via the AWS console under the compute category, or you can go directly to the homepage of AWS Lightsail, which sits outside of the Management Console and can be found here. </p>
<p>If you want to launch a new instance, select create instance, where you can then create your instance all from just one page of options. Nice and simple. Firstly, you need to select your region and availability zone as required as to where you’d like to provision your Lightsail instance. Next, you can select your platform, Linux or Windows based, and then additional blueprint if required. If you didn’t need a blueprint, you can simply select to use the operating system only. Next, you have the option to add a launch script and a different key pair. The launch script can be a shell script that will run at the time of the launch, much like user data for an EC2 instance. By default, you are provided with a key pair to connect to your instance. However, you can select to choose an alternative one if required. Following this, you must then select your instance plan. This section defines the resources of your instance and how much you’re going to be paying on a monthly basis. The price per month option shows preset configurations based on memory, processing power, storage, and data transfer. However, you can tab through the corresponding tabs and customize the values of each to meet your needs. As you can see, it’s very clear, simple, and obvious as to what you will be paying and the resources you will get in return. The instances are charged as an on-demand price, so you’ll only pay for the resource when you’re using them. The dollar per month price is based on having the instance on continuously, which AWS calculates as 31.25 days multiplied by 24 hours. </p>
<p>The configuration options requires you to provide a unique name for your Lightsail instance. In addition to this, you’re also prompted to add key-value tags to help organize your resources. Now all of your configuration is complete. Simply click on create instance. As you can see, it’s very easy and simple to create your Lightsail VPS compared to number of different screens and configuration options required when deploying an EC2 instance. Once your Amazon Lightsail service is up and running, you then have a number of management and monitoring options, which are clear and easy to use. Connect. This option allows you to connect to your newly created instance using SSH either via inline SSH software provided by Lightsail or with your own SSH software using the key pair provided. The instance is given a public IP to allow you to connect. Storage. This provides an overview of your current storage, showing the capacity and the disk path. For example, &#x2F;dev&#x2F;sda1. You also have the ability to attach additional disks to your instance. Metrics. This allows you to view graphical metrics of your instance, such as CPU utilization, network in, network out, StatusCheckFailed, StatusCheckFailed_Instance, and StatusCheckFailed_System. These graphs can be viewed over a number of different time periods, from one hour through to two weeks. Networking. The networking tab allows you to view your IP address information along with a very simple virtual file, allowing you to control which ports your instance can accept connections from. You can also gain additional information on load balancing your traffic between instances. Snapshots. This provides a simple way to backup your instance. Tags. Here you can configure additional or edit existing tags to help you filter and organize your resources. Key-value tags can also be used to help manage your billing and control access. History. This provides simple order information of your instance, such as the date and time the instance was created or when configuration changes occurred. Delete. When you have finished with your instance, this tab allows you to delete your instance along with any data that was stored in it. </p>
<p>As you can see, Amazon Lightsail provides a lightweight solution for small projects and use cases which can be deployed quickly and cost effectively in just a few clicks. That brings us to the end of this lecture. Coming up next, I’ll provide a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/compute-fundamentals-for-aws/summary-to-aws-compute-fundamentals/">summary</a> of the key points and topics that we’ve learned throughout this course.</p>
<h1 id="What-is-an-Elastic-Load-Balancer-ELB"><a href="#What-is-an-Elastic-Load-Balancer-ELB" class="headerlink" title="What is an Elastic Load Balancer (ELB)?"></a>What is an Elastic Load Balancer (ELB)?</h1><p>Hello and welcome to this lecture, which is going to focus on what the AWS Elastic Load Balancer service is and does. </p>
<p>Now the main function of an Elastic Load Balancer, commonly referred to as an ELB, is to help manage and control the flow of inbound requests destined to a group of targets by distributing these requests evenly across the targeted resource group. These targets could be a fleet of EC2 instances, Lambda functions, a range of IP addresses, or even Containers. The targets defined within the ELB could be situated across different Availability Zones for additional resiliency or all placed within a single Availability Zone. Let’s look at this from a typical scenario. </p>
<p>Let’s suppose you have just created a new application, which is currently residing on a single EC2 instance within your environment, and this is being accessed by a number of users. At this stage, your architecture can be logically summarized as shown. If you are familiar with architectural design and best practices, then you would realize that using a single instance approach isn’t ideal although it would certainly work and provide a service to your users. However, this infrastructure layout brings some challenges. For example, the single instance where your application is located can fail, perhaps from a hardware or software fault. And if that happens, your application will be down and unavailable to your users. Also, if you experience a sudden spike in traffic, your instance may not be able to handle the additional load based on its performance limitations. As a result, to strengthen your infrastructure and help remediate these challenges, the unpredictable traffic spikes and high availability, et cetera, you should introduce an Elastic Load Balancer, an additional instance that’s running your application into the design as shown. </p>
<p>As you can see, in this design, the AWS Elastic Load Balancer will act as the point for receiving incoming traffic from users and evenly distribute the traffic across a greater number of instances. By default, the ELB is highly available as this is a managed service provided by AWS. And so, they ensure its resilience so we don’t have to. Although it might seem that ELB is a single point of failure, the ELB is in fact comprised of multiple instances managed by AWS. Also in this scenario, we now have three instances running our application. Now let me revisit the challenges we discussed previously. If any of these three instances fail, the ELB will automatically detect the failure based on defined metrics and divert any traffic to the remaining two healthy instances. Also if you experience a surge in traffic, then the additional instances running your application would help with the additional load. One of the many advantages of using ELB is the fact that it is managed by AWS, and it is, by definition, elastic. This means that it will automatically scale to meet your incoming traffic as the incoming traffic scales both up and down. </p>
<p>If you are system administrator or DevOps engineer running your own load balancer by yourself, then you would need to worry about scaling your load balancer and enforcing high availability. With an AWS ELB, you can create your load balancer and enable dynamic scaling with just a few clicks. Depending on your traffic distribution requirements, there are three ELBs available within AWS to choose from. </p>
<p>Firstly, the Application Load Balancer. This provides a flexible feature set for your web applications running the HTTP or HTTPS protocols. The Application Load Balancer operates at the request level, and it also provides advanced routing, TLS termination, and visibility features targeted at application architectures, allowing you to route traffic to different ports on the same EC2 instance. </p>
<p>Next, there is a Network Load Balancer. This is used for ultra-high performance for your application while at the same time managing very low latencies. It operates at connection level, routing traffic to targets within your VPC, and it’s also capable of handling millions of requests per second. </p>
<p>Finally, the Classic Load Balancer. This is primarily used for applications that were built in the existing EC2 Classic environment and operates at both the connection and request level. We’ll now talk a little bit about the components of an AWS ELB and some of the principles behind them. </p>
<p>Listeners. For every load balancer, regardless of the type used, you must configure at least one listener. The listener defines how your inbound connections are routed to your target groups based on ports and protocols set as conditions. The configurations of the listener itself differs slightly depending on which ELB you have selected. I will dive into the configuration of these as I discuss each ELB in further detail in upcoming lectures. </p>
<p>Target groups. A target group is simply a group of resources that you want your ELB to route requests to, for example a fleet of EC2 instances. You can configure your ELB with a number of different target groups, each associated with a different listener configuration and associated rules. This enables you to route traffic to different resources based upon the type of request. Rules. </p>
<p>Rules are associated to each listener that you have configured within your ELB, and they help to define how an incoming request gets routed to which target group. As you can see, your ELB can contain one or more listener. And each listener can contain one or more rules, and each rule can contain more than one condition, and all conditions in the rule equal a single action. An example rule could look as follows where the if statement resembles the conditions and the then statement acts as the action if all the conditions are met. So, depending on which listener request was responded to by the ELB, a rule based upon a priority listing would be associated containing these conditions and actions. If the request came from within the 10.0.1.0&#x2F;24 network range, which is the first condition, and was trying to carry a HTTP PUT request, the second condition, then the request would be sent to the target group entitled Group1, which is the action. </p>
<p>Health checks. The ELB associates a health check that is performed against the resources defined within the target group. These health checks allow the ELB to contact each target using a specific protocol to receive a response. If no response is received within a set of thresholds, then the ELB will mark the target as unhealthy and stop sending traffic to that target. </p>
<p>Internal or Internet-facing ELBs. There are two different schemes that can be used for your load balancers, either internal or Internet-facing. Internet-facing, as the name implies, the nodes of the ELBs that are defined as Internet-facing are accessible via the Internet and so have a public DNS name that can be resolved with public IP address. This would be in addition to an internal IP address as well. This allows the ELB to serve incoming requests from the Internet before distributing and routing the traffic to your target groups, which in this instance could be a fleet of web servers receiving HTTP or HTTPS requests. When your Internet-facing ELB communicates with its target group, it will only use the internal IP address, meaning that your target group does not need public IP addresses. An internal ELB only has an internal IP address. This means that it can only serve requests that originate from within your VPC itself. For example, you might have an internal load balancer sitting between your web servers in the public subnet and your application servers in the private subnet. </p>
<p>ELB nodes. During the creation process of your ELBs, you’re required to define which Availability Zone you’d like your ELB to operate within. For each Available Zone selected, an ELB node will be placed within that Availability Zone. As a result, you need to ensure that you have an ELB node associated to any Availability Zones for which you want to route traffic to. Without the Availability Zone associated, the ELB will not be able to route traffic to any targets within that Availability Zone even if they are defined within the target group. This is because the nodes are used by the ELB to distribute traffic to your target groups. </p>
<p>Cross-Zone load balancing. Depending on which ELB option you select, you may have the option of enabling and implementing Cross-Zone load balancing within your environment. Let’s presume you have two Availability Zones activated for your ELB with each associated load balancer receiving equal amount of traffic. One Availability Zone has six targets, and the other has four as shown. When Cross-Zone load balancing is disabled, each ELB and its associated AZ would distribute its traffic with the targets within that Availability Zone only. As we can see from the image, this results in an uneven distribution of traffic for each target across the Availability Zones. With Cross-Zone load balancing enabled, regardless of how many targets are in an associated Availability Zone, the ELBs would distribute all incoming traffic evenly between all targets, ensuring each target across the Availability Zones have an even distribution. </p>
<p>That now brings me to the end of this lecture. In the lecture, I shall be discussing server certificates and how they are used with load balancers to help terminate encrypted requests.</p>
<h1 id="SSL-Server-Certificates"><a href="#SSL-Server-Certificates" class="headerlink" title="SSL Server Certificates"></a>SSL Server Certificates</h1><p>Hello and welcome to this short lecture which will provide a high-level overview of server certificates and how they are used within your elastic load balancers. </p>
<p>As I mentioned in the previous lecture the Application Load Balancer provides a flexible feature set for your web applications running the HTTP or HTTPS protocols. As such, the ALB listener options available when creating your ALB are either the HTTP or HTTPS protocol on port 80 and 443 respectively. Configuration of your HTTP port 80 listeners is a fairly simple process, and I’ll cover this in the next lecture. However, there will times when you would need to use the HTTPS encrypted protocol as a listener and this requires some additional configuration. </p>
<p>So let me run through some of the points when using HTTPS as a listener. HTTPS is an encrypted version of the HTTP protocol and this allows an encrypted communication channel to be set up between clients initiating the request and your Application Load Balancer. However, to allow your ALB to receive encrypted traffic over HTTPS it will need a server certificate and an associated security policy. </p>
<p>SSL or Secure Sockets Layer, to give it its full name, is a cryptographic protocol, much like TLS, Transport Layer Security. Both SSL and TLS are used interchangeably when discussing certificates for your Application Load Balancer. The server certificates used by the ALB is an X.509 certificate, which is a digital ID that has been provisioned by a Certificate Authority and this Certificate Authority could be the AWS Certificate Manager service also known as ACM. This certificate is simply used to terminate the encrypted connection received from the remote client, and as a part of this termination process the request is then decrypted and forwarded to the resources in the ELB target group. </p>
<p>When you select HTTPS as your listener, you will be asked to select a certificate using one of four different options available. Either choose a certificate from ACM, upload a certificate to ACM, choose a certificate from IAM, or upload a certificate to IAM. The first two options relate to ACM. An ACM is the AWS Certificate Manager and this service allows you to create and provision SSL&#x2F;TLS server certificates to be used within your AWS environment across different services. This integration with ACM simplifies the configuration process of implementing a new certificate for your elastic load balancer and as a result, it’s the preferred option. </p>
<p>The last two options allow you to use a third-party certificate by using IAM as your certificate manager and you would select this option when deploying your ELBs in regions that are not supported by ACM. For a list of supported regions, please see the following link. For detailed information on how to upload, retrieve, and list server certificates via IAM, please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html">AWS documentation</a>. Using ACM as your certificate manager allows you to both create certificates from within ACM itself and also import existing certificates created from outside of AWS adding additional flexibility for your current third party certificates. The configuration of ACM is out of scope for this course. However, you can find further information on this service using the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">link</a>. </p>
<p>Now it’s brought me to the end of this lecture. In the next few lectures I shall be looking at the configuration of each of the defined load balancers, application, network, and classic, to provide you with more information on their components starting with the Application Load Balancer, the ALB.</p>
<h1 id="Application-Load-Balancers"><a href="#Application-Load-Balancers" class="headerlink" title="Application Load Balancers"></a>Application Load Balancers</h1><p>Hello and welcome to this lecture covering the Application Load Balancer, the ALB. The first of the three load balancers that I shall be discussing. If you are familiar with the open systems interconnection model, the OSI model, then you won’t be surprised that the ALB operates at layer seven, the application layer. The application layer of the OSI model serves as the interface for users and application processes to access network services. Everything at this layer is application specific. The application layer of the model helps to provide network services to the applications. And examples of the application process or services it offers are http, ftp, smtp and nfs. For more information on the OSI model, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/">here</a>.</p>
<p>As you can see AWS suggests you use the application load balancer if you need to provide a flexible feature set including advanced routing and visibility features aimed purely for application architectures such as microservices and containers when used in HTTP or HTTPS. Before configuring your ALB, it’s good practice to set up your target groups. Now I explained in a previous lecture that a target group is simply a group of resources that you want your ALB to route requests to. You might want to configure different target groups depending on the nature of your requests. For example, let’s say you had an internet-facing ALB, you might want a target group allocated to handle and process HTTP port 80 requests and a different target group configured to process requests from the secure HTTPS protocol using port 443. In this scenario, you could configure two different target groups and then route traffic, depending on the request, to different targets through the use of listeners and rules. </p>
<p>I now want to demonstrate how to configure an ALB and in this demonstration, I will also show you how to set up target groups as well. Let’s take a look. </p>
<p>As you can see, I’m in the AWS management console and the first thing we want to do is create our target groups and I can do this by going into the EC2 service which is found here under compute. And then if I scroll down on the left-hand side, I’ll get to the load balancing section here. Then in here, I have load balancers and target groups, but first I want to set up our target groups. So, if you select target group, as you can see there’s no target groups currently configured. So if I click on the blue button, Create target group, I now have a page of information that I need to complete. </p>
<p>So, firstly the target group name and I’m going to call this Web Servers. And then I have my target type and here I can specify it by instance, IP or Lambda function. I’m going to leave it as instance, then we can select what protocol we want. As this is going to be a web service, I’ll leave it as HTTP on port 80 and here we can select our VPC that we want this target group to exist in. So, select my appropriate virtual private cloud there. At the bottom we just have some health check settings and this is the path and protocol that the load balance will use when performing its health checks. So, for the path I’ll just put in index.html as an example. If we take a look at the advanced health check settings, here we have a number of other options that we can select. </p>
<p>This value specifies the healthy threshold which means the load balancer will have to receive five responses from the instance before deeming the previously unhealthy target healthy again, and the unhealthy threshold means that the load balancer only has to receive two failures before marking the instance as unhealthy. The timeout is simply the number of seconds that the load balancer will wait for a response, and the interval is how many seconds between each health check. Once we’re happy with our configuration, we’ll click on Create. Looks like I left this space in the name and you can’t have any spaces. So let’s just delete that and click on Create. And there you go, our target group was successfully created, and now we can see it in our list of target groups. </p>
<p>However, we don’t have any targets associated with this group yet. That was just simply the configuration of the group. So, down here we have the Description, the Targets, the Health checks, Monitoring and Tags, but if we click on the Targets tab, then here we can start adding our targets associated with this target group. And if we click on Edit, we can see here at the bottom that there’s two instances that I have running, web server one, and web server two. Now here I can select which instance I want to add and associate to this target group. For this demonstration, I’m going to select both instances and then add these as registered targets to this group. And as you can see, these two instances have now been added under the Registered targets section. Click on Save. And we can see here, that we have two registered targets which are the ones I just added, web server one and web server two, now associated to this target group. </p>
<p>Let’s just quickly look at these other tags here as well, the Health checks, that’s the health check information that we configured during the creation of the target group; Monitoring, this shows a number of CloudWatch metrics associated with the target group such as number of healthy hosts and unhealthy hosts et cetera. And then we also have Tags if you wanted to create a key-value pair for your target group and you can do so here. So, as you can see, it’s very easy to create different target groups as you need to for your load balancing. </p>
<p>Let us now go ahead and create an Application Load Balancer. So, back on the left-hand side here, again under Load Balancing, we have Load Balancers. So if you select that. Now I don’t have any load balancers configured here. So if I click on Create Load Balancer, and I can create an Application Load Balancer and Network Load Balancer or the Classic. In this example, I’m going to create the Application Load Balancer. So, click on Create. Now here we have a number of different steps. Firstly, we need to give it a name. So this would be WebServerALB, and we’ll have this as internet-facing using ipv4. Now down here we have our listeners. So this is the port and protocol that we want the load balancer to listen on and as this is our web server, let’s leave it as HTTP on port 80. If you want to add additional listeners, then you can do so just by selecting Add Listener and selecting the different protocols et cetera. Now if we scroll down to the bottom here, we can select our Availability Zones that we want to enable for our load balancer. So for eu-west-1a, let me select this subnet and for eu-west-1b I, shall select this subnet. So there are the two subnets that I want to associate with the load balancer, and each of them are in a set per availability zone as you can see here. </p>
<p>Now I need to go and configure my security settings. And I have a message here to say that the load balancer security is not using a secure listener. Now, if we were to go back and change that to HTTPS, then we would be using a secure listener and we’d also have to set up server certificates as well, but for this demonstration, I just want to show you how to create the Application Load Balancer, but generally in a wide-scale production environment, if you’re creating a load balancer for your internet-facing resources, then you’d probably want to use https for that additional security. Click on next. </p>
<p>Now we’ll need to select the security group that is going to be associated to our load balancer. So we could create a new security group, call this our Application Load Balancer. So we’ll have HTTP from any IP address, and we click on that Next Configure Reading. And this is where we can specify our target group for the Application Load Balancer. So we can create a new target group here and go through the same process as we did earlier or click on the dropdown list and select an existing target group and here we can see that we have our WebServers target group that we created earlier with all the settings already pre-filled. Click on Next Register Targets, and here we can see that these are the two targets associated with the target group. Click on Next Review. And this is just a review of all the configuration options that we made during the creation of this. Once you’re happy with that, simply click on Create. And then we have it. </p>
<p>We have our new load balancer, our WebServerALB was successfully created. So, let’s take a look. This might take it a couple of minutes for it to be provisioned. While that’s being provisioned, if we take a look at the bottom here, we can see that we have some basic configuration that we’ve set up with the availability zones, the fact that it’s internet-facing, and we have the ARN et cetera as well. We have our listener configuration here that we can change if we need to. As we can see, at the minute we’re listening on port 80. Again, we have some monitoring metrics here being carried out by CloudWatch. We’ll see a number of different CloudWatch metrics. Actually, we can now see that that the state is now active. So that’s our Application Load Balancer set up and configured. </p>
<p>Now before I finish this quick demonstration, I just want to show you the rules that I mentioned earlier with regards to listeners. So, if we go down to our Listeners tab here, we can see that we can view and edit our rules for our listener. So if we click on that, at the moment we can see that we have our default action here listening on port 80. We can see that this rule cannot be moved or deleted. That’s basically saying that this listener is listening on port 80 and for any requests then forward it to the WebServers. But we can add additional rules in here. So let’s take a look. So if we click on this plus button, we can see that we now have this option here of Insert Rule. So, let me select that, and that allows me to add a new rule in. So, first we need to add a condition. So, for example, let’s have the condition of a Source IP, then we just put in a random IP address here. So, this is saying if the source IP is this IP address, then add the following action, and here we could choose to forward it to another target group. I mean, I’ve only got one target group configured at the minute called WebServers, but if I had other target groups here with different instances associated to those target groups, then I could select a different target group to forward any requests that are received from this IP address. So that allows you to customize how your load balancer directs traffic, depending on what rules you create with your listeners. So, when I was talking about conditions and rules in a previous lecture, then this is the section that I was referring to. So I just wanted to show you that quickly within this demonstration, where you can edit your rules and add customization with conditions and actions. </p>
<p>Okay, and that’s the end of this demonstration.</p>
<h1 id="Network-Load-Balancers"><a href="#Network-Load-Balancers" class="headerlink" title="Network Load Balancers"></a>Network Load Balancers</h1><p>Hello and welcome to this lecture focusing on the network load balancer and its configuration. </p>
<p>Between the ALB and the NLB, the principles are the same as to how the overall process works, so to load balance incoming traffic from a source to its configured target groups. However, whereas the ALB work to the application level analyzing the HTTP header to direct the traffic, the network load balancer operates at Layer 4 of the OSI model enabling you to balance requests purely based on the TCP and UDP protocols. As such, a request to open a TCP or UDP connection is established to load balance the host in the target group. The listener supported by the NLB include TCP, TLS and UDP. The NLB is able to process millions of requests per second making the NLB a great choice if you need ultra high performance for your application. Also if your application logic requires a static IP address, then the NLB will need to be your choice of elastic load balancer. Unlike the application load balancer that has cross-zone load balancing always enabled, for the NLB this can either be enabled or disabled. When your NLBs are deployed and associated to different availability zones, an NLB node will be provisioned in these availability zones. The node then uses an algorithm which uses details based on the sequence, the protocol, source port, source IP, destination port and destination IP to select the target in that zone to process the request. When a connection is established with a target host, then that connection will remain open with that target for the duration of the request. Let me now provide a demonstration on how to configure and set up a network load balancer. </p>
<p>As you can see, I’m in the AWS management console. So to create our network load balancer, let’s go to EC2 under Compute. Then if we go down the left-hand side again under Load Balancing, click Load Balancers, we can see here our existing application load balancer we created before. So let’s click on Create Load Balancer and this time we’re going to create a network load balancer. So click on Create. And again, it’s very similar configuration to the application load balancer. So let’s firstly give it a name. Let’s call this DNS-NLB. This time we’ll have it internal facing. For our listener, let’s select the UDP protocol and the load balancer port is port 53 which is DNS. Again, we can select our availability zones where we want our load balancer to reside. So under eu-west-1a, let me select that subnet. And on the b, that one there. Next, configure security settings. Again, we receive this message because we’re not using a secure listener and for this demonstration that’s okay. Configure routing, now we need to associate our target group. Let’s create a new target group this time and we’ll call this DNS. For the target type, I shall leave as instance. We have our port and protocol there. Health checks under TCP. And if you wanted to, you can make any changes to your advanced health check settings there. Next, click on Register Targets. As we can see, we don’t have any registered targets as yet. If I scroll down, I can see I have one instance here so I’m going to add that to the registered list of targets. Once that’s been added, click on Next Review. Once you’re happy with all your configuration settings, click on Create. And there you have it. Your network load balancer is now created. We can see here provisioning which is our network load balancer. This is our previous application load balancer that we created earlier. So it’s a very similar process with different ports and protocols available between the load balancers. And that’s the end of this demonstration.</p>
<h1 id="Classic-Load-Balancers"><a href="#Classic-Load-Balancers" class="headerlink" title="Classic Load Balancers"></a>Classic Load Balancers</h1><p>Hello and welcome to this lecture covering the last of the load balancers that are available, the classic load balancer. The classic load balancer supports TCP, SSL&#x2F;TLS, HTTP, and HTTPS protocols. However, it does not offer as wide a range of features as the other load balancers. It is considered best practice to use the ALB over this classic load balancer unless you have an existing application running in the EC2-Classic network. Now, many of you will be unfamiliar with the EC2-Classic platform, and this is because it is no longer supported for newer AWS accounts. In fact, any account created after the 12th of April 2013 will not support EC2-Classic. </p>
<p>The EC2-Classic platform was originally introduced when the first release of EC2 was made generally available a number of years ago. The EC2-Classic platform enabled you to deploy your EC2 instances in a single flat network shared with other customers instead of inside a VPC. Although the classic load balancer doesn’t provide as many features as the application load balancer, it does offer the following which the ALB does not. It supports EC2-Classic, it supports TCP and SSL listeners, and it has support for sticky sessions using application-generated cookies. Again, the classic load balancer works in much the same way as the other load balancers already discussed, and again, cross-zone load balancing can either be enabled or disabled. Let’s now take a look at the creation of a classic load balancer. </p>
<p>So let’s now create the last type of load balancer, the classic load balancer. So again, let’s go to EC2. Down the left-hand side to load balancers. We have our previous application load balancer and our network load balancer. Let’s now create the classic load balancer. So we go across here to create. Give this a name. I’ll just call it classic. Select the VPC that I’d like to do. Now here we have our listener configuration, so for ease, let’s just have this listed on port 80. And then here, we need to select our availability zones that we’d like. So let’s select this one and also this one here. Once we’ve selected our subnets for our load balancer, we can then assign security groups. I’m going to use an existing security group that I’ve created previously. Once that’s selected, click on Configure Security Settings. Again, it’s telling us we’re not using a secure listener. Again, for this demonstration, that’s more than okay. Now we can configure our health checks. This will probably look familiar to you when we’re discussing the application load balancer. So the port and protocol using and the path, as well, the ping path, which is what the load balancer will check to make sure it can reach to determine if the instance is healthy or not. Once you’re happy with those details, select Add EC2 Instances. </p>
<p>Now here we can select the instances that you want to associate to the load balancer, and this is different to the application load balancer and the network load balancer, where we used target groups. With the classic, we simply select the instances that we want included, so we don’t use target groups for a classic load balancer. So for this example, we can select those two options, coming down across to add tags. Put in any tags you want associated for the load balancer. Click on Review and Create, confirm that you’re happy with your settings, and then click on Create. And there we have it. So if we go back here, you can now see that we have our three different load balancers that we’ve created. Here we have our application load balancer, this was our network load balancer, and here we have our classic load balancer. And it’s as simple as that. </p>
<p>Before I finish this lecture, it’s a good time to take a quick look at the comparison between the three load balancers that we’ve looked at. To help with this, AWS Provides a great table to show the feature differences between each ELB, which can be found using the link shown on screen. We can clearly see that the ALB is the most feature-rich. However, the NLB supports some significant differences to that of the ALB, such as support for static IPs, EIPs, and preserving source IP addresses. </p>
<p>That now brings me to the end of this lecture. Coming up next, I shall be looking at auto scaling and the benefits that this feature brings.</p>
<h1 id="EC2-Auto-Scaling"><a href="#EC2-Auto-Scaling" class="headerlink" title="EC2 Auto Scaling"></a>EC2 Auto Scaling</h1><p>Hello and welcome to the first of the lectures that will be covering EC2 Auto Scaling. So what exactly is EC2 Auto Scaling? Put simply, Auto Scaling is a mechanism that automatically allows you to increase or decrease your EC2 resources to meet the demand based off of custom defined metrics and thresholds. </p>
<p>In AWS, there is EC2 Auto Scaling which focuses on the scaling of your EC2 fleet, but there’s also an Auto Scaling service. This service allows you to scale Amazon ECS tasks, DynamoDB tables and indexes, in addition to Amazon Aurora replicas. For this course I will just be focusing on EC2 Auto Scaling. Let’s look at an example of how EC2 Auto Scaling can be used in practice. </p>
<p>Let’s say you had a single EC2 instance acting as a web server receiving requests from the public users across the Internet. As the requests and demand increases, so does the load on the instance. Additional processing power will be required to process the additional requests and therefore the CPU utilization would also increase. To avoid running out of CPU resource on your instance, which would lead to poor performance experienced by your end users, you would need to deploy another EC2 instance to load balance the demand and process the increased requests. With Auto Scaling, you could configure a metric to automatically launch a second instance when the CPU utilization got to 75% on the first instance. By load balancing traffic evenly, it would reduce the demand put upon each instance and reduce the chance of the first web server failing or slowing due to high CPU usage. Similarly, when the demand on your web server reduces, so would your CPU utilization. So you could also set a metric to scale back. In this example, you could configure Auto Scaling to automatically terminate one of your EC2 instances when the CPU utilization dropped to 20% as it would no longer be required due to the decreased demand. </p>
<p>By scaling your resources back helps to optimize the cost of your EC2 fleet as you only pay for resources when they are running. Through these customizable and defined metrics, you can increase, scale out, and decrease, scale in, the size of your EC2 fleet automatically with ease. This has many advantages and here are some of the key points. Firstly, automation. As this provides automatic provisioning based off of custom defined thresholds, your infrastructure can elastically provision the required resources, preventing your operations team from manually deploying and removing resources to meet demands put upon your infrastructure. Greater customer satisfaction. If you are always able to provision enough capacity within your environment when the demand increases, then it’s unlikely that your end users will experience performance issues, which will help with user retention. And cost reduction. With the ability to automatically reduce the amount of resources you have when the demand drops, you will stop paying for those resources. You only pay for an EC2 resource when it’s up and running, which is based on a per second basis. When you couple Auto Scaling with an Elastic Load Balancer, you get a real sense of how beneficial building a scalable and flexible architecture for your resources can be. </p>
<p>In the next lecture, I shall be explaining the different components of Auto Scaling before providing a demonstration on how to configure it.</p>
<h1 id="Components-of-EC2-Auto-Scaling"><a href="#Components-of-EC2-Auto-Scaling" class="headerlink" title="Components of EC2 Auto Scaling"></a>Components of EC2 Auto Scaling</h1><p>Hello and welcome to this lecture where I’ll focus on the different components of EC2 auto scaling, to help you understand how the process and service works. There are two distinct steps to the configuration. The first step is the creation of the launch configuration or launch template. And the second part is the creation of an auto scaling group. </p>
<p>When using EC2 auto scaling, you can either create a launch configuration or launch template. Both define how an auto scaling group builds new EC2 instances. They both answer a number of questions required when launching a new instance, such as which Amazon Machine Image to use or AMI, which instance type to select. If you’d like to use Spot Instances to help lower costs. If and when public IP addresses should be used for your instances. If any user data is required for automatic scripting on first boot. What storage volume configuration should be used, and what security group should be applied. You will probably be familiar with most of these steps if you have ever created an EC2 instance manually, it’s much the same. </p>
<p>A launch template is essentially a newer and more advanced version of the launch configuration. Being a template you can build a standard configuration allowing you to simplify how you launch instances for your auto scaling groups. Let me now demonstrate how to create both a launch configuration and a launch template. </p>
<p>As you can see, I’m logged into my AWS account and I’m at the Management Console. And to create our launch templates and launch configurations we need to go into EC2 under compute. So let’s take a look. Now I’m going to start by creating the launch template first. And then after that, I’ll create the launch configuration so you can see the differences between them. </p>
<p>Now on the left hand side here, under instances, you can see launch templates. So if you select that, then it’s just a quick splash screen here, just saying welcome to launch templates. And this gives you a brief summary of what it is. So to create a launch template we’ll click on the blue create launch template button. Now here we have a single page with a number of configurable parameters on them. So let’s go through each of them and take a look. So firstly, we can either create a new template or create a new template version. Now as I don’t have an existing template, we can’t create a new version of that template. So let’s start from scratch by creating a new template. Let’s give this a name. I’ll just call this launch template. And a description of demo. Now here we can specify the source template, which essentially allows you to create a template from an existing template that you might already have. And as I explained, I don’t have any other templates at the moment. So we can’t do that and I just want to show you how to create a template from scratch anyway. Now further down we have launch template contents. Now this is where we started getting to the actual configuration of what we’re going to launch. </p>
<p>And we can start by selecting the AMI ID. If we click on the search for AMI, we can have a look at the different catalogs. Either quick start if you have any of your own AMIs on the marketplace or the community AMIs. Let’s just go with a quick start. And then we can select an AMI, let’s just go with the top one here, the Amazon Linux. And select AMI, now we can select an instance type. Let’s just go for a t1 micro. And if we have any existing key pairs, we can select an existing key pair to allow us to connect to our instances. For this one, I’ll just select an existing key pair. And then network type that this instance will reside in, whether it’s in a VPC environment, or the classic environment, we’re going to go with the VPC. Now here we can also attach any security groups Again as a drop down list to allow you to select an existing security groups that you might have. So I’ll just select a couple of different groups here. Now further down, if you want to add any additional network interfaces, then you can do so here simply by clicking on add network interface and filling out the relevant fields. Don’t need to do that in this example. Now here we have storage volumes. So I’ll come with an eight gig EBS volume, general purpose. And we can specify encryption here if we want to, and the delete on termination, either yes or no, and the IOPS et cetera. And we can add additional volumes if we want to as well here. Further down we can instance tags. So let’s add a tag, say for example project cloud academy. And this will tag both the instance and the volume. If we go into advanced details. We can select if you want this to be a Spot Instance or not. We can select an instance profile which allows you to associate a role to your EC2 instance when it launches. We can select the shutdown behavior, whether you want it to terminate or simply stop when we shut it down. And there’s a number of other more advanced options that you can select with regards to your instance. And then at the very bottom we have user data if you want to run any commands on boot. Now once you’re happy with all of your information, all you need to do is simply click on create launch template. And that’s it. </p>
<p>If we get on to close, we can now see our launch template has been created. And the default version is one and the latest version is one. We can create another launch template based on this and give it a different version if we want to. So let’s see how the launch template compares to the launch configuration. </p>
<p>Now the launch configuration is further down on the left hand side on the auto scaling right at the bottom here. So if you click on launch configurations, and this is essentially the same as the launch template. Although the launch template has a few more options, and it’s more simplistic in its creation and is the preferred method. However, you can still create launch configurations, so let’s take a look. Click on create launch configuration. Now here we can select our AMI. And again, we have the different catalogs here like we do with the launch template. It’s just presented differently essentially. Select the AMI. Here we can select our instance type. And again we had this option in the launch template. Here we can give it a name. Let’s call this launch configuration. Again, you can select if you want to use Spot Instances, and you can select an IAM role. Whereas in the launch template you can select the instance profile. If we get on to advanced, there’s not as many advanced options here as there is with the launch template as we had a much longer list. However, you do still have a number of options here should you need it such as user data, specifying the Kernel ID et cetera. If we go to storage. Again, it comes with the default storage for the instance type you selected. And again you can add new volumes if you need to. So again very similar to the launch template. Here you can select your security groups, so you can select an existing group. So again, you can just select the security groups that you need here. Then click on review. And here’s a summary of all the options that you’ve selected. And once you’re happy with those, simply click create launch configuration. And finally, here you can also choose a key pair or create a new key pair should you need to. So again, let’s just let that cloud academy key pair. Then click create launch configuration. And there you go, there’s our launch configuration. </p>
<p>So there’s two different methods of creating that configuration to allow your auto scaling groups to know what instances to launch and how they should be configured. The main difference between the two is that the launch templates is presented all on a single page to allow you to quickly select your options rather than going through a number of different screens. And it also has a few more advanced features and options as well. Okay, that’s the end of the demonstration, thank you. </p>
<p>Without either the launch configuration or launch template, auto scaling would not know what instance it was launching and to which configuration. So before you create your auto scaling group, you need to have your launch configuration defined. But what does the auto scaling group do? Well, the auto scaling group defines the desired capacity and other limitations of the group using scaling policies and where the group should scale your resources, such as which availability zone. Let’s look at each of these details further via another demonstration on how to create an auto scaling group. And during this demonstration, I will create a new auto scaling group based on our previous launch template. And I’ll set up an auto scaling policy defining when to both increase and decrease the group size. Let’s take a look. </p>
<p>Again I’m within the AWS Management Console. And to create our auto scaling group, we need to go into EC2, which is under compute. And then if you scroll to the bottom on the left hand side, we can see under auto scaling, auto scaling groups, so let’s click on that. And this is where we can create our group. Firstly we click on the blue button create auto scaling group. And here we can either create it from a launch configuration or a launch template. So let’s select the launch template which is the new and preferred option. And down here we can select which launch template we would like to use. And this is the one that I created in the previous demonstrations. So once I’ve selected my launch template that I’d like to use, click on next step. Now we can give it a group name. So I just call this demo. If we had multiple versions of our launch template, then we can select the different versions there. But at the moment we just have the single version. With regards to the fleet, we can adhere to our launch template configuration. Or we can use a combination of different purchase options and instances. I just want to use the configuration that we used within our launch template. With regards to the group size, let’s start with two instances in our auto scaling group. And I can select the appropriate VPC that I’d like this to be launched in. And once I select the VPC, then I can select the subnets that I’d like. So let’s just select a couple of subnets in our VPC. Now if we go down to advanced details, here we have a number of other options. Now if you want to associate our auto scaling group to a load balancer, then we can do so here. And we can select our load balancer and target groups. But at the moment I’m going to leave this blank because in a later demonstration in the next lecture, I’m going to show you how to associate an existing auto scaling group with one of your new load balancers. So I’m just going to leave that blank for now. But if you did want to associate your auto scaling group to a load balancer during creation. then this is the place you do it. We have our instance protection down here. And we have an option protect from scale in. So if this is selected then during the scale in procedures, auto scaling won’t terminate any instances that are protected. I’m just going to remove the options for now. And also and finally a service linked role is selected. And this enables access to AWS services and resources that are used or managed by auto scaling. Once we have our configuration set, we can move on to configuring scaling policies. </p>
<p>Now here, we have two options, we can keep this group at its initial size. And as you know I set it to two instances, or we can you scaling policies to adjust the capacity of the group. And that’s what I’d like to do. Now I want to scale between two and five instances say. ‘Cause that’s the minimum and maximum number of instances that this group will scale to. Now I want to scale more auto scaling group using step or simple scanning policy. So I’m going to click on that. Now we have a policy here for increasing the group size and also a policy here for decreasing the group size. So the name for this policy, I’m just going to leave as increase group size. So auto scaling knows when to execute this policy, we need to set an alarm. So click on add new alarm. And this will send a notification. At that the moment I’ve got a notification set up as CADemo, which is an SNS topic. And I want this alarm to trigger whenever the average CPU utilization is greater than or equal to 75% for one consecutive period of five minutes. The name of this alarm will be deploy new instances. And then I simply click on create alarm. Now I want to take the action of adding just a single instance when the CPU utilization is greater than 75%. </p>
<p>Now we can do the same for the decrease group size, so whenever the average CPU utilization is less than or equal to 30%, for two consecutive periods of five minutes, and I’m going to call this remove instances and create that alarm. I’m going to say remove one instance when the CPU is less than 30%. Once your policies are set, you can click on configure notifications. So if you want to configure your notifications, simply click on add notification. And that’s my SNS topic that I had. So whenever instances are launched, terminated, fail to launch and fail to terminate, I want to receive those notifications. Let’s click on configure tags. And here, you can add any tags to auto scaling group. Just going to leave that blank for this demonstration, click on review. And here we have all of our configuration that we just made. Once you’re happy with that, click on create auto scaling group. </p>
<p>And we now have our auto scaling group configured and except here the minimum and maximum instances two and five and the availability zones et cetera. Now at the bottom here you can go into the auto scaling group details. Here we can see it’s launching two new instances because we said we want to start with two instances to start with. We can review our scaling policies. We can look at instances, monitoring, notifications, et cetera, et cetera. So if we go over to our instances, we should see two new instances that are launching. and here we can see that these two here are both initializing. So that’s our two new instances that are running because of our auto scaling group which was based off of our launch template. And that’s it. </p>
<p>In the next lecture, I should be looking at how both ELB and auto scaling combined can be used to manage your EC2 infrastructure.</p>
<h1 id="Using-ELB-and-Auto-Scaling-Together"><a href="#Using-ELB-and-Auto-Scaling-Together" class="headerlink" title="Using ELB and Auto Scaling Together"></a>Using ELB and Auto Scaling Together</h1><p>Hello and welcome to this short lecture where I shall discuss the relationship of ELBs and EC2 auto scaling. As you saw in the demonstration on the previous lecture, it’s easy to associate your EC2 auto scaling group to an elastic load balancer and this is because the two services go hand in hand to provide optimal efficiency for both the performance and cost perspective. Each service by itself provides a great way to solve particular operational hurdles. The ELB allows you to dynamically manage loads across your resources based upon target groups and rules whereas EC2 auto scaling allows you to elastically scale those target groups based upon the demand put upon your infrastructure. However, one without the other can cause an operational burden. </p>
<p>For example, let’s say you have an ELB configured but without any auto scaling. You will need to ensure that you manually add and remove targets based upon the demand. You will need to monitor this demand allowing you to manually add or remove instances as required. Now let’s look at the reverse. Let’s assume you have EC2 auto scaling configured but no elastic load balancer. How are you going to evenly distribute traffic to your EC2 fleet? </p>
<p>Hopefully, you can see the benefit of combining an ELB and auto scaling to help manage and automatically scale your EC2 compute resources both in and out. When you attach an ELB to an auto scaling group, the ELB will automatically detect the instances and start to distribute all traffic to the resources in the auto scaling group. When you want to associate an application load balancer or network load balancer, you associate the auto scaling group with the ELB target group. When you attach a classic load balancer, the EC2 fleet will be registered directly with the load balancer. </p>
<p>Let me now provide another demonstration on how to associate your ELBs with an auto scaling group. </p>
<p>Okay, so to attach an existing load balancer to your auto scaling group is very quick and simple. So firstly we go to our EC2 dashboard under Compute. We then go down to our Auto Scaling Groups at the very bottom on the left-hand side. We can see here our auto scaling group that we created in a previous demonstration. Now you’ll notice on the Details section at the bottom here there’s a section for classic load balancers and target groups and both of these are empty fields. So let’s look at how you would associate either a classic load balancer or your target group. So once our auto scaling group is selected, we go to up to Actions and then Edit. Now if we scroll down, we get to the section here Classic Load Balancers and Target Groups. Now if we want to add a classic load balancer, then you can select here and select any classic load balancers that you have configured so I can select that one for example or if you’re using an application load balancer or network load balancer, then you associate it with the target groups because the target groups are the pool of resources that the application load balancer or network load balancer are associated to. Now here we have two target groups that we created in a previous demonstration either DNS or WebServers. So we could select WebServers as our target group and simply click on Save. </p>
<p>And as you can see now, this entry here for target groups, the auto scaling group is now associated to the WebServers target group. And it’s as simple as that. So all you need to do to associate an existing load balancer to your auto scaling group is to edit the auto scaling group and if it’s a classic load balancer, add in the classic load balancer or if it’s an application load balancer or network load balancer, then you associate it to the relevant target group. And that’s it. </p>
<p>That now brings me to the end of this lecture. Coming up next, I will provide a summary of the key points made throughout this course.</p>
<h1 id="2What-is-Compute-in-AWS"><a href="#2What-is-Compute-in-AWS" class="headerlink" title="2What is Compute in AWS?"></a>2<strong>What is Compute in AWS?</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/products/compute/">Cloud Compute Index</a></p>
<h1 id="3EC2-Elastic-Compute-Cloud"><a href="#3EC2-Elastic-Compute-Cloud" class="headerlink" title="3EC2 - Elastic Compute Cloud"></a>3<strong>EC2 - Elastic Compute Cloud</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Shared Responsibility Model and Security Groups</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/create-your-first-amazon-ec2-instance-1/">Lab: Create your first Amazon EC2 Instance (Linux)</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/">Lab: Ceate your first Amazon EC2 Instance (Windows)</a></p>
<h1 id="4ECS-Elastic-Container-Service"><a href="#4ECS-Elastic-Container-Service" class="headerlink" title="4ECS - Elastic Container Service"></a>4<strong>ECS - Elastic Container Service</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-docker-2/">Course: Introduction to Docker</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/basics-of-using-containers-in-production/container-orchestration-1/">Basics of using Containers in Production</a></p>
<h1 id="5ECR-Elastic-Container-Registry"><a href="#5ECR-Elastic-Container-Registry" class="headerlink" title="5ECR - Elastic Container Registry"></a>5<strong>ECR - Elastic Container Registry</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">Course: Overview of AWS Identity &amp; Access Managment (IAM)</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html">Docker Push</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html">Docker Pull</a></p>
<h1 id="6EKS-Elastic-Container-Service-for-Kubernetes"><a href="#6EKS-Elastic-Container-Service-for-Kubernetes" class="headerlink" title="6EKS - Elastic Container Service for Kubernetes"></a>6<strong>EKS - Elastic Container Service for Kubernetes</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-kubernetes/">Course: Introduction to Kubernetes</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-aws-eks/">Course: Introduction to EKS</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">Install Kubectl</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/aws-iam-authenticator">Linux IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/darwin/amd64/aws-iam-authenticator">MacOS IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/windows/amd64/aws-iam-authenticator.exe">Windows IAM Authenticator</a></p>
<p><a target="_blank" rel="noopener" href="https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml">Configuration map to joing the Worker Node to the EKS Cluster</a></p>
<h1 id="7AWS-Elastic-Beanstalk"><a href="#7AWS-Elastic-Beanstalk" class="headerlink" title="7AWS Elastic Beanstalk"></a>7<strong>AWS Elastic Beanstalk</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/deploy-php-application-using-elastic-beanstalk-26/">Lab: Deploy a PHP Application using AWS Elastic Beanstalk</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/run-controlled-deploy-aws-elastic-beanstalk-43/">Lab: Run a controlled deploy with AWS Elastic Beanstalk</a></p>
<h1 id="8AWS-Lambda"><a href="#8AWS-Lambda" class="headerlink" title="8AWS Lambda"></a>8<strong>AWS Lambda</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html#supported-event-source-s3">AWS Lambda Event Sources</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-lambda-to-run-scale-code/">Course: Understanding AWS Lambda to Run and Scale your Code</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/introduction-aws-lambda-22/">Lab: Introduction to AWS Lambda</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/aws-lambda-s3-events-55/">Lab: Process Amazon S3 events with AWS Lambda</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/automating-ebs-snapshots-lambda-and-cloudwatch-events-45/">Lab: Automating EBS snapshots with AWS Lambda</a></p>
<h1 id="10Amazon-Lightsail"><a href="#10Amazon-Lightsail" class="headerlink" title="10Amazon Lightsail"></a>10<strong>Amazon Lightsail</strong></h1><p><a target="_blank" rel="noopener" href="https://lightsail.aws.amazon.com/ls/webapp/home/resources">Amazon Lightsail dashboard</a></p>
<h1 id="12SSL-Server-Certificates"><a href="#12SSL-Server-Certificates" class="headerlink" title="12SSL Server Certificates"></a>12<strong>SSL Server Certificates</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#acm_region">Regions supported by ACM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html">How to retrieve and list server certificates via ACM</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html">Additional information on ACM</a></p>
<h1 id="13Application-Load-Balancers"><a href="#13Application-Load-Balancers" class="headerlink" title="13Application Load Balancers"></a>13<strong>Application Load Balancers</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/">Course: OSI and TCPIP Networking Models</a></p>
<h1 id="15Classic-Load-Balancers"><a href="#15Classic-Load-Balancers" class="headerlink" title="15Classic Load Balancers"></a>15<strong>Classic Load Balancers</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/elasticloadbalancing/features/#compare">Table showing the differences between the Load Balancers</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-4/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:23" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:23-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:46" itemprop="dateModified" datetime="2022-11-20T19:01:46-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-4/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-AWS-Global-Infrastructure-Availability-Zones-Regions-Edge-Locations-Regional-Edge-Caches-4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Cloud-Concepts-CLF-C01-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Cloud-Concepts-CLF-C01-3/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Knowledge-Check-Cloud-Concepts-CLF-C01-3</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:22" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:22-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:00:44" itemprop="dateModified" datetime="2022-11-20T19:00:44-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Cloud-Concepts-CLF-C01-3/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Cloud-Concepts-CLF-C01-3/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Your-profile-has-been-updated-with-the-following-skills.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Cloud-Concepts-CLF-C01-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Cloud-Concepts-CLF-C01-2/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Cloud-Concepts-CLF-C01-2</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:20" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:20-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:59:12" itemprop="dateModified" datetime="2022-11-20T18:59:12-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Cloud-Concepts-CLF-C01-2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Cloud-Concepts-CLF-C01-2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Cloud Computing Concepts, where we’re here to help you begin your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that cover the objectives from Domain 1 of the official AWS Certified Cloud Practitioner exam guide, focusing on Cloud Concepts. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#64;&#x63;&#x6c;&#x6f;&#x75;&#100;&#97;&#x63;&#97;&#100;&#101;&#109;&#121;&#46;&#99;&#x6f;&#x6d;">&#115;&#117;&#112;&#x70;&#x6f;&#114;&#116;&#64;&#x63;&#x6c;&#x6f;&#x75;&#100;&#97;&#x63;&#97;&#100;&#101;&#109;&#121;&#46;&#99;&#x6f;&#x6d;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course is specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn the fundamentals of cloud computing in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to the fundamentals of cloud computing as well as to define:</p>
<ul>
<li>Key cloud concepts such as scalability, elasticity, and security;</li>
<li>Cloud deployment models such as public, private, and hybrid; and</li>
<li>Cloud service models including Infrastructure-as-a-Service, Platform-as-a-Service, and Software-as-a-Service.</li>
</ul>
<p>We’ll also look at some common use cases and business benefits of cloud computing, and provide a framework for deciding if moving to the cloud is right for your organization.</p>
<p>Together, these objectives cover the first of four domains in the official AWS Certified Cloud Practitioner exam blueprint: Cloud Concepts, which accounts for 26% of the exam content. The other courses in this learning path will cover the remaining three domains and ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="What-is-Cloud-Computing"><a href="#What-is-Cloud-Computing" class="headerlink" title="What is Cloud Computing?"></a>What is Cloud Computing?</h1><p>The phrase cloud computing has been used heavily within the IT industry for many years now, and more recently it’s being used within other sectors such as retail and finance, as it becomes increasingly popular. Many people often refer to the technology simply as the Cloud. Cloud computing is a rapidly growing technology, and the adoption of this is a key strategy for many organizations, and there is a very good reason behind this. It’s changing the landscape of how many companies operate on a huge scale, with significant business and technical advantages and benefits that can’t be ignored. The growth of cloud computing has been exponential over the past few years, so what is it? Put simply, cloud computing is a remote virtual pool of on-demand shared resources offering to compute, storage, database and network services that can be rapidly deployed at scale. Now there may be a couple of terms within this definition that are new to you, or not too clear, such as virtual or computer. Don’t worry, I’m going to break these down over the next couple of slides, after which I shall give you the definition again, and you should be able to fully understand this concept. </p>
<p>Before fully understanding cloud computing, we must be aware of some existing technology that it’s based upon, that being, virtualization, and this is being used in on-premise data centers for a long time. But this virtualization maximizes the power of cloud computing, and without this virtualization, it would just not be possible. So what is virtualization? In essence, it allows the possibility of having multiple virtual machines, VM’s, each running essentially a separate operating system and applications, all installed on one physical server. These VM’s all run at the same time without being aware of each other’s existence while sharing the underlying hardware resources. This sharing of hardware resources is a key element of understanding of virtualization and is achieved through a hypervisor. A hypervisor is a piece of software you use to create the virtualized environment, allowing for multiple VM’s to be installed on the same host. When installed, the hypervisor sits logically between the physical server hardware and the virtual machines and creates a shared pool of virtual hardware resources for each of them to access. All VM’s installed on the host see the hardware as they normally would. </p>
<p>However, any request to the hardware goes via the hypervisor, which then handles that access ensuring the hardware resources are shared between all other VM’s as needed and as configured. Now there are obvious benefits of virtualization, which include, reduced capital expenditure. As less hardware is required, as you have the ability to provision multiple VM’s on a single host. Reduced operating cost, because as there is less hardware, there is less space, power, cooling required within your data center, and the footprint also reduces within your data center as less space is required to house your server hardware. This optimization of resources when in a cloud environment means everyone can benefit from virtualization, from the cloud vendor to the consumer. Now just a quick note before we leave this topic of virtualization. A VM within the public cloud is sometimes referred to as an instance, this term is very vendor specific but it refers to the same object as a virtual machine. When discussing resources within cloud computing, it won’t be long before you come across the terms such as compute, storage, database and network resources. Increasingly you may also hear other terms such as machine learning or artificial intelligence. However, for <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/introduction-8/">this beginners course</a>, I just want to keep the focus on the core foundation services. </p>
<p>It’s a good idea to have a clear distinction between compute, storage, database, and network. And what each of these refers to, is that it will certainly help you going forward when identifying what services you want to move to the cloud, should you decide to do so. Compute objects provide the brains to process your workload. Including what’s required to process a run request from applications and services. As a comparison, if you think of hardware devices with CPU’s and RAM, typically servers and how they work in a classic, on-premises environment, compute resources in a cloud, are comparable to these. Storage resources simply allow you to save and store your data. Any resource that allows you to save your data in the cloud, is classed as a storage resource. Again as a comparison, in the typical environment these will be seen as your server hard disks, or your network-attached storage, which provides file-level shed storage over the network, or your high-speed storage area network, your SAN, which is block-level storage accessed over a high-speed network. Database resources allow you to store structured sets of data used by your applications. Again as a comparison, databases are widely used in Data Centers with some common database engine types being SQL Server, Oracle, and MySQL. Within the Cloud, there are a wide variety of database engines available for different use cases. Network resources provide the connectivity allowing all other resources, compute, storage, and database, to communicate with each other. In a typical environment, you would find hardware, such as routers to route traffic between network switches, which provide the backbone of network connectivity allowing the host to talk to one another and firewalls to allow or deny traffic into the environment. </p>
<p>So now if we go back to our definition of cloud computing given earlier of, cloud computing is a remote virtual pool of on-demand shared resources, offering to compute, storage, database and network services that can be rapidly deployed at scale. You should now have a clearer understanding of what this actually means. The red section refers back to the virtualization and the blue section refers to the typical resource types available within cloud computing that we just discussed. That has now brought me to the end of this lecture, coming up next, I shall be discussing the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/cloud-deployment-models/">cloud deployment models</a> that are typically referred to in cloud computing.</p>
<h1 id="Cloud-Deployment-Models"><a href="#Cloud-Deployment-Models" class="headerlink" title="Cloud Deployment Models"></a>Cloud Deployment Models</h1><p>Hello and welcome to this lecture where I shall explain some of the different deployment models used when adopting Cloud technology. Typically within Cloud computing, there are three different Cloud model types, each offering different levels of management, flexibility, security and resilience, and these are Public, Private and hybrid. Let’s start with the Public Cloud. A Public Cloud model is where a vendor makes available use of shared infrastructure including, but not limited to, compute, storage, database and network resources, that can be provisioned on demand and typically accessed over the internet for Public usage. The consumer will never see the hardware used nor know the exact physical location of their data, but they will be able to specify the geographic region in which it resides to aid with data latency depending on where you end users are located. It makes sense from a design perspective to host your infrastructure as close to the geographical region as your customers or end users are, as this will provide the best overall performance for them. All back and maintenance of the physical location services such as power calling et cetera, along with the physical maintenance of hosts such as hardware failures, will be maintained by the Cloud vendor and seemingly invisible to the end user. As a general rule, you can access your services on the Public Cloud from anywhere as long as you have an internet connection. </p>
<p>A Private Cloud is different to a Public Cloud in that the infrastructure is Privately hosted, managed, and owned by the individual company using it, giving greater and more direct control of it’s data. Enterprises who wish to keep a tighter grasp of security control may adopt this architecture. As a result, the hardware is usually held on premise. How this differs from a typical on-premise server farm approach, is that the same Cloud principals are applied to the design such as the use of virtualization, creating a pool of shared computer storage and network resources, making use of scalability and on-demand provision. With this approach, more capital expenditure is requited to acquire the host and the data center that they will physically reside in. Not only this, additional resource will be needed for the day to day operations and maintenance of this equipment. And so your daily operational cost will also increase. compared to that of a Public Cloud model. As you may have already guessed, a hybrid Cloud is a model that makes use of both Public and Private Clouds. This model may be used for seasonal burst traffic or for Disaster Recovery. A hybrid model is established when a network link is configured between the Private Cloud to services within the Public Cloud, essentially extending the logical internal network of the Private Cloud. </p>
<p>This makes the benefits given from both the Public and Private models and allows you to architect your services in the most appropriate model. However, be aware that they also contain the same negatives from both solutions too. Hybrid Clouds are normally short-term configurations, maybe for test and dev purposes, and can often be a transitional state for enterprises before moving a service to reside purely in the Public Cloud. This table highlights some of the differences between the Cloud types. So feel free to pause the video just to examine the contents of this table just so you really understand the differences. That brings me to the end of this lecture covering the three main deployment types of Cloud technology. For the remainder of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/introduction-8/">this course</a>, I’ll be primarily focusing on Public Cloud deployments. Coming up in the next lecture, I’ll be looking at some of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/key-cloud-concepts/">key Cloud concepts</a> to be aware of.</p>
<h1 id="Key-Cloud-Concepts"><a href="#Key-Cloud-Concepts" class="headerlink" title="Key Cloud Concepts"></a>Key Cloud Concepts</h1><p>Hello and welcome to this lecture aimed at providing you with an understanding of some of the concepts which make cloud technology so important. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/introduction-8/">Cloud computing</a> has a number of key characteristics that allow it to be the powerful service that is it today, and it’s a good idea to have an understanding and awareness of these and what they offer. On-demand resourcing. This essentially means that when you want to provision a resource within the cloud, it’s almost immediately available to you to allocate where and when you need it. No more waiting around for hardware to be ordered, installed, cabled, and configured before using it. Scalability. Cloud computing offers you the ability to rapidly scale your environment’s resources both up and down and in and out, depending on your requirements and demands of your application and services. When scaling up and down, you effectively alter the power and performance of an instance, perhaps using one with a greater CPU or memory power. When scaling in and out, you are simply adding or removing the number of instances you’re using to your fleet of compute resources. This offers a significant advantage compared to on-premise solutions, from a cost-perspective alone. Economy of scale. Due to the huge scale of resources public cloud offerings provide, which are optimized and shared between different organizations, thanks to virtualization technology, you as the end user benefit from exceptionally low resource costs compared to traditional hosting. Flexibility and elasticity. Cloud computing offers huge flexibility and elasticity to your design approach. You can choose to have as many or as few resources as you require. You decide how much and how long you want it for, and at what scale. </p>
<p>The amount of choice you have allows you to fully customize exactly how you want and need your environment, using only the resources required. Growth. Cloud computing offers your organization the ability to grow using a wide range of resources and services. Couple with the on-demand element, and your growth constraints are significantly reduced compared to a classic on-premises environment. This growth also includes the ability to reach global customers with ease, by provisioning resources across the cloud vendor’s global network. Utility-based metering. With many cloud services, you only pay for what you use. What do I mean by this? If you only have an instance running for two hours, and then shut it down, then you only pay for two hours worth of compute resources, and that’s it. Think of it like this. In your house, you only pay for your electricity when you use it, and to help keep costs down, you turn off the lights when you’re not using them. So it’s the same billing process for many resources and services. You only pay for resources when you are using them. Shared infrastructure. As discussed previously, during the virtualization section, hosts within the cloud are virtualized. As a result, multiple tenants can be running instances on the same piece of hardware. This significantly reduces the amount of physical hardware required, which in turn reduces the amount of power, cooling, and space required in the data center. And in turn, helps with the economy of scale, all resulting in cheaper costs to you as the customer. Highly available. By design, many of the core services within the public cloud and its underlying infrastructure are replicated across different geographic zones and regions. Having data copied to multiple different places automatically, helps you to ensure the durability and availability of your data and services, without even having to configure and architect for this resilience. It’s provided by the vendor as a part of their service. Security. </p>
<p>This is one of the most discussed topics within cloud computing, and many enterprises still have concerns over how secure it is. However, public cloud vendors such as AWS and Microsoft Azure are considered to be more secure than your own data center. This is down to the fact that they have to adhere to global compliance programs across multiple industries and by applying the shared responsibility model. The vendor will operate to an exceptionally high standard of security for the underlying infrastructure of the cloud, and it’s down to you, the end user, to then architect security in the cloud, using the tools, services and applications available. For more information on the shared responsibility model for Amazon Web Services, please take a look at our existing blog here. These are just some of the key characteristics of cloud computing, and you can see how differently it operates from your traditional on-premise data center deployments that you may be using today. That now brings me to the end of this lecture. Next I’ll be looking at the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/cloud-service-models/">cloud service models</a> available.</p>
<h1 id="Cloud-Service-Models"><a href="#Cloud-Service-Models" class="headerlink" title="Cloud Service Models"></a>Cloud Service Models</h1><p>Hello and welcome to this lecture, whilst you’ll be looking at some of the different cloud service models that you will likely encounter as you become more familiar with cloud computing. So now you have an idea of the different cloud types that there are, public, private, and hybrid. You will need to know which service model you would like to deploy within it. There are many different service models available, and more being defined all the time, although three of the most common are that of infrastructure as a service, platform as a service, and software as a service. Each service offering provides a different level of manageability and customization over your solution. So let’s look at the lowest level of customization first, that being software as a service. You would have used many examples of software as a service applications, perhaps without even realizing it. Software as a service allows for the delivery of an application that can be widely distributed and accessed. An example of this would be Google’s email service, Gmail. This email-based application is fully managed by Google and is accessed over the internet, and there are no requirements to install any software on your local device to be able to use it. They are usually simple in their design, focusing on the ease of use to appeal to the wider audience. </p>
<p>From the user perspective, this offers the least amount of customization to the application itself. Platform as a service. This service offering gives a greater level of management and control to you, as you have access to an application framework that sits on top of the operating system and up. The underlying architecture, the host hardware, network components, and operating system are typically managed by the vendor and taken care of from a maintenance and support perspective, which makes this a great deployment service for developers. Developers are then free to focus and concentrate on developing great new apps sitting on top of the platform. Infrastructure as a service. This provides the highest level of customization and management. This service allows you to architect your own portion of the cloud by configuring a virtual network, which is segmented from other networks, allowing you to deploy any resources you require. In addition to this, you have the ability to configure instances from the operating system and up, including the type of operating system you install. This service offers the highest level of customization. </p>
<p>However, the underlying host is still managed by the vendor for maintenance and security purposes. There are a number of other service models, such as disaster recovery as a service, communications as a service, and monitoring as a service. For the purpose of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/introduction-8/">this course</a>, we do not need to delve into these. However, I wanted you to be aware that there are more of these services that are making their way into the industry. For now, though, as this is an introduction, you simply need to focus and be aware of software as a service, platform as a service, and infrastructure as a service, as these are the most common within the industry. That has brought me to the end of this lecture. Coming up next, I shall be explaining some of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/common-use-cases-of-cloud-computing/">common use cases of cloud computing</a>.</p>
<h1 id="Common-use-cases-of-Cloud-Computing"><a href="#Common-use-cases-of-Cloud-Computing" class="headerlink" title="Common use cases of Cloud Computing"></a>Common use cases of Cloud Computing</h1><p>Hello and welcome to this lecture. Looking at some of the use cases of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/introduction-8/">cloud computing</a>, and how enterprises have adopted this technology. Following the cloud concept section discussed previously you may already be thinking of some of your own uses that you could use a public cloud for. However, in this lecture I just want to cover some of the common use cases of why organizations implement cloud computing. I want to start by touching one of the bigger use cases where people migrate production services from their existing on-premise solutions into the cloud. We have discussed the benefits of the cloud, and so with all those in mind many businesses are choosing to do just that. Migrate their existing production services to the public cloud. Some companies even have all of their infrastructure within the cloud. Traffic Bursting. As an example you may experience times within the year perhaps for predicted seasonal circumstances where your infrastructure takes a heavier load impact than that of other times of the year. Perhaps you are in the retail business, and over the Christmas holiday period demand increases on your infrastructure significantly. </p>
<p>In a classic data center environment you would need to provision your compute storage database, and network capacity to reflect this additional traffic, and have it take up additional space, power, and cooling all of the time. This is not an effective method of scaling not only this, but there will be additional costs for this extra infrastructure to obtain, maintain and operate, and you may only use it for a couple of months of the year. A far better method of handling this peak traffic road will be to look at cloud computing. The public cloud can be used to scale your networking resources to manage, and handle this additional traffic over your peak season. When the traffic has reduced you can then terminate your infrastructure within this cloud and stop paying for it. Remember you only pay for what you use when you use it. Backup and Disaster Recovery. </p>
<p>Due to the public clouds built-in resiliency, and durability, this makes way for a great solution for your backup requirements. To a degree you have access to unlimited storage space with built-in data management lifecycle policies allowing you to make use of even cheaper storage. For example, using Amazon Web Services S3 service, you can implement a policy to archive any data that is over 30 days old to another service called AWS Glacier which is a cold storage service with an even lower storage cost. The data is still available to you for as long as you have access to the Internet. These storage services are often replicated by the vendor to ensure its durability. Couple that with a very low cost of storage, and you can see why more and more enterprises are adopting cloud computing for this very reason. Web Hosting. Many organizations choose to host their web services on the cloud due to its ability to load balance across multiple instances as well as scale up and down quickly and automatically as traffic increases and decreases with demand. The ability to provision, and implement automatic scaling simplifies the whole process and takes out much of the administrative input, and maintenance required. Not only can your web application, and database service be enhanced by design, but they can also make use of other services such as a content delivery network a CDN, and Domain Name Services DNS. </p>
<p>Remember earlier when we were talking about selecting a geographic region for your instance depending on where end-users are. Well, what if you had end-users all over the world? A CDN is a set of systems which redirects traffic to the closest caching server which can deliver the content much faster. As a result a CDN can reduce the latency of a website for users across the globe if there are sufficient caching servers in place. DNS services can help to manage demand on your web servers by redirecting any requests to a load balancer first. This load balancer can then evenly distribute the requests to multiple web instances that you may have, therefore, reducing demand on a particular web server. Test and Development Environments. Similarly to our first point of traffic bursting, you may not have the capacity to hosting lots of servers, and storage in your data center for tests, and development purposes and from a financial perspective this would be a huge expense. Using the public cloud allows you to spin up instances as and when you need them, and then shut them down when finished. This also allows you to provision the size, and capacity of your compute resources say, for example, if you need a high-end powerful instance for your testing for an hour you can have it. </p>
<p>It would not be financially viable for you to have this wide range of compute resource within your own data center. Proof of Concept. The cloud easily allows you to implement a proof of concept design and ideas to help bring them to life at a fraction of the cost for the reasons that I’ve already covered in the previous points. This includes hosting costs and only paying for what you use. The results of your proof-of-concept can help you to build a successful business case when presenting to senior management. Big Data and Data Manipulation. The cloud also makes it easier, and cheaper to manage big data. Maintaining and implementing compute resources to handle huge data sets can be expensive and complicated. By using cloud computing resources you have the ability to use only the resources you need to analyze data when you need it. Some public cloud vendors offer specialized managed Big Data services. Which gives you a managed resource infrastructure, and framework to run your workloads on, in addition to allowing for scalability, scheduling, monitoring et cetera. Having some of these elements managed by the vendor allows you to focus on the data, and processing and not worry about the maintenance, or the underlying architecture. There are many many more use cases of services within the cloud computing space, and you’ll more than likely have a few ideas, or requirements of your own. Whatever you choose to do you will have the benefits of the concepts discussed earlier at your disposal. That now brings me to the end of this lecture. Coming up next I shall look at the comparisons between <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/data-center-architecture-cloud/">Data Center Architecture</a> and that of the cloud.</p>
<h1 id="How-Data-Center-architecture-is-reflected-in-the-Cloud"><a href="#How-Data-Center-architecture-is-reflected-in-the-Cloud" class="headerlink" title="How Data Center architecture is reflected in the Cloud"></a>How Data Center architecture is reflected in the Cloud</h1><p>Hello and welcome to this lecture. Where I want to explain how certain technology that we are used to on premises such as routers and switches are reflected within the public cloud. You may be new to cloud computing, but you may have the experience of architecture from a classic on premise solution within a datacenter. This lecture assumes you have the knowledge of what an on premise datacenter is and how they operate. And during this lecture will make comparisons between those datacenters and how cloud computing operates. The datacenter as a whole and its architecture can be logically broken down as follows. Location. where it’s geographically located. Physical Security, both External and Internal. Mechanical and Electrical Infrastructure such as Computer Room Air conditioning units, generators, UPS and fire suppression. Network Infrastructure. Including Switches, Routers and Firewalls. Servers. Such as Application Director and Database servers. Storage. For example, your NAS, SAN Block Storage and Backup. Let’s Look at each of these in turn and discuss starting with Location. Depending on the size of your enterprise, you may only have one office and as a result, you house all of your infrastructure on site. However, many of you probably have multiple datacenters to help with resilience and availability. Public cloud providers do the same. They will have regions all over the globe. And with each of these regions, they will have at least two datacenters. These datacenters will be in a different geographic location within that region. </p>
<p>But close enough to provide high speed interconnectivity between them for data transfer in addition to assisting with high availability and resilience. Physical Security. As we have already discovered, the Public cloud is operated, managed and maintained by the vendor. As a result, the end user has no access to the physical datacenter where the resources are located. It is the vendors responsibility to ensure it is implementing and achieving the correct certification and governance regarding security. Public cloud vendors adhere to the most stringent of security controls, and for audit purposes, you can access our accreditation and compliance certifications online if required to do so. I’ve included links to two of the leading public cloud vendors, where you can find more information about their security and compliance:</p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/compliance/">https://aws.amazon.com/compliance/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/trust-center/compliance/compliance-overview">https://www.microsoft.com/en-us/trust-center/compliance/compliance-overview</a></p>
<p>Mechanical and Electrical. Mechanical and Electrical Infrastructure such as generators, UPS systems, Computer Room Air conditioning units for cooling, Fire suppression et cetera is situated at the datacenter itself. As a result, the same rule applies. It is the vendors responsibility to ensure they are implementing the correct capacity, resiliency and testing to ensure availability and uptime of their infrastructure. So again, this burden is removed from the end user. Where roles and responsibilities begin to change between vendor and the customer is from this point onwards. Network Infrastructure. Network Infrastructure can be quite an extensive list of equipment. However, I’m going to cover some of the more common components and how they sit within the public cloud. Networking operates at software level, there are no options to install a switch or router. How this is achieved is beyond the scope of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/introduction-8/">this course</a>. </p>
<p>But what is important is this, to a degree, you are able to implement controls, services and configurations to simulate the same effects within your infrastructure that these devices provide. There are options to create a virtual network, which can be segmented into different IP address ranges allowing you to deploy compute storage and other network resources as you require. These virtual networks are usually simple to set up requiring just a few small details before it’s created. Within AWS, these are classed as Virtual Private Clouds VPCs whereas Microsoft Azure refers to these networks as an Azure Virtual Network VNet. From here you can create different network segments both public facing and private. Routing and Access Control List for security can also be configured dynamically for enhanced control and once you have created your logical virtual network, and of course different geographic regions If desired you can then provision your instances within these different subnets. So networking components such as switches, routers and even firewalls have been replaced with virtual networks and their configurable components. </p>
<p>The backend element of how these services work is again maintained and managed by the vendor. But architecting how your virtual network is created is your responsibility and your responsibility to ensure it secure and not let vulnerable to attacks. Servers. Depending on your vendor, servers are typically referred to as instances or virtual machines VMs. But as there are different servers within the typical datacenter, vendors provide different services to affect them in the cloud. For example, providers offer servers that are specific to hosting databases, and others focused on heavy processing power that is needed to process big data. This is part of the benefit of the cloud. The vendors are capable of replicating the functions that you use with your in house datacenter but with a lot more advantages. Storage. Storage is fantastic within the Cloud as it’s often regarded as unlimited, hugely scalable and highly durable. As with compute power, there are different storage services depending on what sort of data you’re using and where you intend to use it. Within your datacenter environment, you may have access to a Storage Area Network a SAN. </p>
<p>Public cloud providers can also provide block level storage too by their services. AWS offers the Elastic Block Store EBS service. Which offers persistent block level storage, and can we detach from one instance and re-attached to another instance again all configured and deployed within a few clicks. There are other great storage services within the public cloud that cater for file level storage and object storage too. And between the leading public cloud vendors, AWS and Azure, they host a myriad of storage services to cater to different solutions. For more information on these storage services, please see the following links.</p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/products/storage/">https://aws.amazon.com/products/storage/</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/storage/">https://azure.microsoft.com/en-us/services/storage/</a></p>
<p>Now I’ve only skimmed the surface of the available services and offer within the public cloud. And there are many other great services that offer amazing solutions to requirements. Many organizations have within those ideas date. However, I just wanted to point out how your current datacenter solutions and elements easily translate into the public count environment. Was at the same time carrying significant advantages and benefits as discussed earlier in this course. In the next and final lecture, I shall be providing a high-level <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/what-is-cloud-computing-introductory/summary/">summary</a> of the key points taken throughout this course.</p>
<h1 id="Is-the-Cloud-right-for-you"><a href="#Is-the-Cloud-right-for-you" class="headerlink" title="Is the Cloud right for you?"></a>Is the Cloud right for you?</h1><p>Hello and welcome to this lecture. Here we’ll begin looking at what the cloud offers organizations that typical on-premise solutions do not. Understanding this can help you decide if moving to the cloud is right for your business, so let’s get started.</p>
<p>Before making this decision, you need to ask yourself where is the business going, what are you trying to achieve, and what your business objectives? If you have answers for these questions, then you have an end goal. This is the first step in understanding if the cloud can provide you with the correct business strategy.</p>
<p>If you don’t have answers to these questions, then now would be a good time to consider them. Without knowing what your goals and priorities for your business are, it would be difficult to really understand if cloud adoption would be a viable direction to go.</p>
<p>To identify these start by focusing on your key drivers for improvement, either strategic or financial or customer-focused that may relate to specific governance controls. Understanding where your organization lacks in development, where challenges exist, which could do with re-evaluation. These factors may be infrastructure-driven or even business process-related, but it’s important to know where your weaknesses lie and where you need to streamline additional business functionality. Be sure to identify what challenges you face as an organization. Every business will have challenges, so identify what these are and try to define what you need to achieve to overcome them.</p>
<p>Once you have performance assessment, you have a much clearer mindset and be in a far greater position to understand if the cloud could aid your business and help drive it forward.</p>
<p>Okay, so let’s say you now have your goals, targets, and key achievements identified. The question is can all of these be achieved by utilizing cloud computing? Let’s take a look.</p>
<p>So why would you migrate to the cloud? What will it bring to the business? Let’s try and answer these questions.</p>
<p>There are so many advantages to cloud computing, which is why it’s such a hugely growing technology. To start with I want to take some time to talk about some of the characteristics of the cloud to clarify some points. As we go through, make note of any that can be useful in achieving your end goals and be of benefit to your organization. To be honest, all of them sound appealing. I did cover this at a high level in a previous course, What Is Cloud Computing, but in this course I’m going to expand on each of these and explain in greater detail the benefit from a business perspective. Let’s start with on-demand resourcing.</p>
<p>On-demand resourcing essentially means that when you need to provision a resource within the cloud, it’s almost immediately available to you to allocate where and when you need it. For example, if you had a server in the cloud and its CPU utilization was steadily increasing with demand, you’d be able to deploy a second server and it would be ready within minutes to take off some of the load of the first.</p>
<p>Now if you compare this to how you’re currently sourcing additional resources for your infrastructure, whether it be compute, storage or network, you will likely be going through a lengthy process. From engaging with your supplier, obtaining a quote and then defining and ordering the equipment, you will then have to wait for delivery of the new hardware, arrange for the installation of the equipment into a data center, where it can be cabled and configured before it’s ready to be released into the production environment. This whole process can take weeks depending on the equipment required. Weeks is often not good enough in today’s world. Sometimes days or even hours is too long to wait for additional resources when it’s required, specifically if it’s affecting your customers’ experience with you.</p>
<p>The on-demand resourcing aspect of cloud computing alleviates this issue entirely by providing almost instant access to a resource that you have selected and configured via a series of options.</p>
<p>Scalability, cloud computing offers you the ability to rapidly scale your environment’s resources, whether that’s compute, storage or network, both up and down and in and out depending on your requirements and demands of your applications and services. When scaling up and down, you are altering the power of an instance, perhaps using one with greater CPU power to scale up. When scaling in and out, you are simply adding or removing the number of instances you’re using.</p>
<p>It’s only because of the on-demand resourcing of the cloud that this scalability is possible. Imagine trying to quickly add additional servers and bring them online within your environment after a sudden surge of demand. Not only would this have huge financial implications impacting your budgets to cater for these potential peaks in additional assets, but it would also take valuable footprint space in your data center, which is also very expensive.</p>
<p>To give you an example of some of the scaling possibilities within the cloud, I want to review a couple of case studies. Firstly, Philips Healthcare, which focuses on consumer lifestyle, was able to scale out their infrastructure to analyze over 15 petabytes of patient data from over 390 million sources, such as medical records, imaging studies, and patient input. This data was then used to benefit the care of the patient. The cloud provider was able to reliably perform the operations required and scale to meet their needs at a growth rate of one petabyte a month. For more information on this case study, please use the following link.</p>
<p>Secondly, Airbnb managed to migrate and scale out their entire fleet of databases to AWS RDS, which is their Relational Database Service. They achieved all of this with only 15 minutes of service interruption. This was key to Airbnb as it didn’t want to impact their community users for a long period of time. For more information on this case study, please use the link below.</p>
<p>Economy of scale. Due to the huge scale of resources public cloud offerings provide, which are optimizing share between different tenants, you the end user benefit from exceptionally low compute, storage and network costs compared to traditional hosting.</p>
<p>As you will likely be aware, often when purchasing products, the more you buy, the cheaper it becomes. Public cloud providers can make use of this when buying compute, storage, and network infrastructure in such huge scales, and this allows them to pass on the savings to you. Purchasing at this capacity simply isn’t possible for the vast majority of organizations. This makes for very cheap resources within the cloud when compared to managing and hosting that same infrastructure on-premise.</p>
<p>Flexibility and elasticity. Cloud computing offers huge flexibility and elasticity to your design approach. You can choose to have as many or as few resources as you require without having to guess your capacity up front.</p>
<p>When your business experiences peaks and troughs of traffic for usage of your customer base, then architecting for these surges on-premises can be tricky to plan for as well as being potentially expensive. When doing so, you can base your resourcing provisions on usage history over the months or even years. But there were times when there will be inconsistencies. It’s these times that could see your customers suffering due to lack of resources to support their needs. This can cause a negative impact to your business reputation, something no one aspires to have. Consumers now expect to access what they need with an almost instant response. Having failing and slow response times could easily see them taking their business elsewhere. If you provide goods via a website, then this is especially true.</p>
<p>The sheer flexibility and elasticity of the cloud allows you to deploy different resources and services within minutes or seconds, and gives you the possibility to have as much or as little resource as and when needed. This is often coupled with automation, which can be built into cloud services, allowing your infrastructure to stretch and shrink elastically based on custom thresholds, ensuring your infrastructure will always deliver great service to your customers.</p>
<p>I have another case study whereby Unilever, who sell consumer goods such as food, homecare, and personal care products, had two goals to be reached by their cloud migration. Firstly, to deliver a common technology platform for the web content, and secondly, to migrate all their existing web properties to the cloud. This was achieved by utilizing over 400 instances using the flexibility of a VPC, virtual private cloud, in an auto scaling configuration to handle traffic loads. For more information on this case study, please see the link below.</p>
<p>Growth, cloud computing offers your organization the ability to grow using a wide range of resources and services. Couple this with the on-demand element, and your growth constraints are significantly reduced compared to a classic environment.</p>
<p>To enable your business to grow quickly and keep up with customer demand, it can be a struggle for some organizations that lucky enough to have that success, especially when providing services from an on-premise solution. You may need a larger building to house your infrastructure. This could take many months to acquire before being in a position to allow you to extend your network. If you are a startup company who has just happened to launch the next game that’s taking the mobile gaming market by storm, then it’s unfeasible to host and scale your infrastructure out to cope with the demand.</p>
<p>The growth that the cloud offers many businesses is almost limitless. With its rapid deployments the cloud offers the opportunity for your business to grow at exceptional rates.</p>
<p>Utility based metering. With many cloud services, you only pay for what you use. What do I mean by this? If you only have one server running for two hours and then shut it down, then you only pay for two hours’ worth of compute resources and that’s it. Think of it like this. In your house you only pay for electricity when you use it, and to help keep costs down, you turn off the lights when you are not using them. So it’s the same bit in process. You only pay for resources when you are using them.</p>
<p>Typically within a data center, your infrastructure is up and running 24&#x2F;7&#x2F;365, always running, always on, but not always utilized. The power on coiling costs alone for this infrastructure can be phenomenal over the year, especially when you’re talking hundreds or even thousands of servers. Wouldn’t it be great if you could just go along and switch off the servers when you’re not using them? Well, in essence you could, but it rarely occurs, and even if you did, you’d still be paying for the footprint space that those servers took up.</p>
<p>The cloud offers you the ability to shut down any instance that isn’t in use. At that point you instantly stop paying for that resource. This could even be scripted. So for example, at 6:00 p.m. every day, all test and development servers are switched off, and then turned back on again at 8:00 a.m. the next morning.</p>
<p>Shared infrastructure. Hosts within the cloud are virtualized. As a result multiple tenants can be running instances on the same piece of hardware, which significantly reduces the amount of physical hardware required, which in turn reduces the amount of power, coiling and space required in the data center. This all helps with economy of scale, all resulting in cheaper costs for you.</p>
<p>With some cloud providers you can in fact have a dedicated instance whereby no other customer can run instances on the same hardware that your instance is running on. Or for greater management and control, you can even have a dedicated host, which also prevents any other customer running instances on that same host. The difference between a dedicated instance and a dedicated host is that a dedicated host gives you far greater control and visibility, allowing you to decide which instances to place on that same host over the time period. Dedicated hosts also give you the visibility of sockets and cores which you can use to manage licensing on your own server-bound software, which is licensed per socket or per core. Dedicated instances do not give this control of the host, and they are automatically deployed on a host of the cloud provider’s choice.</p>
<p>The greater control and management reduces the overall optimization of that host, and as a result the cloud provider would compensate for this by charging you an increased price compared to that of the same instance type on a shared host.</p>
<p>So unless there are strict governance or security controls requiring you to have a dedicated host, or you have server license issues whereby you need to be able to see the sockets and cores of the server, then I would suggest using the shared tenancy option. By doing so, you would benefit from a greater cost savings across your overall cloud operating costs.</p>
<p>Highly available. By design many of the core services within the public cloud and its underlying infrastructure are replicated across different geographic zones and regions.</p>
<p>This alone can be huge to many businesses. To have offsite replication built into some services offers a significant advantage over on-premise business continuity solutions. Many organizations do not have the luxury of having multiple sites to replicate their data, or they are often too small to operate a wide-scale disaster recovery program. To have the availability and resiliency of the cloud helps to ensure the durability and availability of your data often without additional configuration dependent on the service type. It’s all provided by the vendor as a part of their service.</p>
<p>This offers great comfort to many businesses, but it’s important to understand at which point the resiliency is managed by the vendor and when it’s managed by you, the end user. You do not want to become too complacent and be caught out when disaster does strike. Understanding where the responsibility of the vendor stops from a resiliency perspective and where your responsibility starts varies from vendor-to-vendor and from service-to-service. Ensure you understand which services are managed by your chosen vendor as these managed services that often offer built-in management and resiliency which prevents you from having to architect such means. It’s one thing to add resources to the cloud, but architecting them in the resilient and highly available configuration is another.</p>
<p>Security, this is one of the most discussed topics within cloud computing and many enterprises still have concerns over how secure it is. However, public cloud vendors such as AWS, Amazon Web Services, and Microsoft Azure, are considered to be more secure than your own data center.</p>
<p>Public cloud vendors have to operate their security at an extremely high standard. They must adhere to global governance and compliances covering all industries and sectors. They are global services and as such must meet certifications from all over the world such as PCI DSS, ISO, HIPAA, and SOX. Trying to meet these standards in your local data center would be a struggle. Security is the number one topic that cloud vendors pour huge amounts of capital into to develop their security infrastructure further, advancing with technologies and developing new services to help the end users with securing their data. As a result the same level of security that large financial corporations require would be applied to an individual user who may be using the cloud to store their college documents. Security is a global service and meets global standards. To attain the level of security certification controlled and governance these vendors have within your own data center wouldn’t really be feasible and to a degree, unnecessary, as some compliances may not be applicable to your industry. However, utilizing their infrastructure covers you in their umbrella of security.</p>
<p>Public cloud vendors provide exceptional security for the underlying infrastructure of the cloud. It’s down to you and us, the end users, to then architect security in the cloud using the tools and services and applications available. As I previously mentioned, there are a number of ways we can secure our infrastructure, and it’s wise to get a security specialist to manage this aspect for you to apply security at every level of your deployments.</p>
<p>That brings us to the end of this lecture, and I hope it’s provided an overview of how cloud technology can be used to change the way you manage your current workloads, and the benefits that the cloud can offer many organizations. Next we’ll be looking at the benefits that the cloud can bring to your business.</p>
<h1 id="Cloud-Business-Benefits"><a href="#Cloud-Business-Benefits" class="headerlink" title="Cloud Business Benefits"></a>Cloud Business Benefits</h1><p>Hello, and welcome to this lecture. So we’ve covered some of the characteristics that the Cloud offers and reflected on how these can help you as an organization, should you decide to migrate. As you can see, there are some very compelling reasons to do so, so besides these positive points, you may ask yourself, okay, so the Cloud can offer advantages against my on-premise architecture, but my on-premise solution is working okay, so what else can the Cloud bring me as a business? Well the answer to that, other than the main financial aspect, is quite a lot. Let’s take a look at a few of those advantages.</p>
<p>The Cloud brings a new wave of life to your business and its architecture. You are presented with a whole new way of working, new ways of deployment, tools and services to help with automation, and self-healing of your infrastructure. It can be a little overwhelming when looking at how differently infrastructure operates within the Cloud. However, it’s very important to know how it operates, and how the Cloud, as a technology, is constantly evolving and developing.</p>
<p>When a technology like this exists, it’s potential and development can grow at an exponential rate. New services are being released that offer new ways of architecting your infrastructure all the time. This offers a massive potential within your business to innovate and take advantage of these changes. Your own on-premise solutions, likely remain the same month to month and year to year, with regards to technologies and deployment options. As a result, trying to innovate on top of this outdated infrastructure, would be hard and sometimes laborious to complete. It would take additional time, resulting in slow releases, which ultimately end in slower benefits offered to your customers. This whole process can be a drain on your budget.</p>
<p>The Cloud offers you the chance to work with some of the latest architecture and services, which wouldn’t always be as easy to acquire on-premise. Making use of new technologies, such as serverless computing, can be a game changer for many organizations. Imagine not having to configure, setup and manage any servers for your developed applications to run on. Imagine the only focus you needed for deploying your applications was the code, and that was it. Once you’d written your code, you can send it out to a service, and that service would provision all your infrastructure required to run that code for you. Deployment options such as this, especially for start ups, just wouldn’t be feasible.</p>
<p>You can be brave and experiment with new ideas and products at a fraction of the cost it would be, compared to an on-premise solution.</p>
<p>If you are operating within a business where you are bringing new products and applications or services to market, then the Cloud offers a great advantage in helping you to complete this over a standard, on-premise solution. You’re able to utilize all the resources you require in getting your products released. Having the desired resource you need when you need it, to scale up and speed the process up, can help you bring launch dates from months to weeks and to days. The Cloud is especially effective during your development and testing phase. Being able to do this and bring your product quicker to your customer, allows your organization to develop the next range of new products and releases even faster.</p>
<p>A fine example of bringing products to market quickly and effectively, is that of Netflix. Netflix is an online content provider of TV and films. Using the benefits of the Cloud, they’re are able to quickly deploy tens of thousands of servers, with terabytes of storage, within minutes, which serve over 1,000,000,000 hours worth of content per month. For more information on this case study, please use the following link.</p>
<p>Within our current climate, governments globally are putting additional pressure on organizations to reduce their carbon footprint and become more green. In this fast paced world that we live in, businesses are struggling to meet these thresholds set out for climate change. And one of the largest culprits of carbon emissions is that of the power and energy used within our large data centers. Utilizing the Cloud to migrate some of your services can help offset some of this burden and lead to your organization becoming more green, which is a huge plus point for your business as a differentiator in leading change for the better.</p>
<p>Now you may be thinking, what’s the difference between having the infrastructure at my data center or in a Cloud provider’s data center? Well for one, all infrastructure is virtualized, as we know, and so it’s greatly optimized, whereas within your data center, this may not be the case. Also their power and cooling that’s providing, will likely be using far more energy effective mechanisms than that of the systems many of us utilize today. Cloud vendors will have huge data centers that would have had huge amounts of investment put into them. The latest efficiency and cooling technology would have been implemented, and probably in many instances, free air cooling may have been used, whereby outside air is brought into the data center through various means and filters to cool the infrastructure. It’s this kind of technology and implementations within their data centers, that help reduce the carbon footprint for everyone else using their infrastructure. Many of us are likely using outdated and older technologies when it comes to the mechanical and electrical infrastructure within our data centers, making it hard to compete with the efficiencies and greeness of the new, modern data centers used by the Cloud vendors today.</p>
<p>If architected correctly on the Cloud, your business infrastructure can be accessed from anywhere. This opens opportunity to you as a business, as it allows for a change of process when it comes to support and management of that infrastructure. Your entire Cloud management could be outsourced to another provider, which, depending on your current structure, may significantly reduce your IT operational costs from a salary perspective alone. Alternatively, it opens the door to increase the scope and sourcing talent for your organization. You can source your skilled IT engineers from almost anywhere, if all systems are available via the internet. For example, I live and work in the UK, however, Cloud Academy do not have any offices here. I can access all my work related services via the internet, over Cloud-based systems. As you can see, remote working becomes far more feasible for your employees, which in turn, could result in acquiring smaller offices and facilities, reducing costs even further.</p>
<p>Utilizing the Cloud allows you to reduce many of your business risks. One of your biggest risks is that of data durability, the loss or corruption of your own, or even your customer’s, data. We have already spoken about high availability, and the resiliency that it offers, and this will help with reducing that risk. However, you could even take this one step further, and utilize multiple Cloud vendors. Perhaps both AWS and Azure to hold customer data as an even greater risk mitigation strategy.</p>
<p>Product risks often come when moving from test and development to production. Much of this is down to the fact that it’s not always easy to replicate your test environment to reflect that of your production when on-premise. Largely, this is normally down to not having enough available resources. As a result, tests are often not completed to full requirements of a new product or service. With Cloud deployments, there are services that allow you to have the ability to copy your architecture exactly as it is in production, which can then deploy for testing where it’s built exactly the same. You can have this purely for the period only required for testing, and then remove the infrastructure so you stop paying for it. So now, you’re able to fully test, from end to end, as if it were in a production environment for a small cost, mitigating a lot of the existing risks you would have had otherwise, resulting in a more reliable release for your customers.</p>
<p>Time is something all businesses wish they had more of, deliverables and deadlines are always rushed, and as a result, processes and procedures are always challenged. Cloud adoption changes the way your business will operate. It will become significantly more dynamic, it’s approach to change will often become swift and smooth. In a well designed environment, your organization will end up spending far less time focusing on your IT solutions, and the management of those underlying solutions, as much of the maintenance and operational aspects would be automated and managed by the Cloud vendor. It’s then this time, that would have otherwise been spent on these tasks, and allows you to use it focusing more on the delivery of the core functions of the business, your products and your services.</p>
<p>Having your infrastructure within the Cloud makes it more feasible to have real-time collaborations between your business and other third parties that you utilize to drive forward and sell your services. Opening up access to certain data and services for authorized third party vendors to interrogate and analyze on your behalf, can be of significant benefit. There are many approved partners that have been sanctioned by public Cloud providers that can provide some incredible monitoring and analysis of your data, which can ultimately lead to a change of business strategy, making your organization stronger within the market.</p>
<p>That brings us to the end of this lecture, coming up next, we look at the constraints of Cloud computing and where you may experience restrictions in your migrations.</p>
<h1 id="4Key-Cloud-Concepts"><a href="#4Key-Cloud-Concepts" class="headerlink" title="4Key Cloud Concepts"></a>4<strong>Key Cloud Concepts</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Blog: AWS Shared Responsibility Model: Cloud Security</a></p>
<h1 id="7How-Data-Center-architecture-is-reflected-in-the-Cloud"><a href="#7How-Data-Center-architecture-is-reflected-in-the-Cloud" class="headerlink" title="7How Data Center architecture is reflected in the Cloud"></a>7<strong>How Data Center architecture is reflected in the Cloud</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/compliance/">AWS Security &amp; Compliance</a></p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/trust-center/compliance/compliance-overview">Azure Security &amp; Compliance</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/products/storage/">AWS Storage Services</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/product-categories/storage/">Azure Storage Services</a></p>
<h1 id="8Is-the-Cloud-right-for-you"><a href="#8Is-the-Cloud-right-for-you" class="headerlink" title="8Is the Cloud right for you?"></a>8<strong>Is the Cloud right for you?</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/solutions/case-studies/philips/">Philips Case Study</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/solutions/case-studies/airbnb/">Airbnb Case Study</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/solutions/case-studies/unilever/">Unilever Case Study</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Learning-Path-Introduction-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Learning-Path-Introduction-1/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Learning-Path-Introduction-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:19" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:19-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:00:30" itemprop="dateModified" datetime="2022-11-20T19:00:30-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Learning-Path-Introduction-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Learning-Path-Introduction-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Learning-Path-Introduction"><a href="#Learning-Path-Introduction" class="headerlink" title="Learning Path Introduction"></a>Learning Path Introduction</h1><p>Hello, and welcome to this learning path that has been designed to help you prepare for and pass the AWS Certified Cloud Practitioner Certification. </p>
<p>My name is Danny Jessee, and I am an AWS certification specialist here at Cloud Academy. Feel free to connect with me to ask me any questions using the details shown on the screen. Alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> where one of our cloud experts will reply to your question. </p>
<p>The AWS Certified Cloud Practitioner certification has been designed for those who are not necessarily in a technical role, but do have some exposure to and experience with AWS and the services that it provides. For example, your role might have some involvement regarding AWS architecture from a sales, financial, or managerial perspective.</p>
<p>The certification itself is broken down into four distinct domains:</p>
<ol>
<li>Cloud Concepts,</li>
<li>Security and Compliance,</li>
<li>Technology, and</li>
<li>Billing and Pricing.</li>
</ol>
<p>Each of these domains carry a specific percentage weighting within the exam. Each domain also contains a series of subdomains, which you can see in the official AWS exam guide here. [Landing page: <a target="_blank" rel="noopener" href="https://aws.amazon.com/certification/certified-cloud-practitioner">https://aws.amazon.com/certification/certified-cloud-practitioner</a>, Exam guide: <a target="_blank" rel="noopener" href="https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Exam-Guide.pdf]">https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Exam-Guide.pdf]</a></p>
<p>Let’s start by taking a look at each of these domains to give you a greater understanding of the topics that will be covered on the exam.</p>
<p>Domain 1: Cloud Concepts. This domain accounts for 26% of the exam content and focuses on three key areas: define the AWS cloud and its value proposition, identify aspects of AWS cloud economics, and explain the different cloud architecture design principles.</p>
<p>This domain will test your knowledge of general cloud concepts and principles, such as looking at the benefits of the cloud and what it can bring to your business from both a financial and operational perspective. </p>
<p>Domain 2: Security and Compliance. This domain accounts for 25% of the exam content and focuses on four components of cloud security: define the AWS shared responsibility model, define AWS cloud security and compliance concepts, identify AWS access management capabilities, and identify resources for security support.</p>
<p>This domain will assess your understanding of cloud security, ensuring you understand the boundaries of where specific responsibilities lie between you and AWS. It will also test your knowledge of access control mechanisms to your AWS resources and how you can maintain compliance within your environment, as well as how to utilize security services to enhance the security posture of your environment.</p>
<p>Domain 3: Technology. This domain accounts for 33% of the exam content and like the Security domain, also has four key areas of interest: define methods of deploying and operating in the AWS cloud, define the AWS global infrastructure, identify the core AWS services, and identify resources for technology support.</p>
<p>This domain will test your knowledge of the core components of AWS, its global infrastructure, and its key services across a spectrum of categories such as compute, storage, databases, networking, and security. You must know the differences between specific services and what those services are used for. </p>
<p>Domain 4: Billing and Pricing. This domain accounts for 16% of the exam content and assesses you in three areas: compare and contrast the various pricing models for AWS, recognize the various account structures in relation to AWS billing and pricing, and identify resources available for billing support.</p>
<p>This domain is all about your understanding and awareness of how to track, trace, and optimize your cloud spend with AWS using a variety of tool sets that are available. Knowing the differences between these tools is essential. </p>
<p>Throughout this learning path, you will be guided through a number of courses, hands-on labs, resources, and assessments that cover every element within the domains I just discussed. This will ensure that you have the required knowledge and sufficient understanding to enable you to pass this certification exam.</p>
<p>Feedback on our learning paths here at Cloud Academy is valuable to both us as trainers and any students looking to take the same learning path in the future. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, it would be greatly appreciated if you could email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>That brings me to the end of this introduction. Coming up next, we’ll begin our journey to becoming certified cloud practitioners by answering the question, what is cloud computing?</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Network-Specialty-Cert-Prep-Certified-Advanced-Networking-Specialty-for-AWS-27/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Network-Specialty-Cert-Prep-Certified-Advanced-Networking-Specialty-for-AWS-27/" class="post-title-link" itemprop="url">AWS-Network-Specialty-Cert-Prep-Certified-Advanced-Networking---Specialty-for-AWS-27</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-18 21:08:55 / Modified: 21:08:56" itemprop="dateCreated datePublished" datetime="2022-11-18T21:08:55-04:00">2022-11-18</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Network-Specialty-Cert-Prep-Certified-Advanced-Networking-Specialty-for-AWS-27/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Network-Specialty-Cert-Prep-Certified-Advanced-Networking-Specialty-for-AWS-27/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/54/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/54/">54</a><span class="page-number current">55</span><a class="page-number" href="/page/56/">56</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/56/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
