<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/46/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/46/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Networking-CLF-C01-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Networking-CLF-C01-16/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Networking-CLF-C01-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:41" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:41-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:46" itemprop="dateModified" datetime="2022-11-20T18:58:46-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Networking-CLF-C01-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Networking-CLF-C01-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Networking in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various Networking services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#115;&#117;&#x70;&#112;&#111;&#114;&#116;&#x40;&#x63;&#x6c;&#x6f;&#x75;&#100;&#97;&#x63;&#97;&#x64;&#101;&#x6d;&#121;&#46;&#99;&#111;&#109;">&#115;&#117;&#x70;&#112;&#111;&#114;&#116;&#x40;&#x63;&#x6c;&#x6f;&#x75;&#100;&#97;&#x63;&#97;&#x64;&#101;&#x6d;&#121;&#46;&#99;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Networking services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Networking services in AWS, including:</p>
<ul>
<li>Amazon Virtual Private Cloud, or VPC;</li>
<li>Configuring security groups and Network Access Control Lists, or NACLs;</li>
<li>AWS Virtual Private Network, or VPN solutions; and</li>
<li>AWS Direct Connect.</li>
</ul>
<p>You’ll also learn about global networking services such as Route 53: Amazon’s highly available and scalable Domain Name System, or DNS service; Amazon CloudFront: a high-performance Content Delivery Network, or CDN; and the AWS Global Accelerator, which leverages the AWS global infrastructure to reduce latency and improve the overall performance of your applications.</p>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#x73;&#x75;&#x70;&#112;&#111;&#x72;&#116;&#64;&#x63;&#108;&#111;&#x75;&#100;&#97;&#x63;&#x61;&#100;&#x65;&#109;&#x79;&#46;&#x63;&#x6f;&#x6d;">&#x73;&#x75;&#x70;&#112;&#111;&#x72;&#116;&#64;&#x63;&#108;&#111;&#x75;&#100;&#97;&#x63;&#x61;&#100;&#x65;&#109;&#x79;&#46;&#x63;&#x6f;&#x6d;</a>. Thank you!</p>
<h1 id="What-is-a-VPC"><a href="#What-is-a-VPC" class="headerlink" title="What is a VPC?"></a>What is a VPC?</h1><p>Hello and welcome and I’m going to be talking to you about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>s. Virtual Private Clouds. Now to understand what a VPC is, let’s just take a look at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> infrastructure. </p>
<p>So, here is the AWS Cloud. Very simple. And a VPC resides inside of the AWS Cloud and it’s essentially your own isolated segment of the AWS Cloud itself, so here is your VPC sitting inside the AWS Cloud. </p>
<p>Now by default when you create your VPC, the only person that has access to this is your own AWS account, just you. It is totally isolated and no one else can gain access to your VPC other than your own AWS account. Now obviously there are millions upon millions of other VPCs within the AWS network created by other customers all across the world. So, there are millions of customer VPCs. However, they do not have access to your VPC and likewise, you do not have access to their VPC. </p>
<p>Now what do you use a VPC for? Well, essentially it allows you to start deploying resources within your VPC, for example, different compute resources or storage or database and other network infrastructure among others and this allows you to start building and deploying your solutions within the Cloud. </p>
<p>Now by default from a limitation perspective, you are allowed up to five VPCs per region per AWS account and it’s very simple to create a VPC. All you need to do is to give it a name, when you create your VPC and also define an IP address range that the VPC can use and this is done in the form of a CIDR block which stands for Classless Inter-Domain Routing. And I’ll talk more about that when I talk more about subnets in a few minutes. </p>
<p>So, just to recap at a high level, simply put, a VPC is an isolated segment of the AWS public cloud that allows you to provision and deploy resources in a safe and secure manner. I now want to dive deeper into the VPC architecture and start talking about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnets</a> and how you can segment your VPC out into different areas across multiple availability zones for resiliency and high availability, so let’s take a look.</p>
<h1 id="Subnets"><a href="#Subnets" class="headerlink" title="Subnets"></a>Subnets</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/vpc-cidr-blocks/">VPC TCP&#x2F;IP Addressing</a></p>
<p><strong>Transcript</strong></p>
<p>So <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-what-vpc/">now we know what a VPC is</a>, let’s take a look at subnets. Now, subnets reside inside your VPC, and they allow you to segment your VPC infrastructure into multiple different networks. Now, you might want to do this to create better management for your resources, or to isolate certain resources from others, or even to create high-availability and resiliency within your infrastructure. So let’s take a look at the subnets. </p>
<p>Firstly, then we’ll just draw our VPC quickly. So this is our VPC. And I mentioned when talking about VPCs that when you create your VPC, there’s two pieces of information that you need. You need to give it a name and also a CIDR block address. Now, the CIDR block address is a range of IP addresses and this CIDR block range can have a subnet mask between a range of IP addresses from a &#x2F;16 all the way through to a &#x2F;28. Now, if you’re not familiar with TCP&#x2F;IP addressing, now please take a look at the link on screen and check out the following course and this will dive into the CIDR block and TCP&#x2F;IP addressing in greater detail. </p>
<p>Now, for our example, let’s say we created our VPC with the following CIDR block. 10.0.0.0&#x2F;16. Now, this is important because any subnets that we create within our VPC need to reside within this CIDR block range. So let’s take a look at a couple of subnets. </p>
<p>Now, in this section, I want to talk to you about public subnets and also private subnets. So let’s just create a public subnet there and also a private subnet here. This yellow one can be our public subnet and the green one can be our private subnet. Now, similarly, when we create a VPC, we need to give it a CIDR block range. We need to do the same with our subnets as well. So let’s say for example this is 10.0.1.0&#x2F;24. Now, this range of addresses sits within this bigger CIDR block here, and then this private subnet can be 10.0.2.0&#x2F;24. And again, this CIDR block sits within the bigger VPC CIDR block. </p>
<p>Now, what makes a subnet public and what makes a subnet private? Well, essentially a public subnet is accessible from outside of your VPC. So essentially from the Internet. For any resources created within your public subnet, for example web servers, would be accessible from the Internet. Now, because we want these web service accessible from the Internet, I have two IP addresses. So they have their own internal IP address which will be within the range of the subnet, which, for this subnet, it’s 10.0.1.0&#x2F;24. And then also we’re going to assign them a public IP address as well, because to be accessible from the Internet, the instance itself has to have a public IP address. </p>
<p>Any resources created within your private subnet, for example your backend databases, would be considered private and inaccessible by default from the Internet. So how do you make a subnet public and how do you make one private? When you create a subnet, you create them both exactly the same. It’s what you configure afterwards that will dictate if a subnet is public or private. </p>
<p>There’s two changes you need to make to your infrastructure to make a subnet public. The first is to add an Internet gateway. Now, an Internet gateway is a managed component by AWS that is attached to your VPC and acts as a gateway between your VPC and the outside world. So essentially the Internet. So let’s just add in an Internet gateway here, IGW for Internet gateway. So now we have our Internet gateway attached to our VPC. And this Internet gateway then also connects out to the Internet. So we now have a bridge between our isolated VPC to the Internet by the Internet gateway which is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. </p>
<p>Now, you might think that a public subnet now has access to the Internet because there’s an Internet gateway. However, before the public subnet can access the Internet, we need to add a route to the public subnet’s route table. Now, associated with every subnet when it’s created will also be an associated route table. Now, you can have the same route table associated to multiple subnets. That’s not a problem. However, you can’t associate more than one route table to a single subnet. </p>
<p>Now, by default, when your subnet’s created, it will have a default route in it, and this is a local route. Let’s take a look. Now, your route table will contain a destination field and also a target field. Now, the destination field is the destination address that you’re trying to get to. The target essentially specifies the route to that destination. Now, within every route table that’s created, there will be this local route here. Now, what this enables your subnets to do is simply talk to each other. So any subnet within your VPC is able to communicate with each other without you having to configure any routes. It’s there by default. Every route table has this local route. It can’t be deleted, and it simply allows all subnets within your VPC to communicate with each other. </p>
<p>So what we need to do is we need to add a route to this route table that’s associated to the public subnet. Now, this new route here that’s been added to the route table has a destination of 0.0.0.0&#x2F;0. Now, that essentially means that for any IP address that’s not known within the route table already, send it to this target. Now, this target is the Internet gateway as shown by the IGW. This part here is simply the ID of the Internet gateway. So by adding this route to the route table, this public subnet now knows exactly how to get to the Internet by going by the Internet gateway as shown in the route table. Now, those two components are essentially what makes a subnet public, the fact that we have an Internet gateway attached to the VPC and this subnet has a route pointing to the Internet gateway for any traffic that it doesn’t know how to get to. </p>
<p>Now, if we compare the route table of the private subnet, we can see that it only has this local route, so it has no route to the Internet gateway. It’s not aware that an Internet gateway even exists, so it has no route out to the public Internet. So this is considered a private subnet. Now, if we go back to the public subnet, before we added this route here, so let’s just take that out. This subnet is effectively a private subnet, because it doesn’t have a route to the Internet gateway. So with that in mind, every time you create a subnet, it is a private subnet to begin with and that is until you attach an Internet gateway to your VPC and then add this additional route. </p>
<p>So now we’ve looked at public subnets and also private subnets. Let’s now look at architecting multiple subnets across your VPC for high availability and resilience. So let’s just clear the screen, give us a blank VPC to work with. So let’s consider we have three subnets this time. We’ll have a public subnet, and we’ll have two private subnets. This can be our public subnet, and these two will be our private subnets. This will be our web layer. This will be our application layer, and this will be our database layer. </p>
<p>So in our public subnet, we will have some web servers. In our application layer, we’ll just have some EC2 instances. And in our database layer, we’ll just have some databases. So there we have our three tiers of our deployment. So as we know with any subnet, we have a local route, so each of these will all have a local, as you can’t remove that local route. And this enables all of these subnets to communicate with each other. The public subnet, as we know, will also have a route to the Internet gateway. Now, when you create a subnet, you have to create it in one of the availability zones that are available within that region. Now, if you’re not too familiar with the AWS global infrastructure, then please take a quick read of the blog post below. </p>
<p>Let’s say for example when we created this subnet, we created it in availability zone one, and we’ve done the same for the remainder of our subnets as well, and placed them all in the same availability zone. And that’s all okay, we can deploy infrastructure all within the same availability zone and our solution would be operating fine. However, should AWS have an issue with availability zone within this region, for example they might experience a flood or a fire or a natural disaster, and it took out the services to availability zone one, what would happen to our resources? </p>
<p>Well, effectively, these would also be taken down because they’re all running in availability zone one. So that’s not ideal. It’s not best practice to deploy all of your resources within the same availability zone, within a single region, simply because it doesn’t offer high-availability and resilience. So what should you do in that situation? Well, the best thing to do to ensure high-availability is to add additional subnets to allow for resiliency. So we’ll add an additional web tier and also additional application, and also a database. </p>
<p>So now we have six subnets, and again, we’d replicate our resources, so it’d have our web infrastructure here, we’ll have our application service here and our databases here. Again, then we’ll have the routes and then Internet gateway route as well. This will have a local route, and as we know, this allows communication between all subnets. So now, every subnet can talk to every other subnet with a local route, and also this also has a route to the Internet gateway as well. So now let’s look at the availability zones that we’ll deploy our infrastructure in this time. </p>
<p>Let’s say for example we’ll deploy this in AZ-1, this application subnet in AZ-2, and this database subnet in AZ-3, and similarly, down here, we have this public subnet in AZ-3. This one in one, and this final one in two. So now let’s run through the scenario again. Let’s imagine AZ-1 experienced a failure. So what would happen here? This public subnet would be out of action. This application subnet and that is it. So in this situation, we still have one subnet available in each layer of our infrastructure. So should we experience a failure with availability zone one, our services will remain up and running. </p>
<p>So let’s do the same with availability zone two. What would happen in this situation? Well, both of our public subnets would be okay, because everyone’s in AZ-1 and three. This application subnet would be down, and this database subnet would also be down. So again, at least one subnet in each of our layers is operational and available. So again, our services would still be up and running. </p>
<p>Now, finally, just for clarity, if we take down availability zone three, this web layer would go and also this database layer. So again, we still have at least one subnet in each tier or each layer of our infrastructure operational. So this is a much better design. This allows you to ensure your resources stay up and running should a failure occur in one of the availability zones. Before we move onto some security features, let me just clear the screen because I want to talk to you just quickly about IP addressing, just a couple of points that I want to mention with regards to the subnets. So let’s just clear this quickly. </p>
<p>So I mentioned that when you create your subnet, you have to assign it a CIDR block range that fits within the VPC CIDR block. So say for example we created a subnet here, and we give the subnet the address of 10.0.1.0&#x2F;24. Now, with a slash 24 mask, this gives this subnet a total of 256 IP addresses. You can only actually use 251 IP addresses. And I’ll explain why. So the very first IP address in this subnet is 10.0.1.0 and this is known as the network address. Now, you’re not able to use this as an IP address to assign to your host addresses. This is reserved for networking. Now, the next available IP address after the network address is 10.0.1.1. And this is reserved for AWS routing. So again, not a network address. You can’t use this address as a host network in your subnet. Now, the next available IP address is 10.0.1.2. And again, this one is reserved by AWS, but this time for DNS. So you cannot use this IP address. Now, the fourth IP address that you won’t be able to use in this subnet is 10.0.1.3. And this is actually reserved by AWS for future use. Now, the fifth and final address that you can’t use in an AWS subnet is the last available address in the subnet. So in this case, it’d be 10.0.1.255, and the last address in any subnet is known as the broadcast address, and again, you cannot use this for host resources. </p>
<p>So when working with TCP&#x2F;IP addresses within your subnet, first four addresses in any subnet are reserved and you cannot use for host addresses and also the very last address is reserved. So that’s why use only 251 IP addresses available to you that you can use to assign to your host resources. So now we’ve covered what a VPC is. We’ve looked at subnets, both public and private, and also how it’s best to architect your subnets across multiple availability zones for high-availability. So now let’s look at some security features. I want to start with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-network-access-control-lists-nacls/">network access control lis</a>t. Let’s take a look.</p>
<h1 id="Network-Access-Control-Lists-NACLs"><a href="#Network-Access-Control-Lists-NACLs" class="headerlink" title="Network Access Control Lists (NACLs)"></a>Network Access Control Lists (NACLs)</h1><p>Security is a key part of any deployment within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, and managing security around your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">virtual private cloud</a> is no different. So I want to talk to you about a couple of different components here. </p>
<p>Firstly, I want to talk to you about NACLs which are network access control lists. Now these are essentially virtual network-level firewalls that are associated to each and every <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnet</a>, and they help to control both ingress and egress traffic moving in and out of your VPC and between your subnets. So let’s just quickly draw out our VPC. Very simple, and let’s just draw in a public subnet for example. So this is going to be public. Up here, we’ll have our internet gateway attached to our VPC, and obviously, we have a route out to the gateway, which then communicates with the internet. </p>
<p>So what we can do here to help maintain security is to configure the network access control list associated to this subnet. Now much like route tables, whenever you create a new subnet, it will also associate a network access control list. Now by default, this NACL will allow all traffic, both inbound and outbound, so it’s not very secure, so it’s a really good practice to configure your NACLs to only allow the traffic that you want to come in and out of your subnet. </p>
<p>Now with this being a public subnet, we’ll probably have some web servers in here talking over HTTP and HTTPS, so let’s look at the inbound network access control list that could be associated to this subnet. Now as you can see, there’s a number of different fields. We have the rule number, the type, the protocol, port range, source, and allow or deny. Now the rule numbers allow you to specify what order the rules will appear inside the NACL, and as soon as traffic hits one of these rule where it matches all of the type, protocol, port range, and source, et cetera, it will carry out the action at the end, whether that is allow or deny. </p>
<p>So let’s look at the requirements required to match this rule here. The type of traffic will need to be HTTP under port 80 using the TCP protocol, and again, the port range is 80 as that’s what used for HTTP. Now the source can be any IP address, so any IP address running HTTP coming into our subnet will be allowed. So as long as they’re running this protocol, then the traffic will be allowed inbound into our public subnet. </p>
<p>Now let’s look at the second rule. Now the second rule uses HTTPS using the TCP protocol using port 443, and again, any source, and the action will be allowed. Now the last rule here, now this is a default rule that’s applied at the end of every network access control list, and that’s why it doesn’t have a rule number, and it states that all traffic using any protocol in any port range from any IP address, then deny that access. So this rule is kind of a cover rule. So basically, what that allows you to do is ensure that any traffic that doesn’t meet the rules that you’ve entered is deleted and denied access to your subnet. </p>
<p>So with this in mind, the only traffic allowed in our public subnet is essentially HTTP and HTTPS, which is exactly what we want for our web servers here, and all other traffic will be denied. So that’s the inbound NACL. Let’s now take a look at the outbound. Now the field types are all exactly the same other than this one here. This has a destination whereas on the inbound, it has the source. So on the outbound, we restrict traffic against its destination. </p>
<p>So the first rule we have here says any traffic using any protocol in any port range going to any destination, then allow that traffic. Anything else should be denied, but in this case, there won’t be anything else because this outbound rule is essentially saying send any traffic you want to using any protocol out from this subnet to any destination. </p>
<p>Now an important point to make about NACLs is that they are stateless, and this means that any response traffic generated from a request will have to be explicitly allowed and configured in either the inbound or the outbound ruleset, depending on where the response is coming from. Now again, much like route tables, you can have the same NACL applied to a number of subnets, but only a single NACL can be associated to one subnet. So network access control lists are a great way to control traffic that comes into and out of a particular subnet. </p>
<p>Let me now talk about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-security-groups/">security groups</a>, and these are another method of controlling traffic, but this time, they work at the instance level rather than the network level like NACLs do.</p>
<h1 id="Security-Groups"><a href="#Security-Groups" class="headerlink" title="Security Groups"></a>Security Groups</h1><p>So, staying with security, I now want to talk to you about security groups. Now these are similar to network access controllers where they filter traffic both inbound and outbound but whereas <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-network-access-control-lists-nacls/">NACLs</a> worked at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-subnets/">subnet</a> level, security groups work at the instance level and I’ll explain more about this as we go. </p>
<p>So let’s say we have three subnets, okay, so we just draw these out quickly and these would be three private subnets, for example. Each of them will have their IP addresses listed, first one being 10.0.1.0&#x2F;24. .2.0. And the last one, 3.0. Now this first subnet will have EC2 instances in them. The second subnet will have RDS instances running MySQL or Aurora and the last subnet here will also have EC2 instances in them. </p>
<p>Now each of these three subnets are associated to the same network access control list. So, this one is linked with this one and also this one. And this network access control list looks like this. And this is simply saying that any traffic that is running a TCP Protocol across any port range from any source, then allow it and deny all other traffic. So, between these subnets, any TCP Protocol on any port can be used and for simplicity, the same NACL rules are being used for both inbound and outbound. Now that’s not very secure, it’s not very restrictive but from a subnet network level, that is what it’s controlling. Now what we want to do is restrict access to which instances can actually talk to our RDS and Aurora databases here. </p>
<p>Now we only want to allow access from this subnet over here and deny access from this subnet here and we can use security groups to do just that. So, let’s take a look at the security group for this subnet here, from where our databases are. Now security groups have similar fields to NACLs but there’s just a couple less. So, there’s no rule number with the security group which means all the rules within the security group will be assessed before a decision is made on the action and you’ll also notice, there’s no allow or deny either. With security groups, if there’s a rule in there, then it’s considered allowed, if there’s no rule, then all traffic is dropped by default, so with this security group is stating that any MySQL or Aurora traffic using a TCP Protocol on the port 3306 from the source 10.0.1.0 which is this subnet here, then it’s considered allowed as we don’t have another rule in this security group for the source of 10.0.3.0&#x2F;24 which is this subnet here. Then it’s considered denied. It doesn’t exist, so it’s not allowed access, so how do both these NACLs and security groups work together? </p>
<p>Well, the NACL works at the subnet level, so let’s say the NACL is this purple line and as this NACL is associated to this subnet as an example, let’s just put that NACL around the edge of the subnet like so and let’s say this orange is our security group and that security group is associated to our databases inside this subnet. So, let’s assume that our EC2 instances here are looking to communicate with the RDS and Aurora databases over here, so let’s have a look how that traffic would flow through the NACL and also the security group. </p>
<p>So, the request would be sent, it would get to the NACL and the NACL say okay, is this traffic TCP traffic within this port range from any source? And it is. So, the traffic is allowed. So, that traffic is now allowed inside the subnet. It then hits the security group and the security group says is this a MySQL or Aurora traffic running the TCP Protocol using port range 3306 coming from 10.0.1.0? And it is as we’re trying to communicate with the databases, then access is allowed. Now if we look from this subnet here, the 10.0.3.0 and do the same thing where these two EC2 instances are trying to communicate with the RDS and Aurora instances using port 3306, let’s follow the same process. </p>
<p>So, the request is sent, it hits the NACL, the NACL says are you running TCP within this port range from any source? The answer is yes, so access is allowed. It then hits a security group and it says is this traffic MySQL or Aurora using TCP Protocol on port range 3306? At this point, everything is correct, yes. However, the source is different. We don’t have a source address of 10.0.3.0. It doesn’t exist in the security group. So, at this point, the traffic is dropped at the security group and access is not allowed. </p>
<p>So, you can see how NACLs and security groups can be used to filter traffic at different layers. The NACLs are used for the subnet and network layer and the security groups are used at the instance layer. Now one final thing I wanna say about security groups is that unlike NACLs which are stateless by design, security groups are stateful which means you don’t have to configure specific rules to allow return traffic from requests like you have to do with NACLs.</p>
<h1 id="NAT-Gateway"><a href="#NAT-Gateway" class="headerlink" title="NAT Gateway"></a>NAT Gateway</h1><p><strong>Resources referneced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">AWS Shared Responsibility Model</a></p>
<p><strong>Transcript</strong></p>
<p>I now want to talk to you about another VPC component, and that is the NAT gateway. To help explain what this does, let me just draw out our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a> quickly. So we have a very simple VPC, and we’re gonna have two subnets in this VPC, we’ll have a public subnet and also we’ll have a private subnet as well, and it’s the private subnet that we’re going to be focusing on. </p>
<p>So this will be our public, and the green one will be our private subnet. Now obviously we’ll have an Internet gateway attached to our VPC, which will then connect out to the Internet. Okay, so we have a public subnet, and a private subnet. Now in our private subnet we’ll have a number of EC2 instances running our applications, and in our public subnet we’re likely to have a number of web servers as well. As we know, each of these subnets also have a route table attached. Public route table will have access to the Internet gateway, and also to the other private subnet. </p>
<p>Now we need to start thinking about security again. Now, looking at our EC2 instances in the private subnet, we are responsible, as a part of the AWS Shared Responsibility Model, to update and patch the operating systems running on each of our EC2 instances. Now if you’re not familiar with the AWS Shared Responsibility Model, I suggest you take a look at it. It’s critical to all of your AWS deployments, and it essentially defines the boundaries of security as to what your roles and responsibilities are of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-security-groups/">implementing security within the cloud</a>, and what AWS’s responsibility is of maintaining security of the cloud. For more information, you can take a look at this blog post here. </p>
<p>Okay, so with that in mind, if we have the responsibility of maintaining the operating systems of our EC2 instances, then we need to be able to download updates as and when we need to. However, this subnet is private. Meaning it has no access to the Internet gateway, and therefore the Internet, so how can we download those updates? Well, what we can do, we can add a NAT gateway. </p>
<p>Now, a NAT gateway sits within the public subnet. Because it sits within the public subnet, it has to have a public IP address in the form of an EIP which is an Elastic IP address, and this is assigned to the instance itself. Now because it sits within the public subnet, it has a route out to the Internet gateway, and to the Internet. Now once we have our NAT gateway set up and configured, we need to update the route table of our private subnet. Now, by default our route table in our private subnet will just have the local route that all route tables have. But if we update that to provide a route to the NAT gateway, and we can see that I’ve added this additional route in here. Now this looks very familiar to the route we added to the public subnet to get access to the Internet via the Internet gateway, and it is essentially the same. So we’ll add the 0.0.0.0&#x2F;0 which is essentially a destination to any IP address unknown in the route table already. Then, send it to the target of the NAT gateway. And they can tell it’s a NAT gateway as this first part here, is prefixed with nat. And then this section along here, is essentially the ID of the NAT gateway within your VPC. </p>
<p>So what this route table is telling us, is that if any resource within this subnet needs to gain access to the Internet to perform an update, then it can do so via our NAT over here. This NAT gateway will then take the request, go via the Internet gateway, and download the appropriate software that’s required, and send it back to the EC2 instance requesting it. Now the important thing with a NAT gateway, is that it will not accept any inbound communication initiated from the Internet. It will only accept outbound communications originating from within your VPC. So it will deny all inbound traffic that’s been initiated from the Internet. </p>
<p>Now the NAT gateway itself is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, so you don’t have to provision the instance itself. It’s very easy to do, you simply create the NAT gateway, specify what subnet it should reside in, and associate an Elastic IP address, and AWS will manage all other configuration. Because it’s managed by default, AWS will set up multiple NAT gateways for resiliency, but you’ll only see the one NAT gateway within your account with the associated ID. </p>
<p>Now, earlier I mentioned about configuring your resources across Multi-Availability Zones. So if you have multiple public subnets in different Availability Zones, you will need to set up another NAT gateway within that subnet as well. AWS will not automatically deploy a NAT gateway within each of your public subnets. </p>
<p>So just as a quick summary, a NAT gateway allows instances within a private subnet access to the Internet, but the NAT gateway itself will block all incoming initiations from the Internet. So it protects the private subnet in that way. And this allows you to ensure that you maintain the security of your EC2 instances ensuring that their OS is kept up to date, and any patch management is taken care of as well. Now the next component I want to talk to you about is the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-bastion-hosts/">bastion host</a>. So let’s take a look.</p>
<h1 id="Bastion-Hosts"><a href="#Bastion-Hosts" class="headerlink" title="Bastion Hosts"></a>Bastion Hosts</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/security/securely-connect-to-linux-instances-running-in-a-private-amazon-vpc/">SSH Agent Forwarding</a></p>
<p><strong>Transcript</strong></p>
<p>In this section, I want to talk to you about bastion hosts. Now, consider a scenario where you might have EC2 instances sitting in a private subnet, but you want to be able to gain access to those instances from maybe your home office or from somewhere else on the internet. But because they’re sitting in the private subnet, how can you do that? Well, one of the ways you can do this is via a bastion host. </p>
<p>So let’s draw out our VPC configuration to allow me to explain how this works. So here, we have our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>. We’re going to have a public subnet and we’ll also have a private subnet as well. So this will be our public and this green one here will be our private. Now, obviously, we have an internet gateway attached as we have a public subnet, our IGW, and this connects out to the internet. We also have routes added to allow the subnets to talk to each other and also the public subnet to route out to the internet gateway as well. </p>
<p>Now, also, in the outside world, we have an engineer. Now, this engineer might be sitting at home in their home office in front of their laptop and what they need to do is to connect to resources sitting within the private subnet over here. Now, in this private subnet, we’re going to have a couple of EC2 instances. Now, we know it’s not possible to initiate an outside request to connect through to the internet down through to the internet gateway of our VPC and then across to our private subnet. It’s not possible. There aren’t routes to enable us to do that. Access isn’t allowed and it is private by design. </p>
<p>However, this engineer here needs to gain access to the EC2 instances to perform some maintenance or updates to those resources. Now, to enable you to do this, you need to use a bastion host. Now, this bastion host sits within the public subnet and this is just another EC2 instance. Now, this instance, to follow best practices, needs to be very secure. It needs to be hardened and very robust, but effectively, it needs to be tightened down to remove any kind of vulnerabilities and loose access controls. </p>
<p>Now, this EC2 instance is a part of a security group and this security group needs to be configured as shown. Now, what this security group shows is the inbound connectivity, and it allows SSH on port 22 from this IP address, which is from the engineer’s IP. So it’s being configured for this engineer over here. So this bastion host will essentially allow an SSH connection coming from our engineer over here. Now, that’s great because this engineer can then gain access to the bastion host here. And then, what that engineer can do is then use this as like a jump server and connect from the bastion host through to our EC2 instances here. </p>
<p>But before any of that can happen, we need to set up another security group for our EC2 instances here. So we’ll have another security group around our EC2 instances and this will be configured as shown. Again, this is the inbound rule set, and we can see that SSH is allowed on port 22 from this source here. Now, this source is actually a security group. It is prefixed with sg, which is security group, and this security group is actually this one here. This is associated with the bastion host. So what this is saying is any instances associated to this security group allow inbound SSH from any resource sitting within this security group, which as we know, is associated to our bastion host. So that will just allow the bastion host SSH access to these instances. </p>
<p>So now, we have our security groups set up and configured. However, let me just talk you through the connection process. So our engineer here will connect to our bastion host. Now, the engineer will be able to access the bastion host using the private key. So let’s just follow this process through. So the engineer will SSH to our bastion host, so it’ll connect via the internet. The connection will then come through the internet gateway. Let’s assume that any net calls that we have allow the access and we come to the security group here. Now, this security group says, allow connection if it’s an SSH connection from this IP address and this is the IP address of our engineer over here. So it allows access through. So now, this engineer has access to our bastion host. But now, our engineer needs to jump across to our private instances. Now, again, we’re going to need a private key to do that. </p>
<p>Now, one method would be to store the private keys on this bastion host and then run the command to SSH and access would be allowed, but that’s not best practice at all. We really don’t want to be installing private keys within the public subnet or on the bastion host because if this bastion host ever got compromised, then the malicious user will be able to use any private keys that are stored on the bastion host and connect to our private instances, which would be very bad. So how does this engineer SSH into our EC2 instances if he doesn’t have the private key? </p>
<p>Now, the best way to do this is to set up something called SSH agent forwarding. Now, what this allows us you to do is to store the private keys for the instances within the private subnet on your local client, so that when you connect through to the bastion host, you can then SSH, but using the private key to the EC2 instances that is stored on your client rather than storing it on the bastion host. Now, with that in mind, once you have connected to your bastion host, using the example that I just showed you, you can then SSH into your private instances, at which point, it will hit the private security group that allows any SSH access on port 22 from the security group associated with the bastion host and then there, you can gain access. </p>
<p>So just to summarize exactly what we’ve done here. We started off by creating an EC2 instance within the public subnet marked as our bastion host. We then hardened that instance to try and protect it against as many security threats as possible and to lock down access to that instance. We then associated a security group that only allowed SSH inbound access from a particular IP address or a particular range of IP addresses. We then added a rule to the security group associated to our private instances that allowed SSH inbound access from the bastion host security group. You then need to ensure that SSH agent forwarding is configured on your client and then this allows you to firstly connect to your bastion host using the private key of the bastion host and then using that as the jump server to jump into your private subnet from your bastion host using the private instances, private key, which is also stored on your client PC. </p>
<p>In the next section, I’m going to be coming away from the security aspects of VPC’s list and I’m going to be focusing more on <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpn-direct-connect/">VPC connectivity</a>.</p>
<h1 id="VPN-amp-Direct-Connect"><a href="#VPN-amp-Direct-Connect" class="headerlink" title="VPN &amp; Direct Connect"></a>VPN &amp; Direct Connect</h1><p><strong>Resources referenced within this lecture:</strong></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-ipsec-vpns-understanding-building-and-configuring/">Amazon VPC IPSec VPNs- Understanding, Building and Configuring</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/">AWS Virtual Private Cloud: Subnets and Routing</a></p>
<p><strong>Transcript</strong></p>
<p>Hello and welcome to this section and I’m going to be talking about VPN connections, Virtual Private Networks. Now a Virtual Private Network is essentially a secure way of connecting two remote networks across the internet. So, let’s have a look how we can use VPNs within our <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a>. </p>
<p>So, if we just create a VPC over here and we also have our remote data center over here. Just with a little door and this is our data center and this is our VPC. Now within our VPC, we’re going to have a single subnet. Now this is going to be a private subnet, so there’s no internet gateway and there’s no route to the internet gateway as well. It’s totally isolated. So, a bit more information relating to IP addressing, so our VPC will have a CIDR block of 10.0.0.0&#x2F;16 and for our data center, let’s say the IP address sits on a 192.168.0.0 address space. </p>
<p>Okay, so we have our VPC over here sitting in AWS and we have our remote data center over here, maybe sitting in London somewhere. Okay, now we have resources within our data center in London and we also have some resources over in our private subnet, for example, some EC2 instances. Now what we want to do is enable communications between our resources in our private subnet in our VPC in AWS and to resources that are held on premise within our data center. Now we want to do this via a secure connection. </p>
<p>So, one option is to create a VPN connection, a Virtual Private Network. Now let’s look at some of the components involved with that. Firstly, on your VPC side, you need to create something called a virtual gateway and this attaches directly to your VPC. Much in the same way an internet gateway does to enable public subnets access out to the internet and again, this is managed by AWS. So, here we have our virtual gateway. Now over in our data center, we also need another endpoint and this will be our customer gateway. And then this could be a piece of hardware or it can be a software virtual appliance, other way it need to be host within your data center. So, now we have an endpoint at our data center, the customer gateway and we also have an endpoint attached to our VPC, the virtual private gateway. </p>
<p>Now during the creation of our virtual private gateway, we’ll need to supply some additional information that’s going to be used in our customer gateway such as the customer gateway’s IP address and the type of routing to be used whether it’s dynamic or static. Now if you’re not familiar with dynamic or static routing, then please see our existing course shown on the screen and this dives into different types of routing across subnets and across site to site as well. So, that will give you a little bit in-depth information on how the routing would work. </p>
<p>Now once your virtual private gateway is attached to your VPC and configured and also your customer gateway’s installed, then what we can do is initiate a tunnel between the two endpoints. Now, this VPN tunnel can only be initiated from your customer gateway. It can’t be initiated from your virtual gateway. Now if there was some idle activity across this link for a period of 10 seconds or more, then this VPN tunnel connection would drop. So, to prevent that from dropping, you can set up network monitoring to set up continuous network pings from the customer gateway side to the virtual gateway to ensure that connection remains up and running. </p>
<p>So, now we have our VPN tunnel up and running created between our virtual private gateway and our customer gateway, we need to change the route table associated to this private subnet, so our EC2 instances know how to connect to the 192.168 network. So, let’s take a look at that. Now we can see here that we have the local route which we have with every route table as we know but we also have this additional route here. Now the destination is 192.168.0.0&#x2F;16 which points to our data center network and the target is this virtual gateway. Now we know that’s a virtual gateway ‘cause it’s prefixed with vgw and then this is the ID of the virtual gateway itself and this relates to our virtual gateway up here. So the instances within this subnet now have an additional route that points to this virtual gateway to get to the network of the data center. </p>
<p>What you can do is also enable route propagation within your route table as well. Now what this will do is once your VPN tunnel is up and running, then any routes that are represented across your VPN connection will be automatically added to your route table, so you might have other networks within your data center other than the 192.168 that are configured to use that VPN tunnel, so any traffic from another network received by your virtual gateway will allow these routes to be automatically propagated to the route tables that you’ve enabled route propagation on. Now depending on what sort of customer gateway you installed, will depend if it supports the BGP Protocol, which is the Border Gateway Protocol and if it does, then this supports dynamic routing, so this will populate all the routes for the VPN connection for you which means you won’t have to implement any static routing. Now it is recommended that if you can install a customer gateway that does support BGP, then it’s probably best to do so. </p>
<p>Now once our routes were in place, we also need to ensure we have our security groups configured for our instances as well to allow traffic to come from my resources over here and via the customer gateway across the VPN link to our virtual private gateway and then onto our instances but as we know, they are protected by a security group, so we need to ensure that the right protocols et cetera are allowed on the inbound rule set of our security group for our resources that are based over here. So, if we wanted to allow SSH access, for example, or RDP access, then the security group would look as shown. Now we can see that this security group allows both SSH and also RDP and it’s from the source 192.168.0.0 which is of course our network that we’re using on our data center. </p>
<p>So, to quickly recap. We have our virtual private gateway attached to our VPC and we have our customer gateway installed at our remote location. We then configure it with either dynamic or static routing and here we have a static route added for our subnet that points to the virtual private gateway within our VPC to get to our destination network which is of course our destination network of the remote data center. And then we also have our security group protecting our resources within our VPC allowing only specific ports and protocols which are inbound for my remote data center network. So, that’s just a simple example of a site-to-site connection using a VPN which is a secure connection across the internet. </p>
<p>I now want to talk to you about using another site-to-site connection called Direct Connect but this does not use the internet. This is totally isolated infrastructure. So, let me explain how this works. </p>
<p>Okay, so in this section, I’m going to be talking to you about Direct Connect. Now this is another method of connecting your remote location such as your data center or remote office to your AWS environment. Now whereas your VPN connection used the internet to get to your VPC, a Direct Connect connection doesn’t traverse the internet. Instead it uses private infrastructure and connects directly to your VPC. So, there’s no public network that the traffic traverses, so let’s look at the architecture of this to see how it works and how it’s different to a VPN. </p>
<p>Now I’m not going to go into fine configuration details on this, I just want to provide you a high-level overview of how the Direct Connect infrastructure is presented, so let’s take a look. So, let’s start with our on-premise data center that we’ll just over here. This would be our data center. And within our data center, we’ll have a router. Now with a Direct Connection, there’s a middle entity before you get to AWS infrastructure, now this is usually an AWS partner or an AWS customer that holds Direct Connect infrastructure and there’s two parts to this. </p>
<p>The first part is the partner’s infrastructure or the customer’s infrastructure and the other part will be managed by AWS. So, effectively we will have a customer side and also an AWS side as well. Now this is all held within a facility owned and managed by a partner of AWS. This is a separate building entirely to your remote data center. Now again in the customer side, there will also be a router and another router in the AWS side as well. Okay, let’s move on to the final section. So, here we have our AWS region because with AWS Direct Connect, it enables you to create a connection between your data center and an AWS region, not just a VPC, it’s actually connected to a defined region. So, this will be our region here. And within that region we also have our VPC as well. So, this is our VPC and again, within our VPC we’ll have a subnet with perhaps an EC2 instance in it, for example. </p>
<p>Now the reason it’s connected to a region and not a VPC is that a Direct Connect connection allows you to access public as as private resources, so an example of a public source could be Amazon S3 and that’s because Amazon S3 resources can be accessed over the internet via a public connection. Now attached to our VPC, we’ll also have a virtual gateway much like we did when we was talking about our VPN connection. </p>
<p>Okay, let me just recap the three elements that we have here before we go any further. So, we have our customer data center over here with a router. Now here in the middle we have our Direct Connect location, so this is our Direct Connect location. And this sits between our on-premise data center and our AWS infrastructure and this is separated into two cages effectively. We have our customer partner router and we also have the AWS router as well. And then finally we have our AWS infrastructure over here with our region and inside the region, a VPC and we have our public components over here. Let’s just nest that in the AWS cloud. So, that all sits within AWS. </p>
<p>Now I mentioned previously that you can have a private connection and also a public connection and as a part of that configuration, you can configure private virtual interfaces and also public virtual interfaces on your router. So, let’s take a look. So, there’ll be two virtual interfaces. So, one of them will be a private virtual interface and we’ll define this by using this gray line here. So, that connects from your on-premise router to the customer side of the Direct Connect location. Now from here there’ll be a cross connect from the customer router to the AWS router within the same Direct Connect location. And then from here, this virtual private interface will then connect to your virtual gateway. Then this will allow connectivity through to your resources within your VPC. </p>
<p>Now the second interface is a public virtual interface, so let’s use this reddish color for that. So again the connection comes from your on-premise router into the customer side of the Direct Connect location, then there a cross connect across to the AWS router and from here it connects to inside of your AWS region and from here you can access your public AWS resources such as Amazon S3 et cetera. So, now we’ve established a connection from our on-premise data center into a region within AWS where we can access both private resources and also public resources and it’s all done without having to traverse the public internet. Instead there’s dedicated and isolated infrastructure using the Direct Connect locations. Now to be able to use Direct Connect, the only path that you need to establish is from your on-premise data center to a Direct Connect location to enable you to establish a connection to this customer router here. So, as long as you have a dedicated network route to a co-location that provides a Direct Connect connection, then you can establish this dedicated network that we can see here. </p>
<p>Now the great thing when working with Direct Connect is that it’s private connection and also you get speeds from 1 through to 10 gigabits per second. Okay, the final section I want to talk to you about in this course is relating to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpc-peering/">VPC peering</a> and also the Transit Gateway, so let’s take a look at this final section.</p>
<h1 id="VPC-Peering"><a href="#VPC-Peering" class="headerlink" title="VPC Peering"></a>VPC Peering</h1><p>In this section, I want to talk to you about VPC peering. Now we’ve looked at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpn-direct-connect/">VPN connectivity</a> which looked at connecting your on-premise data center or remote office to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPC</a> and also Direct Connect which done the same thing but over an isolated network. Now with VPC peering, it’s relating to connectivity again but what it allows you to do is connect two VPCs together. </p>
<p>So, we have one here and another VPC here. Now each of these VPCs will have resources in them. EC2 instances or databases, et cetera and what we want to allow to happen is for these two VPCs to be able to communicate with each other. Now these VPCs might be in the same region to they might be in different regions. Either way we can allow VPC peering to allow them to communicate with each other. Now the peering connection itself here that links the two VPCs is actually run and hosted on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> infrastructure. So, this is highly resilient, there’s no single point of failure and also there’s no bottlenecked bandwidth either. So, it’s a very good way of linking two VPCs together to allow you to exchange information and for each VPC to communicate with another. </p>
<p>Now you might have multiple VPCs for organization or management and there will be times when you want resources in one VPC to communicate with another. And a quick and simple solution is to implement VPC peering. Now it’s important to mention that this peering connection is a one-to-one connection only. So, if we had a third VPC down here, call this one VPC-3, again this had additional resources in as well and you had a VPC peering connection between two and three, resources in VPC-1 could not go via VPC-1 and then through VPC-2 to get through to VPC-3. That simply is not allowed as it’s a one-to-one connection only. If you wanted VPC-1 to connect to VPC-3, then you’d have to set up a separate VPC peering connection between one and three. So, that’s a very important point when mapping out your peering connections between your VPCs. </p>
<p>Now another important point relates to IP addressing, so for example, if VPC-1 had an address of 10.0.0.0&#x2F;16, VPC-2 was 172.31.0.0&#x2F;16 and then VPC-3 was also 10.0.0.0&#x2F;16, then this connection here would not be possible because when you create VPC peering connections, each VPC cannot have an IP address overlap between them and these two VPCs have the same IP addressing scheme, so this VPC connection would not be possible. So, that’s also something else to bear in mind when creating your VPC peers. So, let’s take that connection away. </p>
<p>Now I also mentioned that you can have VPC peering configured between the same region or between different regions. So, let’s say VPC-1 and VPC-2 was in one region and VPC-3 was in another region. Then this link here would be an inter-region VPC connection. Let me now run through the process of how this peering connection is initiated. </p>
<p>So, let me just get rid of what we have on the screen here and start again. So, we have two VPCs. Our first one and also our second one. VPC-1 and VPC-2. Now VPC-1 is going to be known as the requester and VPC-2 is going to be known as the accepter. Now the owner of VPC-1 needs to send a VPC peering request to the owner of VPC-2. And again, remember, we need to make sure that the CIDR blocks of these VPCs do not overlap, so that request comes across to the VPC accepter and that’s the first stage. If the VPC accepter is happy with that peering connection, then an acknowledgement and acceptance of that request is sent back to the requester and that’s the second stage and this creates the peering connection between the two. At this stage, each VPC needs to update their routing tables to allow the traffic from VPC-1 to get to the destination of VPC-2. </p>
<p>Now to do this, we need to know the CIDR blocks of these VPCs. So, let’s assume VPC-1 is 10.0.0.0&#x2F;16 and VPC-2 is 172.31.0.0&#x2F;16. So that are two CIDR blocks that we have for our VPCs and as we know, they’re not overlapping, so from an IP perspective, there’s no issues there. So, now let’s look at the route table for each of these. So, firstly, VPC-1. As we can see, we always have our local route and then we also have this additional route here. So, the destination 172.31.0.0&#x2F;16 which is VPC-2 to go via the target of this peering connection. And the pcx simply means that this target is a peering connection. And these digits here are the ID of that peering connection. Now this VPC knows how to get to the 172.31 network by going via the peering connection here. </p>
<p>So, let’s now look at the route table for VPC-2. Again, we have our local route which every route table has and then also this additional route that points to VPC-1 again across the same peering connection. So, this VPC can now access the network of VPC-1 again via the same peering connection. </p>
<p>Now the final part of the configuration would be to modify the security groups that are hosting any resources within your VPC. So, you might have a security group here and a security group here each with EC2 instances or databases and we’ll simply need to update the rules to allow the correct resources, ports and protocols to communicate with each other. </p>
<p>So that’s a high-level overview, that is VPC peering. Now what I want to talk to you about is the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-transit-gateway/">AWS Transit Gateway</a> and again, this looks at how to connect more than one VPC together but through a one-to-many connectivity method, so let’s take a look at that.</p>
<h1 id="Transit-Gateway"><a href="#Transit-Gateway" class="headerlink" title="Transit Gateway"></a>Transit Gateway</h1><p>So, the final element I want to talk to you about is the AWS Transit Gateway. And this is essentially a development on from the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-vpc-peering/">VPC peering</a>. In today’s world we’re using more and more <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-introduction/">VPCs</a> to segment and manage different workloads and as our organization gets bigger and bigger, we’re creating more and more VPCs, we have more and more connections from our remote locations such as our data centers and offices, et cetera and creating VPC peering connections to each one of these bearing in mind it’s a one-on-one connection can be very cumbersome and time consuming and just not very well to manage. </p>
<p>So, let’s say we had four VPCs represented by these circles here. And we also had a couple of remote offices as well. So, one there and one there. Now if we wanted to connect these VPCs into our office locations, now based on what we’ve already spoken about so far, we can use VPC peering to link our VPCs together. But as we know, this is just a one-on-one connection, so we also need a connection across there and also a connection across there. So, we have one, two, three, four, five, six VPC peering connections there. Now one of these remote locations might be using a VPN connection to get to that VPC, and also a VPN connection there and maybe even a third VPN connection to this VPC as well and this remote location might be used in Direct Connect to get to a couple of different VPCs in different regions. Now, that is a lot of connections and a lot of gateways to manage. We have customer gateways at the remote ends and also private gateways within our VPCs as well. </p>
<p>What AWS Transit Gateway allows you to do is to connect all of this infrastructure, so all of your VPCs, all of your remote locations, whether it’s over Direct Connect or VPN via a central hub. So, let’s take a look at how that looks. So, again we have our four VPCs and also we have our two data centers here at the bottom, our two remote locations. However, this time, we have the AWS Transit Gateway in the middle. Now, for each VPC or remote location that we want to allow to talk to each other, then all we need to do is to create a single connection to the Transit Gateway, so one from each of the VPCs and also one each from the remote locations as well. Again, these will be a VPN connection and maybe a Direct Connect connection. So, either way, VPN, Direct Connect or VPC, they all connect to this central hub, this AWS Transit Gateway. </p>
<p>As you can see between the two designs, this one over here has a lot more connections than this one over here. So, the AWS Transit Gateway simplifies your whole network connectivity. It allows all of your VPCs to easily communicate with one another and also communicate with your remote locations as well. All the routing is managed centrally within that hub and when any new remote locations or VPCs are created, for example, you might have another two VPCs created, all you’d need to do is to connect it to the AWS Transit Gateway and each of these new VPCs can then communicate with the entire rest of your infrastructure. </p>
<p>Now because the Transit Gateway goes through this central hub, it allows you to centralize all your monitoring as well for your network traffic and connectivity all through the one dashboard which is great. So, that was just a very quick high-level overview of AWS Transit Gateway and how it differs from the VPC peering. And that’s the last component I want to discuss in this course relating to VPCs and network connectivity. </p>
<p>So, in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-networking/vpc-summary/">final lecture</a>, I’m just going to quickly review what we’ve covered throughout this course.</p>
<h1 id="Amazon-Route-53"><a href="#Amazon-Route-53" class="headerlink" title="Amazon Route 53"></a>Amazon Route 53</h1><p>Welcome to this lecture covering an overview of Amazon Route 53, which is used as the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/introduction/">DNS service</a> offered by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>, but firstly, what is DNS?</p>
<p>DNS, or Domain Name System, is a hierarchical distributed naming system for computers, services or any resource connected to the internet or a private network. It associates various information with domain names, such as CloudAcademy.com or Amazon.com, and is responsible for the translation of domain names to IP addresses. A common analogy used to explain DNS, is that it is the phone book of the internet, as you can look up a human-friendly name. For example, <a target="_blank" rel="noopener" href="http://www.cloudacademy.com/">www.CloudAcademy.com</a>, and it will provide the respective IP address.</p>
<p>Now going back to Route 53, Route 53 is Amazon’s highly available and scalable domain name system that provides secure and reliable routing of requests, both for services within AWS and infrastructure that is outside of AWS. Route 53 is able to provide this service through its global network of authoritative DNS servers that reduce latency and can be managed via the management console or API.</p>
<p>To understand Route 53, let me explain some of the components and elements used within the service, starting with Hosted Zones.</p>
<p>A hosted zone is a container that holds information about how you want to route traffic for a domain such as CloudAcademy.com. Route 53 supports the following type of zones: A public hosted zone. This zone determines how traffic is routed on the internet and can be created when you register your domain with Route 53. A private hosted zone. This zone determines how traffic is routed within the Amazon VPC. If your resources are not accessible outside of the VPC you can use any domain name you wish.</p>
<p>Next, there are different domains that are supported by Route 53, and these include: generic top-level domains, known as TLDs. For example .watch which may be used for websites relating to streaming, video, or watches. Or perhaps .clothing, used by those in the fashion industry, such as retailers and department stores, as well as designers. So essentially TLDs are used to help determine what information you might expect to find on the website Then we have: Geographic domains For example .com.au (Australia), or .uk for the United Kingdom. So these are used to represent the geographical location of the site itself.</p>
<p>Next, we have resource record types, and Route 53 supports the most common types, which will meet the needs for the majority of customer DNS requirements as shown in this table. In addition to these record types, Route 53 also uses Alias records, which are a Route 53-specific extension to DNS. These Alias records which act like a CNAME record allow you to route your traffic to other AWS resources, such as Elastic load balancers, Elastic Beanstalk environments, CloudFront distributions, VPC Interface Endpoints, or S3 buckets configured as static websites.</p>
<p>Routing Policies. When you create a resource record set, you must choose a routing policy that will be applied to it, and this then determines how Route 53 will respond to these queries. The routing policies available within Route 53 include:</p>
<p>Simple routing policy: This is the default policy, and it is for single resources that perform a given function. For example, a single web server, in this case, all responses to the DNS query are based solely on the values you entered into the resource record when you created it.</p>
<p>Failover routing policy: This allows you to route traffic to different resources based upon the health of those resources. If the primary resource is healthy then traffic will be directed to that resource, if it becomes unhealthy then the routing policy will route the traffic to an alternate healthy resource. This is considered as an active-passive failover mechanism.</p>
<p>Geo-location routing policy: This lets you route traffic based on the geographic location of your users. You can define geographic routing policies based on continent, country or state in the U.S. If you have overlapping geographic regions, for example continent and country, it will direct to the smallest denominator, and in this case, that would be country. Geo-location can also be used to restrict access to your site based on location the geographic origin of the traffic. Or perhaps you want to direct all DNS queries from Europe to an elastic load balancer in the London region.</p>
<p>Geoproximity routing policy: This policy is based upon the location of both the users and your resources, whereas geo-location is based purely on the location of the users. Geoproximity allows you to set a bias against resources that can either route more or less traffic to your resource. This bias can either expand or reduce the geographic regional scope of traffic that can be routed to your resource.</p>
<p>Latency routing policy: This is suitable when you have resources in multiple regions, and you want Route 53 to respond to DNS queries with resources that provide the lowest latency for the request.</p>
<p>Multivalue answer routing policy: This allows you to get a response from a DNS request from up to 8 records at once that are picked at random, all of which will be healthy resources. This helps to factor in a level of load balancing and enhance availability.</p>
<p>Weighted routing policy: This is suitable when you have multiple resource records that perform the same function, such as a website, and you want to route traffic between them based on proportions that you specify. To determine the probability, the formula is the weight of the individual resource record divided by the sum of the total value in resource record set. For example, if you have three servers, weights are assigned two, two and six, a sum of 10. The first two are selected 20% of the time, and the last one 60% of the time.</p>
<h1 id="Amazon-CloudFront"><a href="#Amazon-CloudFront" class="headerlink" title="Amazon CloudFront"></a>Amazon CloudFront</h1><p>Hello and welcome to this lecture which will introduce you to the Amazon CloudFront service.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/amazon-cloudfront/">Amazon CloudFront</a> is AWS’s fault-tolerant and globally scalable content <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-dns-content-delivery-aws/introduction/">delivery network service</a>. It provides seamless integration with other Amazon Web Services services to provide an easy way to distribute content.</p>
<p>Amazon CloudFront speeds up distribution of your static and dynamic content through its worldwide network of edge locations. Normally, when a user requests content that you’re hosting without a CDN, the request is routed back to the source web server which could reside in a different continent to the user initiating the request. However, if you’re using CloudFront, the request is instead routed to the closest edge to the user’s location which provides the lowest latency to deliver the best performance through cached data.</p>
<p>So essentially Amazon CloudFront acts as a content delivery network service, which provides a means of distributing the source data of your web traffic closer to the end-user requesting the content via AWS edge locations as cached data. As this data is cached, after a set period, this cached data will expire and so AWS CloudFront doesn’t provide durability of your data. Instead, it distributes the source data which resides on durable storage, such as Amazon S3.</p>
<p>AWS edge locations are sites deployed in major cities and highly populated areas across the globe. While edge locations are not used to deploy your main infrastructure, such as EC2 instances or EBS storage, they are used by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services such as AWS CloudFront to cache data and reduce latency for end user access. For example, you may have your website hosted on EC2 instances or S3 within the Ohio region, with an associated CloudFront distribution. When a user accesses your website from Europe, they would then be redirected to their closest edge location in Europe, where cached data could be read off your website. This significantly reduces latency.</p>
<p>CloudFront uses distributions to control which source data it needs to redistribute and to where. These distributions can be configured as one of two different delivery methods. Firstly, a web distribution and this type of distribution is used if you want to speed up distribution of static and dynamic content, for example, .html, .css, .php, and graphics files, distribute media files using HTTP or HTTPS, add, update, or delete objects, and submit data from web forms, and use live streaming to stream an event in real-time. Alternatively, you can create an RTMP distribution, which is used if you want to distribute streaming media with the Adobe Flash media service RTMP protocol. The benefit of using RTMP distribution is that your end user can start viewing the media before the complete file has been downloaded from the edge location. The source data for an RTMP distribution can only exist within an S3 bucket and not an EC2 web server.</p>
<p>When configuring your distributions, you will be required to enter your origin information, this is essentially where the distribution is going to get the data to distribute across edge locations and it will be the DNS name of the S3 bucket or the HTTP server. If the origin is an S3 bucket, then it can be selected from a drop-down list. If you are using S3 as a static website you must enter the static hosting website endpoint.</p>
<p>If using an S3 bucket as your origin, then for additional security you can create a CloudFront user called an origin access identity, known as OAI, which can be associated with your newly created distribution. This simply ensures that only this OAI can access and serve content from your bucket and therefore preventing anyone circumventing your CloudFront distribution by accessing the files directly in the bucket using the object URL.</p>
<p>You will also be required to select a host of different caching behavior options, defining how you want the data at the edge location to be cached via various methods and policies. Lastly, you will define the distribution settings themselves, and this will look at which edge locations you want your data to be distributed to, which can either be US, Canada, and Europe, US, Canada, Europe, and Asia, or all edge locations for the best performance. You can also define if you want your distribution to be associated to a web application firewall access control list for additional security and web application protection. For more information on AWS WAF, please see the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/protecting-web-apps-aws-waf-shield-firewall-manager/introduction/">course</a>. In addition to using a web application firewall access control list, you can also implement additional encryption security by specifying an SSL certificate that must be used with a distribution.</p>
<p>Once your distribution is configured, you simply enable the distribution for it to be created. When content from your website is accessed, the end-user will be directed to their closest edge location in terms of latency, to see if the content is cached by CloudFront at that edge location. If the content is there, the user will access the content from the edge location instead of the origin, therefore reducing latency. If the content is not there, or the cache has expired for that content at the edge location, then CloudFront will request the content from the source origin again. This content will then be used to maintain a fresh cache for any future request until it again expires.</p>
<p>That now brings me to the end of this lecture and to the end of this course. If you have any feedback, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="AWS-Global-Accelerator"><a href="#AWS-Global-Accelerator" class="headerlink" title="AWS Global Accelerator"></a>AWS Global Accelerator</h1><p>Hello and welcome to this lecture covering the AWS Global Accelerator, which is a Global <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service and therefore not tied to a specific region.</p>
<p>The ultimate aim of the AWS Global Accelerator is to get UDP and TCP traffic from your end user clients to your applications faster and quicker and more reliably, through the use of the AWS global infrastructure and specified endpoints, instead of having to traverse the public internet, which is not as reliable and carries a higher security risk.</p>
<p>Global Accelerator uses two static IP addresses associated with a DNS name which is used as a fixed source to gain access to your application which could be sitting behind a load balancer, such as a network or application load balancer, or directly connected to your EC2 instance or the Elastic IP address. These IP addresses can be mapped to multiple different endpoints, each operating in a different region if a multi-region application is deployed to enhance performance of routing choices.</p>
<p>Because the routing of your request is based across the AWS Global Infrastructure, Global Accelerator intelligently routes customers requests across the most optimized path using its global reach of edge locations, for the lowest latency and avoids any resources that are unhealthy. This helps to improve regional failover and high availability across your deployment.</p>
<p>To set up and configure AWS Global Accelerator there are effectively four steps to follow.</p>
<p>Firstly, you must create your accelerator and give it a name. You must also select if you want to use two IP addresses from AWS’ pool of IP addresses or use your own. For each accelerator created, you must select two IP addresses.</p>
<p>Next, you need to create a listener. The listener is used to receive and process incoming connections based upon both the protocol and ports specified, which can either be UDP or TCP based.</p>
<p>Once your listener is created you must associate it with an endpoint group. Each endpoint group is associated with a different region, and within each group there are multiple endpoints. You can also set a traffic dial for the endpoint group, and this is essentially a percentage of how much traffic you would like to go to that endpoint group. And this helps you with blue and green deployments of your application to control the amount of traffic to specific regions. At the stage of adding your endpoint groups you can also configure health checks to allow the global accelerator to understand what should be deemed as healthy and unhealthy. </p>
<p>Finally, you must associate and register your endpoints for your application. And this can either be an application load balancer, a network load balancer, an EC2 instance or an EIP. For each endpoint, you can also assign a weight to route the percentage of traffic to that endpoint in each of your endpoint groups.</p>
<p>Let me now provide a very quick demonstration to show you how this creation looks within the AWS Console.</p>
<p>Okay so I’m logged in to my AWS Management Console and I need to go to the Global Accelerator which is under the Network and Content Delivery category. So if I select the Global Accelerator, now at the moment I don’t have any Global Accelerators configured. So from here I’ll simply click Create Accelerator.</p>
<p>Now, to start with, I need to select a name for my accelerator. So let me just call this MyAccelerator. Now here we have the IP address type, which is IPv4, and then we have the IP address pool selection. And the default is to use Amazon’s pool of IP addresses but if you want to use your own pool of addresses, then this is where you could change it. And also you can add any tags to this service if you need to.</p>
<p>So onto the next stage, this is where we add our listeners. So we can add in a port, for example, port 80. Either TCP or UDP as the protocol, and then you also have Client affinity here. And we can see that if you have state full applications, Global Accelerator can direct all requests from a user at a specific client IP address to the same endpoint resource to maintain client affinity. The default for this option is None. We don’t need that for this demonstration, so I’m just gonna leave that as None. And if you want to add any more listeners, simply click on Add Listener, and fill in the relevant details.</p>
<p>For this demonstration, I’m just gonna leave it as the one listener. Once your listeners are configured as you need to, click on Next. And here we have our endpoint groups. Here you select your regions that you want your application to reside in. So, for example, I’ll select the London region. And also we have our traffic dial, which as I explained previously, is essentially the percentage of traffic to this region. We can add additional endpoints, so we can have multiple regions if we want to. And you can keep going, add in more more regions. So let’s go and remove those two, just leave it as the one region. If you select on the configure Health checks, then you can set your health check configuration as need be just so the AWS Global Accelerator knows what it deems as healthy.</p>
<p>Once you have set your health checks, then you can select Next. On the final stage we need to add our endpoints. So here we have our endpoint group, and we select add endpoint. Now we can either add an Application Load Balancer, Network Load Balancer, an EC2 instance or an Elastic IP address. For this example, I’m gonna select an EC2 instance. And I won’t need to select the specific instance. I have one here called MyApplication. And again we have weight information, which directs the amount of traffic to each of your endpoint in your groups. I only have the single endpoint in this group, so I’m just gonna change that to 255. It can be from zero all the way to 255.</p>
<p>To add additional endpoints, simply click on Add endpoint. Select the endpoint that you’d like and then the related resource. I’m just gonna have the one endpoint in this endpoint group. And then once you’ve done that, simply click Create accelerator. And this would take a few minutes to configure itself and become active. And as you can see the status is in progress.</p>
<p>Also, as I explained earlier, we can see that this Global Accelerator has been given a DNS name, which results to this two static IP addresses. And these are the two IP addresses from the AWS pool of addresses. So that means you can add or change your endpoint groups and related endpoints in any of the regions without having to change the DNS name or the static IP addresses that it results to. So it’s very easy to change and increase region availability and high availability for your Global Accelerator. And it’s as simple as that.</p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="3Subnets"><a href="#3Subnets" class="headerlink" title="3Subnets"></a>3<strong>Subnets</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/vpc-cidr-blocks/">VPC TCP&#x2F;IP Addressing</a></p>
<h1 id="6NAT-Gateway"><a href="#6NAT-Gateway" class="headerlink" title="6NAT Gateway"></a>6<strong>NAT Gateway</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">AWS Shared Responsibility Model</a></p>
<h1 id="7Bastion-Hosts"><a href="#7Bastion-Hosts" class="headerlink" title="7Bastion Hosts"></a>7<strong>Bastion Hosts</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/blogs/security/securely-connect-to-linux-instances-running-in-a-private-amazon-vpc/">SSH Agent Forwarding</a></p>
<h1 id="8VPN-amp-Direct-Connect"><a href="#8VPN-amp-Direct-Connect" class="headerlink" title="8VPN &amp; Direct Connect"></a>8<strong>VPN &amp; Direct Connect</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-vpc-ipsec-vpns-understanding-building-and-configuring/">Amazon VPC IPSec VPNs- Understanding, Building and Configuring</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-virtual-private-cloud-subnets-and-routing/">AWS Virtual Private Cloud: Subnets and Routing</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:39" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:39-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:10" itemprop="dateModified" datetime="2022-11-20T19:01:10-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Databases-CLF-C01-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Knowledge-Check-Databases-CLF-C01.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:38" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:38-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:08" itemprop="dateModified" datetime="2022-11-20T19:08:08-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-RDS-Database-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Databases-CLF-C01-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:36" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:36-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:58:38" itemprop="dateModified" datetime="2022-11-20T18:58:38-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Databases-CLF-C01-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Databases in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various Database services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#x70;&#112;&#x6f;&#114;&#116;&#64;&#99;&#108;&#111;&#x75;&#100;&#x61;&#99;&#x61;&#x64;&#101;&#109;&#121;&#46;&#x63;&#x6f;&#109;">&#x73;&#117;&#x70;&#112;&#x6f;&#114;&#116;&#64;&#99;&#108;&#111;&#x75;&#100;&#x61;&#99;&#x61;&#x64;&#101;&#109;&#121;&#46;&#x63;&#x6f;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Database services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Database services in AWS, including:</p>
<ul>
<li>Managed relational databases using the Amazon Relational Database Service, or RDS;</li>
<li>Managed NoSQL databases including Amazon DynamoDB; and</li>
<li>Data warehousing using Amazon Redshift.</li>
</ul>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="The-AWS-Database-Landscape"><a href="#The-AWS-Database-Landscape" class="headerlink" title="The AWS Database Landscape"></a>The AWS Database Landscape</h1><p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> cloud has thousands of services and features. It’s easy to be amazed and overwhelmed by the number of choices available. </p>
<p>However, inside AWS, the three primary types of services used by customers are compute, storage, and databases.</p>
<p>My focus, in this course, is on the managed database offerings available from AWS and the types of workloads they support.</p>
<p>Databases are the foundation of modern application development. A database’s implementation and how data is structured will determine how well an application will perform as it scales.</p>
<p>There are two primary types of databases, relational and non-relational. </p>
<p>Relational databases, sometimes called SQL databases because they use the Structured Query Language to manage information storage and retrieval, have been commercially available since the 1970s and are optimized around data storage.</p>
<p>Sometimes the Structured Query Language is called Ess-Queue-Ell and other times it’s pronounced Sequel. Both are correct.</p>
<p>Non-relational databases, also called NoSQL databases because data is stored and retrieved primarily using methods other than SQL, became popular in the 21st Century. Over the past 20+ years, developers have created web-based applications that needed to process large amounts of unstructured and semi-structured data quickly and reliably. </p>
<p>NoSQL databases are often distributed databases where data is stored on multiple computers or nodes.</p>
<p>A database is any type of mechanism used for storing, managing, and retrieving information. It is a repository–or collection–of data. </p>
<p>Though, an application developer probably thinks of a database as a place to put the stuff their application needs to run.</p>
<p>There are nine primary categories of databases available on AWS.</p>
<ol>
<li>Relational Databases</li>
<li>Key-Value Databases</li>
<li>Document Databases</li>
<li>In-Memory Stores</li>
<li>Graph Databases</li>
<li>Columnar Databases</li>
<li>Time Series Databases</li>
<li>Quantum Ledger Databases</li>
<li>Search</li>
</ol>
<p>Each one of these database types is optimized to support a specific type of workload. </p>
<p>Matching an application with the appropriate database type is essential for highly performant and cost-efficient operation.</p>
<p>I can remember a time when choosing a database was a platform choice. The underlying technology was almost secondary. Three or four vendors would be considered, one would be chosen as a primary platform, and every application would be built using it. It was less of a database management system and more of a vendor relationship.</p>
<p>Many people had a favorite database and felt that it could be used to manage any type of workload. On a small scale, this might be true. </p>
<p>If I have a small car, I can take 2-3 kids to school every day without issue.</p>
<p>However, what happens when I need to get 20 kids to school? The small car would still work but would be terribly inefficient. It would take several trips, take a considerable amount of time, be very hard on the car, and waste resources. A better option is the school bus.</p>
<p>Thankfully, the days of choosing a single database solution for all applications are long past. </p>
<p>While it is possible to make a general-purpose database manage just about any type of workload, it will not scale as demand increases. </p>
<p>One of the reasons this is important is because the cloud has the promise of agility. That is, being able to respond to changes in a business environment as they happen. </p>
<p>This includes the ability to grow as needed and it’s called scalability.</p>
<p>Another promise of the cloud, one that is talked about but not always understood, is elasticity. That is, it’s great to be able to scale up to meet demand but, elasticity allows for the opposite to happen. When demand decreases, so does the scale and its related expenses.</p>
<p>Who wants to pay for idle resources?</p>
<p>Today, it’s important for application developers to examine the data and consider its size, shape, and computational requirements for processing and analysis. These three things determine what type of database is needed for a particular application to allow for scalability and elasticity. </p>
<p>This also means there could be multiple databases used by an application; each one serving a unique purpose. </p>
<p>The two primary workload types are operational and analytical.</p>
<p>Operational applications are the ones often referred to as Online Transactional Processing applications, OLTP. </p>
<p>OLTP applications are among the most common built today. A transaction is a record of an exchange. OLTP is centered around a set of common business processes that is regular, repeatable, and durable. </p>
<p>It could be something like e-commerce, a content management system, or information management. </p>
<p>Data goes in and reports come out. This is regular, expected work. Often, the database on the back end is relational. The data is very structured, and the results are predictable.</p>
<p>The other type of workload is the analytical application. Online Analytics Processing applications, OLAP, are run–as needed–for things like business intelligence workloads and data analysis. </p>
<p>The goal is to gain insight. Workloads are often Retrospective, Streaming, and Predictive.</p>
<p>Retrospective analytics examine an organization’s history. What happened last quarter, last year, or–maybe–the last five years. </p>
<p>Streaming workloads are gathering data, in real-time, to discover trends or raise an alarm.</p>
<p>Predictive analytics uses data to try to look into the future. There are applications that are beginning to use Machine Learning and Artificial Intelligence to improve. </p>
<p>Applications that process analytics workloads are run on-demand. It is unknown what questions they’re going to answer. The data used has little structure and the workloads, themselves, are unpredictable.</p>
<p>Databases power 21st Century applications.</p>
<p>The efficient use of resources in the cloud requires organizations to be agile. Agility implies being responsive to change. However, because costs in the cloud are primarily based on consumption, it is important to consider the requirements of scalability and elasticity.</p>
<p>Resources should grow as demand increases but they also need to shrink as that demand subsides.</p>
<p>Online Transactional Processing Applications, OLTP, are usually powered by relational databases. The data is highly structured, controlled, and predictable.</p>
<p>Online Analytics Processing Applications, OLAP, are often powered on the backend using non-relational databases. The data for these applications is either semi-structured or unstructured, the workloads are unpredictable, and the output is used to answer questions about the unknown. </p>
<p>That’s a little about what databases do, but why is structured data important? The answer to that question involves something called a schema, and you’ll learn about it in the next lecture on relational databases.</p>
<h1 id="Relational-Databases"><a href="#Relational-Databases" class="headerlink" title="Relational Databases"></a>Relational Databases</h1><p>Relational databases have been commercially available since the 1970s. They provide an efficient, intuitive, and flexible way to store and report on highly-structured data. </p>
<p>These structures, called schemas, are defined before any data can be entered into the database.</p>
<p>Schemas are designed and built based on reporting requirements. </p>
<p>This means that a database’s expected output drives the creation of the database and how data is stored inside it.</p>
<p>Once a schema has been defined, database administrators and programmers work backward from these requirements to define how data will be stored inside the database.</p>
<p>No data can be stored in a database until this work has been completed. </p>
<p>Schema changes to existing databases are expensive in terms of time and compute power.  It also has a risk of corrupting data and breaking existing reports.</p>
<p>Data in a relational database is stored in tables. Each table–sometimes called a relation–contains one or more rows of data. </p>
<p>Each row–sometimes called a record–contains a collection of logically related data that is identified by a key. </p>
<p>The pieces of data stored in a row are called attributes or fields.</p>
<p>Visually, a table looks like a spreadsheet that has rows and columns. </p>
<p>Stored in one of the columns, each table has a primary key that uniquely identifies the information stored in each row. </p>
<p>Relationships between tables are created using these keys and there are rules that govern their behavior. The primary key in one table is a foreign key in another.</p>
<p>Data integrity is of particular concern in a relational database, there are a number of constraints that ensure the data contained in tables is reliable and accurate.</p>
<p>These reliability features–commonly referred to as ACID transactions–are atomicity, consistency, isolation, and durability. </p>
<p>Atomicity refers to the elements that make up a single database transaction. A transaction could have multiple parts. It is treated as a single unit that either succeeds completely or fails completely.</p>
<p>Consistency refers to the database’s state. Transactions must take the database from one valid state to another valid state.</p>
<p>Isolation prevents one transaction from interfering with another.</p>
<p>Durability ensures that data changes become permanent once the transaction is committed to the database.</p>
<p>Data in a relational database must be kept in a known and stable state. </p>
<p>As part of the requirements to maintain database stability, Primary and Foreign Keys are constrained–they have rules that govern them–to ensure the integrity of database tables.</p>
<p>Entity Integrity ensures that, in a table, the primary key is unique to the table and it has a value. Primary keys cannot be blank or null.</p>
<p>Referential Integrity requires that every value in a Foreign Key column exists as the Primary Key of its originating table. If four tables are related and a record is deleted in one of them, then the corresponding records in related tables must be deleted as well.  </p>
<p>The standard user and application programming interface–or API–of relational databases is the Structured Query Language, SQL. </p>
<p>Pronounced as either Ess-Queue-Ell or Sequel, it can be used either interactively or programmatically to create, update, and maintain the data inside a relational database.</p>
<p>SQL is the dominant query language for relational databases. </p>
<p>SQL is an industry standard, it is interoperable between database engines and application programming languages, well-documented, and stable.</p>
<p>Security is one of the most important responsibilities of a database administrator. </p>
<p>Relational database engines have built-in features for securing and protecting data but planning and effort are required to properly implement them. </p>
<p>These features include user authentication, authorization, and audit logging.</p>
<p>As part of the structure, data stored in relational databases is highly normalized. Normalization is a process where information is organized efficiently and consistently before storing it.</p>
<p>Duplicate data is discarded.</p>
<p>Closely related fields are grouped together.</p>
<p>Data should only be stored one time in a relational database. Fields that are logically related, like a first and last name, should be stored in the same table. </p>
<p>Removing redundancy and keeping similar data close reduces storage costs and improves the efficiency of data retrieval.</p>
<p>Relational databases are not partition tolerant. A data partition, in this case, refers to the disk. </p>
<p>Adding another disk would be like creating a second copy of the database. This copy, or partition, is called a shard. </p>
<p>When a shard is created, it uses the original database’s schema. This is a horizontal partition of a database.</p>
<p>To use it, logic outside of the database must be created to direct queries to the correct database.</p>
<p>This is because relational databases are designed to validate how data is stored. They do not check to see if information belongs inside it.</p>
<p>To illustrate, here’s a weather database split into a pair of shards, Rain and Snow. They are identical except for the information stored inside them.</p>
<p>An application determines if data should be stored in Rain or if it should be stored in Snow.</p>
<p>If a record belonging in Rain ends up in Snow and it matches the database schema, it will be stored. </p>
<p>However, since that record belongs in Rain, the reports will be wrong and applications will break when trying to query data.</p>
<p>Because of this complexity, most of the time relational databases are scaled vertically. </p>
<p>Horizontal scaling adds a copy of the database server. Vertical scaling is growing the server; usually by adding memory, CPU, or expanding a disk volume.</p>
<p>Vertical scaling has limits. There are only so many resources that will fit inside a server. Once these limits have been reached, a database will either need to be redesigned or broken into shards.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> has six fully-managed database engines available inside the Relational Database Service, RDS. </p>
<p>They are Amazon Aurora, MySQL, Postgres, MariaDB, Oracle, and Microsoft SQL Server.</p>
<p>Amazon Aurora is AWS’s cloud-native version of MySQL and Postgres. </p>
<p>As a review…</p>
<p>Relational databases are highly-structured data stores.</p>
<p>The structure is called a schema.</p>
<p>The schema defines how data is stored in tables.</p>
<p>Inside tables there are rows and columns.</p>
<p>A row is a record and each column is an attribute or field of the record.</p>
<p>Tables have keys that identify data in a table.</p>
<p>A Primary Key uniquely identifies a row in a table.</p>
<p>Foreign Keys are used to connect data in a row to rows in other tables.</p>
<p>Scaling is usually done vertically by adding compute resources to an existing database.</p>
<p>Horizontal scaling is called sharding and requires logic outside of the database.</p>
<p>Relational databases are ideal for applications that do online transactional processing. </p>
<p>These OLTP applications include online banking, e-commerce sites, inventory management, human resource management, and financial services.</p>
<p>OLTP transactions usually perform specific tasks and involve a single record or a small selection of records.</p>
<p>An online banking customer might send money from a checking account to a saving account.</p>
<p>A transaction like this involves two accounts and no other customers of the bank.</p>
<p>But, what about analytical applications where hundreds, thousands, or millions of transactions need to be processed quickly, efficiently, and at a low cost? </p>
<p>That’s where non-relational databases are helpful. Though, unlike the various relational database engines that have similar needs around structured data, the size &amp; shape of the unstructured or semi-structured data determine the type of non-relational database to choose.</p>
<p>These non-relational databases are often called NoSQL databases because, when they were first developed, they used something other than SQL to store and retrieve data. </p>
<p>However, over time, SQL has been adapted to be used with some of these non-relational databases. Because of this, NoSQL can also mean “Not Only SQL.”</p>
<p>If any type of data can be stored in a relational database, why bother with a non-relational database? </p>
<p>In the next lecture, let’s learn about NoSQL Databases, what they are, and what differentiates them from relational databases.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/course-introduction/">Course Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/the-aws-database-landscape/">The AWS Database Landscape</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/nosql-databases/">NoSQL Databases</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/types-of-managed-nosql-on-aws-part-1/">Types of Managed NoSQL on AWS - Part 1</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/types-of-managed-nosql-on-aws-part-2/">Types of Managed NoSQL on AWS - Part 2</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-differences-between-aws-database-types-1109/summary-and-conclusion/">Summary and Conclusion</a></p>
<h1 id="NoSQL-Databases"><a href="#NoSQL-Databases" class="headerlink" title="NoSQL Databases"></a>NoSQL Databases</h1><p>Relational databases are highly structured repositories of data. They use schemas to define how information is organized and that schema must exist before the database can even be created.</p>
<p>This fixed nature of data structures makes relational databases sub-optimal for analytical processes where data is semi-structured or unstructured.</p>
<p>While relational databases are highly-structured repositories of information, non-relational databases do not use a fixed table structure. They are schema-less.</p>
<p>Since it doesn’t use a predefined schema that is enforced by a database engine, a non-relational database can use structured, semi-structured, and unstructured data without difficulty.</p>
<p>NoSQL is a general term that refers to a particular type of database model. It encompasses a wide variety of different models that don’t fit into the relational model.</p>
<p>Non-relational NoSQL-type databases have been around since the 1960s, but it wasn’t until the early 2000s that the NoSQL approach started to have broad appeal and a new generation of NoSQL systems began to hit the market.</p>
<p>Today, the term NoSQL describes a family of schema-less, non-relational, distributed data stores.</p>
<p>NoSQL databases are popular with developers because they do not require an upfront schema design; they are able to build code without waiting for a database to be designed and built.</p>
<p>It’s this flexibility–a dynamic approach to organizing data–that has been popular with companies needing to store unstructured or rapidly changing data.</p>
<p>The term NoSQL has two meanings. In the beginning, it described databases that used mechanisms other than SQL to manage data. </p>
<p>There was “No SQL” used when accessing and manipulating data.</p>
<p>The definition has been expanded to mean, “Not Only SQL.” Some systems use SQL along with other technologies and query languages.</p>
<p>There are people that argue that the one thing all NoSQL databases have in common is that they’re non-relational and that a better name would be, “NoREL.” </p>
<p>Personally, I don’t think I have enough free time to care that much about it.</p>
<p>NoSQL databases, in general, share a few basic characteristics. </p>
<p>They are non-relational, open-source, schema-less, horizontally scalable, and do not adhere to ACID constraints.</p>
<p>Most NoSQL databases access data using their own Application Programming Interface, API. However, some NoSQL databases use a subset of SQL for data management.</p>
<p>In many cases, the non-relational model is a good fit for an application’s requirements. </p>
<p>The data might be unstructured or semi-structured. The amount of data might be impractical for a relational database. Or, the data might be of one single type and doesn’t need the controls that come with a relational database.</p>
<p>Being open source is not a requirement of NoSQL databases. It’s more of a NoSQL observation. There are many relational and non-relational databases that open source projects. However, the developers of NoSQL databases lean towards providing open-source solutions.</p>
<p>Most NoSQL databases have no fixed schema. </p>
<p>Relational databases require a schema to be designed before the database is created. NoSQL databases don’t. Instead, schemas can be created dynamically as data is accessed or embedded into the data itself.</p>
<p>NoSQL databases have a reputation for being more flexible with the data they can accept and support agile and DevOps philosophies.</p>
<p>NoSQL databases are often run in clusters of computing nodes.</p>
<p>Data is partitioned across multiple computers so that each computer can perform a specific task independently of the others.</p>
<p>Each node performs its task without having to share CPU, memory, or storage with other nodes.</p>
<p>This is known as a shared-nothing architecture.</p>
<p>Most NoSQL databases relax ACID constraints found in relational databases.</p>
<p>NoSQL solutions were developed around the purpose of providing high availability and scalability in a distributed environment.</p>
<p>To do this, either consistency or durability has to be sacrificed. By relaxing consistency, distributed systems can be highly available and durable. </p>
<p>Using a NoSQL approach, inconsistent data is expected. There’s no problem as long as it’s recognized and managed appropriately.</p>
<p>Currently, there is no standard query language that is supported by all NoSQL databases. </p>
<p>Some NoSQL databases have their own query language. Others use languages such as JavaScript, Java, Python, XQuery, and SPARQL.</p>
<p>NoSQL databases are a family of non-relational databases that include Key-Value Databases, Column Family Stores, Document Stores, and Graph Stores.</p>
<p>Key-Value databases are the simplest NoSQL data stores to use from an API perspective. Using a RESTful API, a client can get the value for the key, put a value for a key, or delete a key from the data store. </p>
<p>A Document Store Database is a database that uses a document-oriented model to store information. Each document contains semi-structured data that can be queried. Essentially, the schema for the data is built into the document, itself, and can change as needed. </p>
<p>Here is an example of a simple document store. It’s written in JSON, JavaScript Object Notation. What makes this different than a key-value store is that, for some of the values, there are nested key-value pairs that can be indexed and retrieved.</p>
<p>A Graph Store is a database that uses a graphical model to represent and store information. It has two primary components, Vertices and Edges.</p>
<p>Those are some of the types of NoSQL databases that are available, and I’ll eventually cover them in more detail, but why use them? What advantages do NoSQL databases have over relational databases?Scaling a NoSQL database is easier and less expensive than scaling a relational database because the scaling is horizontal instead of vertical. In general, for relational databases to scale, they must add memory, CPU, or storage. This is vertical scaling. However, NoSQL scaling is done by adding a compute or disk node. This is horizontal scaling. NoSQL databases generally trade consistency for performance and scalability.</p>
<p>Relational databases have four properties that support reliability. These properties, commonly referred to as ACID, are atomicity, consistency, isolation, and durability.</p>
<p>Consistency refers to the database’s state. In a relational database, a transaction takes a database from one valid state to another valid state. With most NoSQL databases, it’s possible for data to be inconsistent; a query might return old or stale data.</p>
<p>You might hear this phenomenon described as being eventually consistent. Over time, data that is spread across storage nodes will replicate and become consistent. What makes this behavior acceptable is that developers can anticipate this eventual consistency and allow for it. That said, some NoSQL databases do support strong consistency. </p>
<p>To review, NoSQL is a general term that refers loosely to a particular type of database model, or database management system.</p>
<p>NoSQL databases generally share a number of characteristics. They are Non-relational, databases, Open-source, Schema-less, and Horizontally Scalable.</p>
<p>Additionally, NoSQL databases do not generally adhere to the ACID principles found in relational databases and most do not use SQL to access data.</p>
<p>This is a good time to discuss the types of fully-managed NoSQL databases available from <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Or, it would be, but this is the end of this lecture.</p>
<p>In the next lecture, I’m going to describe, in some detail, the types of managed NoSQL database available on AWS. It won’t be overly technical. It’s a discussion, really, about what’s possible and how to start thinking about your data.</p>
<h1 id="Types-of-Managed-NoSQL-on-AWS-Part-1"><a href="#Types-of-Managed-NoSQL-on-AWS-Part-1" class="headerlink" title="Types of Managed NoSQL on AWS - Part 1"></a>Types of Managed NoSQL on AWS - Part 1</h1><p>Hello and welcome to this lecture about the types of managed NoSQL databases available on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. Managed services are those where the provisioning and maintenance is done by AWS according to your specifications on your behalf. </p>
<p>This lecture will serve as an overview of four types of managed NoSQL database technologies available on AWS. I will cover Key-Value stores, Document stores, Column Family stores, and In-Memory stores. In the next lecture, I will continue my overview and discuss Graph databases, Time Series databases, Ledger databases, and Search databases. </p>
<p>This lecture–as well as the one that follows it– will not cover implementation details. It is mostly a discussion of what’s possible.  </p>
<p>I’m going to provide an overview of each NoSQL technology, name the service from AWS that uses it, and provide a use case.</p>
<p>Let’s get started.</p>
<p>In a relational database, data is stored in tables composed of rows and columns. These tables and the types of data they’re going to store are defined prior to application development. This allows for storage and access patterns to be optimized.</p>
<p>It also means that relational databases are relatively inflexible. </p>
<p>Key-Value Databases, also called Key-Value Stores, are often considered to be the simplest type of NoSQL database. They are typically more flexible than relational databases and offer fast performance for reads and writes. </p>
<p>The AWS managed NoSQL database that is a Key-Value store is DynamoDB.</p>
<p>Key-Value stores are designed for storing, retrieving, and managing associative arrays and are well suited for working with large amounts of data.</p>
<p>An associative array, also known as a dictionary or a hash table, stores data with a unique identifier called a key. The data stored, which could be one or more items, is the value. </p>
<p>These are simple examples of key-value pairs. </p>
<p>It is also possible to store lists as the value.</p>
<p>Key-Value Stores have no schema that defines the structure of the data. There is only the key and its associated value.  </p>
<p>The key in a key-value pair must be unique. This is a unique identifier that allows access to the value associated with that key.</p>
<p>Before using a key-value store, it helps to have a naming convention for key names. It will help keep key-value stores consistent and minimize confusion.</p>
<p>The value in a key-value store can be text, numbers, a list of items, documents, or another key-value pair.</p>
<p>In key-value stores, data is stored and retrieved using operations such as get, put, and delete. </p>
<p>Queries to Key-Value Stores are simple. Lookups are based on the key and retrieval is often measured in milliseconds regardless of the size of the data returned.</p>
<p>Key-Value Stores are not optimized for search. It’s very expensive to scan in terms of time and cost. </p>
<p>They are not suitable for applications requiring frequent updates or complex queries involving specific data values. </p>
<p>There are several types of data and access patterns that are well suited for Key-Value Stores.</p>
<p>Web applications can store user profiles, shopping cart data, and preferences in a Key-Value Store. </p>
<p>Real-time recommendation engines and advertising systems are often powered by Key-Value Stores. </p>
<p>Key-Value Stores are commonly used for in-memory data caching. They can speed up applications by minimizing reads and writes to slower disk-based systems. </p>
<p>Binary objects, such as pictures and other multimedia items–can be stored in key-value databases. However, a better solution–in terms of time and cost–is to save binary files in object storage and use a key-value database for lookups.</p>
<p>DynamoDB is a key-value database. However, since it can store key-value pairs as a value, it is also a type of NoSQL Document database.</p>
<p>Document Databases were invented to store semi-structured data. Instead of having the structure defined as part of the database in advance–like a relational database–each document in the database has its own unique schema that defines its structure.</p>
<p>The AWS managed NoSQL document database service is Amazon DocumentDB. As a document database, Amazon DocumentDB is designed to store, query, and index JSON data.</p>
<p>Document Databases are similar to Key-Value Databases in that they also have a key and a value. The difference is that, in a Document Database, the value contains structured or semi-structured data. This structured&#x2F;semi-structured value is referred to as a Document.</p>
<p>Here is an example of a document that could be stored in a document database. It is written in JSON, JavaScript Object Notation.</p>
<p>In semi-structured data, there is no separation between the schema and the data. Each document stored has its own unique schema that defines what it contains.</p>
<p>The database engine uses this structure of the stored data to create metadata that is used for database optimization and queries.</p>
<p>Consider an application to track patient records in a doctor’s office. A patient–a person–does not fit in a relational database row.  There is no schema that can be used to describe every person on earth.</p>
<p>When visiting the doctor, data is generated and entered by multiple people. There’s insurance information, billing, height, weight, blood pressure, medications, and related information.</p>
<p>Defining a person’s medical history in rows is impractical and inefficient.</p>
<p>A more efficient way is to think of patient information as a collection of documents. At every appointment, a new document is added with updated information.</p>
<p>Document Stores scale horizontally. Data can be stored over multiple nodes that can number in the 1,000s.</p>
<p>One benefit that document store databases have over key-value databases is that, in a document store, the data inside the document can be queried.</p>
<p>This is different from a Key-Value store where a query returns the value in its entirety. </p>
<p>In a document store, queries can be run against the structure of a document as well as the elements inside it to return only the information required.</p>
<p>Document Databases have a variety of use cases. They are used in web applications, for managing user-generated content, shopping catalogs, gaming, and for storing sensor data from IoT devices.</p>
<p>Where a relational database uses rows to store similar types of data, a Column Store is a type of NoSQL database that stores data using a column-oriented model. </p>
<p>On AWS, the NoSQL column store available as a managed service is Amazon Keyspaces.  </p>
<p>Using columns allows the database to precisely access data needed to answer a query without having to scan each row in a table and discard unwanted items.</p>
<p>Column Store databases are also referred to as:</p>
<ul>
<li>Column databases</li>
<li>Column-Family databases</li>
<li>Column-Oriented databases</li>
<li>Wide-Column Stores</li>
<li>Columnar databases</li>
<li>Columnar stores</li>
</ul>
<p>A column store database uses a concept called a keyspace to define the data it contains.</p>
<p>A keyspace is similar to a relational database’s schema. The keyspace contains a collection of column families that look like tables from a relational database.</p>
<p>The column families contain rows and these rows contain columns.</p>
<p>A closer look at a column family shows:</p>
<p>A Column Family consists of multiple rows.</p>
<p>Each row can contain a different number of columns.</p>
<p>Each column is limited to its row.</p>
<p>Columns are kept in their own row. They do not span all rows like a relational database does. Each column contains a name-value pair along with a timestamp.</p>
<p>Here’s how each row is constructed. From left to right there is a row key and one or more columns. </p>
<p>The row key is a unique identifier for that row.</p>
<p>Each column contains a name-value pair and a timestamp.</p>
<p>The timestamp is the date and time the data was inserted. This is often used to determine the most recent version of the data.</p>
<p>Some Column-Family databases have composite columns that allow for objects to be nested inside a column.</p>
<p>Column stores are efficient doing data compression and partitioning.</p>
<p>Due to their structure, columnar databases excel at doing aggregation-type queries. That is, they can SUM, COUNT, and calculate AVG values easily.</p>
<p>Columnar databases scale well. They are suitable for workloads that do Massively Parallel Processing where data is spread across a large cluster of compute nodes that could number in the 1,000s.</p>
<p>Columnar stores can be loaded fast and efficiently. A one-billion row table can be loaded into a columnar store in seconds with queries and analysis starting almost immediately.</p>
<p>From an end-user perspective, the metadata in a columnar database looks and feels like a relational database. Some columnar database engines are SQL compliant and support the same controls that maintain the data’s state.</p>
<p>NoSQL databases tend to be either Key-Value type stores or Document stores. Columnar Store databases are neither.</p>
<p>Columnar databases are typically used with analytical applications, data warehousing, and Big Data processing.</p>
<p>In-Memory data stores are used by applications that require real-time access to data. Since the data is stored in memory, In-Memory stores provide microsecond latency to applications.</p>
<p>These stores are used as caches and the managed NoSQL service available from AWS is Amazon ElastiCache. </p>
<p>Amazon ElastiCache has two NoSQL In-Memory database engines; Redis and Memcached.</p>
<p>Before I go too much farther, I think it’s important to explain that a caching system is not a database. It is something that sits in front of a database to improve throughput. It also removes the need for putting a caching layer inside an application.</p>
<p>The primary purpose of an in-memory key-value store is to provide inexpensive access to data with sub-millisecond latency.</p>
<p>Most data stores have areas of data that are frequently accessed but rarely updated. Querying a database and getting the results from disk is always slower and more expensive than locating a key in a key-value pair cache. </p>
<p>Some relational database queries are expensive to perform. This might be a query that requires data from multiple tables or one that does a number of calculations before returning a result. </p>
<p>By caching query results, the cost of the query is only incurred once. The data can be returned multiple times without needing to run the query again.</p>
<p>An In-Memory data store keeps its entire dataset in RAM and is not stored on disk. The reward is speed. However, there is a downside. The risk when using an In-Memory store is that when a machine goes down the data is lost. </p>
<p>Some In-Memory stores, like Redis, are able to add persistence for recovery by saving a transaction log to disk and taking snapshots of datasets stored in memory.  </p>
<p>Cached data is stale data. It is important to know, before implementing an in-memory cache, if an application can tolerate stale data and, if it can, in what context.</p>
<p>As an example, if an application displays stock prices, customers might be willing to accept staleness with a disclaimer saying prices are delayed by 5 minutes. However, a stockbroker will want real-time data.</p>
<p>Caching should provide a speed or cost advantage. It doesn’t make sense to cache data that is dynamic or that is seldom accessed. </p>
<p>For caching to provide a benefit, data should be relatively static and frequently accessed like a personal profile on a social media site. </p>
<p>An in-memory store is well-suited to be a frond-end for relational databases and key-value stores. </p>
<p>It can provide a high-performance middle-tier for applications having high request rates or low-latency requirements.</p>
<p>In-memory stores can be used to cache session data, web pages, and leaderboards.</p>
<p>This has been a high-level overview of Key-Value stores, Document stores, Column Family stores, and In-Memory stores available on AWS. In my next lecture, I’m going to continue the discussion and cover Graph databases, Time Series databases, Ledger databases, and Search databases.</p>
<h1 id="Types-of-Managed-NoSQL-on-AWS-Part-2"><a href="#Types-of-Managed-NoSQL-on-AWS-Part-2" class="headerlink" title="Types of Managed NoSQL on AWS - Part 2"></a>Types of Managed NoSQL on AWS - Part 2</h1><p>Hello and welcome to this lecture about the types of managed NoSQL databases available on AWS. Managed services are those where the provisioning and maintenance are done by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> according to your specifications on your behalf.</p>
<p>This lecture is a continuation of the previous one and will serve as an overview and discussion of Graph databases, Time Series databases, Ledger databases, and Search databases.</p>
<p>Like the previous lecture, it will not cover implementation details. It is mostly a discussion of what’s possible.  </p>
<p>I will provide an overview of each NoSQL technology, name the service from AWS that uses it, and provide a use case.</p>
<p>Let’s get started.</p>
<p>A Graph database is a database that uses a graphical model to represent and store data about relationships. Relationship data is important for things such as building social networking applications, recommendation engines, doing fraud detection, creating knowledge graphs, and modeling the life sciences.</p>
<p>The AWS managed NoSQL graph database is Amazon Neptune. </p>
<p>Graph databases are composed of three elements, vertices, edges, and properties.</p>
<p>Vertices, also called nodes, are objects such as people or artifacts. Each node in a graph database has a unique identifier expressed in key-value pairs.</p>
<p>The singular of vertices is vertex. A vertex can represent data such as integers, string, people, locations, and buildings.</p>
<p>Edges represent the connection–or relationship–between two objects. Each edge is defined by a unique identifier that provides details about a starting or ending node along with a set of properties.</p>
<p>The vertices and edges can each have properties associated with them. This allows a graph database to depict complex relationships between otherwise unrelated data.</p>
<p>Here is a simple graph database.</p>
<p>The circles are the vertices and the arrows represent edges. Each edge has a property that defines the relationship.</p>
<p>The composer node, John Williams, has relationships with a number of movies. However, he’s got two relationships with the movie Schlinder’s List because he wrote the movie’s score and won an Academy Award for his work.</p>
<p>As more data is added to the database, the schema changes to match the relationships.</p>
<p>Many graph database management systems use their own proprietary query language. Some graph database systems support access using methods such as Gremlin from Apache TinkerPop, JavaScript, JSON, XQuery, and SPARQL. </p>
<p>Depending on the graph database, they can process either transactional or analytical workloads.</p>
<p>Graph databases can process large sets of user profiles and interactions to build social networking applications.</p>
<p>Graph databases can store relationships between customer interests, friends, and purchase history to create recommendations.</p>
<p>Use graph databases to process financial and purchase transactions in near real-time to detect fraud patterns.</p>
<p>A knowledge graph stores information in a graph model and uses graph queries to enable the navigation of highly connected datasets.</p>
<p>Use a knowledge graph to add topical information to product catalogs, build and query complex models of regulatory rules, or model general information.</p>
<p>Graph databases can be used to create applications that store and navigate the life sciences.</p>
<p>Use a graph database to map a computer network and answer questions about hosts and application usage. If a malicious file is on a host, a graph database could be used to find the connection between the hosts that spread the malicious file and trace it back to the host that downloaded it.</p>
<p>Time-series Databases efficiently collect, synthesize, and derive insights from data that changes over time.</p>
<p>The AWS managed NoSQL database for time-stream data is Amazon Timestream.</p>
<p>In a Time-Series Database data is collected at regular intervals as the value and is stored with the time as the key. </p>
<p>While it’s possible to retrieve a single item from time-series data–like the price of an item–computation is usually applied over a range of time data to return a result.</p>
<p>The primary purpose of a Time-Series Database is to provide answers. A query will process a range of data, do the appropriate computations, and return the results.  </p>
<p>For example, determining the MIN, MAX, and AVG of CPU utilization on a database server over the past seven days.</p>
<p>Time-series databases are ideal for DevOps applications that collect data millions of times per second and analyze that data in real-time to improve application performance and availability. </p>
<p>Use Time-Series databases to quickly analyze time-series data generated by IoT applications using analytic functions such as smoothing, approximation, and interpolation. </p>
<p>Time stream databases can be used to store and analyze clickstream data to understand user activity across applications over a period of time. </p>
<p>Use a Time Stream database to store and analyze time-series data for industrial equipment maintenance, trade monitoring, fleet management, and route planning.</p>
<p>Ledger Databases provide a centralized and trusted authority to maintain a scalable, immutable, and cryptographically verifiable record of transactions for an application.</p>
<p>The AWS managed NoSQL ledger database is the Amazon Quantum Ledger Database.</p>
<p>These databases maintain their trust, in part, by being fully auditable and transparent. All transactions are recorded in a log to track activity.</p>
<p>QLDBs are immutable. This means that the data in the database remains unchanged once saved. Instead, the action of updating data creates a new version of the record. Changes to the database do not overwrite existing database records.</p>
<p>Cryptographic verification is used to ensure data is immutable. When a record is committed, a hash is created by the database.</p>
<p>Hashing is an algorithm performed on data to produce a number called a checksum or hash. This hash is used to verify that data has not been modified, tampered with, or corrupted. </p>
<p>No matter how many times the hashing algorithm is run against the data, the hash will always be the same when the data is the same.</p>
<p>Quantum Ledger Databases use blockchain technology when creating hashes. This means they use two pieces of information to create a hash value; the record data and the hash of the previous record. This ensures that the entire chain of records is valid.</p>
<p>Anyone can create an audit log to show how data is used, but how can they legally prove that the data has not been altered? </p>
<p>Even with the best user interfaces and audit tracking, a skilled programmer can change electronic records without leaving a trace.</p>
<p>Blockchains can be used to build trust and ensure policy, governance, and regulation of data processes. </p>
<p>Banks often need a centralized ledger-like application to keep track of critical data such as credit and debit card transactions. </p>
<p>Instead of building a custom ledger with complicated auditing functionality, a ledger database can easily store an accurate and complete record of financial transactions.</p>
<p>Manufacturing companies have a need to reconcile data between supply chain systems to track the manufacturing history of a product. </p>
<p>A ledger database can be used to record the history of each transaction and provide the details of each individual batch of a product manufactured at a facility.</p>
<p>Insurance applications need a way to track the history of claims.</p>
<p>Instead of building complex auditing functionality using relational databases, insurance companies can use a ledger database to maintain the history of claims.</p>
<p>When conflicts arise, a ledger database can cryptographically verify the integrity of the claims data.</p>
<p>HR systems have to track and maintain a record of employee details such as payroll, bonus, benefits, performance history, and insurance. </p>
<p>By implementing a system-of-record application using a ledger database, companies can easily maintain a trusted and complete record of the digital history of employees in a single place.</p>
<p>Retailers need to access information on each stage of a product’s supply chain.</p>
<p>With a ledger database, retail companies can track the full history of inventory and supply chain transactions.</p>
<p>Search engines help people find the information they need. Search databases are optimized to store and retrieve search-related data and typically offer specialized methods such as full-text search, complex search expressions, and the ranking of search results.</p>
<p>The managed NoSQL offering from AWS is the Amazon Elasticsearch Service.</p>
<p>Search databases securely ingest unstructured data from multiple locations, store and index it, and make it searchable.</p>
<p>Data ingestion is the process of taking raw data from a variety of sources, then parsing, normalizing, and enriching it.</p>
<p>The raw data sources include logs, system metrics, and web applications.</p>
<p>Once ingested, the data is indexed inside the search database. An index is a collection of documents that are related to each other. The search database, Elasticsearch, stores indexes as JSON documents. Each document has a set of keys that have corresponding values.</p>
<p>Elasticsearch uses a data structure called an inverted index that provides fast full-text searches.</p>
<p>An inverted index lists every unique word that appears in any document and identifies all of the documents where each word occurs.</p>
<p>Search databases can be used to provide a fast, personalized search experience for applications, websites, and data lake catalogs.</p>
<p>A real estate business could use a search database to help people find homes in a desired location, at a chosen price range, from among thousands of properties. </p>
<p>Search databases can be used to store, analyze, and correlate application and infrastructure log data to find and fix issues.</p>
<p>Use search databases to analyze network and systems logs for real-time threat detection and incident management. </p>
<p>Well, that covers the types of fully-managed NoSQL databases available on AWS. It was a fair amount of information. However, it should give you an idea of what’s possible in the AWS cloud.</p>
<p>In my next lecture, I’m going to do a quick summary of relational and non-relational databases, the AWS fully-managed options, and their use cases. </p>
<p>I’ll also give you some options for your next steps; depending on your needs and interests.</p>
<p>This has been a high-level overview of Graph databases, Time Series databases, Ledger databases, and Search databases. </p>
<p>NoSQL databases let developers divide complex applications into manageable pieces and create rich experiences for customers.</p>
<p>In the final lecture in this series, I’ll summarize what I’ve covered about the managed database types on AWS and give you an idea of what next steps you should take.</p>
<p>Thanks for watching.</p>
<h1 id="Amazon-Relational-Database-Service"><a href="#Amazon-Relational-Database-Service" class="headerlink" title="Amazon Relational Database Service"></a>Amazon Relational Database Service</h1><p>Hello and welcome to this lecture, where I shall be discussing the first of the AWS database services that I will be covering in this <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/course-introduction/">course</a>, the Amazon Relational Database Service, commonly known as RDS. I will look at a number of different common features of the service to give you a general idea of how it’s configured. So as the name suggests, this is a relational database service that provides a simple way to provision, create, and scale a relational database within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. It’s a managed service, which takes many of the mundane administrative operations out of your hands, and it’s instead managed by AWS, such as backups and the patching of both the underlying operating system in addition to the database engine software that you select.</p>
<p>Amazon RDS allows you to select from a range of different database engines. These currently include MySQL, and this is considered the number one open source relational database management system. MariaDB. This is the community-developed fork of MySQL. PostgreSQL. This database engine comes in a close second behind MySQL as the preferred open source database. Amazon Aurora. Amazon Aurora is AWS’s own fork of MySQL, which provides ultrafast processing and availability, as it has its own cloud-native database engine. Oracle. The Oracle database is a common platform in corporate environments. And SQL Server. This is a Microsoft database with a number of different licensing options.</p>
<p>In addition to so many different database engines, you also have a wide choice when it comes to selecting which compute instance you’d like to run your database on. The varying different options offer different performance and allowed to architect your environment based on your expected load. When you create your RDS database, you must select an instance to support your database from a processing and memory perspective, as shown here in this screenshot, using the MySQL database engine. Currently, these are the different instance types available to you for each of the database engines, which are categorized between general purpose and memory-optimized.</p>
<p>For a breakdown of the performance of each of these instance types, please refer to the following <a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">AWS documentation</a>. For each of these instance types, you also have various instance sizes, each equating to a different performance level from a vCPU and memory perspective. For example, when looking at the T3 instance, we have the following sizes available.</p>
<p>You can deploy your RDS instance in a single availability zone. However, if high availability and resiliency is of importance when it comes to your database, then you might want to consider a feature known as Multi AZ, which stands for multi availability zones. When Multi AZ is configured, a secondary RDS instance is deployed within a different availability zone within the same region as the primary instance. The primary purpose of the second instance is to provide a failover option for your primary RDS instance. The replication of data between the primary RDS database and the secondary replica instance happens synchronously.</p>
<p>Let’s look at how Multi AZ would work in a production environment. If you have configured Multi AZ and an incident occurs which causes an outage to the primary RDS instance, then the RDS failover process takes over automatically. This process is managed by AWS, and it’s not something that you need to manually perform or trigger. RDS will update the DNS record to point to the secondary instance. This process can typically take between 60 and 120 seconds. The length of time is very dependent on the size of the database, its transactions, and the activity of the database at the time of failover. This automatic changeover enables you to continue using the database without the need of an engineer making any changes to your environment. The failover process will happen in the following scenarios. If patching maintenance has been performed in the primary instance, if the instance of the primary database has a host failure, if the availability zone of the primary database fails, if the primary instance was rebooted with failover, and if the primary database instance class on the primary database is modified.</p>
<p>As you can see, activating Multi AZ is an effective measure and precaution to implement to ensure you have resiliency built in should an outage occur. For detailed information on RDS Multi AZ, please refer to our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">here</a>.</p>
<p>Over time, your workloads on your database will fluctuate. And so how can you optimize your RDS database to ensure it is capable of meeting the demands of your load, both from a storage and compute perspective? When it comes to scaling your storage, you can use a feature called storage autoscaling. MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server all use Elastic Block Store, EBS volumes, for both data and log storage. However, Amazon Aurora uses a shared cluster storage architecture and does not use EBS. The database engines that use EBS support general purpose SSD storage, provisioned IOPS SSD storage, and magnetic storage.</p>
<p>The general purpose SSD storage is a good option for a broad range of use cases which provides single-digit millisecond latencies and offers a cost-effective storage solution. The minimum SSD storage volume for your primary dataset is 20 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle. However, the maximum for SQL Server is 16 tebibytes. Provisioned IOPS SSD. Now, this option is great for when you have workloads that operate at a very high I&#x2F;O. You can provision a minimum of 8,000 IOPS and a maximum of 80,000 for MySQL, PostgreSQL, MariaDB, and Oracle, but the maximum for SQL Server is 40,000.</p>
<p>In addition to being able to provision the IOPS needed for your workload, the minimum storage for your primary dataset is a 100 gibibytes with a maximum of 64 tebibytes for MySQL, PostgreSQL, MariaDB, and Oracle, and 16 tebibytes for SQL Server. Magnetic storage is simply supported to provide backwards compatibility. And so instead, AWS recommends that you select general purpose. The following screenshot shows the configuration screen when setting your storage requirements during the creation of a MySQL RDS database. </p>
<p>n this example, general purpose has been selected as the storage with a minimum of 100 gibibytes of primary storage. Under the storage autoscaling section, I’ve enabled the feature with the checkbox and set a maximum storage threshold of 1000 gibibytes. This means that one storage will start at 100 gibibytes when the database is created and the storage will automatically scale with demand up to a maximum of 1000 gibibytes without you having to intervene in any way. The maximum storage threshold can be set at 65,536 gibibytes. Aurora doesn’t use EBS and instead uses a shared cluster storage architecture which is managed by the service itself.</p>
<p>When configuring your Aurora database in the console, the option to configure and select storage options like we saw previously does not exist. Your storage will scale automatically as your database grows. To scale your compute size, which is effectively your instance, is easy to do in RDS both vertically and horizontally. Vertical scaling will enhance the performance of your database instance. For example, scaling up from an m4.large to an m4.2xlarge. Horizontal scaling will see an increase in the quantity of your current instance. For example, moving from a single m4.large to three m4.large instances in your environment through the means of read replicas.</p>
<p>At any point you can scale your RDS database vertically, changing the size of your instance. When doing so, you can select to perform the change immediately or wait for a scheduled maintenance window. For horizontal scaling, read replicas can be used by application and other services to save read only access to your database data via a separate instance. So, for example, let’s assume we have a primary RDS instance which serves both read and write traffic. Due to the size of the instance and the amount of read-intensive traffic being directed to the database for queries, the performance of the instance has taken a hit. So to help resolve this, you can create a read replica. </p>
<p>A snapshot will be taken of your database, and if you’re using Multi AZ, then this snapshot will be taken of your secondary database instance to ensure that there are no performance impacts during this process. Once the snapshot is complete, a read replica instance is created from this data. The read replica then maintains a secure asynchronous link between itself and the primary database. At this point, read only traffic can be directed to the read replica to serve queries. Before implementing read replicas, please check with the latest AWS documentation to identify database engine read replica compatibility.</p>
<p>As I mentioned previously, many of the administrative tasks for RDS are taken care of by AWS. For example, patching and automated backups. As Amazon RDS is a managed service, and from a shared responsibility model is considered a container service where you have no access to the underlying operating system on which your database runs on. As you can see, both platform and application management and operating systems falls under the realm of AWS responsibilities. As a result, AWS is responsible for both the patching of the operating system and any patching for the database engine themselves. More information on the AWS shared responsibility model can be found in my existing blog post found <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">here</a>.</p>
<p>From a backup perspective, by default, Amazon RDS provides an automatic feature seen here. This is enabled on all new RDS databases, which backs up your RDS instance to Amazon S3. You are able to configure the level of retention in days from zero to 35 and implement a level of encryption using the key management service, or KMS. More information on KMS can be found in our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>You can also perform manual backups anytime you need to, which are known as snapshots. However, these snapshots are not bound by the retention period set in the automatic backup configuration and are only deleted through a manual process. When using a MySQL-compatible Aurora database, you can also use a feature called backtrack, and this allows you to go back in time on the database to recover from an error or incident without having to perform a restore or create another database cluster.</p>
<p>As you can see from the configuration page during the Aurora database creation process, it is enabled via a checkbox and allows you to enter a number of hours of how far you would like to backtrack to with a maximum of 72 hours. In this example, I’ve entered 12 hours, and so Aurora will retain log data of all changes for 12 hours as specified. We’ve now covered the fundamentals of Amazon RDS and some of the key features to be aware of. If you’d like to get some hands-on experience of using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/demo-creating-amazon-rds-database-2/">Amazon RDS</a>, feel free to check out the following labs.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="DEMO-Creating-an-Amazon-RDS-Database"><a href="#DEMO-Creating-an-Amazon-RDS-Database" class="headerlink" title="DEMO: Creating an Amazon RDS Database"></a>DEMO: Creating an Amazon RDS Database</h1><p>Hello, and welcome to this lecture, which will be a demonstration on how to create an RDS database. So let’s get straight into it. So as you can see, I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console. And the first thing I need to do is to go to RDS. Now you can find RDS under the Database category, and you can see that it’s the first database option. So if you select the RDS service, and this is the dashboard for <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/amazon-relational-database-service-2/">Amazon RDS</a>. As you can see, I don’t have any database instances running. So let’s go ahead and create our first database.</p>
<p>So under the Create database section here, we can either restore an existing database from Amazon S3, from a backup, or we can create a new database. For this demonstration, I’m going to create a new database. So let’s click that option. Now, firstly, we need to choose a database creation method. We can either do a standard create or an easy create. Now the standard create allows us to set more-configurable options. So for this demonstration, that’s the option I’m going to select.</p>
<p>Then we have our database engine types. As I explained previously, we have Amazon Aurora, MySQL, MariaDB, Postgres, Oracle, and Microsoft SQL Server. I’m just going to select MySQL for this demonstration. So scrolling down again, we can then select our version of MySQL, whichever version we’d like. I’ll just leave it as the default option. And then we have something called Templates.</p>
<p>Now, depending on what template we select here will predefine a list of other configurable components. So here that’s highlighted is Production and this uses defaults for high availability and fast and consistent performance. The Dev&#x2F;Test template is intended for development use outside of a production environment. And the Free Tier is simply to allow you to get hands-on experience with RDS and doesn’t really use many of the features. But I want to show you the full feature set. So I’m going to select Production.</p>
<p>Now, if we go down to Settings. Here, we can enter a database instance identifier. So this is the name of the database instance, not the actual name of a database table. So I’m happy to call it database one, just leave that as default. Now we have our credentials. Now we have a master user name to connect to the database instance, so we can have admin. We can either auto-generate a password or we can select our own. So I’ll just go ahead and enter my own one.</p>
<p>Next, we have the option to select the database instance size. So we have our Standard classes or Memory Optimized or even Burstable classes. So I’m going to select the Standard class and using this dropdown, we can select the size of the instance that we want. And as you can see, there’s a different number of VCPUs and RAM. And so I’m just going to select the smallest instance size.</p>
<p>Now, if we go down to Storage, we can select our storage type. So we have General Purpose or Provisioned IOPS. If we look at the Provisioned IOPS, we can define the allocated storage and then also the number of IOPS as well, which is the input and output operations per second. For this demonstration, I’m just gonna leave it as General Purpose. I’ll just accept the storage defaults there of 20. Here we have Storage autoscaling. If we want to enable that or not, it’s just a simple tick box. RDS will automatically scale up to whatever value we put in here. So for example, 100 gig. That will give us the flexibility of starting at 20 and scaling all the way up to 100 automatically. </p>
<p>Now, if we go down to Availability and durability. Here, we have a Multi-AZ deployment. So it’s enabling this, will create another standby instance in a different availability zone to create high availability and data redundancy. For this demonstration, I’m just gonna leave it as a single AZ deployment. So I’m gonna select Do not create a standby instance. If we go down to Connectivity, we can select the VPC that we’d like this RDS instance to reside in. And you can see here, after a database is created, you can’t change the VPC selection. If you expand this option here for additional connectivity configuration, we can see a few additional options.</p>
<p>So we can select a database subnet group. And this simply defines which subnets and IP ranges the database instance can use in the VPC. Have an option here, if the database should be publicly accessible or not. If you select yes, then it will be issued a public IP address and devices and instances outside of your VPC will be able to try and connect to your database, if the VPC security groups allow it. For this demonstration, I’m just going to keep it a private RDS database, so it won’t assign any kind of public IP address. And only instances inside the VPC will be able to connect. Here, we can choose our security group, which will essentially define which resources can talk to our RDS instance.</p>
<p>Now, if we select an existing, then we can use this dropdown box here to select which security group that we’d like to use. I’m just going to select the default. I haven’t set any kind of security groups up for this as this is just a demonstration, but that’s where you would apply your security groups for your RDS instance. And as you can see, it’s added it in there. If you’d like to deploy your RDS instance in a particular availability zone, then you can select one. If you have no preference, simply select no preference. And also what port the database will be using.</p>
<p>If you go down to Database authentication. We have two options here for MySQL. Password authentication. Now this will allow anyone to connect to the instance just using the database passwords. If you want it more secure, then you can use the database password in addition to verifying that the user has permissions to access the RDS database, through permissions that were assigned directly to the user or group or role. So that just offers an additional level of security. If we go down further to Additional configuration, we can configure additional options.</p>
<p>So here we have our database options. You can enter an initial database name that will run on your database instance. Let’s just say my database. You can select a parameter group. Parameter groups is essentially a grouping of configurable parameters that operate at the database engine level. You’re able to create different parameter groups that contain different settings for the same database engine, depending on your use case and how you’d like these parameters to be configured. Now the parameter group itself sits outside of the database, and this means that the same parameter group can be applied to multiple databases. So if you update the values within the parameter group, then this will update all the databases that use that same parameter group. Depending on which database engine you select, you are able to select an option group. And these option groups allow you to configure additional features to help you manage and secure your databases. Again, like parameter groups, they sit outside of the database itself.</p>
<p>Here we have our Backup section, so we can enable our automatic backups. If you don’t want this, you can simply un-tick it, but it’s pretty useful, so I tend to leave that enabled. And here we have our backup retention period. And you can select the number of days, up to 35 days. Just leave that at seven. We have a backup window. We can select a window, select in the time and the duration. So if you have a particular time that you’d like to run your backups, you can simply add in the hours and minutes, and also how long it should run for. If you don’t have a specific window, you can simply select no preference. If you have any tags for your database, you can copy that to your backup snapshots as well.</p>
<p>When it comes to encryption, you can either encrypt your database. The default is to have your database encrypted, and then you can select your key here. Now, this is the default AWS managed key for RDS, which is used by KMS, the key management service. or you can select your own CMK, your own customer master key, if you have a different one yourself. I’m just gonna leave it as the default AWS RDS managed key.</p>
<p>Down here, we have performance insights. Performance insights allows you to implement a level of performance tuning and monitoring, which enables you to see and review the load on your database, and if any actions should be taken. Here we can make some additional monitoring changes. We can enable enhanced monitoring, and we can set the granularity of this monitoring from anything from 60 seconds to one second. I’ll leave that as a default of 60 seconds for enhanced monitoring. And I’m just going to leave RDS to create the default role for that enhanced monitoring.</p>
<p>We can export our logs to Amazon CloudWatch Logs. Either the error, the general or the slow query log or all of them, or any combination. So if you want to export any of your logs to CloudWatch Logs for additional monitoring and queries then you can do so.</p>
<p>Then we have maintenance. We can enable or disable auto minor version upgrade. And here we can see that by enabling auto minor version upgrade, it will automatically upgrade to new minor versions as they are released. And the automatic upgrades occur during the maintenance window that we’ve scheduled for the database.</p>
<p>Now, speaking of maintenance window, we can select one here. So we can select a window. We can say on a particular day, that’s good for us, Saturday at four o’clock in the morning for two hours, for example, that could be our maintenance window. So if there’s any auto minor version upgrades to take place, then these will be scheduled during our maintenance window. And then finally you have deletion protection. And this simply prevents a database from being deleted accidentally.</p>
<p>Now at the very bottom, it has your estimated monthly costs. So we can see the cost of the database instance and also for the storage. Once you’re happy with all your options, simply click Create database. And now we can see that here’s our database instance, and we can see the status is Creating. And we have a message up here saying that the database might take a few minutes to launch. Okay, we now have a message that says the database has successfully been created. And it’s as simple as that.</p>
<h1 id="Amazon-DynamoDB"><a href="#Amazon-DynamoDB" class="headerlink" title="Amazon DynamoDB"></a>Amazon DynamoDB</h1><p>Hello and welcome to this lecture covering Amazon DynamoDB. Amazon DynamoDB is a NoSQL database, which means that it doesn’t use the common Structured Query Language, SQL. It falls into a category of databases known as key-value stores. A key value store is simply a collection of items or records, and you can look up data by using a primary key for each item or through the use of indexes.</p>
<p>Amazon DynamoDB is designed to be used for ultra high performance, which could be maintained at any scale with single-digit latency, making this a very powerful database choice used commonly for gaming, web, mobile and IoT applications to name but a few. Much like Amazon RDS, DynamoDB is also a fully managed service, taking many of the day-to-day administration operations out of your hands, giving you more time to focus on the business logic of your database. That’s one of the great things about Amazon DynamoDB, there’s no database administration required by us as a customer, no service to manage and nothing to back up. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> handles all of this for you. This makes the creation of a DynamoDB database very easy. All you have to do is set up your tables and configure the level of provision throughput that each table should have. Provision throughput refers to the level of read and write capacity that you want AWS to reserve for your table. You are charged for the total amount of throughput that you configure for your tables plus the total amount of storage space used by your data.</p>
<p>If we actually look at the configuration screen when creating a new DynamoDB database, as seen here, you can see that there are very few options required to create a new database. And in fact, in its simplest form, you can just provide a table name and a primary key, which is used to partition data across hosts for scalability and availability. You can then accept any remaining defaults and create your database, it’s as simple as that. DynamoDB tables are considered schemaless because there’s no strict design and schema that every record must conform to. As long as each item has an appropriate primary key, the item can contain varying sets of attributes. The records in a table do not need to have the same attributes or even the same number of attributes. This can be very convenient for rapid application development and if you want to add a new column to a table, you don’t need to alter the table, you can just start including the new field as an attribute when you insert new records. Likewise, you never need to adjust the data type for a column as DynamoDB generally isn’t interested in data types for individual attributes.</p>
<p>If when creating your DynamoDB database, you choose not to reset all the defaults, what other options exist? Let’s take a look. Unchecking the use default settings from the Table settings section provides you with the following. Firstly, you’ll be asked about secondary indexes, which allow you to perform queries on attributes that are not part of the table’s primary key. The default option provides no secondary index. However, you can add them here if required. DynamoDB lets you create additional indexes so that you can run queries to search your data by other attributes. If you’ve worked with relational databases, you’ve probably used indexes with those, but there are a couple of big differences in how indexes operate in DynamoDB.</p>
<p>First, each query can only use one index. If you want to query and match on two different columns, you need to create an index that can do that properly. Second, when you write your queries, you need to specify exactly which index should be used for each query. It’s not like a relational database that has a query analyzer, which can decide which indexes to use for our query. Here you need to be explicit and tell DynamoDB what index to use. DynamoDB has two different kinds of secondary indexes, global indexes let you query across the entire table to find any record that matches a particular value and by contrast, local secondary indexes can only help find data within a single partition key.</p>
<p>Following secondary indexes, you can modify the default settings applied to your table’s read&#x2F;write capacity mode. When you create a table in DynamoDB, you need to tell AWS how much capacity you want to reserve for the table. You don’t need to do this for disk space as DynamoDB will automatically allocate more space for your table as it grows. However, you do need to reserve capacity for input and output for reads and writes. Amazon charges you based on the number of read capacity units and write capacity units that you allocate. It’s important to allocate enough for your workload, but don’t allocate too much or DynamoDB could become prohibitively expensive.</p>
<p>By default, when you create a table in the AWS Console, Amazon will configure your table with five read capacity units and five write capacity units. There are two modes that you can choose from, provisioned and on-demand. Provisioned mode allows you to provision set read and writes allowed against your database per second by your application and is measured in capacity units, RCUs for reads and WCUs for writes. Depending on the transaction, each action will use one or more RCUs or WCUs. Provisioned mode is used generally when you have a predicted and forecasted workload of traffic. On-demand mode does not provision any RCUs or WCUs, instead they are scaled on demand. The downside is that it is not as cost effective as provisioned. This mode is generally used if you do not know how much workload you are expected to experience. Over time, you are likely to get more of an understanding of load and you can change your mode across to provisioned.</p>
<p>Once you have selected the provisioned mode, you will then have the opportunity to add configuration information relating to how your RCU and WCU are scaled as demand increases and decreases. As you can see, by entering your minimum and maximum provisioned capacity along with your target threshold utilization as a percentage, you can confidently rely on Amazon DynamoDB to manage the scaling operations of your throughput.</p>
<p>The last main point of the configuration allows you to set encryption of your tables, which is enabled by default for data at rest. Through the use of the key management service, KMS, you are able to select either a customer managed or AWS managed CMK to use for the encryption of your table instead of the default keys used by DynamoDB. For more information on CMKs and the key management service in general, please refer to our existing course found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Before I finish this lecture covering DynamoDB, I just want to cover some of its advantages and also what can be considered disadvantages. Some of the advantages of DynamoDB is that it’s fully managed by AWS, you don’t have to worry about backups or redundancy, although you’re welcome to set up these kinds of safeguards using some more advanced DynamoDB features.</p>
<p>As mentioned previously, DynamoDB tables are schemaless so you don’t have to define the exact data model in advance, the data model can change automatically to fit your application’s needs.</p>
<p>DynamoDB is designed to be highly available and your data is automatically replicated across three different availability zones within a geographic region. In the case of an outage or an incident affecting the entire hosting facility, DynamoDB transparently routes around the affected availability zone.</p>
<p>DynamoDB is designed to be fast, read and writes take just a few milliseconds to complete and DynamoDB will be fast no matter how large your table grows, unlike relational database, which can slow down as the table gets large. DynamoDB performance is constant and stays consistent even with tables that are many terabytes in size. You don’t have to do anything to handle this, except adjusting the provisioned throughput levels to make sure you’ve preserved enough read and write capacity for your transaction volume.</p>
<p>There are also some downsides to using DynamoDB too. As I just mentioned, your data is automatically replicated. Three copies are stored in three different availability zones and that replication usually happens quickly in milliseconds, but sometimes it can take longer and this is known as eventual consistency. This happens transparently and many operations will make sure that they’re always working on the latest copy of your data, but there are certain kinds of queries and table scans that may return older versions of data before the most recent copy. You need to be aware of how this works and you may need to adjust certain queries to require strong consistency.</p>
<p>DynamoDB’s queries aren’t as flexible as what you can do with SQL. If you are used to writing advanced queries with joins and groupings and summaries, you won’t be able to do that with DynamoDB. You’ll have to do more of the computation in your application code. This is done for performance reasons to ensure that every query finishes quickly and that complicated queries can’t hog the resources on a database server. </p>
<p>DynamoDB also has some strict limitations in the way you’re allowed to work with it. Two important limitations are the maximum record size of 400 kilobytes and the limit of 20 global indexes and five secondary indexes per table. There are other limitations that can be adjusted by contacting AWS customer support like the maximum number of tables in an AWS account.</p>
<p>Finally, although <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/demo-creating-dynamodb-database-2/">DynamoDB</a> performance can scale up as your needs grow, your performance is limited to the amount of read and write throughput that you’ve provisioned for each table. If you expect a spike of the database use, you’ll need to provision more throughput in advance or database requests will fail with a ProvisionedThroughputExceededException message. Fortunately, you can adjust throughput at any time and it only takes a couple of minutes to adjust. Still, this means that you’ll need to monitor the throughput being used in each table or you’ll risk running out of throughput if your usage grows.</p>
<h1 id="DEMO-Creating-a-DynamoDB-Database"><a href="#DEMO-Creating-a-DynamoDB-Database" class="headerlink" title="DEMO: Creating a DynamoDB Database"></a>DEMO: Creating a DynamoDB Database</h1><p>Hello, and welcome to this lecture. This is going to be a demonstration on how to quickly, and easily create a DynamoDB database. Now, first I’ll need to go to the database category, and here we can see <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-amazon-rds-dynamodb-1062/amazon-dynamodb-2/">DynamoDB</a>. Now I don’t have any DynamoDB databases sets up yet. So if you select create table, and you’ll be presented with this screen.</p>
<p>So first we’ll need to give it a table name, just call this my database and also a primary key. Now the primary key is essentially used to uniquely identify each item in the table. And the primary key is essentially comprised of a partition key. So let me just add one in. I’ll just call this product ID, and we can slate either a string, binary, or a number. I’ll leave that as a string. If we need to, we can also add in a sort key as well. And as we can see here the sort key simply allows you to search within a partition. Just remove that sort key.</p>
<p>Now essentially you can now create your table simply from providing that information, because this tick box here allows you to use lots of default settings that essentially fills in the rest of the configuration for you. So if you’re happy with your table name and primary key, with these default settings for your table, you can simply click create and it’s done. However, I want to uncheck the default settings so you can see the different configurable components used. So let’s take a look.</p>
<p>Now, firstly we have our secondary indexes, so you can add a secondary index, and these allow you to perform queries on attributes that are not part of the table’s primary key. Next, we have our read and write capacity mode, either provisioned or on demand. If we select the provisioned capacity mode, then we can select our read capacity units, and also our write capacity units.</p>
<p>Now scrolling down a bit further to auto scaling. We can set up auto scaling for our read and write capacity units. So when the read capacity gets to 70% utilization, we can scale up to a maximum of 40,000 units, and the same with the write capacity. So you can alter these figures if you need to, and change them to whatever values you need. As a part of that auto scaling process, DynamoDB needs an auto-scaling service link role to give it permission to do so.</p>
<p>Once you’re happy with your read and write capacity units, we can then scroll down to encryption at rest. Now by default encryption is enabled. The default option uses a key that’s owned by DynamoDB, and you are not charged for the use of any encryption keys in this default setting. However, you can use a KMS custom managed CMK, which is the CMK that you may have created, and you can select it from this box here, if you have any and enter the ARN, or you can use the KMS <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> managed CMK, which is this key here.</p>
<p>So it depends on the level of control that you want for the encryption key. First demonstration I’m just gonna leave it as the default. At the bottom here, you can add any text to a database if you’d like. So once you’re happy with the configuration, you simply click on create. As we can see the table is being created. And once it’s created, you can then use these tabs along the top to set up any alarms and review your capacity units set up your indexes, backups, etc, etc, etc. But for this demonstration, I simply wanted to show you how quickly and easy it is to set up and configure a DynamoDB table.</p>
<h1 id="Amazon-Redshift"><a href="#Amazon-Redshift" class="headerlink" title="Amazon Redshift"></a>Amazon Redshift</h1><p>Hello, and welcome to this lecture where I will look at Amazon Redshift. Amazon Redshift is a fast, fully-managed, petabyte-scale data warehouse. And it’s designed for high performance and analysis of information capable of storing and processing petabytes of data and provide access to this data, using your existing business intelligence tools, using standard SQL. It operates as a relational database management system, and therefore is compatible with other RDBMS applications. Redshift itself is based upon PostgreSQL 8.0.2, but it contains a number of differences from PostgreSQL. These differences are out of scope for this course, but for more information, please refer to the documentation <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">here</a>.</p>
<p>A data warehouse is used to consolidate data from multiple sources to allow you to run business intelligent tools, across your data, to help you identify actionable business information, which can then be used to direct and drive your organization to make effective data-driven decisions to the benefit of your company.</p>
<p>As a result, using a data warehouse is a very effective way to manage your reporting and data analysis at scale. A data warehouse, by its very nature, needs to be able to store huge amounts of data and its data may be subjected to different data operations such as data cleansing, which as an example, may identify, correct, replace or remove incomplete records from a table or recordset.</p>
<p>This can be expanded upon for the need to perform an extract, transform and load or an ETL job. This is the common paradigm by which data from multiple systems is combined to a single database data store or warehouse for legacy storage or analytics.</p>
<p>Extraction is the process of retrieving data from one or more sources. Either online, brick &amp; mortar, legacy data, Salesforce data and many others. After retrieving the data, ETL is to compute work that loads it into a staging area and prepares it for the next phase.</p>
<p>Transformation is the process of mapping, reformatting, conforming, adding meaning and more to prepare the data in a way that is more easily consumed. One example of this is the transformation and computation where currency amounts are converted from US dollars to euros.</p>
<p>Loading involves successfully inserting the transform data into the target database data store, or in this case, a data warehouse. All of this work is processed in what the business intelligent developers call an ETL job.</p>
<p>Now we have an understanding of what Amazon Redshift is. Let’s move on to looking at the architecture of the service and the components that is built upon.</p>
<p>Let me start with clusters and nodes. A cluster can be considered the main or core component of the Amazon Redshift service. And in every cluster, it will run its own Redshift engine, which will contain at least one database. As the name implies, a cluster is effectively a grouping of another component, and these being compute nodes.</p>
<p>Each will contain at least one compute node. However, if the cluster is provisioned with more than one compute node, then Amazon Redshift will add another component called a leader node.</p>
<p>Compute nodes all contain their own quantity of CPU attached storage and memory. And there are different nodes that offer different performances. For example, the following RA3 node types. Also, as you can see here, the dense compute node types.</p>
<p>The leader node of the cluster has the role of coordinating communication between your compute nodes in your cluster and your external applications accessing your Redshift data warehouse. So the leader node is essentially gateway into your cluster from your applications. When external applications are querying the data in your warehouse, the leader node will create execution plans, containing code to return the required results from the database.</p>
<p>If the query from the external application references tables associated with the compute nodes, then this code is then distributed to the compute nodes in the cluster to obtain the required data, which is then sent back to the leader node. If the query does not reference tables stored on the compute nodes, then the query will run on the leader node only.</p>
<p>Each compute node itself is also split into slices, known as node slices. A node slice is simply a partition of a compute node where the nodes memory and disk spaces split. Each node slice then processes operations given by the leader node where parallel operations can then be performed across all slices and all nodes at once for the same query. As I mentioned previously, compute nodes can have different capacities and these capacities determine how many slices each compute node can be split into.</p>
<p>When creating a table, it is possible to distribute rows of that table across different nodes slices based upon how the distribution case is defined for the table. For a deeper understanding on how to select the best distribution style, please see the following link <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">here</a>.</p>
<p>When your Amazon Redshift database is created, you will of course connect to it using your applications. Typically these applications will be your analytic and business intelligence tools, that you’re running with your organization. Communication between your BI applications and Redshift, will use industry standard open database connectivity, ODBC. And Java database conductivity, JDBC drivers for PostgreSQL.</p>
<p>The performance that Amazon Redshift can generate is a huge benefit to many organizations. In fact, at the time of writing this course, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> currently boasts that it’s three times faster than other cloud data warehouses.</p>
<p>From a query perspective, Amazon Redshift has a number of features to return results quickly and effectively. Let’s take a look at a few of them.</p>
<p>Firstly, massively parallel processing. As highlighted in the previous section by associating rows from tables across different nodes slices and nodes. It allows the node leader to generate execution plans, to distribute crews from external applications across multiple compute nodes at once, allowing them to work together to generate the end result, which is an aggregated by the leader node.</p>
<p>Columnar data storage. This is used as a way of reducing the number of times the database has to perform disk I&#x2F;O, which helps to enhance query performance. Reducing the data retrievals from the disk means there is more memory capacity to carry out in memory processing of the query results. Result caching. Caching in general is a great way to implement a level of optimization.</p>
<p>Result caching helps to reduce the time it takes to carry out queries by caching some results of the queries in the memory of the leader node in a cluster. As a result, when a query is submitted, the leader node will check its own cache copy of the results and if a successful match is found, the cached results are used instead of executing another query on your Redshift cluster.</p>
<p>Amazon Redshift also integrates with Amazon CloudWatch, allowing you to monitor the performance of your physical resources, such as CPU utilization and throughput. In addition to this, Redshift also generates query and load performance data that enables you to track overall database performance. Any data relating to query and load performance is only accessible from within the Redshift console itself and not Amazon CloudWatch.</p>
<p>During the creation of your Redshift cluster, you can as an optional element, select up to 10 different IAM roles to associate with your cluster. This allows you to grant the Amazon Redshift principle, redshift.amazonaws.com access to other services on your behalf, for example, Amazon S3 where you might have a data lake. Accessing data within S3 will require a set of credentials to authorize Redshift access to S3. And the best way to do that is by using an IAM role. Therefore, if you intend to perform actions such as this when using your Amazon Redshift cluster, you might need to consider which access you need and what roles you will need to create.</p>
<p>To learn more about IAM and roles, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">here</a>. In the next lecture, I want to show you how to create a new Redshift cluster.</p>
<h1 id="DEMO-Creating-an-Amazon-Redshift-Cluster"><a href="#DEMO-Creating-an-Amazon-Redshift-Cluster" class="headerlink" title="DEMO: Creating an Amazon Redshift Cluster"></a>DEMO: Creating an Amazon Redshift Cluster</h1><p>Hello and welcome to this lecture. This is going to be a quick demonstration on how to set up an Amazon Redshift cluster. So as you can see, I’m in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> management console at the moment, and to find Redshift, we can scroll down to the database category and we can see Amazon Redshift here. So if we select on that, we’re then taken to this splash screen here. So I don’t have any Redshift clusters at the moment.</p>
<p>So to start with, all I need to do is to click on the orange create cluster button, and this takes us to the configuration page. And the first thing we need to do is give it a name. So I’m just going to call this my cluster. Then we can choose our node type. So we have the RA3 nodes and the dense compute nodes here. Now, AWS recommends the RA3 nodes due to the high performance and the managed storage aspect, or you have the dense compute nodes here.</p>
<p>Now, for this demonstration, I’m just going to select a dense compute node. Now, if you scroll down, we can select the number of nodes that we would like. As you can see, it ranges from one to 32, so we can scroll up or down. And also if you see in this configuration summary section the cost per month, and the total compressed storage will also change with the amount of nodes that I select. So there you can see it is scrolling up and down. I’m just going to leave it as two nodes.</p>
<p>Now I can scroll down to the database configurations. We can give the database a name, and also the port that it’s going to be using, and also a master username and password. So let me just enter a password. And then we have cluster permissions. Now, this is an optional step. So if you want your AWS Redshift cluster to interact with other AWS services on your behalf, for example, maybe Amazon S3, you might want to import data, then you can associate an IAM role that has access to S3 to allow that process to happen. But as I said, this is an optional component.</p>
<p>Now, at the very bottom here, we have additional configuration. Now, these are the default settings. So we have a default network, default backup options, maintenance, default security groups, and also a parameter group, as well. But if you turn off those default settings, then you can go through and modify any of those components. For example, network and security. You can select the VPC for it to run in. You can select the security groups that are associated with your clusters to define what resources can access it. You can also define a subnet group which defines what subnets that the clusters will be launched in and also any availability zones. You can also specify if you want any cluster traffic to purely route through your VPC and if you want your cluster to be publicly accessible or not. So there’s a few network and security features that you can change there.</p>
<p>Looking at database configurations, here you can select a parameter group if you have any configured, and you can also configure any encryption using AWS KMS, and if you want to use the default Redshift key, or if you want to use one of your own CMKs, for example, I have a CMK here in my account. For this demonstration, I’m just gonna disable encryption.</p>
<p>Under maintenance, you can set a maintenance window so that the day and time of the week that any maintenance will be carried out to your cluster. And also you can specify which cluster version you’d like, and you have three options. Either use the most current approved cluster version, use the cluster version before the current version, or use the cluster version with beta releases of new versions. I’ll just leave that as current.</p>
<p>Under monitoring, you can have CloudWatch alarms. So for example, you can create a new alarm for disk usage threshold when that reaches 80%, and then you can notify people via an SNS topic that you might already have configured. I’ll say no alarms. And finally, backup. And also you can specify your snapshot retention, which is how long you’ll keep the backups for. And finally, if you want to configure cross-region snapshot, you can either enable that or disable it. And this will back up your cluster to a different region. So if you enable it, you can then select an alternate region to where your cluster currently resides. I’m just going to disable that.</p>
<p>So there are the different options that are available, but I’m just going to select the defaults that it already suggested. And then once you’re happy with your settings, simply click create cluster. As we can see here now, it’s now creating our cluster. This might take a few minutes, so I’ll come back when that’s done.</p>
<p>Okay, as you can see, the cluster is now available. If we select the dashboard, then we can see that we have one new cluster in the Ireland region with two nodes, and we can see that it’s already taken an automated snapshot, as well.</p>
<p>So cluster overview here, so we can see a number of queries, any database connections, disk space used, CPU utilization. As you can see, there’s not much going on at the moment. We’ve simply just created it. If we have any alarms, and down here, any events, and also a query overview here. So I won’t go into any more detail than that.</p>
<p>This is just a very high-level, quick introduction on how to create an Amazon Redshift cluster. And that’s it.</p>
<h1 id="7Amazon-Relational-Database-Service"><a href="#7Amazon-Relational-Database-Service" class="headerlink" title="7Amazon Relational Database Service"></a>7<strong>Amazon Relational Database Service</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/ec2/instance-types/">Instance Types Performance</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-rds-multi-az-read-replicas/introduction-to-rds-read-replicas/">Course: When to use RDS Multi-AZ &amp; Read Replicas</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-shared-responsibility-model-security/">Shared Responsibility Model Security</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/create-your-first-amazon-rds-database/">Lab: Creating your first Amazon RDS Database</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/lab/getting-started-with-amazon-aurora-database-engine/">Lab: Getting started with Amazon Aurora Database Engine</a></p>
<h1 id="9Amazon-DynamoDB"><a href="#9Amazon-DynamoDB" class="headerlink" title="9Amazon DynamoDB"></a>9<strong>Amazon DynamoDB</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Dat</a></p>
<h1 id="11Amazon-Redshift"><a href="#11Amazon-Redshift" class="headerlink" title="11Amazon Redshift"></a>11<strong>Amazon Redshift</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html">Differences between Redshift and PostgreSQL</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">Selecting a Distribution Style</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Course: Overview of AWS IAM</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:35" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:35-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:18" itemprop="dateModified" datetime="2022-11-20T19:01:18-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Storage-CLF-C01-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Knowledge-Check-Storage-CLF-C01.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:33" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:33-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:00" itemprop="dateModified" datetime="2022-11-20T19:08:00-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Introduction-to-the-Elastic-File-System-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:32" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:32-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:08:16" itemprop="dateModified" datetime="2022-11-20T19:08:16-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-S3-Bucket-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Storage-CLF-C01-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:31" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:31-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 18:59:02" itemprop="dateModified" datetime="2022-11-20T18:59:02-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Storage-CLF-C01-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course on Storage in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Cloud Practitioner certification.</p>
<p>Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications. In this course, the AWS team will be presenting a series of lectures that introduce the various Storage services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Cloud Practitioner exam and is ideal for anyone who is looking to learn more about the various Storage services in AWS in preparation for the exam. Passing the AWS Certified Cloud Practitioner exam is a great first step for anyone looking to grow within their current career, or transition to a new career entirely.</p>
<p>The objective of this course is to provide a high-level introduction to Storage services in AWS, including:</p>
<ul>
<li>The Amazon Simple Storage Service, known as S3;</li>
<li>Amazon Elastic Block Store, or EBS; and the</li>
<li>Amazon Elastic File System, or EFS.</li>
</ul>
<p>We’ll also discuss AWS services that can assist with large-scale data storage, migration, and transfer both into and out of AWS using the AWS Snow family, as well as hybrid cloud storage services and on-premises data backup solutions using AWS Storage Gateway.</p>
<p>These objectives are covered by Domain 3 in the official AWS Certified Cloud Practitioner exam blueprint: Technology, which accounts for 33% of the exam content. The other courses in this learning path cover the remaining exam content and will ensure that you are fully prepared to sit this exam.</p>
<p>This course is designed for anyone who is new to cloud computing, so no prior experience with AWS is necessary. While it may be helpful to have a basic understanding of AWS and its services, as well as some exposure to AWS Cloud design, implementation, and operations, this is not required as all of the concepts we will introduce in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Thank you!</p>
<h1 id="Overview-of-Amazon-S3"><a href="#Overview-of-Amazon-S3" class="headerlink" title="Overview of Amazon S3"></a>Overview of Amazon S3</h1><p>Hello and welcome to this lecture where I will introduce the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon Simple Storage Service</a>, commonly known as S3. Amazon S3 is probably the most heavily used storage service that is provided by AWS simply down to the fact that it can be a great fit for many different use cases, as well as integrating with many different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. Amazon S3 is a fully managed, object-based storage service that is highly available, highly durable, very cost-effective, and widely accessible.</p>
<p>The service itself is promoted as having unlimited storage capabilities making Amazon S3 extremely scalable, far more scalable than your own on-premise storage solution could ever be. There are, however, limitations on the individual size of a single file that it can support. The smallest file size that it supports is zero bytes and the largest file size is five terabytes. Although there is this size limitation on a maximum file size, it’s one that many of us will not perceive as an ongoing inhibitor in the majority of use cases.</p>
<p>The service operates an object storage service which means each object uploaded does not conform to a data structure hierarchy like a file system would, instead its architecture exists across a flat address space and is referenced by a unique URL. Now, if you compare this to file storage, where your data is stored as separate files within a series of directories forming a data structure hierarchy much like your own files are on your own laptop or computer then S3 is very different in comparison. S3 is a regional service and so when uploading data you as the customer are required to specify the regional location for that data to be placed in.</p>
<p>By specifying your region for your data Amazon S3 will then store and duplicate your uploaded data multiple times across multiple availability zones within that region to increase both its durability and availability. For more information on regional availability zones and other AWS global infrastructure components, please see the following blog <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-global-infrastructure/">here</a>. Objects stored in S3 have a durability of ninety-nine point nine nine nine nine nine nine nine nine nine percent, known as eleven nines of durability and so the likelihood of losing data is extremely rare and this is down to the fact that S3 stores multiple copies of the same data in different availability zones. The availability of S3 data objects is dependent on the storage class used and this can range from 99.5% to 99.99%.</p>
<p>The difference between availability and durability is this: when looking at availability AWS ensures that the uptime of Amazon S3 is between 99.5% to 99.99%, depending on the storage class, to enable you to access your stored data. The durability percentage refers to the probability of maintaining your data without it being lost through corruption, degradation of data, or other unknown potential damaging effects. When uploading objects to Amazon S3, a specific structure is used to locate your data in the flat address space.</p>
<p>To store objects in S3, you first need to define and create a bucket. You can think of a bucket as a container for your data. This bucket name must be completely unique, not just within the region you specify, but globally against all other S3 buckets that exist, of which there are many millions. And this is because of the flat address space, you simply can’t have a duplicate name. Once you have created your bucket you can then begin to upload your data within it. By default your account can have up to a hundred buckets, but this is a soft limit and a request to increase this can be made with AWS. Any object uploaded to your buckets are given a unique object key to identify it.</p>
<p>In addition to your bucket, you can if required create folders within the bucket to aid with categorization of your objects for easier data management. Although folders can provide additional management from a data organization point of view, I want to reiterate that Amazon S3 is not a file system and many features of Amazon S3 work at the bucket level and not a specific folder level and so the unique object key for every object contains the bucket, any folders that are present, and also the name of the file itself. Let me now provide a quick overview via a demonstration of the Amazon S3 console and I’ll show you how to create a bucket within the service and upload an object to that bucket and then show you the unique object key of that object. Okay so I’m currently logged into my AWS management console and I can find amazon S3 under the storage category which is down here. So if I select S3 and this has taken me to the S3 dashboard.</p>
<p>Now up here we have our buckets and this list that we have here are a list of buckets that I have already created in my account so this is the bucket name which is the unique bucket identifier. And over here we have the region in which that bucket exists in, so we have some in London some in Ireland some over in US as well, and also, the date created. Over here we have access whether these buckets can be accessed by the public or not. I’m not going to dive too deep into access and security the buckets at this stage, as this is just more of an introduction to give you an overview of the console, but we do have other courses that focus on security.</p>
<p>Now, if I go into one of these buckets, for example, this one here cloudacademyaudio, we can see that I have created a folder in here called Stuart. There’s no other objects, it’s just a folder we can see that by this little icon here, so there’s no actual objects in here, this is just a folder that I created just to help me manage and categorize any objects that I do upload. If I select that folder, I can see I have two more folders here. If I go into this one, for example, I can see that I have an object I have a PNG file. So this is an object that I have uploaded to S3 and we can see that it’s in the courses folder under Stuart under the cloudacademyaudio bucket.</p>
<p>Now, if I select this object, I can get some information about it. I can open it and download it etc. We can see the last time it was modified, the storage class that it belongs to, and I’ll be talking more about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/storage-classes/">storage classes</a> in the next lecture, if there’s any encryption at rest activated, the file size, and here is the unique identifier of the key for this object. So we can see that the key comprises of any folders within the bucket and then also the object name at the end. And this here is the unique identifier of this object on S3, so it gives it a URL.</p>
<p>Now if I want to open this I can simply click on open and as we can see, it’s just an image file, or I can download the object if I want to. So I just wanted to show you there how you can use folders within your bucket and also what the object key looks like as well. So now if I go back to the console, the main dashboard where all my buckets are, I want to show you how to create a bucket quickly. It’s very simple simply click on create bucket, then we need to give it a unique bucket name.</p>
<p>Now remember, this has to be a globally unique name, so if I type in stubucketdemo and then I can select a region that I want this bucket to be in, I’m just going to select the London region, and if I want to, I can copy settings from an existing bucket, but it’s going to go through the different screens to show you the options quickly that you can have when you’re creating a bucket. Click on Next. Here’s some management, we have some management options such as versioning and server access logging. Versioning keeps all versions of an object in the same bucket and server access logging logs requests for access to your bucket. You can also use key value pair tags, you can activate object level logging which will record any API activity with CloudTrail associated with your objects and you can also encrypt your objects as well. I’m just gonna leave all those options as default for this demonstration. Click on next.</p>
<p>Here we can set different permissions, I’m just gonna leave the default block all public access so this will prevent anyone from outside of my VPC accessing any data within my bucket. Click on next. We just have a review of the settings that we selected and then all you need to do is click on create bucket. So if I scroll down to my bucket that I just created, which was stubucketdemo. If I select that, we can see here that I’ve got no folders and no objects. So if you want to create a folder, you simply click on create folder, just give it a name. If you want to add any encryption you can do so here. I’m just going to use none as a default. Now, I can either add an object directly under this bucket or I can add it into that folder. Let me just add it directly under the bucket name of stubucketdemo.</p>
<p>So to upload an object you simply click on upload, add files, select your object that you’d like to upload or objects, click on next, you know we have some permissions here as to who can read or write to the object, and as we can see here, we have the block public access setting turned on for this bucket. Click on next. Now here we have our storage classes and depending on what storage class we select it will affect the durability, the availability, and also the cost of your object being stored. Now, I’m gonna go deeper into the different storage classes in the next lecture so I won’t go over this too deep now. For the sake of this demonstration I’m just going to select the standard storage class.</p>
<p>Then we have a review page and then simply upload. And then we have my object that’s been uploaded to my bucket. Again, if I select it, I can see the object key and also the unique object URL as well. So that’s just a very quick demonstration to show you what the S3 console looks like, how to create buckets, how to create folders, and also upload objects as well, just so hopefully you can piece things together a little bit easier if you’ve not used Amazon S3 before.</p>
<h1 id="Storage-Classes"><a href="#Storage-Classes" class="headerlink" title="Storage Classes"></a>Storage Classes</h1><p>Hello and welcome to this lecture covering <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon S3</a> storage classes. As we just saw in the demonstration, I had an option to select which storage class I wanted my uploaded object to reside in. Amazon S3 offers these different storage classes to allow you to select a class based on performance features and costs and it’s down to you to select the storage class that you require for the data. The storage classes available are as follows S3 Standard, S3 Intelligent Tiering, S3 Standard Infrequent Access, S3 One Zone Infrequent Access, S3 Glacier, and S3 Glacier Deep Archive.</p>
<p>S3 Standard. This storage class is considered a general-purpose storage class. It is ideal for a range of use cases where you need high throughput with low latency with the added ability of being able to access your data frequently. By copying data to multiple availability zones, S3 Standard offers eleven nines of durability across multiple availability zones, meaning the OData remains protected against a single availability zone failure. It also offers a 99.99% availability across the year, which is the highest availability that S3 offers. From a security standpoint this storage class also has the added support of SSL, Secure Sockets Layer, for encrypting data in transit in addition to encryption options for when the data is at rest. With management features such as lifecycle rules, objects in S3 Standard can automatically be moved to another storage class. For those unfamiliar with life cycle rules, they provide an automatic method of managing the life of your data while it is being stored on Amazon S3. By adding a life cycle wall to a bucket you are able to configure and set specific criteria that can automatically move your data from one class to another or delete it from Amazon S3 altogether. You may want to do this as a cost saving exercise by moving data to a cheaper storage class after a set period of time.</p>
<p>S3 Intelligent Tiering. This storage class is ideal for those circumstances where the frequency of access to the object is unknown. Effectively, we have unpredictable data access patterns and so by using this storage class, it can help to optimize your storage costs. Depending on your data access patterns of objects in the Intelligent Tiering Class, S3 will move your objects between two different tiers, these being frequent and infrequent access. Now, these classes are a part of the Intelligent Tiering Class itself and are separate from the existing storage classes I listed earlier. When the objects are moved to Intelligent Tiering, they are placed within the frequent access tier, which is the more expensive of the two tiers. If an object is not accessed for 30 days then AWS will automatically move the object to the cheaper tier known as the infrequent access tier. Once that same object is accessed again, it will automatically be moved back to the frequent tier. Much like S3 Standard, S3 Intelligent Tiering also offers 11 nines of durability across multiple availability zones offering protection against the loss of a single AZ. However, its availability isn’t quite as high as S3 Standard as it set at 99.9%. This storage class also has the added support of SSL for encrypting data in transit in addition to encryption options for when the data is at rest. S3 Intelligent Tiering also supports the lifecycle rules and matches the same performance throughput and low latency as S3 Standard.</p>
<p>S3 Standard infrequent access. This can be seen as the equivalent to the infrequent tier from the Intelligent Tiering class as it is designed for data that does not need to be accessed as frequently as data within the Standard tier, and yet still offers high throughput and low latency access, much like S3 Standard does. As with all other S3 storage classes, it carries that 11 9s durability across multiple AZs, again by copying your objects to multiple availability zones within a single region to protect against AZ outages. It shares the same availability as Intelligent Tiering of 99.9 percent. As a result, this storage class comes at a cheaper cost than S3 Standard. Common security features such as SSL for encryption in transit and data at rest encryption is supported as well as management controls such as lifecycle rules to automatically move objects to an alternate storage class based on your requirements.</p>
<p>S3 One Zone Infrequent Access. By now you can probably assume what this storage class comprises of based off of the previous classes that I’ve already discussed. However, again, being an infrequent storage class it is designed for objects that are unlikely to be accessed frequently. It also carries the same throughput and low latency. However, the durability, although remaining at eleven nines only exists across a single availability zone. As the name implies to this class it is one zone, as in one availability zone. So the objects will be copied multiple times to different storage locations within the same availability zone instead of across multiple availability zones. This results in a 20% storage cost reduction when compared to S3 Standard. One Zone IA does, however, offer the lowest level of availability which is currently 99.5 percent and this is down to the fact that your data is being stored in a single availability zone. Should the AZ storing your data become unavailable then you will lose access to your data or even worse it may become completely lost should the AZ be destroyed in a catastrophic event. Again, life cycle rules and encryption mechanisms are in place to protect your data both in transit and at rest.</p>
<p>S3 Glacier. The next two storage classes are associated with S3 Glacier which is used for archival data. Firstly let me explain more about S3 Glacier, as it can be accessed separately from the Amazon S3 service but closely interacts with it S3 Glacier storage classes directly interact with the Amazon S3 lifecycle rules discussed previously. However, the fundamental difference with the Amazon Glacier storage classes come at a fraction of the cost when it comes to storing the same amount of data than the S3 storage classes. So what’s the catch? Well, it doesn’t provide you the same features as Amazon S3 but more importantly, it doesn’t provide you instant access to your data.</p>
<p>So what do Amazon Glacier classes offer exactly? Well, they offer an extremely low-cost long term durable storage solution which is often referred to as cold storage, ideally suited for long term backup and archival requirements. It’s capable of storing the same data types as Amazon S3, effectively any object, however, like I just mentioned it doesn’t provide instant access to your data. In addition to this, there are other fundamental differences which makes this service fit for purpose for other use cases. The service itself has 11 nines of durability making this just as durable as Amazon S3. Again this is achieved by replicating the data across multiple different availability zones within a single region but it provides the storage at a considerably lower cost compared to that of Amazon S3. And this is because retrieval of data stored in Glacier is not an instant access retrieval process. When retrieving your data it can take up to several hours to gain access to it depending on certain criteria. The data structure within Glacier is centered around vaults and Archives. Buckets and folders are not used. They are purely used for S3.</p>
<p>A Glacier vault simply acts as a container for Glacier archives. These vaults are regional and as such during the creation of these vaults, you are asked to supply the region in which they will reside. Within these vaults, we then have our data which is stored as an archive and these archives can be any object similar to S3. Thankfully you can have unlimited archives within your Glacier vaults, so from a capacity perspective, it follows the same rule as S3. Effectively you have access to an unlimited quantity of storage for your archives and vaults. Now whereas Amazon S3 provided a nice graphical user interface to view, manage, and retrieve your data within buckets and folders, Amazon Glacier does not offer this service.</p>
<p>The Glacier dashboard within AWS management console allows you to create your vaults, set data retrieval policies, and event notifications. When it comes to moving data into S3 Glacier for the first time it’s effectively a two-step process. Firstly, you need to create your vaults as your container for your archives and this could be completed using the Glacier console. Secondly, you need to move your data into the Glacier vault using the available API or SDKs. As you may be thinking, there’s also another method of moving your data into Glacier and this is by using the S3 lifecycle rules that I discussed earlier. When it comes to retrieving your archives, which is your data, you will again have to use some form of code to do so, either the APIs, SDKs or the AWS CLI. Either way, you must first create an archival retrieval job, then request access to all or part of that archive.</p>
<p>Now you have more of an understanding of S3 Glacier, let me review the two S3 Glacier storage classes. Firstly, S3 Glacier. This is the default Standard storage class within S3 Glacier offering a highly secure using in transit and at rest encryption low-cost and durable storage solution. The durability matches that of other S3 storage classes, being 11 9s across multiple availability zones, and the availability of S3 Glacier is 99.9%. It’s simple to add data to this storage class using the S3 put APIs is in addition to S3 lifecycle rules. However, it does offer a variety of retrieval options depending on how urgently you need the data back, each offering a different price point. These being expedited, Standard, and bulk.</p>
<p>Expedited. This is used when you have an urgent requirement to retrieve your data but the request has to be less than 250 megabytes. The data is then made available to you in one to five minutes and this is the most expensive retrieval option of the three.</p>
<p>Standard. This can be used to retrieve any of your archives no matter their size but your data will be available in three to five hours, so much longer than the expedited option and this is the second most expensive of the three options.</p>
<p>And finally, bulk. This option is used to retrieve petabytes of data at a time, however, this typically takes between five and twelve hours to complete. This is the cheapest of the retrieval options so it really depends on how much data and how quickly you need it, as the retrieval speed and cost to be made by your retrieval option.</p>
<p>S3 Glacier Deep Archive. Out of all the storage classes offered by S3, Glacier Deep Archive is the cheapest and again being a Glacier class, it focuses on long-term storage. This is an ideal storage class for circumstances that require specific data retention regulations and compliance with minimal access, such as those within the financial or health sector where data records might need to be legally retained for seven years or even longer. The durability and availability matches that of S3 Glacier with eleven 9s durability across multiple AZss with 99.9% availability.</p>
<p>Adding data into deep archive follows the same processes as S3 Glacier, using S3 put APIs in addition to S3 lifecycle rules. Deep Archive, however, does not offer multiple retrieval options. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> states that the retrieval of the data will be within 12 hours or less. To summarize some of the common features between the storage classes, this table clearly shows how they differ. As you can see, the main difference of the classes is the durability and availability percentages, in addition to the pricing.</p>
<p>So when selecting your class for your data you really need to be asking yourself the following questions: how critical is my data? Does it require the highest level of durability? How reproducible is the data? Can it be easily created again if need be? and how often is the data likely to be accessed? For detailed information on Amazon S3 pricing covering all storage classes discussed please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/introduction/">Understanding and Optimizing Costs with AWS Storage Services</a>.</p>
<h1 id="EC2-Instance-Storage"><a href="#EC2-Instance-Storage" class="headerlink" title="EC2 Instance Storage"></a>EC2 Instance Storage</h1><p>Hello, and welcome to this lecture covering EC2 Instance Level Storage. Which is referred to as an instance store volume. The first point to make about EC2 instance store volumes, is that the volumes physically reside on the same host that provides your EC2 instance itself, acting as local disc drives, allowing you to store data locally to that instance. Up until now within this course we have discussed persistent storage options. But instance store volumes provide ephemeral storage for you EC2 instances. </p>
<p>Ephemeral storage means that the block level storage that it provides offers no means of persistency. Any data stored on these volumes is considered temporary. With this in mind, it is not recommended to store critical or valuable data on these ephemeral instance store volumes, as it could be lost, should an event occur. By an event, let me explain under what conditions that your data would be lost, should it be stored on one of these volumes. </p>
<p>If your instance is either stopped or terminated, then any data that you have stored on that instance store volume associated with this instance will be deleted without any means of data recovery. However, if your instance was simply rebooted, your data would remain intact. Although, you can control when your instances are stopped or terminated, giving you the opportunity to either back-up the data or move it to another persistent volume store, such as the elastic block store service. Sometimes this control is not always possible. Let’s consider you had critical data stored on an ephemeral instance store volume and then the underlying host that provided your EC2 instance and storage failed. You had no warning that this failure was going to occur, and as a result of this failure, the instance was stopped or terminated. Now all of your data on these volumes is lost. When a stop and start, or termination occurs, all the blocks on the storage volume are reset, essentially wiping data. So, you might be thinking, why use these volumes? What use do they have if there is a chance that you are going to lose data? They do, in fact, have a number of benefits. </p>
<p>From a cost perspective, the storage used is included in the price of the EC2 instance. So, you don’t have an additional spend on storage cost. The I&#x2F;O speed on these volumes can far exceed those provided by the alternative instance block storage, EBS for example. When using store optimized instance families, such as the I3 Family, it’s potentially possible to reach 3.3 million random read IOPS, and 1.4 million write IOPS. With speeds like this, it makes it ideal to handle the high demands of no SQL databases. However, any persistent data required would need to be replicate or copied to a persistent data store in this scenario. Instance store volumes are generally used for data that is frequently changing; that doesn’t need to be retained, as such, they are great to be used as a cache or buffer. They are also commonly used for service within a load balancing group, where data is replicated across the fleet such as a web server pool. </p>
<p>Not all instance types support instance store volumes. So, if you do have a need where these instance store volumes would work for your use case, then be sure to check the latest AWS documentation to ascertain if the instance type you’re looking to use supports the volume. The size of your volumes, however, will increase as you increase the EC2 instance size. </p>
<p>From a security stance, instance store volumes don’t offer any additional security features. As to be honest, they are not separate service like the previous storage options I have already explained. They are simply storage volumes attached to the same host on the EC2 instance, and they are provided as a part of the EC2 service. So, they effectively have the same security mechanisms provided by EC2. This can be IAM policies dictating which instances can and can’t be launched, and what action you can perform on the EC2 instance, itself. If you have data that needs to remain persistent, or that needs to be accessed and shared by others, then EC2 instance store volumes are not recommended. If you need to use block level storage and want a quick and easy method to maintain persistency, then there is another block level service that is recommended. This being the elastic block store service.</p>
<h1 id="Overview-of-EBS"><a href="#Overview-of-EBS" class="headerlink" title="Overview of EBS"></a>Overview of EBS</h1><p>In this lecture, I shall be talking about the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-block-store-ebs-1060/course-introduction/">Amazon Elastic Block Store</a> service, known as EBS, which provides storage to your EC2 instances via EBS volumes, which offer different benefits to that of instance store volumes used with some EC2 instances.</p>
<p>EBS provides persistent and durable block level storage. As a result, EBS volumes offer far more flexibility with regards to managing the data when compared to data stored on instance store volumes. EBS volumes can be attached to your EC2 instances, and are primarily used for data that is rapidly changing that might require a specific Input&#x2F;Output operations Per Second rate, also known as IOPS.</p>
<p>EBS volumes are independent of the EC2 instance, meaning that they exist as two different resources. They are logically attached to the instance instead of directly attached like instance store volumes. From a connectivity perspective, only a single EBS volume can only ever be attached to a single EC2 instance. However, multiple EBS volumes can be attached to a single instance.</p>
<p>Due to the EBS ability to enforce persistence of data, it doesn’t matter if your instances are intentionally or unintentionally stopped, restarted, or even terminated, the data will remain intact when configured to do so. EBS also offers the ability to provide point in time backups of the entire volume as and when you need to. These backups are known as snapshots and you can manually invoke a snapshot of your volume at any time, or use Amazon CloudWatch events to perform an automated schedule of backups to be taken at a specific date or time that can be recurring.</p>
<p>The snapshots themselves are then stored on Amazon S3 and so are very durable and reliable. They are also incremental, meaning that each snapshot will only copy data that has changed since the previous snapshot was taken. Once you have a snapshot of an EBS volume, you can then create a new volume from that snapshot. So, if for any reason you lost access to your EBS volume through or incident or disaster, you can recreate the data volume from an existing snapshot and then attach that volume to a new EC2 instance. To add additional flexibility and resilience, it is possible to copy a snapshot from one region to another.</p>
<p>Looking at the subject of high availability and resiliency, your EBS volumes are, by default, created with reliability in mind. Every write to a EBS volume is replicated multiple times within the same availability zone of your region to help prevent the complete loss of data. This means that your EBS volume itself is only available in a single availability zone. As a result, should your availability zone fail, you will lose access to your EBS volume. Should this occur, you can simply recreate the volume from your previous snapshot and attach it to another instance in another availability zone.</p>
<p>There are two types of EBS volumes available. Each have their own characteristics. These being SSD backed storage, solid state drive, and HDD backed storage, hard disk drive. This allows you to optimize your storage to fit your requirements from a cost to performance perspective.</p>
<p>SSD backed storage is better suited for scenarios that work with smaller blocks. Such as databases using transactional workloads. Or often as boot volumes for your EC2 instances. Whereas HDD backed volumes are designed for better workloads that require a higher rate of throughput, such as processing big data and logging information. So, essentially working with larger blocks of data.</p>
<p>These volume types can be broken down even further. Looking at the following table we can see how different volumes can be used for both SSD and HDD volumes types.</p>
<p>You can see that depending on the use case for your EBS volume you can select the most appropriate type. Each of these volumes also offer different performance factors which include: Volume size, Max IOPS per volume, Max throughput per volume, Max IOPS per instance, Max Throughput per instance, and Dominant performance.</p>
<p>The performance of volumes change frequently and so for the latest information on these volume types, please refer to the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">here</a>.</p>
<p>For those who are not familiar with provisioned IOPS (input&#x2F;output operations per second) volumes, they deliver enhanced predictable performance for applications requiring I&#x2F;O intensive workloads. When working with these volumes you also have the ability to specify at IOPS rate during the creation of a new EBS volume, and when the volume is attached to an EBS-optimized instance, EBS will deliver the IOPS defined and required within 10%, 99.9% of the time throughout the year.</p>
<p>The throughput optimized HDD volumes are designed for frequently accessed data and are ideally suited to work well with large data sets requiring throughput-intensive workloads, such as data streaming, big data, and log processing. These volumes will deliver the expected throughput 99% of the time over a given year, and an important point to make is that these volumes can’t be used as boot volumes for your instances.</p>
<p>The cold HDD volumes offer the lowest cost compared to all other EBS volumes types. They are suited for workloads that are large in size and accessed infrequently. They will deliver the expected throughput 99% of the time over a given year, and again, it is not possible to use these as boot volumes for your EC2 instances.</p>
<p>One great feature of EBS is its ability to enhance the security of your data, both at rest and when in transit, through data encryption. This is especially useful when you have sensitive data, such as personally identifiable information, stored in your EBS volume. And in this case, you may be required to have some form of encryption from a regulatory or governance perspective. EBS offers a very simple encryption mechanism. Simple in the fact that you don’t have to worry about managing the data keys to perform the encryption process yourself. It’s all managed and implemented by EBS. All you are required to do is to select if you want the volume encrypted or not during its creation via a checkbox.</p>
<p>The encryption process uses the AES-256 encryption algorithm and provides its encryption process by interacting with another AWS service, the key management service, known as KMS. KMS uses customer master keys, CMKs, enabling the encryption of data across a range of AWS services, such as EBS in this instance.</p>
<p>To learn more about the Key Management Service, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Any snapshot taken from an encrypted volume will also be encrypted, and also any volume created from this encrypted snapshot will also be encrypted. You should also be aware that this encryption option is only available on selected instance types.</p>
<p>One final point to make on EBS encryption is that you can create a default region setting that ensures that all EBS volumes created will be encrypted by default.</p>
<p>For a detailed overview of exactly how this encryption process works, please take a look at the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/how-to-encrypt-an-ebs-volume-the-new-amazon-ebs-encryption/">blog post</a>.</p>
<p>As EBS volumes are separate to EC2 instances, you can create an EBS volume in a couple of different ways from within the management console. During the creation of a new instance and attach it at the time of launch, or from within the EC2 dashboard of the AWS management console as a standalone volume ready to be attached to an instance when required. When creating an EBS volume during an EC2 instance launch, at step four of creating that instance, you are presented with the storage configuration options. Here you can either create a new blank volume or create it from an existing snapshot. You can also specify the size and the volume type, which we discussed previously. Importantly, you can decide what happens to the volume when the instance terminates. You can either have the volume to be deleted with the termination of the EC2 instance, or retain the volume, allowing you to maintain the data and attach it to another EC2 instance. Lastly, you also have the option of encrypting the data if required.</p>
<p>You can also create the EBS volume as a standalone volume. By selecting the volume option under EBS from within the EC2 dashboard of the management console, you can create a new EBS volume where you’ll be presented with the following screen.</p>
<p>Here you will have many of the same options. However, you can specify which availability zone that the volume will exist in, allowing you to attach it to any EC2 instance within that same availability zone. As you might remember, EBS volumes can only be attached to EC2 instances that exist within the same availability zone.</p>
<p>EBS volumes also offer the additional flexibility of being able to resize them elastically should the requirement arise. Perhaps you’re running out of disk space and need to scale up your volume. This can be achieved by modifying the volume within the console or via the AWS CLI. You can also perform the same resize of the volume by creating a snapshot of your existing volume, and then creating a new volume from that snapshot with an increased capacity size.</p>
<p>As we can see, EBS offers a number of benefits over EC2 instance store volumes. But EBS is not well suited for all storage requirements. For example, if you only needed temporary storage or multi-instance storage access, then EBS is not recommended, as EBS volumes can only be accessed by one instance at a time. Also, if you needed very high durability and availability of data storage, then you would be better suited to use Amazon S3 or EFS, the elastic file system.</p>
<p>That now brings me to the end of this lecture and to the end of this introductory course and you should now have a greater understanding of the Amazon Elastic Block Store service and how it can be used as a storage option for your EC2 instances.</p>
<p>If you have any feedback, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="What-is-the-Amazon-Elastic-File-System"><a href="#What-is-the-Amazon-Elastic-File-System" class="headerlink" title="What is the Amazon Elastic File System?"></a>What is the Amazon Elastic File System?</h1><p>Hello and welcome to this lecture where I will explain what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-file-system-1137/course-introduction/">Amazon EFS</a> service is and how it fits into the storage ecosystem. Let me start by taking a step back and looking at where the EFS service fits in within the world of AWS storage. Firstly, I want to look at the array of AWS storage offerings and compare a few of them. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> has more storage solutions than I’m going to discuss in this course and I will probably continue to add more in the future. But I’m just going to focus on three different services. The reason I’ve selected these three is that, at first glance, they may seem similar and many people can be unsure which of these solutions to choose from to fit their current storage requirements. </p>
<p>Amazon Simple Storage Service or S3 is an object storage solution. Object storage stores everything as a single object, not in small chunks or blocks. With this type of storage, you upload a file and if the file changes to replace it, the entire file will be replaced. This type of storage is best for situations where files are written once and then accessed many times. It’s not optimal for situations that require both heavy read and write access at the same time. So Amazon S3 is usually used for storage of large files such as video files, images, static websites, and backup archives. For example, Netflix uses S3 for their data streaming service. They upload large movie files once and then subscribers access and play the movies many, many times. </p>
<p>The next service is the Amazon Elastic Block Store or EBS, and it’s block-level storage. Files are not stored as single objects. They’re stored in small chunks of blocks so that only the portion of the file that is changed will be updated. This type of storage is optimized for low latency access and when fast, concurrent read and write operations are needed. EBS provides persistent block storage volumes for use with a single EC2 instance. As described, EBS is persistent, meaning that even if you stop or terminate an EC2 instance that’s using EBS, the data on the EBS volume remains intact. You should use this type of storage like a computer hard drive where you store operating system files, applications and other files you wish to obtain for use with your EC2 instance.</p>
<p>Amazon Elastic File System, or EFS, is considered file-level storage and is also optimized for low latency access, but unlike EBS, it supports access by multiple EC2 instances at once. It appears to users like a file manager interface and uses standard file system semantics such as locking files, renaming files, updating files and uses a hierarchy structure. This is just like what we’re used to on standard premise-based systems. This type of storage allows you to store files that are accessible to network resources. </p>
<p>Before diving deep on EFS, let me discuss how people are traditionally used to accessing network files and resources. In traditional premises-based networks, users access files by browsing network resources that connect to a server, perhaps via a mapped drive that has been configured for them, and once they connect, they will see a tree view of available folders and files. This functionality is generally provided by various local area network systems such as file servers or storage area network, a SAN, or network-attached storage, a NAS. </p>
<p>Now let’s move on from the traditional premises-based solutions and talk about cloud-based solutions, specifically within AWS and the Amazon Elastic File System service. EFS provides simple, scalable file storage for use with Amazon EC2 instances. Much like traditional file servers, or a SAN or a NAS, Amazon EFS provides the ability for users to browse cloud network resources. EC2 instances can be figured to access Amazon EFS instances using configured mount points. Now, mount points can be created in multiple availability zones that attach to multiple EC2 instances. So, much like your traditional land servers, EC2 instances are connected to a network file system, Amazon EFS. So from a user standpoint, the result is the same. The user accesses network resources just as they always have done except for now, it’s done using cloud resources. </p>
<p>EFS is a fully managed, highly available and durable service that allows you to create shared file systems that can easily scale to petabytes in size with low latency access. EFS has been designed to maintain a high level of throughput in addition to low latency access response, and these performance factors make EFS a desirable storage solution for a wide variety of workloads, and use cases and can meet the demands of tens, hundreds or even thousands of EC2 instances concurrently. Being a managed service, there is no need for you to provision any file servers to manage the storage elements or provide any maintenance of those servers. This makes it a very simple option to provide file-level storage within your environment. It uses standard operating system APIs, so any application that is designed to work with standard operating system APIs will work with EFS. It supports both NFS versions 4.1 and 4.0, and uses standard file system semantics such as strong consistency and file locking. It’s replicated across availability zones in a single region making EFS a highly reliable storage service. </p>
<p>As the file system can be accessed by multiple instances, it makes it a very good storage option for applications that scale across multiple instances allowing for parallel access of data. The EFS file system is also regional, and so any application deployments that span across multiple availability zones can all access the same file systems providing a level of high availability of your application storage layer. At the time of writing this course, EFS is not currently available within all regions. For a list of supported regions, please visit the following link: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region">https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region</a>. </p>
<p>That now brings me to the end of this lecture.</p>
<h1 id="Storage-Classes-and-Performance-Options"><a href="#Storage-Classes-and-Performance-Options" class="headerlink" title="Storage Classes and Performance Options"></a>Storage Classes and Performance Options</h1><p>Hello and welcome to this lecture, where I will be discussing the different storage class options that EFS provides in addition to how you can alter and configure certain performance factors depending on your use case of EFS. Now, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/introduction/">Amazon EFS</a> offers two different storage classes to you, which each offer different levels of performance and cost, these being Standard and Infrequent Access, known as IA. The Standard storage class is the default storage used when using EFS. However, Infrequent Access is generally used if you’re storing data on EFS that is rarely accessed. By selecting this class, it offers a cost reduction on your storage. </p>
<p>The result of the cheaper storage means that there is an increased first-byte latency impact when both reading and writing data in this class when compared to that of Standard. The costs are also managed slightly differently. When using IA, you are charged for the amount of storage space used, which is cheaper than that compared to Standard. However, with IA, you are also charged for each read and write you make to the storage class. This helps to ensure that you only use this storage class for data that is not accessed very frequently, for example, data that might be required for auditing purposes or historical analysis. </p>
<p>With the Standard storage class, you are only charged on the amount of storage space used per month. Both storage classes are available in all regions where EFS is supported. And importantly, they both provide the same level of availability and durability. If you are familiar with S3, then you may also be familiar with S3 lifecycle policies for data management. Within EFS, a similar feature exists known as EFS lifecycle management. When enabled, EFS will automatically move data between the Standard storage class and the IA storage class. This process occurs when a file has not been read or written to for a set period of days, which is configurable, and your options for this period range include 14, 30, 60, or 90 days. </p>
<p>Depending on your selection, EFS will move the data to the IA storage class to save on cost once that period has been met. However, as soon as that same file is accessed again, the timer is reset, and it is moved back to the Standard storage class. Again, if it has not been accessed for a further period, it will then be moved back to IA. Every time a file is accessed, its lifecycle management timer is reset. The only exceptions to data not being moved to the IA storage class is for any files that are below 128K in size and any metadata of your files, which will all remain in the Standard storage class. </p>
<p>If your EFS file system was created after February 13th, 2019, then the life cycle management feature can be switched on or off. Let me now take a look at the different performance modes that EFS offers. AWS are where the EFS can be used for a number of different use cases and workloads, and as such, each use case might require a change of performance from a throughput, IOPS, and latency point of view. As a result, AWS has introduced two different performance modes that can be defined during the creation of your EFS file system. These being General Purpose, and Max I&#x2F;O.</p>
<p>Now, General Purpose is a default performance mode and is typically used for most use cases. For example, home directories and general file-sharing environments. It offers an all-round performance and low latency file operation, and there is a limitation of this mode allowing only up to 7,000 file system operations per second to your EFS file system. If, however, you have a huge scale architecture, where your EFS file system is likely to be used by many thousands of EC2 instances concurrently, and will exceed 7,000 operations per second, then you’ll need to consider Max I&#x2F;O. Now, this mode offers virtually unlimited amounts of throughput and IOPS. The downside is, however, that your file operation latency will take a negative hit over that of General Purpose. </p>
<p>The best way to determine which performance option that you need is to run tests alongside your application. If your application sits comfortably within the limit of 7,000 operations per second, then General Purpose will be best suited, with the added plus point of lower latency. However, if your testing confirms 7,000 operations per second may be reached or exceeded, then select Max I&#x2F;O.</p>
<p>When using the General Purpose mode of operations, EFS provides a CloudWatch metric percent I&#x2F;O limit, which will allow you to view operations per second as a percentage of the top 7,000 limit. This allows you to make the decision to migrate and move to the Max I&#x2F;O file system, should your operations be reaching that limit. </p>
<p>In addition to the two performance modes, EFS also provides two different throughput modes, and throughput is measured by the rate of mebibytes. The two modes offered are Bursting Throughput and Provisioned Throughput. Data throughput patterns on file systems generally go through periods of relatively low activity with occasional spikes in burst usage, and EFS provisions throughput capacity to help manage this random activity of high peaks.</p>
<p>With the Bursting Throughput mode, which is the default mode, the amount of throughput scales as your file system grows. So the more you store, the more throughput is available to you. The default throughput available is capable of bursting to 100 mebibytes per second, however, with the standard storage class, this can burst to 100 mebibytes per second per tebibyte of storage used within the file system.</p>
<p>So, for example, presume you have five tebibytes of storage within your EFS file system. Your burst capacity could reach 500 mebibytes per second. The duration of throughput bursting is reflected by the size of the file system itself. Through the use of credits, which are accumulated during periods of low activity, operating below the baseline rate of throughput set at 50 mebibytes per tebibyte of storage used, which determines how long EFS can burst for. Every file system can reach its baseline throughput 100% of the time. By accumulating, getting credits, your file system can then burst above your baseline limit. The number of credits will dictate how long this throughput can be maintained for, and the number of burst credits for your file system can be viewed by monitoring the CloudWatch metric of BurstCreditBalance. </p>
<p>If you are finding that you’re running out of burst credits too often, then you might need to consider using the Provisioned Throughput mode. Provisioned Throughput allows you to burst above your allocated allowance, which is based upon your file system size. So if your file system was relatively small but the use case for your file system required a high throughput rate, then the default bursting throughput options may not be able to process your request quick enough. In this instance, you would need to use provisioned throughput. However, this option does incur additional charges, and you’ll pay additional costs for any bursting above the default option of bursting throughput. That brings me to the end of this lecture, now I want to shift my focus on creating and connecting to an EFS file system from a Linux based instance.</p>
<h1 id="What-is-the-Snow-Family"><a href="#What-is-the-Snow-Family" class="headerlink" title="What is the Snow Family?"></a>What is the Snow Family?</h1><p>In this lecture I want to answer 2 simple questions:</p>
<ol>
<li>What is the snow family </li>
<li>and what does it consist of?</li>
</ol>
<p>So firstly, what is it? The snow family consists of a range of physical hardware devices that are all designed to enable you to transfer data into <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> from the edge or beyond the Cloud, such as your Data Center, but they can also be used to transfer data out of AWS too, for example, from Amazon S3 back to your Data Centre. </p>
<p>It’s unusual when working with the cloud to be talking about physical devices or components, normally your interactions and operations with AWS generally happen programmatically via a browser or command line interface. The snow family is different, instead, you will be sent a piece of hardware packed with storage and compute capabilities to perform the required data transfer outside of AWS, and when complete, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">device</a> is then sent back to AWS for processing and the data uploaded to Amazon S3.</p>
<p>You can perform data transfers from as little as a few terabytes using an AWS snowcone all the way up to a staggering 100 petabytes using a single AWS snowmobile, and I’ll be talking more about these different snow family members shortly. Now of course when we are talking about migrating and transferring data at this magnitude, using traditional network connectivity is sometimes simply not feasible from a time perspective. For example, let’s assume you needed to transfer just 1petabye of data over a 1gbps using Direct Connect it would take 104 Days, 5 Hours 59 Minutes, 59.25 Seconds, not forgetting the cost of the data transfer fees too! </p>
<p>In addition to these devices packing some serious storage capacity for data transfer, some of them also come fitted with compute power, allowing you to run usable EC2 instances that have been designed for the snow family enabling your applications to run operations in often remote and difficult to reach environments, even without having a data center in sight, and when working with a lack of persistent networking connectivity or power. For example, the snowcone comes with the ability to add battery packs increasing their versatility. The enablement of running EC2 instances makes it possible to use these devices at the edge to process and analyze data much closer to the source.   </p>
<p>So let’s now take a look at what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/introduction/">snow family</a> consists of to get a better understanding of what these devices are.</p>
<p>As you can see from this table, both from a physical and capacity perspective, the snowcone is the smallest followed by the snowball and finally the snowmobile. You may also notice that the snowball comes in 3 choices, compute optimized, compute optimized with GPU, and storage optimized, each offering a different use case, however, each of these 3 offerings all come in the same size device. </p>
<h1 id="Which-Snow-Device-Do-I-Need"><a href="#Which-Snow-Device-Do-I-Need" class="headerlink" title="Which Snow Device Do I Need?"></a>Which Snow Device Do I Need?</h1><p>In this lecture, I will be looking at different scenarios to help you decide which <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/introduction/">snow device</a> to use and when.</p>
<p>So we have the following snow devices AWS Snowcone, AWS Snowball, and AWS Snowmobile. </p>
<p>The AWS Snowcone is the smallest of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/what-is-the-snow-family/">snow family</a>, this has been designed to be lightweight, easily portable, allowing you to easily use the device pretty much anywhere and under any conditions due to the ruggedness of the casing, and the added advantage of being able to run on battery should a persistent mains connection not be available. It can easily fit into a standard backpack, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> have even demonstrated that the snowcone can be attached to a drone emphasising its portability and versatility. Packed with 8TB of storage and an EC2 instance, this device is perfect for taking your computing needs way beyond the cloud and your Data Centre allowing you to capture, process, and analyze data, perhaps via IoT sensors, which can then be shipped back to AWS for data transfer, or you could even use AWS DataSync to transfer the data on-line over your traditional network connectivity. </p>
<p>For those unaware of AWS DataSync, it’s a service that allows you to easily and securely transfer data from your Snowcone or your on-premise data center, to AWS storage services. It can also be used to manage data transfer between 2 different AWS storage services too, so it’s a great service to help you migrate, manage, replace and move data between different storage locations.  </p>
<p>This is essentially the elder sibling of the Snowcone, it’s bigger in size and it contains a greater amount of storage and compute power. This brings a new set of use cases for this device, it’s primarily used for large scale data transfer operations, up to 80TB at a time, both in and out of AWS. The devices themselves can be rack mounted in your data centre, and if need be clustered in groups of 5-10 devices. Unlike Snowcones, they can’t be powered by battery expansion packs, and they are not as portable, for example, you can’t stick a snowball in a backpack and walk up a mountain, or strap it to a drone! </p>
<p>The Storage optimized snowball is targeted for data migrations and transfers with its storage being compatible with both S3 object storage and EBS volumes. The Compute optimized snowball is a great option if you need to handle compute intensive edge computing workloads in disconnected environments. From a storage perspective, it also comes with 42 TB of usable HDD capacity which comes compatible with EBS volumes and S3 object storage. The Compute Optimized with GPU option is used to accelerate AI, HPC, and graphics, which is great when working with video analysis and graphic intensive use cases. </p>
<p>Both the Snowcone and Snowball can be used for many of the same use cases which I will reference in just a moment, so if that’s the case, when would you use the snowcone over the snowball and vise versa?</p>
<p>You would use the snowcone if you:</p>
<ul>
<li>Needed a portable device that you could easily carry to difficult to reach locations and situations</li>
<li>Only needed a maximum of 8TB storage</li>
<li>If you needed the ability to perform on-line data transfer using AWS DataSync, preventing you the need to send the Snowcone back to AWS for an off-line data transfer</li>
<li>If you didn’t have a consistent power support and you needed the support of a battery pack</li>
</ul>
<p>Alternatively, you would use the Snowball device if you: </p>
<ul>
<li>Didn’t need to provide mobility to the snow device and it could remain in one location for a set period of time</li>
<li>Needed to transfer data of up to 80TB</li>
<li>Needed the ability to run enhanced graphics processing by using the Compute Optimized with GPU option</li>
<li>Had a requirement to transfer data using S3 API’s</li>
<li>Required the use of usable SSD Storage </li>
<li>Needed to optimized network ports that could reach speeds of up to 100Gbit, as Snowcones only have network port speeds of 10Gbit</li>
<li>Needed to cluster your snowballs. Clustering allows you to order between 5-10 snowball devices, acting as a single pool of resources. This allows you to gain a larger storage capacity, and also enhance the level of durability of the data should a snowball fail.  Clustering is only an option if you are looking to simply perform local compute and storage workloads without transferring any data back to AWS.</li>
<li>Needed to rack mount your devices providing the opportunity to implement temporary installations of both compute and storage</li>
<li>Required the snow device to be HIPAA compliant</li>
</ul>
<p>Ok, so that should provide a better understanding of how the Snowcone and Snowball differ. Let me now run through a couple of scenarios of when you might use these devices in the real world.</p>
<p>Both the snowcone and Snowballs are perfectly suited to provide a level of portable edge computing allowing you to collect data from wireless sensors or networked resources, for example in locations such as industrial warehouses or manufacturing plants, where you might need to collect environmental metric data. By collecting and gathering data it can then be transferred to AWS offline, or if using the snowcone it can be transferred on-line using AWS DataSync, which can then be analyzed at scale using other AWS services.</p>
<p>With storage capabilities of up to 80TB of usable HDD storage from a single snowball, it easily allows you to provide a means of securely storing and transferring a large amount of data into AWS, and you can run multiple snowballs in parallel allowing you to transfer petabytes of data if required. Being of rugged design and portable, the devices can be used in remote locations, such as mining and oil sectors, or even in the travel industry, fitted to trucks, trains, and boats, providing a mechanism of easily collecting data and then transferring it back to AWS.</p>
<p>Another common use case is from within the media and entertainment industry, the Snowcone and snowball can be used as a way to aggregate data from multiple sources before shipping it back to AWS for transfer into Amazon S3. You might get video and audio data from multiple feeds, especially if you are working in the film industry, this data can then be aggregated to your snowball device and shipped back to AWS for further processing and editing from your wider production team.</p>
<p>So we have covered Snowball and Snowcone, but let’s now turn the attention to the AWS Snowmobile, what is the primary use case for this? Well, it’s quite simple, the AWS Snowmobile is used to transfer MASSIVE amounts of data from a single location, up to 100PB per snowmobile which arrives on a truck as a ruggedized shipping container. When you are talking about data transfer of this scale you are normally looking at migrating entire data centers to a new location, or migrating entire storage libraries or repositories, and so AWS snowmobile is a great solution to help you with this when you need it done quickly, securely and cost-efficiently. You can run multiple snowmobiles in parallel which will allow you to transfer Exabytes of data! Generally, you would use AWS snowmobiles if you needed to transfer more than 10petabytes of data, anything less than this then you might want to consider using multiple snowball devices. </p>
<h1 id="Key-Features-of-the-Snow-Family"><a href="#Key-Features-of-the-Snow-Family" class="headerlink" title="Key Features of the Snow Family"></a>Key Features of the Snow Family</h1><p>The snowcone and the snowball are similar in their size and construction with regards to the casing compared to that of the snowmobile which is in a class of its own, I mean it comes on a Truck! </p>
<p>So let me talk about the Snowcone and Snowball for a moment. These two hardware appliances share some key features between them that I want to highlight. </p>
<p>When transferring data to these devices it is automatically encrypted to protect the data. This encryption is backed by keys generated by the Key Management Service (KMS). To enhance the security of the device, the encryption keys are not stored on the device during transit. If you would like to learn more about the Key Management Service, then you can see our existing course here. </p>
<p>Following on from encryption, another security feature that these devices have in common is that their enclosure is Anti-tamper in addition to specific verification checks on the boot environment when the device is first switched on. These measurements and checks help to validate the integrity of the data to ensure that it has not been interfered with at all during transit.</p>
<p>In addition to their enclosure being secure by design, it is also highly rugged and able to withstand the harshest of environments, for example, the snowcone can operate in conditions of -32ºC&#x2F;-25.6ºF to 63ºC&#x2F;145.4ºF. It’s all windproof, dustproof and water-resistant! The enclosures are designed to withstand operational vibrations and shockproof should you drop the device.</p>
<p>Each of the snow devices uses an E Ink shipping label which is pre-loaded with the delivery details entered on the associated job. When the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">snow device</a> leaves AWS premises it can be tracked via SNS, text messaging and the AWS Management Console. When the device is prepared to be returned to AWS premises, the E Ink automatically updates with the appropriate location</p>
<p>As these devices are packed full of data, it’s essential that the data is deleted once the transfer has been completed and the snow device is no longer required. As a result, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> carries out a secure erase which meets the National Institute of Standards and Technology, more commonly known as NIST for the sanitization of the media and storage. </p>
<p>Before I finish this lecture I just want to point out a couple of features related to the snowmobile. </p>
<p>Each snowmobile is sent with a connector rack allowing you to connect it to the backbone network of your own data center, as a result, this rack comes with up to 2 kilometers of network cabling.</p>
<p>The Snowmobile is designed to operate within ambient temperature up to 85F (29.4C). If the temperature exceeds this then an additional auxiliary chiller unit can be supplied by AWS following a site assessment survey. If there isn’t sufficient power to feed the snowmobile at the data center then AWS can also send a separate generator to power the snowmobile, however, this requires the same space required to home a snowmobile.</p>
<p>As with the Snowcone and snowball, the snowmobile also encrypts data backed by the Key Management Service. Also, the snowmobile is only ever operated by AWS personnel and can also be escorted by an additional security vehicle during the transit of the container to and from premises, in addition to having GPS tracking available. As added security, the container is also protected via 24&#x2F;7 video surveillance systems and alarms.</p>
<h1 id="AWS-OpsHub"><a href="#AWS-OpsHub" class="headerlink" title="AWS OpsHub"></a>AWS OpsHub</h1><p>This is going to be a very quick review of AWS OpsHub, which has been designed with the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/key-features-of-the-snow-family/">AWS Snow family</a> of services in mind. It provides a graphical user interface to help you manage your snow family of devices. It doesn’t come as a service as you would normally see in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Management Console, instead, it’s an application that can be downloaded onto a Mac OS or Windows client. </p>
<p>When you receive a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">snow device</a> from the snow family, you can download the AWS OpsHub management software which can then be used to unlock your device allowing you to configure and manage it and it’s operations. As a result, within a very short space of time, you are able to begin your data transfer through simple drag and drop operations. Previously, you were required to configure and manage the device by using the AWS CLI or REST APIs, this approach of using AWS OpsHub makes it a lot more intuitive and easier to do.</p>
<p>It can also be used to configure fleets of clustered snow devices if you have requested more than one. It also comes with a dashboard which provides an overview and summary screen displaying metrics relating to your snow device and these metrics relate to both storage and compute resources. Integrating with AWS Systems Manager, it can also easily help you with the automation of different tasks.</p>
<p>So in summary, it’s a very useful tool allowing you to quickly and easily manage your snow device upon arrival using a simple GUI.</p>
<h1 id="2Overview-of-Amazon-S3"><a href="#2Overview-of-Amazon-S3" class="headerlink" title="2Overview of Amazon S3"></a>2<strong>Overview of Amazon S3</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-global-infrastructure/">AWS global infrastructure components</a></p>
<h1 id="5Overview-of-EBS"><a href="#5Overview-of-EBS" class="headerlink" title="5Overview of EBS"></a>5<strong>Overview of EBS</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">Course: How to Use KMS Key Encryption to Protect Your Data</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/how-to-encrypt-an-ebs-volume-the-new-amazon-ebs-encryption/">Blog post: How to Encrypt an EBS Volume</a></p>
<h1 id="6What-is-the-Amazon-Elastic-File-System"><a href="#6What-is-the-Amazon-Elastic-File-System" class="headerlink" title="6What is the Amazon Elastic File System?"></a>6<strong>What is the Amazon Elastic File System?</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region">List of supported regions</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:29" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:29-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:01:36" itemprop="dateModified" datetime="2022-11-20T19:01:36-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Knowledge-Check-Compute-CLF-C01-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<p><object data="Knowledge-Check-Compute-CLF-C01.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7/" class="post-title-link" itemprop="url">AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:03:27" itemprop="dateCreated datePublished" datetime="2022-11-18T22:03:27-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:02:06" itemprop="dateModified" datetime="2022-11-20T19:02:06-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Practitioner/" itemprop="url" rel="index"><span itemprop="name">AWS-Practitioner</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Cloud-Practitioner-Create-Your-First-Amazon-EC2-Instance-Windows-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/45/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/45/">45</a><span class="page-number current">46</span><a class="page-number" href="/page/47/">47</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/47/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
