<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/133/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/133/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/" class="post-title-link" itemprop="url">AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:14:57" itemprop="dateCreated datePublished" datetime="2022-11-14T11:14:57-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:32:02" itemprop="dateModified" datetime="2022-11-15T00:32:02-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Configuring-Azure-Blob-Storage-Lifecycle-Management-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Configuring-Azure-Blob-Storage-Lifecycle-Management"><a href="#Configuring-Azure-Blob-Storage-Lifecycle-Management" class="headerlink" title="Configuring Azure Blob Storage Lifecycle Management"></a>Configuring Azure Blob Storage Lifecycle Management</h1><p>Welcome to “Configuring Azure Blob Storage Lifecycle Management”. I’m Guy Hummel. To get the most from this course, you should already have some knowledge of Azure Blob Storage. In this short course, I’ll explain how you can save money by setting up lifecycle management policies to automatically move your blobs to less expensive access tiers when certain conditions are met.</p>
<p>First, let’s review Azure Blob Storage access tiers. The Hot tier is designed for blobs that will be accessed frequently. This is the default tier. It has the most expensive storage costs but the lowest access costs.</p>
<p>The Cool tier has lower storage costs but higher access costs. That’s why you should only put infrequently accessed blobs in the Cool tier. If you were to put frequently accessed blobs in this tier, then the access costs would very quickly outweigh the lower storage costs. Also, you have to leave data in the Cool tier for at least 30 days. If you delete it or move it to a different tier in less than 30 days, you’ll have to pay an early removal penalty.</p>
<p>The Archive tier has the lowest storage costs and the highest access costs. It’s intended for data that you rarely need to access, such as long-term backups. To avoid an early deletion penalty, you need to leave data in the Archive tier for at least 180 days. It’s significantly different from the hot and cool tiers because you can’t access your data right away when you need it. That’s because it uses offline storage. So you have to wait for your data to be rehydrated before you can access it. This process can take up to 15 hours.</p>
<p>Since there’s a big difference in cost between the different tiers, it’s a good idea to set up an automated system to move your data between tiers when the time is right. For example, you might have monthly reports that get accessed frequently in the first month after they’re created, are accessed much less frequently for the next 11 months, and are rarely accessed after that but need to be retained for a total of 10 years for compliance reasons.</p>
<p>In this case, you might want to create a policy that:</p>
<ul>
<li>Moves these reports from the Hot tier to the Cool tier after 30 days,</li>
<li>Moves them from the Cool tier to the Archive tier when they’re 12 months old,</li>
<li>And deletes them from the Archive tier after they’re 10 years old.</li>
</ul>
<p>Notice that I said 12 months instead of 11 months for the second one because the blobs would have already been in the Hot tier for 1 month before being moved to the Cool tier, so they would be 12 months old. I’ll show you two different ways to set up these rules: using the Azure portal and using the command line.</p>
<p>First, let’s use the portal. I’ve already created a storage account and a blob container. In the menu on the left, select “Lifecycle management”. This is where we create the rules we want. Notice that it says lifecycle management is only available for general-purpose v2 accounts and blob storage accounts. Since most storage accounts are general-purpose v2 accounts these days, you probably won’t need to worry about having the right account type.</p>
<p>Now click “Add a rule”. Let’s call it “LifecycleForReports”. We’ll leave the rule scope as “Apply rule to all blobs in your storage account”. We’ll leave the blob type as just “Block blobs”. Append blobs are used for files that frequently have new data appended to them, which likely wouldn’t be the case for our monthly reports.</p>
<p>You might not be familiar with the blob subtype. If you turn on blob versioning, then a copy of the blob will be saved every time the blob is modified. That way, you could retrieve a previous version of the blob if you needed to. Snapshots are similar except that you create them manually. If you don’t use either versioning or snapshots, then you can just leave “Base blobs” checked.</p>
<p>Okay, now we can click “Next” and define the rule. A rule needs to contain one or more conditions. The first condition we want to have is to move blobs to Cool storage after 30 days, so let’s change this to “If base blobs were created more than 30 days ago, then move to cool storage. Then we click “Add conditions” again to add the next one, which is to move blobs from Cool storage to Archive storage when they’re 12 months old. So, we’ll put 365 days since they were created.</p>
<p>Finally, we’ll click “Add conditions” again, and this time, we’ll say that after 3,650 days (which is 10 years if you don’t count leap years), then delete the blob.</p>
<p>Now we click “Add” and we’re done. However, this lifecycle policy won’t go into effect immediately. Azure only runs policies once a day, so it can take up to 24 hours before any actions triggered by a new policy will take place.</p>
<p>This example was pretty simple, but there are some other features we can use to customize it. First, we could create a rule that only applies to certain blobs. To do this, we’d need to create a filter. Technically, when we set the blob type to only block blobs, that was a filter, but we can also create a filter that looks at the name of a blob.</p>
<p>For example, suppose we want a lifecycle rule to only apply to blobs that start with the word “report”, then we’d select “Limit blobs with filters”. Now there’s a tab called “Filter set”. There are two types of filters we can apply: blob prefix and blob index match. The second one is more complicated and requires tagging your blobs before it’ll work. Blob prefix is the one we need for our example. We just need to type “report” in this field. And we have to click the Update button. Now the rule will apply to any blobs that start with those letters.</p>
<p>Here’s another change we can make. Suppose you don’t want a blob to be moved to the Cool tier until a certain number of days after the last time it was accessed rather than since it was created. You might have noticed that this wasn’t an option in the list of possible conditions, so how would we do it? First, we have to check the “Enable access tracking” box. Then, when we go back to the first condition, the list includes “Last accessed”, so we can select it. And click “Update”.</p>
<p>This visual interface makes it very easy to create lifecycle rules, but if you had to apply lifecycle policies to lots of different storage accounts, it would probably be faster to create a standard configuration and apply it using the command line. Then you wouldn’t have to keep pointing and clicking in the interface over and over again.</p>
<p>To do this, you need to create a JSON file that contains your rules. But this can be a bit of a daunting task, so there’s a shortcut you can use. Once you’ve created one or more rules in this interface, you can go to the Code View tab, and there’s the JSON code you need.</p>
<p>The rule definition is divided into two sections: the rule actions and the rule filters. For each of the conditions we created, it shows the action to take and the condition required to take that action. Here’s the one for moving blobs to the Cool tier. Here’s the one for moving to the Archive tier, and here’s the one for deleting blobs. In the filters section, it shows the blob type filter and the prefix match filter that we set.</p>
<p>If we wanted to change anything, we could actually edit it right here and click the Save button, which would apply the updated policy to this storage account. But in most cases, we’d probably want to download the code, modify it, and use the command line to apply it to other storage accounts.</p>
<p>So, we’ll click “Download”. Then, to avoid having to install the Azure command-line utility on your desktop, we can upload the JSON file to the Cloud Shell and run the command there. So, I’ll open the Cloud Shell. Then I’ll select “Upload”. Here’s the file we downloaded. It’s called “policy.json”.</p>
<p>I’m not going to modify anything, but I’ll show you what command you’d use to apply this lifecycle policy to a storage account. I put the command in the transcript below in case you want to try this yourself.</p>
<p>It starts with “az storage account management-policy create”. Then you specify the account name. My storage account is called “camonthlyreports”. Then you type “–policy” and the name of the JSON file. And finally, you tell it the resource group where your storage account resides. I called mine “camonthlyreportsrg”. Now hit Enter. It might not be obvious from this output, but it successfully applied the policy to that storage account. </p>
<p>And that’s it for Blob Storage lifecycle management. Please give this course a rating, and if you have any questions or comments, please let us know. Thanks!</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19/" class="post-title-link" itemprop="url">AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:14:08" itemprop="dateCreated datePublished" datetime="2022-11-14T11:14:08-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:30:58" itemprop="dateModified" datetime="2022-11-15T00:30:58-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer"><a href="#Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer" class="headerlink" title="Managing Azure Storage With AzCopy and Azure Storage Explorer"></a>Managing Azure Storage With AzCopy and Azure Storage Explorer</h1><p>Welcome to Managing Azure Storage with AzCopy and Azure Storage Explorer. I’m Guy Hummel. To get the most from this course, you should already have some knowledge of Azure Storage. I’ll show you two different ways of working with Azure Storage. If you prefer a graphical user interface, then you can use Azure Storage Explorer, which is an application that runs on Windows, MacOS, and Linux. If you’re comfortable with the command line, then you can use AzCopy. Let’s start with Azure Storage Explorer. First you need to download it from Microsoft. The URL for this page is in the transcript below. Select your operating system and click the ‘Download’ button. On a Mac, you just need to open the zip file and then run the Storage Explorer application. On Windows and Linux, you need to run an installation program. The first thing you’ll need to do is sign into your Azure Storage account. Unless you’re using a special Azure environment, leave this set to Azure and click ‘Next’.</p>
<p>It opens a browser tab and asks you to authenticate. I’m already logged in, so once I select my account, I’m authenticated and I can close this tab and go back to Storage Explorer. Over here, you can choose your subscription and your storage account. I have an account called camonthlyreports and a blob container in it called monthly-reports. When I click on the container, it shows me the blobs in it. I don’t have any blobs in this container which is why nothing shows up here. I’ll upload some files into it so we have something to work with. You can either upload a whole folder or individual files. If I were to choose folder, then it would create a virtual folder inside the container, which isn’t what I want in this case. So, I’m going to upload the files that are in the folder. I can select all of the files by holding down the ‘Shift’ key when I click the bottom one. That way they’ll all get uploaded at the same time.</p>
<p>Now all of the files are in the container. If you right click on one of the files, you can see that there are lots of options for what we can do with it. For example, we could copy it, or delete it, or change its access tier, such as if we wanted to move it to the cool tier. I’ll show you how to copy a blob to another container. First, let’s create a second container so we have a place to copy the blob to. If we right-click on blob containers, there’s an option to create a blob container. Let’s call it Copies. Now, if we go back to the first container, we can right-click on a file and select ‘Copy’. It’s not obvious that we did anything, but if we go to the second container, we can click the ‘Paste’ button and it’ll copy that blob into this container. You might have noticed something interesting down here. It says ‘Copy AzCopy Command to Clipboard’. It turns out that Azure Storage Explorer uses the AzCopy command when it needs to copy blobs. And if you want to see the actual command, you can click here.</p>
<p>Now it’s on the clipboard, so we have to paste it somewhere to see it. I’ll paste it to a text editor. It kind of looks like gibberish, doesn’t it? Well it’s not as bad as it looks. The basic command is just ‘azcopy copy’ then the URL of the source and the URL of the destination. So, why do you have to put ‘copy’ after ‘azcopy’? That’s because the AzCopy command can also do other things besides just copying files, so the name of the utility is a bit misleading. For example, if you wanted to delete a blob, you’d start the command with azcopy remove. Okay, so why are the URLs so long? That’s because the URL contains a shared access signature, or SAS. It’s the part after the question mark. A SAS gives access to resources in a storage account for a limited amount of time. You can generate a SAS yourself if you want to give someone access to a particular resource for, let’s say, the next week. But Storage Explorer generated a SAS for the source and a SAS for the destination for you.</p>
<p>You don’t have to use them though, because there’s another way to provide authentication. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-azure-active-directory-3142/introduction/">Azure Active Directory</a>. If you have an Azure account that’s authorized to access the source and destination, then you don’t need to use shared access signatures. There’s something very important you need to know to get this to work though. Even if you’re the owner of the source and destination storage accounts, you could still get a permission error. You’re probably thinking, what? How is that possible? Well, it turns out that your Azure Active Directory account needs to have the Storage Blob Data Contributor role for the source and destination storage accounts if you want to copy blobs using AzCopy. In this example, I’m using the same storage account for both the source and the destination, so I only need to give myself the Storage Blob Data Contributor role for that storage account.</p>
<p>To see whether you have that role assigned to you or not, go into the storage account, select ‘Access Control’, and then click ‘View my access’. You can see that I have that role assigned to me, but if you don’t then click on the role assignments tab and assign it to yourself. Or if you don’t have permission to do that, then ask an administrator to do it for you. If that would be a problem, then you could use shared access signatures instead. Okay, now I’ll show you how to run the AzCopy command. I could install AzCopy on my desktop, but since it’s already installed in Cloud Shell, let’s use that instead. But we still need to do one more thing first. Type ‘azcopy login’ to authenticate. Then go to this URL and copy and paste this code. Then choose your Azure account and click ‘Continue’. All right, now we can finally try to copy a blob. First type azcopy copy, then put in the URL of the source blob.</p>
<p>This is the name of the storage account, this is the name of the container, and this is the name of the blob. Note that if we were including a shared access signature at the end of the URL, we’d need to put quotes around the whole thing, but since we’re not using a SAS, we don’t need to. Now put in the URL of the destination. If we wanted to give the blob copy a different name than the original one, we type a different name here. We could even leave the blob name out and just give the URL of the container where we want to copy the blob, but it doesn’t hurt to give the name. In the AzCopy command we got from Azure Storage Explorer, there were some options at the end. The first one was overwrite&#x3D;prompt. This means that if the command will overwrite a blob in the destination, then it should ask whether you actually want to overwrite it before doing so.</p>
<p>Since we’re copying the same blob that we already copied using Storage Explorer, this option will matter for our example. If we don’t include this option, then it’ll overwrite the blob by default. Let’s include this option so we can see what the prompt looks like. The next one is s2s-preserve-access-tier&#x3D;false. This tells it not to ensure that the access tier of the destination blob is the same as the one for the source blob. By default, this option is true. For our example, there’s no good reason to set this to false, so we can leave this option out entirely. The next one is include-directory-stub&#x3D;false. This is kind of an obscure option and it’s false by default anyway, so we can leave it out. The next one is recursive, which means that if you’re copying a folder from your local file system, it will recursively copy its subfolders as well. That’s not what we’re doing, so we can leave this out too. The last one is log-level&#x3D;INFO.</p>
<p>This refers to how much information is sent to the AzCopy log file and INFO is the default, so we don’t need to include this one either. Okay, let’s run it. There’s the prompt we were expecting that’s asking if we want to overwrite the blob. To say yes, type y. And it’s done. If you want to see all of the options that are available, you can type azcopy copy -h. But it’s easier to read if you go to this web page. Now after seeing that AzCopy is more difficult to use than Azure Storage Explorer, you might be wondering why you’d even use AzCopy. After all, Storage Explorer actually runs AzCopy under the hood, so you don’t have to use it yourself. Well, if you need to automate your copying activities, then you can write scripts that contain AzCopy commands. It’s also possible to do it by calling the Azure Storage Client Libraries in your code. They’re available for a variety of languages such as .NET, Java, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/python-beginners/hello-world/">Python</a>.</p>
<p>But if you want to put something together quickly, it’s much easier to use AzCopy. However, Azure Storage Explorer is better than AzCopy for general storage management because it supports more operations and storage types than AzCopy does. You can use it to manage not only blobs but also queues, tables, file shares also known as Azure Files, and Data Lake Storage Gen2. The Azure Storage Client Library support all of the storage types too, of course, but you have to write code to use them, so they’re not meant for doing storage management manually. AzCopy supports blobs, file shares, and Data Lake Storage Gen2, but it doesn’t support queues or tables, so it’s not as robust as Storage Explorer. And that’s it for AzCopy and Azure Storage Explorer. Please give this course a rating, and if you have any questions or comments, please let us know. Thanks.</p>
<h2 id="1Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer"><a href="#1Managing-Azure-Storage-With-AzCopy-and-Azure-Storage-Explorer" class="headerlink" title="1Managing Azure Storage With AzCopy and Azure Storage Explorer"></a>1<strong>Managing Azure Storage With AzCopy and Azure Storage Explorer</strong></h2><p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/products/storage/storage-explorer">Azure Storage Explorer</a></p>
<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/azure/storage/common/storage-ref-azcopy-copy">Azcopy Copy</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18/" class="post-title-link" itemprop="url">AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:13:27 / Modified: 19:32:32" itemprop="dateCreated datePublished" datetime="2022-11-14T11:13:27-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Knowledge-Check-Introduction-to-Azure-Storage-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Introduction-to-Azure-Storage-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Introduction-to-Azure-Storage-17/" class="post-title-link" itemprop="url">AZ-204-Introduction-to-Azure-Storage-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:12:45" itemprop="dateCreated datePublished" datetime="2022-11-14T11:12:45-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:29:10" itemprop="dateModified" datetime="2022-11-15T00:29:10-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Introduction-to-Azure-Storage-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Introduction-to-Azure-Storage-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Hi there. Welcome to Introduction to Azure Storage. My name is Thomas Mitchell and I’ll be taking you through this course.</p>
<p>I’m an Azure instructor at Cloud Academy and I have over 25 years of IT experience, several of those with cloud technologies. If you have any questions about this course, feel free to connect with me on LinkedIn, or send an email to <a href="mailto:&#x73;&#x75;&#x70;&#112;&#111;&#114;&#116;&#64;&#99;&#108;&#x6f;&#117;&#x64;&#97;&#99;&#x61;&#100;&#x65;&#109;&#x79;&#46;&#99;&#x6f;&#x6d;">&#x73;&#x75;&#x70;&#112;&#111;&#114;&#116;&#64;&#99;&#108;&#x6f;&#117;&#x64;&#97;&#99;&#x61;&#100;&#x65;&#109;&#x79;&#46;&#99;&#x6f;&#x6d;</a>.</p>
<p>This course is intended for those who wish to learn about the basics of Microsoft Azure storage.</p>
<p>We’re going to start things off with an introduction to azure storage. You’ll learn about the core storage services in Azure and about the different storage account types that are available. You also get to see a demonstration that shows you how to create a storage account in Microsoft Azure.</p>
<p>We’ll then take a look at the different storage services in a little more detail. We will start with blob storage, where you will learn what it is and what it offers. Next, will dive into Azure Files.</p>
<p>After covering Azure Files, we’ll dive into Azure queues. You’ll learn what Azure queues are and how to create a queue.</p>
<p>After learning about Azure queues, you’ll learn about Azure tables, where you will learn why and when to use Azure table storage.</p>
<p>We’ll wrap things up by taking an introductory look at Azure disks. </p>
<p>By the time you finish this course, you should have a solid understanding of the storage basics of Microsoft Azure.</p>
<p>We’d love to get your feedback on this course, so please give it a rating when you’re finished. If you’re ready to learn about Azure storage, let’s get started.</p>
<h1 id="Core-Storage-Services-in-Azure"><a href="#Core-Storage-Services-in-Azure" class="headerlink" title="Core Storage Services in Azure"></a>Core Storage Services in Azure</h1><p>Hello and welcome to core storage services in Azure. In this lesson, we will take a look at some of the core storage offerings, including blob storage, Azure Files, queue storage, table storage, and disk storage. However, before we get into the separate storage services, let’s take a look at some of the benefits of Azure storage.</p>
<p>Because the Azure storage platform is designed for virtually all modern-day data storage requirements, it was built to offer several key benefits. First and foremost, all Azure storage services are durable and highly available. Built-in redundancy keeps data safe in the event of underlying hardware failures within the Azure infrastructure. In addition to this redundancy, you can also cross multiple data centers and even multiple geographical regions. This provides protection against natural disasters or local failures at the data center level.</p>
<p>Because all data that gets written to Azure storage accounts is encrypted automatically, it is inherently secure as well. Azure also offers the ability to maintain fine-grained control over data access. </p>
<p>Azure storage offerings are also scalable and widely accessible. As I mentioned earlier, the Azure storage platform is designed to support all modern-day data storage requirements. That being the case, it’s designed to be massively scalable. You can access Azure data from anywhere in the world over HTTP or HTTPS. There are also numerous client libraries available for Azure storage in numerous languages. This means you can access Azure storage using things like .NET, Java, Python, PHP, and many others.</p>
<p>You can also access Azure storage through PowerShell scripting, through the Azure CLI, through the Azure portal, and through Azure Storage Explorer.</p>
<p>So now that you understand some of the key benefits of the core Azure storage services, let’s take an introductory look at each of them.</p>
<p>Let’s start with Azure blobs.</p>
<p>Azure blob storage provides object storage for the cloud. This offering has been optimized to support massive amounts of unstructured data. Unstructured data is data that does not fit a specific data model. For example, text and binary data would be the type of unstructured data that falls under blob storage.</p>
<p>Azure Files is essentially a fully managed file share system available in the cloud. You can access Azure Files through the typical SMB protocol. You can mount Azure file shares from Windows, Linux, and MacOS machines that reside both on-prem and in the cloud. You can even cash Azure file shares on Windows servers, using the Azure file sync service. This helps make data more accessible for remote offices.</p>
<p>Azure queue storage is a special type of storage service. It’s not meant for storing files. Instead, it’s designed for storing large numbers of messages. These messages are used in communications between the different components of a distributed application. Such messages can be accessed from anywhere in the world through authenticated calls via HTTP or HTTPS. Each queue message can be up to 64 KB in size. A typical queue can contain millions of messages.</p>
<p>Azure table storage is a storage offering intended for the storage of structured NoSQL data. It offers a key&#x2F;attribute store and a schema-less design. The schema-less design of Azure table storage allows you to more easily adapt the data to the needs of your business or application.</p>
<p>Lastly, let’s take a look at Azure managed disks. Azure managed disks are essentially block-level storage volumes. These storage volumes, which are managed by Azure, are used, to provide storage capabilities for virtual machines. A managed disk is much like a physical desk that you would see in an on-prem server. However, it’s virtualized. There are several types of disks available in Azure. They include ultra disks, premium SSD disks, standard SSD disks and standard HDD disks. We will touch on these in more detail later on.</p>
<p>So, to wrap things up for this lesson, let’s review. There are essentially five different core storage services available in Microsoft Azure. They include Azure blobs, Azure Files, Azure queues, Azure tables, and Azure disks. Each of these storage services are accessed through an Azure storage account. Over the next several lessons, we will dive into storage accounts and into the details of each core storage service in Microsoft Azure.</p>
<h1 id="Storage-Account-Types-in-Azure"><a href="#Storage-Account-Types-in-Azure" class="headerlink" title="Storage Account Types in Azure"></a>Storage Account Types in Azure</h1><p>Hello and welcome to storage account types in Microsoft Azure. In this lesson, we are going to take a look at the different types of storage accounts that are available when it comes time to provision storage in your Azure subscription.</p>
<p>Before we get into the different types of storage accounts. Let’s talk a little bit about what a storage account is and what it’s used for. A storage account in Azure can be viewed as the container, so to speak, that houses all of your Azure storage data objects. A storage account can host blobs, Azure Files, queues, tables, and disks.</p>
<p>When you provision a storage account. You are asked to provide a unique name for that storage account. This is necessary because data within a storage account is accessible from anywhere in the world. That being the case, the storage account namespace must be unique across the Azure landscape.</p>
<p>There are several different types of storage accounts available. Each type offers different features, and each has a different pricing model.</p>
<p>The first type of storage account is the general-purpose V2 account. A general-purpose V2 storage account is a basic storage account that can be used, to host blobs, files, queues, and tables. Microsoft recommends using the general-purpose V2 storage account for most scenarios that require Azure storage.</p>
<p>The general-purpose V1 storage account is similar to the V2 account. This legacy type account can also host blobs, files, queues, and tables. While a general-purpose V1 account offers similar functionality to the V2 accounts, Microsoft recommends using the general-purpose V2 account instead. This tells me that the general-purpose V1 storage account will probably go away at some point in the future.</p>
<p>The next type of storage account we are going to take a look at here is the block blob storage account. Block blob storage accounts offer premium performance for block blobs and append blobs. You would typically use a block blob storage account for situations where high transaction rates are in play. Block blob storage accounts are also a good choice for scenarios that require low storage latency.</p>
<p>File storage accounts are exactly what the name says they are. They are files only storage accounts. Because they feature high-performance characteristics, Microsoft recommends using these kinds of accounts for enterprise applications or for high-performing applications.</p>
<p>The last storage account type touch on here is the blob storage account. The blob storage account is a legacy account that is used for blob only storage. Microsoft actually recommends that, instead of using blob storage accounts, you use general-purpose V2 accounts. Like the general-purpose V1 accounts, I suspect the blob storage accounts, because of their legacy status, will likely go away sometime in the future.</p>
<p>The table that you see on your screen right now shows each of the different types of storage accounts, along with their key features. As you can see on your screen. Every storage account type is encrypted using storage service encryption, or SSE, for data that is at rest.</p>
<p>I should point out that archive storage and blob level tiering only support block blobs.</p>
<p>Another key point I should mention here is that zone redundant storage and Geo zone redundant storage are only available for standard general-purpose V2 accounts, block blob accounts, and file storage accounts in certain regions.</p>
<p>The premium performance that you see listed for general-purpose V2 and general-purpose V1 accounts is only available for disk storage and page blobs. Premium performance for block blobs and append blobs is only available for block blob accounts. Also, just as importantly, premium performance for files is only available on file storage accounts.</p>
<p>For more information on the different storage accounts available in Microsoft Azure, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview">https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview</a></p>
<h1 id="DEMO-Creating-a-Storage-Account"><a href="#DEMO-Creating-a-Storage-Account" class="headerlink" title="DEMO: Creating a Storage Account"></a>DEMO: Creating a Storage Account</h1><p>Hi everyone and welcome back. In this brief demonstration, I’m going to show you how to create a storage account using the Azure portal. Now, every storage account needs to belong to a resource group. Now a resource group, as you may know is a logical container that groups all of your Azure resources.</p>
<p>Now when we create our storage account here, we’re going to have two options, we can create a storage account and then as part of that process, we can create the new resource group or we can select an existing resource group. What we’re going to do here is add this new storage account to an existing resource group.</p>
<p>So on the screen here, I’m logged into my Azure portal and I’m at the home page. To create our general purpose V2 storage account, what we’ll do here is go up to the hamburger here and select all services. And then from here, we can search for storage accounts. And we’ll select it from the results page here. And we can see in my existing subscription here, I already have a storage account called VMLab diag 601.</p>
<p>What we’re going to do here is create a new one by clicking Add. And then from here, we need to provide some basic information, we need to specify the subscription that’s going to host the storage accounts. We need to select the resource group we want to deploy the storage account into or we can create a new one here. And then of course, we need to give our storage account a name, tell Azure which locations going into and then the performance of the storage account.</p>
<p>We can see that standard storage accounts are backed by magnetic drives and they are the cheapest cost per gigabyte. We can see in this popup, that standard storage accounts are good for applications that need bulk storage or where you’re accessing the data within that storage account infrequently. The premium storage accounts are backed by solid state drives. You use these when you need a consistent low latency performance.</p>
<p>Now you will notice here that a premium performance storage account can only be used with Azure virtual machine disks and they are best for IO intensive applications like databases. Another important item to note here is that virtual machines that use premium storage for all of their discs, do qualify for a 99.9% SLA even if they’re not running within an availability set so that’s pretty important to remember.</p>
<p>For this exercise, we’ll just use a standard storage account here and then the account kind here is where we can select what type of storage account to deploy. Here, we have three options storage V2, storage which is basically the legacy general purpose V1 and Blob storage.</p>
<p>Now in the replication dropdown, we can specify the redundancy that we need for our storage whether it be locally-redundant, zone-redundant, geo-redundant, read-access geo-redundant, geo zone-redundant or read access geo zone-redundant. Instead of going through each of these individually, you can visit the URL that you see on your screen for more information about each of these types of redundancy options. And lastly, we have the access tier here.</p>
<p>Now the hot access tier is really used for data that’s accessed frequently. The cool access tier is typically used for infrequently access data. We can also see that there is a another option and it’s the archive access tier but that can only be set at the Blob level and not on the actual storage account level.</p>
<p>So let’s go ahead and see here. We’ll deploy our storage accounts and we only have one subscription here, our labs subscription. So we’ll leave that set and then we have a couple of different resource groups here. So I’ll just deploy this into my VMLab resource group. And then we need to give our storage account a name and this name must be unique across all storage accounts in Azure, it needs to be unique across the landscape. The name can be as short as three characters but as long as 24 and it can only contain lowercase letters and numbers. So let’s call this test9878storage.</p>
<p>We get the green checkbox which means we’re good and we’ll deploy into the Central US region, that’s all we really need for this demonstration. And we’re going to deploy a general purpose V2 and we’ll accept the default read-access geo-redundant storage. We’ll do the same thing for the access tier.</p>
<p>Now, when we click Next for Networking, we can specify our network connectivity requirements, including the connectivity method and any kind of network routing or routing preferences. Now we can see here we have three options, we have a public endpoint for all networks, public end point for selected and private end points.</p>
<p>Essentially, storage accounts have a public endpoint that’s accessible through the internet. That’s what this public endpoint would be. Now, if we select public endpoint, what we do is we’re enabling that public endpoint to all networks, that’s why we have all networks here. If we select the second option here, we can see we then allow a public endpoint but then we can select which networks can access this storage account. </p>
<p>So that allows us to segment our traffic and block certain traffic. And then we have private endpoint. Now you would create a private end point to allow only private connections to this storage accounts. Now, what this would do is assign a private IP address from the virtual network that we select and it would take it and assign it to these storage accounts. And then as a result, all traffic between that virtual network and the storage account would be secured over a private link.</p>
<p>For this exercise here, we’re just going to go public and the only option we have here for routing is Microsoft network routing defaults and that’s because the combination of the storage account kind performance and replication along with location does not support internet routing which we probably wouldn’t do anyway.</p>
<p>We’ll go into data protection and then we can see, we have the Blob soft delete option, we have the file share soft delete option here and then we have versioning but versioning is not offered for this storage account due to the type of storage account combined with the subscription replication and location options.</p>
<p>So we’ll leave these at their defaults. We’ll click Next through to advanced. Now here’s where we could configure some advanced features. We could configure the secure transfer which essentially enhances the security of the storage account by only allowing the requests to that storage account by secure connections.</p>
<p>When you enable the Blob public access, the storage accounts Blobs can be read publicly without needing to share an account key or even a shared access signature. So your Blobs are wide open. We have the TLS versioning. Now this large file shares option here, turning this on provides file share support for a maximum of a hundred terabytes.</p>
<p>Now you’ll notice here that large file share storage accounts don’t have the ability to convert to geo-redundant storage offerings. And down here, we have some Data Lake Storage Gen2 options. We’ll leave these other defaults, we’ll click next for tags, we’re not going to do any tagging here. This just allows us to categorize our resources and then we’ll go ahead and click Next to Review and Create. And at this point we can see our validation has passed, we can review our configuration and then we can go ahead and click Create. And what this will do is deploy our general V2 storage account. And we can go ahead and click, Go to Resource and we are now in our test9878 storage account.</p>
<p>So that is how you walk through the process of creating a storage account in the Microsoft Azure portal.</p>
<h1 id="An-Introduction-to-Blob-Storage"><a href="#An-Introduction-to-Blob-Storage" class="headerlink" title="An Introduction to Blob Storage"></a>An Introduction to Blob Storage</h1><p>Hello and welcome to blob storage. In this lesson, we are going to take a closer look at blob storage and when you should use it.</p>
<p>Azure Blob storage, as I mentioned earlier in this course, is an object storage solution in Azure. It’s optimized to allow the storage of massive amounts of unstructured data, including text and binary data.</p>
<p>You would typically use blob storage to host images and documents that you wish to serve up to a web browser. Think images on a website. Blob storage is also used when you wish to stream video or audio or to store log files. Organizations will also often use blob storage to store backup data, archive data, and data that needs to be analyzed by some on-prem process or Azure-hosted process.</p>
<p>To access objects in blob storage, you can use HTTP or HTTPS. You can also access blob storage objects through the Azure storage rest API, through Azure PowerShell, through the Azure CLI, or through an Azure storage client library such as Java, PHP, .NET, or several others.</p>
<p>There are three types of resources that you should be familiar with when discussing blob storage. They include the storage account that hosts the blob storage, containers within the storage account, and the blobs within those containers.</p>
<p>The image on your screen depicts the relationship between these resources.</p>
<p>The storage account creates the unique namespace in Azure that you use to access your data. When you access your data, you’ll use a combination of the storage account name and the Azure Storage blob endpoint that you are trying to access. Together, these two pieces form the base address for the objects that reside in the storage account.</p>
<p>For example, the URL that you see on your screen would be used to access blob storage in a storage account called MyStorage:</p>
<p><a target="_blank" rel="noopener" href="http://mystorage.blob.core.windows.net/">http://mystorage.blob.core.windows.net</a></p>
<p>Containers within a storage account are used to organize the blobs within the account. You can view containers like directories in a file system. You can create an unlimited number of containers within a storage account, and each container can store an unlimited number of blobs.</p>
<p>There are actually three types of blobs that Azure storage supports. They include block blobs, append blobs, and page blobs. Block blobs can contain up to about 190.7TiB of text and binary data. They are called block blobs because they consist of blocks of data that can be managed individually.</p>
<p>Append blobs are similar to block blobs insofar as they, too, consist of blocks of data. However, unlike block blobs, append blobs are optimized for append operations. This makes append blobs a good choice for logging data from virtual machines.</p>
<p>Page blobs are used to store random access files up to 8 TiB in size. You would typically use page blobs to store VHD files, which would serve as disks for Azure virtual machines. </p>
<p>To learn more about blob storage, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction</a> </p>
<h1 id="DEMO-Uploading-a-Block-Blob-to-Azure"><a href="#DEMO-Uploading-a-Block-Blob-to-Azure" class="headerlink" title="DEMO: Uploading a Block Blob to Azure"></a>DEMO: Uploading a Block Blob to Azure</h1><p>Hello and welcome back. In this demonstration, we are going to create a container in our storage account. And then what we’re going to do is upload a block blob to that storage account. Now, before we upload our block blob to our storage account, we need to create a container.</p>
<p>The container is what will host our blob. So on the screen here, we have my test9878storage account. To create my container here, I simply select the containers chiclet here. And I can see I have no containers configured yet. So what we’ll do here, we’ll create a container.</p>
<p>Now the container name must be lowercase. So we’re gonna call it mycontainer. And we’ll create it. So now we have our container created. We can see that the default access level for that container is private. There’s no anonymous access to it. With our container created, we can upload our block blob to our storage.</p>
<p>To upload our block blob, we simply select our container and then we click upload here. And then in the right pane here, we can select the file we want to upload. And we’re just going to upload this, mytextdocument.text. And then we have an overwrite if it already exists. And then some advanced features here. None of these are required. We just get to specify what kind of authentication type, block size, the access tier and any folders we want to upload to.</p>
<p>For this demonstration, we’re just going right into the container. And then we’ll go ahead and click upload. And we can now see my text document listed in our container. We’ll close this out. And with that, we now have our container created and we were able to upload a block blob or file to our container.</p>
<p>Now, if we wanted to download our block blob, we simply right click it, and then we could select download here. So with that, you now know how to create a container in your blob storage and you know how to upload and download block blobs to, and from it.</p>
<h1 id="An-Introduction-to-Azure-Files"><a href="#An-Introduction-to-Azure-Files" class="headerlink" title="An Introduction to Azure Files"></a>An Introduction to Azure Files</h1><p>Hello and welcome to an intro to Azure Files. In this lesson, we are going to take a look at what Azure Files is and talk a little bit about what it offers.</p>
<p>Azure Files is an offering that makes file shares available in the cloud. It’s a fully managed solution that supports access to these cloud-based file shares via the industry-standard server message block protocol, or SMB.</p>
<p>You can mount Azure file shares from cloud deployments and on-prem deployments of not only Windows machines, but also Linux, and Mac OS machines. You can also use the Azure file sync service with Azure Files to cache your Azure file shares on Windows servers that are located close to your users. By leveraging Azure file shares with Azure file sync, you can speed data access for your end users. </p>
<p>Organizations will often use Azure Files to replace on-prem file servers or to supplement them. While earlier iterations of Azure Files were not a good replacement for on-prem file servers, this is no longer the case. Because popular operating systems like Windows, Linux, and Mac OS can mount Azure file shares, Azure Files can now completely replace traditional on-prem file servers and even NAS devices. As a matter of fact, the release of Azure Files AD Authentication means Azure file share permissions can even be controlled through on-prem active directories.</p>
<p>Azure Files is also helpful when lifting and shifting applications to the cloud. This is especially true for applications that require file shares to store application data and user data.</p>
<p>Because Azure Files are fully managed, you can create as many file shares as you need without worrying about hardware management and OS installation. This means you also have no need for OS patching or security upgrades.</p>
<p>You can also use familiar PowerShell commands and Azure CLI commands to create, mount, and manage Azure file shares. They can also be created and managed through the Azure portal and through Azure Storage Explorer.</p>
<p>Because Azure Files are built to be resilient, you no longer need to worry about file server upgrades or local power outages and network issues that typically affect access to on-prem file shares.</p>
<p>Join me in the next lesson, where I will show you how to create an Azure file share using the Azure portal.</p>
<h1 id="DEMO-Creating-an-Azure-File-Share"><a href="#DEMO-Creating-an-Azure-File-Share" class="headerlink" title="DEMO: Creating an Azure File Share"></a>DEMO: Creating an Azure File Share</h1><p>Hello and welcome back. Now, once we have a storage account created, we can go ahead and create file shares if we’re going to use the Azure file service. What I’m going to do here is show you how to create a file share within an Azure Storage account.</p>
<p>Now on the screen, you can see I’m in the storage account and I’m on the overview page. Now, from this overview page, to create a file share, I simply navigate to the file shares tab. The file shares tab allows us to see any file shares we’ve already created. And as you can see on the screen, we have none.</p>
<p>To create a new file share, we simply click the file share link here, and then we can give our file share a name and a quota. So what we’ll do here, we’ll call this file share MyShare. Now we can see the little exclamation point here and this is telling me that the file share name can only be lowercase letters, numbers, and hyphens. So we’ll go ahead and rename this myshare in lowercase letters.</p>
<p>Now, if I hover over the quota icon here, we can see that we can set a quota essentially up to five terabytes that limits the total size of this file share. We are not going to set a quota here, so we’ll go ahead and click create. And now we can see that our share is created and that we have the default quota. Since we didn’t specify one, it’s just giving us the entire size that’s available to us for the share.</p>
<p>If we select the share here, we can click the connect link here and this will give us information on how to connect to this share from Windows, Linux, or MacOS. We can also upload files directly to it, and we can add a directory, so let’s call this marketing, and now we have a directory within our myshare file share called marketing. So you can build a hierarchy of your file shares within your Azure portal.</p>
<p>So with that, you know how to create a file share in Microsoft Azure using the Azure portal.</p>
<h1 id="An-Introduction-to-Azure-Queues"><a href="#An-Introduction-to-Azure-Queues" class="headerlink" title="An Introduction to Azure Queues"></a>An Introduction to Azure Queues</h1><p>Hello and welcome to Azure queues. In this brief lecture. We are going to take an introductory look at Azure queues and what they are used for. </p>
<p>The Azure queues storage service is designed to store large numbers of messages. Now these messages aren’t the type that you would normally think of. We’re not talking about emails or anything like that. Instead, these messages are used to facilitate communication between the components of distributed applications. When using Azure queue storage, you can access these messages from anywhere in the world through authenticated calls via HTTP or HTTPS.</p>
<p>The Azure queue service is comprised of several components. These include the URL format, a storage account, a queue, and messages.</p>
<p>To access a queue, you must do so through a specific URL format. The URL for a specific queue will include the storage account name and the queue name. For example, the URL that you see on your screen would be used to access a queue called images to process in a storage account called mystorageaccount.</p>
<p>Speaking of storage accounts, all access to virtually all Azure storage services is provided through a storage account. You can view the storage account as the overarching container that hosts your Azure storage.</p>
<p>The queue itself is actually a set of messages. When you name a queue, you must use all lowercase letters.</p>
<p>And last but not least we have messages. Messages in any format can be up to 64 kB. As I mentioned previously, these messages are used to facilitate communication between the different components of a distributed application.</p>
<p>Join me in the next lesson where I will show you how to create a queue in Microsoft Azure.</p>
<h1 id="DEMO-Creating-a-Queue"><a href="#DEMO-Creating-a-Queue" class="headerlink" title="DEMO: Creating a Queue"></a>DEMO: Creating a Queue</h1><p>Welcome back. In this quick demonstration, I want to show you how to create a queue in Microsoft Azure using the Azure portal. Now, since this isn’t a course on using queues, we’re not going to get into all the details of how to use it, but I wanted to at least show you how to create a queue.</p>
<p>On the screen here, you can see I’m logged into my Azure portal. I’m at the home page and I’m logged in as an admin. To create a queue service, what I need to do is browse to my storage accounts. And then from the overview page of my storage accounts, I simply click on the queues box here.</p>
<p>Now from the queues page, I can see what existing queues I have set up. I can also see what authentication method I’m using. Now to create my queue, it’s a pretty straightforward process. I simply click the plus queue button up here and then give my queue a name.</p>
<p>Now my name for the queue needs to be all lowercase. It must start with a letter or a number, and it can only include letters, numbers, or hyphens. So I’m just going to call this myqueue. And then we’ll click okay here. And then we can see here myqueue is created and it gives me the URL to access that queue. And that’s really all there is to creating a queue.</p>
<p>Now, before we go, I’ll just show you quickly how to add a message to that queue through the portal. And to do that, I simply click on my queue here and then add a message. Now what I’ll do here, I’ll just add some kind of message text here. And we can set the expiration. I’ll just leave the defaults there.</p>
<p>Now because Microsoft recommends encoding binary data, this checkbox here for encode the message body in Base64 is checked by default. So we’ll leave it there and we’ll go ahead and click okay. And now we can see the message in our queue. If we select the message, we can look at some of the message properties. We can see the body of the message, when that message was inserted into the queue and when it expires. </p>
<p>Now, if we wanted to dequeue this message or remove it from the queue, we simply click the dequeue message. Now since this is the first and only message in my queue, it’s asking me if I’m sure I want to remove the first message in the queue. We’ll go ahead and click yes. And now we can see, we have no more messages in our queue.</p>
<p>So that’s the long and short of it on how to create a queue, how to add a message to a queue, and how to dequeue that message.</p>
<h1 id="An-Introduction-to-Azure-Table-Storage"><a href="#An-Introduction-to-Azure-Table-Storage" class="headerlink" title="An Introduction to Azure Table Storage"></a>An Introduction to Azure Table Storage</h1><p>Hello and welcome to Azure table storage. In this lesson, we will take a look at what table storage is and what it’s used for. </p>
<p>Azure table storage is a storage service offered by Microsoft that allows you to store structured no SQL data in the cloud. It provides a key&#x2F;attribute store with a schema-less design. When I say schema-less here, what I mean is that the data does not conform to a rigid schema. In other words, it doesn’t conform to specific data types that a typical relational database would conform to. </p>
<p>Organizations will often use table storage to store flexible databases that include things like user data for Web applications or maybe even address books or device information. Table storage allows you to store all kinds of entities in a table. Storage accounts can contain as many tables as you need up to the capacity limits of the storage account itself. </p>
<p>Azure table storage, which is actually a NoSQL datastore, often comes into play when an organization needs to store large amounts of structured data. It’s perfect for scenarios that require the storage of structured non-relational data. For example, table storage would be a good choice if you need to store terabytes of structured data required for web scale applications. If you have datasets that don’t require complex joins, foreign keys, or stored procedures, table storage would be a good choice, assuming those data sets can be denormalized for fast access.</p>
<p>Leveraging Azure table storage allows you to store and query massive sets of structured nonrelational data.</p>
<p>Table storage consists of several different components. The URL format for accessing Azure table storage can be seen on your screen.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://&lt;storage account&gt;.table.core.windows.net/&lt;table&gt;</span><br></pre></td></tr></table></figure>

<p>Notice the URL includes the storage account name and the table name. It’s the inclusion of these values that makes the URL address unique.</p>
<p>The storage account is another component that helps comprise the Azure table storage offering. The function of the Azure storage account is simple. Access to all Azure storage, including table storage, is provided through a storage account.</p>
<p>The third piece of Azure table storage is the table itself. All a table is is a collection of entities. What makes an Azure table storage table different from a typical relational database table is the fact that it doesn’t enforce a schema on the entities within it. This means that a single table can, and often does, contain all kinds of entities with many different sets of properties.</p>
<p>Speaking of entities, the entity is the fourth component that makes up the Azure table storage offering. The best way to describe an entity is that it is similar to a typical database row. It’s essentially a set of properties. Each entity in Azure storage can be up to 1 MB in size.</p>
<p>The properties, which make up the fifth component, are name-value pairs. You can include up to 252 properties for each entity. There are also three system properties associated with each entity as well. These include a partition key, a row key, and a timestamp.</p>
<p>The image on your screen depicts the relationship among each of these components. As you can see the storage account holds the tables. Each table consists of the entities. Each entity consists of many different properties. The URL that we mentioned earlier is how you access all of this stuff.</p>
<p>In the next lesson, I will show you how to create an Azure storage table using the Azure portal.</p>
<h1 id="DEMO-Creating-an-Azure-Storage-Table"><a href="#DEMO-Creating-an-Azure-Storage-Table" class="headerlink" title="DEMO: Creating an Azure Storage Table"></a>DEMO: Creating an Azure Storage Table</h1><p>Welcome back. In this brief demonstration, we are going to walk through the process of creating an Azure Storage table in the Azure portal. As you can see here, I’m logged into my portal as my admin, and I’m on the homepage. We have my storage account here, test9878storage. This obviously is a prerequisite to create a table.</p>
<p>So what we’ll do to begin the process of creating our storage table is select our storage account. And then from the overview page, we can select tables. Now from this table storage page, we can see I have no tables defined yet.</p>
<p>Now, one thing I didn’t mention in the lecture is that there is a premium table experience and it’s not necessarily table storage per se so I didn’t cover it in the course, but this premium table experience is offered through Cosmos DB. And if I open this up here, it takes me out to Azure Cosmos DB and what Azure Cosmos DB is is a globally distributed database service.</p>
<p>Now, what Cosmos DB offers is a premium Azure table API. And so that’s why we didn’t get into it because it’s not really table storage as part of the Azure Storage service offering. But just keep that in mind that there is a premium experience for Azure table API if it comes up in an exam somewhere.</p>
<p>So let’s go back to our test9878storage here. So to create our table, we simply click plus table and we’ll give our table a name. I’ll just call it MyTable and we’ll OK it. And from there, we can see our new table has been created along with a URL to access that table.</p>
<p>If we click the ellipsis here for the context menu, we can configure access policies or delete the table altogether. Now, since this is an introduction to Azure Storage, we’re not going to get into table design and actually doing things with the table storage. I just wanted to introduce you, hence the name of the course, to this storage option so you have an idea of what it is, where you access it and how you create a table.</p>
<p>So with that, we’ll call it a wrap and I’ll see you in the next lesson.</p>
<h1 id="Introduction-to-Azure-Managed-Disks"><a href="#Introduction-to-Azure-Managed-Disks" class="headerlink" title="Introduction to Azure Managed Disks"></a>Introduction to Azure Managed Disks</h1><p>Hello and welcome to Azure managed disks. In this lesson, we’re going to take a look at the benefits of managed disks and at the types of encryption you can use with managed disks. Let’s start by identifying what a managed disk is in Azure. An Azure managed disk is similar to a physical disk that you would find in a physical on-prem server, but it’s virtualized. It’s a block level storage volume that’s used with Azure VMs and is managed by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>.</p>
<p>Creating a managed disk in Microsoft Azure is as simple as specifying the size of the disk, and the type of the disk you want to use. When you provision a managed disk, you have a choice of several different types. You can deploy ultra disks, premium SSD disks, standard SSD disks, and standard HDD disks. We will actually look at these types in a little more detail in the next lesson. But for now, we’re just going to look at the benefits of using managed disks.</p>
<p>For starters, managed disks are designed for 99.999% availability. That’s five nines of availability. To achieve this level of availability, there are three replicas of the data stored on each managed disk. This type of durability protects you from not only one, but two failures of disk replicas.</p>
<p>Managed disks make it easy to deploy and scale VMs. Microsoft Azure allows up to 50,000 VM disks of a specific type per region in each subscription. This allows you to create thousands of virtual machines in one subscription. Because Azure supports so many disks, you can create VM scale sets that include up to 1000 VMs per set, provided you use a marketplace image. </p>
<p>I should point out that managed disks are integrated with both availability sets and availability zones. The integration with availability sets ensures that VM disks within an availability set are isolated from one another. This protects your applications from a single point of failure within an Azure datacenter. Availability zone integration protects applications from entire Azure datacenter failures.</p>
<p>Since Azure backup supports the backup and restore of managed disks, you can use Azure backup to create backup jobs to protect your data. This makes VM restores a snap. I should mention, however, at the time of this course publication, Azure backup supports disk sizes up to 32 terabytes.</p>
<p>Through Azure role-based access control, or RBAC, you can specify granular access control for managed disks. You can assign specific permissions for managed disks to your users.</p>
<p>Lastly, Azure managed disks make it easier to upload your on-prem VMs to Azure because you can use direct upload to transfer your VHD files to Azure managed disks. There are far fewer steps to uploading your VHDs than there used to be.</p>
<p>There are two types of encryption that you can use with managed disks. They include server-side encryption, or SSE, and Azure disk encryption, or ADE.</p>
<p>Server-side encryption is performed by the Azure storage service, and is enabled by default for all managed disks. This type of encryption provides encryption at rest for your data. Server-side encryption is also enabled by default for snapshots and images in regions where managed disks are available.</p>
<p>Azure disk encryption is enabled on the OS and data disks of a VM. Using Azure disk encryption, you can encrypt the OS and data disks for a virtual machine, including managed disks. On Windows VMs, the disks are encrypted using bit locker technology. While on Linux VMs, the disks are encrypted using DM-crypt technology.</p>
<p>There are three disk roles in Azure. These roles include data disks, OS disks, and temporary disks.</p>
<p>Data disks are managed disks that you attach to a virtual machine. They’re used to store applications and other sorts of data that you need. When you attach a data disk to a VM, it’s registered as a SCSI drive. You can assign a drive letter to a data disk just like any other physical disk in a physical server. Data disks have a max capacity of 32 terabytes, and the number of data disks that you can attach to a virtual machine will be determined by the size of the virtual machine itself.</p>
<p>OS disks are pretty self-explanatory. When you deploy a virtual machine, it’s deployed with a single OS disk attached. The OS disk, as you may have guessed, hosts the VM’s operating system and boot volume. The max capacity of an OS disk is four terabytes.</p>
<p>Temporary disks are probably the most misunderstood of the three disk types. Every VM contains a temporary disk. I should mention, however, that the temporary disk is not a managed disk. The temporary disk is not intended for storage of important data. Instead, temporary disks are used to host things like page files and swap files. Data that is stored on a temporary disk is often lost during maintenance events and when a VM is redeployed. The temporary disk is assigned the drive letter of D on Windows machines by default. On Azure Linux VMs, the temporary disk is &#x2F;dev&#x2F;sdb.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Congratulations. You’ve come to the end of Introduction to Azure Storage. Let’s review what you’ve learned.</p>
<p>We kicked things off with an introduction to azure storage. You learned about the core storage services in Azure and about the different storage account types that are available. You also got to see a demonstration that showed you how to create a storage account in Microsoft Azure.</p>
<p>We then took a look at the different storage services in a little more detail. We started with blob storage, where you learned what it is and what it offers. Next, we dove into Azure Files.</p>
<p>After covering Azure Files, we got into Azure queues, where you learned what Azure queues are and how to create a queue.</p>
<p>After learning about Azure queues, you learned about Azure tables. You learned why they are used and when to use Azure them.</p>
<p>We wrapped up with Azure disks. </p>
<p>At this point, you should have a solid understanding of the storage offerings in Microsoft Azure.</p>
<p>To learn more about Microsoft Azure storage options, you can, and should, read Microsoft’s published documentation. You should also keep an eye out for new courses on Cloud Academy, because we’re always publishing new ones. Be sure to give this course a rating, and if you have any questions or comments, please let us know. Thanks for watching and happy learning.</p>
<h2 id="3Storage-Account-Types-in-Azure"><a href="#3Storage-Account-Types-in-Azure" class="headerlink" title="3Storage Account Types in Azure"></a>3<strong>Storage Account Types in Azure</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview">Azure Storage Accounts Overview</a></p>
<h2 id="5An-Introduction-to-Blob-Storage"><a href="#5An-Introduction-to-Blob-Storage" class="headerlink" title="5An Introduction to Blob Storage"></a>5<strong>An Introduction to Blob Storage</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction">Azure Blob Storage Overview</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16/" class="post-title-link" itemprop="url">AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:12:15 / Modified: 11:12:16" itemprop="dateCreated datePublished" datetime="2022-11-14T11:12:15-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Integrate-Services-with-Azure-Function-Apps-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Introduction-to-Azure-Functions-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Introduction-to-Azure-Functions-15/" class="post-title-link" itemprop="url">AZ-204-Introduction-to-Azure-Functions-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 11:11:30" itemprop="dateCreated datePublished" datetime="2022-11-14T11:11:30-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-15 00:12:24" itemprop="dateModified" datetime="2022-11-15T00:12:24-04:00">2022-11-15</time>
              </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Introduction-to-Azure-Functions-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Introduction-to-Azure-Functions-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Aloha, I’m David Gaynes. And I have been developing .NET and cloud software for more than 20 years. I spent some time at Microsoft, and I’ve written a book or two in the past as well. This course is intended for software developers who want to learn how to implement <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/azure-functions-overview/">Azure Functions</a> as a part of their cloud software design. You don’t need any specific background to get something out of the course, but it will certainly help if you understand event driven programming, what servers and APIs are in general and if you have experience with C#, JSON and how to create a project in Visual Studio.</p>
<p>Azure Functions are little bits of your application logic that live in the cloud. You can think of them in the same way that you think of any API endpoint, meaning you can get to an <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Function using HTTP. You just call them whenever you need to run that logic. The course will include how to activate, or what we call trigger, your Azure Functions, how to pass data to and from them and also how to tie different Azure Functions together.</p>
<p>Note that this course covers Azure Functions 2.0 and up. As is typical with Microsoft, there were several breaking changes from V1 to V2. So if you have used Azure Functions V1 in the past, then, obviously, there are some places where the material in this course won’t match what you are used to.</p>
<p>For any additional help, just email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. And please rate my course when you’re finished. Thanks! And on we go.</p>
<h1 id="Azure-Functions-Overview"><a href="#Azure-Functions-Overview" class="headerlink" title="Azure Functions Overview"></a>Azure Functions Overview</h1><p>You would typically use <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a> to run bits of your overall application logic on-demand in the same way that you use any other HTTP-accessible endpoint. Just as with any other API endpoint or library function, you create a serverless <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Function to do some compute work as a part of a larger logic landscape. You can configure Azure Functions and pass data to and from them using a wide range of architectures.</p>
<p>As I mentioned, Azure Functions are just HTTP endpoints that you can call in the same way that you call any other API endpoint from within your code or other process. However, instead of residing on a standard web server that is operational 24&#x2F;7&#x2F;365, your Azure Function is available, quote, on demand whenever you call it. The endpoint is not provisioned into the cloud until it is called. As a result, because your cloud services charge you per minute for any resource in use, an Azure Function is typically less costly to operate than a typical API endpoint web server that’s running and available all the time.</p>
<p>And when do you use Azure Functions? Well, Azure Functions have <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-triggers/">triggers</a> that cause them to execute. So they are especially useful when responding to events such as data coming or going, timers going off, or even HTTP requests coming in. If you have some code that you want to execute based only on a particular event, one that may or may not occur regularly, Azure Functions can be a solution.</p>
<p>As I mentioned, triggers for your Azure Function can include timers, data operations, and standard HTTP calls or webhooks. Triggers are created in a configuration file that is a companion to your Azure Function. As we’ll see when we write our first Azure Function, the trigger type you choose actually defines your project.</p>
<p>Azure Functions connect to your data stores and resources through the use of bindings. Input bindings provide any needed data to your Azure Function and output bindings receive any results. Bindings are created in the same configuration file I mentioned that is the companion to your Azure Function when you publish it.</p>
<p>We will cover the implementation of triggers and bindings in the first part of the course.</p>
<p>Because <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Functions employ the on-demand services model I mentioned earlier, they are typically stateless. Azure Functions are serverless meaning there is no host server to hold or store information between calls. Your Azure Function is provisioned into the cloud when it needs to run and de-provisioned immediately afterwards so there is no concept of state connected to it.</p>
<p>However, there is an extension to standard Azure Functions called Durable Functions that does allow you to manage state&#x2F;coordination of different Azure Functions. It’s a very useful feature in a serverless environment. We will cover the use of Durable Functions in the second part of the course.</p>
<h1 id="Introduction-to-Triggers"><a href="#Introduction-to-Triggers" class="headerlink" title="Introduction to Triggers"></a>Introduction to Triggers</h1><p>Triggers are how your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> starts, so it seems the best place for us to start as well. A trigger defines how your function is invoked. There are several kinds of triggers you can employ to run your function when desired. In general, they have three primary types, including HTTP&#x2F;Webhooks, timers, and data operations.</p>
<p>So for HTTP trigger, your Azure Function fires whenever an HTTP request hits that endpoint.</p>
<p>For a generic webhook trigger, your function fires when a webhook HTTP request, hits that endpoint from any service that supports webhooks, such as Stripe or Twilio.</p>
<p>A GitHub webhook trigger in specific means that your function fires when an event occurs in your GitHub repository, such as create, delete, download, fork, push, pull requests, commit comment or any of the other several dozen GitHub events.</p>
<p>For a timer trigger your function is called at intervals, we’ll actually be creating a timer trigger in our exercise.</p>
<p>A queue trigger means your function fires when a new message comes into your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> queue storage. The message contents are then passed on as input to the function. We will also be doing a queue trigger in our exercise.</p>
<p>For a service bus trigger, your function fires when a new message arrives from a service bus queue or topic. The message contents are then passed in as input to your function.</p>
<p>A BLOB trigger means that your function fires when a new or updated BLOB is detected, and then the BLOB contents are passed to your function as input or an event hub trigger your function fires when any events are delivered to an Azure Event Hub.</p>
<p>To define those triggers and other bindings for your Azure Function, you create a function.json file that creates the configuration metadata for the function itself. Here you can see an example of a function.json file.</p>
<p>You can see within it that a function can only have a single trigger binding. As we’ll discuss later, it can have multiple input slash output bindings. All bindings and triggers will have a direction property and note that for triggers, the direction is always in.</p>
<p>Here you can see examples of several common types of trigger bindings used for implementation of those that we just listed. This code would all be found in your function.json file, typically auto generated by Visual Studio, but you do have direct access to edit it when you desire.</p>
<p>So you can see httpTrigger with an authorization level, a timerTrigger with a schedule, a blobTrigger with the file path to the blob, a queueTrigger with the name of the queue, and so on all of the details necessary for your binding to enable itself to be properly connected to whichever resource you are binding to.</p>
<p>The easiest way to create an Azure Function, and to see all of this is using Visual Studio. You just write your functions, starting with the templates provided, and then publish in the usual way to your Azure Cloud. Note that because any Azure Function can have exactly only one trigger as specified, the trigger type you intend to use is what differentiates your Azure Functions when you <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-creating-a-new-function/">create them</a> in Visual Studio, so let’s take a look at that.</p>
<h1 id="DEMO-Creating-a-New-Function"><a href="#DEMO-Creating-a-New-Function" class="headerlink" title="DEMO: Creating a New Function"></a>DEMO: Creating a New Function</h1><p>To begin, we’ll be creating the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a> equivalent of Hello World. Although, in this case, we’ll actually be logging to our console. So, just use your create a new project template in the typical way to create an Azure Function project. If you don’t see it immediately in your list of projects, you can always use the filter control at the top by <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a>, and you’ll find it quickly.</p>
<p>We’re gonna go ahead and name our function according to the primary type of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-triggers/">trigger</a> that we intend to use, which in this case for simplicity’s sake, we’ll just go ahead and make a timer. So, we’ll call our project, AzureFunctionTimer. And although we will be using a couple of other types of timers throughout the project, including an HTTP trigger, which is the kind that you would want to use to access your function as an API endpoint.</p>
<p>As we discussed earlier, we’ll also be touching on Queue trigger, but, as I mentioned for the moment for simplicity’s sake, we’ll start with a Timer trigger that runs on a simple interval. The interval that we’ll specify here will be the default value of five minutes, and we’re also going to want to be engaging our default development storage emulator provided by Visual Studio and Azure, which is the AzureWebJobsStorage account. That’s going to allow us to interact with our local Blob storage, Queue storage, anything that we might need as Azure storage without having to provision any live resources into the cloud, which would incur charges and require a live Azure account.</p>
<p>So, for development work, we can use the free local storage emulator provided by Visual Studio. We’ll go ahead and create that function, and let Visual Studio do its magic for us. And when it’s finished doing so, you can see that we have a base function signature for an Azure function, and that it includes the necessary trigger attribute applied to the first argument, which in this case is the Timer trigger with the interval specified in part of its constructor argument.</p>
<p>That attribute applies to the TimerInfo variable, myTimer. And, in this function, all we’re going to be doing is logging information that lets us know, yes, that function ran. To do so, we’re going to use the dependency injection ILogger variable log. If you’re not familiar with dependency injection, don’t worry about it for right now. It just means that Visual Studio and Azure have some built-in functionality that you can access by using the right types of arguments in your function’s signature, things like logging and configuration.</p>
<p>So, in this particular case, we’re gonna go ahead and use logging, and then we’re just gonna go ahead and run that function and see the sort of output that we get. And you’ll see the first thing that happens when you run locally is that Azure Functions will pop up your console, so that you can see the logging activity that’s happening inside of your function.</p>
<p>Here, you can see it picking up some configuration information, including the Cron schedule here that indicates that this function will be running every five minutes at five, 10, 15, 20, 25 minutes past the hour, etc. And so, as soon as we hit one of those specific timer intervals, our Timer trigger will fire, and there it is. As we see, we have successfully executed Function1. Because the timer fired, we have logged our information to the console, and we have confirmed that our function has executed successfully from end-to-end. So, congratulations, you have created your first Azure function.</p>
<p>Now, if you’d like to see the function.json file that goes along with that Azure function and defines all of the configuration inside the function, you can drill down by doing Show All Files inside of your Visual Studio. Then, you’re gonna want to go into the bin directory, into your compilation, down inside of Function1, and there you will see your function.json file with the specified trigger binding automatically injected into it. So, your function.json file will hold all of your bindings, all the bindings that are created throughout the rest of the project, triggers, or input or <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-output-bindings/">output bindings</a>, whatever they are, but they will all go into your function.json file, which was automatically created when you built the project and will automatically be deployed with your project when you deploy it into the cloud.</p>
<h1 id="Introduction-to-Output-Bindings"><a href="#Introduction-to-Output-Bindings" class="headerlink" title="Introduction to Output Bindings"></a>Introduction to Output Bindings</h1><p>Creating and then deploying your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> to the cloud in your usual way in Visual Studio automatically <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-creating-a-new-function/">creates</a> and deploys that function.json file we talked about earlier that contains the configuration&#x2F;metadata information for your Azure Function that it requires based on the code you just wrote. The cloud runtime environment reads the file when it spins up your Azure Function and then runs the logic in the function itself. All in response to the activated trigger.</p>
<p>Triggers are one kind of binding. But Azure Functions also make use of input and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-output-bindings/">output bindings</a> to allow you to easily connect to your related cloud resources such as data and other resources. Azure Functions 2.0 and later supports a variety of bindings that you can use for inbound or outbound connection to those resources.</p>
<p>Note quickly that all bindings except HTTP and Timer must be registered. .NET functions can access and register bindings just by using NuGet packages. Other functions can use extension bundles to access bindings through configuration settings.</p>
<p>Notice in the function.json file that input and output bindings can use directions both in and out, depending on the type of the binding. And some bindings even support a special direction InOut that is available using the Advanced editor in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> portal, but we won’t be talking about InOut bindings in this opening course on Azure Functions.</p>
<p>So what kinds of resource bindings are supported in Azure Functions? Well, for input you’ve got Blob Storage, Cosmos DB, IoT Hub, Microsoft Graph Excel tables, Microsoft Graph OneDrive files, Microsoft Graph events, Microsoft Graph Auth tokens, mobile apps, SignalR, Table storage and even Twilio. And for output bindings, it’s all of the above except for the Graph Auth tokens, IoT Hub and SignalR. Plus, Event Grid, Event Hubs, HTTP and webhooks, Microsoft Graph Outlook email, Notification Hubs, Queue Storage, SendGrid, and Service Bus. </p>
<p>In other words, your ability to create interactions in your Azure Functions between the various parts of your cloud architecture is nearly limitless through the use of input and output bindings.</p>
<p>So let’s continue our code example by adding an Output binding to our Azure Function. Suppose that we have a scenario in which whenever our current Timer triggered Azure Function goes off, we want to send a message to our Queue. Let’s just hop back into Visual Studio to implement that code.</p>
<h1 id="DEMO-Output-Bindings"><a href="#DEMO-Output-Bindings" class="headerlink" title="DEMO: Output Bindings"></a>DEMO: Output Bindings</h1><p>Implementation of your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-output-bindings/">output binding</a> in your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> inside of Visual Studio is very straightforward. All you’re actually doing is adding an output parameter argument to your function signature and then applying an attribute to that argument to actually indicate the binding that you’re looking to create.</p>
<p>So you can begin to see a pattern developing here. For example when we used the TimerTrigger attribute, we simply applied that attribute to our initial argument of the type that we want it to be using. And we’re doing exactly the same thing in our second argument where we apply this binding attribute for the queue to the out message that we’re going to be creating using our queue.</p>
<p>If the Visual Studio that you’re using doesn’t recognize this particular item, make sure that you have a couple of NuGet packages installed which would be the Microsoft.Azure.Storage.Queue and then the Microsoft.Azure.WebJobs.Extensions.Storage will give you everything that you’re gonna need to be able to interact with your queue both presently and in the future if you’re ever gonna be creating functions to peek into your queue or anything like that.</p>
<p>So those two namespaces will give you everything that you need. And then the only caveat here before we run our function would be to make sure that we actually have a queue that matches this name that exists because what our code is saying here is that when our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-output-bindings-with-triggers/">trigger</a> function runs, we will automatically be sending data.</p>
<p>Because it’s an output binding, we’re going to be sending data to the object specified here in this argument. So to make sure that our queue exists, we just pop open our local cloud explorer, and it will include a menu item for local, you may have other connections established, but initially all you’re interested in for this project would be your local connection, using that as your WebJobs storage connection string. And you can see as you drill down that in your storage account you’ll have blob containers and queues and tables and so on and so forth. And so in our case we’re just checking our queues that we, yes, we do have an item in here called thequeue, and if we right-click on that queue, we can pop it open and we can actually see that there is nothing in it. Queue contains no messages.</p>
<p>So in theory when we run our function, we should be able to go back into that queue and see that this QMessage, Timer triggered, has been created. So you’ll note that there is another name for the string value that we’re passing to QMessage. I’ll get into the details of this in just a second.</p>
<p>We’re gonna use this when we do a combination of an input binding to combine with our output binding. But again right now our output binding simply acts as a receiver for data that we’re gonna be sending, in this case it’s our queue object and the particular message that we’re going to be sending is Timer triggered. This should all happen automatically when our timer function runs.</p>
<p>So we’ll just go ahead and run the function and see what we get. And so Visual Studio pops open our console as is typical for a local development Azure Function. It reports on its status in the usual way. And then we can see that our function has executed. So our Timer trigger function has executed and successfully.</p>
<p>So now in order to verify that everything worked properly with our queue, we just close that up, stop the host, head over to our queue, open it up, and we see that we have the first message successfully in there, Timer triggered. If you’re curious about what that looks like inside your function.JSON file, here it is, here is your trigger binding with your queue output. </p>
<p>Remember that Visual Studio takes care of all of this by simply creating the arguments properly with the attributes inside of your Azure Function. So typically you don’t even have to see inside of your function JSON file, you don’t have to make any changes to it. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> and Visual Studio will automatically take care of all of those bindings for you. So you have successfully implemented your first output binding in an Azure Function.</p>
<h1 id="DEMO-Output-Bindings-with-Triggers"><a href="#DEMO-Output-Bindings-with-Triggers" class="headerlink" title="DEMO: Output Bindings with Triggers"></a>DEMO: Output Bindings with Triggers</h1><p>So now that we’ve done that, successfully added an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-output-bindings/">output binding</a> to our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a>. Recall that, queue can actually be a trigger as well. So we have now created a timer trigger to inject something into our queue, we could create another Azure Function that would respond to that queue entry, as a queue trigger.</p>
<p>So let’s go ahead and add another Azure Function. We’ll just go ahead and call that function two for the moment. Then when we do, we’ll get the choice of trigger types that we wanna create. We select queue trigger. We go ahead and we use the same <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> web jobs storage that we have been using for our default development environment. We can even rename the queue according to the particular queue that we have been working with. And then we select OK.</p>
<p>Visual Studio will whip up our new as your function with a queue trigger. And this particular function, again indicated with a queue trigger binding is named properly and using our connection and all this function does is again use the dependency injection parameter for logging. Logging information that says that our queue trigger function has actually run and then it should indicate the content of the queue item when we run the function.</p>
<p>So we just go ahead and run all of that and it should all weave together automatically. Once again, Visual Studio pops up our console so that we can see what’s going on inside of our Azure Function. And it reports back to us on its progress in the usual way. And it runs our function and we can see that everything that we wanted to happen has happened. Our timer trigger function has run and our queue trigger function has also run.</p>
<p>We can see that both functions succeeded. You can see the detail that our function with the queue trigger ran because a new queue message was detected. So our timer has run, our output binding has functioned in providing a message to the queue. And our queue trigger function has run in response to that.</p>
<p>So we have now chained together different Azure Functions with different triggers. That is one of the most valuable things that you can do with Azure Functions. You have a wide variety of these kinds of conditions that you can create. To use logical sequences in your programming a very valuable use of Azure Functions. And so you have successfully created your first function, your output binding, and your second function to respond as a trigger to that output binding.</p>
<h1 id="Introduction-to-Input-Bindings"><a href="#Introduction-to-Input-Bindings" class="headerlink" title="Introduction to Input Bindings"></a>Introduction to Input Bindings</h1><p>For input bindings, you’ll see in a moment that when using .NET, you just define the data types using function parameters in the typical way. For example, as your argument for an inbound stream, such as from a BLOB, you would use type stream from the standard System.IO namespace, while for the inbound text coming from a queue <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-triggers/">trigger</a> you would use a string argument. You can also use any custom type of your choice as an argument to de-serialize input to that object type.</p>
<p>For dynamically typed languages, such as JavaScript, you would need to set the dataType property in the function.json file as a part of that binding definition. For example, to read stream input from a BLOB, set the dataType to stream, like you see here.</p>
<p>Other options for dataType are byte array, binary, and string. So this time let’s go back into our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> inside Visual Studio so we can see an example of how we employ the use of an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-input-bindings/">input binding</a> as an argument.</p>
<h1 id="DEMO-Input-Bindings"><a href="#DEMO-Input-Bindings" class="headerlink" title="DEMO: Input Bindings"></a>DEMO: Input Bindings</h1><p>To implement an <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/introduction-to-input-bindings/">input binding</a> in your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a>, let’s go ahead and return to our queue trigger function and provide an attribute to that function so that we can create our binding. </p>
<p>Now remember that a queue trigger will be activated by any entry placed into the specific queue that is referenced. So although in our previous example, we were using our timer to create a queue entry, any connection to your queue from anywhere in your enterprise software that creates an entry in your queue will activate this trigger.</p>
<p>So let’s suppose that somewhere in your enterprise, you have a security application that takes a photograph every time a secure area or restricted area is violated, and that at the moment that that violation occurs an image is taken and sent to your blob storage. And that at the same moment, a queue entry is created saying, “Hey, there’s a new security image that somebody needs to review or that we need to process in order to find out what’s going on.” So what we need is an input binding that is expecting that data to be there, and then a queue trigger activated off of that queue entry, that will then go look for that image inside of our storage.</p>
<p>Here’s a quick look at what that activity does inside of your function, .json file, here you can see the direction of the input binding and the same specifications that we applied inside of our function parameter. So we’ve provided a blob attribute which references this container and this specific image.</p>
<p>So assuming that this image is here, we will not get a null value return. Let’s just go check our Cloud Explorer real quick and make sure yes, we do have an image in our Cloud Explorer that I put there earlier, QR2.jpg, which will match this name.</p>
<p>So when this trigger is fired, we should be able to have a valid log entry created that says, “New image was found,” and then just the size of the picture in this case, will help us separate which picture was found. Keep in mind that just for the moment that I have hard-coded that value, QR2.jpg so that we can verify our connection to blob storage and that when an image exists, we can actually effectively process it. We’ll soon be creating a dynamic value in there as a variable but in this case, we expect it QR2.</p>
<p>So let’s go ahead and run that function. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> will pop up your console as usual to let you see what’s going on inside of your Azure function. We can see that it has found two functions, both one and two. And we can also see that because we have a timer triggered, queue trigger firing, that we know was created by our timer, it has gone in and it has found the image that we were looking for QR2, 4388 bytes, we can go in, we can go back to our blob container, and we can verify that yes, the size of that image was 4.3 K.</p>
<p>So we know that in this particular case, the function has run properly, our connection to our storage is working. And now we can get on to an actual real-world example, in which we would not be providing a hard-coded value here. The key to this is that this value can be dynamic, based on the text content of the queue item that came in.</p>
<p>So we would want to have the queue item that had just come in from your security application, indicating some specific information in the queue, an image name that would help us go into our blob container and fetch that image.</p>
<p>So how do we do that? And the answer is by using special syntax, and a special argument called queue trigger, that is going to provide us with the text information that came into the my queue item argument that we have the queue trigger attribute applied to.</p>
<p>So now what we are expecting is that our blob will have an image of the same name as whatever the item was, that was created inside of our queue. So let’s go back into our Cloud Explorer, and we’ll pop open our blobs container again. And we’ll see that we have an image in here, an image in here that has been created in this case manually, but could have been sent in by our security application. That is called SecurityViolation.jpg.</p>
<p>So assuming that we create an item in our queue of the same name, we should now be able to see that a security image was found and with the actual length of that particular picture. So our queue item has to contain the word security when we create it. And of course, we need to actually have our function running to make sure that our Azure function will be responsive when we create our item in the queue.</p>
<p>So now that we know that we have that up and running, we just head back over to our queue and we create a message with very specific text called security violation, because that is the name of the image that we are expecting to be able to match. So we go ahead and we create that text entry in our queue. And as soon as we do, we can see that our queue trigger is immediately processed, and that it has indicated to us that it is in specific a security violation that it executed.</p>
<p>So now we have been able to see that our queue trigger Azure Function can be responsive to multiple ways of inputting information to our queue, that we can narrow down inside of that function, the specific item information that we’ve received from the queue, and that we can use that information to apply specific logic for that situation. And so you have effectively combined your input binding of expected data with your queue trigger function.</p>
<h1 id="Durable-Functions-Types-and-Uses"><a href="#Durable-Functions-Types-and-Uses" class="headerlink" title="Durable Functions: Types and Uses"></a>Durable Functions: Types and Uses</h1><p>Okay that covers our introduction to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a>. We got one running, we used bindings to interact with remote resources, we called functions from it, triggered functions from it and logged our activities. I spoke briefly earlier about the ILogger Dependency Injection that we were using for that.</p>
<p>Azure Functions 2.0 and later is much, much better at handling Dependency Injection than was version 1.0. And since it’s pretty much required in a stateless environment that’s a very good thing.</p>
<p>As I mentioned, if you’re not familiar with Dependency Injection, that’s okay. In this case the point of it is that Azure Functions as a runtime has some built-in tools such as Logging and Configuration that you can use for all of your Azure Function.</p>
<p>And you get access to those tools when you pass the right kind of object types in the Constructor of your Azure Function. Since you haven’t created those objects yourself you know that <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> must be doing it for you in the background. Because those background objects are actually controlling much of what happens in your function, Dependency Injection of objects is sometimes called Inversion of Control.</p>
<p>The need for Dependency Injection brings us back to a point I mentioned earlier and again just now, the whole on-demand serverless model is by nature stateless. It’s hard to hold state in something that doesn’t exist most of the time such as is the case with your Azure Function.</p>
<p>But what happens when you do have several Azure Functions and you would like them to be aware of what the others have done before they execute in a smarter way than we just employed using our bindings and triggers? It’s a common enough scenario and it’s why Durable Functions were created.</p>
<p>Durable Functions is an extension of Azure Function that lets you orchestrate, or tie together in one way or another, stateful functions in a serverless environment. The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/use-of-orchestrator-and-activity-functions/">orchestration</a> is typically a sequence of logical processing. A Durable Function application is a solution you create that is composed of different Azure Functions, each one playing a different role within that orchestration.</p>
<h1 id="Use-of-Orchestrator-and-Activity-Functions"><a href="#Use-of-Orchestrator-and-Activity-Functions" class="headerlink" title="Use of Orchestrator and Activity Functions"></a>Use of Orchestrator and Activity Functions</h1><p>There are currently four durable function types in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a>, Activity, Orchestrator, Entity, and Client. But we’ll start by looking at just the first two, Activity and Orchestrator. </p>
<p>Activity Functions are pretty much basic <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> Functions written more or less as usual with the one important difference being the requirement of an ActivityTrigger attribute that indicates that this function can be triggered from your Orchestrator function.</p>
<p>Your Orchestrator orchestrates the various Activity Functions by executing them as you specify. The Orchestrator function type requires an OrchestrationTrigger as we’ll soon see.</p>
<p>When you start your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/durable-functions-types-and-uses/">Durable function</a>, it creates the necessary control queues for each of the Activity Functions that it finds, and one worker item queue. Take note that although your Orchestrator looks like it is directly calling the function, under the covers it is instead actually sending a message to that work-item queue.</p>
<p>Your Activity Functions will receive a message from the queue through implementation of your ActivityTrigger and then execute any of its internal logic. When the Activity Function completes its execution, the function sends a response message to the control queue. The Orchestrator function then receives that response message via its OrchestrationTrigger, and the full behavior of the Durable Function is complete.</p>
<p>So an example might be you have a total of order items that you need to pass to another function to get taxes, or you have area parameters for something that then you need to calculate the total volume. Basically, any situation in which you have a value that you would like to pass on to another separate function to return a dependent value would be a use case for Durable Functions orchestrating your Azure Functions.</p>
<p>So let’s take a look at how this works in code by jumping back into our Visual Studio project.</p>
<h1 id="DEMO-Durable-Functions"><a href="#DEMO-Durable-Functions" class="headerlink" title="DEMO: Durable Functions"></a>DEMO: Durable Functions</h1><p>The <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/durable-functions-types-and-uses/">Durable Functions</a> equivalent of hello world, would be to create a set of activity functions that could be executed from your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/use-of-orchestrator-and-activity-functions/">orchestrator function</a>. So that’s what we’re going to do in this exercise, we’ll even create our own as <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Function</a> outside of the template, and we’ll have a value returned from that function that we can then pass on. In this case, we’ll be passing it back to the same function to show a change in value, but we could take that value and pass it along anywhere, thus maintaining state which is usually very challenging in the serverless world of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/use-of-orchestrator-and-activity-functions/https://cloudacademy.com/library/azure/">Azure</a> Functions.</p>
<p>To add a Durable Function to your Azure Functions project, just right click on the project name and do Add New Azure Function. We’ll actually go ahead and name this one FunctionDurable since we know that’s what we’re gonna do with it, and then the list of Azure Function Types pops up, and you will see that Durable Functions Orchestration is in the list. So we just click Okay, and Visual Studio will do its thing and create for us our basic Durable Functions template, which is going to include three different types of functions in it. It will include first our orchestrator function that will be responsible for calling all of the other activity functions. And then there will also be an HTTP trigger function, that is going to be how we are going to activate our orchestrator function in the first place, meaning that yes, at last, we will finally be executing one of our Azure Functions using the much mentioned HTTP trigger as if our function was an API endpoint.</p>
<p>So first, just to make sure that we can see what’s going on here, we’re expecting some text to be written to the screen when we run our orchestrator calling our activity functions. So let’s just run the Azure Function and make sure that everything is looking good inside of our Durable Function. As usual, Visual Studio pops up your console so that you can see all the logging. We’re going to need some important bits of this, you can see right there durable task has been created. And you can see in the green there just flashing by us was a critical bit of information that we need. This is our actual HTTP endpoint that we would need to use from a browser to call our Azure Function.</p>
<p>So we’ll go ahead and do a quick copy on that, we’ll pop open a new window, we’ll paste that in, and we’ll run our Durable Function. You can see we got something back from running it so everything’s looking good. And when we come over to our console, we can see it’s full of a lot of stuff, including here’s a hello to London, here’s a hello to Seattle. And so we can see that each of the activity functions within our Durable Function has run properly.</p>
<p>So now let’s take advantage of some of the power of that so that we can actually pass some values from one Durable Function to another and see that important sequence happening. So let’s just create a little bit of space here. And then I have a pre-created function that I can drop in that does nothing but add five to the value that we pass in. So here’s x equals five, here’s the logging information telling us that we’re gonna be adding that value to whatever the value that is passed in. And then we’re gonna be going ahead and returning that value.</p>
<p>So how would we get access to some of that stuff up here inside of our orchestrator function? Let’s go ahead and take out a few thing. And this is the simple process of creating an integer, having the value of that integer then reassigned to the return value from our add five function, and then doing the same thing again. So between Seattle and London, we should have one output that says we’re adding five to zero, and then we should have another output that says we’re adding five to five. And we can watch that value change dynamically as our Durable Function runs.</p>
<p>So let’s go ahead and run that function and see what we get. Azure Functions, once again, displays our log, verifying we have a durable task running. And there’s the endpoint that we’re going to need to use, which of course, fortunately, turns out to be the same endpoint that we just used.</p>
<p>So now that we know that everything is up and running, we can actually just come back to our open browser, make another request to the same endpoint. When it’s completely finished, we can go in, and we can take a look. And we can see that our function durable hello has been running properly, and so has our add function. here we can see down in the list, we’re adding five plus five. Here further above, we’re adding five plus zero.</p>
<p>So, as we are going through our Azure Function, and here’s the beginning saying hello to Seattle, adding five plus zero, adding five plus five, and then at the end here, saying hello to London. And so, in this process, we have really been able to see very powerfully that Azure Functions can tie together in a number of ways, orchestrating your Azure Functions to create a sequence like this is one of the most powerful ways that you can manage your state and the logic within your larger application picture.</p>
<p>So, now we have successfully created an Azure Durable Function that has called individual activity functions. We have activated that function using an HTTP trigger endpoint. And that concludes the first part of our introduction to Durable Functions.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>That concludes our introductory look at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/course-introduction/">Azure Functions</a>. We’ve seen that Azure Functions are actually just little bits of your application logic that you can have reside in the cloud as a part of your general software architecture, and they can perform on demand pretty much anything that you want them to, using a variety of triggers such as timers or data events or HTTP triggers, as if they were regular API endpoints.</p>
<p>You have the ability to string together different bindings of a wide range of data sources and resources such as queue, BLOB, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> tables, etc, etc, that you can weave all of these things together into a sequence of bindings and triggers that allow you to really dictate a logical flow of data or processing through one piece of your application logic. That’s just one way to orchestrate Azure Functions together.</p>
<p>The formal Azure Functions way of orchestrating things together is by using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-azure-functions-990/demo-durable-functions/">Durable Functions</a>. We’ve seen that Durable Functions can, through the use of orchestration, call other Azure Functions that can pass data to and from each other using the orchestration function as that sort of nexus that lets you be in complete control of logical and data flow.</p>
<p>So, hopefully, you’ve seen some things within Azure Functions that get you excited to use them. Hopefully, you can apply them to the benefit of your enterprise coding structure soon. Thank you again for taking my course. I hope you enjoyed it, I hope you got a lot out of it. Happy coding!</p>
<h2 id="6DEMO-Output-Bindings"><a href="#6DEMO-Output-Bindings" class="headerlink" title="6DEMO: Output Bindings"></a>6<strong>DEMO: Output Bindings</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/visualstudio/azure/vs-azure-tools-resources-managing-with-cloud-explorer?view=vs-2022">Cloud Explorer Alternatives</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14/" class="post-title-link" itemprop="url">AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:10:37 / Modified: 11:10:38" itemprop="dateCreated datePublished" datetime="2022-11-14T11:10:37-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Lab-Creating-a-Topic-and-Subscription-in-Azure-Service-Bus-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Designing-Azure-Solutions-Using-Platform-Services-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Designing-Azure-Solutions-Using-Platform-Services-13/" class="post-title-link" itemprop="url">AZ-204-Designing-Azure-Solutions-Using-Platform-Services-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:09:43 / Modified: 23:56:20" itemprop="dateCreated datePublished" datetime="2022-11-14T11:09:43-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Designing-Azure-Solutions-Using-Platform-Services-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Designing-Azure-Solutions-Using-Platform-Services-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Greetings! Welcome to the Cloud Academy Microsoft Azure Design Solutions using Platform Services course! I am delighted to have you join me on what is bound to be yet another educational and delightful adventure into the world of Microsoft Azure. </p>
<p>First I will let you know a bit about myself before I get into the course outline. My name is Jonathan. I am one of the course developers with Cloud Academy. I work professionally as a technical consultant specializing in DevOps, data engineering, and security. I am also a former high school teacher, so it is a treat to get back into teaching all you lovely people about technology.</p>
<p>So enough about me, let’s get into this course. Who is this course for exactly? This course is also for anyone looking to improve their knowledge of Azure platform solutions in the fields mentioned above. If you happen to be a software engineer, a product manager, a CTO, or just someone who happens to be trying to build a product using Microsoft Azure, then this course could save you a lot of time and money as you discover Azure’s powerful product toolset.</p>
<p>So what exactly are the prerequisites for this course then - what do I expect you to know in order to understand the material? Well, not too much actually. You don’t need to be a programmer or have a deep knowledge of computer science. The course will focus mostly on explaining Microsoft Azure systems and how to use the Azure user interface. You should have a general understanding of the domains related to each product category. You will need a basic understanding of terms like “Internet of Things,” “Machine Learning,” “Messaging Queue,” and, “File Encoder.” If you have no idea what any of these things are, this course may be premature for you.</p>
<p>The course will cover a lot of information given its short length. We are going into detail on a number of Azure services and practices. Our goal is to explain in depth each Azure service and ensure that you have a stronger understanding of which service to choose to solve a particular problem. After going through all of the content, you should have a thorough grasp of several Azure products. You should be ready to start creating products in any of the four domains described above. </p>
<p>So as you may have gathered, there are four core learning objectives - takeaways, if you will: 1. The student will have an in-depth knowledge of Azure artificial intelligence services and know when to opt for specific offerings. 2. The student will have an in-depth knowledge of Azure IoT services and know when to opt for specific offerings. 3. The student will be able to design a scalable messaging architecture using Azure services. 4. The student will have a deep understanding of Azure media services.</p>
<p>Lastly, I want to encourage everyone to leave feedback. Email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> if you have any questions, comments, suggestions, or concerns. We always appreciate people taking the time. Now without further ado, let’s get started.</p>
<h1 id="Section-1-Introduction"><a href="#Section-1-Introduction" class="headerlink" title="Section 1 Introduction"></a>Section 1 Introduction</h1><p>We have divided section one, focusing on AI and machine learning, into two parts. The first part will give a thorough survey of Microsoft Azure’s machine learning solutions. We’ll explain what problems each solution solves and explain in-depth how to actually use them. The second part of this section will focus on specific use-cases. We will offer several examples of problems and show how a specific Azure machine learning solution can help.</p>
<p>So without further ado, let’s get right into it with an overview of Azure artificial intelligence products.</p>
<h1 id="Azure-Artificial-Intelligence-Services"><a href="#Azure-Artificial-Intelligence-Services" class="headerlink" title="Azure Artificial Intelligence Services"></a>Azure Artificial Intelligence Services</h1><p>‘Artificial Intelligence’ and ‘Machine Learning’ are related terms that until now I have used somewhat interchangeably. It is worth explaining the exact difference before we start talking about Azure products. Artificial Intelligence, or AI for short, refers to intelligent decision making performed by non-living agents such as computers. Machine Learning, also called ML, is the science of teaching computers to improve at specific tasks by using algorithms that cause the computer to ‘learn’. With some reflection, it should become clear how AI and ML are related. Machine learning is one of the core disciplines that powers AI.</p>
<p>AI and ML power a number of software applications. Apple’s Siri personal assistant is an example of AI. She has to intelligently respond to user questions and provide useful answers. Machine learning is what powers Siri’s understanding. Siri’s understanding of language comes from training against thousands of hours of audio of human conversations. This training fuels her intelligence - her ability to understand what a human is asking for and how to properly respond.</p>
<p>With Microsoft Azure, you get a number of services that can help you to build similar systems that employ machine learning and AI. They are all listed under the Azure Artificial Intelligence category. As you can see there are several products as well as a number of command-line tools, frameworks, SDKs, and extensions. We will not go in-depth into every single offering this course. Instead, we will cover some of the most important products and then in the next lesson talk about which services solve specific problems.</p>
<p>Just to further emphasize the scope it is worth mentioning that Azure also has a large set of Analytics services (<a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/#analytics">https://docs.microsoft.com/en-us/azure/#analytics</a>). These include tools like Data Catalog and Azure Stream Analytics. You can see the entire catalog using the link below. This course will only very briefly mention Azure Analytical tools where they may help differentiate use cases. In general, however, they are out of scope for this course, as data analytics is a distinct domain from AI. The science of analytics is about deriving useful information from your data. By contrast, AI is about putting that data to work in some intelligent way.</p>
<p>So for our purposes, the best place to start is with Azure Machine Learning. This is Azure’s flagship product for anything related to ML. As you can see from the diagram the Azure Machine Learning studio does A LOT. Once you import your data, you have a lot of tools for processing, cleanup, visualization, and model training. You can easily shuffle and split your data into training sets and test different learning algorithms. You also get great third party support for things like Jupyter notebooks and Hadoop.</p>
<p>Making use of Azure Machine Learning requires a solid knowledge of data science and programming. It integrates with a number of systems and APIs such as Spark, Docker, Jupyter, Kubernetes, TensorFlow, and many others. The platform itself is actually built on top of many of these systems. See the slides here for a more exhaustive list.</p>
<p>Azure also has a separate related product called Machine Learning Studio. Unlike the core Azure Machine Learning platform, Studio is a drag and drop service with a narrower focus. It is designed to simplify creating predictive tools on top of an existing data set.</p>
<p>Cloud Academy has an in-depth course focused solely on Azure Machine Learning Studio. I recommend going and checking that out if you want to get really thorough walkthroughs and explanations. For our purposes, the high level overview and diagrams should be enough to get you started.</p>
<p>Now Azure has another powerful offering in the algorithm training space called Azure Batch AI. There is a lot of overlap in Batch AI with Azure Machine Learning so to simplify we are going to focus on the differences. Batch AI, somewhat like Machine Learning Studio, is a more comprehensive ML solution. It offers ML job scheduling and resource coordination. To this end when you create a Batch AI job, it will take as an input a description of your cluster, which are the resources, and your job, which is the task. The cluster input is really just the hardware, so basically you will tell Azure how many VMs you need, what type, what disk image to use, what region, and what remote storage to access for the data. The ML job description will reference the cluster you just defined and also take other inputs such as a particular AI framework, command-line arguments, VMs, and input or output directory locations.</p>
<p>Batch AI, as the name implies, is better suited to one-off jobs of a larger scale. While it can be incorporated into a long-lived application stack, you may be better off with something more configurable such as Azure Machine Learning.</p>
<p>Azure also has a service that covers both analytics and ML for people with a background in Apache Spark. It is called Azure Databricks. Databricks is an “Apache Spark-based analytics platform optimized for the Microsoft Azure cloud services platform.” It incorporates the entire set of Spark open-source APIs and services including Spark streaming, Spark ML, SparkSQL, and GraphX. So if you have some expertise with Spark or an existing Spark codebase, Databricks may be the way to go. Setup is similar to Batch AI. You go into the portal and create a Databricks workspace. You define your cluster resources and your Spark job. Clusters can be configured to live only for a set amount of time to reduce costs as well. Thus in short, Databricks is basically just “easy mode Spark” for Azure.</p>
<p>The last big offering Microsoft Azure Cognitive Services are, “a set of APIs, SDKs, and services available to developers to make their applications more intelligent.” In short, Cognitive Services refers to a set of ready to use AI tools that you can add to an existing application. They include things like image classifiers for faces and text analysis tools for automatic phrase completion and search. To make use of these tools you need to create a Cognitive Services API account in the Azure portal. This is easy to do with a few clicks. You can even get a free account if your transactions are below a certain threshold per month.</p>
<p>In the final part of this section, I’ll just briefly mention some AI products with a narrower focus. There is Microsoft Genomics, a tool for people working with gene sequencing. I know precious little about genetics so I won’t even try to explain the use case. Suffice to say if you are working in that field you should check out the Microsoft Genomics documentation linked below. There is also the Azure Bot Service. This is basically a toolkit for creating interactive agents such as chatbots. You can create a wide variety of bots by training them to respond in specific ways to different inputs. The Bot Service is also serverless, so you don’t need to worry about administration, which is nice. And finally, there is also Azure Search. As the name implies this is Search as a Service.</p>
<p>So that about wraps up our discussion of Azure AI services. You should have a good broad understanding of each offering. In our next lesson, we will start talking about how to pick the right AI tool for your specific needs. See you there.</p>
<h1 id="Choosing-the-Right-A-I-Service"><a href="#Choosing-the-Right-A-I-Service" class="headerlink" title="Choosing the Right A.I. Service"></a>Choosing the Right A.I. Service</h1><p>Knowing which Azure AI service will fit your needs first requires understanding the problem you are trying to solve. While there is considerable overlap in what the different solutions can do, you can save yourself a lot of stress by picking the optimal product from the start.</p>
<p>Fortunately, some Azure AI services are quite narrow in focus. They should stand out if your needs are in that particular domain. The three I am referring to here are Azure Bot Service, Microsoft Genomics, and Azure Search. If you are looking to create a bot that interacts with users, work on sequencing genomes, or create a tool for searching heterogeneous data, then you should know where to turn.</p>
<p>The challenge comes when you have a more open-ended or nuanced problem. For example, let’s say your company has accumulated a lot of data over time and wants to see if there may be some value in analyzing it. This is a broad question that requires a deep understanding of both your business and the data itself. Which Azure data-related solution will offer the most value? Perhaps you could create some sort of facilities or office automation system using Azure Cognitive Services. Or perhaps you could make predictions about visitors to your site by using Machine Learning Studio. Or perhaps you really want to run large scale ETL operations on your data for some brand new business case or software service.</p>
<p>There is a more general point I am trying to make here with all of these hypothetical examples. Big data and machine learning are popular terms today. Often they can become solutions looking for a problem. In many cases, companies really have no practical need for investing the time and resources needed to make use of machine learning technologies, even in cases where you have relatively easy to use solutions in Microsoft Azure. So really, the first question to ask when trying to figure out which Azure AI solution to use is, “Do I actually even need to get into ML and big data analysis at all?” Smaller companies in particular can often get more value with less effort out of a few smart employees working with Excel.</p>
<p>But let’s say you have crossed that threshold and have a strong business case for digging into Azure’s more comprehensive solutions. The way to determine which tool is relevant is to first clarify your understanding of the problem you are trying to solve.</p>
<p>If you are in a very early stage where the work is exploratory meaning there is no clear desired outcome or deliverable, then what you want is something that can just give you insight into your data. This is not what Azure Machine Learning services or studio are really designed for - rather ML is more about using data to predict something. In this scenario, you do not even know what you want to predict. You would be better off using one of Azure’s many analytics services such as Data Catalog, Data Lake Analytics, Stream Analytics, or Azure Analysis Services.</p>
<p>Once you have narrowed down the problem to being of a specific type, now you can start thinking about Azure AI solutions. For example, if you know you want to make some kind of prediction by training an algorithm against data, then Azure Machine Learning is the way to go. The specific choice of whether or not to use Machine Learning Studio or not really depends on the state of your data and data science team. As Studio is a “drag and drop” system it is not going to be as flexible as Azure Machine Learning Services which gives you more control over how you prepare your data. If you are doing more resource-intensive training work on a more complex or long-range data science task, you should consider Azure Batch AI.</p>
<p>As implied early Azure Data Bricks is the best solution for ETL, especially if you have some familiarity with Spark. Lastly, Azure Cognitive services is for when you are ready to take the results of your analytical work and incorporate it into an application.</p>
<p>So to summarize, ask yourself what you are trying to do with your data. Do you want to make a prediction? Look at Machine Learning services. Do you want to transform data into some more usable form? Check out Data Bricks. Do you want to incorporate some understanding of your data into an app to make it more intelligent? Try Cognitive Services. And finally, if you are unsure of exactly what you want to do, first ask whether or not you need any AI service at all and consider using Azure Analytics Services to get a better understanding of your data.</p>
<p>So that about wraps it up for section 1. Congrats on making it this far. We’re going to continue on now with a few lessons on Azure and the Internet of Things. It should be a blast. See you there.</p>
<h1 id="Section-2-Introduction"><a href="#Section-2-Introduction" class="headerlink" title="Section 2 Introduction"></a>Section 2 Introduction</h1><p>The “Internet of Things.” A curious name for a curious phenomena. It may seem like a silly meme but believe it or not, IoT, or Internet of Things, is a big deal. We’re going to talk about why it is a big deal in this section and we’re going to talk about how Azure can help you make use of it.</p>
<p>The first lesson will talk about exactly what IoT is. It is a short concept lesson. Feel free to skip if you feel like you already know it well. The second lesson will be long and dense. We’re going to go cover several different Azure technologies all related to IoT. There will be a lot of documentation links and useful diagrams. Plan accordingly. The third and final lesson in this section will be all about ensuring you understand WHEN to use which IoT service over another. Should not be too tricky.</p>
<p>So without further ado, let’s dive into that wonderful world of the Internet of Things.</p>
<h1 id="IoT-Concept-Explanation"><a href="#IoT-Concept-Explanation" class="headerlink" title="IoT Concept Explanation"></a>IoT Concept Explanation</h1><p>Get ready for a super short lesson that is completely skippable if you already know the subject. All we’re going to do is briefly define exactly what we mean by, “Internet of Things,” or “IoT.”</p>
<p>Now, a lot of people, when they hear the term “Internet of Things,” immediately get a very superficial image. They imagine cheesy home automation gadgets, like an internet-connected talking toaster. Due to the dubious value of many trendy items billed as “IoT,” there is an unfortunate tendency to be dismissive of the term.</p>
<p>In reality, “Internet of Things,” refers to a much more consequential and larger trend. The Internet of Things refers to a historically recent growth of internet access and network-connected devices in all manner of fields. So when we talk IoT, we’re not just talking toasters. We’re also talking about agriculture, solar power, elderly care, transportation, and manufacture. We’re talking about internet-connected cameras and audio sensors deployed by municipal police to combat crime. We’re talking about internet-connected crop watering systems that let farmers automate much of their work and generate larger crop yields. We’re talking about a massive new economic opportunity creating jobs and new fields of technological inquiry. We are talking about all of the ways cheaply adding internet access to everyday devices creates new opportunities to make our communities safer, our lives easier, and our work more intelligent and efficient.</p>
<p>IoT is not just a meme. It is one of the few truly futuristic technological developments that will push us toward living the George Jetson &#x2F; Back to the Future lifestyle we’ve all dreamed of since we were kids. IoT is a natural consequence of computers getting smaller and high-speed internet access getting cheaper. Thousands of startups are exploiting new business opportunities as a result. With the right vision, it could be a great opportunity for you as well.</p>
<p>So that is all you really need to know at the conceptual level. So now the natural question is, “Where does Azure factor into all of this?” Well, to make IoT work, you need the right software infrastructure. Azure has many tools in place to simplify setup of IoT systems. In the next lesson, we will take a thorough survey of these technologies. So if you’re ready, let’s dive in. Good luck.</p>
<h1 id="IoT-Azure-Technology-Review"><a href="#IoT-Azure-Technology-Review" class="headerlink" title="IoT Azure Technology Review"></a>IoT Azure Technology Review</h1><p>Internet of Things software infrastructure is all about messaging and events. In the next major section, we will do a deep dive on messaging infrastructure in Azure. For this section all you need to understand is that messages between IoT devices and the cloud backend are used to transmit events. These events could be temperature measurements from an IoT thermostat, humidity readings from an IoT greenhouse monitor, a configuration change for an IoT home light switch, or any other sort of data transmission.</p>
<p>So how do we handle these events efficiently? Well, you could build our own event handling system using something like Apache Kafka. If you have the expertise in-house, this could work well, but it will take time and effort.</p>
<p>Here is where Azure comes in. Azure has a number of different solutions for managing IoT events. We’re going to talk about event handling in two parts. First we’ll talk about initial event ingestion and processing. That will entail an explanation of three core Azure technologies - Azure Event Hubs, IoT Hub, and Azure Service Bus. Next, we’ll talk about actually using the ingested event data both by end users and our backend services. Relevant technologies here are Azure Notification Hub, Azure Stream Analytics, and Azure Event Grid.</p>
<p>So let’s start with event ingestion. Azure Service Bus, Event Grid, and IoT Hub are all capable of ingesting events - they just go about it in different ways. Service Bus is the more generalized solution. It is not strictly speaking an event ingestion system. Rather it is a managed message broker. Like Kafka or RabbitMQ, Service Bus is designed to decouple applications and services from each other with a reliable system for asynchronous state transfer.</p>
<p>Not all messages are events but all events are messages. Again, we’ll dig into the concept of messages in section three. Service Bus can do much of what IoT Hub and Event Hub can do, only it is not specialized around events or IoT. It doesn’t integrate with all of the same Azure services as well. As you can see from the documentation link, whatever your needs, Service Bus can likely accommodate you. The enterprise level support and security make it great for sensitive data like financial transactions. What’s more it has strong library support for .NET as well as Java and JMS.</p>
<p>Service Bus really shines when you need to support a variety of state transfer paradigms. Perhaps you need transactions that let you group operations together. Perhaps you need batching, or scheduled delivery of messages.</p>
<p>However this whole section is on IoT, and as we discussed IoT is focused on events. While Service Bus can serve as our main event ingestion system, it would require more development knowledge to get it to do everything we want. To make it easier for us to focus on events and in particular IoT, Azure offers IoT Hub and Event Hubs.</p>
<p>You can think of IoT Hub as a specialized version of Event Hub. Event Hubs act as the “front door” for your data. As you can see from the diagram, event data can come from a range of sources. End user devices, laptops, phones, tablets, software applications, devices, clients - all kinds of things can transmit event data. They need a place to send that data to - an endpoint essentially that speaks their language and understands the event data it is receiving.</p>
<p>So as an event ingestor Event Hub will have a few responsibilities. It has to validate that event data is properly formatted. It will use SAS tokens to identify and authenticate the event publisher. It will capture data and store it in an Azure storage service if needed. Even nicer: Event Hub can scale with your app. You control traffic using throughput units that can handle a certain number of MB per second. You can manually scale through the portal or enable auto-inflate to let Azure scale for you.</p>
<p>So what about IoT Hub? Why pick it over Event Hub? To add to your initial skepticism, know that IoT Hub is more expensive than Event Hub. So why go to the trouble and cost of using it instead of Event Hub or Service Bus? Simple: It provides functionality for IoT workloads that you won’t find anywhere else.</p>
<p>With IoT Hub you get the ability to not only ingest data from clients but also to easily push data back to those same endpoints. This can be hugely valuable for managing IoT devices. You get access to the Azure IoT Edge system that enable on-device data processing for analytics, app changes, configuration, and security. IoT Edge in particular is very powerful from some use-cases as we’ll see in a bit. Key takeaway is that IoT Hub is like Event Hub on steroids…Magic steroids that grant powers for IoT…Anyway, see the chart for a proper feature comparison.</p>
<p>(link comparing IoT Hub and event Hubs: <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs">https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs</a>)</p>
<p>So now that the event data has been ingested either using Service Bus, Event Hubs, or IoT Hubs, we want to do stuff with those events. Most likely we want to do one of two things. We either want to pass those events on to some other service to power our application or business logic, or, we want to immediately start analyzing that data directly and start deriving insights.</p>
<p>So let’s start with the former case. How can we easy mobilize all of this ingested data? Azure Event Grid is the answer. Event Grid is basically a managed intelligent routing service that consumes events in a publish-subscribe model. So take a look at the diagram here. As you can see Event Grid is basically a middle layer between your event ingestion systems and your downstream services.</p>
<p>You can see it supports Service Bus, Event hubs, and IoT Hubs as topics for ingestion. However it can also take in data from other Azure components such as Media Services and Blob Storage. The services that will then subscribe to your Event Grid will depend on your infrastructure. So for example you may have an Azure Function or an Azure Logic App that is powered by event data pulled from an Event Grid subscription. Your app then takes that event data, which may have originated from some user activity, and then return some output or value to the end user.</p>
<p>Event Grid is easy to set up using the Azure portal. All you need to do is define a topic and give it some input - perhaps your IoT Hub or Service Bus. Then on the other end you create a subscription. For something like an Azure Function this is as easy as clicking “Add Event Grid Subscription” in the top right of the UI. Event Grid can also be managed using Azure CLI tools or PowerShell.</p>
<p>Now let’s say that instead of piping our events to some other system we would rather process them directly to derive useful insights. Here is where Azure Stream Analytics comes in to save the day. If you look at the diagram you can see that, much like Event Grid, Stream Analytics act as as a kind of middleware. It takes streams of event data from Event Hubs or IoT Hub as an input, performs some processing, and then outputs to some downstream consumer.</p>
<p>Exactly what Stream Analytics will do for output will vary based on your business. You might want to save Stream Analytics outputs to a DB for storage or batch jobs. You might push the results to Service Bus, an Azure Function, or even to another Event Hub. It all depends on the type of job you are doing. The key thing that distinguishes Stream Analytics from Event Grid is the ability to visualize and utilize data immediately from the source pipeline. This is helpful when you want to make sense of telemetry data, send alerts based on sensor readings, or trigger some other backend functionality based on a real-time understanding of the data.</p>
<p>So with Event Grid and Stream Analytics we have covered how we can use event data for our purposes by either coupling events with other services or doing real time analytics. But what about if we want to use event data on the client side? How can we do useful work away from the cloud or even just interact with clients and users?</p>
<p>This is where Azure Notification Hubs and IoT Edge come in. The former is ready made smart device notification solution. Need to send push notifications to iPhones, Android phones, or tablets? Notification Hubs is your answer. The great thing about it is that it takes away a lot of the pain involved in supporting a variety of mobile devices. If you have experience as a mobile developer then you’ll really know what I am talking about. Unlike other forms of messaging, push notifications often have tricky platform-dependent logic. Scaling, managing tokens, and routing messages to different segments of users on different hardware and different versions of Android is non-trivial work for even an experienced tech team.</p>
<p>Notification Hub takes away most of that pain. It lets you broadcast to all platforms with a single interface. It can work both in the cloud or on-premises and includes security features like SAS, shared access secrets, and federated authentication. See the “How To” guide link for more details. (<a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-aspnet-cross-platform-notification">https://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-aspnet-cross-platform-notification</a>)</p>
<p>Lastly we need to talk about IoT Edge. IoT Edge is a tool for empowering IoT devices. It lets you run code directly on client devices instead of in the cloud. So you can do things like change device configuration, get real-time analytical data, or detect anomalous conditions. The chief value of doing such work directly on the device is that it saves the effort of having to send event data back and forth over the internet. So for example, we might want an IoT home sensor to alert us to some unusual condition without having to phone home to the cloud to run analysis. You can imagine how valuable this could be for something like an IoT smoke detector or security system. IoT Edge can also empower devices where network connectivity is not always reliable.</p>
<p>So how exactly does IoT Edge let you run code on the device itself? It’s pretty ingenious actually. What you do is install an IoT Edge runtime on your client device. This runtime executes IoT Edge modules. These modules are just lightweight Docker containers that run whatever business logic you want. Modules can call other Azure services, run 3rd party software, or execute your own custom code. Whatever it is you need your device to do. The IoT Edge runtime will manage your modules, track module health, facilitate communication with downstream itself and other endpoints, and manage security. Best of all: You can track all of this activity in the Azure web portal. With a few clicks you can monitor device health and push updates or configuration changes. IoT Edge gives you ultimate control over your entire fleet of IoT devices.</p>
<p>IoT Edge does not integrate with Azure Event Hub. To properly make use of IoT Edge you need an IoT Hub account. Note that with the IoT Hub basic tier account, you can only use IoT Edge for testing and evaluation. You need a paid IoT Hub Standard tier account to get proper support for IoT Edge production use cases. As mentioned above this is what enables the cloud-to-device messaging that powers IoT Edge.</p>
<p>So that about wraps it up. We have gone over all of Azure’s offerings in the IoT space. You should know enough now to be dangerous. To really solidify your understanding, play around with some of these tools in the Azure portal or at least read some of the quickstart and how-to guides in the Azure documentation. So for our next section we’re going to offer some additional insight about selecting the right IoT Azure service for your particular use-case. See you there.</p>
<h1 id="IoT-Azure-Technology-Selection-Insight"><a href="#IoT-Azure-Technology-Selection-Insight" class="headerlink" title="IoT Azure Technology Selection Insight"></a>IoT Azure Technology Selection Insight</h1><p>The third and last part of our section on Azure IoT services is really meant for reinforcement. We’re not really presenting anything new about the services we discussed in the last section. Rather we’re just going to briefly talk about what differentiates the services to help you understand when to favor one solution over another.</p>
<p>We’ll start with the more specialized technologies with narrower use-cases. This covers Azure Stream Analytics, Azure IoT Edge, and Azure Notification Hubs. Let’s begin with the last one. Notification Hubs, as the name implies, is for sending notifications to mobile devices. That’s basically it. Notification Hubs is a Push Engine and API for notifications. Azure Stream Analytics is your toolkit for real-time analytics on event streams. So if you are looking to gain some business insight on streams of data, that’s where to look. Finally, IoT Edge is for implementing data analysis and processing directly on your IoT devices. Basically it is a method for taking workloads off of your backend allowing you to send fewer messages back and forth.</p>
<p>So now we’re left with four key IoT services: Event Grid, Service Bus, Event Hubs, and IoT Hub. Now of these four, one of these things is not like the other. For the folks playing at home, that would be Azure Event Grid.</p>
<p>Event Grid is NOT an event ingestion or processing system. Rather it is middleware meant to help you create applications with an event-based architecture. It is designed to help connect sources of event data to different endpoints or event handlers. So for example it can take event data from Service Bus or IoT Hub and connect it to your Azure Functions to create a serverless application stack. So think of Event Grid as the glue that connects your event data and downstream applications.</p>
<p>So finally, the remaining three services, Azure IoT Hub, Azure Event Hubs, and Azure Service Bus. All three can be used for event ingestion and processing, so how do we know which one to use?</p>
<p>I think the easiest approach is to think of them in terms of decreasing levels of specialization. So, Azure Service Bus is the most general purpose of the three. It is a cloud messaging service that can be used not only for events but any sort of asynchronous FIFO messaging system. If you are not even thinking in terms of events Service Bus may be the place to start. It also is great when you need ACID guarantees, meaning you need real transactions that are durable and won’t get dropped. This is especially important when dealing with temporal transactions with things like finance or manufacturing.</p>
<p>From there we go to Azure Event Hubs, a more specialized service than Service Bus. This is really your event “on ramp” for your entire infrastructure. It is definitely Azure’s most cost-effective and performant event handling system at scale. IoT Hub, by contrast, is more expensive and specialized even more. IoT Hub is designed specifically for IoT workflows. It allows for greater control of bidirectional events between devices and your backend system.</p>
<p>So in short, if you need something specialized for IoT and don’t have to be super tight on budget, IoT Hub is the way to go. If you need a more generalized event handling system that is extremely robust and competitively priced, go with Event Hubs. If what you really need is messaging with transactions and durability, go with Service Bus.</p>
<p>So that should about cover it all. Hopefully, now you have a clear sense of which service is best for a variety of situations. This will help for technology professionals trying to design a solution using Azure. From here we will move on to section 3 and get into the magical world of messaging. See you there.</p>
<h1 id="Section-3-Introduction"><a href="#Section-3-Introduction" class="headerlink" title="Section 3 Introduction"></a>Section 3 Introduction</h1><p>We talked all about event handling in the last section and I explained that events are a type of message. Messages are the more general concept. Messages are data - encapsulations of system state transmitted electronically. They may be text, binary, or some other format.</p>
<p>Messaging infrastructure is a big field in the world of cloud-based applications. It is an integral part of most SaaS (software as a service) systems. At some point, data about the state of the system at one point will need to be transmitted to some other system. Your user signup service will need to take new account data, transmit it to some storage, and then perhaps some other service will need to use that data. How do we efficiently move data around in systems with potentially many services? How do we coordinate between services that push data somewhere and services looking to subscribe and receive data from somewhere?</p>
<p>This is the essence of messaging systems. There are numerous technologies that have come about to try to solve this issue. On the one hand you have message queues like RabbitMQ. There are also more robust systems like Apache Kafka and Ignite.</p>
<p>We’re going to cover Microsoft Azure’s messaging solutions in this section and talk about the specific use-case of push notifications on smart devices. Now, some of this will be review, as we talked about some of these solutions while covering events in the context of IoT. I will try my best to not repeat myself too much, but if you decide to watch the video at 2x speed when I am covering Azure Service Bus for a second time, I won’t be offended.</p>
<p>So before we do our technology review and go about creating a push notification system, we are going to talk a little bit about messaging more generally. If you’re ready, let’s dive in.</p>
<h1 id="Messaging-and-Push-Notifications"><a href="#Messaging-and-Push-Notifications" class="headerlink" title="Messaging and Push Notifications"></a>Messaging and Push Notifications</h1><p>Messages are one of the fundamental building blocks of software systems. So what exactly is a ‘message’? At a basic level, messages represent system state. Messages have a sender and a receiver. A typical message will be formatted with a body, representing its content, and additional metadata for things like origin, destination, authentication, authorization, timestamp, and encryption. Messages can come in the form of sensor data from an IoT device. They can come from user input on a website or game. They can be generated by an analytics pipeline outputting the results of some workflow to a downstream system. The key thing to understand is that messages represent real world information. For that reason we have to treat them carefully. We have to care about order and we can’t afford to drop them.</p>
<p>This is why software teams are so wary of making changes to servers that deal with ingesting or storing messages. If you have ever worked for a startup, you may have had this experience. Your deployment system, say Amazon Cloud Formation or something like it, is really great for stateless services that can be torn down and relaunched without losing data. But once something stateful needs an upgrade, something like a database, or Redis, or Kafka, things get sticky.</p>
<p>To reduce the pain, good messaging systems should have a few features. They should be resilient. They should be scalable. They should be capable of carefully tracking message order. They should have good security. And finally, they should be easy to integrate with various clients - both consumers and producers of messages. Now, just for clarification, by “resilient,” I mean that you need a messaging system that can survive minor failures. You want something that can be deployed on multiple servers and possibly across multiple regions. By “scalable” I mean that you need something that can process messages quickly and can be easily expanded by adding more hardware if necessary.</p>
<p>Apache Kafka meets all of these requirements. An internally developed proprietary system in theory could do. Now, happily, you can also save yourself some time by using Azure’s messaging solutions and get all of the above features right with your Azure account.</p>
<p>So now that we have some context around messaging and messaging systems, I want to briefly go over push notifications. Push notifications are messages that pop up on mobile devices, usually in the upper area. They look like SMS text messages but are transmitted via the internet only to users of the relevant app. So when you send a message to someone via Google Hangout or Facebook Messenger, the little notification that person receives is a push notification.</p>
<p>Push notifications are an integral part of any cloud based software application. They are one of the key ways we can alert users to important changes and solicit critical user feedback. Being able to create a stable, performant push notification system is an extremely useful skill if you are on a development team. And that, my friends, is precisely what we’re going to do in the coming lessons.</p>
<p>So, now that we’re all up to speed on messaging and push notifications, it’s time to take a deep dive into the world of Azure messaging solutions. Good luck and see you there.</p>
<h1 id="Azure-Messaging-Technology-Review"><a href="#Azure-Messaging-Technology-Review" class="headerlink" title="Azure Messaging Technology Review"></a>Azure Messaging Technology Review</h1><p>Now that you have some context around messaging systems and push notifications we’re going to do a thorough survey of the relevant Azure solutions. Some of them you likely already know. We covered several in the previous section. Others we have covered in other Cloud Academy courses. For this lesson, we will do a short review of those technologies covered in the section on IoT. The other technologies that have not yet been introduced in this course will be covered after that.</p>
<p>So let’s start with a brief review of Azure Event Grid, Azure Event Hubs, Azure Service Bus, and Azure Notification Hubs. Azure Event Grid, as you may recall, is an intelligent event routing system. It takes, as an input, some event source, such as IoT Hubs or blob storage, and connects it to a downstream service such as Azure Functions or Logic Apps. It is not meant for message processing or ingestion but rather relaying. Azure Event Hubs, like IoT Hub, IS an event ingestion system. It can be used to ingest generic messages since events are a type of message, however, it is optimized for an event-based use case. It is designed to be extremely scalable and performant. With the right configuration, you can ingest literally millions of messages per second.</p>
<p>Now as for Azure Notification Hubs, which we talked about for IoT, we’re actually going to give it its own lesson.</p>
<p>So that leaves Service Bus for review. Azure Service Bus is probably most directly relevant to this section as it is indeed designed for generic message handling. Service Bus is more than just a queuing system that supports publish&#x2F;subscribe based approaches. As you can see in the comparison below it is a “high-value enterprise messaging” system. Similar to Kafka, it can use topics for sending and receiving messages. Topics can have multiple subscriptions making it easy to scale horizontally.</p>
<p>Now as we discussed previously Service Bus has a number of additional features that make it more robust than just a typical message queue. You get durable transactions, scheduled delivery, dead-lettering for messages that cannot be delivered - you can store and process them later - you also get client-side batching options, filtering for subscription rules to ensure downstream consumers only get specific messages, message duplicate detection, and some handy security features including role-based access control and SAS aka shared access signatures.</p>
<p>So, whew! That’s a lot that it can do. It may seem like Service Bus should then be your go-to solution for messaging with Azure. Mmm…not quite. There are tradeoffs with Service Bus that we will discuss when we start talking about the other messaging solutions later in this section.</p>
<p>One other important thing we need to cover before moving away from Service Bus is Azure Relay. This is a very important component of the Azure product line that has gone under significant change and is directly related to the Service Bus overall platform. At a high-level Azure Relay is a system for connecting corporate enterprise networks to the wider public cloud, without the need to open firewalls or make changes to corporate network infrastructure. If you have had to work with corporate network security with Windows, you may already be familiar with WCF relays - Windows Communication Foundation, that is. WCF is the legacy relay for enabling remote procedure calls securely. Azure Relay subsumes this feature and offers a second feature known as Hybrid Connections.</p>
<p>So what’s the difference between WCF and Hybrid Connections. Well to start, see the chart here. Hybrid Connections is actually an open-protocol evolution of the WCF relay approach. It can be implemented in any language with WebSockets support. Its feature set is entirely based on HTTP and WebSockets. To set it up all you need to do is define a relay namespace in your Azure account.</p>
<p>Now we don’t need to get into the nitty-gritty technical details of how Azure Relay implements all of this. The diagram should give you some idea and we’ve included a link to the documentation for people that want to know more. The key thing is to understand the problem it solves: Allowing your messaging system to safely connect traffic from the public internet to an on-premise or corporate network. (<a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/service-bus-relay/relay-what-is-it">https://docs.microsoft.com/en-us/azure/service-bus-relay/relay-what-is-it</a>)</p>
<p>Now let’s get into the as yet uncovered messaging solutions. We need to talk about Azure Storage Queues, Azure Functions, and Azure Logic Apps.</p>
<p>Let’s take them in order. Azure Storage Queues are the most popular message queuing system within Azure. This is because of their incredible simplicity and flexibility. All you need is an Azure storage account and you can begin creating queues of work to process asynchronously. Using HTTP or HTTPS calls you can process millions of messages from a single queue. The two limitations to keep in mind are 1. The total capacity of your storage account and 2. Individual messages in a queue can only go up to 64 KB in size. Service Bus, by contrast, supports messages from 256 KB up to 1 MB in size depending on your tier. Even with these restrictions, Azure Storage Queues are usually better for most simple message queue use cases. They are simpler to configure than Service Bus and often cheaper. They also have wider library support as there are Azure storage libraries for C++, Python, Ruby, PHP, Node.js, and Java, as well as PowerShell and .NET.</p>
<p>Now, one tradeoff is that Azure Storage queues do not give you the same ordering guarantees as Azure Service Bus. You get an “at least once” delivery guarantee and have to make sure your app has logic to properly order operations and transactions. For this and other reasons Storage Queues are generally better for simply managing messages within your Azure app. See the detailed comparison in the slide here. As you can see there are a number of feature differences between Storage Queues and Service Bus. So thinking about use cases, if, for example, you are dealing with backend worker processes that need to communicate with frontend services, all within an Azure infrastructure, Storage Queues would probably work fine. In this use case, the overhead of Service Bus is likely unnecessary. Once you need to start integrating with external apps and you need more guarantees and real transactions, Service Bus may be the better way to go.</p>
<p>So now let’s switch gears and talk about Azure Functions. If you’re familiar with Amazon Lambda then you already have a basic idea of Azure Functions. They are a service for executing “code on demand.” They are not a message queue or ingestion system. Rather they are for processing messages coming from some other source. So often you will have something like an Event Hub or Service Bus act as an input to an Azure Function. The function takes the message, executes some code to process it, and then outputs it to some other endpoint based on your architecture. Functions are ‘pay per use’ in terms of pricing and super easy to scale; since they are serverless, Azure will do all of the scaling for you.</p>
<p>So how do we actually use an Azure Function? The basic approach is to set triggers. As you can see here there are many kinds of triggers. A Function can be triggered by messages arriving in an Azure Storage Queue, or an Event Hub, or Service Bus. Functions can also be triggered by HTTP requests, Github webhooks, timers, and other generic types of webhooks. You get a range of options and strong integration with Azure services like Event Grid, Notification Hubs, Cosmos DB, and Azure Storage. So if you’re application logic - your startup’s “special sauce” - can be easily translated into discrete code calls to be run as an Azure Function, this may be the simplest and most scalable paradigm for message processing available.</p>
<p>Finally, let’s talk about Azure Logic Apps. Logic Apps are a tool for automating workflows in Azure. So what exactly is a workflow? Well, a workflow is just a business process broken down into a series of steps. It may involve grabbing data from storage, executing a function, posting a message somewhere, sending an alert, firing an email, etc. etc. What Logic Apps do is allow you to automate these workflows and add, surprise surprise, logic! This means you can add conditions - you can make it so that workflows only trigger after specific events. You can define logic apps using the Azure Portal or by writing JSON definitions. You can also use PowerShell.</p>
<p>Logic Apps are not for creating custom code or complete software applications from scratch. They are really for wiring together parts of your Azure infrastructure or connecting it with legacy systems. So how does this relate to messaging? Well, consider the diagram here. Your messaging system needs to do more than just ingest messages. It may need those messages to trigger additional processes. You may be able to do that with an Azure Function, but that would require writing code. With an Azure Logic App you could, for example, have a message trigger an update to a Slack chatroom by having the app monitor your blob storage. This could be set up with just a few clicks on the Azure portal. Pretty cool.</p>
<p>So that about does it for our review of Azure messaging solutions. There is A LOT of content in this lesson so don’t feel bad if you need to review it a few times. We’re going to focus in on our push notification use case in the next lesson and do some light review in the final part of the section covering scalability. See you there!</p>
<h1 id="Creating-a-Push-Notification-System"><a href="#Creating-a-Push-Notification-System" class="headerlink" title="Creating a Push Notification System"></a>Creating a Push Notification System</h1><p>There are many scenarios where you will need your application to send notifications to end users’ smart devices. It may be a fitness app that tracks how many steps we have walked and wants to let us know that we reached a daily goal. It could be a messenger app letting alerting us to a new contact request. If you have a smartphone you can surely think of many other examples.</p>
<p>To demonstrate how to set up a push notification system, we’re going to take the example of a bank application. It needs to notify users whenever they receive a deposit over a certain amount of money. The centerpiece for our system will be Azure Notification Hubs. We’re going to walk through it step by step.</p>
<p>So let’s start by considering what our backend might look like. Consider this architecture. Financial data is managed by a legacy system. We’re using Azure Service Bus to ingest messages and event data. Then there is a mobile backend that subscribes to specific topics and is responsible for pushing messages to our banking mobile app.</p>
<p>The easiest way to integrate our mobile app backend with notification hub would be to use something like Azure App Service Mobile Apps. This is an Azure Platform as a Service tool that has direct integration with notification hubs.</p>
<p>App Service mobile Apps: <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/app-service-mobile/app-service-mobile-value-prop">https://docs.microsoft.com/en-us/azure/app-service-mobile/app-service-mobile-value-prop</a></p>
<p>However if we want to take a more open-ended approach, the basic solution is to create a notification hub and connect it to our mobile app project. We have included a link to a detailed example for how to do that with Android. It is a little out of scope to cover how to set up an Android project, so we’re going to focus on setting up the notification hub. You can do this in the web UI by simply clicking on “create a resource,” then selecting “Web + Mobile” and finally selecting “Notification Hub.” The next menu will ask for some basic information to configure your hub, such as your Azure region, a name for the hub, and a resource group.</p>
<p>Android example: <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-android-push-notification-google-fcm-get-started">https://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-android-push-notification-google-fcm-get-started</a></p>
<p>Once the hub exists you will need to get the connection string from the Access Policies page. This is the credential that will let your mobile backend actually connect to your hub for pushing messages. It will be part of your mobile backend code. From there, usually your code will use some library to handle transmitting the notification with relevant target details. If a library is not available for your use-case, you can use the REST API to send notification messages.</p>
<p>The nice thing about Azure notification hubs is that it makes it easy to test push notifications once you are ready. As you can see from the diagram, in the web UI you can select a platform, create a payload, and do a “test send.” If you have configured your app correctly you should see it on targeted phones running the app.</p>
<p>So that’s basically it, at a high level at least. Actually implementing all of this in a real software stack is of course more complex and requires some programming knowledge. What we wanted to demonstrate here was just problem solved by Azure Notification Hubs - namely, giving your app a centralized easily managed service for handling transmission of notifications.</p>
<p>So now we will close out this section by talking about scaling. It’s all well and good to be able to handle lots of messages with Service Bus and send notifications with Notification hub. But how do we deal with bottlenecks and sudden increases in traffic? How do we optimize our performance? Stick around for the next lesson and you’ll find out!</p>
<h1 id="Scaling-Your-Messaging-Infrastructure"><a href="#Scaling-Your-Messaging-Infrastructure" class="headerlink" title="Scaling Your Messaging Infrastructure"></a>Scaling Your Messaging Infrastructure</h1><p>Scalability is often your biggest and most critical challenge when designing cloud infrastructure. If we cannot tolerate traffic spikes or sudden growth then we have been derelict in our duty. The first place to start when addressing scalability is to identify bottlenecks. What components of our system are most vulnerable to catastrophic failure due to changes in usage patterns?</p>
<p>With the notification and messaging systems we discussed in the previous lessons, there are, at a high level, three places we want to focus: 1. Messaging and event ingestion, or, the “on-ramp” into our system. 2. Message routing, meaning getting ingested messages to the right place without latency. And 3. Message processing, meaning once we are ready to do something with our messages, be it run some code or store them somewhere, we can do that thing quickly.</p>
<p>So let’s start with message ingestion. We’ll address Azure Storage Queues and Azure Service Bus. For Storage Queues, you get Azure’s enterprise storage SLAs, which include certain performance guarantees. See the links for full details in the Azure documentation. Basically, unless you have an extremely high volume system, Azure should be able to handle your needs. Storage Queues can tolerate 20,000 messages per second per storage account at a rate of 2000 messages per second per queue.</p>
<p>With Service Bus, performance varies a bit depending on the tier you select. In both cases you will be able to handle larger message sizes than storage queues. The main difference between the Service Bus Premium and Standard tiers is in network performance. The Premium tier guarantees consistent high throughput performance at a fixed price with the ability to scale workloads up and down. With the standard tier it is “pay as you go” with variable latency and throughput. Also note that the size of individual queues with Service Bus is fixed at a maximum of 80 GB, wherease Azure Storage queues can go all the way up to 500 TB. Keep this in mind if you have an unusual use case.</p>
<p>So the basic scaling takeaway here is that with both services you get strong performance guarantees but you may need to think carefully about whether Service Bus or Storage Queues make more sense for your needs as there are key differences.</p>
<p>Now for routing messages we’re going to just briefly mention Event Grid. It has strong scalability guarantees as well and is great for serverless architectures that need to route data to various endpoints. Event Grid includes 24-hour retry with exponential backoff to give you some breathing room if there is a temporary issue with your system. You also get a throughput guarantee of support for millions of events per second. Be aware, though, that the pricing model is pay-per-event. If cost is an issue it could be cheaper to handle events with your own custom system and only pay for network bandwidth.</p>
<p>Finally, how do we scale our message processing? Well, that depends on exactly what is doing the processing. If we’re using Azure Functions, happily, we don’t need to worry about scaling - Azure will do it for us automatically as traffic increases. Be aware that you will of course be charged based on function usage. If we have some sort of app running on a VM doing the processing, then we may need to implement some form of autoscaling. Our endpoint could also be an Azure Logic Apps. See the documentation link for details on its limitations. There are http request and message size limits that are not really changeable. If your bottleneck is needing to run more actions in a logic app workflow, you can add nested workflows to get more done.</p>
<p>So that about does it for scaling. We have talked about ingesting, routing, and processing messages using Azure solutions. In many cases we get solid default guarantees from Azure. In some cases we may need to select specific service tiers or change our configuration. The nice thing is that Azure is such a mature and robust system that, unless you are Amazon or Google, you probably can get your work done with their solutions. So we will end it there for our section on messaging. Our last part of this course will focus on handling media with Azure services. See you there!</p>
<p>Logic Apps: <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-limits-and-config">https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-limits-and-config</a></p>
<p>Storage queues performance: <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/common/storage-scalability-targets#azure-queue-storage-scale-targets">https://docs.microsoft.com/en-us/azure/storage/common/storage-scalability-targets#azure-queue-storage-scale-targets</a></p>
<h1 id="Section-4-Introduction"><a href="#Section-4-Introduction" class="headerlink" title="Section 4 Introduction"></a>Section 4 Introduction</h1><p>In the final section of the Design Solutions Using Platform Services course, we are going to focus on media - specifically video content. Video media content is notoriously tricky to work with both for developers and cloud platforms. It is more difficult to analyze than text or images. It also tends to be far larger in size and therefore more expensive to store and transmit. You also have to worry about encoding and optimizing user experience for different browsers, media players, and bandwidths.</p>
<p>With Azure, these challenges do not go away, but they are made easier to deal with by the available toolset. Azure Media Services, Microsoft’s flagship solution for dealing with media, will help you navigate most of the above challenges. We will spend a good amount of time in the next lesson covering all of its capabilities. We will also discuss Azure Video Indexer, Computer Vision API, and a few other useful products. In the last part of the section, we will go over file-based encoding thoroughly and introduce Azure Media Analytics.</p>
<p>Once you finish this section, you should be a lot less nervous about dealing with videos and streaming content on the Azure platform. So if you’re ready, let’s dive in and get started.</p>
<h1 id="Media-Technology-Review"><a href="#Media-Technology-Review" class="headerlink" title="Media Technology Review"></a>Media Technology Review</h1><p>Azure Media Services is your base of operations for managing video media. It will handle storage, encoding, and packaging video for streaming delivery. To use it you need to create an Azure Media Services account. This is as simple as a few clicks in the Azure web portal.</p>
<p>As you can see from the diagram Media Services manages video content in five parts. First, there is upload and storage for administering content at rest. Then there is the encoding step to prepare content for media players. Next is setting asset delivery policies for clarifying how content can be used. From there, content is published to an OnDemand locator, and finally, content can be streamed to clients. Mixed into those five steps are some important security decisions. In this diagram, we add those in. There is setting the encryption content key and an authorization policy. Together these allow for content to be dynamically encrypted during playback. Media Services also includes options for encrypting content at rest.</p>
<p>Azure Media Services supports a number of approaches to delivering content. You can create streaming endpoints for video content using multi-bitrate (adaptive bitrate for unstable network conditions) streams over HLS, HTTP Live Streaming, or MPEG-DASH. You can create URL endpoints for both streaming or download. It’s up to you to decide whether you want users to be able to copy your media files, play them in browsers, or stream them in smart phones and other devices.</p>
<p>Honestly, it would take its own course to cover every single thing Media Services can do. See the documentation if you really want to go down a rabbit hole. One very important thing I will leave you with regarding Media Services, however, is that you can interact with its functionality in multiple ways. Like other Azure products Media Services works with the Azure web dashboard, a REST API, and has an SDK for .NET language support. So regardless of how you want to access your Media Services account, you will always have options.</p>
<p>Now, an important related service is the Azure Video Indexer product. Video Indexer replaces the older Video API service Azure supported in the past. Video Indexer is a relatively straightforward set of tools built on top of Azure Media Analytics, which we will cover later. It is designed to extract information from videos for easier management. So for example you can use Video Indexer to do visual text recognition on a video, transcribe audio, detect faces, translate the video to another language, and much much more. See the list of features in the slide here to get a full sense. Important to note: These tools are also supported by the SDK and can be accessed using .NET or REST calls.</p>
<p>Finally, the Azure Computer Vision API is somewhat similar to the image classification tools we discussed in the section on AI. This API gives developers tools for processing images using Azure’s advanced classification algorithms. You can access the API using languages like Python, C#, Ruby, Java, and PHP, or even just use cURL. Assuming your image meets the requirements shown in the slide, you can then call the API to do things like recognize text, flag adult content, apply tags based on content, generate descriptions, detect human faces, and even recognize domain-specific content.</p>
<p>So there you have it - we have gone over most of the basic media technologies in the Azure software ecosystem. We still have one major component to cover - the Azure Media Analytics toolset. We will cover that and briefly discuss file-based encoding systems in Azure in the next lesson. See you there.</p>
<h1 id="File-Based-Encoding-and-Azure-Media-Analytics"><a href="#File-Based-Encoding-and-Azure-Media-Analytics" class="headerlink" title="File-Based Encoding and Azure Media Analytics"></a>File-Based Encoding and Azure Media Analytics</h1><p>We will wrap up our lesson on Azure Media with a discussion on media file encoding and an introduction to the Azure Media Analytics tool set.</p>
<p>We aren’t going to go super deep here with respect to compression algorithms or the history of different codecs. For our purposes you just need to know what a video codec is. It is simply a piece of software that compresses or decompresses digital media. An encoder is for compression and a decoder is for decompression, and the codec is the software doing the work. Generally when we talk about encoding we are talking about audio or video files. There are numerous formats such as mp3, wma, mp4. Some forms are lossy or lossless depending on the compression algorithms used. Lossy formats usually take far less space but will lose information over time as the file is compressed and decompressed. Lossless formats take more space but maintain their quality over time. This is why your hipster friend is really into using FLAC for his music collection.</p>
<p>So our focus is file-based encoding. File-based encoding, as you might have guessed, relies on video files. These will be the inputs and outputs for your codec software. So you will start with some raw video file taken from a camera or other source, run that through an encoding program, and the output will be a compressed version of the video that will use less space and be playable in a wide range of media players.</p>
<p>Microsoft Azure has its own Encoding service. See the link below for the full list of supported input file types and output file options. Suffice to say, with Azure Encoding, you should have everything you need to get your video content properly formatted for delivery. The trickier question may be deciding which version of the service to use. Azure offers both a standard and a “Premium Workflow” version. Pause the video on this slide to get a thorough look at the feature differences. Probably the biggest advantage for Premium is the ability to apply conditional logic on file encoding. This will allow you to automate encoding rules and thereby encode large numbers of files quickly. If you only have a small number of video files to manage, the standard service may suit you just fine.</p>
<p>(List of formats: <a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/media-services/previous/media-services-compare-encoders">https://docs.microsoft.com/en-us/azure/media-services/previous/media-services-compare-encoders</a>)</p>
<p>In the last part of this section, with the other Azure media services and file encoding now behind us, we will talk about how to gain useful insights from our media using Azure Media Analytics. As you can see in the diagram Media Analytics works in parallel with the components responsible for encoding, streaming, and storing your media.</p>
<p>Media Analytics is actually a combination of several small tools that work together to give you useful intelligence on your video content. There is the Indexer service that makes your content searchable and helps generate closed-captioning tracks. There is the Hyperlapse service that adds video stabilization and time-lapse capability. There is an optical character recognition tool as well as a face and motion detector application. There is a tool for face redaction if needed for legal or security purposes. There are also content moderation and video summarization systems to make administration of large amounts of video content simpler.</p>
<p>Azure Media Analytics is your one stop shop for efficiently analyzing and optimizing your video library. It can save you thousands by baking together several tools that would be expensive if purchased separately.</p>
<p>So that about wraps it up for our lesson on encoding and media analytics. Congratulations. You are now an expert on media management with Azure. Great job!</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>So congratulations! You made it! Give yourself a pat on the back because it has been a long and tough ride. We went through a lot of pretty dense material so before we pop the champagne bottles, let’s take a minute to briefly review what we have accomplished.</p>
<p>We learned how to design a variety of software solutions using Azure platform services. You should now be ready to tackle a wide range of problems using Azure technologies. We focused on four broad categories: Artificial Intelligence, Internet of Things, Messaging, and Media.</p>
<p>Recall our four learning objectives:</p>
<p>\1. The student will have an in-depth knowledge of Azure artificial intelligence services and know when to opt for specific offerings. To that end, we covered various Azure tools including Azure Machine Learning, Machine Learning Studio, Batch AI, Cognitive Services, and Databricks.</p>
<p>\2. The student will have an in-depth knowledge of Azure IoT services and know when to opt for specific offerings.  In section 2 we went deep on this, covering many services, including Azure Event Grid, Service Bus, IoT Edge, Stream Analytics, and Notification Hubs.</p>
<p>\3. The student will be able to design a scalable messaging architecture using Azure services. In what was probably the densest section we covered both the concept of messaging in software systems and how to scale. The relevant Azure technologies were Azure Service Bus, Storage Queues, and Event Hubs, some of which were covered in IoT but required a deeper dive here.</p>
<p>\4. The student will have a deep understanding of Azure media services. So, “Azure Media Services” is the name of the core Azure tool for managing video. However, we also covered media file encoding in some depth, as well as other media services including Azure Media Analytics, Computer Vision API, and Azure Video Indexer.</p>
<p>With all this information at your disposal, you should be ready to get to work on a number of different problems. Keep in mind, however, that practice makes perfect. You won’t be a true expert until you actually make use of these different technologies and learn the nuances. To be a master Azure technologist, you will need to dive in and get your hands dirty.</p>
<p>Now that you are done I’d like to invite you to send any feedback you have about the course to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. We greatly appreciate your comments, questions, and suggestions. Congratulations again on fighting through the whole course and good luck in your future endeavors.</p>
<h2 id="Azure-Artificial-Intelligence-Services-1"><a href="#Azure-Artificial-Intelligence-Services-1" class="headerlink" title="Azure Artificial Intelligence Services"></a><strong>Azure Artificial Intelligence Services</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/#analytics">Analytics Tools Catalogue</a></p>
<h2 id="7IoT-Azure-Technology-Review"><a href="#7IoT-Azure-Technology-Review" class="headerlink" title="7IoT Azure Technology Review"></a>7<strong>IoT Azure Technology Review</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs">IoT Hub and Event Hubs Comparison</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/notification-hubs/notification-hubs-aspnet-cross-platform-notification">How-To Guide</a></p>
<h2 id="12Creating-a-Push-Notification-System"><a href="#12Creating-a-Push-Notification-System" class="headerlink" title="12Creating a Push Notification System"></a>12<strong>Creating a Push Notification System</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/app-service-mobile/app-service-mobile-value-prop">App Service mobile Apps</a></p>
<h2 id="13Scaling-Your-Messaging-Infrastructure"><a href="#13Scaling-Your-Messaging-Infrastructure" class="headerlink" title="13Scaling Your Messaging Infrastructure"></a>13<strong>Scaling Your Messaging Infrastructure</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-limits-and-config">Logic Apps</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/storage/common/storage-scalability-targets#azure-queue-storage-scale-targets">Storage queues performance</a></p>
<h2 id="16File-Based-Encoding-and-Azure-Media-Analytics"><a href="#16File-Based-Encoding-and-Azure-Media-Analytics" class="headerlink" title="16File-Based Encoding and Azure Media Analytics"></a>16<strong>File-Based Encoding and Azure Media Analytics</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.microsoft.com/en-us/azure/media-services/previous/media-services-compare-encoders">List of formats</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Getting-Started-with-Azure-Container-Instances-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Getting-Started-with-Azure-Container-Instances-12/" class="post-title-link" itemprop="url">AZ-204-Getting-Started-with-Azure-Container-Instances-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:08:32 / Modified: 11:08:34" itemprop="dateCreated datePublished" datetime="2022-11-14T11:08:32-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Getting-Started-with-Azure-Container-Instances-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Getting-Started-with-Azure-Container-Instances-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AZ-204-Building-Containers-with-Azure-DevOps-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AZ-204-Building-Containers-with-Azure-DevOps-11/" class="post-title-link" itemprop="url">AZ-204-Building-Containers-with-Azure-DevOps-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-11-14 11:07:58 / Modified: 23:32:16" itemprop="dateCreated datePublished" datetime="2022-11-14T11:07:58-04:00">2022-11-14</time>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AZ-204-Building-Containers-with-Azure-DevOps-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AZ-204-Building-Containers-with-Azure-DevOps-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to “Building Containers with Azure DevOps”. My name is Thomas Mitchell and I’ll be taking you through this course. </p>
<p>I’m an Azure Instructor at Cloud Academy and I have over 25 years of IT experience, several of those with cloud technologies. If you have any questions, feel free to connect with me on LinkedIn, or send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>This course is intended for DevOps professionals who wish to learn how to design and implement, through the use of containers, strategies for developing application code and infrastructure that allow for continuous integration, testing, delivery, monitoring, and feedback.</p>
<p>To get the most from this course, you should have a basic understanding of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Azure</a> platform and of container concepts.</p>
<p>We’ll kick off by talking about ways to create deployable images. You’ll learn about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/the-power-of-docker-containers/">docker containers</a> and their role in development. You’ll also learn about microservices and where they fit in.</p>
<p>After discussing microservices, we’ll dive into the different Azure container-related services. We’ll talk about Azure Container Instances, the Azure Kubernetes Service, the Azure Container Registry, and Azure Service Fabric. We’ll also tough on Azure App Service.</p>
<p>Once we finish up with the different container-related services, we’ll look at a typical Dockerfile.</p>
<p>Later on, in this course, we’ll look at Docker multi-stage builds. You’ll learn what multi-stage builds are, and things to consider when working with multi-stage builds.</p>
<p>We’ll round things out with a hands-on demonstration that shows you how to create an Azure Container Registry,</p>
<p>By the time you complete this course, you’ll have a better understanding of containers and how they are used in Azure DevOps.</p>
<p>We’d love to get your feedback on this course, so please give it a rating when you’re finished. If you’re ready to learn how to build containers with Azure DevOps, let’s get started.</p>
<h1 id="The-Power-of-Docker-Containers"><a href="#The-Power-of-Docker-Containers" class="headerlink" title="The Power of Docker Containers"></a>The Power of Docker Containers</h1><p>Hello and welcome to “The Power of Docker Containers”. Let’s talk a little bit about what Docker containers bring to the table as far as the development process goes.</p>
<p>As a software containerization platform, Docker offers developers a common toolset and packaging model. It also provides a deployment mechanism for containerized apps. This results in simplified management, regardless of the host, as well as a seamless <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/introduction/">DevOps</a> experience.</p>
<p>Docker images can be created and deployed identically across virtually any environment in seconds. The Docker ecosystem is huge. This ecosystem includes hundreds of thousands of apps that are packaged in Docker containers. DockerHub, which is the Docker-maintained public containerized application registry, publishes almost 200,000 applications in the public community repository. </p>
<p>The notion that you can deploy a SQL Server Linux instance in seconds, using a Docker image, is a testament to the power of containers. It is this power that <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/microservices-and-containers/">containerization</a> brings to the DevOps sphere.</p>
<h1 id="Microservices-and-Containers"><a href="#Microservices-and-Containers" class="headerlink" title="Microservices and Containers"></a>Microservices and Containers</h1><p>Hi there. Welcome to Microservices and Containers. Although containers have been most commonly used to simplify <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/introduction/">DevOps</a> through simplified developer-to-test-to-production flows, there are other uses for them. Microservices, for example, is a quickly-growing use-case for containers. </p>
<p>The term “Microservices” refers to an application development strategy where each part of an application is actually deployed as a completely self-contained component (or microservice). The microservices that comprise an application can then be individually scaled and updated. </p>
<p>The easiest way to explain the concept of microservices is to use an example scenario.</p>
<p>Let’s imagine for a second that your organization is the author of a large, monolithic tax application. As part of the development of the next revision of the software, your organization wants to migrate it to a collection of microservices. </p>
<p>Now, the current app might include a piece of code that does some specific tax calculation in certain circumstances – and this code may exist in several spots within the app. Whenever new tax laws are approved or changed, changes are needed in the calculation of taxes. This means that the same changes need to be made anywhere this tax calculation is performed within the app.</p>
<p>By moving to a collection of microservices, you could allow the application to create a notification that a tax calculation needs to be made in response to some scenario. Microservices involved in that calculation can be subscribed to those notifications – and then those individual microservices can do what they need to do to perform the tax calculations. You’d likely have one specific microservice that does the actual calculation.</p>
<p>Whenever a new tax law is enacted that affects tax calculations, you would only need to update the microservice(s) responsible for the calculations. You wouldn’t need to make changes throughout the app code base.</p>
<p>While you might operate a development or testing environment on a single server or by using a single instance of each microservice, production is typically a different story. In such an environment, you are likely going to want to be able to scale things out. You’ll probably want to scale out to multiple instances across a cluster of servers, rather than running things on one server. You’ll want the ability to scale in as well. You may also want different teams within your development department to be able to independently work on, and update, different microservices that each team is responsible for.</p>
<p>This is where microservices can really shine. By leveraging the benefits of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/the-power-of-docker-containers/">Docker containers</a> when working with more complex microservice-based applications, organizations can become more agile, because those microservices can be quickly scaled out and in to meet the loads on the application. While doing so, however, the isolation of resources and namespaces offered by containers ensures that one microservice instance does not interfere with any other instances. This makes it easier to design a solid microservice architecture, which, in turn, allows organizations to deal with the management, deployment, orchestration, and patching needs of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/container-related-services-in-azure/">container-based services</a> – while limiting the risks to availability.</p>
<h1 id="Container-Related-Services-in-Azure"><a href="#Container-Related-Services-in-Azure" class="headerlink" title="Container-Related Services in Azure"></a>Container-Related Services in Azure</h1><p>Hi there. Welcome to “Container-Related Services in Azure”. In this lecture, we’ll take a quick 30,000 foot view of each of the container-related services that are available in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>. We’ll look at Azure Container Instances, the Azure Kubernetes Service, the Azure Container Registry, Azure Service Fabric, and Azure App Service.</p>
<p>Let’s start with Azure Container Instances.</p>
<p>When you run workloads in Azure Container Instances, you can focus on app development and deployment instead of the deployment and management of the underlying infrastructure that’s necessary to run those apps.</p>
<p>While Azure Container Instances are easy to deploy, the main advantage of using them is the security that the hypervisor isolation that they provide for each container group. With this type of isolation available, you can be sure that your organization’s containers aren’t sharing their OS kernel with other containers.</p>
<p>You can read more about Azure Container Instances by visiting the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/container-instances/">https://azure.microsoft.com/en-us/services/container-instances/</a></p>
<p>The Azure Kubernetes Service started out as Azure Container Services, or ACS. It originally supported Docker Swarm and Mesos&#x2F;Mesosphere DC&#x2F;OS for orchestration management. However, when Kubernetes support was added, it became so popular that Microsoft eventually renamed the Azure Container Service to the Azure Kubernetes Service, or AKS.</p>
<p>At this point, Kubernetes is really the standard for container orchestration. Using the Azure Kubernetes Service, you can not only deploy and manage Kubernetes, but you can also scale and run your applications in a secure environment.</p>
<p>To learn more about the Azure Kubernetes Service, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/kubernetes-service/">https://azure.microsoft.com/en-us/services/kubernetes-service/</a></p>
<p>The Azure Container Registry is another container-centric service offering available in Azure. It allows you to store and manage your container images in a central registry, which is integrated with several other Azure services, including the App Service, Batch, and Service Fabric, among others. </p>
<p>Azure Container Registry supports many types of container deployments, including DC&#x2F;OS, Docker Swarm, and Kubernetes. Because of the broad support that Azure Container Registry offers, you can manage the configuration of your applications without being locked into the configuration of the target hosting environment. </p>
<p>To read more about the Azure Container Registry, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/container-registry/">https://azure.microsoft.com/en-us/services/container-registry/</a></p>
<p>Azure Service Fabric is a distributed systems platform that allows you to build and operate always-on, scalable, distributed apps. The service makes it easier to package, deploy, and manage scalable and reliable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/microservices-and-containers/">microservices and containers</a>. It can also host and orchestrate containers.</p>
<p>By leveraging Azure Service Fabric, you can avoid infrastructure problems and focus solely on the deployment of mission-critical workloads that are not only reliable, but also scalable. </p>
<p>For more details on the Azure Service Fabric, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/service-fabric/">https://azure.microsoft.com/en-us/services/service-fabric/</a></p>
<p>Azure Web Apps is an Azure offering that provides you with a managed service for both Windows-based and Linux-based web applications. This service allows you to deploy and run containerized apps for both platforms and it also offers auto-scaling and load balancing options. You can even integrate with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/introduction/">Azure DevOps</a>.</p>
<p>To read more about the Azure App Service, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/app-service/">https://azure.microsoft.com/en-us/services/app-service/</a></p>
<h1 id="Anatomy-of-a-Dockerfile"><a href="#Anatomy-of-a-Dockerfile" class="headerlink" title="Anatomy of a Dockerfile"></a>Anatomy of a Dockerfile</h1><p>Hi there. Welcome to “Anatomy of a Dockerfile”. In this lecture, we’re going to look at a basic Dockerfile, line by line.</p>
<p>Dockerfiles are used by docker build to assemble images. They are essentially text files that contain the commands that are necessary to build an image. On your screen is an example of a very basic Dockerfile. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu </span><br><span class="line">LABEL maintainer=&quot;tmitchell@cloudacademy.com&quot; </span><br><span class="line">ADD appsetup / </span><br><span class="line">RUN /bin/bash -c &#x27;source $HOME/.bashrc; echo $HOME&#x27; </span><br><span class="line">CMD [&quot;echo&quot;, &quot;Hi everybody!&quot;] </span><br></pre></td></tr></table></figure>

<p>Generally speaking, every image is based off another existing image. In this sample Dockerfile here, line one refers to the parent image that this new image will be based on. The Ubuntu image referred to in line one would be retrieved from either a local cache or from DockerHub.</p>
<p>I should point out here that an image that doesn’t have a parent is called a base image. If that were the case here, what we could do with this Dockerfile is completely omit the FROM line altogether. We could also replace it with FROM scratch, instead. However, since we’re using the ubuntu base image, we are referencing it in the file.</p>
<p>The second line in this sample Dockerfile uses the LABEL command to set the email address of the person who maintains the file.</p>
<p>The third line in our example uses the ADD command to add a file, called “appsetup” into the root folder of the image being created.</p>
<p>The fourth line is a RUN command that runs when the image is being created by docker build. This part of the Dockerfile is typically used to configure things within the image. </p>
<p>The last line in our sample file calls a command that we want to execute once the new container is created from the image.</p>
<p>Of course, every Dockerfile will be different – and they can be as simple or as complex as they need to be. Simpler, however, is always better than complex, when possible.</p>
<p>For more information, visit the Dockerfile reference URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a></p>
<h1 id="Multi-Stage-Builds"><a href="#Multi-Stage-Builds" class="headerlink" title="Multi-Stage Builds"></a>Multi-Stage Builds</h1><p>Hi there. Welcome to “Multi-Stage Builds”. In this lecture, we’re going to take a look at what multi-stage builds are and what they bring to the table.</p>
<p>So, Multi-stage builds are a new feature that makes life easier when working with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/anatomy-of-a-dockerfile/">Dockerfiles</a>. They are extremely helpful when trying to optimize Dockerfiles while ensuring that they are still easy to read.</p>
<p>Prior to the introduction of multi-stage builds, what you would typically have is one Dockerfile to use for development. This Dockerfile would contain everything that you need to build the application that you wished to deploy. You’d also have a second, slimmed-down Dockerfile that you would use for production. This second file would contain just the application and only the resources needed to run it. This “builder pattern” of maintaining two Dockerfiles, obviously, isn’t ideal.</p>
<p>The code that you see on your screen is a good example of what the typical “builder pattern” consists of. Notice we have 3 different files. We have DockerFile.build, the Dockerfile, and build.sh.</p>
<p>Dockerfile.build:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3</span><br><span class="line">WORKDIR /go/src/github.com/alexellis/href-counter/</span><br><span class="line">RUN go get -d -v golang.org/x/net/html </span><br><span class="line">COPY app.go .</span><br><span class="line">RUN go get -d -v golang.org/x/net/html \</span><br><span class="line"> &amp;&amp; CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br><span class="line">Dockerfile:</span><br><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY app .</span><br><span class="line">CMD [&quot;./app&quot;] </span><br><span class="line">build.sh:</span><br><span class="line">#!/bin/sh</span><br><span class="line">echo Building alexellis2/href-counter:build</span><br><span class="line">docker build --build-arg https_proxy=$https_proxy --build-arg http_proxy=$http_proxy \ </span><br><span class="line">  -t alexellis2/href-counter:build . -f Dockerfile.build</span><br><span class="line"> </span><br><span class="line">docker create --name extract alexellis2/href-counter:build </span><br><span class="line">docker cp extract:/go/src/github.com/alexellis/href-counter/app ./app </span><br><span class="line">docker rm -f extract</span><br><span class="line">echo Building alexellis2/href-counter:latest</span><br><span class="line">docker build --no-cache -t alexellis2/href-counter:latest .</span><br><span class="line">rm ./app</span><br></pre></td></tr></table></figure>

<p>If you look at this example, you’ll see that it artificially compresses two different RUN commands together. This is done to avoid creating an additional layer in the image. As is the case with any sort of coding, the more code you have, the more error-prone it becomes – and the more difficult it becomes to maintain.</p>
<p>When the build.sh script is run, it first has to build the first image. Then, it needs to create a container from it, so that it can copy the artifact out, before building the second image. In this scenario, you are left with 2 images – both of which take up room on your system. You are also left with the app artifact on your local disk as well.</p>
<p>Enter Multi-stage builds, which greatly simplify things.</p>
<p>Prior to multi-stage builds, keeping image sizes down was a challenge. This is because every instruction included within a Dockerfile adds a layer to the image. Not only that, but you also need to remember to clean up unneeded artifacts before moving on to the next layer. </p>
<p>Until multi-stage builds became available, authoring an efficient Dockerfile meant using shell tricks and other logic to keep the layers as small as possible. You also had to use the same tricks to ensure that each layer has only the artifacts that it needs from the previous layer and nothing else.</p>
<p>On the screen is an example of a multi-stage file.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3</span><br><span class="line">WORKDIR /go/src/github.com/alexellis/href-counter/</span><br><span class="line">RUN go get -d -v golang.org/x/net/html </span><br><span class="line">COPY app.go .</span><br><span class="line">RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=0 /go/src/github.com/alexellis/href-counter/app .</span><br><span class="line">CMD [&quot;./app&quot;] </span><br></pre></td></tr></table></figure>

<p>When you leverage multi-stage builds, you can use multiple FROM statements within the Dockerfile. Each of the FROM statements begins a new stage. The stages, themselves, are numbered in order, starting with stage 0. What you would typically do, though, to make the Dockerfile easier to maintain, is use the AS clause to name, or alias, each stage.</p>
<p>Notice the aliasing that’s been added to the file on your screen:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3 AS builder</span><br><span class="line">WORKDIR /go/src/github.com/alexellis/href-counter/</span><br><span class="line">RUN go get -d -v golang.org/x/net/html </span><br><span class="line">COPY app.go  .</span><br><span class="line">RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR /root/</span><br><span class="line">COPY --from=builder /go/src/github.com/alexellis/href-counter/app .</span><br><span class="line">CMD [&quot;./app&quot;] </span><br></pre></td></tr></table></figure>

<p>This example on your screen names the stage and uses the name in the COPY instruction. By referencing the name in the copy instruction, even if the instructions in this Dockerfile are re-ordered later on for some reason, the COPY won’t break.</p>
<p>I should also mention that when you build an image, you don’t have to build the entire Dockerfile, including every stage. Instead, you can specify a single target build stage. The command that you see on your screen, when using our example Dockerfile, stops at the stage named builder:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker build --target builder -t alexellis2/href-counter:latest</span><br></pre></td></tr></table></figure>

<p>The –target option, in this command, tells docker build to create an image up to the target of builder, which is a named stage in our example file. </p>
<p>In the next lesson, we’ll take a look at some best practices that you can follow when working with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/best-practices-for-multi-stage-builds/">multi-stage builds</a>.</p>
<h1 id="Best-Practices-for-Multi-Stage-Builds"><a href="#Best-Practices-for-Multi-Stage-Builds" class="headerlink" title="Best Practices for Multi-Stage Builds"></a>Best Practices for Multi-Stage Builds</h1><p>Hi there. Welcome to Best Practices for <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/multi-stage-builds/">Multi-Stage Builds</a>. In this lecture, we’re going to review some of the best practices that you should be following when working with multi-stage builds. We’re going to talk about adopting container modularity, avoiding unnecessary packages, choosing an appropriate base, and avoiding the inclusion of application data.</p>
<p>When working with builds, you really want to avoid overly complex container images that couple together several applications. Instead, what you should be doing is using multiple containers, with each one intended for a single purpose. For example, you might want to put a website in one container but relegate the database for the website to another container.</p>
<p>While there are always going to be exceptions to this rule, splitting up the components of an application into separate containers makes it more likely that you will be able to minimize work effort by being able to reuse containers. Adopting container modularity will also often make it easier to scale an application. Using our website example from earlier, container modularity would allow you to add replicas of the website container while leaving the database container alone. </p>
<p>An easy way to minimize image sizes is to avoid including unnecessary packages in your images. For example, instead of including packages that you think you MIGHT need, leave them out until you are sure you need them. Once you are sure you need them, you can include them.</p>
<p>Choosing an appropriate base image, or parent image, allows you to optimize the contents of your Dockerfile. By starting with an image that only contains the packages that you need, you can keep your Dockerfiles in check. </p>
<p>Although it’s possible to store your application data right inside the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/demo-create-an-azure-container-registry/">container</a>, doing so increases the size of your image. This runs counter to the idea of optimizing things. Instead of storing app data in your containers, you should consider using docker volume support. By doing so, you can maintain isolation of the application itself, and its data. </p>
<p>For more Dockerfile best practices, visit the URL that you see on your screen:</p>
<p><a target="_blank" rel="noopener" href="https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/">https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/</a></p>
<h1 id="Demo-Create-an-Azure-Container-Registry"><a href="#Demo-Create-an-Azure-Container-Registry" class="headerlink" title="Demo: Create an Azure Container Registry"></a>Demo: Create an Azure Container Registry</h1><p>Hi there and welcome back. In this demonstration, we’re going to do a couple of different things. First, we’re going to deploy a Container Registry in Microsoft Azure using the Azure portal. Once we’ve got that deployed, we’re going to log in to that registry, using the Azure CLI from our local workstation. Once we’ve logged into our registry, what we’re going to do is use the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/demo-add-docker-support-to-an-existing-application/">Docker</a> terminal which is also installed on our local workstation. And we’re going to first, pull down a basic hello world image from the Docker Hub.</p>
<p>Once we have the Hello World image pulled down from Docker Hub, we’ll take that image and we’ll push it to our Container Registry in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a>. Once we have it pushed to our Container Registry in Azure, we’ll go back to the Azure portal, and we’ll take a look and confirm that our hello world image has in fact been pushed into our Container Registry. So let’s get started here.</p>
<p>On the screen, you can see I’m logged into my Azure Portal here, I’m at my homepage. I am logged in as the admin here. To deploy a Container Registry, we’re simply going to create a resource. And we’ll search the marketplace for Container Registry. And we can see Container Registry here and we’ll create it. We need to give our registry a unique name. And we’ve called it my9878 and it appends, what Azure will do here is append this azurecr.io domain name to your registry name.</p>
<p>So the registry name needs to be unique across Azure. We’ll deploy into our lab subscription and into my resource group. We can leave the rest of this stuff here at its default. We’ll go ahead and click Create. The deployment here shouldn’t take long, usually a couple minutes at most. We can see it’s been deployed. So we’ll go to our resource here. And if we go into repositories within our registry, we can see we have no repositories here.</p>
<h1 id="Demo-Add-Docker-Support-to-an-Existing-Application"><a href="#Demo-Add-Docker-Support-to-an-Existing-Application" class="headerlink" title="Demo: Add Docker Support to an Existing Application"></a>Demo: Add Docker Support to an Existing Application</h1><p>So now that our my9878 <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/demo-create-an-azure-container-registry/">container registry</a> has been deployed. Let’s log into it using the Azure CLI from my workstation. So let me bounce down here. I actually have my PowerShell running and my Docker terminal here. So let’s open up PowerShell here. And from PowerShell what I’m going to do is run the az acr login command. And when I do this, I need to specify the name of my repository.</p>
<p>Now oddly enough, I don’t need to nor should I include the azurecr.io domain name when I specify the name for my repository here. So we bounced back out to overview. See the full login server here, my9878.azurecr.io Instead, I just use the name that I gave my repository. So we’ll go ahead hit enter here and we can see that our login has succeeded. Now what we’re going to do into this exercise is push the basic hello world image up to my container registry. That being said, I have to obtain that hello world image first and I’m going to obtain that from Docker Hub.</p>
<p>Now to do that, I’m going to switch over to my Docker terminal here. Now from Docker terminal, what I’m going to do is run a Docker pull hello world command and what this will do is pull the latest hello world image from Docker.io So it’s coming from Docker Hub. Now before we push this image into our container registry. We need to first tag it with the fully qualified domain name of our ACR login server.</p>
<p>If we bounce out to our Azure portal, we can see the FQDN for our login server is my9878.azurecr.io So that’s what we’re going to use here. So let’s bounce back into our terminal here. And we’re going to use the Docker tag command to perform this tagging. Along with the tag command, we need to specify the image. After specifying the name of the image we want to tag, we need to specify the ACR login server. And with that we need to specify the name of our image and the versioning for it.</p>
<p>So we’ll go ahead and tag it. And then what we’ll do now is perform the push using the Docker push command. And essentially we’re going to specify the name we just called in the Docker tag command. And we can see it prepares and then pushes and tells us the image has been pushed. To confirm that our image has been pushed, we can go into our portal and then take a look at our repositories. And we can see hello world is now listed as a repository.</p>
<p>At this point, we can now try to run the image from our container registry. And to do that we’ll bounce back down into our Docker terminal. And from here we’ll use the Docker run command. So we’ll go Docker run and again, we’ll specify the image from our registry. And if we look closely here we can see we get a message from Docker telling me that our installation appears to be working correctly.</p>
<p>So with that, we’ve deployed a container registry in <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/azure/">Microsoft Azure</a> using the portal. We used Azure CLI within Azure PowerShell from our local workstation to log in to our registry. And then we used the Docker terminal to pull down an image from Docker Hub. We tagged it and then we pushed that image up into our own registry. Once we confirmed that the push was successful, we were also able to successfully run the image from our registry. So with that let’s call it a wrap.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Congratulations! You’ve come to the end of “<a target="_blank" rel="noopener" href="https://cloudacademy.com/course/building-containers-with-azure-devops-978/introduction/">Building Containers with Azure DevOps</a>“. Let’s review what you’ve learned!</p>
<p>We kicked things off by talking about ways to create deployable images. You learned about docker containers and their role in development. You also learned about microservices and where they fit in.</p>
<p>After discussing microservices, we looked at the different Azure container-related services. We talked about Azure Container Instances, the Azure Kubernetes Service, the Azure Container Registry, and Azure Service Fabric. We also touched on Azure App Service.</p>
<p>After finishing up with the different container-related services, you learned what a typical Dockerfile looks like.</p>
<p>Later on, you learned about Docker multi-stage builds. You learned what multi-stage builds are, and things to consider when working with multi-stage builds.</p>
<p>We wrapped up with a hands-on demonstration that showed you how to create an Azure Container Registry.</p>
<p>At this point, you should have a better understanding of containers and how they are used in Azure DevOps.</p>
<p>I should point out, before you go, that, in addition to completing courses like this one, you should always keep up with the latest features and services by reading Microsoft’s published documentation as well.</p>
<p>As always, thanks for watching, and happy learning!</p>
<h2 id="4Container-Related-Services-in-Azure"><a href="#4Container-Related-Services-in-Azure" class="headerlink" title="4Container-Related Services in Azure"></a>4<strong>Container-Related Services in Azure</strong></h2><p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/container-instances/">Azure Container Instances</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/kubernetes-service/">Azure Kubernetes Service</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/container-registry/">Azure Container Registry</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/service-fabric/">Azure Service Fabric</a></p>
<p><a target="_blank" rel="noopener" href="https://azure.microsoft.com/en-us/services/app-service/">Azure App Service</a></p>
<h2 id="5Anatomy-of-a-Dockerfile"><a href="#5Anatomy-of-a-Dockerfile" class="headerlink" title="5Anatomy of a Dockerfile"></a>5<strong>Anatomy of a Dockerfile</strong></h2><p><a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">Dockerfile reference</a></p>
<h2 id="7Best-Practices-for-Multi-Stage-Builds"><a href="#7Best-Practices-for-Multi-Stage-Builds" class="headerlink" title="7Best Practices for Multi-Stage Builds"></a>7<strong>Best Practices for Multi-Stage Builds</strong></h2><p><a target="_blank" rel="noopener" href="https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/">Dockerfile best practices</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/132/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/132/">132</a><span class="page-number current">133</span><a class="page-number" href="/page/134/">134</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/134/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
