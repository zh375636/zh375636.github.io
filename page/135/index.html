<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/135/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/135/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:19" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:19-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:02" itemprop="dateModified" datetime="2022-11-19T22:53:02-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Enforcing-Compliance-Security-Controls-with-Amazon-Macie-23/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Lecture-Transcript"><a href="#Lecture-Transcript" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello, and welcome to this course, which has been designed to give you an overview in the introduction to the Amazon Macie service. I will explain what the service is and does, and how you can use it within your own environment to enhance your security level, especially when it comes to identifying the potential exposure of sensitive data, such as PII, or secret keys stored on Amazon S3. This is critical when it comes to maintaining your specific compliance programs, such as GDPR. </p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy, specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the details shown on screen, alternatively, you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#x70;&#112;&#x6f;&#x72;&#x74;&#64;&#99;&#x6c;&#x6f;&#117;&#100;&#97;&#x63;&#x61;&#100;&#x65;&#109;&#121;&#x2e;&#x63;&#111;&#109;">&#x73;&#117;&#x70;&#112;&#x6f;&#x72;&#x74;&#64;&#99;&#x6c;&#x6f;&#117;&#100;&#97;&#x63;&#x61;&#100;&#x65;&#109;&#121;&#x2e;&#x63;&#111;&#109;</a>, where one of our Cloud experts will reply to your question. </p>
<p>The content of this course is centered around security and compliance. As a result, this course is beneficial to those who are in the roles of, or similar to, Cloud security architects, compliance managers, Cloud administrators, and Cloud support and operation engineers. </p>
<p>This course is made up of the following lectures to explain the service and how it operates:</p>
<ul>
<li>What is Amazon Macie? Within this lecture you will understand exactly what the service is and the benefits that it provides. </li>
<li>Enabling and associating Macie with S3. There are specific requirements that must be configured before you enable this service. This lecture looks at those requirements and how to fulfill them. In addition to this, I’ll also show you how to associate your Amazon S3 buckets with Amazon Macie. </li>
<li>Alerts. In this lecture, I focus on the different types of alerts that Amazon Macie generates to allow you to resolve and rectify any issues identified. </li>
<li>Dashboard. Here I look at the different metrics that are available to help you understand the data that Amazon Macie has captured, monitored, and identified. </li>
<li>Users. This lecture looks at how Amazon Macie categorizes users and how to gain statistics on individuals for further analysis. </li>
<li>Research. In this lecture explain how you can perform deeper analysis of the data recorded by Macie using queries. </li>
<li>Classifying and protecting data. A key component of Amazon Macie is the classification of your data and this lecture looks at how that process works. </li>
<li>Multiple AWS accounts with Amazon Macie. This lecture is a demonstration on how to configure one AWS account as a master, and one as a member account with an Amazon Macie to consolidate and manage your results centrally. </li>
<li>And finally, the course summary. This lecture will highlight the key points taken from each of the previous lectures.</li>
</ul>
<p>The objectives of this course are, to provide an understanding and awareness of what Amazon Macie is, and what it’s used for. Also, to provide an explanation of each configured component of the service, to allow you to gain maximum benefit from Macie’s capabilities. You’ll understand how the service can provide a customizable approach to maintaining compliance. You’ll also understand how through automation and machine learning, Amazon Macie detects and categorizes S3 content to detect potential security threats and exposures. </p>
<p>As a prerequisite of this course, you should have an understanding and awareness of the following, Amazon S3, and AWS CloudTrail, including how to setup a trail in CloudTrail. </p>
<p>Throughout this course, I will reference a number of different URL links, which will help and direct you to related information on specific topics. To make these links easily accessible to you, I have included them at the top of the transcript, within the lecture that they are referenced. </p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could contact <a href="mailto:&#x73;&#117;&#112;&#112;&#111;&#x72;&#x74;&#64;&#x63;&#x6c;&#111;&#117;&#x64;&#x61;&#x63;&#x61;&#100;&#101;&#x6d;&#121;&#46;&#x63;&#111;&#109;">&#x73;&#117;&#112;&#112;&#111;&#x72;&#x74;&#64;&#x63;&#x6c;&#111;&#117;&#x64;&#x61;&#x63;&#x61;&#100;&#101;&#x6d;&#121;&#46;&#x63;&#111;&#109;</a>. </p>
<p>That brings me to the end of this lecture. Coming up next, I start off by answering the question, what is Amazon Macie?</p>
<h1 id="What-is-Amazon-Macie"><a href="#What-is-Amazon-Macie" class="headerlink" title="What is Amazon Macie?"></a>What is Amazon Macie?</h1><h2 id="Resources-Referenced"><a href="#Resources-Referenced" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/webinars/establishing-privacy-program-gdpr-compliance-beyond-62/">GDPR Compliance Webinar</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">AWS Regional Product Service Table</a></p>
<h2 id="Lecture-Transcript-1"><a href="#Lecture-Transcript-1" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture, which will answer the question, what is Amazon Macie? </p>
<p>Amazon Macie was introduced in August of 2017 as a powerful security and compliance enabling service which sits within the security identity and compliance category of the AWS management consult. The main function of the service is to provide an automatic method of detecting, identifying, and also classifying data that you are storing within your AWS account. Macie currently supports Amazon S3 storage. However, additional support for other storage systems will be developed and added over time. The service is backed by machine learning, allowing your data to be actively reviewed as different actions are taken within your AWS account. Machine learning can spot access patterns and user behavior by analyzing cloud trail event data to alert against any unusual or irregular activity. Any findings made by Amazon Macie are presented within a dashboard which can trigger alerts, allowing you to quickly resolve any potential threat of exposure or compromise of your data. </p>
<p>There are a number of key features that are offered by Amazon Macie during its detection and classification process. These can be summarized as follows. Amazon Macie will automatically and continuously monitor and detect new data that is stored in Amazon S3. Using the abilities of machine learning and artificial intelligence, this service has the ability to familiarize over time, access patterns to data. Amazon Macie also uses natural language processing methods to help classify and interpret different data types and content. NLP uses principles from computer science and computational linguistics to look at the interactions between computers and the human language. In particular, how to program computers to understand and decipher language data. The service can automatically assign business values to data that is assessed in the form of a risk score. This enables Amazon Macie to order findings on a priority basis, enabling you to focus on the most critical alerts first. In addition to this, Amazon Macie also has the added benefit of being able to monitor and discover security changes governing your data. As well as identify specific security-centric data such as access keys held within an S3 bucket. </p>
<p>This protective and proactive security monitoring enables Amazon Macie to identify critical, sensitive, and security focused data such as API keys, secret keys, in addition to PII and PHI data. It can detect changes and alterations to existing security policies and access control lists which effect data within your S3 buckets. It will also alert against unusual user behavior and maintain compliance requirements as required. </p>
<p>Over the past few months, you will have likely heard about numerous occurrences whereby huge quantities of PII data being stored in the cloud, have been exposed unnecessarily. Many of these instances can be attributed to a lack of understanding of key security controls offered by Amazon S3 by those storing data within the service, in addition to simple human error. Also, the sensitivity of the data may not of been understood before being stored. Understanding your data and its business value is essential. Therefore, having a managed service which provides in essence, a double-check, against your sensitive business data is invaluable. </p>
<p>For example, checking to ensure you are not allowing sensitive data to be accessible via the internet, which will almost certainly have adverse negative effects. There are a wide variety of compliance programs that need to be adhered to and ensuring you maintain your compliance is crucial to your business. For example, from a general data protection regulation, GDPR perspective, you are required to keep any personal information of EU citizens protected and secured at all times with adequate protection. If you inadvertently expose data of EU citizens, you could be faced with significant financial penalties, which can total 4% of your annual global turnover or up to 20 million euros, whichever is greater. So maintaining compliance and having the available tools and services to help you enable this, is fundamental for businesses storing data in the cloud. </p>
<p>If you would like to learn more about GDPR, you can listen to our existing webinar <a target="_blank" rel="noopener" href="https://cloudacademy.com/webinars/establishing-privacy-program-gdpr-compliance-beyond-62/">here</a>, entitled Establishing a Privacy Program GDPR Compliance and Beyond. Currently, Amazon Macie is not available in all regions of AWS, so I recommend you check the AWS regional product service table found <a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">here</a> before relying on a service to work with your S3 data, which is also a regional service.</p>
<h1 id="Enabling-and-Associating-Amazon-Macie-with-Amazon-S3"><a href="#Enabling-and-Associating-Amazon-Macie-with-Amazon-S3" class="headerlink" title="Enabling and Associating Amazon Macie with Amazon S3"></a>Enabling and Associating Amazon Macie with Amazon S3</h1><h2 id="Resources-Referenced-1"><a href="#Resources-Referenced-1" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=MacieServiceRolesMaster&templateURL=https://s3.amazonaws.com/us-east-1.macie-redirection/cfntemplates/MacieServiceRolesMaster.template">US East (Virginia) CloudFormation Template</a></p>
<p><a target="_blank" rel="noopener" href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=MacieServiceRolesMaster&templateURL=https://s3-us-west-2.amazonaws.com/us-west-2.macie-redirection/cfntemplates/MacieServiceRolesMaster.template">US West (Oregon) CloudFormation Template</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-setting-up.html#macie-setting-up-enable">CloudFormation Templates for all regions available</a></p>
<p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-automation-how-to-use-cloudformation/">How to use Cloudformation for Automation</a></p>
<p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-aws-cloudformation/">Advanced use of AWS CloudFormation</a></p>
<p>Lab: <a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/deploy-wordpress-cloudformation-17/">Deploy Wordpress using CloudFormation</a></p>
<p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/">AWS Cloudtrail: An Introduction</a></p>
<p>Lab: <a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/monitoring-aws-cloudtrail-events-amazon-cloudwatch-76/">Monitoring AWS CloudTrail Events with Amazon CloudWatch</a></p>
<h2 id="Lecture-Transcript-2"><a href="#Lecture-Transcript-2" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture. Now we have an understanding of what Amazon Macie is, let me now explain how to enable it so you can associate it to your Amazon S3 data to understand any potential security issues. </p>
<p>As I mentioned previously, the services located under the security identity and compliance category within the AWS Management Console. When you first go into the service for the first time you’ll be presented with a splash screen similar to the following. From here you simply click on get started. Now at this point Amazon Macie will check your AWS account for specific requirements that are needed before you can go ahead and fully enable Amazon Macie. These requirements are to check the existence of IAM roles, specifically the AWSMacieServiceCustomerSetupRole and to check if AWS CloudTrail is enabled within your AWS account. Both of these are prerequisites to being able to enable Amazon Macie. If these components are not configured within your AWS account you will see the screen like this. As you can see, there are two red check marks against each of these requirements. </p>
<p>To resolve the first issue of the IAM roles, you will need to launch a preconfigured AWS CloudFormation Stack that has been created by AWS that will automatically set up and configure the roles that are needed. Dependent on your region that you’re using will depend on the CloudFormation Stack used. Currently there are templates for US East Virginia and US West Oregon. The URL links to these stacks can be found in the transcript of this lecture along with the URL source of these templates, allowing you to find more regional stacks as and when they are released. </p>
<p>Before I continue I just want to quickly mention that if you’d like additional information on AWS CloudFormation, then you can view our existing content here. We have a couple of courses, how to use CloudFormation for AWS Automation and Advanced Use of AWS CloudFormation, and we also have a lab Deploying Wordpress using AWS CloudFormation. </p>
<p>Okay, back to our CloudFormation Stacks. Regardless of which one you use dependent on your region, the process is very simple. So don’t worry if you’re not familiar with CloudFormation. When you click on one of the links for the stacks it will open up CloudFormation within your AWS account, providing you are logged into your account already. The data and information will already be prefilled and all you need to do is to accept the defaults. On the select template page you will see that it has already preconfigured the relevant template under the specify an Amazon S3 template URL heading. To proceed to the next screen press next. At the specified details screen leave the default stack name of MacieServiceRolesMaster and click next. On the options screen leave all settings as default and click next. On the final review screen you will need to acknowledge the message that CloudFormation might create IAM resources with custom names via checkbox. Once you have done so click create. At this point the CloudFormation Stack will be created and will generate the required resources defined by the stack, which includes the necessary IAM roles and policies that Amazon Macie requires. </p>
<p>The next requirement needed by Amazon Macie is the enablement of AWS CloudTrail. If you are not familiar with CloudTrail then you can see our existing content of the service here. We have a course, AWS CloudTrail: An Introduction and a lab, Monitoring AWS CloudTrail events with Amazon CloudWatch. The following demonstration will explain and show how to create and enable a new trail within CloudTrail to fulfill this second requirement. </p>
<p>Okay, so I’m logged into my AWS account and CloudTrail is under management tools here. So if we just select CloudTrail, and that will take us to the dashboard. And now from the dashboard on the left-hand side we can then create a trail here, which is what we need to do. So let’s create a new trail. We’ll call this trail name Macie-demo, and we can apply this trail to all regions. And just leave that as a default yes. Under management events, we want to be notified of all read&#x2F;write events. </p>
<p>If we scroll down to data events here you can see S3 and lambda. We’re only interested in S3 so let’s select all S3 buckets in your account. And by default it selected all read and write actions. If we wanted just to do specific buckets then we could add the bucket here, but we’ve said we will select all three S3 buckets in our account. Under storage location this is where it’ll store the CloudTrail logs, and we can create a new S3 bucket for this. So let’s give this a name of CloudTrail logs. And if we go down to advanced, here we can add a log file prefix if we wanted to, but there’s no need to for this demonstration. We can crypt our log files if we wanted to, and we can select a KMS key. For this demonstration I’m just going to leave that as a default of no. </p>
<p>Log file validation determines if the log file has been tampered with, so you can either have that on or off, and also here you can select to have an SNS notification for every long file delivery. And again, I’m going to set the default for no. So once that’s been selected you can click on create. As I should have expected, this bucket order exists. So I’m going to add cloud academy to the end of that because it needs to be a unique bucket name, as we know. That will then go ahead and create the trail. And here we have the new trail and the status is running. And that’s it. Now that we have both requirements completed for Amazon Macie we will now be able to enable the service. As you can see I now have both requirements fulfilled indicated by tick. There is one final element to address and that is the permissions checkbox that you can see at the bottom of the page. This simply asks you to acknowledge that through the enablement of Amazon Macie you are happy that the service will have permission to analyze you AWS CloudTrail logs and events. It’s worth noting that you can always disable Amazon Macie at any point, and this will in turn stop the monitoring and analysis of your logs. When you select the tick box you will then be able to enable Macie. </p>
<p>Once Macie is enabled you will be taken to the console of the service, which will look something like this. From here our next step is to associate our Amazon S3 buckets that we want Amazon Macie to monitor. The best way to show you how to do this is via a quick demonstration. </p>
<p>Okay, so I’m back in my AWS account, and I’ve just opened up Amazon Macie. From here what I need to do is on the left-hand side go down to integrations. And then you can see at the top here accounts and services. What I need to select is services. Now I need to select the account I want to integrate other services with Amazon Macie, and this is my AWS account. And at the moment the only viable option is Amazon S3. Overtime there will be more and more storage services added to this section. So all I need to do is click on add. And now here it’s asking me which S3 buckets I want Macie to monitor. So I’ve created a bucket down here called macie-demo-cloudacademy. So I just want Amazon Macie to monitor this bucket here. And I also want Macie to classify all the data within this bucket. And by classifying all the content within that bucket, as I add more and more content to this bucket then Amazon S3 will automatically detect and classify all that data within it to notify me of any potential issues. Once I’ve selected that I then click on review and save. </p>
<p>There’s a couple of tick boxes that you need to select here, the first one to say that you understand that S3 object-level logging is enabled for all buckets and that you understand that choosing to classify all objects in the selected S3 bucket can significantly affect the content classification costs. And then you click on save. And that’s it. Now your bucket will be classified by Amazon Macie, and it will be monitored as well. So it’s a very simple quick and easy process. We now have our Amazon Macie account enabled and our Amazon S3 bucket’s being monitored, which brings me to the end of this lecture. </p>
<p>Over the next few lectures I’ll be explaining the different elements of Amazon Macie console to help us understand how we configure and utilize it effectively as a compliance and security tool, starting with alerts.</p>
<h1 id="Alerts"><a href="#Alerts" class="headerlink" title="Alerts"></a>Alerts</h1><h2 id="Resources-Referenced-2"><a href="#Resources-Referenced-2" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-research.html#macie-query">Constructing Queries in Amazon Macie</a></p>
<h2 id="Lecture-Transcript-3"><a href="#Lecture-Transcript-3" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture on Amazon Macie alerts. As Amazon Macie is a security analysis and compliance tool, we can expect it to identify and notify us of any potential issues that it finds, and this action is performed by Macie Alerts. By default, the service is pre-configured with a wide range of alerts based on security best practices and the sensitivity of data that the service will check against. Depending on your organization and the industry that you’re in, it can affect what you deem as sensitive information, and so there may not be an alert which fulfills your requirements. Thankfully, Amazon Macie provides the functionality of being able to create custom alerts which you can configure which Macie will then use against your data. This is very useful when it comes to maintaining and control and compliance of your data sets. </p>
<p>Alerts exist as two different types, these being basic and predictive. </p>
<p>Basic alerts. The basic alert consist of both pre-built alerts that come with Amazon Macie and custom alerts. These customized alerts are designed and built by you, which relate to specific security checks. The pre-defined alerts and checks defined by Macie have been categorized as follows. </p>
<ul>
<li>Anonymized access. This is where users are trying to gain access to your AWS account and its resources while trying to mask and hide their own identity. </li>
<li>Config compliance. This focuses on settings and policies that can relate to compliance issues within your environment such as a change to a CloudTrail login policy. </li>
<li>Credential loss covers scenarios where access control data may have been compromised. </li>
<li>Data compliance. This checks your data for identification of security content, such as access keys, credential data, or PII and PHI information. </li>
<li>File hosting. This check relates to compromised instances where malware may be detected. </li>
<li>Identity enumeration. This check helps to detect a potential attack or weakness in access control and credentials by identifying a rise in API calls or access attempts across your account. </li>
<li>Information loss. This looks at unusual behavior and irregular activity of access to information classed as sensitive. </li>
<li>Location anomaly. Checks for access requests where the source of the request is from an unexpected and unusual location from outside the normal operations based on history. </li>
<li>Open permissions. This check is very useful as it checks your permissions to sensitive data to see if they are overly permissive and potentially allow for unnecessary data exposure. </li>
<li>Privilege escalation. These checks focus on all access attempts, which will result in a privileged level of access control which could potentially cause further damage and harm to your environment. </li>
<li>Ransomware. This focuses on compromised infrastructure where ransomware has been detected. </li>
<li>Service disruption. This checks if a disruption to your environment is likely when making your own internal configuration alterations to your infrastructure. For example, the alteration of a security group that will result in the denial of access of previously used communications. </li>
<li>Suspicious access. Checks for unusual and risky access being requested by anonymous users where source data is being masked, such as their IP address, etc. For example, a malicious user masking their connection across a number of compromised hosts.</li>
</ul>
<p>It’s worth noting that these alerts provided by Amazon Macie cannot be altered or modified in any way. </p>
<p>Predictive alerts. Predictive alerts look at the behavior of your AWS account to automatically identify activities that sit outside the realms of normal operations. Over time, Macie forms a baseline understanding of day-to-day operations and learns what is normal and what is unusual behavior through analysis of your CloudTrail logs and events using machine learning. For example, this could be a sudden spike in user access of a particular S3 bucket that may normally only be accessed very rarely. When this happens, Amazon Macie will send a predictive alert informing you of the anomaly. The same principle applies to predictive alerts as with basic in that they can’t be edited or changed in any way. </p>
<p>Let me now explain the different points of interest on the alert itself so you know what to expect and what information comes with an alert as and when you receive one. This image shows an alert within my Amazon Macie console. This is the view of an alert that appears in the alerts section of the console for Macie, and it gives a high-level overview of information relating to that event that triggered the alert. The top half displays the severity, which is set to low, the name of the alert, which is Change to Cloudtrail logging policy, the alert type and category, these being basic and config compliance, and the bottom half gives some additional detail, such as when the alert was triggered, the identity that triggered the alert, and in which region, along with the number of results captured in the alert and how many times it’s been viewed. </p>
<p>To drill down into this further, you can click on the alert and it will display a detailed finding. This view is broken down into two parts, the alert summary and the alert details. The summary provides additional information allowing you to respond to the alert appropriately with the findings given. The description of the alert provides a deeper level of understanding of why the alert has been generated. A breakdown of results is also displayed, and as this relates to a CloudTrail action rather than an S3 action, it displays the API calls related and captured in the event. If it was related to S3 data, then it would list the S3 buckets and objects affected by the alert. The alert details section allows you to gain even more information. By clicking on the type icon, a whole host of additional information is displayed. I won’t go through all the data, but as an example, the following are just some elements of data captured for this alert. As you can see, you can obtain a substantial amount of information and useful data from these alerts to ascertain if there is a viable threat. If you analyze the alert and identify that this is in fact a legitimate security incident, then you can make the necessary changes to your infrastructure to ensure this doesn’t happen again. For example, restrict the user permissions or revoke the user entirely. However, you may decide that this activity is a normal operation and is expected of this user. In this situation, you have the ability to whitelist the user for this alert. By doing so, Amazon Macie will no longer register an alert for Change to Cloudtrail logging policy for this particular user. As you can see, you can also archive the alert for future reference or edit the alert. As this alert was predefined by Amazon Macie, you are unable to edit it. If it was a custom alert, then you could edit it as required. </p>
<p>Before I move on to show you how to create a custom alert, I just want to explain the different level of severity associated with these alerts and what these severities generally mean. There are five different severities for an alert, these being:</p>
<ul>
<li>Informational. If you receive this alert, it doesn’t indicate a threat or any risk to your current operations. These alerts are used to provide information to allow you to make informed decisions and changes to your infrastructure if you deem it necessary. </li>
<li>Low. This is the lowest level of a potential security threat that could compromise your data from a confidentiality, integrity, and availability perspective. Action isn’t required immediately, but it should be investigated and resolved in future changes and fixes. </li>
<li>Medium. This indicates the next level up for a threat that could impact the CIA of your data, and action should be taken against this prior to any low severities. </li>
<li>High. If an alert is set to high, this requires immediate action and attention, as there is a very high chance that your data can be compromised from a CIA perspective. </li>
<li>Critical. This is very similar to high. However, the main difference is that it is likely that your data and services have already been compromised from a CIA standpoint.</li>
</ul>
<p>Let me now move on to show you how you can add your own customized alert via a quick demonstration. In this demonstration, I will show you how to create a custom basic alert using a very simple query. </p>
<p>Okay, so I’m on the dashboard of my Amazon Macie account, and what I need to do to create my own alert is go down to Settings and then scroll down and select Basic alerts at the bottom. Now, this shows all the basic alerts that have been created, and as you can see, many of them have been created by Macie. But if I want to add my own alert, I can click on the green button here, click on Add New, and it’ll ask me for a number of different types of information. So we can give it an alert title. I’ll just call it This is a test alert. Same for a description. </p>
<p>Next I’ll need to select a category, and these are all the different categories for basic alerts. For this one, I’m going to select Anonymized Access. Now, this next section is the query. Now, this is where you enter the query information that defines what the alert is, the information that it’s searching for within the data that’s been collected by Macie through CloudTrail logs and also through S3 data. How to construct these queries is outside of the scope of this course. However, you can find more information on these queries in the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-research.html#macie-query">link</a> on the screen. </p>
<p>So I’m going to add in my query here, and what this will do, this will search for every time the GetBucketPolicy API is called by the user identity type of IAMUser. So this alert will not be triggered if an AWS service called the GetBucketPolicy, for example. It’ll only be alerted if an IAMUser requested the GetBucketPolicy. Next we have the index, and this is the source data that it’ll search upon, either CloudTrail data, S3 bucket properties, and S3 objects. For this particular query, I want it to search through the CloudTrail data. Then you can have the minimum number of matches. For this demonstration, I’m going to have it as one. For the severity, we can have informational, low, medium, high, or critical. I’m just going to leave this as informational. And then at the bottom here, we have enabled Yes - active. And all I need to do at this point is then click on Save. </p>
<p>And that’s now added our alert to this list of basic alerts, and we can filter and order these alerts. So if I click on the Created by, we can see here our alert, This is a test alert, and it’s a custom alert. Now, if we want to test that just to make sure it’s picking up data, we can go across to the magnifying class and click on Research current query. This takes us to the Research section, and as you can see, currently there’s a total of 13 matched results based on that query. But I’ll be covering more on this research feature in another lecture in later in this course. </p>
<p>So if you go back and find our alert again, then what we can do, as well, we can delete this alert or edit it, and if we want to edit it, we can just change the details here. And that’s how you create your own basic alert.</p>
<h1 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h1><h2 id="Lecture-Transcript-4"><a href="#Lecture-Transcript-4" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture where I am going to be looking at the Amazon Macie dashboard. The Amazon Macie dashboard is the central hub of information that is collated, monitored, and classified through Amazon CloudTrail logs and any services associated to Macie, such as Amazon S3. The dashboard is accessed via the Amazon Macie console, which will look something like this. </p>
<p>Let’s start by looking at the four metric boxes at the top of the page and what they mean, starting with critical assets. This metric defines as a percentage how many of your assets have been identified as high-risk, which is anything with a risk value of eight, nine, or 10. These values are assigned to your assets in two ways, either by data classification made by Macie monitoring S3 data, or from specific API calls detected in your CloudTrail logs that have been marked with a set risk value. If the risk value has been classified based on S3 data, then the following categories are used to define its risk. The content type. Examples of this are plaintext, document, or source code. File extensions. Examples are .bat, .dmg, .sql. Themes. Examples of themes include financial keywords and social security keywords. And finally, regex configurations, which are regular expressions used to search for data patterns. More on these classification types and the methods behind it will be covered in the classifying and protecting data lecture later in this course. If the risk relates to an API call relating to your AWS infrastructure in some way, then the CloudTrail events in CloudTrail errors risk management will determine their value. Again, more on this will be discussed in that same lecture. </p>
<p>Next is the total event occurrences metric. This relates to your Amazon CloudTrail logs and calculates the number of API calls that Amazon Macie has monitored as a part of the security analysis of your infrastructure. The total user sessions metric is a count of user sessions which Macie has processed. A user session is defined by a five-minute aggregate of CloudTrail data. This metric provides its count from when Amazon Macie was first enabled in your AWS account. Finally, the total users metric, which shows the number of users that have been identified by CloudTrail data, which are then are then categorized into Platinum, Gold, Silver, or Bronze, depending on which API calls those users have been requesting and initiating, will relate to their perceived risk level, Platinum being high-risk, Bronze being low-risk. More on Amazon Macie users will be covered in the next lecture.</p>
<p>Let me now move on to the bottom half of the dashboard. So the bottom half of the dashboard screen is used to present a number of different views of graphs, charts, and statistics that Amazon Macie has detected and monitored through its sources. They are accessed via the following icons, and these icons represent the following. S3 objects for selected time range, S3 objects, S3 objects by PII, S3 objects by ACL, high-risk CloudTrail events and associated users, high-risk CloudTrail errors and associated users, activity location, CloudTrail events, activity ISPs, and CloudTrail user identity types. Let’s have a quick look at what each of these filter against starting with:</p>
<p> S3 objects for selected time range. This metric includes a slider bar which defines the objects that are visualized in the graph. The value of the slider goes between one to 10 and represents the minimum risk value of the object to be included. For example, if the slider is set to five, all objects represented in the graph will have a risk value of five or greater. The graph also shows when the object was last modified using the time ranges of zero to six months ago and beyond six months from the date Macie was enabled. </p>
<p>S3 objects. This metric shows your monitored S3 objects grouped together by Amazon Macie themes. The complete list of themes can be found under Settings &gt; Themes from within the Macie console. For each theme identified in this view, a percentage will be shown which indicates how much of your total objects are categorized to that particular theme. In addition to this, it’ll also provide a count number of the objects within that theme. More on themes will be discussed in the later lecture Classifying and Protecting Data. </p>
<p>S3 objects by PII, personally identifiable information. The third metric relating to S3 is split into two sections, S3 objects by PII priority and S3 objects by PII types. S3 objects by PII priority displays the objects that have been classified by Amazon Macie as having PII-related data associated with them, such as names, email addresses, credit card numbers, et cetera. These objects are then split out into different priority levels ranging between none, low, moderate, and high, depending on the quantity of PII data detected. Again, each of these priority classifications also have a percentage showing how much of your total objects are categorized within that PII priority value as well as a total count of objects in that priority. S3 objects by PII types metric displays the PII objects by their type classification. For example, here you can see the type of IPV4 and name where Amazon Macie has detected this data within my S3 objects. Again, the percentage and count values also exist per PII type. </p>
<p>S3 objects by ACL, access control list. The final metric relating to S3 displays three graphs relating to your access control list. The first graph is S3 objects by ACL URIs, uniform resource identifiers, which is used to define the object’s location. This shows how many URIs appear in your S3 access control list that your objects are associated to. The usual percentage and count statistics are also applied. Next we have S3 objects by ACL display names, which simply shows the different ACL display names that are associated to your objects along with the percentage of objects relating to each ACL name and its count of objects. Lastly, S3 objects by ACL permissions. This metric looks at the different level of access control list permissions, such as full control, read, write, et cetera. In this example, we can see that within the current set of ACLs used and associated to objects, the only permission given is full control to all objects. </p>
<p>High-risk CloudTrail events and associated users. This is the first of the metrics that relate to AWS CloudTrail. Much like the first S3 metric discussed earlier, this also provides a slider representing risk values from one to 10, which will affect the results displayed in two different charts. The first chart relates to the top 20 high-risk CloudTrail events detected in the last 60 days. These are events that have been captured through analysis of CloudTrail logs where Amazon Macie has classified specific API calls at a certain risk level. These APIs are then visualized on the bar chart displayed. The chart has date depicted along the x-axis and the API count on the y-axis. There is also a key provided for API identification, and you can also view this data as either a daily count or weekly. The second chart relates to the different users detected within the CloudTrail logs, and they’re presented in the same format as the previous graph, again showing the count, date, and key for easy identification of users. </p>
<p>High-risk CloudTrail errors and associated users. This focuses on any AWS CloudTrail errors detected, and those are errors resulting from API actions detected in the CloudTrail logs. Similarly to the previous metric, two charts are used again. The first relates to the top 20 high-risk CloudTrail errors detected in the last 60 days, and these errors are then visualized on the bar chart display. The second chart, again represented in the same way, displays the CloudTrail users who made the API calls that resulted in these errors. </p>
<p>Activity location. This represents a global map showing the locations of activity of actions that Amazon Macie is monitoring and analyzing. The map is interactive, allowing you to zoom in and out and also change the date range from the past 15, 30, or 90 days or the past year. </p>
<p>CloudTrail events. This metric identifies all of the CloudTrail events that Amazon Macie is monitoring and capturing. As with the other metrics, there is a count, and this count represents the number of user sessions for the API that was recorded. The percentage represents the occurrence of that API in the total list of events. </p>
<p>Activity ISPs. This metric simply records the ISPs that have been used when actions have been monitored in CloudTrail. </p>
<p>CloudTrail user identity types. This final metric indicates which user type has been used when calling APIs based on your CloudTrail logs. As you can see from this image, the majority of events recorded have been made by an account that belongs to a service indicated by the AWS Service user type. </p>
<p>That has now brought me to the end of this lecture covering the AWS dashboard. Coming up next, I will be looking at Amazon Macie users.</p>
<h1 id="Users"><a href="#Users" class="headerlink" title="Users"></a>Users</h1><h2 id="Lecture-Transcript-5"><a href="#Lecture-Transcript-5" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture covering users. I want to define how Amazon Macie classifies users and the different user types available that I briefly mentioned in the previous lecture when looking at the CloudTrail User identity types within the dashboard. Let me first start out by talking about the users section within the console. You may recognize the platinum, gold, silver, and bronze categories which are always present on the dashboard. But this users screen provides additional information surrounding this metric. This screen essentially shows the different users that have been identified via CloudTrail data when linking to API calls. Depending on the history of those users will dictate how Amazon Macie classifies them between these four category ratings which ultimately reflect the level of perceived risk that those users pose. Let me define the differences between these categories a bit further. </p>
<p>Platinum. A user or role placed within this category is considered to make regular high risk API calls such as DeleteFlowLogs or UpdateTrail. These can often be contributed to users with administrative privileges, such as the root account. Due to the amount of control and elevated permissions these users pose, you should monitor their activity for any signs of compromise. </p>
<p>Gold. Users or roles categorized as gold will have been identified through their use of calling and making API requests relating to infrastructure changes, such as CreateRouteTable or RequestSpotinstances. Again, due to the permission level of these power users, they should also be monitored to ensure that they are not compromised in any way. </p>
<p>Silver. As we progress down the track, users or roles in the silver category are identified as performing medium level risk API calls. Remember, these risk levels are managed by Amazon Macie and automatically assigned these risk levels to the API calls. As an example, a medium level risk API could be DescribeRouteTables or ListObjects. </p>
<p>Bronze. Users or roles in this category provide the lowest level of risk due to history of their API call usage, which may include API calls such as DescribeSubnets and DescribeHosts. </p>
<p>These categories give you a very quick visualization as to how many users are operating with potentially over-elevated permissions. This allows you to drill down into these users further to ascertain exactly what tasks they are performing. On the main page of the user screen, you’ll see a list of users that Amazon Macie has identified. These users are categorized under the DLP, data loss prevention value column. It also provides a high level overview of the most recent activity, total events, errors, and unique IP addresses that have been attributed to that particular user. However, should you wish to investigate a particular user and gain further insight into events surrounding them, then you can click on their name within the user column. This will then provide you with a condensed version of the dashboard discussed in the previous lecture, with metrics and data that only relate to that user in question.</p>
<p>In this example, you may recognize the dashboard icons at the top where we have views relating to high-risk CloudTrail events, high-risk CloudTrail errors, activity location, CloudTrail events, activity ISPs, and CloudTrail user identity types. Again, these metrics in each view will only relate to events and data associated with that specific user. The last point I want to discuss in this lecture is how Amazon Macie defines user identity types, which is used in the main dashboard and the condensed dashboard for each specific user. </p>
<p>Amazon Macie identifies user identity types from the CloudTrail logs that it monitors and analyzes. Specifically using the user identity element from the events in the logs. The different user types are defined as follows. </p>
<ul>
<li>Root. This means that the request was initiated by the root account of your AWS account. </li>
<li>IAM user. Here the request was made by an IAM user. </li>
<li>Assumed role. This identity type defines that the request was made by credentials that were temporarily assumed having been obtained by calling the assumed role API for the security token service. </li>
<li>Federated user. Here the request, again, was made with temporary credentials but this time from calling the AWS STS GetFederationToken API. </li>
<li>AWS account. This simply defines that the request of the API was made by a different AWS account. </li>
<li>AWS service. Here the request was made by an AWS service rather than a particular user.</li>
</ul>
<p>That now brings me to the end of this lecture. Coming up next I will be discussing the research functionality of Amazon Macie.</p>
<h1 id="Research"><a href="#Research" class="headerlink" title="Research"></a>Research</h1><h2 id="Resources-Referenced-3"><a href="#Resources-Referenced-3" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://lucene.apache.org/core/2_9_4/queryparsersyntax.html">Apache Lucene Query Parser Syntax</a></p>
<h2 id="Lecture-Transcript-6"><a href="#Lecture-Transcript-6" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello, and welcome to this lecture what looks at the research feature offered by Amazon Macie. This is a very useful function providing having an awareness and understanding of how to create queries using Apache Lucerne. How to construct queries is outside the limitations of this course. However, for further information you can visit the Apache Lucerne Query Parser syntax page. </p>
<p>This research function allows you to create your own queries against all of the data Amazon Macie has collected and monitored for AWS CloudTrail and Amazon S3. By doing so it enhances the flexibility of the service by providing a way of enabling deep dive analysis of your data that relates to your specific requirements within your business. Using the query parser you can build and construct your own queries to return the exact results that you need. </p>
<p>Underneath the query parser are a number of options that provide additional filters for your results. The first option allows you to limit the data source that Amazon Macie uses to perform your query. The options included here are CloudTrail data, S3 bucket properties and S3 objects. There are also two other filters that allow you to restrict the number of results found along with the date range filter. The number of results filters here include top 10, top 50, top 100 and top 500. The date range include seven days, 30 days, 90 days, 365 days, all, and a custom time frame. Amazon research is in fact closely tied with most of the other sections we have already discussed and look at. For example, the dashboard and alerts of Amazon Macie. When looking at the information represented in your dashboard graphs and visual representations of statistical information you will find that the graphs and images are interactive. If you were to click on the data or magnifying glass, by doing so you will often be redirected to the research feature. </p>
<p>For example, if I were to select CloudTrail events in the dashboard and then select the magnifying glass next to one of these events. Let’s say, get bucket policy, I am redirected to the research feature which will automatically fill out the query parser, allowing me to investigate the data further. As you can see, there are 707 results matched which we already knew from the dashboard page with the statistics provided there. However, what we could do now is perform additional analysis by drilling down in this information further. If I only wanted to look at IAM users that perform this API call rather than all user identity types which as we know know includes the root account IM users assumed roles of federated users, et cetera, et cetera then I can add some additional commands to the query parser which only currently states event name, error code key, get bucket policy. By adding the following commands after the current content, this query will then only display results where the get bucket policy API is used by IAM users only. </p>
<p>The results of this query are then displayed as follows. This query now only shows a total of nine matched results. This was a very simple example of how you can use the query parser to get further detailed analysis of your data that’s specific to the results that you need to investigate instant security threats and compliance requirements further. As you become more and more familiar with Amazon Macie and the components and elements that are of particular interest to you you are able to save your queries as a favorite to save you having to repeatedly type them into the query parser, using the following icon. </p>
<p>You can also create your queries and have them saved as a custom alert. This ensures that whenever criteria that matches your query appears it will appear within your alert screen to allow it to quickly identify and take the appropriate action as necessary. Again, this level of customization allows you to filter and direct your search for specific elements, allowing you to achieve different levels of compliance regulation. The following icon allows you to save your query as one of these custom alerts. </p>
<p>That now brings me to the end of this lecture. Coming up next I’ll be looking at how Amazon Macie classifies and protects your monitored data.</p>
<h1 id="Classifying-amp-Protecting-Data"><a href="#Classifying-amp-Protecting-Data" class="headerlink" title="Classifying &amp; Protecting Data"></a>Classifying &amp; Protecting Data</h1><h2 id="Resources-Referenced-4"><a href="#Resources-Referenced-4" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://www.regexbuddy.com/regex.html">Regular Expressions (Regex)</a></p>
<h2 id="Lecture-Transcript-7"><a href="#Lecture-Transcript-7" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture where I’ll be explaining how Amazon Macie makes its decisions on data classification through AWS CloudTrail logs and Amazon S3 actions. </p>
<p>Data being stored on Amazon S3 within your AWS account is classified by Macie which determines its level of business sensitivity and criticality. Every data object within your Amazon S3 buckets automatically receives a perceived level of risk based on this classification process. The data values depicted within the dashboard, discussed earlier, are all driven from this classification and risk assessment. So what are these categories of classification that Amazon Macie uses? </p>
<p>There are four categories for classification, which can be found under the settings menu within the Amazon Macie console. These being content type, file extensions, themes, and regex. The classifications within these categories can not be ordered or modified in any way. Neither can you add additional entries within each of these classifications. </p>
<p>Content type. The content type classification allows Macie to detect the type of file that is being stored on S3. For example, a binary file, a document, or source code object. Amazon Macie will then embed an identifier in the header of the file for classification. If you look at the different content types available, you will notice that there is a long list of types and every entry has the following fields. Name, description, classification, risk, and enabled. The first two fields are obvious. The classification field actually specifies the content type of that type of file. For example, an Adobe Illustrator file is classified as a document and a WireShark packet capture is classified as binary. The risk is a value between 1 to 10 and defines the business risk value of that type of content. Respectively, Adobe Illustrator files have a risk value of one and the WireShark files have a risk value of six. Finally, you can choose to have the content type entry enabled or disabled. If it’s enabled and active, the value will read yes. If it’s disabled the value will read no. This setting can be changed by selecting the entry and making the change. This is the only value that you can change on the content types. </p>
<p>File extensions. The file extension classification looks at the file extension of the object to ascertain its risk value. The same field types are used with file extensions as is for content type we just discussed. </p>
<p>Themes. Themes operate differently to both content type and file extensions in the fact that they assess the object based upon a series of key words that are detected within the actual object itself. Depending on these key words and their combinations will determine the risk level assigned to the object. The field types for themes are theme title, minimum keyword combinations, risk, and enabled. Examples of these titles are ‘American Express Credit Card Keywords’ or ‘Audit Keywords’, allowing you to determine the types of words that are being assessed. You can click on any of these entries to look at the actual words that are being scanned for. In the case of ‘Audit Keywords’, these are audit, risk assessment, security, and evaluation. The minimum keyword combinations is a numerical value showing how many of these keywords must be present in the object to dictate the risk level. </p>
<p>Regex. Like themes, regex or regular expression classifies content based on the actual content within the object. These regular expressions contain a text string for describing a specific search pattern allowing Amazon Macie to look for specific data within the content to calculate its risk. If you would like to learn and understand more about regex, then you can look at the link <a target="_blank" rel="noopener" href="https://www.regexbuddy.com/regex.html">here</a>. For each object stored on S3, Amazon Macie will assign a content type, file extension, theme, and regex value before defining its final risk value. This is determined by the highest value that was detected in each of these categories. For example, you may have an Excel document containing UK Passport numbers and the risk values may be classified as follows with a content type, file extension, and theme all with a risk value of one and a regex value of five. This would give a result of five as that was the highest value obtained for that data object. </p>
<p>During Amazon Macie’s process to classify data, it also performs automatic PII classification. This uses a list of predefined metrics relating to PII which include the following and are assigned a low, moderate, or high rating which is dependent on the quantities found within the object. The PII data searched includes full names, mailing addresses, email addresses, credit card numbers, IP addresses, driver license IDs, national identification numbers, and birth dates. </p>
<p>Amazon Macie uses two methods for protecting your data using AWS CloudTrail and artificial intelligence and machine learning to assess and review historical patterns of access. Using this historical data provided by CloudTrail, Macie can detect if there is unusual behavior occurring within your account that could potentially lead to your data being compromised. These methods include the use of AWS CloudTrail events and AWS CloudTrail errors. Both of these can be accessed by the settings menu within the Macie console, along with the data classification categories. </p>
<p>CloudTrail events provides a list of CloudTrail events along with their associated risk value of the API. The fields include name, description, classification, risk, and enabled. The classification field relates to the resource that a particular API is actionable against. It’s also possible to search the events by name. As you can see in the image, I’ve searched for get, which as returned all events with get in the title. The image shows just three of the results returned. As expected, an API starting with get is likely to have a higher risk value then an API starting with list, as a get is generally a request to retrieve data which could indicate an intrusion of some kind depending on the API in question. These three get events have a rating of eight, eight, and nine, which again is marked out of 10. </p>
<p>CloudTrail errors. This looks at the different errors that are generated and reported within CloudTrail. If you perform an action within AWS and receive an error back, it is generally because you did not have permission or access to the resource, you were using the wrong credentials, or some other kind of invalid request. These are all common errors that can occur when someone is trying to access or perform a function or action against something that they shouldn’t be. So from a security awareness and assessment point of view, these are crucial. As a result, the minimum risk value to these errors is five, with some reaching a risk value of 10, the highest possible value. </p>
<p>That now brings me to the end of this lecture covering the classification and protection of data within Amazon Macie.</p>
<h1 id="Multiple-Accounts-with-Amazon-Macie"><a href="#Multiple-Accounts-with-Amazon-Macie" class="headerlink" title="Multiple Accounts with Amazon Macie"></a>Multiple Accounts with Amazon Macie</h1><h2 id="Resources-Referenced-5"><a href="#Resources-Referenced-5" class="headerlink" title="Resources Referenced"></a>Resources Referenced</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-integration.html#macie-integration-member">CloudFormation Templates</a></p>
<h2 id="Lecture-Transcript-8"><a href="#Lecture-Transcript-8" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this very quick lecture where I want to demonstrate how you can use a single AWS Master account to gather data for multiple AWS accounts that you may have, to gain a full understanding of your business risk value based on everything that I’ve discussed so far. To do this, I shall perform a quick demonstration to explain how to complete this process. </p>
<p>OK so, to carry out this demonstration, I’m going to need two AWS accounts. I’m going to need a Master account, and also a second account that’ll be used as a Member account. And what I’ll do, I’ll add the Member account to the Master account, then all the dashboard findings will be sent through to the Master account rather than having to view Macie across all your different AWS accounts. </p>
<p>So I’ve logged into one of my AWS accounts, and this is going to be my Member account. Now the first step I need to do, is run a cloud formation stack. Now this cloud formation stack is provided by AWS, and it is different from the cloud formation stack that you first use to enable Macie, because that was to enable Macie as your Master account, whereas here, we want to enable Macie as a Member account. These stacks can be found on the following webpage, and I’ll also put a link to them within the transcripts as well. </p>
<p>So I’m going to run the cloud formation template for the Virginia region. Now again it’s very simple on this select template screen. All I need to do is go across to Next. I’ll leave the stack name as MacieServiceRolesMembers, and here I need to enter the Master account. So my Master account number is as follows, once I’ve put that Master account number in, I can then click on Next. Now here I can change a number of options if I need to, but I’m just going to leave all these as the default, then click on Next. And then at the Review screen, we just need to acknowledge the fact that AWS cloud formation might create IAM resources with custom names. So that’s just a simple checkbox, and then you scroll down, and go across to Create. </p>
<p>This will then go ahead and run the cloud formation stack. Just refresh the page there. We can see here the stack name is the MacieServiceRolesMembers stack, and it’s currently in progress. OK, now that’s complete, what I’ll do, I’ll go across to my other AWS account, which will be my Master account. I then open up Amazon Macie and I’ll add this Member account into my Macie console on my Master. </p>
<p>OK, so I’m now in my Master account, so if I go across to Amazon Macie. And then on the left hand side, if I go down to Integrations. And on the Accounts page here, we can see currently that we don’t have any Member AWS accounts. So, to add the account we click on the Plus across this side, enter the Account ID. Now that’s the Account ID of the Member account where we’d just run the account formation script. Then click on Add Accounts. And then we can see that it’s now been approved. So if we click on Close, and we can check our Member AWS account section here, we can now see that, that account is now a Member account, and up here is our Master account, which is the account that I’m currently logged into. So if you have multiple AWS accounts, you’ll have one as your Master account, where you set it up and enable it using the cloud formation script that we discussed earlier in this course, and if you need to add Member accounts, then you need to use a different cloud formation script, which will configure that Macie account as a Member account. And once you have those Member accounts configured, you then go to your Master account and simply add them here. </p>
<p>That brings me to the end of this lecture. Coming up next will be a summary of all the key points taken from the previous lectures.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><h2 id="Lecture-Transcript-9"><a href="#Lecture-Transcript-9" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello, and welcome to this final lecture within this course covering Amazon Macie. Hopefully you should now have a better understanding of Amazon Macie, and how it can be used to enhance your security level within your AWS account. Specifically across your data being stored in Amazon S3. The abilities of being able to automatically classify data and its potential risk value, and being able to identify security loopholes and potential exposures with your business critical data is invaluable. Amazon Macie provides a means of ensuring you have a way of enabling compliance on different levels. For example, it can be used as a service to help you enable compliance regulations to be met by GDPR. Through the use of alerts, metrics, deep analysis, best practices and customization, you can use Amazon Macie to meet stringent compliance needs within your business. </p>
<p>I now want to review and highlight some of the key points taken from each of the lectures within this course. I started by focusing on what Amazon Macie was and in this lecture we learnt that the main function of this service is to provide an automatic method of detecting, identifying, and also classifying data that you are storing within your AWS account. It’s backed by machine learning to detect access patterns and look for unusual or irregular activity. Findings are presented within a dashboard which can trigger alerts, and Amazon Macie, automatically and continuously monitors data in S3. Natural language processing methods are used to help classify data types and content. Objects are assigned a risk value based on data classification. Amazon Macie can monitor and discover security changes governing your data, and it can detect sensitive and security focused data, such as API keys, secret access keys, in addition to PII and PHI data. It can also detect changes to security policies and access control lists, and alert against unusual user behavior . This all helps you to maintain compliance requirements as needed. </p>
<p>Following this lecture, I looked at how to enable the service and associate your S3 data. In this lecture I explained that your AWS account needs to meet two requirements before you can enable Macie. You need to check the existence of IAM roles, specifically the AWS Macie service customer setup role. And to check that AWS CloudTrail is enabled within your AWS account. Point one here can be implemented through the use of CloudFormation templates provided by AWS. And point two simply requires that you create a trail within CloudTrail. When both requirements are met, you can then enable the service. You can then associate your Amazon S3 buckets with Macie via the integrations menu in the console. During this particular lecture, I provided a demonstration on how to do this. </p>
<p>Next I focused my attention on the different types of alerts generated and that are available with Amazon Macie. By default, Macie is pre-configured with a wide range of alerts based on security best practices and the sensitivity of data that the service will check against. Macie offers the ability to create custom alerts. These alerts exist as two different types. Basic, which consist of prebuilt alerts that come with Amazon Macie, and also custom alerts. Predictive alerts look at the behavior of your AWS account to automatically identify activities that sit outside the realms of normal operations. Alerts displayed in the console show summarized details, however these details can be expanded by clicking on the alert itself. The alert summary shows information allowing you to respond to the alert appropriately with the findings given. The alert detail section offers a whole host of additional information retrieved by CloudTrail events. It’s possible to whitelist users for specific alerts that are identified. The severity of an alert can either be informational, low, medium, high, or critical. I also provided a demonstration on how to create your own alerts. </p>
<p>Following this lecture, I discussed the Amazon Macie dashboard. In this lecture, we learnt that the Amazon Macie dashboard is the central hub of information that is collated, monitored and classified through Amazon CloudTrail logs and any services associated to Macie, such as Amazon S3. The dashboard has four metric boxes at the top of the page. Critical assets, this metric defines as a percentage how many of your assets have been identified as high risk, which is anything with a risk value of eight, nine, or 10. Total event occurrences metric. This relates to your Amazon CloudTrail logs and calculates number of API calls that Amazon Macie has monitored as a part of the security analysis of your infrastructure. Total user sessions is a count of user sessions which Amazon Macie has processed. And total users shows the number of users that have been identified by CloudTrail data. The bottom of the dashboard is used to present a number of different views in graphs, charts and statistics of monitored data. These being S3 objects for the selected time range. This displays the S3 objects within a time range at a minimum risk level. S3 objects, this metric shows your monitored S3 objects grouped together by Amazon Macie themes. S3 objects by PII. This shows PII data grouped by priority and type. S3 objects by ACL. This groups S3 objects by their ACL URIs, display names, and permission levels. High-risk CloudTrail events and associated users. This metric relates to the top 20 high-risk CloudTrail events detected in the last 60 days. High-risk CloudTrail errors and associated users. This displays the errors resulting from API actions detected in the CloudTrail logs. Activity location, this represents the global map showing the locations of activity of actions that Amazon Macie is monitoring and analyzing. CloudTrail events, this identifies all CloudTrail events monitored by Macie. Activity ISPs, this records the ISPs that have been used by users. And finally, CloudTrail user identity types. This groups users detected by their identity type, such as an IAM user. </p>
<p>Once I had reviewed the Macie dashboard, I looked at the users section. In this lecture I covered the following points. Users are grouped by platinum, gold, silver and bronze. Which represents their perceived level of risk based on their history of API calls. Platinum poses the highest risk, and bronze the lowest risk. Additional data can be generated by selecting the user, which will present you with a condensed version of the dashboard displaying metrics only relating to that particular user. User identity types are defined from CloudTrail logs that it monitors and analyzes via the user identity element. User identity types include, root, which means the request was initiated by AWS root account. IAM user, the request was made my an IAM user. Assumed role, defines that the request was made by credentials that were temporarily assumed by the assumed role API. Federated user, the request again was made with temporary credentials, but using the STS GetFederation Token API. AWS account, the request was made by a different AWS account. And AWS service, the request was made by an AWS service. </p>
<p>Following this lecture, I then focused on the research feature of Amazon Macie. Here I covered the following points. The research function allows you to create your own queries against all of the data that Amazon Macie has collected and monitored via AWS CloudTrail and Amazon S3. It enables you to perform deep dive analysis of your data that relates to your specific requirements within your business using the query parser. You can filter the results based on CloudTrail data, S3 bucket properties, and S3 objects. You can also filter on the number of results found, along with a date range filter. Research is integrated with all elements of Macie, for example the dashboard and alerts. An example of an entry on the query parser is as follows. Which will only display the results with a get bucket policy API call is used by IAM users only. It’s possible to save your favorite queries within a favorites list, and you can also create your queries and have them saved as a custom alert. </p>
<p>Following this lecture, I then moved my focus on to explain how Amazon Macie classifies your data to assign its risk value. In this lecture, I explained that every data object within the Amazon S3 bucket automatically receives a perceived level of risk based on a classification process. There are four classification categories. Content type allows Macie to detect the type of file that is being stored on S3. For example, a binary file, a document, or source code object. File extensions. This looks at the file extension of the object to ascertain its risk value. Themes. This assesses the object based on a series of key words that are detected within the actual object itself. And Regex, regular expression. This classifies content based on content within the object using a text string for describing a specific search pattern. Each S3 object has a risk value for each category. The object’s final risk value is given by the highest value received between the four categories. Amazon Macie also performs automatic PII classification using a list of predefined metrics. Amazon Macie uses AI and machine learning to assess and review historical CloudTrail data access patterns using CloudTrail events and CloudTrail errors. CloudTrail events provide a list of CloudTrail events along with the associated risk value of the API. CloudTrail errors looks at the different errors that are generated and recorded within CloudTrail. </p>
<p>The final lecture was a demonstration where I showed you how to use a single AWS master account to gather data for multiple AWS accounts. </p>
<p>That now brings me to the end of this lecture, and to the end of this course. You should now be able to effectively use Amazon Macie to help you protect your data, and to meet and maintain governance and compliance regulations within your environment. </p>
<p>If you have any feedback on this course, positive or negative, please do contact us at <a href="mailto:&#115;&#x75;&#112;&#x70;&#x6f;&#x72;&#x74;&#64;&#99;&#108;&#x6f;&#x75;&#100;&#x61;&#99;&#x61;&#x64;&#101;&#x6d;&#121;&#x2e;&#x63;&#111;&#x6d;">&#115;&#x75;&#112;&#x70;&#x6f;&#x72;&#x74;&#64;&#99;&#108;&#x6f;&#x75;&#100;&#x61;&#99;&#x61;&#x64;&#101;&#x6d;&#121;&#x2e;&#x63;&#111;&#x6d;</a>. You feedback is greatly appreciated. Thank you for your time, and good luck with your continued learning of cloud computing. </p>
<p>Thank you.</p>
<h1 id="2What-is-Amazon-Macie"><a href="#2What-is-Amazon-Macie" class="headerlink" title="2What is Amazon Macie?"></a>2<strong>What is Amazon Macie?</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/webinars/establishing-privacy-program-gdpr-compliance-beyond-62/">GDPR Compliance Webinar</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/">AWS Regional Product Service Table</a></p>
<h1 id="3Enabling-and-Associating-Amazon-Macie-with-Amazon-S3"><a href="#3Enabling-and-Associating-Amazon-Macie-with-Amazon-S3" class="headerlink" title="3Enabling and Associating Amazon Macie with Amazon S3"></a>3<strong>Enabling and Associating Amazon Macie with Amazon S3</strong></h1><p><a target="_blank" rel="noopener" href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=MacieServiceRolesMaster&templateURL=https://s3.amazonaws.com/us-east-1.macie-redirection/cfntemplates/MacieSer">US East (Virginia) CloudFormation Template</a></p>
<p><a target="_blank" rel="noopener" href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=MacieServiceRolesMaster&templateURL=https://s3-us-west-2.amazonaws.com/us-west-2.macie-redirection/cfntemplate">US West (Oregon) CloudFormation Template</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-setting-up.html#macie-setting-up-enable">CloudFormation Templates for all regions available</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-automation-how-to-use-cloudformation/">Course: How to use Cloudformation for Automation</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/advanced-aws-cloudformation/">Course: Advanced use of AWS CloudFormation</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/deploy-wordpress-cloudformation-17/">Lab: Deploy Wordpress using CloudFormation</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/">Course: AWS Cloudtrail: An Introduction</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/amazon-web-services/labs/monitoring-aws-cloudtrail-events-amazon-cloudwatch-76/">Lab: Monitoring AWS CloudTrail Events with Amazon CloudWatch</a></p>
<h1 id="4Alerts"><a href="#4Alerts" class="headerlink" title="4Alerts"></a>4<strong>Alerts</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-research.html#macie-query">Constructing Queries in Amazon Macie</a></p>
<h1 id="7Research"><a href="#7Research" class="headerlink" title="7Research"></a>7<strong>Research</strong></h1><p><a target="_blank" rel="noopener" href="https://lucene.apache.org/core/2_9_4/queryparsersyntax.html">Apache Lucene Query Parser Syntax</a></p>
<h1 id="8Classifying-amp-Protecting-Data"><a href="#8Classifying-amp-Protecting-Data" class="headerlink" title="8Classifying &amp; Protecting Data"></a>8<strong>Classifying &amp; Protecting Data</strong></h1><p><a target="_blank" rel="noopener" href="https://www.regexbuddy.com/regex.html">Regular Expressions (Regex)</a></p>
<h1 id="9Multiple-Accounts-with-Amazon-Macie"><a href="#9Multiple-Accounts-with-Amazon-Macie" class="headerlink" title="9Multiple Accounts with Amazon Macie"></a>9<strong>Multiple Accounts with Amazon Macie</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/macie/latest/userguide/macie-integration.html#macie-integration-member">CloudFormation Templates</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:18" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:18-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:02:20" itemprop="dateModified" datetime="2022-11-19T23:02:20-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Detecting-EC2-Threats-with-Amazon-GuardDuty-22/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:15" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:15-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:38" itemprop="dateModified" datetime="2022-11-19T22:53:38-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Understanding-Amazon-GuardDuty-21/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course covering the AWS security service, Amazon GuardDuty, which was announced during AWS re:Invent 2017. This course will explain what the service is and guide you through its features and configuration.</p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I’m one of the trainers here at Cloud Academy, specializing in AWS <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon web services</a>. Feel free to connect with me with any questions, using the details shown on the screen, or tentatively, you can always get in touch with us, here, at Cloud Academy, by sending an email to <a href="mailto:support@CloudAcademy.com">support@CloudAcademy.com</a>, where one of our Cloud experts will reply to your question.</p>
<p>As a pre-requisite to this course, you should have a basic understanding of the fundamentals of AWS, along with an awareness of different security measures and mechanisms that are offered by different AWS services, such as within IAM, specifically IAM Policies.</p>
<p>This course has been designed for those who are in a role of a security consultant or specialist, security analyst, security auditor, Cloud architect, or Cloud operational support analyst. This would also be valuable to anyone looking to learn more about AWS Security and threat detection within AWS.</p>
<p>This course has been designed to lead someone who is new to Amazon GuardDuty through to becoming someone who has a sound understanding of the service. The lectures have therefore been constructed as follows:</p>
<ul>
<li>What is Amazon GuardDuty? This lecture focuses on explaining what the service is and the function that it provides.</li>
<li>Components and configuration: This looks at the different components and elements that make up a service. This lecture also includes a demonstration on how to configure a service.</li>
<li>Managing multiple accounts: If you have multiple accounts, then this lecture will explain how you can configure Amazon GuardDuty to work across all your AWS accounts that you have.</li>
<li>Managing permissions: As with any service, you need to ensure you have the correct permissions configured for both the service with the service-linked role, and also your operational staff who will manage the service. This lecture looks at the different permissions required.</li>
<li>Understanding Amazon GuardDuty findings: Here, we’ll look at how to view the findings generated by Amazon GuardDuty and the different components of these findings to help you remediate any issues.</li>
<li>Benefits to the enterprise: This lecture focuses on how Amazon GuardDuty can be of benefit to your business.</li>
<li>Costing: Understanding that cost is important when using the features of a new service. This lecture examines those costs and provides an example.</li>
<li>Partner offerings: A number of different third parties offer services that seamlessly interact with GuardDuty. So, here, I will look at a couple of these examples.</li>
<li>Summary: Finally, I provide a summary lecture, which will highlight the main points from each lecture.</li>
</ul>
<p>There are a number of key objectives to this course. These being: to understand what Amazon GuardDuty offers as a service, you’ll understand how to manage and configure the service for single and multiple accounts, you’ll understand how to implement the correct permissions to both enable and manage the service, you’ll have an awareness on how to manage and resolve findings generated, and you’ll be able to explain how Amazon GuardDuty can play an important role within your organization.</p>
<p>Feedback on our courses, here at Cloud Academy, are valuable to both us, as trainers, and any students that can take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you can contact us at <a href="mailto:support@CloudAcademy.com.">support@CloudAcademy.com.</a></p>
<p>That brings us to the end of this lecture. Coming up next, I’ll be explaining exactly <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/what-is-aws-amazon-guardduty-1/">what Amazon GuardDuty is</a>.</p>
<h1 id="What-is-AWS-Amazon-GuardDuty"><a href="#What-is-AWS-Amazon-GuardDuty" class="headerlink" title="What is AWS Amazon GuardDuty?"></a>What is AWS Amazon GuardDuty?</h1><p>Hello and welcome to this lecture where I want to provide an introduction to the service, explaining what it is, what it does, and the problem that it solves.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> still treats security as its number one priority across its public cloud. They know that without adequate security techniques, mechanisms, and measures in place to safeguard and protect their customers and their data, their customers will not have the confidence to use their services. Cloud security can still be seen as one of the main reasons that companies are slow to adopt cloud technology from a public cloud provider such as AWS. Much of this can be attributed to the lack of cybersecurity skills within an organization. Not having the knowledge and ability to confidently implement a high level of security within the cloud can be damaging to an organization.</p>
<p>Security is an ongoing development process. As technology changes, so do threats and risks against that technology. With this comes a need for newer, more advanced and powerful tools to protect against these threats, and AWS is at the forefront of this development.</p>
<p>Prior to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a>, there were 10 other services that sat within the security, identity, and compliance category of the AWS Management Console, making this service the 11th. Each security service has a very specific function and benefit that it provides to assist and help customers control, manage, and operate a secure and safe environment within the cloud. The services within this category already cover a wide scope of features and security mechanisms, so how does this new service differ from the rest that already exist?</p>
<p>Amazon GuardDuty is a regional-based intelligent threat detection service, the first of its kind offered by AWS, which allows users to monitor their AWS account for unusual and unexpected behavior by analyzing AWS CloudTrail event logs, VPC flow logs, and DNS logs. It then uses the data from logs and assesses them against multiple security and threat detection feeds, looking for anomalies and known malicious sources, such as IP addresses and URLs.</p>
<p>The service itself is powered by machine learning, and this allows the service to continuously evolve by learning and understanding operational behavior within your infrastructure. Amazon GuardDuty then uses this data to look for erroneous patterns within your AWS account that could indicate potential threats to your environment. These threats could be behavioral based, where a resource has been compromised by an account or credential exposure, unexpected API calls that sit outside security best practices, or even communications from suspicious sources.</p>
<p>Using different threat detection feeds, some generated from public sources and some by AWS, Amazon GuardDuty provides automatic and continuous security analysis for safeguarding your entire AWS environment. Any findings generated by the service are presented and issued with a priority level that enables you to investigate the issue further to ensure that your environment is not compromised and exposed unnecessarily. Amazon GuardDuty is very simple to activate within your account, and unlike other more traditional threat detection mechanisms, there is no need to install any agents or software on your resources, meaning that this is a very scalable and flexible security tool to have enabled.</p>
<p>With this in mind, it’s also possible to link your AWS accounts together to perform a threat detection layer across all of your accounts. In addition to this, the service itself operates entirely on AWS infrastructure, providing zero impact of the performance of your own existing resources within your account. Threat detection is key in the defense against a security breach. Having the ability to respond to a potential threat as it is detected significantly reduces the chances of a breach. Cyber criminals are using more advanced techniques to infiltrate networks and hosts using zero-day threats, and Amazon GuardDuty is the latest service to help defend against these attacks.</p>
<p>That now brings me to the end of this lecture. Coming up next, I will be discussing the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/components-and-configuration-1/">components</a> of the service and how it fits together.</p>
<h1 id="Components-and-Configuration"><a href="#Components-and-Configuration" class="headerlink" title="Components and Configuration"></a>Components and Configuration</h1><h2 id="Resources-mentioned-in-this-lecture"><a href="#Resources-mentioned-in-this-lecture" class="headerlink" title="Resources mentioned in this lecture:"></a>Resources mentioned in this lecture:</h2><p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail: An Introduction</a></p>
<p>AWS Resource: <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types.html#actual-types">Amazon GuardDuty Finding Types</a></p>
<h2 id="Lecture-Transcript"><a href="#Lecture-Transcript" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture. Now we have an understanding of what the service is, I want to talk about the components that make up Amazon GuardDuty and its configurable options.</p>
<p>Firstly, let me look at the data sources that the service uses to perform its analysis. As mentioned in the previous lecture, the service uses three data sets to monitor your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account for threats, these being AWS CloudTrail Event Logs. These logs are generated from the output of the CloudTrail service in a JSON format and they hold all of the information and data relating to API calls that have been captured within your account. If you’d like further information on AWS CloudTrail, then please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">here</a>, which covers the service in detail.</p>
<p>VPC Flow Logs, these logs capture and store network traffic information flown into an out of your network interfaces from instances within your VPC. They are often used to troubleshoot networking issues for instances and can be used as a security tool by monitoring what traffic is reaching your instance.</p>
<p>DNS Query Logs, these logs contain queries that DNS resolvers forward to Amazon Route 53 and they can include information such as the domain and subdomain that was requested, a timestamp of the request, the DNS record type, and the DNS response code. If you’re not currently running these logs or have them configured within your account, then you need not worry. When you enable <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a>, it will automatically make these logs available to the services itself for analysis. However, it doesn’t manage these logs in anyway. If you want management of these logs, then you’ll need to configure this through the relevant service dashboard or API.</p>
<p>Amazon GuardDuty also incorporates machine learning. This allows the service to learn and adapt to what classes as unusual behavior within your account over time to then highlight it as a potential threat. This is an ongoing process. The more you work with your account, the more Amazon GuardDuty will be able to distinguish between intended operational changes and that of malicious or unusual behavior that sits outside of your normal parameters of operation. In addition to the analysis that is undertaken against the various logs that I’ve already discussed. </p>
<p>It’s also possible to upload your own list of trusted IPs and threat list. Any IP information that is added to the trusted IP list is white listed. This simply means that GuardDuty will not generate findings based on these IP addresses. They are trusted and known IP addresses. It’s worth mentioning, but you can only have one active trusted IP listed any time. You can also include your own list of threats as well based on IP information. This list will contain a known list of malicious IP address or networks, but you want to ensure Amazon GuardDuty generates findings for if any traffic is detected with this information. Unlike the trusted IP limit list of one, you are allowed six threat list to be simultaneously active within guardduty at any one time. To add either a trusted IP list or threat list is a very simple process. You simply need to provide three bits of information from within the Amazon GuardDuty dashboard. Firstly, you need to give your list a name, then provide the URL of whether source list is located on S3, and then the format of that list.</p>
<p>The main functions of Amazon GuardDuty is of course to detect any potential threats within your environment. When a threat is found, it is labeled as a finding within the GuardDuty dashboard, allowing you to take appropriate actions against them to resolve any security vulnerability that might exist. The content of the finding itself contains a lot of useful information and is essentially broken down into five parts. The finding summary, the resource affected, the action, actor, and additional information. Let me just run through each of these elements so you have a brief understanding of the complete finding.</p>
<p>The summary of the finding contains some key data, including the finding type, the severity, the region, the account ID, resource ID, time of detection, and depending on the threat detected, it can also contain information on which threat list was used to detect the finding. Much of this data is self-explanatory. However, I want to explain more about the finding type as this is a very useful piece of data when trying to ascertain what the finding relates to.</p>
<p>The finding type is a single phrase composed of concatenated data, which follows a specific syntax that defines a potential threat event that is detected. For example, the finding type of unauthorized access, EC2 &#x2F; SSH Brute Force shows that an EC2 instance has been involved with an SSH brute force attack. This makes it very easy to quickly identify the nature of the threat from the summary screen. As I said, this finding type is defined by a set syntax, which is as follows.</p>
<p>Using our previous example, we an see the finding type with the above syntax. As you can see, not every finding type will contain all elements of the four syntax. For a complete list of the available finding types and what they define, see the AWS documentation <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types.html#actual-types">here</a>.</p>
<p>Resource affected, this section is dedicated to providing information and by the affected resource in the threat that has been detected. Depending on the finding type, the information in this section and all remain sections can vary.</p>
<p>Action. The action section provides information relating to the action that was carried out, that resulted in the threat being detected.</p>
<p>Actor. The actor section contains data relating to the source of the detected threat, such as geographical information, IP address, port and domain.</p>
<p>Additional information. Finally, the additional information section may contain information such as which threat list was used to detect the threat and if the activity was considered to be unusual compared to historical data. Each finding is associated with the severity level and score. The score value will affect the severity. As of writing this course, the severity is set as follows. High is labeled is seven to 8.9, medium is four to 6.9, and low is 0.1 to 3.9. These different severity levels allow you to quickly identify which findings you should act upon first as a priority.</p>
<p>Any finding that is marked as high should be investigated immediately as it assumes that a security breach has occurred and the resource in question has been compromised which poses a significant threat to your infrastructure. Remediation of this finding should be your first priority before the threat extends further within your account with additional malicious activity.</p>
<p>A medium severity will genuinely indicate that GuardDuty had to take the suspicious activity within your account against a specific resource. Although not as critical as high, this level of finding should still be investigated as soon as possible as it could be the beginnings of a greater threat and could soon escalate to a high if left unchecked.</p>
<p>Low severity findings do not require you to take any immediate action. These are threats that were detected and blocked before any issues arose within your environment. However, it is still worth looking at any low findings to see if there are improvement that can be made to prevent it from happening again.</p>
<p>I now want to perform a quick demonstration introducing you to the service itself. I’ll show you how to enable the service, configure trusted IP and threat list, and I’ll walk you through each of the screens within the Amazon GuardDuty dashboard. So let’s get started.</p>
<p>Okay, so I’m signed into my AWS management console, and what I want to do first is go to Amazon GuardDuty and we can find that down under Security, Identity, &amp; Compliance. And if this is the first time that you’re using the service, you’ll be presented with this splash screen, and it just enables you to get started and gives you a quick overview of exactly what it is. So, let’s get started.</p>
<p>Firstly, we need to enable the service. And like I say, it’s a very simple process. All we need to do is to click enable guard duty, And when we do this, as it says here, when you enable GuardDuty, you grant GuardDuty permissions to analyze AWS CloudTrail Logs, VPC Flow Logs, and DNS query logs to generate security findings. And this will set up a service role permission which has this access here, but I’ll go more into permissions in a later lecture to explain these service roles et cetera.</p>
<p>So if I click on Enable GuardDuty. It’s as simple as that. So we now have GuardDuty running within this account within this region. And at the moment, we don’t have any findings which is absolutely fine. I don’t really have many resources running in this account. It’s just a test account. So if I just take you through the dashboard and each of the screens and we can just take a look and get a feel for the service.</p>
<p>So on the left-hand side, you can see we have three different headings. We have Findings, Settings, and Free trial. So let’s start at Findings. In the screen, it will show any findings that have been found by Amazon GuardDuty and it will list under here as a finding and when it was last seen, etc. Like I say, at the moment, we’ve only just enabled the service. I don’t really have any resources running, so there is no findings at the moment. If we go into Archived, this will show a list of archived findings that you’ve had, again, with the same columns.</p>
<p>If we go down to General under Settings, here it talks about the permissions of the service role that GuardDuty uses to monitor your resources. And like I say, I’ll dive deeper into permissions in a later lecture, but this is where you can just see the service role permissions that it has and the trust relationships as well. On the CloudWatch events, GuardDuty does support CloudWatch and if you click on this link here, it will tell you how to configure this.</p>
<p>Under Sample findings, this is quite useful. So, sample findings help you visualize and analyze the finding types that GuardDuty generates. And if we want to list a number of sample findings for us to kind of get familiar with the console and the different findings that it has. Then we can simply click on Generate sample findings. And now if we go back to our finding settings under Current, we can see that GuardDuty has enabled a number of different sample findings for us, and they’re indicated here as samples. Don’t worry, these aren’t real findings in your account, these are all samples, and we can see that we have 33 samples.</p>
<p>If we look along these icons here, we can see that one of them has a high severity and 32 of them have a medium severity. And if you click on each of these, you can filter on those to identify exactly where the finding is. And if you want to remove the filters, just simply click on the X here. So if we keep going through these settings on the left. So that’s Generate sample findings. Underneath this, it gives you two settings, one to enable you to suspend GuardDuty and another to disable GuardDuty.</p>
<p>When you suspend GuardDuty, it stops monitoring your AWS environment, and it won’t generate any new findings at all. But any existing findings you have will remain in the console. If however you disable GuardDuty, then again monitoring will stop and it won’t generate new findings and you’ll also lose all your existing findings as well. If you want a copy of those findings before you disable it, then it’s best to export that data before you do so.</p>
<p>Next, if we go down to Lists, now this is where we can perform list management of your trusted IP lists and also your threat list as well. Okay, so let’s add a trusted IP list and a threat list whilst we’re here. So if I click on Add a trusted IP list, specify the list name, let’s just call this TrustedIP, enter the location, and I’ve called it TrustedIP.txt. So that’s my bucket. And if I specify the format just as plain text. And you need to click on I agree to accept and agree to the GuardDuty service terms. And then click on Add list. There we’ve added the trusted IP list.</p>
<p>If we move down to a Threat list, we can do the same. So let’s just call this ThreatList in the same bucket, and I’ve called it ThreatList.txt. Specify the format, plain text again. I agree. Add list. Then we got it. So now we have our trusted IP list and also our threat list that GuardDuty will use. And if you had a number of threat list, then you can select different ones to be active, etc. So it’s very simple to add your trusted IP list and your threat list as you can see.</p>
<p>If we go down to Accounts on the left-hand side, this is where you can set up and manage multiple accounts. So you can use a master GuardDuty account. And then if you have other AWS accounts, you can add them as member accounts, and then view all the findings from those member accounts in this one master account So it makes management a lot easier, and I’ll talk about this in a further lecture when I talk about multiple accounts. So I’ll go into that a lot deeper then.</p>
<p>Now if we look at Free trial, we look at the details on the Free trial. When you first enable Amazon GuardDuty, you get 30 days free of the service. And using the screen here, you can see how many events have been monitored by CloudTrail logs and how many bytes we’re using through VPC Flow logs and DNS logs. And it’ll give you an estimate in cost on how much that would actually cost once you’re outside of your 30-day free trial. So it gives you a good estimation on understanding how much this service would cost you over a typical month. So like I say, when you first enabled the service, you get 30 days free, and you can use this screen to kind of get an estimation on how much it would cost you going forward, which I think is a really good idea.</p>
<p>And then finally, on the left-hand side, we have Partners, and this will take you off to a list of Amazon GuardDuty partners. That’s essentially it for the console. Like I say, it’s a very simple service. There’s not much to it at all.</p>
<p>Before I finish the demonstration, if we go into one of these findings, pick this one for example, we can see here that we have a brief description of what the finding is. We can see that an instance with an unusual type was launched by a IAM principal, has a severity, and tell us the region it was in, how many. The account ID that it came under and any resource information, and also the treat list name as well. Again, remember these are just samples. It tells us about the resource effective and the access keys and IDs and usernames, what action was used, like the API call and the service name, and also some geographic information as well about where that was initiated from. So when you select the finding, you can get quite a lot of information from this to help you resolve the issue. And I have a later lecture coming up that dives deeper into analyzing findings as well. So I’ll talk more about these findings in that lecture. Let’s click on Close. And there you have it.</p>
<p>So, it’s very simple to enable GuardDuty. It’s imply a one click and it’s enabled for that region, and you just have a few menus on the left-hand side that are very self-intuitive.</p>
<p>That now brings me to the end of this lecture. Coming up next, I’m going to be talking about more on how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/managing-multiple-accounts-1/">manage multiple accounts</a>, so you can miss out of the dashboard. So I’ll explain how to share the findings from all your member AWS accounts and push it to one master GuardDuty account.</p>
<h1 id="Managing-Multiple-Accounts"><a href="#Managing-Multiple-Accounts" class="headerlink" title="Managing Multiple Accounts"></a>Managing Multiple Accounts</h1><p>Hello and welcome to this lecture where I’m going to explain how it’s possible to link multiple <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> accounts when using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a> for centralized management.</p>
<p>Many organizations have multiple AWS accounts for many different reasons. When using GuardDuty, it’s possible to have one of these accounts act as a master account, and then all other AWS accounts act as members. In this scenario, all the findings from the member accounts are configured to send a copy of the results to the dashboard of the master account. This is ideal for your security teams who manage multiple AWS accounts, as it allows them to view all findings in a central location, instead of having to view the findings from within each AWS account.</p>
<p>It’s important to point out that Trusted IP lists and threat lists within the member accounts are not used within the master account. The member accounts each have their own lists, which can be configured by the users within those member accounts. However, the master account does give you added control and administrative functions, such as having the ability of being able to suspend Amazon GuardDuty within its own account and other member accounts. If you wanted to disable the service, then you must first remove the member accounts from the master account, as the master account is not allowed to disable GuardDuty on its member accounts.</p>
<p>The operation of Amazon GuardDuty with the member accounts remains the same. The users can still perform the same functions from within the dashboard, and it’s still possible to view and review the findings, upload trusted IP threat lists, as well as suspend or disable the service on that member account.</p>
<p>To set up your AWS accounts in a master and membership <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/components-and-configuration-1/">configuration</a> for GuardDuty is a simple three stage process. You must first add an AWS account from within the master account, send an invitation to the member account, and then accept the invitation from within the member account. This process prevents a single account from simply adding member accounts without authorization. The member account has to accept the invitation before sharing its findings with the master account.</p>
<p>To show you how to create the link between accounts, I will now demonstrate these three steps by adding a member account to a master account.</p>
<p>Okay, so I’m at the dashboard of the Amazon GuardDuty, and what I need to do is go down on the left hand side to Accounts. Now, I’m going to have this account as my master account, and I want to invite a second AWS account that I have to be a member account. So, that member account’s findings and details will be fed through to this master account. So, the first thing I need to do is click on Add Accounts, and I need to enter the account ID of the member account, and also the associated email address of that account. Now, if you have maybe 10, 20, 30 AWS accounts, or even more, then you might find it a little laborious to add each individual account, so what you can do, you can upload a CSV file with all that information in with all the account IDs and the associated email addresses. As I only have the single account, I’m going to add, I can put that account number in, and also the associated email address. Click on Add, and at the bottom here, we can see these are the accounts to be added, and then click on next, and we see here that member accounts share the findings with you, and members must first accept your invitation.</p>
<p>So, if we go down to under the Status column and click on Invite. Now, if you wanted to, you can enter a personal message here that will go to owner of the secondary account, and I’m just going to say “Please accept.” At this point, we just need to click on Send Invitation. And that has now sent an email to the owner of the account that we added. And we can see the status is now saying “Pending.” So, if I go across to my email and take a look at the email that we’ve received.</p>
<p>Now, this is the email that I’ve received. Now, the title states that there’s action requested, and it says “The master account wants to become the AWS GuardDuty administrator for this secondary AWS account.” And we can see down here where it says the following notes were provided with this invitation, and it says “Please accept.” So, that’s that custom message that I added. Now, to view the invitation, we can click this link here, and it takes me straight to this screen, and it says you have a membership invitation, but we must enable GuardDuty before we can accept invitations. So, we’ll need to enable GuardDuty on this account first.</p>
<p>GuardDuty’s now enabled, and it states that “The following AWS accounts have requested permission <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/managing-permissions-1/">permission</a> to view and manage GuardDuty items on your behalf. You can accept only one invitation.” So, this is our master account here, and I want to accept, and then I’ll go across to Accept Invitation. And that’s it. So, this is now the member account, and it will push its findings across to the master account. So now, if I go down to Accounts on this member account, I can see that this is a member account, and that pushes its findings to this master account, so I’ve accepted the invitation there.</p>
<p>So now, if I go across to the master account. So, I’m back in my dashboard for my master account, and now if I go down to Accounts, I can see here that I have a member account added, and the status is now monitored. And that’s it. So, it’s a very simple process to achieve. So, from your account that you want to be the master, you then invite any member accounts. Those member accounts then simply accept that invitation, and from that point onwards, your master account will then monitor the status and findings of all your member accounts.</p>
<h1 id="Managing-Permissions"><a href="#Managing-Permissions" class="headerlink" title="Managing Permissions"></a>Managing Permissions</h1><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_managing_access.html">AWS Policies for Access Management</a></p>
<p>Course: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Identity &amp; Access Management</a> </p>
<h2 id="Lecture-Transcript-1"><a href="#Lecture-Transcript-1" class="headerlink" title="Lecture Transcript"></a>Lecture Transcript</h2><p>Hello and welcome to this lecture where I want to talk about the different permissions required and used when using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a>, both from a user perspective and from the service itself.</p>
<p>This lecture will primarily focus on permissions to perform the following functions. How to access the Amazon GuardDuty Dashboard. How to enable the Amazon GuardDuty within a region and to manage your Trusted IP and Threat Lists. Before a user even begins to use Amazon GuardDuty specific permission are required to access the dashboard and enable the service. For example, if a user is trying to access the dashboard to enable GaurdDuty on your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account but receive an error that prevents them from doing so then it’s most likely related to their permissions.</p>
<p>If the user selects Amazon GuardDuty from the homepage of the AWS Management console the following error message may appear preventing them from access to the GuardDuty dashboard. This error indicates that user doesn’t have the relevant permissions to access the GaurdDuty service. They would need to speak to their administrator to ask them to revise their permissions for GuardDuty. However, if additional permissions were then given to that user by allowing all actions within GuardDuty using the following policy, they will then be able to access the dashboard without an issue. This does not mean however that the user has all the permissions they need to initially enable the service within the region. If the user attempts to enable the service with the above permissions alone they will receive the following error.</p>
<p>To enable the Amazon GuardDuty service a user will need specific IAM permissions that allows them to create a service-linked role that allows GuardDuty to retrieve information about some of your resources. So although the user will have full access to GuardDuty actions the user will still need these additional permissions relating to IAM and service-linked roles.</p>
<p>AWS has created an AWS management policy with the relevant permissions to allow you to enable GuardDuty within a region. The name of this policy is Amazon GuardDuty Full Access. This policy essentially allows full access to Amazon GuardDuty actions with the added permissions of being able to create the service-linked role as shown in this policy. One point to be made aware of with this Amazon GuardDuty Full Access is that it doesn’t allow you to upload a Trusted IP or Threat List as this again, requires different IAM permissions. If you do try to add a list with the Full Access policy you will receive the following error.</p>
<p>To allow a user to be able to manage your Trusted IP and Threat lists you will need to add the following permissions to the user group or role. Do remember to replace the AWS account number with your own account. With all this in mind, AWS have provided a policy that you can create as a custom policy which allows you genuine full access to Amazon GuardDuty which will allow you access to the dashboard to enable the service in all regions and perform operations within the service including updating and managing Trusted IP and Threat Lists. And this policy is as shown, however you must remember to replace the AWS account number with your own. While I am on the topic of AWS management policies you may also notice that there is another AWS management policy entitled Amazon GuardDuty Read Only Access. As expected this policy provides the user to have read-only permissions to Amazon GuardDuty allowing them to review findings.</p>
<p>Earlier I mentioned the fact that GuardDuty uses the service-linked role during the enablement of the service. This role, AWS Service Role for Amazon GuardDuty contains the following permissions. This is used to enable GuardDuty to have read-only visibility of your EC2 instances should a finding become known relating to one of these resources. If there is a finding against your EC2 instance GuardDuty can use these permissions to retrieve metadata about the resource to present in the finding it generates to help resolve the security issue and threat. This service-linked role also has an associated trust relationship which allows Amazon GuardDuty to adopt this role. Again, when this service is enable against a region these permissions are granted to Amazon GuardDuty automatically.</p>
<p>As with other AWS services you can be very specific with what actions a user, group or role can perform with Amazon GuardDuty by creating custom IAM policies. For further information and details on how to create custom IAM policies please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">here</a>.</p>
<p>That now brings me to the end of this lecture. Coming up next I shall be diving into <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/understanding-amazon-guardduty-findings-1/">GuardDuty findings</a> in more detail.</p>
<h1 id="Understanding-Amazon-GuardDuty-Findings"><a href="#Understanding-Amazon-GuardDuty-Findings" class="headerlink" title="Understanding Amazon GuardDuty Findings"></a>Understanding Amazon GuardDuty Findings</h1><p>Hello, and welcome to this lecture. I’m going to take a deeper look at the findings generated by <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a>, and how to look into the details further to help you remediate the issues you find.</p>
<p>In an earlier lecture, I briefly provided an overview of the findings section, however, in this lecture I want to look at these findings in a bit more detail and also the findings area of the dashboard to show you some additional features that it has. To do this, I feel it would best to demonstrate this form within the management console, so let’s take a look.</p>
<p>Okay, so I’m at the dashboard of Amazon GuardDuty. And, as you can see, I have a number of sample findings, which is what we went through earlier in a previous demonstration. So, let’s take more of a look around this section of the findings dashboard.</p>
<p>On the far left-hand side of this table you can we have a number of check boxes, and we can select individual findings. Now, what we can do with these selected findings, is we can go to actions at the top here, and either archive those findings or export them. So let’s just run through each of those. If we click on archive, it says our findings have now been archived. So, if we go over to the left-hand side and select archive, we can see those findings that we archived there. Now you might do this for ease of management, or for general housekeeping, just keepin’ a record of the findings that you’ve had. And if you want to select all findings in a table, you simply click on the top check box, there. Pretty standard stuff. I’m going to move those findings back into current.</p>
<p>Okay, and the other option we had under actions was export. And this will export the JSON data for all of those findings. And you simply click on download and it will export the JSON file. If you need to refresh your findings list, there’s the refresh button at the top, here, simply click on that and it’ll refresh your findings.</p>
<p>Over on the right-hand side, at the top, we have indicators that let you know how many findings are of a particular severity. So, at the moment, I have zero findings that are low severity, 32 findings that are medium and 1 finding that is high. That is a very quick way to have a look at your dashboard to see your most critical findings, and you can simply click on each of these to filter on these findings. So, that’s the high, and there’s all your medium, and obviously, we don’t have any low. You can add additional filters to your findings. So, for example, if we looked at this top finding, here, that would bring up all the details for this finding. Now, if there’s any hyperlinks on this screen, as you can see here with the plus or minus or the actual resource id, here or the instance, if you click on the plus, then that will add that as a filter. As you can see, here, in this row we have a filter row with severity of medium. Now I can select, exclude that, or currently, it’s included.</p>
<p>So, if we go back into that same finding and I want to filter on this account id only, I can click on the plus, and again add it into the filter. And, one more, if I filter on resource type of instance. And now that’s filtered all the findings that have a severity of medium, with this account id, and this resource type. Now you can see that it now only showing 19 of the total 33 findings. If you like, you can save this as a set filter. So, I’m just going to call this, instance. If I remove all my filters now, I can then revert back to that filter at anytime by clicking on the dropdown box, clicking on the name and it will bring up the filter. So, you can set up many different filters if you have different accounts, especially if you’re filtering for member to master accounts or you can filter on resource types, or severities. Anything that has a hyperlink within the details of that finding, you can add a filter against. For example, again here, an action type of DNS_REQUEST, so if I go ahead and add that, we now have only eight findings of the 33 that match this filter.</p>
<p>If you wanted to exclude any of these filters, then just simply hover over it and click on exclude. So now a filter on all findings with a medium severity, under that account id that do not include a resource type of instance. You can either include or exclude filters as you see fit. Let’s just clear those filters, just by clicking on this x, here.</p>
<p>So, let’s now take a closer look at a couple of these sample findings. Let’s take a look at this one here, let’s do a Bitcoin. When you click on a finding, it will open up this additional window which gives you lots more information about the finding. At the very top, here, we have the finding type, which just gives us a breakdown of what the finding’s relating to. And then, from here, we can see that this EC2 instance is querying a domain name that is associated with Bitcoin-related activity.</p>
<p>Now, unless you’re using this EC2 instance to mine for Bitcoin, then it’s pretty fair to say that this instance has been compromised in some way or another. So, you could either take a look at that instance and see if it has any software on it that it shouldn’t do, any kind of malware or anything like that. Failing that, it’s probably best to terminate that instance and then set up a new EC2 instance. And then, when you’ve done that, just make sure you’re applying best practices to harden that instance against any kind of security threats using the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> security best practices.</p>
<p>So, looking more at this window, here, we can the Severity of the finding, and it’s currently set as Medium, the Account ID that this finding is associated with, and that’s helpful if you have <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/managing-multiple-accounts-1/">multiple accounts</a> feeding through to a master account. We can see when this finding was last seen, two days ago. The Region it was in, and also the Resource ID that is affected by this, and how many times this finding’s come up. And if we scroll down to the Resource affected, this gives us a bit more information on the resource itself. So we can see the role of this resource was a TARGET, Resource type is set as an Instance and it gives us the Instance ID. And we can click on that instance id and it will take us into the EC2 management console direct to that instance. As this is just a sample, made-up instance id, it won’t lead me anywhere if I click on it. It will take me to the EC2 console, but it’ll explain that it can’t find the instance.</p>
<p>If we go down to Action, this gives us the Action type of the threat which was a DNS_REQUEST, and as we know, the instance was drawn to a query, a domain name that’s related to Bitcoin mining. And then down to Actor, this gives us the actual domain, and then in this particular finding there wasn’t really much in the Additional information section. So, in this instance, I would probably terminate the instance and then launch it again. And then just ensure that we harden that instance with security best practices.</p>
<p>Just while I’m here, these icons at the top just simply tell you where this additional window will appear on your screen. So we can have it at the bottom, to the side, or just full screen. I tend to have it to the side, just makes it a little bit easier to read. And then when you’re done with the information, just click on close.</p>
<p>Let’s take a look at another example.Let’s take a look at this one, here. This time the finding type relates to a Behavior threat, to do with an EC2 instance and Network Port Unusual. This is stating that the EC2 instance is communicating with a remote host on an unusual server port 22. So what this is telling us is, generally during the history of this instance, communicating on port 22 to this remote host hasn’t happened before so it’s an unusual behavior. It’s flagged it as a potential threat. So, we can take a look again, it’s given it a severity, you can see the account id, the region etc. This time we have a threat list name, so it’s found it due to one of the configured threat lists. If you go down to the Resource affected, again we can see that the resource is the TARGET, and again it give the instance id as well. Under the Action section, we can see that it’s an OUTBOUND network connection that’s being made.</p>
<p>We can see some more information, here, about the remote host, it give the IP address, the Port that it’s actually using and some information relating to the city and the country. This time under Additional information it does give us a bit more, it gives us threat list name, the unusual port, which is 22, and the protocol, UDP. So, it does depend on the finding type as to what level of information you get under these additional section such as Actor, Action and Additional information. So, again, it looks like your instance is being compromised. You want to check out the security groups and ACLs, and also the EC2 instance itself. Again, a good way if you have an EC2 instance that’s been compromised, is just terminate it and launch a new instance, and harden that instance according to best practices.</p>
<p>So, it’s very self-intuitive. The findings themselves do provide some good information as to what has been affected, what kind of threat it is, and some information to help you remediate the problem as well. It’s a good idea to get familiar with these sample findings as the more familiar you become with these, the easier it will be to help you remediate real issues when you generate your own findings with your production environment.</p>
<p>That brings me to the end of this demonstration and the end of this lecture. Coming up next, I’m going to be talking about how you can use Amazon GuardDuty as a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/benefits-to-the-enterprise-1/">benefit to your enterprise environment</a>.</p>
<h1 id="Benefits-to-the-Enterprise"><a href="#Benefits-to-the-Enterprise" class="headerlink" title="Benefits to the Enterprise"></a>Benefits to the Enterprise</h1><p>Hello, and welcome to this lecture, where I shall be focusing on a number of benefits that this service provides to the enterprise.</p>
<p>By now, you may already have a number of ways of how this service could benefit your own organization, as this is a very powerful and useful security service to have at your disposal. Any service that is able to offer assistance into the protection of your data within the public cloud is very valuable. All too often, we hear about organizations that are being probed and hacked within the cloud environment, with millions of records being stolen, containing sensitive, personally identifiable information. This is one of the reasons that cloud adoption is stalled, due to security risks and an awareness of how to secure the environment correctly.</p>
<p>The main benefit of the service is that it is simply an intelligent threat detection service, which performs continual and automatic analysis and threat detection within your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> account by analyzing your cloud trail logs, DNS query logs, and VPC flow logs. Essentially, you now have an active service monitoring and detecting anomalies throughout your environment with the addition of having it powered by machine learning and utilizing multiple threat detection feeds, looking for any communications with un-trusted and malicious sources.</p>
<p>Whether you are running 10 instances or 10,000 instances, the service does not impose any performance issues against your resources, and will provide the same high level of detection used within huge global enterprise deployments as it does to small-scale, single availability zone deployments. Security is crucial, and having these security resources available from day one is not something a small organization would have had in a traditional datacenter deployment of a solution.</p>
<p>The technology, skill set, and resource to implement a threat detection system in a traditional environment will be very costly and unlikely to make it into the forefront of priority in most cases. Regardless of the size of your AWS account and the resources within it, you will be able to use the power or full force of this intelligent threat detection service for a minimal cost.</p>
<p>As I explained in an earlier lecture, it’s possible to aggregate the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/understanding-amazon-guardduty-findings-1/">findings</a> of all your AWS accounts into a single master account. This simplifies the management for your security team by allowing them to monitor any findings through a single console. Any efficiencies such as this that can be taken advantage of saves time and reduces risk of something being missed. As with any monitoring solution, being able to assess your entire infrastructure through a single pane of glass effect pays dividends in its productivity as a service, and makes management that much easier.</p>
<p>With many security detection and vulnerability solutions out there today, either an agent or other software is often required to be installed onto the server that you want to monitor and detect potential security threats for. With Amazon GuardDuty, this is not required. All threat detection is performed without the need to monitor the incidents with additional software or agents.</p>
<p>One of the many great things about Amazon GuardDuty is that it comes with no upfront costs at all. You only pay for the processing of your log files, which I’ll come onto in a later lecture of this course. Traditionally, to install and configure a scalable, intelligent threat detection solution, would require a considerable amount of capital expenditure. With Amazon GuardDuty, you simply click enable, and it starts working straightaway without any upfront costs.</p>
<p>Having a powerful, intelligent threat detection system such as <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a> is one thing, but being able to automate responses to findings to help remediate potential security loopholes is another. You can use Amazon CloudWatch event rules and targets in conjunction with AWS Lambda to help you automate a response to a particular finding. With the ability to trigger automated responses based on GuardDuty findings, you are able to quickly and easily lock down a particular resource or restrict permissions that could stop an attack. For example, if you had a resource that was the target of a brute force SSH attack, you could set an automatic response to block SSH. For more information on AWS Lambda and Amazon CloudWatch, please see our content library for labs and courses on these services.</p>
<p>There are many features and reasons as to why this service will be beneficial to your business, many of which could save you a lot of money, should malicious activity occur within your environment. That now brings me to the end of this lecture. Coming up next, I want to talk about how much this service <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/costing-1/">costs to run</a>.</p>
<h1 id="Costing"><a href="#Costing" class="headerlink" title="Costing"></a>Costing</h1><h2 id="AWS-Resource"><a href="#AWS-Resource" class="headerlink" title="AWS Resource"></a>AWS Resource</h2><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/pricing/">Amazon GuardDuty Pricing</a></p>
<h2 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello, and welcome to this very short lecture just to explain how much <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a> is likely to cost you, and how the pricing structure works.</p>
<p>Essentially, the pricing for this service is broken down into two parts. CloudTrail Event Analysis, and VPC Flow Log and DNS Log Analysis. CloudTrail Event Analysis is charged at per one million events per month, whereas the VPC Flow Logs and DNS Logs are charged at per gig of log analyzed per month.</p>
<p>The cost for each depends on which region you are running the service in. For a full listing of each region, visit the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> pricing page of the service found <a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/pricing/">here</a>. When you first enable Amazon GuardDuty, you are able to use the service free for the first 30 days. In addition to this, you are able to see how much Amazon GuardDuty would have cost you for those 30 days, to help you estimate your ongoing payment should you continue to use the service. As you can see from this image within my own account for testing, there are minimal events and log data. As such, after a week, it wouldn’t have cost me anything as yet. I’ve not reached one million events or one gig of data. However, you can see from here that should you be running this is your Enterprise account, as opposed to my personal test account, where you may have thousands of resources, it would allow you to estimate the ongoing costs associated with GuardDuty.</p>
<p>Before I finish this lecture, I just want to provide a costing example, based on running Amazon GuardDuty from within the London EU region, which currently states pricing as follows. Please note, for the latest pricing information, please refer to the AWS documentation. With this in mind, let’s presume we have the following data per month: 55 million CloudTrail Events, 3,000 Gigabytes of VPC Flow Logs, and 2,000 Gigabytes of DNS Query Logs. Using the charges from the table listed for the London region, we can calculate the costing as follows. The CloudTrail Events would work out at $242, and the VPC Flow Logs and DNS Query Logs will total 2,350, giving a total of $2,592 for that month.</p>
<p>The charges for this service are very simple to understand and estimate. You may be thinking, is this worth the cost? But at the same time, you should also be thinking of the cost against not using the service, and suffering the effects of a security breach. Not only will this have a financial impact on your organization in the event you have to shut systems down to remediate the issue, but there is also a significant cost of reputation to your business. All of this needs to be taken into consideration when looking at the cost of security.</p>
<p>That now brings me to the end of this lecture. Coming up next, I shall be looking at some of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/partner-offerings-1/">partner offerings</a> that are available, that work in conjunction with Amazon GuardDuty.</p>
<h1 id="Partner-Offerings"><a href="#Partner-Offerings" class="headerlink" title="Partner Offerings"></a>Partner Offerings</h1><h2 id="Resources-1"><a href="#Resources-1" class="headerlink" title="Resources"></a>Resources</h2><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/marketplace/pp/B0764JH55Q?ref=_ptnr_AL_Website_CloudInsightEssentials_AWS_Marketplace">Alert Logic Cloud Insight Essentials (US)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.crowdstrike.com/products/falcon-intelligence/">Crowdstrike</a></p>
<p><a target="_blank" rel="noopener" href="https://www.trendmicro.com/aws/guardduty/">TrendMicro</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/resources/partners/">Full Partner List</a></p>
<h2 id="Transcript-1"><a href="#Transcript-1" class="headerlink" title="Transcript"></a>Transcript</h2><p>Hello and welcome to this lecture. Well, I just want to briefly highlight some of the partner offerings that integrate with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/introduction-60/">Amazon GuardDuty</a> and how to find out more information about them.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> work with many approved vendors to seamlessly interact their services with existing tools and products on the market today. Many of these vendors focus on monitoring and security. And as a result, there are many partners that provide services that can interact with Amazon GuardDuty to help you get even more from the service. Many organizations will probably be familiar with some of the partners and would likely already be using some of their products within their current infrastructure and environment. I just want to highlight a few of these partners currently committed with the Amazon GuardDuty. So you are aware that there are additional benefits to be had from a security standpoint.</p>
<p>Starting with Alert Logic Cloud Insight Essentials for AWS. Alert Logic offer a product that allows you to gain additional insight into your Amazon GuardDuty findings. Again without any agent required on your resources. Cloud Insight Essentials is designed to help you respond to your GuardDuty findings faster by providing further intelligence about the threat, in addition to providing more information about how to remediate the issue with specific actions. It also adds a feature that allows you to produce reports to analyze trends with your AWS account from a threat detection perspective. For further information on this offering and what it can do please visit the <a target="_blank" rel="noopener" href="https://aws.amazon.com/marketplace/pp/B0764JH55Q?ref=_ptnr_AL_Website_CloudInsightEssentials_AWS_Marketplace">link</a> on the screen.</p>
<p>CrowdStrike is another partner of Amazon GuardDuty. However, they integrate their technology and threat intelligence feeds which are used within CrowdStrike Falcon to Amazon GuardDuty. GuardDuty can then pull data and information from CrowdStrike which uses AI and machine learning to provide protection and block against cyber security threats. For more information on CrowdStrike and CrowdStrike Falcon please visit the <a target="_blank" rel="noopener" href="https://www.crowdstrike.com/products/falcon-intelligence/">link</a> on-screen.</p>
<p>I’ll take a look at one more example. This time with Trend Micro. Trend Micro have an existing product called Deep Security which uses an agent that can be installed on EC2 or ECS deployments to help protect against an array of threats such as anti-malware and adds features such as intrusion prevention. Using automation through CloudWatch and Lambda triggers can be used to invoke deep security and engage its rich features to analyze and detect any issues that may have occurred on the resource. More information on how these two products can work together can be found <a target="_blank" rel="noopener" href="https://www.trendmicro.com/aws/guardduty/">here</a>.</p>
<p>These were just a few of the partners from the Amazon GuardDuty partners list which can be found <a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/resources/partners/">here</a>.</p>
<p>You may find that you are already using services from these partners. If you are then you may find you already have additional security features available to you when and if you enable Amazon GuardDuty.</p>
<p>That brings me to the end of this lecture. Coming up next, I will be providing a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/course-summary/">summary</a> of the key points taken from the lectures throughout this course.</p>
<h1 id="Course-Summary"><a href="#Course-Summary" class="headerlink" title="Course Summary"></a>Course Summary</h1><p>Hello and welcome to this final lecture of the course. We want to summarize the key points from each of the lectures.</p>
<p>I started the course by explaining what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/what-is-aws-amazon-guardduty-1/">Amazon GuardDuty is and what it does</a>, and I explained that Amazon GuardDuty is a regional based intelligent threat detection service. It allows users to monitor their AWS Account for unual and unexpected behavior by analyzing AWS CloudTrail event logs, VPC flow logs, and DNS logs. The logs that were assessed against multiple security and threat detection feeds, looking for anomalies and known malicious sources. The service itself is powered by machine learning, and Amazon GuardDuty provides automatic and continuous security analysis for safeguarding your entire AWS environment. Findings are presented with a priority level that enables you to investigate the issue further, and the service does not require any agents or software on your resources. It’s also possible to link your AWS accounts together to perform a threat detection layer across all of your accounts. And this service has zero impact of the performance of your existing resources.</p>
<p>Following this, I then discussed the different component and the elements of the service itself. Within this lecture, we learned that there are three data sources that Amazon GuardDuty uses to perform its analysis: AWS CloudTrail event logs, VPC flow logs, and DNS query logs. Machine learning is interwoven with Amazon GuardDuty, allowing it to learn and adapt to what it classes as unusual behavior within your account at the time to then highlight it as a potential threat. List management allows you to upload your own list with trusted IPs and threat list, and any IP information that is add to the trusted IP list is whitelisted. Threat lists contain a known list of malicious IP addresses on networks that you want to ensure guide you to generate findings for if any traffic it detected with this information. GuardDuty findings are listing within the GuardDuty dashboard and allow you to take the appropriate actions against them to resolve any security vulnerabilities that may exist. The content of the finding itself can be broken down into five parts: the finding summary, the resource affected, the action, actor, and additional information. Each finding is associated with a severity level and score. The score value will affect the severity. I also gave a demonstration that introduced you to the service portion and how to enable the service, configure trusted IP and threat lists, and an overview of the different options within the Amazon GuardDuty dashboard.</p>
<p>Next, I spoke about how to link multiple AWS accounts for Amazon GuardDuty, allowing for centralized management. In a multi-account scenario, one account can act as a master account and then all others can act as members. Findings from member accounts send a copy of the results to the dashboard of the master account. And this allows you to view all accounts in a central location. Trusted IP lists and threat lists within the master account are not used within the member accounts, and the master account has additional control and administrative functions such as having the ability of being able to suspend Amazon GuardDuty within its own account and other member accounts. To set up your AWS accounts in a master-member configuration it’s a simple three-stage process. Add an AWS account from within the master account, send an invitation to the member account, and then accept the invitation from within the member account. Ie then performed a simple demonstration showing you how to carry out these steps.</p>
<p>Next, I focused on how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/managing-permissions-1/">manage permissions with Amazon GuardDuty</a> and here I covered the following points. To enable the Amazon GuardDuty service, the user will need some specific IAM permissions that allows them to create a service-linked role that allows GuardDuty to retrieve information about some of your resources. AWS has created an AWS management policy to allow you to enable GuardDuty within a region called Amazon GuardDuty full access. This policy allows full access to Amazon GuardDuty plus the ability to create a service-linked role. For full permissions to Amazon GuardDuty, you can create a custom policy which allows a full access to Amazon GuardDuty, the ability to enable the service, and permissions to update and manage trusted IP and threat lists. AWS offers another management policy entitled Amazon GuardDuty read-only access, and this provides read-only permissions to GuardDuty’s findings.</p>
<p>Following this section covering permissions, I then provided a demonstration where I looked at GuardDuty findings in greater detail.</p>
<p>Next, I looked at how Amazon GuardDuty can be used to bring benefit to the enterprise which included that it’s an intelligent threat detection service, it provides high-level security, regardless of deployment size, it has centralized management, there are no agents required, there are no upfront costs, and you can perform automation of remediation.</p>
<p>Next, I focused on the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/costing-1/">costing</a> of the service which offers a very simple charging method. Pricing for this service is broken down into two parts: CloudTrail event analysis and VPC flow log and DNS log analysis. CloudTrail event analysis is charged at one million events per month, and VPC flow logs and DNS logs are charged at per gig of log analyzed per month. And the cost varies depending on which region you have GuardDuty in. When you first enable Amazon GuardDuty, you are able to use the service free for the first 30 days. As an example of cost, the following table shows you the cost for the London, EU region. And the charges for this service are very simple to understand and estimate.</p>
<p>Finally, I looked at a number of different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-amazon-guardduty/partner-offerings-1/">partners</a> that seamlessly interact the services with Amazon GuardDuty. Some of these included Alert Logic Cloud Insight Essentials for AWS, CrowdStrike, and Trend Micro. For more information on these partners and others, the full listing can be found here. That now brings me to the end of this lecture and to the end of the course.</p>
<h1 id="3Components-and-Configuration"><a href="#3Components-and-Configuration" class="headerlink" title="3Components and Configuration"></a>3<strong>Components and Configuration</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail: An Introduction</a></p>
<p><a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types.html#actual-types">Amazon GuardDuty Finding Types</a></p>
<h1 id="5Managing-Permissions"><a href="#5Managing-Permissions" class="headerlink" title="5Managing Permissions"></a>5<strong>Managing Permissions</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_managing_access.html">AWS Policies for Access Management</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">Identity &amp; Access Management</a></p>
<h1 id="8Costing"><a href="#8Costing" class="headerlink" title="8Costing"></a>8<strong>Costing</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/pricing/">Amazon GuardDuty Pricing</a></p>
<h1 id="9Partner-Offerings"><a href="#9Partner-Offerings" class="headerlink" title="9Partner Offerings"></a>9<strong>Partner Offerings</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/marketplace/pp/B0764JH55Q?ref=_ptnr_AL_Website_CloudInsightEssentials_AWS_Marketplace">Alert Logic Cloud Insight Essentials (US)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.crowdstrike.com/products/falcon-intelligence/">Crowdstrike</a></p>
<p><a target="_blank" rel="noopener" href="https://www.trendmicro.com/aws/guardduty/">TrendMicro</a></p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/guardduty/resources/partners/">Full Partner List</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:14" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:14-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:02:46" itemprop="dateModified" datetime="2022-11-19T23:02:46-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Follow-Best-Practices-with-AWS-Trusted-Advisor-20/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:12" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:12-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:55:58" itemprop="dateModified" datetime="2022-11-19T22:55:58-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Using-AWS-Trusted-Advisor-to-Follow-and-Implement-Best-Practices-19/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to this course which will look at how you can use AWS Trusted Advisor to help you follow and implement some best practices and recommendations across your AWS environment with your organization.</p>
<p>Before we start I’d like to introduce myself, my name is Stuart Scott, and I am the AWS content and security lead here at Cloud Academy. Feel free to connect with me to ask any questions using the details shown on the screen, alternatively you can always get in touch with us here at Cloud Academy by sending an e-mail to <a href="mailto:&#x73;&#x75;&#x70;&#x70;&#111;&#114;&#x74;&#64;&#99;&#108;&#x6f;&#117;&#100;&#x61;&#x63;&#97;&#100;&#x65;&#x6d;&#x79;&#46;&#x63;&#111;&#109;">&#x73;&#x75;&#x70;&#x70;&#111;&#114;&#x74;&#64;&#99;&#108;&#x6f;&#117;&#100;&#x61;&#x63;&#97;&#100;&#x65;&#x6d;&#x79;&#46;&#x63;&#111;&#109;</a> where one of our Cloud experts will reply to your question.</p>
<p>This course has been designed for those who might be in one of the following roles:</p>
<ul>
<li>Security Professionals &amp; Security Auditors</li>
<li>Systems Engineers and Administrators</li>
<li>Compliance Managers</li>
<li>Anyone looking to learn to become AWS certified</li>
</ul>
<p>There are a number of learning objectives to this course, you will understand:</p>
<ul>
<li>The purpose and benefits of Trusted Advisor</li>
<li>How to navigate the Trusted Advisor Console</li>
<li>How to use Trusted Advisor to optimize your AWS resources and account</li>
<li>How to take actionable steps with Trusted Advisor to improve your AWS infrastructure</li>
</ul>
<p>As a prerequisite to this course, I recommend that you have a basic understanding of some of the core services, such as IAM. This is because AWS Trusted Advisor provides recommendations and best practices across a range of other AWS services.</p>
<h1 id="What-is-AWS-Trusted-Advisor"><a href="#What-is-AWS-Trusted-Advisor" class="headerlink" title="What is AWS Trusted Advisor?"></a>What is AWS Trusted Advisor?</h1><p>Hello and welcome to this lecture where I am going to be looking at AWS Trusted Advisor, explaining what it is and the different components that make up this service. </p>
<p>Trusted Advisor plays an integral part in helping you to optimize your infrastructure across a number of key areas, allowing you to make decisions upon recommendations made by the service which follow and best practices that have been honed over the years by AWS.</p>
<p>The service itself can be found within the AWS Management Console under the Management &amp; Governance category, alongside services such as Amazon CloudWatch, Control Tower and Systems Manager. </p>
<p>The main function of Trusted Advisor is to recommend improvements across your AWS account to help optimize and streamline your environment based on these AWS best practices. These recommendations cover 5 distinct categories:</p>
<ol>
<li><strong>Cost optimization</strong> - Helps to identify ways in which you could optimize your resources to help you reduce costs by implementing features such as reserved capacity and removing unused capacity</li>
<li><strong>Performance</strong> - This reviews your resources to highlight any potential performance issues across your infrastructure, determining if you could take benefits from performance-enhancing capabilities such as provisioned throughput</li>
<li><strong>Security</strong> - This analyses your environment for any potential security weaknesses or vulnerabilities that could potentially lead to a breach.</li>
<li><strong>Fault Tolerance</strong> - This helps to suggest best practices to maintain service operations by increasing resiliency, should a fault or incident occur across your resources.</li>
<li><strong>Service Limit</strong> - This identifies and warns you when your resources reach 80% capacity of their service limit quota.</li>
</ol>
<p>Within each of these 5 categories, Trusted Advisor has a list of control points and checks to see how your account, resources and architecture is implemented to determine if you’re aligned with best practice. So it essentially acts as an automatic auditor across your account, which can save you money, increase the efficiency of your resources, maintain a tighter and more secure environment, help to ensure your resources remain operational should a failure occur and that you remain in line with your service limitations, allowing you to request an increase where possible.</p>
<p>Between the 5 different categories and at the time of writing this course, there are over 115 different checks. Please note, that the number of these checks are constantly changing, so for the most up to date figures, please review the following link: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>
<p>Although there are a lot of these checks that Trusted Advisor can perform, not all of them are freely available to anyone with an AWS account. The list of checks that you have access to is very dependent on the support agreement with hold with AWS.</p>
<p>The full power and potential of AWS Trusted Advisor is only available if you have a Business or Enterprise Support Plan with AWS. Without either of these plans then you will only have access to 6 core checks in the security category and all the Service Limits </p>
<p>The 6 checks within security are as follows:</p>
<ul>
<li>S3 Bucket permissions</li>
<li>Security Groups - Specific Ports Unrestricted</li>
<li>EBS Public Snapshots</li>
<li>RDS Public Snapshots</li>
<li>IAM Use</li>
<li>MFA on root account</li>
</ul>
<p>At the time of writing this course, here are the available service limit checks.</p>
<p>Now if you compare this to the full list of checks here: <a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/</a></p>
<p>….that are included with Business and Enterprise support plans, you will see that the full checklist can provide a huge wealth of valuable information to help you optimise your infrastructure. </p>
<p>In addition to these extra checks that these support plans offer, you will also get the additional benefit of being able to administer certain functions of Trusted Advisor, such as:</p>
<ul>
<li>being able to track the most recent changes to your AWS account by bringing them to the top of your AWS Trusted Advisor dashboard.</li>
<li>using the AWS Support API to retrieve and refresh trusted advisor results. </li>
<li>Also you’ll have the added advantage of having Amazon CloudWatch integration to detect and react to changes made to your Trusted Advisor checks</li>
</ul>
<p>There are also a number of features that everyone has access to, including those outside of the Enterprise and Business support plans, these being:</p>
<ul>
<li><p><strong>Trusted Advisor Notifications</strong> - This is an opt-in or opt-out feature which is completely free to everyone and can be configured within the preferences pane of the Trusted Advisor console. It tracks your resource check changes and cost saving estimates over the course of a week and it will then email up to 3 recipients, for billing, operations and security notifications with a report.</p>
</li>
<li><p><strong>Exclude Items</strong> - This allows you to select specific resources to be excluded from appearing in the console within a specific check. You may want to do this if you are not interested in the reporting for that particular resource and so you decide to exclude it. You can decide to include it again at any point if you do change your mind. This feature can make viewing and managing your checks easier by eliminating some resources within the console.</p>
</li>
<li><p><strong>Action Links</strong> - Many of the items identified within the Checks against resources have hyperlinks associated, these are known as Action Links which allow quick access to the resource in question allowing you to remediate the issue identified. For example, if you reached 80% of the number of VPC’s within a Region, the <strong>‘VPC’</strong> Service Limit Check would highlight this as an issue. The Action Link against the resource would lead you to an AWS Support Center page to create a case to increase the quantity of VPCs you’re allowed within a single region.</p>
</li>
<li><p><strong>Access Management</strong> - AWS Trusted Advisor is tightly integrated within Identity &amp; Access Management. You can grant different levels of access to Trusted Advisor, including Full Access, Read Only, or even restrict access down to specific Categories, Checks and Actions. For example, the following IAM policy allows access to AWS Trusted Advisor, but denies the user from performing a refresh and updating notification preferences.</p>
</li>
</ul>
<p>For a full list of IAM permissions using the trustedadvisor namespace please see the following AWS reference: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html">https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html</a></p>
<ul>
<li><strong>Refresh</strong> - The data within Trusted Advisor is automatically refreshed if the data is more than 24 hours old when you view it within the console. However, after any refresh, you can perform a manual refresh 5 minutes after the previous refresh. You can either choose to perform a refresh against individual checks or against all checks.</li>
</ul>
<p>Before I finish this lecture I just want to give a high level overview of how Trusted Advisor works in a few simple steps:</p>
<ul>
<li>Once you connect to AWS Trusted Advisor, the service will scan your infrastructure </li>
<li>It will then compare the state of your infrastructure against best practices defined within the 5 categories of Cost Optimization, Security, Performance, Fault Tolerance and service limits</li>
<li>The output of this scan will generate a number of recommendations of how your infrastructure could be optimised with a priority factor</li>
<li>This then allows you to optimize your resources based on the recommendations</li>
</ul>
<p>AWS Trusted Advisor uses a service-linked IAM role to access you resources, named <strong>AWSTrustedAdvisorServiceRolePolicy.</strong> This is a predefined role created by AWS and allows the services to call other services on your behalf. The policy summary of this role is as shown here and helps to define which AWS services that Trusted Advisor communicates with.</p>
<p>Please be aware that this list will change over time, so for an updated list please refer to the role within IAM to determine which services <strong>AWSTrustedAdvisorServiceRolePolicy</strong> has access to.</p>
<h1 id="Reviewing-Checks-amp-Taking-Action"><a href="#Reviewing-Checks-amp-Taking-Action" class="headerlink" title="Reviewing Checks &amp; Taking Action"></a>Reviewing Checks &amp; Taking Action</h1><p>- Hello, and welcome to this lecture, while we shall be looking at how to review the status of your checks and take action against any findings identified by Trusted Advisor. Firstly, let me explain the dashboard, when you first go into Trusted Advisor, you’ll be presented with the five different categories. Beneath each of these categories, there are three icons, these being a tick, a triangle with an exclamation mark, and a circle with an exclamation mark. If any of these icons are Grey, it means there are no checks within that category that I’ve met an alert criteria to activate the icon. In this screenshot, the reason that so many of the icons are Grey, will be that my personal AWS account, is not covered by a business or enterprise support plan, and so those checks are locked as explained in the previous lecture. If a check does meet the criteria within the category, then one of the following will be presented. The tick box may become green, meaning no action is necessary for the check that has been reviewed, the triangle may become yellow meaning investigation required, and the circle may become red, identifying an item requires immediate action and attention. Next to each of these icons when active will be a number, and that number represents the amount of checks within that category, with a specific status. So as we can see from this image, I’ve one check within the service limits category, that should have been investigated as a priority. Five checks within the security category when no action is required, but I do have one check within this category that requires investigation. So that’s a very quick glance of the dashboard, I can see that I have a potential security threat that I need to look into, in addition to being very close to a service limitation, that I need to check immediately, as it could potentially cause a production issue. As mentioned previously, if you don’t have an enterprise or business support plan with AWS for your account, like in my example, then you will not be able to take full advantage of what Trusted Advisor has to offer. However, for this lecture I will review the six core checks that are freely available to anyone with an AWS account. Plus, I shall take a look at some of the service limit checks that are available. For every check that Trusted Advisor provides, it will provide four pieces of information. A description of the check and why it is used, the alert criteria, and this shows the conditions under which a check has given a green, no action necessary status, a yellow, investigation required status, or red, immediate action required status. Recommended action. This gives a high-level suggestion on the steps and actions that you could take to remediate any findings, based upon a yellow or red alert criteria. And additional resources, this highlights additional reading material for you to learn more about the topics being discussed. And this usually refers to AWS documentation or white papers. Let me now take a look at the free checks under the security category in more detail. Security groups, specific ports unrestricted. So this particular check, assesses your security groups that you have configured, and checks to see if you have any rules that allow an unrestricted source or destination, such as 0.0.0.0&#x2F;0. Having an unrestricted rules such as this, is not considered a best practice, as it is considered a security risk. And so you should aim to implement a tighter and more restrictive IP address range. However, some ports and protocols might be required to have an unrestricted setting, such as HTTP protective for web traffic on a web server. If you do have any secrets group roles that are exposed and fall within a yellow or red alert criteria, then it could lead to a security breach, allowing the intrusion of malicious activity, within your network and against your resources. When organizations are implementing security at the instance level using security groups, unrestricted access is often given to test or to help resolve incidents, to help identify where a problem might exist, and as a result, the correct and original source and destinations, are sometimes left exposed without intention. Trusted Advisor can help you identify these security groups, to make the necessary changes. By selecting this check, I can drill down into the different security groups, that have triggered the yellow or red status. As you can see from the screenshot, I’ve 13 security groups, that should be investigated. You may also notice that, the security groups listed have hyperlinks associated, and this will allow me to quickly select the security group which will take me directly to the configuration page, to change or modify the security group to make any alterations. If upon investigation I consider this cage group is configured correctly for the use intended, then I can choose to exclude the security group from any further reviews relating to this check. I just simply need to mark the security groups, and then select exclude and refresh. And this will remove the selected security group from any further reviews, carried out by this particular check. To then view the excluded items, I can just change the view from the included items to excluded items. I can then move these security groups back to the included group at any time. IAM use. The IAM use check simply ascertains if you’re using the identity and access management service. It recommends that you should have at least one user created to log in with, instead of operating and managing your AWS account using your route administrator account, which has the highest level of security privileges available. MFA on root account. Your root account has administrative level access to your AWS account. As a result, it is a very powerful account to use. And that such, logging in as the root account, should have additional levels of authentication, in addition to a password to verify the account. Added multi-factor authentication, MFA, to your root account, helps you protect your AWS account, so this check simply looks to see if you have activated MFA on your root account. MFA uses a random six digit number that is only available for a very short period of time before the number changes again, which is generated by an MFI device. There is no additional charge for this level of authentication, however, you will need your own MFA device, which can be a physical token or a virtual device. AWS provides a summary of all supported devices found here. Personally, I use Google Authenticator on my phone, because it is simple and easy to set up and configure. Before a user can authenticate with MFA, it must be configured and associated to the user from within IAM. Amazon EBS Public Snapshots. This check identifies if any of your elastic block store snapshots have been marked as public. When EBS snapshot is public, it is then accessible to all other AWS accounts and users within those accounts. With access to these snapshots, users can then access the data held within the snapshot. There may be circumstances where you need to allow other users or AWS accounts, access to specific snapshots. If this is the case, then he should mark the snapshot as private, and explicitly allow access on a per AWS account&#x2F;user level, rather than exposing all of the data to all accounts by marking it as public. For information on the elastic block store service, please see our existing course here. Amazon RDS Public Snapshots. This check performs exactly the same function as the Amazon EBS Public Snapshots but for your RDS snapshots instead of EBS. For more information on Amazon RDS, please see your existing course here. Service limit category checks. The checks within this category are used to assess when a service limit reaches 80% or more. Unfortunately, this doesn’t perform checks on all AWS services. As a recap from our previous lecture, it does support the following services and limits. It’s important to bear in mind that this list is changing all the time. AWS are constantly evolving and updating their services, and so over time, this list will change. For the most accurate and up-to-date list, please visit the link shown. If I look at a service from this list, such as Amazon Virtual Private Cloud, We can see that it will monitor the service limits of three different features relating to VPCs. These being elastic IP addresses, internet gateways and VPCs. So for each of these services or service features, you are allowed five per region. As a result, the service limit check will highlight if any of these thresholds get to four, which is 80%. The advantage of having this check, gives you enough time to either request an increased limit with AWS if possible or allowed, or you choose to simply reduce the number of AIPs, internet gateways or VPCs that you have. This may also force you to undertake some much needed housekeeping of your environment. I now want to perform a demonstration while I should provide an overview of the Trusted Advisor dashboard, and how to drill down into the issues or identified area. Within this demonstration, I will perform the following steps, navigate to AWS Trusted Advisor, provide an overview of the dashboard, drill down into the Trusted Advisor checks, identify and rectify the issues that are displayed, and refreshed Trusted Advisor to ensure the issues have been resolved. And then I’ll download the status of the checks as an Excel file for offline review. Okay, so I’m in my AWS Management Console, and if I scroll down to the management and governance section here, and I’ll be able to see Trusted Advisor just here. So if I select Trust Advisor, and that will take me to the Trusted Advisor dashboard which we can see here. Now we have our five categories our cost optimization, performance, security, full tolerance, and service limits. Now as I explained previously, I don’t have the enterprise or business support plans, so I’ll just have access to the free checks that are available. So there is six in security, and then all of the service limit checks. Now down here, the recommended actions, this will show all of the checks that I have access to. So it will be the six on the security and the 49 service limit checks here, and it will list them here. Now I’ll list them in a priority. So at the top here it is listed the one for action recommended, and then we have our investigation recommended here, and then we have lots of checks that are all absolutely fine. So there’s two items that I really need to check. Now here we can see, we have the orange triangle, with one investigation recommended, and over here and more importantly, we have one where the action is recommended. So we have an issue with a service limits and also a security issue as well. So let’s take a look at each of these to see if we can rectify them or the actions that we should take to try and rectify them. So firstly, the VPC. So let’s break this out a little bit to take a look. So we can see here that this check, checks for usage that is more than 80% of the VPC limit. Now on our alert criteria, we can see that if it’s yellow, then 80% of it has been reached, it turns red if a hundred percent of the limit is reached. So that tells me straight away that I’m unable to add any more VPCs in a specific region because I’m at a red alert and I’ve reached the maximum limit. So if I scroll down, I’ll be able to see exactly what region this is in, and we can see at the top here. So we have a red alert for the VPC service in EU West one. The limit amount was five and my current usage is five. So I can do two things, I can request a limit increase, and we can see here under recommended action, that if I click on request a limit increase, then it will take me straight to the support center page, where I can then request a service limit increase for the VPC. Or alternatively, I can go to my VPC configuration, in the EU West one Region and delete one of my VPCs. So if I swap over to VPC, and I’m in the region of EU West one, which has we know is where we have the issue, and I can see here that I have my five VPCs. So if I wanted to reduce these, I can simply select any of the VPCs that I want, and then go to actions and delete VPC, and that will take me down to four, which would be 80%, and I would come off the Radler, but I would go into the yellow because I would bet 80%. So like I say, you can either delete VPCs to reduce the current usage, down to a four, three, two whatever you need to, or request a limit increase if you do need all of these VPCs and would likely be needing more in that region. So that’s how we’d resolve the VPC check. Next up, we have our security groups, specific ports are restricted at a investigation required level. So let’s take a look at this. So this as we know, checks for any rules that allow unrestrictive access to specific ports. And if we look at the alert criteria, we’re at a yellow, so we have a security group that provides access to any other port that is unrestricted, that doesn’t fall into either the red or green categories. So let’s take a look at some of the VPCs that it’s detected. So it can see here, that 10 of the 44 security groups that I have, are meeting the investigation required category. So let’s go ahead and take a look at one of these security groups. Let me just pick this one for example, so I can select on the security group name, and it will take me straight to the configuration of that security group. Now, if we look at the details if we look at the inbound rules, we can see there that this is allowing SSH on port 22 from anywhere inbound, and the outbound rules are very open as well. So what we could do, we could edit these inbound rules, and change this from anywhere to just allow my IP address, and then save rules. Then if we go to the outbound, and say edit outbound rules, and again set the destination to only my IP address. If we save that. Now, if we go back to our Trusted Advisor, so as the launch, was it a security group name? So let me just highlight that check, now what I want to do is just refresh this check, to see if that is still an issue or to see if it has been resolved. So I can select the security group that in question, and go up to the check, and then say, refresh this check. So let’s do that now. Now we want to see if this is still an issue or if it’s been resolved. So as we can see it’s done a current refresh at the moment. Okay, so that’s refreshed, and we can see that that security group is no longer in the list, so we resolved the issue. So basically, I just went into the security group and removed the unrestricted access that it was granting from both the source and destination. Now I could do the same for each of the rest of these security groups, I can go into them, all the settings if need be, or leave them as they were, if that is how they were intended to be. And if they were supposed to have the open access and it’s not an issue, then I can simply select the checks and then select exclude and refresh again. No no, I can’t do this straight away because you have to wait five minutes before each refresh, but in another four minutes time, I will be able to simply exclude these from this check and they would move into the excluded items list. Okay, now the last thing I want to show you, is how to download the status of all the checks to an Excel file that you can then view offline. So at the top right-hand of the screen here, if you click on this download arrow, this will then download an Excel spreadsheet, and if we open that up, we can see here there’s a tab for every check, and it will print out the details for each of those checks, to allow you to view all of these issues offline outside of the management console. So here we have the security groups specific ports some restricted, and that we looked at just a moment ago, and we can see that we have all of the security groups that still have problems that security group IDs, and also the protocol status, et cetera. So that’s just another way of viewing your results of Trusted Advisor offline out of the management console. So that’s a very quick overview of the Trusted Advisor dashboard, the different categories and the checks, and how to look into some of those issues if you do have any alerts or investigation that’s required and how to resolve them.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello and welcome to this short lecture where I want to summarise some of the key elements of what we have covered throughout this introductory course.</p>
<p>I started off by explaining what the service is and what it does, here I explained that: </p>
<ul>
<li><p>The main function of Trusted Advisor is to recommend improvements across your AWS account to help optimize your environment based on AWS best practices</p>
</li>
<li><p>Trusted Advisor focuses on 5 categories with a list of best practice checks in each:</p>
</li>
<li><ul>
<li>Cost optimization </li>
<li>Performance </li>
<li>Security </li>
<li>Fault Tolerance </li>
<li>Service Limits</li>
</ul>
</li>
<li><p>The list of available checks to your account is dependant on your AWS Support Plan</p>
</li>
<li><p>Business and Enterprise support can take full advantage of all the checks available</p>
</li>
<li><p>All other AWS accounts only have access to 6 free core checks in the Security category, plus the Service Limit checks.</p>
</li>
</ul>
<p>The Security checks are:</p>
<ul>
<li>Security Groups - Specific Ports Unrestricted</li>
<li>Amazon EBS Public Snapshots</li>
<li>Amazon RDS Public Snapshots</li>
<li>IAM Use</li>
<li>MFA on root account</li>
</ul>
<p>There are a number of useful features within Trusted Advisor, these being:</p>
<ul>
<li>Trusted Advisor Notifications - This tracks your resource check changes and cost-saving estimates over the course of a week and e-mail you a report</li>
<li>Exclude Items - This allows you to select specific resources to be excluded from appearing in the console within a specific check. </li>
<li>Action Links - Action Links lead you on to remediate any issue identified</li>
<li>Access Management - Using IAM you can grant different levels of access to Trusted Advisor</li>
<li>Refresh - You can perform a manual refresh 5 minutes after the previous refresh against either individual checks or against all checks</li>
</ul>
<p>Following this, I focused on reviewing checks and taking appropriate action. </p>
<ul>
<li><p>A summary is provided for each category displaying how many checks require no action, how many need investigation, and how many should be looked at immediately</p>
</li>
<li><p>For every check that Trusted Advisor provides you will see:</p>
</li>
<li><ul>
<li>A description </li>
<li>Alert Criteria</li>
<li>Recommended Action</li>
<li>Additional Resources</li>
</ul>
</li>
</ul>
<p>This lecture also included a demonstration where I provided an overview of the dashboard and performed the following steps:</p>
<ul>
<li>Provided an overview of the dashboard</li>
<li>Drilled down into the Trusted Advisor Checks</li>
<li>Identified and rectified the issues that were displayed </li>
<li>Refreshed Trusted Advisor to ensure the issues had been resolved</li>
<li>Downloaded the status of the checks as an Excel file for offline review</li>
</ul>
<p>I also provided an overview of each of the core checks available to all AWS accounts.</p>
<p>You should now have a greater understanding of what AWS Trusted Advisor is and does and how you can use it within your environment to optimize your infrastructure. It is a powerful tool especially if you do have a Business or Enterprise support plan to fully maximize its potential.</p>
<h1 id="2What-is-AWS-Trusted-Advisor"><a href="#2What-is-AWS-Trusted-Advisor" class="headerlink" title="2What is AWS Trusted Advisor?"></a>2<strong>What is AWS Trusted Advisor?</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/">Best Practice Checklist</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awssupport/latest/user/security-trusted-advisor.html">Full list of IAM permissions</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Amazon-Inspector-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Amazon-Inspector-18/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Amazon-Inspector-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:10" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:10-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:55:02" itemprop="dateModified" datetime="2022-11-19T22:55:02-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Amazon-Inspector-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Amazon-Inspector-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello, and welcome to this course, where I shall be explaining what Amazon Inspector is and does, and how can you use this to help play a key part within your organization’s security policies and assessments.</p>
<p>Before we start, I would like to introduce myself. My name is Stuart Scott. I am one of the trainers here at Cloud Academy specializing in AWS, Amazon Web Services. Feel free to connect with me with any questions using the detail sheet on the screen. Alternatively, you can always get in touch with us here at Cloud Academy using the community form where one of our Cloud experts will reply to your question.</p>
<p>With Amazon Inspector being a security focus service, this course is beneficial to those who are responsible for managing and supporting AWS security, such as Security Architects, Security Assessment Managers, Operations Managers and Security Compliance Managers.</p>
<p>Understanding Amazon Inspector would also be of benefit for Application Developers and those working on Application Delivery. Finally, anyone looking to learn more about AWS Security in general, then this course would also be recommended.</p>
<p>This course will look at Amazon Inspector from the ground up, which cover the following lectures. What is Amazon Inspector? Here, I shall explain at a high level what Amazon Inspector is and why you may want to use it.</p>
<p>Components of Amazon Inspector. Within this lecture, I shall explain the main components of the service and how these fit together.</p>
<p>Then, I’ll provide a demonstration on how to configure Amazon Inspector. In this demonstration, I will cover many of the components that I would of discussed in the previous lecture, showing how to get started and how to configure the service.</p>
<p>I’ll then provide another demonstration on how to work with your findings. This lecture looks at how to view the different Amazon Inspector findings following an assessment.</p>
<p>Integration with CloudWatch and CloudTrail. This lecture explains how Amazon Inspector can be monitored with CloudWatch and CloudTrail.</p>
<p>Service Limitations and Costs. This lecture explains the limitations of the service in addition to how costings are calculated.</p>
<p>And then finally, a summary, and this lecture will summarize the key points learnt from the previous lectures within the course.</p>
<p>Once you have completed this course, you will have gained the knowledge to understand what Amazon Inspector is and does. You will know why you should implement Amazon Inspector within your environment, and you’ll understand how to configure Amazon Inspector. You’ll also know how to view and manage findings that Amazon Inspector detects, and understand the limitations, restrictions and costs of the service.</p>
<p>The only prerequisite of this course is that you are familiar with EC2 instances. Knowledge of the following services would be beneficial, but not essential. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">AWS Identity and Access Management</a>, IAM, Simple Notification Service, SNS, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/">Amazon CloudWatch</a> and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail</a>.</p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could use the comment section found on the landing page of this course.</p>
<p>That brings us to the end of this lecture. Coming up next, we start off by looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/what-is-amazon-inspector/">what Amazon Inspector is and does</a>.</p>
<h1 id="What-is-Amazon-Inspector"><a href="#What-is-Amazon-Inspector" class="headerlink" title="What is Amazon Inspector?"></a>What is Amazon Inspector?</h1><p>Hello and welcome to this lecture, where I answer the question of what <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector</a> is and does, and why you may want to use it. Amazon Inspector is a managed service that is used to help you find security vulnerabilities within your EC2 instances and any applications running on them during any stage of development and deployment.</p>
<p>This is automatically achieved for a series of assessments against specified resources, based on hundreds of best practices and known security weaknesses. Covering common vulnerabilities and exposures; The CVE is a publicly known reference list of security threats that are well documented.</p>
<p>Center for Internet SecurityBenchmarks. These benchmarks are continuously refined, and are used as global standards for best practices for protecting data and IT resources.</p>
<p>Security best practices, which look for weaknesses in common security best practices, and Runtime Behavior Analysis, which looks at the behavior of your EC2 instances during an assessment.</p>
<p>On assessment completion, a detailed assessment report can be produced which will highlight all of the findings, including any threats allowing you to make the necessary changes to resolve any security and compliance issues.</p>
<p>The Amazon Inspector service is agent based, meaning it requires software agents to be installed on any EC2 instances you want to assess. This makes it an easy service to be configured and added at any point to existing resources already running within your AWS infrastructure. This helps Amazon Inspector to become a seamless integration with any of your existing security processes and procedures as another level of security.</p>
<p>Through a level of customization of the vast knowledge base of best practices and vulnerabilities that is constantly updated that Amazon Inspector can call upon, you are able to select which packages are best for your use case, fitting into your own standards that your resources must adhere to. This allows you to customize the security for your environment, and ensures that any specific security loopholes are identified and addressed immediately.</p>
<p>Amazon Inspector records it’s assessments, which makes this a great service to present findings to auditors who may require to see evidence of security compliance and adherence to specific government controls.</p>
<p>Maintaining these records and reports helps you to maintain compliance that you may need for certifications such as PCI. So now we know at high level what the service is used for, why would we use the service? In the industry today we hear more and more about how the level of attacks and sheer quantity of hacking into small and large enterprise infrastructure in the attempt to steal and manipulate data is rising. New methods of cyber security attacks are being devised and as a result, new methods of prevention have to follow suit.</p>
<p>In a traditional data center deployment, most organizations have a level of intrusion detection and prevention plus monitoring systems in place at different levels within their infrastructure. However, not everyone has the same within the cloud. Security as a topic within the cloud is still the number one reason that prevents businesses from adopting the cloud. Much of this can be identified due to the lack of understanding, the correct skillset, and compliance.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> invests a huge amount of capital into security, and as a result, more and more security services and tools are being made available to us as customers, which is what spawned the creation of Amazon Inspector.</p>
<p>By using the Amazon Inspector service, we gain confidence in the level of security built into our applications and services due to the configurable assessments that we can run. The level of confidence not only benefits your organization, but your customers too. Having your service cross-checked for security compliance, threats, and vulnerabilities, ensures a reduction of attacks that your customer may be exposed to.</p>
<p>As you can see, this service offers some amazing benefits when looking at security compliance and reduction of exposure attack points within your infrastructure. Traditionally, to implement, manage, operate, and analyze your infrastructure resources and applications for these threats and best practices, would be difficult and take a very particular security focused skillset. This skillset, along with the systems and applications to implement such a service would come at a high cost to your business. The talent and budget may not be there for many organizations for this to happen. Thankfully, Amazon Inspector offers a solution that is lower in cost than that of a traditional solution.</p>
<p>As your organization grows, Amazon Inspector scales with it through the use of it’s agents. This allows repeatable and automated assessments to take place. With easy to understand assessment reports, it removes the highly skilled resource, that may have been required traditionally, to dissect and implement the necessary fix to resolve any findings.</p>
<p>That now brings us to the end of this lecture. Coming up next, I’m going to look at the service in greater detail, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/components-of-amazon-inspector/">identifying the components</a> used.</p>
<h1 id="Components-of-Amazon-Inspector"><a href="#Components-of-Amazon-Inspector" class="headerlink" title="Components of Amazon Inspector"></a>Components of Amazon Inspector</h1><h3 id="Resoures-amp-Links-used-within-this-Lecture"><a href="#Resoures-amp-Links-used-within-this-Lecture" class="headerlink" title="Resoures &amp; Links used within this Lecture"></a><strong>Resoures &amp; Links used within this Lecture</strong></h3><p><strong>IAM Course:</strong> <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/</a></p>
<p><strong>AWS Agent Installation:</strong> </p>
<p>Linux Install:</p>
<ul>
<li>wget <a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install">https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install</a></li>
<li>curl -O <a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install">https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install</a></li>
<li>sudo bash install</li>
</ul>
<p>Windows Install:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/windows/installer/latest/AWSAgentInstall.exe">https://d1wk0tztpsntt1.cloudfront.net/windows/installer/latest/AWSAgentInstall.exe </a></li>
</ul>
<p><strong>CVE Rules:</strong> <a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/rules-engine/CVEList.txt">https://s3-us-west-2.amazonaws.com/rules-engine/CVEList.txt</a></p>
<p><strong>Amazon Inspectors CIS Certificates:</strong> <a target="_blank" rel="noopener" href="https://www.cisecurity.org/partner/amazon-web-services/">https://www.cisecurity.org/partner/amazon-web-services/</a></p>
<p><strong>CIS Benchmarks:</strong> <a target="_blank" rel="noopener" href="https://www.cisecurity.org/cis-benchmarks/">https://www.cisecurity.org/cis-benchmarks/</a></p>
<p><strong>Security Best Practices:</strong> <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_security-best-practices.html">http://docs.aws.amazon.com/inspector/latest/userguide/inspector_security-best-practices.html</a></p>
<p><strong>Runtime Behavior Analysis:</strong> <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_runtime-behavior-analysis.html">http://docs.aws.amazon.com/inspector/latest/userguide/inspector_runtime-behavior-analysis.html</a></p>
<p>-—————————————————————————————————————————————————–</p>
<h3 id="Transcript"><a href="#Transcript" class="headerlink" title="Transcript"></a><strong>Transcript</strong></h3><p>Hello, and welcome to this lecture covering the different components that make up the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector service</a>. I should look at each component and how they link together to make the service function.</p>
<p>There are essentially nine different components and elements of the service. These are: the Amazon Inspector role, assessment targets, AWS agents, assessment templates, rule packages, assessment runs, telemetry, assessment reports, and findings.</p>
<p>Let me break down each of these to see which part they play within the service, starting with the Amazon Inspector role. When you first start using Amazon Inspector, you are required to create or select a role to allow Amazon Inspector to have read-only access to all of your EC2 instances. Without this role, the service would not have the relevant permissions to be able to gather the telemetry data of the instance during any assessment runs.</p>
<p>If you allow Amazon Inspector to create a role, then it will have a policy attached as detailed here. As you can see, this simply allows the role to have read-only access to all EC2 instances within your AWS account. For more information on IAM and IAM roles, please see our existing course here. Assessment Targets.</p>
<p>An Assessment Target is a grouping of AWS EC2 instances that you want to run an assessment against. This grouping of EC2 instances are managed and defined by the tags that are associated to your EC2 instance. Tagging is simply a way of adding metadata to your instances to help with management and organization, consisting of a key value pair.</p>
<p>For example, you might have a key of ‘project,’ with a value of ‘new app. ‘ Or a key of ‘department,’ with a value of ‘finance. ‘ The tags help you to create custom metadata of your infrastructure. When creating an assessment target, you are asked to select which keys from your tags that you would like to include within your Assessment Target. You can also refine your selection even further by providing the values for each of those keys, too. So you could have an Assessment Target configured as follows. This Assessment Target would then be associated to any instances which had either the project or department key with the corresponding values.</p>
<p>The EC2 instances are not required to contain both keys to be included within this Assessment Target. Only a match of one key is necessary. AWS Agents.</p>
<p>AWS Agents are essential to Amazon Inspector. These are software agents that must be installed on EC2 instances that you with to monitor and run the assessments on. Without this agent, Amazon Inspector would not be able to perform the analysis that it needs to. Once installed, the agent will be able to track and monitor data across the network file system, and any process activity of the instance. This data is then recorded as telemetry data, and is fed back to the Amazon Inspector service via the public endpoint of the service over a Transport Layer Securityprotected channel.</p>
<p>A regular heartbeat is sent from the agent to Inspector, which the Inspector service will respond to with instructions, such as to perform an assessment at a particular time. As the Agent is software-based, it is necessary from time to time to update the agent with the latest version. These new updates are managed and automatically installed by AWS, and so you don’t need to worry about the latest Agent software version.</p>
<p>The auto update of these agents are scheduled outside of any assessments that are due to take place. As this is a key component of Amazon Inspector, I now want to provide a demonstration on how to install these agents for both Linux and Windows operating systems. I will start by installing the agent on a Linux instance, and then a Windows instance.</p>
<p>I shall add all of these commands and links that I use to this lecture’s transcript, allowing you to copy and repeat if required.</p>
<p>Okay, so I’m within the AWS Management Console within the EC2 dashboard. And as you can see, I have two instances up and running. One of them is a Linux box, and another is a Windows box.</p>
<p>Now what I want to do is install the Amazon Inspector agents on each of these. So I’ll start with the Linux box, and it’s a very simple process. So to do this, what we need to do is download an agent installation script using either of these two commands, and then install the agent with this command, here.</p>
<p>So let’s go ahead and download and install the installation script. So if I go across to my instance, which I’m already connected to here. If I run the command. As you can see it’s a very quick install, and now it’s downloaded. I actually need to install the agent. So if I just type in this command here, sudo bash install, that will go ahead and install the Amazon Inspector agent for us.</p>
<p>And that’s the installation completed for a Linux box. So now if we swap over to our Windows box and install the agent in there. So for the Windows install, we need to go to this URL here to download the agent. So if I’m now to swap over to my Windows box, open up Internet Explorer and paste in that URL.</p>
<p>Click on okay. You may get a security warning about trusted sites, so just add that. . . And then try again. Click on okay. Save the download. Now the download is completed, we can then run the . exe file. And that will go ahead and install the agent. So if we agree to the license terms and conditions, and click on install, you can see the usual progress bar of the installation taking place.</p>
<p>That’s now successfully completed. And the agent is now installed. So that’s how you install the Amazon Inspector agents for both a Linux and Windows box.</p>
<p>An assessment template defines a specific configuration as to how an assessment is run on your EC2 instances. These configurable items within the template include the following.</p>
<p>Rules packages to be used during the assessment, and I’ll explain more on this later in this lecture. The duration of the assessment run, which can be 15 minutes, one hour, which is the recommendation by AWS, eight hours, 12 hours, or 24 hours. Specific SNS topics that Amazon Inspector should be used for any notifications, and these notifications can be when an assessment run starts, when it finishes, when it changes state, or when findings are reported.</p>
<p>Do be aware that you’ll need to allow access for Amazon Inspector to publish to the topic to enable these notifications within the topic policy. And any Amazon Inspector-specific attributes that can be assigned to findings generated by the assessment. So essentially, your findings can be tagged with keys and values of your choice. For example, with a key of ‘Assign To’ with a value of ‘Windows Security Team. ‘ Once your assessment template is created, you are not able to modify it again to make changes. You can, however, tag your assessment templates to help with the organization and management of your assessment runs. Rules Packages.</p>
<p>When Amazon Inspector gathers telemetry during an assessment run, it will then compare this data against specific security rules to ascertain its compliance. And these rules are grouped together in what is known as a rules package. So a rules package contains a number of individual rules that are each checked against the telemetry data back from the EC2 instance.</p>
<p>Each rule will also have an associated severity which will be one of the following. High. Should there be a finding with a rule with a severity of high, then this should be looked at immediately and rectified. This is the highest severity level, and as a result would likely compromise the integrity, confidentiality, and availability of your data and instants if left.</p>
<p>Medium. This is the next level down from high, and so any findings with a medium rule should be attended to in a timely manner. This severity also poses a risk to confidentiality and integrity. Low. Any rules highlighted with a low severity need to be rectified, but the urgency is not as severe as medium or high.</p>
<p>If any findings are returned with a high or medium rule, then these should be resolved as a matter of priority over any with a low severity. Informational. These would describe a particular security configuration within your assessment target, which you could then use to make changes to your environment to improve its effectiveness, or simply make a note for future reference.</p>
<p>The rule packages themselves are split across four different categories, these being: Common Vulnerabilities and Exposures, Center for Internet SecurityBenchmarks, Security Best Practices, and Runtime Behavior Analysis. Let me explain each of these in a bit more detail to understand what each of these rule packages look for during an assessment run.</p>
<p>Common Vulnerabilities and Exposures. The CVE is a publicly-known reference list of security threats that are well-documented. The rules used within this package will check the Assessment Target for exposure to any known security holes that would compromise the integrity, confidentiality, and availability of your EC2 instance.</p>
<p>Should any findings from an assessment be found against a CVE, it’s recommended you visit this site and search for specific vulnerability ID to gather additional detailed information to help you resolve and mitigate the issue. To check which CVEs the rules within the rules package are performing an assessment against, you can visit the following <a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/rules-engine/CVEList.txt">link</a>.</p>
<p>As new CVEs are found, they are added to this list by AWS, and the corresponding rules added to the rules package, preventing the need for you to stay up-to-date with the latest known security issues. Center for Internet Security Benchmarks. These benchmarks are continuously refined and used as global standards for best practices for protecting data and IT resources.</p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> is a CIS Benchmarks member company, and Amazon Inspector’s associated certifications can be found here. The rules within this rule package help to assess security for the following operating systems as per the certification <a target="_blank" rel="noopener" href="https://www.cisecurity.org/partner/amazon-web-services/">link</a>. If any findings are made against this rules package, then similarly to the CVE list, you can visit the following <a target="_blank" rel="noopener" href="https://www.cisecurity.org/cis-benchmarks/">link</a> to download the detailed description, explanation, and advice on how to mitigate the security issue found.</p>
<p>Security Best Practices. This rules package looks for weaknesses in common security best practices. However, this only applies to Assessment Targets that are running the Linux operating system. At this stage, it’s not possible to run this rules package on any target that has the marks of Windows OS. The following security checks are covered within this rules package.</p>
<p>Disable root login over SSH. This checks to see if the SSH daemon is configured to allow login into your instances root. Support SSH version two only. This checks to see if the instance is configured to run SSH protocol version one. Disable password authentication over SSH. This rule checks to see if password authentication over SSH is configured.</p>
<p>Configure password maximum age. This simply checks to see if a maximum age for a password has been configured on the instance. Configure password minimum length. Similarly to the previous point, this checks to see if a minimum password length has been configured on the instance.</p>
<p>Configure password complexity. This checks to see if the instance is using a password complexity mechanism for passwords.</p>
<p>Enable ASLR. This simply checks to see if address space layout randomization is enabled.</p>
<p>Enable DEP. This rule checks to see if data execution prevention is enabled on the instance.</p>
<p>And finally, configuration permissions for systems directories. This rule checks to ensure that only the root user has right access to system directories. For detailed information on each of these rules, see the AWS documentation found <a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_security-best-practices.html">here</a>. </p>
<p>During the creation of your assessments, you are able to select more than one rules package. And so if supported by your target assessments, you can run all four rules packages for deep security analysis of your resources. However, do be aware that not all the rules packages are available on all operating systems.</p>
<p>For example, and as I have already mentioned, the security best practice rules package is only available for the Linux OS. The table here shows a quick reference guide as to which rules packages are available for which operating systems. So do be aware of this when planning your security targets and templates.</p>
<p>Assessment Run. As assessment run can happen once you have configured your Amazon Inspector role, installed the agents and configured your Assessment Target and Assessment Templates. Once these components are in place, you are then able to run the configured assessment on your assessment targets. This process is known as the assessment run.</p>
<p>During this time, telemetry data will be sent back to Amazon Inspector and S3 to assess the data against the specified rules packages defined within the assessment template. Multiple assessment runs can be run at the same time, but only if the assessment targets do not have any duplicated EC2 instances within them.</p>
<p>During the assessment run, it is possible from within the management console to view the progress of the run in addition to stopping, starting, and deleting the run. Telemetry. I’ve mentioned telemetry a number of times already, and you are probably now aware of what this is. However, telemetry is a data that is collected from an instance, detailing its configuration, behavior and processes during an assessment run.</p>
<p>Once collected, the data is then sent back to Amazon Inspector in near-real-time over TLS where it is then stored and encrypted on S3 via an ephemeral KMS key. Amazon Inspector then accesses the S3 Bucket, decrypts the data in memory, and analyzes it against any rules packages used for that assessment to generate the findings.</p>
<p>After 30 days, this telemetry data is then deleted using a lifecycle policy attached to the dedicated Amazon Inspector S3 Bucket. Assessment Reports. On completion of an assessment run, it is possible to generate an assessment report which provides details on what was assessed, and the results of that assessment.</p>
<p>As this feature was only released at the end of April 2017, It’s only possible to generate these reports for any assessment runs that were completed on or after the 25th of April, 2017. There are two different types of reports that you can generate: a findings report, and a full report. The findings report will only contain a subset of the full report.</p>
<p>This will include a summary of the assessment that took place, a list of the EC2 instances that were within the assessment targets, the rules packages that were used from within the assessment template, and finally a detailed report on any findings that occurred. The full report, and as expected, contains all of the information from the findings report, in addition to a list of rules that were passed successfully for all the instances within the assessment target.</p>
<p>Findings. Findings are generated from the results of an assessment run. A finding is a potential security issue or risk against one of your EC2 instances within the assessment target. For each finding, an explanation of the issue is given, along with guidance on how to remediate the problem. More on findings will be given in a later lecture within this course.</p>
<p>That now brings us to the end of this lecture. And so to quickly recap, we looked at the following components: the Amazon Inspector role and the permissions required, assessment targets and how they use tagging of EC2 instances, AWS Agents and how they are installed on both Linux and Windows instances, assessment templates including configuration and the different rules packages available, when an assessment run can be performed, an understanding of telemetry data, the different types of assessment reports that are available, and lastly, an overview of what findings are.</p>
<p>Coming up in the next lecture, I’m going to demonstrate <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/how-to-configure-amazon-inspector/">how to configure Amazon Inspector</a>, which will include a lot of the components that I have mentioned in this lecture.</p>
<h1 id="How-to-Configure-Amazon-Inspector"><a href="#How-to-Configure-Amazon-Inspector" class="headerlink" title="How to Configure Amazon Inspector"></a>How to Configure Amazon Inspector</h1><p>Hello, and welcome to this lecture where I’m going to demonstrate how to configure <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector</a>.</p>
<p>For the purpose of this demonstration, I have a single Linux instance, and a single Windows instance running within a public subnet of a VPC. The instances already have the agents installed as per a previous demonstration.</p>
<p>I have also tagged the instances with the key of OperatingSystem, and value of Windows and Linux respectively. In this demonstration, I’m going to complete the following steps. I shall create and confirm the Amazon Inspector rule. Create two assessment targets. Define two assessment templates. Run two assessment runs simultaneously. Generate an assessment report for the assessment run. And automatically schedule future assessment runs via an AWS lambda function. So let’s get started.</p>
<p>Okay so as I just explained, I’ve already created two EC2 instances. A Linux box, and a Windows box. And they both have the agents installed already. And I’ve set up another tag called OperatingSystem with Linux and Windows tagged respectively.</p>
<p>So now I want to go across to Amazon Inspector to start configuring the service. So I go up to services, I’ve got a shortcut here already to get us in Inspector. But it’s also under the security identity and compliance section here. Okay so I’m now at the Amazon Inspector dashboard. And as you can see, I don’t have any findings or any assessments kind of running or completed at the moment.</p>
<p>If we go down to the account settings here, to manage the Amazon Inspector service rule, then we can see here that we can choose or create a new Amazon Inspector rule with the required permissions. So if we click on choose or create rule, it then takes us to a screen where it states that Amazon Inspector is requesting permission to use resources in your account. And all we need to do here is click allow in the bottom right-hand corner. But before we do, let’s take a look at the details.</p>
<p>So it gives it a rule description. Gives it a rule name of Inspector. And it will create a new policy when we click on allow. So let’s take a look at the policy document. As you can see, it allows the action of EC2 describe instances to all resources.</p>
<p>So effectively, read-only access to your EC2 instances. Okay, so that’s the policy document that’ll be associated with the rule. Then we click on allow. And that will go ahead and validate the IAM rule for Amazon Inspector. Okay so now we have the rule configured. If we click on cancel. Okay so the first thing to do is to set up some assessment targets.</p>
<p>Now I want to set up two assessment targets. One for the Windows boxes, and one for the Linux boxes. So if you click on create, and give it a name. Call this one Linux Assessment Target. And then we have the tags here. So we want all EC2 instances with the OperatingSystem key and a value of Linux to be included in this assessment target.</p>
<p>Then click on save. And we want to set up another one for the Windows box as well. So Windows Assessment Target. Again, OperatingSystem, but this time the value of Windows. So that will search all EC2 instances within my AWS account with the tag of OperatingSystem with a value of Windows. Click on save. So now we have our two assessment targets.</p>
<p>Now we’ll need to create our assessment templates. So if we click on assessment templates up here, go to create. Give this a name. So we’ll just call this one Linux. And here we can select our targets that we just created. So I want to select the Linux assessment target. Now here we can select which rule’s packages we want to run against our targets. So I’m going to select all of them here, all four. And then we can select the duration of the assessment run. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> recommends that it’s run for an hour. The longer the better. But you can run it for up to 24 hours. For the sake of this demonstration, I’m just going to leave it at 15 minutes. We should pick up a few <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/working-with-findings/">findings</a> with that.</p>
<p>And like I mentioned earlier as well, you can associate an SNS topic to be notified of any events. So let me associate this topic I created earlier. And I want to see events for when the assessment run is started. When it’s finished, when the state has changed, and any findings that are reported. And then down here we have attributes added to findings. So I’m going to say here assign to Linux Security Team. So if any findings are found, I want a tag to be associated with them with the key of assign to and a value of the Linux Security Team. So let’s create that.</p>
<p>And then we have our template with the target names, and those packages, and the SNS topics, et cetera.</p>
<p>Now I want to create another one for Windows. So I’ll call this one Windows. Set a target of the Windows assessment target. So now I’ll select the rules packages, and we know security best practices doesn’t run on Windows, so I’ll select all the other packages other than that one. Again, duration for 15 minutes.</p>
<p>And again, the SNS topic. And here I’m going to say assign to Windows Security Team. And create. Okay, so we now have two assessment templates. One for Windows, and one for Linux. Each with different rules packages assigned.</p>
<p>So now I want to run the assessment, and I want to run them both at the same time just because I know the EC2 instances do not overlap.</p>
<p>So I’m able to run them both. So I’ve selected the assessment templates. And then I click on run. And I have a message there saying assessment run started. And as you can see, we have information here saying it is now collecting data. And the number of runs, so it’s the first time this is running. So what you should do now is actively use both of those boxes for your applications, for loggin in, just as you would kinda stress test those boxes to try and uncover and unearth as many security flaws as possible.</p>
<p>So use them as much as you can within the assessment run. And that should uncover as many security flaws as possible. So what I’ll do now, I’ll pause the video, and I’ll come back to this when the assessment has finished.</p>
<p>Okay so the assessment run is now completed. And we can see that there are four findings for our Linux box and our Windows box has 230. So if we click on the four findings for the Linux boxes, that will take us over to our Findings section here. And we can see that we have one with a severity of medium, one at low, and two informational. And similarly if we go back to our Windows box, we can see all of our findings here and like I said, we have 230 of them.</p>
<p>So we have quite a lot of severity high which should be rectified immediately. I won’t go through on how to analyze the findings just yet. I’ll be doing that in a later lecture. So I just wanted to show you at this stage that the assessment has complete. And it’s found a number of findings for both assessment targets.</p>
<p>But what I will show you quickly is the reports. So as you can see on the far side here, we have reports, and we have an icon for each. So if we click on the one for Linux, here we can see, we can have two different reports. A findings report, or full report. The findings report just shows information about the findings.</p>
<p>Whereas the full report will contain all that same information plus all the rules that were passed as well. So let’s just have a look at the findings report as a PDF. So if we generate this report. We can see here that it’s formatted with a nice front cover. Gives you the date of which the report was generated and what assessment template was run, and the start and finish times as well. And it will just give you a quick summary of what happened during the assessment, and which rules packages we used. And also what was tested as well within those rules packages. So it’s a nice report to present to any auditors or internal security teams to nicely demonstrate what was found, what was run, and all the detailed information in between.</p>
<p>So let’s just go back to Amazon Inspector. So that’s how you configure Amazon Inspector, and how you create the assessment targets, the assessment templates, and generate an assessment run as well and view the reports.</p>
<p>Okay so what I want to show you now is how to automatically schedule your assessment runs based off your assessment templates.</p>
<p>And to do this, we need to use a different service. We need to use AWS Lambda. So let’s go across to the Lambda service. We will click on Dashboard. Then we need to create a Lambda function. From here, we need to select a blueprint. And if we just type in inspector, then it will filter out all the blueprints that AWS has already created for us.</p>
<p>And here we’ve got the Inspector scheduled run. Which schedules a recurring Amazon Inspector assessment run. So that’s what we need. We’ll need to create a rule name. So create a new rule. And we’ll call this Amazon Inspector demo. Description, daily run. And then we need to enter a scheduled expression. So I want this to run everyday. And there’s a couple of examples under here on how to represent days and times, et cetera. And if we enable the trigger. Then go to next.</p>
<p>Give this Lambda function a name. We’ll call this AI demo. We can leave the description as is. Leave the runtime as NodeJS. And this is all the code within the Lambda function based off of the template that we selected.</p>
<p>But we do need to add an environment variable here. So we need to look for our assessment template ARN. So if we go back to Amazon Inspector. Select our Windows assessment template. And we can see here, we have an ARN for this template. So if we copy and paste that into the variable. If we then go down to the handler, we’ll leave the handler as index-handler as the default. We’ll then need to create a rule for permissions to run this function. So if we create a new rule from template, then Lambda will automatically create an SO permissions for us. We’ll give this rule a name. Lambda demo. And then all we need to do at this point now, is click on next.</p>
<p>This is just a review of the options. We create the function. And that’s it. It’s created. Congratulations, your Lambda function has been successfully created. So now everyday, my Windows assessment target will be automatically run for me.</p>
<h1 id="Working-with-Findings"><a href="#Working-with-Findings" class="headerlink" title="Working with Findings"></a>Working with Findings</h1><p>Hello and welcome to this lecture. What I want to show you, how to work with Findings from your Assessment Runs, that you may find. I thought that this lecture might be easier if we continue on from the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/how-to-configure-amazon-inspector/">previous demonstration</a>, so let’s get back to the environment.</p>
<p>Okay, so we’re back in the console, looking at our two Assessment Runs, that we had and just as a quick reminder, for the Linux template, we had four Findings and for the Windows, we had 230, so I just kind of want to run through how you can look at the Findings in a bit more detail and the information that they provide.</p>
<p>If we go across to the left-hand side here under Findings, there’s a number of Severity Filters, High, Medium, Low and Informational, so if we take a look at the High Severity Findings first. Now, we can see that there’s 227 of these, that the Assessment has found and we can see that the majority of them are against the Windows template, so let’s have a look at one of these Findings, just to see what information we have.</p>
<p>So, if we expand the Finding and get a bit more detail, this will give us the ARN of the Finding, the Run Name, the Target Assessment, that the Finding was run against and which template was used within the Assessment as well and the Start and End Time of the Assessment Run and the Current Status. As we scroll down, we can also see which Rules Package that this Finding came from, so this came from the CIS Benchmarks and it also gives the ID of the Instance as well, I mean, if we was to click on this, for example, it’ll take us straight to that Instance, as we can see there, it’s the Windows box, so there’s a number of hyperlinks, that you can kind of access the Target etc and the template, if you needed to to get more information.</p>
<p>Right, in this section here, the Finding, this is actually the issue that it found and then it raised, so here you can see, it explains that this Instance is not compliant with a specific rule within the Rules Package and it explains that you need to ensure there’s a minimum password age is set to one or more days and this is a requirement against the CIS Benchmark for this Windows server, 2012, so that’s been highlighted as Severity of High.</p>
<p>If we go down to the Descriptions section, it gives us further details again and again, this talks about the minimum password age, it needs to be set to more than one day, it says you can go up to a maximum of 999 days and it also gives a rationale behind the reasoning for this Finding, so there’s a quite a lot of information there to get an understanding of why the Finding has been found and why there should be Recommendations to rectify it and then finally with regards to Recommendations, it does give a Recommendation on what you should do to resolve the issue and here it says you need to establish the recommended configuration, you need to set the following UI path to one or more days and then here, it’s also given us the path as well, so it’s very easy for us to get an understanding of what the issue is, why it’s an issue and what we need to do to remediate the problem.</p>
<p>So, it’s quite a lot of detail within the Findings. Let’s close that one up and again, if we go through any other Finding, it’ll be similar kind of information, again a Description, a rationale and a Recommendation as well.</p>
<p>So, if we look at some of the Medium Severities, so it’s only found one Medium Severity issue here against the Linux box, so if we take a look at that, again we have the ARN, the Run Name, Target and template name and again, what’s important is to know which Rules Package this has come from, so it’s come from the Security Best Practices and against which Agent as well and the Finding here explains that it’s configured to allow users to log in with root credentials over SSH, which increases the likelihood of a brute force attack and again it’s given a Recommendation as to what to do to resolve the issue and it explains here a couple of commands to disable SSH root logins, so again, very good information, very useful and a Recommendation on how to resolve the problem.</p>
<p>So, let’s take a look at if we’ve got any Low Findings and yes, we have a couple, we have one for Windows and one for Linux against Behavior Analysis, Runtime Behavior Analysis, so let’s take a quick look. Again, all very similar information, we have the Rules Package there, the Agent and the Finding as well, it’s saying that there are insecure protocols used to connect to the remote host and again, a Recommendation of replacing those insecure protocols with encrypted versions and if we look at the Linux Finding, we can also see here that it’s the same issue, that insecure protocols were used to connect to the remote host.</p>
<p>So that’s just a couple of Low Findings there, but if it was in a production environment, then you would definitely look to resolve all the High Severities first and then the Mediums and then work on the Lows afterwards and then finally we have Informational and we have a couple of items here for both boxes, Windows and Linux, so if we take a look at one of the Windows, let’s see what it says here. It just explains in the Finding, that this agent was listening on TCP ports, but no connections were using those ports during the Assessment Run, so the Recommendation is to disable any network services, that we don’t use, so we don’t expose those ports and we can reduce the attack surface area of our deployment.</p>
<p>So again, these are just Informational, we can take action upon that, if we need to, but it’s not a great risk to us.</p>
<p>What you can also do as well is download this information by clicking on this button here and it will export the data as a CSV file, so let’s export all columns and take a look. So this opens the CSV file of a lot of the Findings information stood, Severity, Date, the actual Finding itself, which is the important part, which Targets these were found on and which template and against which Rules Package and again, we have the ARN Rule, agent ID, etc. So you might want to export some of those Findings into a CSV file to kind of work against them and maybe track them a little easier and pass this document around to other members within the team.</p>
<p>So let’s go back to the console. So that’s essentially it for Findings, they’ll all be found on this left-hand side in this menu under Findings and then you can Filter them as required against specific Targets or templates or even Rules Packages as well, if you’re just interested in a particular Rules Package, then you can just break those down individually.</p>
<p>Okay, then that’s the end of the demonstration.</p>
<h1 id="Integration-with-CloudWatch-amp-CloudTrail"><a href="#Integration-with-CloudWatch-amp-CloudTrail" class="headerlink" title="Integration with CloudWatch &amp; CloudTrail"></a>Integration with CloudWatch &amp; CloudTrail</h1><p><strong>Resources &amp; Links used in this Lecture</strong></p>
<p><strong>Amazon CloudWatch Course:</strong> <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/">https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/</a></p>
<p><strong>AWS CloudTrail Course:</strong> <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/</a>**<br>**</p>
<p><strong>———————————————————————————————————————————————</strong></p>
<p><strong>Transcript:</strong></p>
<p>Hello and welcome to this very short lecture where I want to show you how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector</a> integrates with Amazon CloudWatch and AWS CloudTrail.</p>
<p>For more information on both of these services, please see our existing courses <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">here</a> on both services.</p>
<p>Being able to monitor and track the performance of a service and trace its actions from an audit perspective is invaluable. And Amazon Inspector integrates seamlessly with both CloudWatch and CloudTrail which performs these functions.</p>
<p>You can use Amazon CloudWatch to monitor and inspect agents against specific metrics for both assessment targets and assessment templates as shown. There is also an aggregate metric which will count the number of assessment runs within your account. These metrics allow you to monitor the performance and activity of Amazon Inspector, providing a greater insight into specific statistics.</p>
<p>From an AWS CloudTrail perspective, all API calls that are performed by Inspector are logged with CloudTrail. This allows you to understand by who, what, and when specific actions were carried out, which are all saved and recorded within the CloudTrail log files that are stored in S3.</p>
<p>These integration elements allow for additional auditing and monitoring to take place throughout your security assessments within your environment which is always an added benefit from a governance and security audit perspective. That now brings us to the end of this lecture.</p>
<p>Coming up next, I want to highlight the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/service-limitations-and-costs/">limitations of the Amazon Inspector service</a>.</p>
<h1 id="Service-Limitations-and-Costs"><a href="#Service-Limitations-and-Costs" class="headerlink" title="Service Limitations and Costs"></a>Service Limitations and Costs</h1><p>Hello and welcome to this lecture. We’re going to explain the different service limitations of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/introduction-82/">Amazon Inspector</a> as well as how much it’s going to cost to use and implement within your own environment.</p>
<p>Across the service there are a number of limitations that you’ll need to be aware of.</p>
<p>Agents per assessment. When creating your target assessments the maximum number of agents that can be included is 500. Be aware that this limit can’t be increased. The number of assessment runs.</p>
<p>There is a default limit of 50,000 assessment runs that you can have per account. If you need this limit to be increased then you need to contact the AWS Customer Support to do so.</p>
<p>The number of assessment templates. Again, by default, there is a limit of 500 assessment templates that can assist in an AWS account. If this limit needs to be increased, then again, you can contact AWS Customer Support.</p>
<p>The number of assessment targets. This has a default limit set to 50, which can also be increased if required to do so.</p>
<p>Although I mentioned this earlier in a previous lecture, it’s worth noticing the limitation of rule packages against specific operating systems.</p>
<p>So how much will using Amazon Inspector cost you to gain all of this additional security information within your environment? Which is always an important factor for any service. For the benefit it brings to you and your environment, I think it’s very cost-effective.</p>
<p>Essentially, Amazon Inspector is priced at per-agent, per-assessment run, which is an agent assessment per month. For example, if you were to run one agent against 20 assessments that would be 20 agent assessments, or two agents against five assessments, that would be 10 agent assessments. There are no other costs associated within Amazon Inspector so there are no up front or on-going maintenance costs.</p>
<p>The pricing for agent assessments per month starts at 30 cents but there is capacity for a discount with the more agent assessments run per month.</p>
<p>That now brings us to the end of this lecture. Coming up in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/summary-10/">last lecture</a>, I will summarize the main points that we have learned from each lecture.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello and welcome to this lecture where I want to briefly summarize the key points that we have learnt throughout the previous lectures.</p>
<p>I started off by looking at <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/what-is-amazon-inspector/">what Amazon Inspector is</a> and what it does. Here I explained that Amazon Inspector is a managed service to help find security vulnerabilities within your applications and services. It uses hundreds of best practices and known security weaknesses to assess EC2 instances. Any findings are detailed to allow you to rectify the security risk within your environment. Amazon Inspector provides confidence in the level of security built into your applications and services. Threats and vulnerabilities can be reduced when using Amazon Inspector.</p>
<p>Next I explained the different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/components-of-amazon-inspector/">components that make up the Amazon Inspector service</a>. The Amazon Inspector role. This role has read only access to all EC2 instances within your AWS account allowing you to run assessments.</p>
<p>Assessment targets. This is a grouping of AWS EC2 instances that you want to run an assessment against, which are grouped together using tags.</p>
<p>AWS agents. These are software agents that are installed on your EC2 instances that need to be assessed to track and monitor data across the network, file system, and any process activity of the instance.</p>
<p>Assessment templates. These templates define a specific <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/how-to-configure-amazon-inspector/">configuration</a> as to how an assessment is run on your EC2 instances.</p>
<p>Rules packages. Rules packages contain a number of individual rules which are individually checked against the telemetry data that comes back from the assessment. The rules packages are common vulnerabilities and exposures, Center for Internet Security benchmarks, and security best practices.</p>
<p>Assessment run. Once you have configured your Amazon Inspector role, installed the agents, and configured your assessment target and assessment templates, you can run the configured assessment on your assessment targets, which is known as the assessment run.</p>
<p>Telemetry. This is the data that is collected from an instance detailing its configuration, behavior, and process during an assessment run.</p>
<p>Assessment reports. On completion of an assessment run, an assessment report can be generated providing details on what was assessed and all the results of that assessment. This is available as either a findings report or a full report.</p>
<p>Findings. A finding is a potential security issue or risk against one of your EC2 instances within the assessment target following an assessment run.</p>
<p>Once there is an understanding of the different components of the Amazon Inspector service, I performed a demonstration where I showed you how to create and select an Amazon Inspector role, create an assessment target, define an assessment template, complete an assessment run, generate an assessment report, and automatically schedule future assessment runs via an AWS lambda function.</p>
<p>This was followed up by showing you how to review your findings through the use of assessment reports, filtering of findings, viewing the findings in a detailed view, remediation recommendations and steps, tagging of findings to allow you to manage the workflow of resolution.</p>
<p>I then looked at how Amazon Inspector has integration with Amazon CloudWatch and AWS CloudTrail.</p>
<p>Within CloudWatch there are a number of metrics for assessment targets, assessment templates, and aggregate metrics. From an AWS CloudTrail perspective, all API calls that are performed by Inspector are logged with CloudTrail.</p>
<p>Finally, I looked at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-inspector/service-limitations-and-costs/">service limitations and costings</a>. From a limitation perspective, a maximum number of agents per assessment is 500. There is a default limit of 50,000 assessment runs per AWS account. A default limit of 500 assessment templates are allowed per AWS account, and a default limit of 50 for assessment targets. Pricing is configured as follows. Amazon Inspector is priced at per agent, per assessment run, which is an agent assessment, per month. There are no other costs associated with Amazon Inspector, so there are no upfront or ongoing maintenance costs. And pricing for agent assessments start at $0. 30 per month.</p>
<p>You should now have a good understanding of Amazon Inspector and what it is used for and how it can use best of service within your own environment to help you mitigate against known security vulnerabilities and exposures to ensure your environment stays as secure as it can be.</p>
<p>If you have any feedback on this course, positive or negative, please do leave a comment on the course landing page. We do look at the comments and your feedback is greatly appreciated.</p>
<p>Thank you for your time and good luck with your continued learning of cloud computing.</p>
<p>Thank you.</p>
<h1 id="3Components-of-Amazon-Inspector"><a href="#3Components-of-Amazon-Inspector" class="headerlink" title="3Components of Amazon Inspector"></a>3<strong>Components of Amazon Inspector</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/">IAM Course</a></p>
<p><a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/linux/latest/install">Linux install</a></p>
<p><a target="_blank" rel="noopener" href="https://d1wk0tztpsntt1.cloudfront.net/windows/installer/latest/AWSAgentInstall.exe">Windows install</a></p>
<p><a target="_blank" rel="noopener" href="https://s3-us-west-2.amazonaws.com/rules-engine/CVEList.txt">CVE rules</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cisecurity.org/partner/amazon-web-services/">Amazon Inspectors CIS Certificates</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cisecurity.org/cis-benchmarks/">CIS benchmarks</a></p>
<p><a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_security-best-practices.html">Security Best Practices</a></p>
<p><a target="_blank" rel="noopener" href="http://docs.aws.amazon.com/inspector/latest/userguide/inspector_runtime-behavior-analysis.html">Runtime Behavior Analysis</a></p>
<h1 id="6Integration-with-CloudWatch-amp-CloudTrail"><a href="#6Integration-with-CloudWatch-amp-CloudTrail" class="headerlink" title="6Integration with CloudWatch &amp; CloudTrail"></a>6<strong>Integration with CloudWatch &amp; CloudTrail</strong></h1><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-cloudwatch/introduction-39/">Amazon CloudWatch course</a></p>
<p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail course</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:09" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:09-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:03:26" itemprop="dateModified" datetime="2022-11-19T23:03:26-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Compliance-Check-Using-AWS-Config-Rules-Managed-Custom-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-AWS-Config-An-Introduction-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-AWS-Config-An-Introduction-16/" class="post-title-link" itemprop="url">AWS-Security-Specialty-AWS-Config-An-Introduction-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:07" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:07-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:16" itemprop="dateModified" datetime="2022-11-19T22:53:16-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-AWS-Config-An-Introduction-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-AWS-Config-An-Introduction-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Hello and welcome to this course covering AWS Config. This service is classed as a management tool service which is fully managed and allows you to have visibility of your entire AWS infrastructure from a configuration perspective. As well as using the service to act as a resource inventory, compliance checker, and manage configuration changes of your resources it can also be used as a part of your security analysis procedure.</p>
<p>Before we go any further, I’d like to introduce myself. My name is Stuart Scott. I am on of the trainers here at Cloud Academy specializing in AWS Amazon web services. Feel free to connect with me with any questions using the detail shown on screen. Alternatively, you can always get in touch with us here at Cloud Academy using the community form where one of our cloud experts will reply to your question.</p>
<p>This course will be beneficial for people who are responsible for managing resource configuration changes within an AWS cloud environment. Also auditors who must have an awareness of all AWS infrastructure and their current configurations and compliants as well as security engineers who are responsible for implementing AWS security and analyzing logs and identifying weaknesses and breaches to their environment.</p>
<p>Throughout this course we are going to be taking a deep look at AWS config and as a result the following areas are going to be discussed. Starting with:</p>
<p>What is AWS Config? Within this lecture, we will understand exactly what the service is and what function it provides.</p>
<p>Next we’ll take a look at key components. This lecture breaks down the service to allow us to look at all the components and their relationships to each other and the role they play as a part of the AWS Config service.</p>
<p>Then we’ll look at service integration. This lecture will look at how the AWS Config service integrates with other AWS services.</p>
<p>Following this, we’ll look at how to manage compliance with AWS Config. Here we will focus on how to maintain compliance using AWS config whether these be internal or external requirements or standards.</p>
<p>And then finally, we’ll look at some use cases and best practices. And this lecture will focus on some of the use cases of when it’s best to use AWS Config to help you maintain, support and operate your AWS environment.</p>
<p>On completion of this course, you will be able to recognize and explain how the AWS Config service can be used by AWS customers to monitor environmental changes. You’ll be able to recognize and explain the core elements of the AWS Config service. You’ll be able to comfortably configure and implement AWS Config within your own environment and you’ll understand how to maintain compliance of your resources using AWS Config. For example, to help maintain security policies.</p>
<p>Although you do not need to be an AWS expert to appreciate and benefit from this course, students will benefit from having a basic understanding of cloud computing and the AWS platform. So we recommend completing the What is Cloud Computing course along with the AWS Fundamentals Learning path. A high level awareness of the following added services would also be beneficial but not essential. Simple notification service, SNS, simple queue service, SQS, simple storage service, S3, AWS CloudTrail, and identity and access management.</p>
<p>Feedback on our courses here at Cloud Academy are valuable to both us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could use the comments section found on the landing page of this course.</p>
<p>That brings us to the end of this lecture. Coming up next, we’ll answer the question what is AWS Config?</p>
<h1 id="What-is-AWS-Config"><a href="#What-is-AWS-Config" class="headerlink" title="What is AWS Config?"></a>What is AWS Config?</h1><p>Hello, and welcome to this lecture, where we will talk about the AWS Config service itself, what it is, and what it does. So let’s get started.</p>
<p>As many of you will be aware, one of the biggest headaches in any organization when it comes to resource management of IT infrastructure is understanding the following. What resources do we have? What devices are out there within our infrastructure performing functions?</p>
<p>Do we have resources that are no longer needed, and therefore, can we be saving money by switching them off?</p>
<p>What is status of their configuration? Are there any security vulnerabilities we need to worry about?</p>
<p>How are our resources linked within the environment? What relationships are there, and are there any dependencies? If we make a change to one resource, will this affect another?</p>
<p>What changes have occurred on the resources, and by whom? Do we have a history of changes for this resource that shows us how the resourced changed over time?</p>
<p>Is the infrastructure compliant with specific governance controls, and how can we check to ensure that this configuration is meeting specific internal and external requirements?</p>
<p>And, do we have accurate auditing information that can be passed to external auditors for compliance checks?</p>
<p>Depending on the size of your deployment with AWS, trying to answer some of these questions can be very time consuming and laborious. Some of this information can be captured via the AWS CLI by performing a ‘describe’, or ‘list’, against the specific resource. But implementing a system to capture those results and output them into a readable format could be very resource intensive. And of course, this will only help you with a small piece of the puzzle.</p>
<p>AWS is aware that, due to the very nature of the cloud and its benefits, the resources within an AWS environment are likely to fluctuate frequently, along with the configurations of the resources. The cloud, by its very nature, is designed to do so, and so trying to keep up with the resource management can be a struggle. Because of this, AWS released AWS Config to help with this very task. The service has been designed to record and capture resource changes within your environment, allowing you to perform a number of actions against the data that helps to find answers to the questions that we highlighted previously.</p>
<p>So what did AWS design AWS Config to do? Well, in a nutshell, AWS Config can capture resource changes, so any change to a resource supported by Config can be recorded, which will record what changed along with other useful metadata, all held within a file known as a configuration item, a CI.</p>
<p>It can act as resource inventory. AWS Config can discover supported resources running within your environment, allowing you to see data about that resource type.</p>
<p>It can store configuration history for individual resources. The service will record and hold all existing changes that have happened against the resource, providing a useful history record of changes.</p>
<p>It can provide a snapshot in time of current resource configurations. An entire snapshot of all supported resources within a region can be captured that will detail their current configurations with all related metadata.</p>
<p>Enable notifications of when a change has occurred on a resource. The Simple Notification Service, SNS, is used with AWS Config to capture a configuration stream of changes, enabling you to process and analyze to changes to resources.</p>
<p>It can provide information on who made the change and when through AWS CloudTrail Integration. AWS CloudTrail is used with AWS Config to help you identify who made the change and when, and with which API.</p>
<p>You can enforce rules that check the compliancy of your resource against specific controls. Pre-defined and custom rules can be configured with AWS Config, allowing you to check resources’ compliance against these rules.</p>
<p>You can perform security analysis within your AWS environment. A number of security resources can be recorded, and when this is coupled with rules relating to security, such as encryption checks, this can become a powerful analysis tool, and it can provide relationship connectivity information between resources. The AWS Management Console provides a great relationship query, allowing you to quickly see and identify which resources are related to any other resource. For example, when looking at an EBS volume, you will able to see which EC2 instance it is connected to, and it does all of this and presents the data in a friendly format. This is a lot of incredibly useful data that can be used across a range of different scenarios, some of which we will cover later in this course.</p>
<p>AWS Config is region specific, meaning that if you have resources in multiple regions, then you will have to configure AWS Config for each region you want to record resource changes for. When doing so, you are able to specify different options for each region. For example, you could configure Config in one region to record all supported resources across all services within that region, and then add a pre-defined AWS-managed config rule that will check if EBS volumes are encrypted. In another region, you could select to only record a specific type of resource, such as security groups, with no pre-defined rules allocated.</p>
<p>Some of you may be wondering, what if the service you want to monitor is not region specific, such as IAM? Well in this case, there is a separate option to include global services, which IAM falls under.</p>
<p>Now we have an understanding of what AWS Config is used for, and what it does. In the next lecture, we will introduce the different components that make up the service, showing what each of them do and how these come together to deliver the service features.</p>
<h1 id="Key-Components"><a href="#Key-Components" class="headerlink" title="Key Components"></a>Key Components</h1><p>Hello and welcome to this lecture where we will be taking a look at the components that make up the AWS Config Service and the functions that each of them carry in delivering the service.</p>
<p>To understand how to get the most of this service, it’s important to understand how the service is pieced together and how it works. So let’s start by identifying the key components. Following this we will then break each of them down to understand their role and how they sit within the service.</p>
<p>The following identifies the main components to the service. AWS resources, configuration items, configuration streams, configuration history, configuration snapshots, configuration recorder, config rules, resource relationships, SNS topics, S3 buckets and AWS config permissions.</p>
<p>Let’s now take a look at each of these in turn in more detail starting with AWS resources. As discussed in the previous lecture, AWS resources are typically classed as objects that can be created, updated or deleted from within the AWS Management Console, or programmatically using the AWS CLI or supported SDK. AWS Config records changes to supported AWS resources within a specific region. For the current list of supported services and resources, please follow the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-resources">link</a> on the screen.</p>
<p>A configuration item or CI as it’s known, is a key component of AWS Config. It is comprised of a JSON file that holds the configuration information, relationship information and other metadata as a point-in-time snapshot view of a supported resource. All the information that AWS Config can record for a resource is captured within the CI. A CI is created every time a supported resource has a change made to its configuration in any way. In addition to recording the details of the affected resource, AWS Config will also record CIs for any directly related resources to ensure the change did not affect those resources too.</p>
<p>For example, if there was a rule change to a security group, additional rules could be added with new ports. AWS Config would record all CI information for that resource, but it would also gather CI information for any instances that were a part of that security group. The CIs would then be sent to a configuration stream, which we will cover next.</p>
<p>As so much data is gathered within these CIs, it’s important we look at this in further detail. So for every CI generated, they’ll be five different sections.</p>
<p>Firstly, metadata. This essentially contains details about the configuration item itself. So within this metadata, we have both a version ID and a configuration ID, which uniquely identifies the CI. In addition to this, other information can include an MD5Hash that allows you to compare other CIs already recorded against the same resource, as well as ensuring there are no duplications. And then we have the time of the capture and state ID, which puts the CI for a particular resource into an order of sequence, which is useful when multiple CIs for the same resource are sent the configuration stream.</p>
<p>Attributes. This holds common attribute information against the actual resource. Within this section, we also have a unique resource ID, and any key value tags that are associated to the resource. The resource type is also listed. For example, if this was a CI for an EC2 instance, the resource types listed could be the network interface, or the elastic IP address for that EC2 instance. The Amazon resource name, the ARN, for the resource would also be shown along with the availability zone that the resource belonged to. Bear in mind that for services and resources that are not fixed to a particular availability zone, such as IAM and IAM roles, then this section would not be applicable. Lastly, the time the resource was created would also be given.</p>
<p>Relationships. This holds information for any connected relationship that the resource may have. So within this section, it would show a clear description of any relationship to other resources that this resource had. For example, if the CI was for an EC2 instance, the relationship section may show the connection to a VPC along with the subnet that the EC2 instance resides in.</p>
<p>Current configuration. This will display the same information that would be generated if you were to perform a describe or list API call made by the AWS CLI. AWS Config uses the same API calls to get the same information. So depending on the API call and resource, different information will be returned, which is resource specific.</p>
<p>Related events. This relates to AWS CloudTrail. This will display the AWS CloudTrail event ID that is related to the change that triggered the creation of this CI. There is a new CI made for every change made against a resource. As a result, different CloudTrail event IDs will be created. This allows you to deep dive into who, or what, and when made the change that triggered this CI. A great feature allowing for some great analysis to be taken, specifically when this affects security resources.</p>
<p>As you can see, the configuration item is a fundamental aspect of AWS Config when recorded change is made to a supported resource. Because it captures so much data, these CIs are used by other features and components of AWS Config such as configuration history. CIs are used to look up all the changes that have been made to a resource. Configuration streams. CIs are sent to an SNS topic to enable analysis of the data. Configuration Snapshots. CIs are used to create a point-in-time snapshot of all supported resources.</p>
<p>Following CIs, let’s move on to configuration streams, which was briefly mentioned a few minutes ago when talking about the configuration items. When a create, update or delete API call is made against a supported AWS Config resource, as we now know, a new CI is created along with additional CIs for any resources that were related to the original modified resource. These CIs are then sent to a configuration stream and this stream is in the form of an SNS topic. However, this stream is also used by AWS Config to send information when other events occur, such as when the configuration history for a resource was delivered to your account, when a configuration snapshot was started and delivered to your account, when the state of your resource compliance changes against any config rules that have been configured, when evaluation begins for rules against resources, and when AWS Config failed to deliver notifications to your account.</p>
<p>This SNS topic can have different notification endpoints, such as email, or an endpoint more commonly used, is SQS, Simple Queue Service. This allows the information to be accessed programmatically to then analyze the data captured. However, you could, like I mentioned, set up your email notification for all stream items, although in a large corporate environment, this may not be ideal as your email inbox would get full up very quickly. If you do decide to do this, then it’s worth setting up an email filter to ensure the information you’re interested in is made available to you. More information on this configuration can be found on this <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html">link</a> here.</p>
<p>This stream for AWS Config should not be confused with the real-time streams that area available through the AWS Kinesis service, which continually capture huge amounts of data from multiple sources such as clickstreams, transactions, social media feeds, and IT logs, etc.</p>
<p>The configuration history uses configuration items to collate and produce a history of changes to a particular resource. This allows you to see the complete set of changes made to a resource over a set period of time. The information can be accessed either programmatically through the AWS CLI using the following command. You can also specify the resource type. So, for example, if you wanted to look at the configuration history for a subnet, you could enter the following into the AWS CLI. Or you could access the history via the AWS Management Console. Additionally, AWS Config also sends a configuration history file for each resource type to a S3 bucket that is selected during the set up of AWS Config. This configuration file is typically delivered every six hours, and like I say, it contains all CI changes for all resources of a particular type.  For example, there would be one configuration history file covering six hours for all RDS DB instance changes in one region.</p>
<p>The configuration snapshot also uses configuration items in its production. The configuration snapshot will take a point-in-time snapshot of all supported resources configured for that region. It will generate CIs for each resource in your AWS account for a specific region, and this configuration snapshot can then be sent to an S3 bucket. Alternatively, this information can be viewed via the AWS Management Console.</p>
<p>Now let’s take a look at the configuration recorder. So this component of AWS Config can be seen as the engine of the service. This element is responsible for recording all of the changes to the supported resources within your account and generating the configuration items. By default, the configuration recorder is automatically enabled and started when you first configure AWS Config. However, it is something that you can stop and then restart again at a later point. When you stop it, AWS Config will no longer track and record changes to your supported resources.</p>
<p>When you first configure AWS Config in the AWS Management Console, you are asked which resource types you would like to record. This is essentially setting up and creating the configuration recorder, as this information is required to enable the configuration recorder to start. If you only select to record specific resource types, then it will capture all creates, changes, and deletes for those resource types and will create a CI record for each occurrence. However, the configuration recorder will still record all creates and deletes of supported resource types within that region, but no changes to those resources will be recorded. Also, if you stop and change your configuration recorder settings to perhaps remove certain resource types from being recorded, then all captured information for those resources up to that point will remain and you will be able to view their history with all data from the previous CIs that had been captured.</p>
<p>AWS config rules are a great way to help you enforce specific compliance checks and controls across your resources, and allows you to adopt an ideal deployment specification for each of your resource types. Each rule is essentially a lambda function that when called upon evaluates the resource and carries out some simple logic to determine the compliance result with the rule. Each time a change is made to one of your supported resources, AWS Config will check the compliance against any config rules that you have in place. If there was a violation against these rules, then AWS Config will send a message to the configuration stream via SNS and the resource will be marked as non-compliant. It’s important to note that this does not mean the resource will be taken out of service or it will stop working. It will continue to operate exactly as it is with its new configuration.</p>
<p>AWS Config simply alerts you that there is a violation, and it’s up to you to take the appropriate action. These rules can be custom defined, or selected from a predefined list of AWS managed rules that AWS has created on your behalf. Being able to create your own rules allows you to adopt best practices that you may have internally within your own enterprise or with other security best practices. By allowing AWS Config to monitor resources at this level, it adds another level of automation, helping to prevent misconfigurations made by human error being left, which could lead to a security risk, or even worse, a breach. I highly recommend using config rules for maintaining security checks and configurations.</p>
<p>AWS have a number of predefined rules that fall under the security umbrella that are ready to use. For example, Rds-storage-encrypted. This checks whether storage encryption is activated by your RDS database instances. Encrypted-volumes. This checks to see if any EBS volumes that have an attached state are encrypted. Optionally you can specify the ID of a KMS key to use to encrypt the volume. Rootaccount-mfa-enabled. This checks whether your root account of your AWS account requires multifactor authentication for console sign in. IAM-user-no-policy-check. This checks that none of your IAM users have policies attached. Best practice dictates that permissions should be provided via roles or groups.</p>
<p>The great thing about these predefined rules is that you can also edit them to make subtle parameter changes as needed. As you can see, being able to ask AWS Config to check your resource’s compliance with rules such as these are invaluable. And when you couple this with being able to create your own custom rules, the scope and potential of automating compliance is huge across your supported resources.</p>
<p>When you first use AWS Config with a set of rules, whether they are predefined or custom, don’t be surprised by the results that you find. It’s likely that you will see more resources being flagged as non-compliant than you may have imagined. As I have already mentioned, these config rules are great as a security tool to help you mitigate potential security issues that may arise within your environment. These config rules are also a great way to help you meet specific compliance and governance controls for auditing. You can currently use 50 config rules per region. However, if you are finding that you require more, then you can request an increase via AWS.</p>
<p>Coming up later in this course, we shall have a demonstration on how to set up custom config rules where I shall talk more about the different options available.</p>
<p>As a part of the CI for a particular resource, for example, an EBS volume, AWS Config identifies relationships with other resources from that resource. In this case, it might be the EC2 instance that the volume is attached to. This allows AWS Config to build a logical mapping of resources and how they connect. This mapping of relationships allows you to quickly jump to other linked resources within the AWS Management Console to view their configuration history and CI data. This is very useful if you’re trying to troubleshoot an issue and pinpoint where the source of an incident may be. For a full listing of available relationship types, see the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-relationships">link</a> on the screen.</p>
<p>As we have already seen, an SNS topic is used as a configuration stream for notifications of various events triggered by AWS Config. You can have various endpoints associated to the SNS stream. Best practice indicates that you should use SQS and then programmatically analyze the results via SQS. The S3 bucket that was selected at the time of configuration is used to store all the configuration history files that are generated for each resource type, which happens every six hours. Also any configuration snapshots that are taken are also stored within the same S3 bucket. The configuration details used for both SNS and S3 are classed as the AWS Config delivery channel by which data can be sent to other services.</p>
<p>When setting up AWS Config, you’re required to select an IAM role. This role is required to allow AWS Config to obtain the correct permissions to carry out and perform a number of functions. For example, AWS Config will need read-only access to all the supported resources within your account so it can retrieve data for the configuration items. Also, we now know that AWS Config uses SNS and S3 both for streams and storage of the configuration history files and snapshots. So AWS Config requires the relevant permission to allow it to send data to these services.</p>
<p>That now covers the key elements of AWS Config and so I hope it makes it clear as to how each part plays a role within the service. To reiterate, let’s take a bulleted point look at how it all comes together. When you first turn on AWS Config, you will configure the elements required for the configuration recorder to begin capturing and recording data. AWS Config will then discover all your supported resources based upon the details entered within the configuration recorder within that region. When a create, change or delete against a resource is made, a CI will be created for this change and a notification will be sent to the configuration stream regarding the new CI. Also, following the CI, AWS Config will check its current config rules to evaluate if the change has made a resource non-compliant. If the evaluated state changed for the resource, a notification will be sent to the configuration stream. Your config rules can also be configured to run periodically, and so they will run at a set given time period regardless if there have been any changes to resources. If a configuration snapshot is taken, AWS Config will create a point-in-time snapshot of the resources with new CIs, and deliver this configuration to your specified S3 bucket within the configuration recorder. After six hours has passed from turning on AWS Config, a configuration history file will be created for each resource type, and again sent to your specified S3 bucket. The configuration file will be sent every six hours from that point.</p>
<p>Now we have learned the theory behind AWS Config, let me introduce you to the service with a demonstration. In this demonstration, I will run through how to set up AWS Config, pointing out components such as the configuration recorder along the way. We will also look at the AWS Config dashboard showing the timeline of events, and how by using relationships of your resources, you can navigate through your infrastructure. During this demo, I’ll also show you the configuration history file within S3.</p>
<p>Okay, so I’ve just logged into my AWS account, and I’m going go up to Services and you can see that AWS Config is under Management Tools. So if we select Config. And this is the first splash screen you’ll be presented with if you don’t have AWS Config started as yet. So, let’s get started by clicking on the Get Started button, and then there’s three steps to setting AWS Config up.</p>
<p>So the first step is configuring the settings, and as I mentioned earlier in the lecture, we have our resource types to record and this is where we essentially start the configuration recorder by identifying what resources we’d like included in our AWS Config configuration. So, starting from the top, we can either select all resources within this region. Remember AWS Config is region specific, so we can choose to record all the resources supported within this region, and currently I’m in the London region. And we can choose to include global services, like we also discussed, services such as IAM, etc. But for this demonstration, we’ll just choose to record all resources in this region. Here you can select specific types if you require, so if I untick this, we can then go down to this drop-down box and select specific resources types that we’d like set up for this configuration. So you can be quite specific, selecting the load balancer or different elements of IAM, and again, different element of RDS, etc. So, there’s a list there that you can go through if you just want to record a specific resource type, but like I said, for this demonstration, I’m just going to select all resources.</p>
<p>Next we need to define our S3 bucket, and this is where our configuration history and snapshot files are stored. So we can ask AWS Config to just create a new bucket for us, and it’ll prefix it with this bucket name here. Or we can choose an existing bucket from my account by selecting the drop-down list, or we can select a bucket from another account. So if you had multiple accounts, you can send the configuration history files and snapshot files for all those accounts to a single, primary AWS account in a single bucket. So for this example, though, for this demonstration, I’m just going ask AWS Config to create a new bucket for us. Config bucket cloudacademy.</p>
<p>Now I’m moving down to the SNS topic. Now this is the configuration stream, as we discussed, so for any events that AWS Config picks up, it will send it to this SNS topic for the configuration stream. Now we can create a new topic, and again, AWS Config will just give us a topic name here, or we can select to choose another topic from your account or again from another account. For this demonstration, we’ll just ask AWS Config to create the topic name for us. And I’ll just add in cloudacademy.</p>
<p>Now, finally, we have the AWS Config role, and this is where we spoke about the different permissions. And this is required to allow AWS Config to send theconfiguration history file and snapshots to S3 as well as having access to send events to the SNS topic. And not forgetting enabling AWS Config to be able to kind of go out and poll your resources as a describe or list API call to get the details of those resources. And again, for this demonstration, I’m just going to ask AWS Config to create a role for us. I’ll just add cloudacademy on the end.</p>
<p>So this screen is essentially your configuration recorder. By setting this up, you’re asking AWS Config to start recording. We have set up the S3 bucket to allow our configuration history files and our snapshot files to be captured. And we have created our configuration stream by configuring an SNS topic. And we have also allowed AWS Config to have the necessary permissions to carry out those functions. So once that is all set up, we click on Next.</p>
<p>And here we have a set of predefined AWS managed config rules. And, like I said earlier, this is a number of templates that you can select to check the compliance of you resources against these rules. We’ll cover more on AWS Config rules in a later lecture and we’ll also have a demonstration on how to create one and modify one of these existing AWS managed rules. So for now, I’m just going to click on Skip.</p>
<p>And here we’re on the final stage, which is review. So we can see here the resource types we’ve selected, which are all resources, and we’ve not chosen to include global resources. We have our S3 bucket set up for our history files and snapshots. And we have our SNS topic for our configuration stream configured, and we have the permissions given by the AWS Config rule. And that’s essentially the elements to setting up AWS Config. So once you’re happy with that, and once you’re done with that, you can click on Confirm, and you can see that it’s now setting up AWS Config for this region.</p>
<p>And that’s it. It’s set up and it’s running. So whatever create, deletes or changes are made to supported resource types within this region, AWS Config will now begin to monitor that and record it and log it within the configuration stream, and also within the configuration history files.</p>
<p>Now what I’ve done, I’ve already set this up in another region and I’ve made a few changes. So what I’ll do now, I’ll swap to that region and we can take a look at some of the other elements, such as the configuration history file, the timeline of events, etc., and look at how the relationships are set up within this dashboard. So let me swap over to another region, which is Ireland.</p>
<p>So as you can see, I’ve just swapped to the Ireland region and we don’t have the option of setting up AWS Config because I’ve already set it up and you only have it running once within the region. So let’s take a look at what we have here. Straight away we can see that I’ve had existing rules. I had a rule name of s3-Bucket versioning-enabled. So I wanted to check if my S3 buckets had versioning enabled. So this is one of the config rules. So if I just open that up, I can see that a number of these buckets do not have versioning enabled, and they have been marked as non-compliant. Now remember, I can still use these buckets. I can still write to the buckets. I can still delete objects from those buckets. It’s just notified me that these buckets are non-compliant with this specific config rule, which is to check if… Right, it says here, it checks whether versioning is enabled for your S3 buckets.</p>
<p>Now let’s have a look at some of the other resources that we have. So I can filter on a specific resource type that I want to take a look at. So let me take a look at my VPC. Let me look at everything to do with my VPC. So I can see here that I have two VPCs, and let’s take a look at one of them. And this is now taking us to the timeline of events of these VPCs. So on the 15th of March at 11:25, there was a resource discovery. So that’s probably when I activated AWS Config within this region and it went out and done a discovery of all resources. And then at 11:32, there was a change on this resource item. So let’s take a look at the rest of the page.</p>
<p>So under the configuration details, we have a number of details here. We have the Amazon resource name, the resource type, the ID… The CIDR block of the VPC. So, there’s different configuration details that we can see for different items. Now we can also see the tag name here, which is CA-Demo, and further down, we can look at the relationships.</p>
<p>So this will show other resources that have a direct relationship with this VPC. So we can see we have a couple of network ACLs there, we have an internet gateway, we have some root tables, security groups and some subnets. So each of these resource types has a direct relationship with this VPC. And if I wanted to, I can select straight from here to click on that network ACL, and it will take me to the details for that ACL. And again, we have some resource type information here, the ARN and the ID, etc. Again, if we click on the relationships, we should see the VPC that we just come from. So let’s go back to our VPC.</p>
<p>So as you can see, it’s very easy to look at the relationships between different resources, and navigate between them. And it’s kind of grouped in a logical manner to allow you to quickly get between the different resource types that you’d like to. Now we can see up here that on the 15th of March, 11:23, there was a change. Now if we go down to our changes here, we can have a look at what that change was. We can see it’s defined by two different categories, a configuration change or a relationship change. We can see that by the number, that this change was to do with relationships. And it looks as though a new subnet was added to this VPC.</p>
<p>And then finally, we have our CloudTrail events, and this will capture any API calls that made changes to resources. However, it will only capture these for the previous seven days. So, because this change was made on the 15th of March, and it’s now the 30th of March, so if we look at the CloudTrail events, there’s nothing there. They have expired. So what I’ll do, I’ll go and make a couple of changes. I’ll create a new subnet within this VPC. So I’ll pause the video, wait a few minutes and then I’ll start it again and we can analyze the CloudTrail event of that new change.</p>
<p>Okay, so I’ve gone off and I’ve created a new subnet within this VPC. So we can see that the 30th of March, which is today, that there was a change that occurred and two new events. So let’s go ahead and take a look at the change. So we can see that there is a new subnet. This is the subnet that I just created. And if we look at the CloudTrail events, we can see that we have the creation of the new subnet there. Now if we click on the actual CloudTrail event itself, it will take us to CloudTrail and we can look at additional information.</p>
<p>So now we’re looking at the details of the API call itself, and we can see a number of different information here. We can see the user that created it, the source IP address, the time, the event source and also the region as well, along with any affected resources as well. So here we can drill down into exactly what time it occurred. As you can see up here, the date and time, and by who, and what event actually was created. So, as you can see, clicking on that CloudTrail event, you can get additional information to kind of help you with resolving incidents, and looking at potential security breaches to try and gather more information as to identifying who is doing what and when.</p>
<p>So now let’s take a look at the configuration history, which is stored in S3. So let’s go across to S3. So if I go to the bucket that I use for Ireland region, and then navigate through the folders to the correct date and time of today, I can see the configuration history folder. And then if I download one of these files, and then open that. We can see here that that configuration history file contained the subnet creation that we just created for our VPC, and it shows you all the different information about it, the availability zone, the CIDR block used, etc., etc. And there’s the subnet ID. And it also highlights the relationships to other resources for that new subnet such as any network ACLs that may be associated. So that’s the configuration history file that’s stored in S3 in a JSON format.</p>
<p>That brings us to the end of this demo. So we looked at how to set up AWS Config for a region. We then looked at the details of a specific resource type, we looked at the VPC and how the timeline of events work. We looked at a couple of changes, and also the CloudTrail event logs as well. And then finally, we looked at one of the configuration history files as a JSON document. That brings us to the end of this lecture. Coming up next, we will look at how AWS Config integrates with other AWS services in a bit more detail.</p>
<h1 id="Service-Integration"><a href="#Service-Integration" class="headerlink" title="Service Integration"></a>Service Integration</h1><p>Hello and welcome to this short lecture on AWS Config Service Integration where we shall look at the relationships between AWS Config and other AWS services.</p>
<p>AWS Config has a specific relationship with the following AWS services, SNS, SQS, S3, CloudTrail and IAM. Let’s start by looking at SNS.</p>
<p>We have already covered much of this in the previous lecture where I explained how SNS is used as the configuration stream for CIs and other important event notifications. By using SNS you can subscribe multiple different endpoints to the SNS topic created as a part of your configuration recorder information to extract data and process information. And this is where SQS comes in. If you had multiple accounts, you may want to have AWS Config in each account subscribed to the same topic in a primary AWS account. This is possible by allowing access of the service principle to publish to the same topic in the primary account. See the ‘permissions for the Amazon SNS topic’ within the following AWS developer guide for a sample policy on how to do this.</p>
<p>The Simple Queue Service, SQS, can be subscribed to the AWS Config topic, the configuration stream, which gives you a highly available and decoupled environment for the data within your configuration streams. By using SQS it allows you to create and use your own applications to extract only information and data that is pertinent to you. There can be vast amount of data coming into the configuration stream but you might only want to be notified and made aware of any changes that may relate to any potential security issues. As a result, you may want to pull information from the queue that only relate to security groups, NACLS, IAM roles etc. or any other resource type that could affect the security of your environment.</p>
<p>If you did decide to have different configuration streams in each region, so effectively different SNS topics, then you could still subscribe the same SQS queue to multiple SNS topics preventing your application from poling from multiple queues to process data from the configuration stream. S3 is used to store the configuration history files and any configuration snapshots of your data within a single bucket. And again, this bucket is defined within the configuration recorder. You can get AWS Config to create a new bucket for you or select an existing bucket. If you have multiple AWS accounts you may want to aggregate your configuration history and snapshot files into the same S3 bucket for your primary account. However, you will need to grant the right access for the service principle which is config.amazonaws.com to be able to write to the S3 bucket. Take a look at the section ‘Granting AWS Config Access to an Amazon S3 Bucket in Another Account’ within the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/config-dg.pdf">link</a> for a sample policy on how to do this.</p>
<p>AWS CloudTrail interacts with AWS Config at the configuration item level. If we remember back to the section in the previous lecture the CI is comprised of five different sections. The final section, Related Events, displays the AWS CloudTrail event ID that is related to the change that triggered the creation of the CI for that resource. This feature is very useful when identifying who or what made the change to the effective resource. This CloudTrail data can be accessed via the AWS Config Dashboard within the AWS Management console, which will then link you directly to the event within CloudTrail. For more information on CloudTrail we have a course AWS CloudTrail, An Introduction that will define exactly what the services and how it works.</p>
<p>Conversely, when CloudTrail tracks and recalls changes made within the AWS Config itself, the following APIs are tracked, DeleteDeliveryChannel. This deletes the delivery channel. DeliverConfigSnapshot. This sends a configuration snapshot to S3. DescribeConfigurationRecorderStatus. This returns the status of a specified configuration recorder. DescribeConfigurationRecorders. This returns the details of a specific configuration recorder. DescribeDeliveryChannels. This returns information about a specific delivery channel. GetResourceConfigHistory. This retrieves a list of configuration items for a specified resource. PutConfigurationRecorder. This creates a new configuration record. PutDeliveryChannel. This creates a new delivery channel for an S3 bucket and SNS topic. StartConfigurationRecorder. This starts recording data for supported resources within your account as per your configuration. And finally, StopConfigurationRecorder. And this stops recording the data.</p>
<p>The final service that has a relationship with AWS Config is IAM. And again, we briefly covered this in the previous lecture. As AWS Config has relationships with other services, specifically SNS and S3, the use of an IAM role is required to enable the service to publish data to an SNS topic for configuration streams and S3 to store configuration history files and configuration snapshots. The policy for this access would look similar to the following on screen. In addition to this access, AWS Config must also be able to perform the described list and some get API calls against all supported services within the region. As a result, the same IAM role also has a second policy attached which allows access to perform these actions against those resources.</p>
<p>That brings us to the end of this lecture of how other AWS services interact with AWS Config. Coming up next we’ll start to look at how to manage specific compliance with AWS Config. We briefly touched on this earlier when we looked at the config rules, so we’ll now look at this in greater depth.</p>
<h1 id="Managing-Compliance-with-AWS-Config"><a href="#Managing-Compliance-with-AWS-Config" class="headerlink" title="Managing Compliance with AWS Config"></a>Managing Compliance with AWS Config</h1><p>Hello and welcome to this lecture where we’ll begin to look at how to best manage your compliance that you need to adhere to within your AWS environment.</p>
<p>Your compliance requirements can come from many different sources, for example, you may have a requirement for your environment to be HIPAA compliant if you’re managing healthcare records. Or perhaps your security theme requires certain criteria to be adhered to such as ensuring SSH is not applied to a specific security groups. Or your operations team may dictate to you that certain EC2 types should only be used for certain environments to conform to internal standards. Maintaining compliance and internal standards for numerous parties, both internal and external, can be difficult to manage and can lead to mistakes, which in turn will eventually lead to non-compliance services making their way into production. This can be a huge risk from an audit perspective and also from a security stance.</p>
<p>AWS Config allows you to utilize config rules to help you manage and organizes compliance which acts as an automatic resource compliance checker. When a change is made to a resource, AWS Config will check to see if the resource matches the rule with the help of a lambda function and if so, it will check the compliance of that resource against the rule following the changes made.</p>
<p>There are two different types of Config Rules within AWS Config, custom config rules and AWS Managed Config Rules. AWS Managed Rules are a set of predefined rules that cover a lot of best practices, so it’s always worth browsing these rules first before setting up your own as there is a chance that the rule may already exist. At the time of writing this course these Managed Rules cover five different topic areas, compute, database, management tools, security, identity and compliance, and storage. Do bear in mind that you can edit specific parameters of Managed Rules within in these topics, and so if there is a rule that very closely matches your requirements, then you may be able to edit it by selecting different triggers and parameters to reflect the changes that you need. This can save you a lot of time trying to recreate your own from scratch. Later in this lecture I will provide a demonstration of how to modify these managed rules, along with how to create your own custom rules.</p>
<p>Before creating, modifying and configuring your Config Rules you should first identify what compliance and standards that you need to adhere to. Define the requirements from all parties but remember, there is a limit of 50 config rules per region before you need to contact AWS to request an increase.</p>
<p>Let’s consider a simple scenario on how these Config Rules could be used. Let’s say we are security architects responsible for ensuring specific security requirements are met in the following deployment. A new web application is being released into the production environment that consists of multi-tiered infrastructure, with front-end web servers in a public subnet, application servers with EBS and a back-end RDS database. The application and the environment itself is expected to scale with demand, both up and down.</p>
<p>We have been told that from a security perspective SSH must not be used for the web server security group under any circumstance. Where used, all EBS volumes must be encrypted, the RDS database must be configured for high availability at all times and data stored by RDS must be encrypted. Now obviously, during deployment we can oversee the implementation and ensure that each of these elements are met. We can make sure the security groups are correctly configured, storage encryption is activated and that multi AZ is configured for high availability for the RDS instance. So, why the need for these config rules?</p>
<p>Well, now let’s consider the following. Once the initial deployment was carried out and security checks were carried out manually, the environment was then handed over to support and operations to maintain and look after. Over time the application scaled up, additional resources were added and there may have been some ad hoc incidence and general maintenance of the environment which was carried out. The support and operations team, although they were made aware of the security requirements, they may not have adhered to them at all times, perhaps due to human error, lack of knowledge or laziness. Your environment now has a situation where some volumes are not encrypted and SSH has been activated on the web server security group perhaps to troubleshoot an existing incident.</p>
<p>As you can see, over time your environment changes and maintaining the same level of security implemented at the start of the project may not be continuous throughout its lifetime, which leads to mistakes and security holes within your infrastructure. AWS Config Rules can notify you of these security misconfigurations. If for example, in this situation during deployment we activated the followed AWS Managed Rules, restricted-SSH, which would check whether security groups would disallow incoming SSH traffic, encrypted-volumes, this would check whether EBS volumes are encrypted, RDS-multi-az-support checks to ensure high availability is configured for your RDS instance, and rds-storage-encrypted, and this checks if storage encryption is enabled for your RDS instances. With these config rules in place, a notification would have been sent to the configuration stream stating that a resource had changed its state from being compliant to non-compliant.</p>
<p>These notifications could have been configured programmatically to notify your security team, who could have then investigated as to why this had happened, and understand who made the change through the use of the configuration item by utilizing the CloudTrail information recorded when the change happened to the resource. By using Config Rules it allows the appropriate action to take place when a notification is received of a non-compliant resource, which may include resolving the security issue identified and ensuring the resource becomes compliant again. Remember, just because a resource is flagged as non-compliant, does not mean it is removed from your environment. It will continue to function.</p>
<p>Identify who or what made the change. If an employee made the change advise and educate them on the importance of security and why that resource requires set configurations. If it was a service, check your automation configurations to ensure it doesn’t happen again. So, as you can see using AWS Config Rules allows a continuous monitoring solution for a wide range of uses to ensure compliance is maintained within your environment.</p>
<p>The example we have used was a simple security scenario, but the scope is huge, especially when implementing custom config rules. Before we finish this lecture, I just want to provide a quick demo to show you how to select and modify an AWS Managed Rule and also how to configure a custom rule where I will show you the different elements involved in the process.</p>
<p>Okay, so I’m at the AWS Config dashboard and at the Ruled page. So firstly, what we’re going to take a look at is how to modify the parameters of an AWS Managed Rule.</p>
<p>So, if we go to Add Rule, and for this demonstrations I’m going to use the Desired Instance Type Managed Rule, so this checks whether your EC2 instances of the specified instance type.</p>
<p>So, we have a name at the top which we can edit if we need to. The description of the rule, it checks whether your EC2 instances are of a particular type. Here’s the managed rule name which is the lambda function and here we have the triggers. Now, there’s two different trigger types here. When there’s changes to configurations or you can have periodic triggers, so at a set period of time. So, we got it as configuration changes for this managed rule.</p>
<p>Now look at the scope of changes. This identifies what resources are in scope for this rule. At the minute it’s selected Resources and under Resources we have EC2 instances. We can also select tags and then add our own tag key pairs in there, so whatever we enter there the managed rule will only apply to any resources that have those tags within them or all changes. But I’m going to leave it as Resources, so we want this rule to run against any EC2 instances, and then triggered when there’s any configuration changes.</p>
<p>Now, down here we can edit the parameters. So, we want to specify which instant types that we want to have running within our environment. Now, for our environment we want to make sure we’re just running T2.small instances or M4.large. So, just take a look at this AWS Managed Rule again. We have the name, the description, the actual lambda function itself, which is the evaluation of the rule, we have the trigger type and we’ve left it as configuration changes, and we have the scope of change, we’ve selected a resource, so we want this to be based on all EC2 instances, rather than specific tags. And then for the instance type we’ve edited and added values of T2.small and M4.large. Now, depending what kind of AWS Managed Rule you have, there’ll be different configuration parameters that you can change. I just wanted to highlight here that you can add your own parameters for specific AWS Managed Rules, and in this instance we’ve added our EC2 instance types here. Then once you’ve done that, you just click on save and then AWS Config will go off and evaluate your current environment. And then after a period of time, that evaluation will come back and give you your compliance results.So, what I’ll do, I’ll pause the video here, wait for that to finish and then show you the results.</p>
<p>Okay, so that evaluation has now been completed and we can see that under our new AWS Managed Rule, the desired instance type we have two non-compliant resources. So, let’s take a look. And here we have our two EC2 instances, both coming back with non-compliance checks. Now, we can take a look at these instances. If we follow around inside here you can see Managed Resource and this will give us a link to each instance. So, let’s take a look. So, we can see here that this instance type is actually a T2.nano, which is non-compliant with our T2 small and M4 large that we specified in the rule.</p>
<p>So, if we go back to Config and take a look at the second instance, and we can see here that this is a T2.micro, so again, non-compliant. So, that’s the end of this demo on showing you how to edit the parameters of a AWS Managed Config Rule.</p>
<p>We’ll now take a quick look at how to create your own config rule, which is very, very similar.</p>
<p>Okay, so to create your own config rule is very similar steps. Again, click on Add Rule and instead of selecting one of the predefined AWS Managed Rules as before, all we do is click on Add Custom Rule, and here you add a number of details. So, you can add your own name, so Cloud Academy Rule, description, this is a test for a demo. Here is where you’ll enter the ARN of the lambda function that you’ve created. That will then evaluate your resources to see if they comply with this rule. Now, I’m not going to go and create a new lambda function because it’s out of scope of this course, but for those who are experienced with lambda, then this where you would enter your ARN function.</p>
<p>Now, further down this is where we add our trigger types like we just discussed with the AWS Managed Rules. You can have the trigger to happen every time there’s a configuration change, or over a specific period. So, with regards to the period, you can select 1 hour, 3 hours, 6 hours, 12 or 24. So, this rule will run at that set period regardless of if there’s any configuration changes, but you can have both ticked regardless.</p>
<p>Now going down to the scope of changes, and again as we mentioned within the AWS Managed Rules, it can be against your resources, particular tags or for both. So, if we select Resources we can specify our resource type, for example, VPCs, subnets, security groups, so you can have multiple resource types applying to the same rule.</p>
<p>And then again, down here you can have your rule parameters. Here you would define any values that your lambda function would need to evaluate for this rule. And once you’ve entered all your information, then all you’d need to do is then just to click on save and then that will go ahead and create your custom rule with your own lambda function and again, it’ll follow the same process of evaluation and then return any non-compliant results.</p>
<p>And that brings us to the end of this demonstration. For a full list of current AWS Managed Rules that we spoke about within this lecture, then please visit this <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html">link</a>. That brings us to the end of this lecture. Coming up next we’re going to look at some different use cases for AWS Config.</p>
<h1 id="AWS-Config-Use-Cases"><a href="#AWS-Config-Use-Cases" class="headerlink" title="AWS Config Use Cases"></a>AWS Config Use Cases</h1><p>Hello, and welcome to this lecture on AWS Config Use Cases. We will look at some of the common scenarios of where and why you would want to use this service.</p>
<p>In an earlier lecture, we looked at some of the scenarios we are faced with, when looking at resource asset and change management and how hard it can be to have deep visibility of your infrastructure. Following this, there are a few key use cases, for when using AWS Config is ideal within your environment. Let’s take a quick look at each.</p>
<p>Security Compliance. As we learned in the previous lecture, AWS Config can be a great tool, when enforcing strict compliance against specific security controls. Being notified of noncompliant resource configurations from a security stance is critical, especially in highly sensitive environments, where these controls are imperative to protect both internal corporate and external customer data. Through the use of config rules, you can have the service continually monitor and check your resources remain compliant throughout its life cycle.</p>
<p>Discovery of Resources. When you first activate AWS Config, or run the configuration recorder, AWS Config will discover all supported resources types, allowing you to view them from within the AWS Config dashboard. A configuration item will be recorded for each and so these resources could also be found in the configuration history file on S3. Being aware of all the resources you have is key to understanding your environment. You may find that you have EBS volumes out there, that are no longer attached to instances, which you could then take a snapshot of to keep the data and then delete the volumes, saving you money or perhaps you have subnets configured, that no longer have any instances in, that you no longer need and so it allows you to perform some essential housekeeping within your network and VPC. There are many benefits to knowing what you have, where it is and what it’s connected to. Many of these benefits will end up saving you money and help you run a streamlined environment.</p>
<p>Audit Compliance. As well as using AWS Config for being compliant for internal security standards, there are also many external audit and governance controls, where the service can also enforce specific controls on resources to maintain compliance. For example, the Health Insurance Portability and Accountability Act, known as HiPAA and Payment Card Industry Data Security Standard, known as PCI DSS. These programs require strict controls in many different areas. Being able to set custom and manage configurals in place help adhere to these external governance controls. In addition to this, you could show the auditors all of your configuration history files, which will allow them to go back to any point in time to check the configuration of any of your supported resources. Having this kind of information to hand is essential from an audit compliance point of view.</p>
<p>Resource Change Management. When planning changes within your infrastructure, it’s often required that you have an understanding of what affect the change will have on other resources. More often than not, this information is not always known, as you may not have full visibility of other attached resources. With AWS Config, you are able to use the dashboard to list all related resources of a particular resource, thanks to the relationship section within the configuration item. This allows you to plan your changes more effectively, by ensuring all resources that have a relationship to the source being changed, continue to function as expected post-changes. This helps to prevent outages and configurational mistakes being made by having an overall better visual awareness of the environment.</p>
<p>Troubleshooting and Problem Management. AWS Config is a great tool to help you troubleshoot issues, that may arise within your environment. Using the config dashboard within the AWS management console, you can see a timeline of events allowing you to go back to any point in time and in the case of an in instant, you’ll be able to go back to just before it happened. By doing this, you can understand what changes happened on your supported resources. If there were changes made to a resource, that was affected by an incident, then this can significantly help you reduce the time to resolution, by identifying the possible cause of the problem. You would also be able to see the changes made to the resource and make any amendments to resolve the issue, not forgetting thanks to its incorporation with AWS CloudTrail, you can see who or what triggered the change, via which API call. If similar events occur frequently, then AWS Config can become a great tool to help you spot potential, underlying problems within your infrastructure, allowing you to find the root cause and manage them effectively.</p>
<p>You might want to look at some Real World Use Cases of other AWS customers. If so, then take a look at their customer success stories found here. That brings us to the end of this lecture. In the next lecture, we will summarize what we have learned throughout this course.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello and welcome to this short lecture where we will briefly look at a high level what we have learnt throughout this course.</p>
<p>In summary, we have identified that AWS Config is a service within the management tools category that can perform a number of useful function when it comes to resource configuration visibility and compliance, such as capturing resource changes, acting as a resource inventory, storing configuration history for individual resources, providing a snapshot in time of current resource configurations, enabling notifications of when a change has occurred on a resource, providing information on who made the change and when through the use of AWS CloudTrail integration, enforcing rules that check the compliancy of your resources against specific controls, allows you to perform security analysis within your AWS environment, and it provides relationship connectivity information between resources.</p>
<p>We also discussed that AWS Config only supports a number of different services and resource types, which can be found here.</p>
<p>Another important point is that AWS Config is configured on a region-by-region basis. As such you can have different resources being monitored and recorded in each region as you’ll have a different configuration recorder.</p>
<p>We also looked at how the service itself was constructed with regards to its different components, which were identified as follows, AWS resources, the resources in which you want to monitor and record. Configuration items, a record that contains information about the resource including specific configuration details. Configuration streams, an SNS topic that can be accessed programmatically to extract data. Configuration history. This allows you to view configuration information on a particular resource over a period of time through the timeline within the management console or via the configuration history file held on S3. Configuration snapshot, a complete point-in-time snapshot of all your supported resources, including all configuration information. Configuration recorder, the configuration used by AWS Config to determine what resources to record and to which SNS topic and S3 bucket data should be sent. Config rules, rules that allow AWS Config to check resources compliance against the rule set. Any non-compliant rules are identified and a notification is sent to the stream. Resource relationships. This allows you to clearly identify which resources link to other resources. SNS topic. This is used as the configuration stream. S3 bucket. This is used to store configuration snapshots and configuration history files. And AWS Config permissions. The use of an IAM role is required to perform, describe and list API calls to supported resources, along with right access to your selected SNS topic and S3 bucket.</p>
<p>Next we looked at how AWS Config is integrated with other AWS services, such as SNS, where a topic is used as a configuration stream. SQS. This is an ideal service to allow you to programmatically extract useful information from the configuration stream by using SQS as an endpoint within the SNS topic. S3. This is required to allow you to store configuration snapshots, along with your configuration history files which are stored there every six hours for each resource type. CloudTrail. This is used as a part of the configuration item to allow you to track the API which created the change on the resource. IAM. This used to create the role that allows AWS Config to perform its functions as already discussed.</p>
<p>We then looked at how to best manage compliance within your environment. Using security as an example, we explained that by using AWS Config you can implement specific config rules that monitor all changes within your environment notifying you when a resource becomes non-compliant. This allows you to rectify configuration mistakes ensuring that your AWS environment is not unnecessarily exposed to weak security configurational changes.</p>
<p>Finally, we looked at a number of different use cases of where AWS Config can be used to help support, maintain and optimize your AWS resources. These included security compliance, discovery of resources, audit compliance, resource change management, and troubleshooting and problem management.</p>
<p>That now brings us to the end of this lecture and to the end of the course. I hope it has given you a good understanding of the AWS Config service and has left you confident enough to start using this service as you need within your own organization.</p>
<p>If you have any feedback on this course, positive or negative, please do leave a comment on the course landing page. We do look at these comments and your feedback is greatly appreciated.</p>
<p>Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="3Key-Components"><a href="#3Key-Components" class="headerlink" title="3Key Components"></a>3<strong>Key Components</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-resources">Supported Services and Resources</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html">Email Filter</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html#supported-relationships">Available Relationship Types</a></p>
<h1 id="4Service-Integration"><a href="#4Service-Integration" class="headerlink" title="4Service Integration"></a>4<strong>Service Integration</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/config-dg.pdf">Sample Policy</a></p>
<h1 id="5Managing-Compliance-with-AWS-Config"><a href="#5Managing-Compliance-with-AWS-Config" class="headerlink" title="5Managing Compliance with AWS Config"></a>5<strong>Managing Compliance with AWS Config</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/config/latest/developerguide/managed-rules-by-aws-config.html">AWS Managed Rules</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15/" class="post-title-link" itemprop="url">AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:05" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:05-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 23:01:20" itemprop="dateModified" datetime="2022-11-19T23:01:20-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-Lab-Monitoring-AWS-CloudTrail-Events-with-Amazon-CloudWatch-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/14/AWS-Security-Specialty-AWS-CloudTrail-An-Introduction-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/14/AWS-Security-Specialty-AWS-CloudTrail-An-Introduction-14/" class="post-title-link" itemprop="url">AWS-Security-Specialty-AWS-CloudTrail:-An-Introduction-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-14 13:52:04" itemprop="dateCreated datePublished" datetime="2022-11-14T13:52:04-04:00">2022-11-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-19 22:53:28" itemprop="dateModified" datetime="2022-11-19T22:53:28-04:00">2022-11-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Security-Specialty/" itemprop="url" rel="index"><span itemprop="name">AWS-Security-Specialty</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/14/AWS-Security-Specialty-AWS-CloudTrail-An-Introduction-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/14/AWS-Security-Specialty-AWS-CloudTrail-An-Introduction-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="AWS-CloudTrail-An-Introduction"><a href="#AWS-CloudTrail-An-Introduction" class="headerlink" title="AWS CloudTrail: An Introduction"></a>AWS CloudTrail: An Introduction</h1><p>Hello, and welcome to this course covering AWS CloudTrail.</p>
<p>CloudTrail is one of the services that falls under the Management Tools categorization within the AWS console. Throughout this course, I shall explain what the service is, what it does, and how it operates, along with its interaction with other AWS services.</p>
<p>AWS CloudTrail is a powerful service that is used to track, audit, and monitor all API requests made in your AWS account, making it an effective security analysis tool. And so it’s worth understanding exactly what it is and what it can do.</p>
<p>Before we start, I’d like to introduce myself. My name is Stuart Scott. I am one of the trainers here at CloudAcademy specializing in AWS, Amazon Web Services. Feel free to contact me with any questions using the details shown on screen. Alternatively, you can always get in touch with us here at CloudAcademy using the community forum where one of cloud experts will reply to your question.</p>
<p>This course has been designed for an audience who have an active roll in managing AWS security, such as a security consultant, security architect, security auditor, etc. Also, if you have a general interest in security or perhaps you are studying for an AWS certification that requires knowledge of AWS CloudTrail, then this course will certainly be of benefit to you as well.</p>
<p>In this course, I will cover a range of topics, including what is AWS CloudTrail? In this lecture, I will explain what CloudTrail is and does and give examples of how the service can be used for a number of different use cases. How does CloudTrail work? In this section, I’ll talk about CloudTrail and its components and elements, and we’ll discuss how would they all link together to create the service. Understanding CloudTrail permissions. In this lecture, we’ll talk about permissions for both read and write access, and also we’ll touch on some IIM policies and S3 bucket policies here as well. Understanding trails. In this section, we’ll define what a trail is, and we’ll go into the configuration components, and I’ll give a demonstration of how to create a trail here too. Insight into CloudTrail logs. Logs are a huge part of CloudTrail. It’s the output of the service itself. So we’ll dive into what logs are, and what you can do with them and how to share logs within your own account and across other accounts as well. And then finally, we’ll look at monitoring with CloudTrail. Here, we’ll look at how CloudTrail interacts with AWS CloudWatch and how to set up monitoring for specific API calls, etc.</p>
<p>As a student of this course, you will have a full understanding of the AWS CloudTrail service and how it interacts with other AWS services, allowing you to implement CloudTrail effectively, ensuring it fulfills your business requirements. You will have the knowledge to confidently configure Trails for your AWS account, whilst at the same time applying the correct level of encryption and access control against your sensitive log files. In addition to this, you will be able to combine CloudTrail with CloudWatch to implement a monitoring solution for your API calls if required.</p>
<p>Pre-requisites for this course include a basic understanding of the following AWS services: Simple Storage Service, so S3; Identity and Access Management, specifically around policies; AWS CloudWatch; Simple Notification Service, SNS, and the Key Management Service, KMS.</p>
<p>Your feedback on CloudAcademy courses are valuable to us as trainers and any students looking to take the same course in the future. If you have any feedback, positive or negative, it would be greatly appreciated if you could use the comment section found on the landing page of this course.</p>
<p>That brings us to the end of this first lecture. Coming up next, I will introduce you to AWS CloudTrail with an explanation of what it is and what it can do.</p>
<h1 id="What-is-AWS-CloudTrail"><a href="#What-is-AWS-CloudTrail" class="headerlink" title="What is AWS CloudTrail?"></a>What is AWS CloudTrail?</h1><p><strong>Resourced referenced:</strong></p>
<p>AWS Whitepaper: <a target="_blank" rel="noopener" href="https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf">https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf</a></p>
<p><strong>Transcript:</strong></p>
<p>Hello and welcome to this lecture. In this lecture, I will explain the basic <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">fundamentals of AWS CloudTrail</a> to give you an overview of the service before we look deeper at the inner workings revealing <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/how-does-aws-cloudtrail-work-1/">how the different elements work together</a>.</p>
<p>So what is CloudTrail and what does it do? CloudTrail is a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS command line interface, from within the AWS management console or even from a request made by another AWS service.</p>
<p>For example, when auto scaling automatically sends an API request to launch or terminate an instance. These API requests are all recorded by CloudTrail.</p>
<p>When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the log file. CloudTrail also records and associates other identifying metadata with all the events. For example, the identity of the caller, the time stamp of when the request was initiated and the source IP address. In a later lecture entitled Insight Into CloudTrail Logs I will look at these log files deeper, where I will provide an example of a log file showing the different attributes recorded.</p>
<p>For greater management, new log files are typically created every five minutes which are then delivered and stored within an S3 bucket that is defined by you during your CloudTrail configuration. This allows you to easily go back and review the history of all API requests made. There is also an option to have these logs delivered to a CloudWatch Logs log file as well. Having this association with CloudWatch enables custom metrics <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/monitoring-with-cloudtrail-1/">to be configured to monitor specific API requests</a>. Thresholds can be set against these metrics and when crossed, the simple notification service SNS can be triggered to notify your security teams to investigate. That, at a very high level, is the overall function of the AWS CloudTrail service.</p>
<p>Now let’s take a look at the CloudTrail architecture to understand where it can be implemented from an AWS region standpoint and which services can be supported. AWS CloudTrail is a global service with support for all regions. Support for the latest region EU-London was added in mid-December 2016. In addition to this worldwide coverage, CloudTrail also provides support for over 60 AWS services and features across a wide range of service categories. As you can imagine, with this extensive coverage CloudTrail can capture a vast amount of data if you have a multi-region, multi-service infrastructure environment deployed.</p>
<p>So armed with this information, what can you do with it? How can you use this data to help you manage and support your AWS infrastructure? Well, there are a number of ways you can use the data captured by CloudTrail to help you enhance your AWS environment. Firstly, and as mentioned earlier, it can be used very effectively as a security analysis tool. CloudTrail events provide very specific information about where an API call originated from and who or what initiated the request. As a result, if malicious activity was detected via irregular trends or restricted API call thresholds with the use of CloudWatch then a number of security controls can be quickly implemented to prevent the user from causing additional damage.</p>
<p>Another common use for CloudTrail is to help resolve and manage day-to-day operational issues and problems. Using built-in filtering mechanisms, it’s possible to quickly find who, what, and when a particular API was used which could’ve potentially caused an outage or service interruption. This enables quicker root cause identification resulting in a speedy resolution. Appropriate actions could then be taken to ensure the incident does not reoccur in your environment.</p>
<p>As API calls to add, modify, or delete resources are captured, CloudTrail can be an effective method of tracking changes to resources within your environment. There is another AWS service that is specifically designed to order and track changes to resources which is called AWS Config which CloudTrail interacts with. However, CloudTrail can be used to capture the actual API request and all associated data which made the change. And if you are not using AWS Config, then this at least provides some base level of monitoring and tracking.</p>
<p>From a governance and security legislation perspective, many certifications require the ability to recall and provide evidence of log files relating to specific changes to resources. CloudTrail provides all of this by default through the use of capturing events and writing them to a log file which is then stored on S3. AWS has a great white paper on achieving compliance using CloudTrail entitled Logging in AWS How AWS CloudTrail can help you achieve compliance by logging API calls and changes to resources. The following <a target="_blank" rel="noopener" href="https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf">URL</a> will take you to that white paper. If you need to be able to capture and track API requests within your AWS account for any of these reasons mentioned or perhaps for other reasons you may have of your own, then CloudTrail can do this for you and deliver the output as a log file into an S3 bucket of your choice.</p>
<p>That brings us to the end of this lecture. Next, we look at how CloudTrail is formulated and how the various components and elements work together.</p>
<h1 id="How-does-AWS-CloudTrail-work"><a href="#How-does-AWS-CloudTrail-work" class="headerlink" title="How does AWS CloudTrail work?"></a>How does AWS CloudTrail work?</h1><p>Hello and welcome to this lecture where I shall discuss the different features and components of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">CloudTrail</a> and how they work together to provide a customizable API call-tracking and monitoring solution.</p>
<p>Firstly, let’s look under the hood of CloudTrail to see what makes up the core features and components that create the service.</p>
<p>Trails. These are the building blocks of the service. You can create <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/understanding-trails-1/">many different trails</a> containing different configurations relating to API requests that you want to capture.</p>
<p>S3. S3 is used by default to store the CloudTrail log files and a dedicated S3 bucket is required during the creation of a new trail.</p>
<p>Logs. <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">Logs are created by AWS CloudTrail</a> and record all events captured. A new log file is created approximately every five minutes and once processed, it is delivered to an S3 bucket as defined by its trail configuration. If no API calls have been made, then no logs will be delivered.</p>
<p>KMS. The use of AWS KMS is an optional element of CloudTrail, but it allows additional encryption to be added to your log files when stored on S3.</p>
<p>SNS. SNS is also an optional component for CloudTrail, but it allows for you to create notifications. For example, when a new log file is delivered to S3, SNS can notify someone or a team via an email. Or it could be used in conjunction with CloudWatch when metric thresholds have been reached.</p>
<p>CloudWatch Logs. Again, this is another optional component. But AWS CloudTrail allows you to deliver its logs to AWS CloudWatch logs as well as S3 for specific monitoring metrics to take place.</p>
<p>Event Selectors. Event selectors allow you to add a level of customization to the type of API requests you want the corresponding trail to capture.</p>
<p>Tags. Tags allow you to assign your own metadata to your trail. For example, you could add a project or department tag indicating which project or department the trail relates to.</p>
<p>Events. For every API request that is captured by CloudTrail it is recorded as an event in a CloudTrail log file.</p>
<p>API Activity Filters. These are search filters that can be applied against your API activity history in the management console for create, modify and delete API calls. These events are held in the management console for seven days, even if the trail itself is stopped or deleted.</p>
<p>Okay, so I’ve now covered the different components that essentially build CloudTrail. Let me introduce you to the process at a high level of how all of this fits together and in what order. In the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/understanding-cloudtrail-permissions-1/">coming lectures</a>, I will explain in greater detail each of the configurable elements.</p>
<p>By default, CloudTrail is not enabled within your AWS account, so the very first step is to create a trail. If no trail exists, then CloudTrail does not know what API calls to capture from which region and which services, global or otherwise. During this trail creation, you will need to specify an S3 bucket for your log files to be delivered to. This can be an existing bucket or a new bucket which can be created at this stage. You will then have the option to encrypt your log files with the Key Management Service, KMS, if required. Also if required, you can configure the Simple Notification Service, SNS, to notify you when new log files are delivered. If you want to add another layer of security integrity, then you can enable Log File Validation, which will ensure your logs have not been modified or tampered with since their delivery to S3.</p>
<p>Your trail is now ready to be turned on and created. Once it has been created, you are able to make further configurational changes that are not available during the trail creation itself. Upon selection of your new trail, you will have the option to configure CloudWatch logs, which allows you to deliver your CloudTrail log files to CloudWatch in addition to S3. This allows you to creat CloudWatch monitoring metrics against specific API calls and will receive notification from SNS when custom thresholds are reached. Another optional configurable element is that of CloudTrail Event Selectors. This allows you to specify the types of events, management or data that CloudTrail logs. Finally, at the last element of your newly created trail configuration, there is the ability to add tags just as you would with other resources within AWS. At this point your trail is configured and actively recording API calls as per your configuration. For every API call that matches the requirement of your trail, it will be captured and recorded in a log file as an event. Each API call will be recorded as a new event.</p>
<p>Once you have captured the data, you may need to find a particular event quickly, maybe for security reasons. This can be achieved using API Activity Filters which can be found within the CloudTrail service from the management console. So from a high level perspective, we know how a trail is configured.</p>
<p>But what happens when an API matching a trail is called upon by user or service? Let’s take a quick look. A user or service calls upon an API. Next, CloudTrail checks to see if this API call matches any configured trails. If a match is found, CloudTrail recalls the API as an event within its current log file. It also associates other identifiable metadata mentioned earlier. Eventually, the event with the log file will be delivered to S3 and possibly CloudWatch Logs depending on the trail configuration. If it is sent to CloudWatch Logs, the log file will be <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/monitoring-with-cloudtrail-1/">monitored by any configured metrics</a>. When in S3, the log file will be stored and the default server site encryption, SSE, unless KMS has been configured for increased security measures with the associated trail. If any S3 life cycle rules are applied to the bucket, then over time the log file may be archived to a different storage class or even glacier.</p>
<p>That essentially concludes the elements of CloudTrail and how they work together. Coming up in the following lectures we will dive deeper into these configuration components, providing you with a greater understanding of exactly what can be achieved at a more granular level.</p>
<p>That brings us to the end of this lecture. Coming up next, I will explain and cover the various permissions that need to be in place for CloudTrail to be effective.</p>
<h1 id="Understanding-AWS-CloudTrail-Permissions"><a href="#Understanding-AWS-CloudTrail-Permissions" class="headerlink" title="Understanding AWS CloudTrail Permissions"></a>Understanding AWS CloudTrail Permissions</h1><p>Hello and welcome to this short lecture on permissions. I’m going to talk about permissions that are required to set up and create trails and those required by S3 for CloudTrail login, where we will also take a look at Bucket policies.</p>
<p>As with all <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">resources within AWS</a>, to be able to use a service or feature you need to have the proper permissions and authorization. If you do not have the permissions to create trails on CloudTrail, or to configure it as you need, then you’ll need to speak to your AWS administrator or security team. If you are the administrator and you want to grant access to others, then you could create your own IAM policies to cover this access. Or use an existing AWS Managed policy. For more information on IAM policies you can see my previous course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-aws-authentication-authorization-accounting/introduction-112/">Understanding of AWS Authentication, Authorization and Accounting</a>. Or <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-iam-policy/">my blog post here on IAM policies</a>. Currently AWS has two Managed policies in IAM, relating to CloudTrail: AWSCloudTrailFullAccess and AWSCloudTrailReadOnlyAccess. These are fairly self explanatory as to the level of access provided here. If someone needs to create or delete trails then they will obviously need the AWSCloudTrailFullAccess policy, associated to the user account.</p>
<p>I just want to quickly point out that if the existing Managed policies don’t quite meet your permissions needs then you can simply copy and modify them to meet your specific requirements. For example, if you only wanted particular users to have some of the full access permissions then you can remove any lines within the policy that you wanted to restrict. If we look at the actual policy for AWSCloudTrailFullAccess we can see that this also includes admin level access to SNS and S3, which you may want to restrict depending on how you control and manage these other services within your organization. It’s likely you have different teams, managing different services and solutions and so you might need to split up the permissions to allow some to configure S3 and others to configure SNS, Cloud Watch and KMS etc. However upon creation of your trails, it can be useful to have someone responsible to set the configuration for these services during the trail creation, for ease and to ensure it doesn’t get overlooked at a later date.</p>
<p>As we know <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">CloudTrail logs</a> are delivered to an S3 Bucket. When creating your trails, there is a step where you need specify which S3 Bucket to send logs to once processed. Here you will have two options. The first option being create a new S3 Bucket. And the second is to use an existing S3 Bucket.</p>
<p>By selecting the first option, CloudTrail applies and configures a Bucket Policy with the relevant permissions, allowing logs to be delivered to the Bucket. As a part of this process, CloudTrail configures the following attributes within the policy. The allowed Statement IDs (SIDs). The folder name where the log files will be saved. The service principal name for the current and future CloudTrail supported regions. The Bucket name. The optional prefix, if one was specified. And the ID of the owning account. This method is the easiest way to allow CloudTrail to write logs to your S3 Bucket.</p>
<p>If, however, you choose to select an existing Bucket, then you need to set up the correct permissions. This is achieved by applying your own Bucket Policy, which must allow CloudTrail to write and install logs within it. Thankfully AWS have provided a JSON template to allow you to do this with relative ease, as you can see from the <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html#using-an-existing-bucket-for-ct-logs">link</a>.</p>
<p>Once you add this policy to your Bucket, you need to set the variable shown in the red italics with your own settings. The optional prefix allows for better organization, allowing you to create a tiered folder like structure for different trail log files if needed. If this existent Bucket already has a Bucket policy, then take caution as you may need to append or edit your existing policy to account for any new statements.</p>
<p>For users of services requiring read access to the CloudTrail logs, authorization will need to be given for S3 read permissions via the usual methods, such as through IAM or an S3 Bucket Policy or even an S3 access control list. If the logs have been encrypted using the Key Management Service, KMS, additional permissions will be required to read the logs. To read the logs that have been encrypted with KMS, the user or service will also need the decrypt permission associated with the Customer Master Key Policy, as well as existing S3 read permissions. Using KMS on your log files is a great way to add another layer of encryption, over the default encryption applied by CloudTrail, which is SSE-S3, Server-Side Encryption with S3 managed keys. For those unfamiliar with SSE, it’s an encryption method used in Amazon S3 to encrypt any object at rest. It’s completely managed by AWS along with the encryption keys, which themselves are also automatically encrypted and rotated regularly by S3. SSE-S3 uses the 256 bit advanced encryption standard, AES 256 algorithm, for its encryption.</p>
<p>You can choose to implement SSE-KMS encryption, which is Server-Side Encryption with AWS KMS managed keys during the creation of your Trail. If this is selected you must either select one of two options when defining your key. The first option is for CloudTrail to create a new KMS key for you. Which will be your Customer Master Key, CMK. And <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">CloudTrail</a> will apply the correct policy to that Key. Or secondly you can select an existing Customer Master Key, to which you must then apply your own policy with the correct permissions. Before moving on from this point, it’s important to note that if you are using an existing KMS Key, the Key and S3 Bucket must be in the same region. Thankfully in case you forget, you are reminded of this during the configuration process.</p>
<p>Again to save you writing your own policy for own your own chosen Key, you can visit this <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/default-kms-key-policy.html">link</a>. This will allow you to copy the default policy that is applied to a newly created CMK in the first option. Again you simply need to replace the areas as indicated in the example. As I mentioned earlier, if the logs are encrypted with SSE-KMS, then the user will need decrypt permissions which can be found in the following section of this default policy. If you wanted to restrict permissions, you would need to edit the Principal attribute as by default this is left wide open, as indicated by the asterix.</p>
<p>That now brings us to the end of this lecture. Coming up next, we will talk in greater depth <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">about Trails</a> that we touched on earlier in the previous lecture.</p>
<h1 id="Understanding-Trails"><a href="#Understanding-Trails" class="headerlink" title="Understanding Trails"></a>Understanding Trails</h1><p>Hello, and welcome to this lecture where we will explain CloudTrail trails and their configuration in further detail.</p>
<p>We learned earlier that Trails are an essential prerequisite for making use of <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">the benefits of CloudTrail</a>. Simply put, without a Trail, the service is unable to capture API calls. We also learned that these Trails are the foundation building blocks that hold all configuration information for isolating which API calls are to be recorded. So let’s start at the beginning. Creating a Trail via demonstration. I’ll talk through how we create a Trail from within the AWS Management Console. Once we’re at the CloudTrail dashboard, I will then create a Trail and explain the different components to configure in each section. Let’s get started.</p>
<p>Okay, so let’s take a look at how to set up a Trail within CloudTrail. As you can see, I’m currently at the Service page within the AWS Management Console. And if we go across to Management Tools, and go down to CloudTrail, that’s where you’ll find the service. So if we click on that. And that will take us to the dashboard of the CloudTrail service. And if we click on Trails, here is the screen where you can add new Trails and see all your existing Trails that you already have configured.</p>
<p>So, for this demonstration, let’s add a new Trail. The first thing one needs to do is add a Trail name. So, let me label this CloudAcademy Trail. We next need to decide if we want to apply this Trail to all regions. It’s either yes or no. If we select no, then it will apply the Trail to the current region that you have configured within your Management console. For this demonstration though, I’m going to apply the Trail to all regions.</p>
<p>Next, we’re asked to create a new S3 bucket. So, the S3 bucket is the destination of where your log files will be stored, so all the events that are captured that relate to API calls are recorded into logs, and then these logs are delivered to an S3 bucket. So here we can either create a new S3 bucket or say no and select an existing bucket that we have. For this demonstration, I’m going to say yes, and by doing so, CloudTrail will apply all the correct bucket policy permissions to allow CloudTrail to deliver logs to that bucket. So, let’s give this bucket a new name. I’ll call it CloudAcademy Trail. Keep it the same name.</p>
<p>And if I click on Advanced, we’re then presented with a host of other options. Firstly is a log file prefix, and if we click on the information button here, you can see that a prefix makes log files easier to browse. And it does this by creating a folder-like structure for your prefix. So let’s give it a prefix of demo. So, as you can see here, the location is given, the prefix is demo, kind of a separate folder underneath your S3 bucket.</p>
<p>Our next option is to encrypt log files. By default, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">CloudTrail logs</a> are stored on S3 with server-side encryption. So it’s automatically encrypted. However, if you wanted to add an additional layer of encryption, you can say yes to this and select to use KMS, which is Key Management Service. So we can create a new Key Management Service here or use an existing one and select an existing key that we may have. Now, I won’t go into this hugely right now, because I do cover KMS encryption later on in the course. But I just wanted to point out at this stage that if you did want to apply additional security to your logs, then you can select yes to encrypt log files here using KMS. But for this demonstration, we’ll leave that as no.</p>
<p>Now, next on the configuration options is log file validation. We can either enable this as yes or no. Now what log file validation does, it will check to ensure that your log files haven’t been altered, modified, or tampered with in any way, once they’ve been delivered to your S3 bucket. Now, this is typically used for security and forensic investigations or to be compliant with certain security and governance controls. For this demonstration, I’m going to select no for log file validation. And again, later on in this course, I do go deeper into this topic.</p>
<p>Next, we have a notification option using SNS, so Simple Notification Service, that will notify us when a new log file is delivered to the bucket. If I click on yes here, then we can create a new SNS topic or select an existing topic. So if we did have this enabled, then any recipients associated to the topic will be notified every time a new log file is delivered. For this demonstration, I’m going to select no for that. And that’s it. So, to create your Trail, it’s just these few options and configuration details that you need. So now I’ve selected all those. I’m going to click on Create.</p>
<p>And there you can see in my list of Trails, you can see the new name, CloudAcademy Trail. And we specified that it was all regions, gave it a bucket name, a log file prefix of demo, we haven’t associated any CloudWatch logs yet, and the logging status is on. So, although we’ve created this new Trail, there are additional configuration options we can apply. But we can only do that once the Trail is created. So to add that additional configuration, such as adding the option to send the logs to CloudWatch, we’ll need to select the Trail here first.</p>
<p>And this opens up another page with additional configurable options on it. So, let’s run through these from the top. So, the first option here is Apply trail to all regions. And we already indicated yes on the previous screen. However, we didn’t have this option of Include global services. So what does this mean? Global services, such as IAM and CloudFront, aren’t region-specific. So, when events are triggered by these, those events are sent to any Trails that include global services. So, where this says yes. Most services are region-specific, so any events triggered from these are sent to the region that they were made from. It’s also worth pointing out as well that global services are management events, so if your event selectors within your Trail do not include management events, then these global services will not be logged. But I’ll cover event selectors as we go through the rest of this configuration.</p>
<p>As we scroll down to the S3 section, we can see all of the details that we added earlier, like the S3 bucket name, the prefix, whether or not to encrypt the log files, etc, etc.</p>
<p>Now, here on the next section is where we can configure CloudWatch logs. Now, this is an optional component, so what this enables you to do is to send your CloudTrail logs onto CloudWatch. And from there, you’re able to set up specific metrics and monitoring to allow you to monitor for specific events within your CloudTrail logs. So any event that’s recorded, you can set up monitoring for. And if you can monitor on that, then you can also set up an SNS alert against that monitoring when it meets certain thresholds that you have configured. I just want to be clear here, though, that just because we’re sending it to CloudWatch, it also sends it to S3. So the logs are delivered to both the S3 bucket and also the CloudWatch logs to allow you <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/monitoring-with-cloudtrail-1/">to set up monitoring</a>.</p>
<p>Now, if we go into Configure, we can enter a new or existing log group within CloudWatch. If you’re setting up a new log group, then as a part of this process, CloudTrail will set up all the necessary permissions and policies and roles as well, because for this to work, CloudTrail needs to assume a role to carry out a couple of API calls itself. But I’ll cover this in more detail in a later lecture. But just do be aware that if you want to configure CloudWatch logs, then it’s done in this section here.</p>
<p>Before I move on, though, I will click on Continue to quickly show you the roles, etc, that it uses. And you can see here that, in order to successfully deliver CloudTrail events to your CloudWatch logs group, CloudTrail will need to assume a role, and that is for these two API calls here. If you look at the details, we can select the IAM role and create a new role or an existing, and then a policy to go around that role as well. But like I said, I’ll go into this in more detail in a later lecture. We’ll cancel that.</p>
<p>Now we come down to event selectors. Again, this is an optional component. And event selectors specify the types of events that CloudTrail logs. Now, as you can see, there’s three different types of event selectors. There’s the Read&#x2F;Write events, Management events, and Data events.</p>
<p>If we go in to edit these, it gives us a couple more options. So, for the Read&#x2F;Write events, we can select either Read-only, Write-only, or all. So, this, remember, this is referring to what events this Trail will capture and write to a log. So any event that happens in our account, CloudTrail will look at all of the Trails and the event selectors, and if that event matches any event selectors within a Trail, then it will get written to the log of that Trail. So, for this particular Trail, we can have Read-only, Write-only, or All. For this example, I’m going to leave it on All, and this Read&#x2F;Write events applies to both Management events and Data events.</p>
<p>If I go down to Data events first. I’ll come back to Management events in a moment. We’ll go down to Data events first. So, Data events are object-level API calls to S3 objects, such as put, get, and delete object. Everything else is classed as a Management event. So, all other API calls fall under the Management event category. So, for the Data events, you would need to enter a bucket name and a prefix, if required, and then, if you have that enabled, then any object-level API calls to objects within that bucket will be logged as an event within your CloudTrail log.</p>
<p>And like I said, all other events are classed as Management events. You can either have that on or off. If we switch off Management events, then we need to have a Data event configured, because otherwise, if we switch off Management events and don’t have any Data events, then effectively this Trail has no event selectors to log. So, I’ll switch the Management events back to yes.</p>
<p>If we go down to Add new event selector, we can see here that we can add multiple events to the same Trail. So, for example, I can have this as Write-only and switch off Management events and then have a Data event that looks at our CloudAcademy Trail bucket. So, if we look at this first event selector, we can see that any read or write event that happens within our account, that is classed as a Management event, and it will be written to the logs. And also, if we have a write-only event, that falls under a Data event within this S3 bucket, so any kind of delete or put event that happens within this bucket will be recorded to the log as well. But I’m just going to cancel out of that. And leave as the default Management Yes, and the Data events No.</p>
<p>So, coming down to our last configurable item, which is Tags, which again, is optional. If we go into Edit, we can add a unique key value pair here, so, perhaps this relates to a particular project. And we can save our Trail project name of Security Course, or any other tag that’s relevant to your solution. And then simply click on Apply.</p>
<p>And that’s it. That is your CloudTrail configured. And just before I finish this demo, you can see that at the very top here, you can switch your CloudTrail logging on and off. So I can switch that off, and it will say you no longer collect log files in your S3 bucket or your CloudWatch log group. Say Continue. And your login is, from that point, turned off. Now, if we click on Trails, and there again, we can see that our CloudAcademy Trail, the login status is now off. And that’s how you create a new Trail in CloudTrail.</p>
<h1 id="Insight-into-AWS-CloudTrail-Logs"><a href="#Insight-into-AWS-CloudTrail-Logs" class="headerlink" title="Insight into AWS CloudTrail Logs"></a>Insight into AWS CloudTrail Logs</h1><p>Hello and welcome to this lecture on CloudTrail logs.</p>
<p>The logs are the output of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">CloudTrail service</a> and they hold all of the information relating to the API calls that have been captured. And so as a result it’s important to know what you can do with these logs in order to maximize the benefit of the data they contain.</p>
<p>So what is a log file and what does it look like? Log files are written in JSON, JavaScript Object Notation format, much like access policies within IAM and S3. This is a small section of a log file. Every time an API is captured as per the corresponding Trail it’s associated with, an event is written to the log. Remember, a new event is written for each API call. New logs are created approximately every five minutes or so, but they are not delivered to the nominated S3 bucket for approximately 15 minutes after the API was called. So if you are expecting to see a log file for an API you called seven minutes ago, then you might not see the log as expected for potentially another eight minutes. The log files are held by the CloudTrail service until final processing has been completed. Only then will it be delivered to the S3 bucket and optionally AWS CloudWatch logs.</p>
<p>When an event reflecting an API call is written to a log, a number of attributes are also written to the same event, capturing key data about that call as you can see from this example. Without going through every attribute here, I just want to point out some of the more interesting ones. These being eventName. This refers to the name of the actual API that was called. EventSource. This refers to the service as to which the API called was made against. EventTime. This is the time that the API call was made. SourceIPAddress. This disposes source IP address of the requester who made the API call. This is a great piece of information when trying to isolate an attacker from a security perspective. UserAgent. This is the agent method that the request was made through. Example values of these are signin.amazonaws.com. This is what we have in our example and it simply means that a user made this request from within the AWS management console. You also have console.amazonaws.com and this is the same as the previous. However if this was displayed, it would mean that the request was made by the root user of the account. And we also have lambda.amazonaws.com. This is fairly obvious and this would reflect that the request was made with AWS Lambda. UserIdentity. This contains a larger set of attributes that provides information on the identity that made the API request.</p>
<p>Once events have been written to the logs and then delivered and saved to S3, they are given a standard naming format of the following. The first three elements of this naming structure are self-explanatory. The account ID, name of the service delivering the log, CloudTrail and the region that it came from. The next part relates to the date and time, the year, month and days. The T indicates the next part is the time, reflecting hour and minutes. The Z simply means the time is in UTC. The unique string value is a random 16 digit alphanumeric character string that is simply used by CloudTrail as a unique file identifier to ensure that it doesn’t get overwritten with the same name of another file. Currently the file name format is defaulted to json.gz which is a compressed gzip version of a JSON text file. Here is an example of a file name of an existing log file.</p>
<p>Whilst we are looking at structures, let me also talk about the bucket structure where your logs are stored. You may think the logs are all stored in one folder within your S3 bucket. However there is a lengthy but very useful folder structure as follows. Firstly you have your dedicated S3 bucket name that you selected during the creation of your Trail. Next is the prefix that is also configured during the Trail creation and is used to help you organize a folder structure for your logs, corresponding to different <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/understanding-trails-1/">Trails</a>. Following this, a fixed folder name of AWSLogs, followed by the originating AWS account ID. Then another fixed folder name of CloudTrail, indicating which service has delivered the logs. And after that, the region name of where the log file originated from. This is useful for when you have Trails that apply to multiple regions. The last three folders show the year, month and day that the log file was delivered. As you can see, although there are multiple folders underneath your nominated S3 bucket, it does provide an easy navigation method when looking for a specific log file. This folder structure comes into an even greater use if you have multiple AWS accounts delivering logs to the same S3 bucket.</p>
<p>Some organizations may be using more than one AWS account and having CloudTrail logs stored in different S3 buckets across multiple accounts can be inconvenient in certain circumstances and requires additional administration to manage. Thankfully AWS offers the ability to aggregate CloudTrail logs from multiple accounts into a single S3 bucket belonging to one of these accounts. This is why there is an account ID folder within your S3 bucket. Please note that you are unable to aggregate CloudTrail logs from multiple AWS accounts into CloudWatch logs. That belongs to a single AWS account.</p>
<p>So to have all your logs from all your accounts delivered to just one S3 bucket is a fairly simple process with the end result allowing you to essentially manage all your CloudTrail logs. Let’s take a look at how this solution is configured. Firstly you need to enable CloudTrail by creating a Trail in AWS account that you want all log files to be delivered to. Permissions need to be applied to the destination S3 bucket, allowing cross-account access for CloudTrail. Follow the instructions from lecture four on how to set bucket policy permissions. Once permissions have been applied to your policy, you need to edit the bucket policy and add an additional line for each AWS account requiring access under the resource attribute in the section shown here.</p>
<p>Create a new Trail in your other AWS accounts and select to use an existing S3 bucket for the log files. When prompted add the bucket name used in step one and when alerted accept the warning that you want to use a bucket from a different AWS account. An important point to make here when configuring the bucket selection is to ensure that you use the same prefix as the one you used when you configured the bucket in step one. That is unless you intend to edit the bucket policy to allow CloudTrail to write to the location of a new prefix you wish to use. When you have configured your Trail, click create and your new Trail will now deliver its log files to the S3 bucket in your AWS account used in step one of this process. Again this is a great solution that allows you to essentially manage all of your CloudTrail logs in one single account in S3 bucket. However there may be users such as system administrators who manage the other AWS accounts where the logs have come from that might need to access the data within these logs.</p>
<p>So how would they gain access to the S3 bucket to allow them to only access their CloudTrail logs that originated from their AWS account? It could be done quite easily by configuring a few elements within IAM. Firstly, in the master account, IAM roles will need to be created for each of the other AWS accounts requiring read access. Secondly, a policy will need to be assigned to those roles, allowing access to the relevant AWS account’s logs only. Lastly, users within the requesting AWS accounts would need to be able to assume this role to gain read access for their CloudTrail logs.</p>
<p>The easiest way to show you how to configure the permissions required is by a demonstration whereby I shall perform the following steps. I’ll create a new role, apply a policy to this role to only allow access for AWS account B’s folder in S3. I’ll show the trust relationships between account A and account B. I’ll then create a new user in account B and then I’ll create a policy and apply the AssumeRole permissions to this user, allowing them to assume the new role we created in account A. So let’s take a look at how and where we apply these permissions.</p>
<p>Okay so as I just said the first thing we need to do is create a new role in our primary account. So if we go across to IAM which is under Security, Identity &amp; Compliance and then once that’s loaded we need to go across to roles and then create new role. So let’s give this role a name. We’ll call it Cross-account-cloudtrail. Click on next step.</p>
<p>We then need to select a role type and what we want to do is select the role for cross-account access because we’ll be allowing users in another AWS account to access the log files in this primary AWS account. And this will set up the trust relationship between this account and then my secondary account. So for that we will select this top option of providing access between AWS accounts you own. Then next I’ll need to enter the secondary account ID that I want to create the trust relationship with. So I’ll just enter that number. Okay and then after you have entered your account ID, click on next step.</p>
<p>And now we need to attach a policy to this role. Prior to this demo I set up my own policy and this allows cross account access to read only from my secondary account to the bucket on this primary account, but I’ll explain this policy in a few moments and I’ll show exactly what it contains. And from here click on next step.</p>
<p>And this is just a review of the role. So we have the role name, the ARN, the Amazon resource name, the trusted entities, so this is the secondary account ID that I entered and then the actual policy and then that link that we can give to users in the secondary account to allow them to switch roles. So create role. And there we go. The Cross-account-cloudtrail role that we just created.</p>
<p>So let’s take a look at this. Firstly I’ll show you the trust relationships. So because we added cross-account role access and then we entered the secondary AWS account ID, we can see that this account is trusted by our primary account and that allows entities in this account to assume this role. Now I mentioned earlier that I previously set up a policy with permissions in. So let’s take a look at that policy. I named it Cross Account Read Only for CloudTrail so if I show the policy, I should say it’s only a very small policy, very simple. And we have an effect of allow which will allow any S3 get and any S3 list command so essentially read only access on this resource here specified by this line. Now this resource links to the bucket and folder where CloudTrail logs are delivered for our secondary account, as you can see here. So essentially what this policy does is allow read only access to any folders within the secondary account’s CloudTrail log folders. So this account won’t be able to access any other account’s CloudTrail logs which is important. So if we come out of this.</p>
<p>So let’s just have a quick recap of what we’ve achieved so far. So, so far what we’ve done, we’ve created a role in our primary account for our secondary account access and we’ve also assigned an access policy to this role in order for the secondary AWS account to access the relevant folder in S3. So now what we need to do is assign a user in the secondary account and then apply the permissions to that user to enable them to assume the new role in the primary account. So let’s go ahead and do that.</p>
<p>Okay so I’ve now logged into the secondary account where I need to create a new user and assign the correct permissions. So to start with I’m going to set up a permission policy to assign to the user. So if I go down to Security, Identity &amp; Compliance and select IAM. And then go across to policies and from here I want to create a new policy. And I am going to create my own policy, so I’m going to select the bottom option. I’m going to call this AssumeRoleforCloudTrail. My description will be assume role in primary AWS account. And for the policy document I’m just going to paste in a policy that I’ve already created. As you can see it’s only a very small policy again and we have an allow effect that allows the AssumeRole action from the security token service against the following resource. And this resource links back to a role on our primary account where we created the role Cross-account-cloudtrail. So this policy will allow the user to assume this role in the primary account. So let’s go ahead and create that policy. Let’s validate it first and then create.</p>
<p>Now what we need to do is to assign a user to use that policy. Now I created a new user earlier prior to this demo so let’s just find our new policy that we just created and here it is at the bottom, AssumeRoleforCloudTrail. And I’m going to attach a user. And I’ve called our user CloudTrailuser1 and then attach policy. And there we go. So we now have one user attached to this policy.</p>
<p>So that’s all the actions and steps necessary to allow a user in a secondary account to access CloudTrail log files that have been delivered to an S3 bucket in a primary account. And it would do this by using the permission policy that we just applied to that user to access the role in the primary account and that role has a policy attached that allows S3 read access to its own CloudTrail logs.</p>
<p>We won’t go through it again, but recall that you can use KMS to encrypt your log files to offer an additional layer of security. I don’t want to repeat the same information. However I just wanted to bring it to your attention again, highlighting that you can use KMS to offer great security of your log files. Remanding with the security aspect of your log files, CloudTrail allows you enable a feature called log file integrity validation which simply allows you to verify that your log files have remained unchanged since CloudTrail delivered them to your chosen S3 bucket. This is typically used for security and forensic investigations whereby the integrity of the log files are critical to confirm that they have not been tampered with in any way. Log file validation is configured during the Trail process as shown in the previous demonstration.</p>
<p>When a log file is delivered to your S3 bucket, a hash is created for it by CloudTrail. A hash value is a set of characters that are unique that are created from a data source, in this case the log file. The hashing algorithms used by CloudTrail are SHA-256. In addition to a hash, for every log file created CloudTrail creates a new file every hour called a digest file which is used to help verify your log files have not changed. This digest file contains details of all the logs delivered within the last hour, along with a hash for each of them. These files are stored within the same bucket as the log files. However they are within their own folder for easier management. These digest files are then signed by a private key of a public and private key pair. When it comes to verifying the integrity of your log files, the public key of the same key pair is used to programmatically check that logs have not been tampered with in any way.</p>
<p>Verification of the log files can only be achieved via a programmatic access and not via the console. Using the AWS CLI, this can be checked by issuing the following command. The red text shows the optional parameters you can use. The folder structure for the digest is very similar to the CloudTrail logs as you can see. The digest files are clearly distinguishable by the CloudTrail digest folder.</p>
<p>This has now taken us to the end of this lecture. Coming up next I’ll explain how you can use CloudTrail and CloudWatch together as a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/monitoring-with-cloudtrail-1/">monitoring solution</a>.</p>
<h1 id="Monitoring-with-AWS-CloudTrail"><a href="#Monitoring-with-AWS-CloudTrail" class="headerlink" title="Monitoring with AWS CloudTrail"></a>Monitoring with AWS CloudTrail</h1><p>Hello, and welcome to this lecture, where we will look at how <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">AWS CloudTrail</a> interacts with AWS CloudWatch and SNS to create a monitoring solution. In addition to S3, the logs from CloudTrail can be sent to CloudWatch Logs, which allow metrics and thresholds to be configured, which in turn can utilize SNS notifications for specific events relating to API activity.</p>
<p>CloudWatch allows for any event created by CloudTrail to be monitored. This enables a whole host of security monitoring checks to be utilized. A great example of this is to be notified when certain API calls request any significant changes to your security groups or network access controllers within your VPC.</p>
<p>Other examples of these checks that are common within organizations are API calls relating to starting, stopping, rebooting and terminating EC2 instances. If instances are being created that shouldn’t be, your AWS costs could rise dramatically and quickly. Also, if instances are being rebooted or stopped, this could have a severe impact on your services if they’re not configured in a highly available and resilient solution.</p>
<p>Also, changes to security policies within IAM and S3. If changes are being made to your policies that shouldn’t be, access can be inadvertently removed for authorized users and access granted to unauthorized users, having a massive impact on operational services. Even a minor change to a policy can pave the way for an untrusted user to exploit the error.</p>
<p>Looking at failed login attempts to the Management Console. Monitoring failed attempts here can help to prevent unauthorized access at your environment’s front door.</p>
<p>API calls that result in failed authorization. Not only does CloudTrail track successful API calls, whereby the correct authorization was met by the authenticated identity, but it also tracks unsuccessful API requests, too, which would likely be due to permissions applied. Special attention should be applied to these unsuccessful attempts, as this could be a malicious user trying to gain access. However, it could also be a legitimate user trying to access a resource they should have access to for their role, but the incorrect permissions have been applied with their associated IAM policy.</p>
<p>To configure CloudTrail to use CloudWatch, you must first create a trail. Once your trail has been created, you can then configure it to use an existing CloudWatch Log Group, or have CloudTrail create a new one. Having CloudTrail create a new one for you is recommended if it’s the first time doing this, as CloudTrail will take care of all the necessary roles, permissions, and policies required.</p>
<p>You may be wondering why roles and policies are required. So let me give you a high-level overview of the simple process that takes place when sending CloudTrail logs to CloudWatch. When a log file is created by CloudTrail, it is sent to your selected S3 bucket and your chosen CloudWatch Log Group, assuming your trail has been configured for this feature. To allow CloudTrail to deliver these logs to CloudWatch, CloudTrail must have the correct permissions. And these are gained by assuming a role with the relevant permissions needed to run two CloudWatch APIs. The first one is CreateLogStream and this enables CloudTrail to create a CloudWatch Logs log stream in the log group. And the second one is PutLogEvents, and this allows CloudTrail to deliver CloudTrail events to the CloudWatch Logs log stream. Then finally, CloudTrail will then deliver logs to the CloudWatch Logs.</p>
<p>When using the AWS Management Console, you can have CloudTrail create this role for you along with the correct policy. By default, the role is called CloudTrail_CloudWatchLogs_Role. For those that are curious, the policy for this role looks as follows.</p>
<p>It’s important to point out that CloudWatch Log Events have a size limitation of 256 kilobytes on the events that they can process. Therefore, any events that are larger than 256 kilobytes will not be sent to CloudWatch by CloudTrail. Now that you have your logs with the associated events being sent to CloudWatch, you must now configure CloudWatch to perform <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/insight-into-aws-cloudtrail-logs-1/">analysis of your CloudTrail events within the log files</a>. This is done by configuring and adding metric filters to the log within CloudWatch. These metric filters allow to search and count a specific value or term within your events in your log file, which then allows for customizable thresholds to be applied against them. When creating these metric filters, you must create a filter pattern, which determines what exactly you want CloudWatch to monitor and extract from your files. These filter patterns are fully customizable strings, but as a result, a very specific pattern syntax is required. So if you are creating these for the first time, you must understand the correct syntax. AWS has a great page within their documentation that gives full examples of how to create your filter patterns, which can be found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html">here</a>.</p>
<p>Just to reiterate what we’ve spoken about so far, I want to provide a demonstration on how to edit an existing trail to configure it to send logs to CloudWatch Logs. I will then configure a metric filter with the associated metric pattern. Finally, I will set up an SNS alert to notify me when a particular threshold is met, so let’s take a look.</p>
<p>Okay, so what we need to start with is going into CloudTrail to edit an existing trail to enable CloudWatch Logs. So if I go down to Management Tools and click on CloudTrail, and then across to Trails, and this will show our existing trail that we created earlier in the previous demo, which is ca-trail. As you can currently see, under CloudWatch Logs Log group, there’s no log group selected. So if we go into the trail, and then scroll down to CloudWatch Logs, click on Configure, and then we can get CloudTrail to automatically set up this group, and it will create the necessary roles and permissions, etc. So let’s call this CloudTrail&#x2F;Demo.</p>
<p>And then click on Continue, so we’ve given it a name, and here it just gives a message to say that for CloudTrail to deliver event logs to CloudWatch Logs, it needs to assume a role with permissions to run two API calls, which are these two here. If we go down into the details, we can see that the IAM role that it’s going to use is the CloudTrail_CloudWatchLogs_Role, and we’ll ask it to create a new policy. And here’s the policy document, which I showed earlier on the presentation. So go down to Allow. And then if we scroll down to our CloudWatch Logs section, we can now see that we have a log group created in CloudWatch called CloudTrail&#x2F;Demo.</p>
<p>So if we now go across to CloudWatch, if we click on Logs on the left-hand side here, we can see that we have our log group that was just created by CloudTrail, and it’s CloudTrail&#x2F;Demo. Now if we go into our log group, select it, you’ll see this log stream, which is the incoming stream of events being sent from CloudTrail. Now as we’ve only just started, there’s only a few events coming in here, so you might want to wait a few minutes before setting up your metric filters to give you more of a test pattern to search on. So what I might do is just leave it a couple of minutes for some more events to start streaming in before we set up our metric filters here, just so we have something to search on. Okay, so I’ve left it a few minutes. So let’s go back into the log group, and you can see we’ve now got a couple of streams, and if we go into these, we can see there’s a lot more events.</p>
<p>So if we go back a couple of pages, back to our log group, now we need to create our metric filters to allow us to define what we want to search on within our logs. So if we select the tick next to our log group, and then go up to Create Metric Filter, and here within the metric filter, we need to define a filter pattern. Now as I explained earlir, filter pattern will define what we’re actually searching for within our logs. So for this example, I’ll keep it fairly simple. I’m goning to search for any API call that’s being made from my machine, so from my IP address. So for that, I need to enter the following command: sourceIPAddress &#x3D; 2.218.11.188, which is my IP address. And now we can test to make sure that that filter pattern is okay using this Test Pattern box here, and what that does, that will run this test filter on some log data we see from this log here, and the output of that log is in this box here. So all we need to do is click on Test Pattern, and we can see at the bottom here that it found 47 matches out of 50 events in the sample log. So we know that the syntax is okay for this filter pattern, so I’m going to go ahead and assign this metric. And we can see up here that we’ve got our filter name and our filter pattern, and I’m going to create a new namespace for this metric, and I’ll call it Demo, and metric name will be IPAddress. And then all we need to do is click on Create Filter. Now as you can see, our filter has been created, and we have the details in this screen here.</p>
<p>Now what we can do at this point is create an SNS alarm so we can be notified if a certain threshold was met. So let’s go ahead and do that. So the first thing that we need to do is add a name. So I’m going to call this SourceIPAddress, and the description will be Too many calls from my IP. Now I’m going to set this to be 30. So whenever my IP address is used as a source IP address that is greater or equal to 30 times for one consecutive period over five minutes, then I’ll want it to set to a state of an alarm, and I’m going to want to be notified, so I’m going to enter a new list, give this a new topic, SourceIPAddressAlarm, and I want that to be sent to myself. So as we can already see, with the current data it’s got, that it has already breached the alarm, but it has dropped back down below, so we’ll see how this goes. And we’ll create the alarm. And this is a message just to say that I need to subscribe to that AWS notification, and I can do that in just a few moments.</p>
<p>So if we go across to Alarms, we can see that we have our SourceIPAddress alarm in the state of OK. So at the minute, it’s currently below the 30 threshold. As soon as it goes above that, it will alarm, and I will get a notification. Now over the past few minutes, I’ve just been having some activity within the Management Console, and as we can now see, we do have an alarm on our alert. We can see that it just crossed the threshold, and so I’ve received an email notification to say that it is now in a state of alarm, and if we take a quick look at that email, we can see here that it was crossed with a data point of 33, and the threshold was 30.</p>
<p>So that is how you set up CloudTrail to use CloudWatch with the inclusion of SNS to create alarms against API activity. </p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Hello, and welcome to this short lecture to close the course on AWS CloudTrail.</p>
<p>At this point, you should now have a greater understanding of what CloudTrail is and what it can do and some of the use cases for this service. It’s a very powerful tool in a never-ending attempt of enhancing your security solution. Being able to capture every API call made within your environment allows for exceptional auditing which, in turn, makes way for a compliance against certain governance controls.</p>
<p>Having the ability to create multiple Trails allows for different teams and departments to use CloudTrail for different use cases. For example, you may find that your security team want to use CloudTrail linked with CloudWatch and SNS to quickly identify unusual or restricted API calls that are not expected. Whereas another team, might want to a Trail to help with day-to-day operational issues when they occur. Being able to look at the last few API calls leading up to an outage or service interruption could be invaluable in identifying the root cause quickly and effectively.</p>
<p>Although the CloudTrail dashboard, via the management console, allows you to view events from the past seven days that relate to any modified create or delete API call, in a simple query, there are many third-party partners out there that are endorsed by AWS that can provide enhanced analysis of your Logs and Events providing yet an even greater insight into what’s happening within your environment. Many of them offer different unique selling points. So, be sure to look at the wide range of partners available. It’s possible you may already be using one of them within your organization for another service. The list of available partners can be found <a target="_blank" rel="noopener" href="https://aws.amazon.com/cloudtrail/partners/">here</a>.</p>
<p>So, to quickly recap on a few things that we have covered. We have learned that CloudTrail captures all API calls in your environment and in all regions that it is configured to do so. For every API call captured, a related Event is recorded with associated metadata within a Log file. How to set up a Trail and understand the different configurable components. CloudTrail Logs are delivered to a specified bucket in S3. CloudTrail Logs from different accounts can be sent to the same S3 bucket in one AWS account through specific permissions and trusted associations between AWS accounts. CloudTrail Logs are encrypted using SSE-S3 by default, but they can be encrypted with SSE-KMS for increased security. There are a number of different permissions required for creating and reading CloudTrail Trails and Logs. CloudTrail Logs can also be sent to CloudWatch Logs to be monitored against specific metrics using metric filters and filter patterns allowing for greater analysis. Further, CloudWatch can work with SNS to send notifications when configured thresholds are reached.</p>
<p>If you have any feedback on this course, positive or negative, please leave a comment on the course landing page. We do look at the comments in earnest and your feedback is greatly appreciated.</p>
<p>So, that now brings us to the end of this lecture and the end of the course. I hope you have found it useful, and it has answered some questions for you may have had surrounding AWS CloudTrail. Thank you for your time and good luck with your continued learning of Cloud computing. Thank you.</p>
<h1 id="2What-is-AWS-CloudTrail"><a href="#2What-is-AWS-CloudTrail" class="headerlink" title="2What is AWS CloudTrail?"></a>2<strong>What is AWS CloudTrail?</strong></h1><p><a target="_blank" rel="noopener" href="https://d0.awsstatic.com/whitepapers/compliance/AWS_Security_at_Scale_Logging_in_AWS_Whitepaper.pdf">AWS Whitepaper</a></p>
<h1 id="4Understanding-AWS-CloudTrail-Permissions"><a href="#4Understanding-AWS-CloudTrail-Permissions" class="headerlink" title="4Understanding AWS CloudTrail Permissions"></a>4<strong>Understanding AWS CloudTrail Permissions</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-s3-bucket-policy-for-cloudtrail.html#using-an-existing-bucket-for-ct-logs">JSON Template</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/awscloudtrail/latest/userguide/default-kms-key-policy.html">Default CMK Policy</a></p>
<h1 id="7Monitoring-with-AWS-CloudTrail"><a href="#7Monitoring-with-AWS-CloudTrail" class="headerlink" title="7Monitoring with AWS CloudTrail"></a>7<strong>Monitoring with AWS CloudTrail</strong></h1><p><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html">Filter Patterns documentation</a></p>
<h1 id="8Summary"><a href="#8Summary" class="headerlink" title="8Summary"></a>8<strong>Summary</strong></h1><p><a target="_blank" rel="noopener" href="https://aws.amazon.com/cloudtrail/partners/">List of Available Partners</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/134/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/134/">134</a><span class="page-number current">135</span><a class="page-number" href="/page/136/">136</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/136/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
