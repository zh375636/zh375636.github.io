<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/24/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/24/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:19" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:19-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:41:16" itemprop="dateModified" datetime="2022-11-20T19:41:16-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Structure-and-Analyze-Data-with-Google-BigQuery-18/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-BigQuery-17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-BigQuery-17/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Introduction-to-Google-BigQuery-17</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:18" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:18-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:37:24" itemprop="dateModified" datetime="2022-11-20T19:37:24-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-BigQuery-17/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-BigQuery-17/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Welcome to “Introduction to Google BigQuery”. My name’s Guy Hummel, and I’m a Google Certified Professional Cloud Architect and Data Engineer. If you have any questions, feel free to connect with me on LinkedIn and send me a message, or send an email to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>.</p>
<p>This course is intended for anyone who’s interested in analyzing data on Google Cloud Platform.</p>
<p>To get the most from this course, it would be helpful to have some experience with databases. It would also be helpful to have some familiarity with writing queries using SQL, but it’s not a requirement.</p>
<p>This is a hands-on course with lots of demonstrations. The best way to learn is by doing, so I recommend that you try performing these tasks yourself on your own Google Cloud account. If you don’t have one, then you can sign up for a free trial.</p>
<p>To save you the trouble of typing in the URLs and commands shown in this course, I’ve created a GitHub repository with a readme file that contains all of them. The link to the repository is at the bottom of the course overview below.</p>
<p>We’ll start by running some basic queries and saving the results.</p>
<p>After that, I’ll show you how to load data into BigQuery from files and from other Google services.</p>
<p>Then, you’ll see how to stream data into BigQuery one record at a time.</p>
<p>Finally, we’ll wrap up with how to export data from BigQuery.</p>
<p>But first, I’ll give you a quick overview of why you’d want to use BigQuery.</p>
<p>Data warehouses have been around for decades. When databases first became popular, they were primarily used for transaction processing (and that’s still the case today). But managers also needed to analyze data and create reports, which is difficult to do when the data resides in numerous databases across an organization. So data warehouses were created to collect data from a wide variety of sources, and they were designed specifically for reporting and data analysis.</p>
<p>If data warehouse technology has been around for so long, why did Google release BigQuery, and why would you use it instead of a more established data warehouse solution? Well, for two main reasons: ease of implementation and speed.</p>
<p>First, building your own data warehouse can be expensive, time consuming, and difficult to scale. With BigQuery, on the other hand, the only thing you have to do to get started is load your data into it. And you only pay for what you use, so you don’t need to spend a lot of money building capacity to handle peak periods.</p>
<p>Second, even if you do build your own high performance data warehouse, it will probably never be as fast as BigQuery because BigQuery can process billions of rows in seconds. This speed is especially valuable if you need to perform real-time analysis of streaming data, such as from online gaming systems or Internet of Things sensors.</p>
<p>Okay, now if you’re ready to learn how to crunch big data with ease, then let’s get started. We’d love to get your feedback on this course, so please give it a rating when you’re finished.</p>
<h1 id="Running-a-Query"><a href="#Running-a-Query" class="headerlink" title="Running a Query"></a>Running a Query</h1><p>To open BigQuery, go to the Google Cloud Platform console, then find BigQuery in the menu. Alternatively, you can type “bigquery” in the search bar, which is probably easier.</p>
<p>Suppose you wanted to see which US state had the most babies with the same name in one year. There’s a public dataset with baby name data available on BigQuery. If you look under bigquery-public-data, you’ll see one called “usa_names”. If you click on it, you’ll see two tables that are almost the same. We’ll use the first one.</p>
<p>When you click on the table, it brings up the schema. If you click on the Details tab, it’ll show you a description of the data in the table. Let’s make this bigger. If you click on the Preview tab, it’ll give you a sample of the data. If you click the “Query Table” button, it will even give you the skeleton of a SQL query.</p>
<p>However, the query that it put in isn’t complete. You can tell because the Validator circle at the bottom right is a red exclamation point, which means there’s a problem. To see why, click on it and open the Validator. It says the “SELECT list must not be empty”. Let’s “SELECT *” from the table, which, if you’re not familiar with SQL, means select everything. Now the exclamation point has turned into a green check mark, so it’s a proper query.</p>
<p>We should sort the results with the biggest number at the top, so use “ORDER BY number DESC” (for “descending”), and then, since we only need to see the top results, let’s put in a LIMIT of 10. This line is quite long now, so if you want to make it easier to read, select the “Format” option. That’s better.</p>
<p>Now click the “Run” button. It only takes a few seconds to run.</p>
<p>The top result is Robert in New York in 1947, with 10,025 occurrences. You might be wondering if we did something wrong with this query because all of the top 10 names are boy’s names. Let’s look only for girl’s names and see what happens. Add “WHERE gender &#x3D; ‘F’”. Remember to put quotes around the F.</p>
<p>Now it makes sense why we only saw boys’ names before. The highest number of occurrences for a girl’s name was “Mary” in Pennsylvania in 1918, with 8,184 occurrences. Although that’s a lot of Marys, there were 9,054 Roberts in New York in 1951 and that was the 10th highest number of occurrences, so no girl’s names showed up in the top 10.</p>
<p>Before we run any more queries, let’s see how much this is costing us. I’ll cover that in the next lesson.</p>
<h1 id="Pricing"><a href="#Pricing" class="headerlink" title="Pricing"></a>Pricing</h1><p>There are two components to BigQuery pricing: storage and queries.</p>
<p>BigQuery’s storage charges are incredibly cheap. It costs two cents per gigabyte per month, which is the same price as Cloud Storage Standard. What’s even better is that if you don’t edit a table for 90 days, then the price for that table drops to one cent per gig per month until you modify the data in the table again. That’s as cheap as Nearline Storage! In fact, it’s even cheaper because when you read data from Nearline Storage, there is a one-cent per gig charge. With BigQuery storage, you aren’t charged for reading data at all.</p>
<p>Since we’ve only been using public datasets so far, there won’t be any storage charges.</p>
<p>The only other charge is for queries. (There’s also a charge for streaming data to BigQuery in real-time, but that doesn’t apply to these examples and I’ll cover it in another lesson.) For queries, the first terabyte per month is free. After that, it costs $5 per terabyte, which is half a cent per gigabyte. Wait a minute, didn’t I just say that you aren’t charged for reading data from BigQuery storage? Yes, that’s true because BigQuery charges query fees regardless of where you read the data from. For instance, if you query a dataset that’s in Cloud Storage, then you get charged at the same rate that you would from querying a dataset in BigQuery storage, so the charge isn’t for reading – it’s for processing.</p>
<p>For high-volume customers, there’s also flat-rate pricing, but it’s only worthwhile if you spend at least $2,000 per month. It only applies to query costs and not storage, which is still separate.</p>
<p>To see how much data is processed by a query, look in the Validator message area. Since there isn’t an error in the syntax, now it’s showing how much data would be processed by the query above. In this case, it’s 163 MB. Considering that the first terabyte of processing in a month is free, this won’t cost us anything, but even if we were already over the 1 terabyte mark this month, it wouldn’t cost much. How much? Less than a tenth of a cent. I’d say that’s pretty reasonable.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Saving-Query-Results"><a href="#Saving-Query-Results" class="headerlink" title="Saving Query Results"></a>Saving Query Results</h1><p>Now that you know how inexpensive it is to store data on BigQuery, you may want to save some of your query results there too.</p>
<p>It’s very easy to do that. First, though, since you don’t have a dataset to put tables in, you’ll need to create one. Click your project name and then click “Create Dataset”. You need to give it a name, so call it “babynames”. That’s the only thing you have to specify in this dialog box, but if you want to, you can set the location, which you might want to do for compliance reasons. You can also set an expiration time so that a certain number of days after a table has been created, it will be automatically deleted. You would do this if it’s temporary data that you don’t want to have to remember to delete later. Now click “Create dataset”, and you should see your new dataset show up under your project name.</p>
<p>Great, now we’re ready to run a query. Let’s open one of our previous queries, so we don’t have to type a new one in. If the list of your previous queries isn’t already showing, then click “Query History”. Click the button to the right of your first query to bring it up again.</p>
<p>Now before you re-run this query, select “Query settings” from the menu…and then select “Set a destination table for query results”. It has already set the right project and dataset name. Now we just need to tell it which table to use. If the table doesn’t already exist, it’ll create it, so you can type in whatever table name you want. Let’s call it “babynames_top10”. Click Save and then click “Run”.</p>
<p>Now if you click on the babynames dataset, you’ll see that it created the “babynames_top10” table. Then click the “Preview” tab and you’ll see the results from the query.</p>
<p>What if you decide to save the results to a table after you’ve run the query? That’s easy too. Click “Query History” and click on the first query. We didn’t specify that the results should be saved to a table when we ran this query the first time, but if you scroll down, you’ll see that it saved the results to a temporary table. Click that to see the table. Now you can click “Copy Table” and specify where you want to save it. You have to select the dataset first and then type in a new table name.</p>
<p>One more thing to be aware of is that if you don’t specify a destination table and it puts the results in a temporary table, the temporary table stays in cache for about a day. So if you run the query again within 24 hours, it’ll retrieve the cached copy and you won’t be charged for the query.</p>
<p>However, if you run a query again and specify a destination table, like we just did, then it won’t read the data from cache. So let’s run it again without the destination table option.</p>
<p>If you go to the “Job information” tab, you can see that it didn’t process any bytes because the results were cached. Also, the duration of the query was zero seconds because it just retrieved the results from cache rather than running the actual query. Of course, since we could have just looked at the results in the temporary table, there may not seem to be a lot of point in re-running the query. That’s probably true if you’re using the web interface, but if you’re using the bq command or the BigQuery API, then it might be useful in some cases.</p>
<p>Before we go, let’s get rid of the table we created. Click on the dataset name, and then click “Delete Dataset”. This will delete the dataset and all of the tables in it, so you have to type the name of the dataset before it’ll delete it. </p>
<p>Okay, that’s it for this lesson.</p>
<h1 id="Loading-Data"><a href="#Loading-Data" class="headerlink" title="Loading Data"></a>Loading Data</h1><p>It’s great having public datasets in BigQuery that you can use, but what about analyzing your own datasets? How do you get them into BigQuery? There are many ways to do this. If your data is already in another Google service, then there’s usually a way to get it into BigQuery, although sometimes it requires an intermediate step. Some of the most commonly used sources are Cloud Storage, Google Drive, Firestore, Cloud Operations, Bigtable, and Google Analytics.</p>
<p>If your data isn’t in a Google service, then you can upload it to BigQuery through the web interface, the command line, or the API. Uploading through the web interface is the simplest, although it does have limitations.</p>
<p>Suppose you want to upload a dataset of the number of occurrences of all baby names across the US for a particular year. Since you don’t already have a copy of this dataset, you’ll need to download it from the Social Security website. The URL is in the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">repository</a> for this course. Then unzip the file. You can see that it contains one text file for each year from 1880 to 2019. Let’s have a look at the 2019 file.</p>
<p>It’s a very simple file, with a name, a gender, and the number of occurrences of that name for babies born in 2019. Although the filename extension is “txt”, it’s actually a comma-separated values file (or CSV). BigQuery can upload files in 5 formats: CSV, JSON, Avro, ORC, or Parquet.</p>
<p>Before you can upload a file, you need to create a dataset and table in BigQuery to hold it. Let’s call it “babynames”.</p>
<p>Now you need to create a table in that dataset, so click the “Create Table” button. Change the Source to “Upload”. If you click on the menu, you’ll see that you could also choose Cloud Storage, Google Drive, or Cloud Bigtable. If your data were in one of those three services, then you could load it into BigQuery from here or you could even leave the data where it is and make it an external data source, also known as a federated data source. However, the performance is usually slower when you query an external data source than if the data resides in BigQuery storage, so it’s often better to copy the data into BigQuery instead.</p>
<p>Okay, back to the task at hand. Set the Location to “Upload” and click the “Browse” button. Then select the “yob2019.txt” file. Change the file format to CSV. Now you need to give your table a name. Let’s call it “names_2019”.</p>
<p>You’ll notice there’s an option to automatically detect the schema and input parameters. That’s often a very handy feature because it saves you from having to enter the schema manually. Let’s see if it works with this data file.</p>
<p>It got an error. The message is a little bit cryptic, but here’s what happened. When BigQuery tries to detect the schema, it only looks at the first 100 records. In this file, the first 100 records all have ‘F’ in the second column. BigQuery assumes this means that the second column can be either an ‘F’ for “False” or a ‘T’ for “True”, so it sets this column to Boolean. When it tries to upload records with ‘M’ in that column, it gets an error because it’s expecting an ‘F’ or a ‘T’.</p>
<p>In situations like this, you have to specify the schema manually. Fortunately, it’s pretty easy in this case. First, I’ll redo everything except the schema.</p>
<p>Okay, now instead of asking it to auto-detect the schema, click “Add field”. The first field is the name, so type “name”. It’s a string, so the type is set correctly. The mode is set to “Nullable” by default, which means that this field can be empty for some records. If we wanted to make sure that no records were missing the name, then we would set the mode to “Required”. This file isn’t missing any names, but let’s leave it as Nullable anyway.</p>
<p>Now we’ll add the second field. Call it “gender”, and leave it as a string. The third field is the number of people who have this name, so call it “count”. It’s a whole number, so set the Type to “Integer”. Okay, now click “Create table”. It only takes a couple of seconds to upload all of the data into the table.</p>
<p>All right, go to the table and click on the Preview tab to see a sample of the data. That looks right. At this point, you could run queries on this table just like you did with the table in the public dataset.</p>
<p>Okay, that was all pretty easy, but remember when I said that the web interface has limitations? One big limitation is that you can only upload files that are 10 megabytes or less in size. There are lots of data files that are bigger than that. Here’s an example. It’s a 35 meg file that contains over 200,000 questions and answers from the game show, Jeopardy. You can find the URL for this file in the GitHub <a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">repository</a>. If you try to load it using the web interface, it’ll give you an error.</p>
<p>In cases like this, you need to use the command line. Alternatively, you could upload it to Cloud Storage first and then move it to BigQuery, but that would be a bit of a hassle, so it’s better to use the command line.</p>
<p>If you haven’t used any of Google Cloud’s command line tools yet (that is, gcloud, gsutil, kubectl, or bq), then you’ll have to install the Google Cloud SDK. The installation instructions are at this <a target="_blank" rel="noopener" href="https://cloud.google.com/sdk/docs/install">URL</a>. If you need to install the SDK, then pause this video while you do that. .</p>
<p>Okay, first you need to create a table to put data in, which we could do through the web interface again, but let’s use the command line for that too. Before we create the table, we need to have a dataset to put the table in. If we had used a more generic name for the previous dataset, instead of “babynames”, then we could have created a table for the Jeopardy data in that dataset too. Let’s not make that mistake again. What should we call it? Maybe we should put all of the data that we downloaded from the internet in it. We could call it “downloads” or something like that, but maybe we should just call it “public”.</p>
<p>The command for all BigQuery operations is “bq”. To create a dataset, type “bq”, and then “mk” for make, and then the name of the dataset, which is “public” in this case.</p>
<p>Okay, the dataset was created. Now we need to create a table. Let’s call the table “jeopardy”. There are a couple of ways to create the table. You could create an empty table and then upload the data into it or you could do it all in one step, which is usually easier.</p>
<p>To upload a file, type “bq load”, then the autodetect flag, which tells it to automatically detect the schema so you don’t have to specify the schema yourself. Then type the name of the table you want to load it into. If you haven’t set a default dataset, then you also need to specify the dataset name. In this case, you would type “public.jeopardy”. Then type the filename you want to upload, which is JEOPARDY_CSV.csv. If you’re not in the directory where the csv file resides, then you’ll have to put in the pathname to that file. It’ll take a little while to upload the file.</p>
<p>By the way, another reason to use the command line instead of the web interface is if you need to upload lots of files at the same time. With the bq command, you can put an asterisk in the filename, which will act as a wildcard and upload all matching files.</p>
<p>Let’s have a look in the web interface again to make sure the file uploaded properly. You have to refresh the page first so you can see the updates. Great, there’s the “public” dataset and there’s the “jeopardy” table. Click on the jeopardy table, and then go to the Preview tab. It looks good. Did you notice that the column names are actually descriptive? They have names like “Show Number” and “Category” instead of generic names like “string_field_0”. That’s because the first line of the csv file listed the field names.</p>
<p>On a different topic, you might be wondering why we didn’t just rename the “babynames” dataset to “public” instead of creating a new dataset. Well, that’s because you can’t rename a dataset in BigQuery. The only way to do it is to create a new dataset, copy all of the tables from the old dataset to the new dataset, and then delete the old dataset and its tables. So choose your dataset names carefully.</p>
<p>Let’s move the names_2019 table from the babynames dataset to the public dataset. Click on it, then select “Copy Table”. Now change the destination dataset to “public” and call the destination table “babynames_2019”. Click Copy. It takes a few seconds. Now click on the “babynames” dataset, and click “Delete dataset”. This will delete the dataset and all of the tables in it, so you have to type the name of the dataset before it will delete it. There, it’s done.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Streaming-Data"><a href="#Streaming-Data" class="headerlink" title="Streaming Data"></a>Streaming Data</h1><p>So far, we’ve run queries on public datasets and on pre-existing data that we loaded into BigQuery. But there’s another way to get data into BigQuery – streaming, where you add data one record at a time instead of a whole table at a time. One example is if you need a real-time dashboard that gives you an up-to-the-minute (or even up-to-the-second) view of your data.</p>
<p>One common way to stream data into BigQuery is to use Cloud Dataflow, but that’s the subject of another course. You can stream data directly into BigQuery by calling the API in your software. I’ll show you an example using the API Explorer, which lets you make an API call from a nice interface in your browser.</p>
<p>Before we do that, we need to create a table to stream the data into. We should also create a new dataset because we’re not going to put public data in the table, so the public dataset isn’t the right place to put it. Let’s call it “streaming”. Now create the table. This time, we’re going to leave it as “Empty table” because we’re not loading any data into it right now. Let’s call the table “stream”.</p>
<p>For the schema, we could put in pretty much anything, since we’re just going to stream some test data, but let’s have at least two fields. Create a string field called “greeting” and a string field called “target”. And click “Create table”.</p>
<p>Okay, now we’ll go to the BigQuery API documentation. The URL is in the GitHub repository for this course. We’re going to use the “tabledata.insertAll” method to stream data into the table.</p>
<p>On the right-hand side is a form you can fill out to call the API. First, put in your project ID (which will be different from mine, of course). You can get your project ID from BigQuery. Then put in “streaming” for the datasetId, and “stream” for the tableId.</p>
<p>Now, in the “Request body” text box, click the plus sign and select “rows”. This saves you from having to type in everything yourself. It’s especially helpful with the brackets because we’re going to have quite a few nested brackets in a minute. Now click the plus sign under “rows” and select “[Add Item]”. It put in a couple more brackets. Then click the plus sign between those and select “json”.</p>
<p>Now we finally need to type something. This is where we say what data we want to put in the row we’re adding to the table. If you’ll recall, the first field is called “greeting”, so type that between quotes after the curly bracket. Then type colon quote Hello quote comma. Hit Enter. We called the second field “target”, so type quote target quote colon quote world quote.</p>
<p>All right, that’s a complete record, so the request body is complete. It added a bunch of junk at the bottom for some reason, so just delete those lines. Good, the errors went away.</p>
<p>Now click the “Execute” button. Okay, the return code is 200 and it’s colored green, which means the API call was successful. It also didn’t return an error, so it looks like it worked. Let’s see.</p>
<p>Go back to the BigQuery page and click on the “stream” table. Click the “Preview” tab. Great, there’s our “Hello world” message that we just sent.</p>
<p>Although that was a quick way to show you how streaming works, you likely won’t be calling the API directly when you write your own streaming code. Instead, you should use the BigQuery client libraries. </p>
<p>Google provides client libraries for C#, Go, Java, Node.js, PHP, Python, and Ruby. There’s a different way of doing a streaming insert for each language, but it’s easier than writing the API call yourself. If you need to write code to stream data into BigQuery, have a look at the documentation for your language.</p>
<p>Oh, and one more thing. Remember in the pricing lesson when I said that there’s a separate charge for streaming? Well, it’s 5 cents per GB to do streaming inserts. That actually makes it the most expensive BigQuery operation. Loading data in any other way is free, querying data costs half a cent per gig, and storing data costs 2 cents per gig, at most. But if you need to do up-to-the-minute analysis of streaming data, then 5 cents a gig is still pretty cheap.</p>
<p>And that’s it for this lesson. </p>
<h1 id="Exporting-Data"><a href="#Exporting-Data" class="headerlink" title="Exporting Data"></a>Exporting Data</h1><p>Sometimes you need to export data from BigQuery, such as when you want to use third-party tools on the data. Exporting is pretty easy, but there is only one place you can export the data to and that’s Cloud Storage. So if you want to export data anywhere else, you have to export it to Cloud Storage first, and then download it from there.</p>
<p>Let’s do that with the babynames data. Click the table name, then select “Export to GCS” from the “Export” menu. It supports CSV, JSON, and Avro formats. Just for something different, let’s select JSON. Then you can choose whether to compress it or not. GZIP is the only compression option. This is a pretty small file, so let’s not bother compressing it.</p>
<p>Now you need to specify the Cloud Storage location where you want to save the file. I’ll put it in my ca-example bucket, but you’ll have to put it somewhere else, of course. Make sure you have write access to whatever bucket you specify. You also have to put in the filename. I’ll call it babynames_2019.json. Then click the Select button…and the Export button.</p>
<p>It tells you that it started an export job. It doesn’t take long to finish. Now if we go to the Cloud Storage bucket, we can see that the file was created. If you click on the filename, you can download it to your computer.</p>
<p>Now if you open it up, you’ll see the data in JSON format, which looks far different from the CSV file we originally uploaded into the table.</p>
<p>If you want to see a history of the exports you’ve done, click on Job History. If you click on one of the jobs in the list, it’ll give you more detail. Notice that it also lists other types of jobs, such as when we loaded the data originally. You can even re-run a load job from here if you want.</p>
<p>You can also use the bq command to run an export job, but the option is called “extract” rather than “export”. That is, you use the “bq extract” command.</p>
<p>As you’ve seen, exports are quite straightforward, but they get a little more complicated if you need to export more than a gig of data. Here, I’ll show you what happens. Take a look at the games_wide table in the baseball dataset. It’s 1.76 gig. I’ll try to export it, and I’ll even compress it, so hopefully the exported file will be less than one gig. </p>
<p>It gives us an error. It says that it’s “too large to be exported to a single file. Specify a uri including a * to shard export.” That second sentence is a little cryptic, isn’t it? What it means is you have to include a wildcard so it knows to export the data in multiple files.</p>
<p>Here’s how to do that. I’ll export it again, and this time, I’ll put an asterisk after “games”. You can put the wildcard anywhere in the path except for the bucket name, but putting it just before the first file extension is usually the best place to put it as you’ll see in a second.</p>
<p>It’s running this time, which is a good sign. It’ll take a lot longer, so I’ll fast forward. Now it’s done, so if I go back to Cloud Storage, you’ll see two new files with long numbers in them. BigQuery simply appends numbers starting from 0 and goes up by one for every file, but it puts in 12 digits with all zeros at the beginning, just in case it needs to split the data into a lot of files.</p>
<p>Also notice that the sum of those two files is nowhere near one gig. It’s less than a hundred meg. That’s because BigQuery looks at the size of the source data rather than estimating the size of the destination file when it decides whether you have to split it into multiple files or not.</p>
<p>Okay, if you don’t want to incur ongoing storage charges for the data you’ve saved in this course, then you should remove it. First, delete the files you exported to Cloud Storage. Select all three files, and then click “Delete”. Now go back to BigQuery, and delete the public dataset you created. You have to type “public” to confirm the deletion. Then delete the streaming dataset. There, now everything you loaded or exported in this course should be gone.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>I hope you enjoyed learning how to use BigQuery. Now you know how to load data, run queries and save the results, stream data one record at a time, and export data. Let’s do a quick review of what you learned.</p>
<p>BigQuery’s advantages over on-premises databases are ease of implementation and speed.</p>
<p>If you want to import data that’s already in another Google service, then there’s usually a way to get it into BigQuery, although sometimes it requires an intermediate step. It’s also possible to query data in certain Google services without importing it into BigQuery. However, the performance is usually slower when you query an external data source than if the data resides in BigQuery storage.</p>
<p>If your data isn’t in a Google service, then you can upload it to BigQuery through the web interface, the command line, or the API. One limitation of the web interface is that it can only upload files that are 10 megabytes or less in size. The command-line tool for all BigQuery operations is “bq”.</p>
<p>When you’re uploading data, BigQuery includes an option to automatically detect its schema, but it doesn’t always work, so you may have to specify the schema manually.</p>
<p>Another way to get data into BigQuery is streaming, where you add data one record at a time instead of a whole table at a time. This is most useful for real-time applications. Although there’s no cost to upload data to BigQuery in bulk, it does cost money to stream data into BigQuery. To add streaming code to your applications, it’s easiest to use Google’s BigQuery client libraries, which are available for many different languages.</p>
<p>BigQuery stores data in tables, and each table must be part of a dataset. You can’t rename a dataset in BigQuery.</p>
<p>When you run a query, if you don’t specify a destination table, it puts the results in a temporary table. This temporary table stays in cache for about a day. So if you run the query again within 24 hours, it’ll retrieve the cached copy, and you won’t be charged for the query.</p>
<p>Cloud Storage is the only place where you can export data from BigQuery. If you need to export more than one gig of data, then you have to shard the data into multiple files by including an asterisk in the destination filename. You can also use an asterisk when you’re uploading files.</p>
<p>To learn more about BigQuery, you can read Google’s online documentation. You can also try one of the other BigQuery courses on Cloud Academy.</p>
<p>Please give this course a rating, and if you have any questions or comments, please let us know. Thanks!</p>
<h1 id="5Loading-Data"><a href="#5Loading-Data" class="headerlink" title="5Loading Data"></a>5<strong>Loading Data</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/cloudacademy/bigquery-intro">Course GitHub repository</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.google.com/sdk/docs/install">Google Cloud SDK Installation Instructions</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:16" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:16-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:37:48" itemprop="dateModified" datetime="2022-11-20T19:37:48-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Introduction-to-Google-Cloud-Firestore-and-Datastore-16/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Google-Cloud-Firestore"><a href="#Google-Cloud-Firestore" class="headerlink" title="Google Cloud Firestore"></a>Google Cloud Firestore</h1><p>Hello and welcome, this is Introduction to Cloud Firestore with App Engine, and in this lesson, we’ll be exploring the basic functionality of Cloud Firestore and how it’s used with App Engine. By the end of this lesson you should be able to: explain the purpose of Cloud Firestore, explain the relationship between Cloud Datastore and Cloud Firestore, explain when to choose Datastore Mode, and explain how to use Datastore with App Engine. </p>
<p>Most applications need a place to store queryable data, and fortunately, there is no shortage of databases to help. However the choice of database is not always a simple one, it depends on multiple factors including but not limited to the type of data being stored, how the data will be consumed, et cetera. Technical limitations are another concern. For example, will the database be able to support the amount of traffic? </p>
<p>No matter what kind of database you’re using, if it can’t support the workload that you intend to use it for, it’s not all that useful. Remember, App Engine can keep scaling up to support the demand and that means the database needs to as well, and that’s why Google created Cloud Datastore and paired it with App Engine. Now, that doesn’t mean it’s the only database you can use though it was designed to be a good likely pairing for many apps. So, if you’re interested in learning more, then let’s get started. </p>
<p>Google released Datastore in 2013 to serve as App Engine’s database, and for years that is how it has been. Datastore is a highly scalable, fully-managed, no-SQL document database. Supporting both eventual and strong consistency, it supports transactions and it offers a SQL-like query language called GQL. It stores data as properties of an entity with support for multiple data types, and it categorizes entities based on a developer supplied Kind. </p>
<p>To make entity lookups perform quickly, entities include a Datastore property named a Key which is a unique ID, and to allow entities to be queried, Datastore allows developers to create indices based on the properties for which we want to filter. </p>
<p>I mentioned queries being similar to SQL, they’re similar but not exact, and there are some limitations for which I’ll include a link App Engine is designed to run web apps and mobile backends which are broad categories with a wide range of storage requirements. There’s really just no single option that’s going to work for all workloads. If we were to compare a mobile chat application with a mobile news feed, both of them have very different storage needs. The same goes for web applications, let’s say a brochureware application is going to have different storage needs than a site such as Twitter. </p>
<p>In 2014, Google acquired a realtime database called Firebase. Realtime databases are used for data which is always changing and require processing to happen very quickly. Shortly after that acquisition, Google started building Cloud Firestore. It took the best parts of Firebase, it took the best parts of Datastore, and it smashed them together into a single service. </p>
<p>Now, the purpose of Cloud Firestore is to serve as the next generation no-SQL database for <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud</a>. This is where we get a bit tangled. Cloud Firestore provides one of two operating modes that are called Native Mode and Datastore Mode. Using Cloud Firestore in Datastore Mode is the next generation of Cloud Datastore. It supports Datastore’s API and it uses Firestore’s data storage layer which removes some of the previous Datastore limitations. </p>
<p>Now, currently the original Datastore and Firestore in Datastore Mode are two different products, however, Google will be moving users over to Firestore seamlessly over time. Using Firestore in Native Mode is not the next generation of Firebase, at least not publicly, at least not in this moment. Using Cloud Firestore in Native Mode is similar to Firebase, though there are some implementation changes. </p>
<p>So with all of this, the logical question is, how do I know which mode to use? Here’s some general advice, now, it’s not universally applicable, though it is a good starting place. Datastore or Firestore in Datastore Mode are intended for server workloads, meaning a service-side application interacts with a database. Now, this is exactly what App Engine does, so if you’re building with App Engine and you need a schema list database, then you’ll probably wanna use Firestore in Datastore Mode. Firestore in Native Mode is similar to Firebase, which is basically an application platform and doesn’t require developers to create service-side applications. </p>
<p>Firestore Native and Firebase both provide STKs which allow read and writes to documents for which users have access and all without a server, and if you need some sort of backend functionality, it also supports running Cloud functions based on event triggers which include, when a document is created, deleted, updated, et cetera. So Firestore in Native Mode is very cool, but it’s not the focus of this lesson, so we’re not going to go into detail, this lesson is really focused on using Datastore Mode with App Engine, though if you wanna learn more, I’ll include some links for further reading. </p>
<p>Before moving on, there are some things to commit to memory, each project can use Firestore in one mode only. So if you select Datastore Mode, that’s the mode for your project and vice versa with Native Mode. Also, and importantly, if you create an App Engine app inside a project, it’s going to automatically choose Datastore Mode, so be careful with that, if you really don’t wanna Datastore Mode, don’t create an App Engine app in that project first. </p>
<p>Alright, with all of this out of the way, how do you actually use Datastore with an App Engine application? Due to the ever-evolving nature of App Engine, there have been different methods for interacting with Datastore which varied by runtime. Currently, Google recommends the use of the Google Cloud Client Libraries. Google provides these libraries for different runtimes, and they allow engineers to interact with Datastore in the language that they’re familiar with. If for some reason you’re using a language that does not have a supported library, you can always fall back to the Rest API and use that directly, it just requires a bit more effort on your part. </p>
<p>Before wrapping up, let’s check out a demo of Cloud Datastore in the console. I’m here on the Datastore Dashboard, I’ve already enabled the Datastore API and I have some sample entities which store some dummy data. Notice here I have a Kind called EmailEvent and there are four entities. This Entities page is where we can see the different entities for a specific Kind which is specified in the dropdown. And if we click on Create it will open a form where we can add a new entity. </p>
<p>The Namespace is used to partition data, an example might be to specify a company name which would allow for a multitenant app, and if you don’t specify a namespace that’s fine, the default is used, just know, they can’t be changed once they’re set. Datastore uses the concept of a Kind to categorize an entity and that makes it easier to query specific types of entities The Key is used to look up an entity quickly, notice these properties here, these are added by Datastore to make it easier for us to populate this, it knows that this is an EmailEvent Kind so it’s given us the properties that exist on some of the other entities. </p>
<p>This is just a nice feature from the user interface to make it easier to enter our data, remember, this is a schema list database, so we don’t have to define specific properties for even the same Kind, I could just remove all of these and have a totally different property for one EmailEvent than I do for all the rest. I can’t imagine a use case in which you’d want to do that, but it is possible. </p>
<p>The query language that Datastore supports is called GQL and again, and it resembles SQL up to a certain point, so if we start by typing select star from followed by the name of our Kind, we can see this returns all EmailEvent Kinds with all the properties, though you could also specify properties to return and that way, you can get just the data you need. </p>
<p>OK, let’s stop here and see how we did. The purpose of Cloud Firestore is to serve as the next generation no-SQL database for Google Cloud. To get there, it took the lessons learned from Datastore and Firestore, probably other stuff internally as well, and put them all into a single product. Cloud Datastore is currently both its own service and a mode of Firestore, where Firestore in Datastore Mode is the latest generation of Datastore and will replace Datastore. Datastore Mode is intended for server workloads and if you’re pairing with App Engine, then it’s likely a good choice. To interact with Datastore or Datastore Mode in software, we would use the Google Cloud Client Libraries, though we could always use the Rest API directly if we needed to. </p>
<p>Alright, that’s going to do it for this lesson, I hope this has been helpful, I hope it’s filled in a few of the gaps, and I will see you in the next lesson.</p>
<h1 id="Google-Cloud-Datastore"><a href="#Google-Cloud-Datastore" class="headerlink" title="Google Cloud Datastore"></a>Google Cloud Datastore</h1><p>In this lesson, we’ll dive deeper into Cloud Datastore. We’ll cover queries and indexes and entity groups and transactions.</p>
<p>Let’s start with queries. A query can specify a kind and then zero or more filters and zero or more sort orders. We can filter on properties, keys and ancestors. Filters are basically pretty simple. Here’s an example in Python outside of the context of an actual application. We define the q variable as a query for the person kind, and then we filter it on the person name &#x3D; John, and then we can add a sort order by calling the order method, so we say order and then we specify the name, and then if we add a hyphen in front of it, it makes it descending, and we can query on ancestors as well with something like this. We can use an ancestor query specifying ancestor &#x3D; and then the key.</p>
<p>Let’s check out an example from our actual application. If we look at the images.py file, we can see that we’re using a class method called for category to fetch all of the images for a given category. It uses an ancestor key as a query filter and this allows us to get all of the images that belong to the category that was passed in. So if we were to break down this code, it would translate into something like we get the key, based on the urlsafe key, and then we use that to query all of the images that have that category as an ancestor. We sort by the created on date, descending, and then we take the last 20 results. It’s a fairly simple-to-use API, but it’s very powerful.</p>
<p>With a traditional relational database, we use indexes to improve performance. Due to the design of Datastore, we use one or more indexes for any query we run. With Datastore, there are basically two types of indexes. We have single property indexes and composite indexes. Single property indexes are automatically created for us which means each individual property is indexed, allowing us to query it. Now, these indexes take up space, which means there’s cost attached to it, so we also have the ability in our code to say index &#x3D; fault. This will allow us to skip indexing properties that we won’t ever be querying. This is going to save us money.</p>
<p>Okay, there are some limits on the queries we can run with single property indexes. We can use equality filters on one or more properties, which is a merged join, so something like first name &#x3D; Bob and last name &#x3D; James, this works because even though we’re querying on two properties, we’re using an equality filter so it’s merging the results, and we can use inequality filters on one property, such as first name &gt;&#x3D; to the letter B and first name is &lt; the letter C, and only one sort order can be defined on a single property query. Now, if we want to query on multiple properties, we can create a composite index. We can create it manually using the if find YAML syntax, or we can run our queries on the development server and it’s going to generate an index.yaml file or a datastore-index.xml file for Java.</p>
<p>Let’s check out our index.yaml file. Right here at the top it says auto-generated and that’s because when we run any code that runs a query against the development version of Datastore, it builds the index.yaml for us. That way when we deploy, App Engine knows what indexes it needs to build. Here’s an example of what a composite index might look like. We have a last name and a first name and they’re both ascending.</p>
<p>For multi-valued properties, like our tags, it looks similar except an index entry gets created for every value of a property. We can query multi-valued properties if at least one value matches the filters. We saw that when we checked out the tags on our images in a previous lesson. It’s considered best practice that we don’t index very long strings. Instead, we should be using the Search API which gives us Google-like search capabilities. Also, we should clean up old indexes using the appcfg vacuum_indexes command, and if we have properties that shouldn’t be indexed, maybe something like a very long string as we just mentioned, we can flag them as not indexed with the indexed &#x3D; false.</p>
<p>We’ve talked about how Cloud Datastore is fast and efficient for querying, but why is that? It’s because we use the indexes to shift the cost of querying to upfront when the index is created, so sometimes it’s going to take a little while for the indexes to initially build if we have very large data sets, though once an index is built, then querying them is very fast. Let’s talk about consistency with Datastore. We’ve talked about eventual and strong consistency a few times.</p>
<p>The difference is basically that for strong consistency, the data we read is the last data that was written, and with eventual consistency, the data that we read may not be the last data written. Eventual consistency is great for when we don’t need anything critical. This can be things like a blog post, and we’d use strong consistency when it’s vital to see the latest updates. Now, this can be for things like the price of a product in our catalog. If we need strong consistency, we have a few options. We can use an ancestor query. We can fetch an entity using the get method on a Key, or we can use a transaction. We’ve talked about the first two throughout our discussion on Datastore. However, we haven’t talked about transactions, so let’s dive into that a bit more.</p>
<p>We can use transactions to gain strong consistency. Let’s say that we wanted to update a property and in this example, it’s the amount of tickets available for a conference. So, we’re able to ensure that if this executes successfully, any future queries will have this data. The ndb library makes it easy to perform transactions with this transactional decorator, along with the other methods that we can find in the API documentation. Now, sometimes we’re going to need to work with entities that are not part of the entity group we’re using.</p>
<p>Let’s say we have two bank account entities that are not part of the same group, and we want to transfer funds from one to another. We want to be able to ensure strong consistency with something like this since eventual consistency could result in something like withdrawing more money than we should have available or not being able to withdraw enough money that we should have. For something like this, we can use cross-grouped transactions to ensure strong consistency. We still use the same transactional decorator. However, we set the xg parameter to true.</p>
<p>There are some best practices for transactions. First, because entity groups can only be written to once per second, we need to consider the design of our entity groups in advance. Next, an entity group’s relationships are immutable, so if we need to make a change to the relationship, we need to delete the entities and recreate them with the new relationships. Also, we have a 60-second time-out on transactions. This is intended to reduce the chances that an entity is edited in another transaction during that same time. Finally, inside of transactions, the only type of query we can run is an ancestor query. So, we may need to fetch data outside of the transaction and pass it off to the code that’s going to be running that transaction.</p>
<p>All right. Let’s summarize what we’ve covered in this course. A query can specify a kind, and then zero or more filters and zero or more sort orders.</p>
<p>We can filter on properties, keys, and ancestors. With Datastore, there are basically two types of indexes. We have single property indexes and composite indexes.</p>
<p>Datastore supports strong and eventual consistency, and we can use ancestor queries calling the get method of a Key and transactions for achieving that strong consistency, and transactions can also be cross-grouped to allow us to support strong consistency for disparate entity groups.</p>
<p>Thanks for taking the time to watch this course. For Cloud Academy, I’m Ben Lambert. Thanks for watching.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:14" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:14-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:39:20" itemprop="dateModified" datetime="2022-11-20T19:39:20-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Deploying-Containerized-Applications-on-Google-Kubernetes-Engine-GKE-15/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:13" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:13-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:36:10" itemprop="dateModified" datetime="2022-11-20T19:36:10-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Managing-Google-Kubernetes-Engine-and-App-Engine-14/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Greetings, welcome to Cloud Academy’s course on managing Google Cloud Kubernetes and App Engine Resources. I’m delighted to have you join me in what is bound to be an educational and exciting adventure into the world of Google Cloud development.</p>
<p>First, I’ll let you know a bit about myself, before I get into the course outline. My name is Jonathan. I’m one of the course developers with Cloud Academy. I’m a former high school teacher turned technical consultant specializing in DevOps and data engineering. It’s a pleasure to get back into the world of teaching, only now talking about technology.</p>
<p>This course is designed to be very practical. It is meant for technology professionals, developers, data architects, CTOs, etc. With the goal of helping them get a solid understanding of how to build infrastructure using Google Cloud services. This course will also help you to prepare for the Google Associate Cloud Engineer Certification exam.</p>
<p>So you may be wondering what the prerequisites are for a course like this. What do you need to already know to be successful in this class? Well a few things. For one, you should have some familiarity with Google Cloud Platform. This course assumes you know basically what GKE and App Engine and Compute Engine are. You should also be comfortable working with command line tools and in a web console. You don’t need to be an expert programmer you don’t need years of experience working with GCP or AWS or something similar, however, general knowledge of what cloud providers do is helpful as this course will not cover rudimentary concepts such as, “What is a VM?”</p>
<p>So now let’s talk learning objectives. In this course, there are two main takeaways related to Google Cloud Platform. Number one, the student will learn how to manage and configure GCP Kubernetes Engine resources. And number two, the student will learn how to manage and configure GCP App Engine resources.</p>
<p>In short, this course covers GCP compute services related to Kubernetes and serverless architectures, specifically GKE, and App Engine. This might seem familiar if you took an earlier course, Planning and Configuring a Cloud Solution, which also covers these topics but the difference here is that the learning objectives in that earlier course was just deploying and configuring, whereas the material was more of an overview because of that.</p>
<p>In this course, we’re going to go deeper. If you have zero familiarity with GCP we definitely recommend taking the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/planning-configuring-google-cloud-platform-solution/introduction/">Planning and Configuring</a> course first. That will more gently introduce you to these compute services. This course, by contrast, will assume that you have a basic understanding of the different services and will instead serve as a deep dive on how to actually manage these systems. By the end of this course, you should be truly ready to be responsible for a GCP environment, specifically App Engine and Kubernetes Engine. You’re not just clicking through a wizard and hoping for the best.</p>
<p>So, one last thing before we start, I want to encourage everyone to leave feedback. Email <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a> if you have any questions, comments, suggestions, concerns. We always appreciate people taking the time. And so now without further ado, let’s get started.</p>
<h1 id="Section-One-Introduction"><a href="#Section-One-Introduction" class="headerlink" title="Section One Introduction"></a>Section One Introduction</h1><p>Welcome to section one. In this quick introduction lesson, we’ll go over the learning outcomes for this first section of the course. Our focus will be on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> Kubernetes Engine. So there are two high-level goals for this section. First, we wanna make sure you understand Kubernetes generally. Whether it is inside GCP or with another cloud provider, there are certain concepts you need to be familiar with to utilize Kubernetes effectively. Second, we will go over GCP Kubernetes Engine’s specific tools for implementing a Kubernetes application and we’ll do this in two lessons, the first focusing on creating a cluster and setting up an application container, and then, the second lesson, we’re focusing on more fine-grained Kubernetes configuration. We’ll drill down into topics like pods, nodes, services, deployments, and stateful applications, so on and so forth. Finally, we’ll end the section with a practical demonstration. We’ll do a video walkthrough on how to utilize GKE starting with nothing and ending with a running app in a GKE cluster. So if you’re ready, let’s dive in.</p>
<h1 id="Kubernetes-Concepts"><a href="#Kubernetes-Concepts" class="headerlink" title="Kubernetes Concepts"></a>Kubernetes Concepts</h1><p>Review of Kubernetes Concepts. If you’re coming from the more introductory, “Planning and Configuring” <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> course, then you recall its basic overview of GKE. The goal there was really to just show how it works with enough information to get you started. In this course, we want to get you ready to be responsible for maintaining a GKE cluster in a real production environment. So this will require a deeper dive into how GKE works, and before we do that, we want to ensure you have a deep understanding of Kubernetes concepts. If you’re already are a Kubernetes expert and you wanna just dive into GKE, feel free to skip this lesson.</p>
<p>So let’s start by talking about what Kubernetes is at a high level. Stated most succinctly, Kubernetes is a system for orchestrating containerized applications. So if you are packing your software application using something like Docker, and wish to deploy and manage those Docker containers, Kubernetes gives you the tools to abstract away most hardware and networking considerations to make managing your system much easier.</p>
<p>Consider the basic resources needed to deploy a Dockerized application. The Docker container needs a server to run on, so that means compute resources. It needs a certain amount of CPU and memory. It also needs network access, port configuration, firewalls, load balancing, DNS configs. The app may need to talk to other backend services, or it may be part of a larger system of microservices involving several other Docker apps. Managing all of this complexity manually is very difficult. We’d need to create all sorts of scripts and documentation for each server, each network setup, each application’s hardware needs, config files, etc., etc. Kubernetes, like other orchestration frameworks such as Docker Swarm or Mesos Marathon, it makes all of this work much easier.</p>
<p>It starts with the concept of a cluster. A Kubernetes cluster is a complete set of resources for an application environment. Hardware resources. So in general, this will be confined to a single data center and will comprise a number of servers and network interfaces. Storage is also a possible resource here as Kubernetes can create ephemeral and persistent volumes. The servers, whether physical machines, VMs running in EC2, or Google Cloud Compute, or somewhere else, they are referred to as nodes. Servers are nodes. A Kubernetes cluster may, for example, run on three nodes, three virtual servers, that will host your container-based applications. Having multiple nodes grants redundancy in case of hardware failure and it also makes scaling up or down easier. Each node will run a kubelet, a lightweight Kubernetes agent, that allows it to communicate with other nodes and stay in sync regarding cluster configuration and help.</p>
<p>A Kubernetes cluster with just nodes, however, is not really doing anything useful. In order to run a container, the cluster must have access to an image repository and it has to create pods. Now, a pod is a basic workload unit in Kubernetes. Often, a pod is an instance of a single container, however, it can also be comprised of multiple containers. A pod will also have a unique IP address within the network as well as storage resources based on its config. This is the smallest unit of what we might think of as a microservice in a software as a service architecture. So for example, you might have a simple stateless Python app run as a pod in your Kubernetes cluster. And we’ll dig in deeper on the container configuration and cluster setup in the next section.</p>
<p>So now, with a basic understanding of clusters, nodes, and pods, there are only two other core concepts of Kubernetes that you need to really get started working with GKE. And these two are Controllers and Services. Now, we’ll link to the <a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/">Kubernetes documentation</a> if you wanna really go deeper on all the other terminology. That’s a bit out of scope for this class, we’re just doing a basic conceptual overview.</p>
<p>But let’s diagram there, let’s start with Controllers. You can think of a Kubernetes Controller as a <em>control loop</em>. This is a basic CS concept. It’s a tool for maintaining a certain desired state. In Kubernetes, this refers to an API that manages a pod or set of pods by preserving a pre-set configuration. So there are actually a few different types of Controllers. One of the most basic is the ReplicaSet, which simply guarantees that a certain number of copies of a given pod are kept running. So for example, let’s say we have our stateless Python app running as three pods across the nodes in our cluster. If they are running as a ReplicaSet, then Kubernetes will make sure that three pods are always running. If there is a failure, a crash, an error of some kind that causes a pod to die, the ReplicaSet controller will try to bring up a replacement.</p>
<p>Now, there are other Controller types for different purposes. There is the DaemonSet controller which is meant to guarantee a specific distribution of pods on each node. This controller is useful for hardware monitoring, logging applications that need a one-to-one mapping to servers for whatever reason. There’s also the StatefulSet controller designed for stateful applications. This controller provides guarantees about pod ordering, uniqueness, and stable storage. Finally, there is the Deployment Controller. This is designed to work with other Controllers, such as ReplicaSets. The Deployment Controller, as the name implies, is designed for declarative updates to a set of pods. It handles transitioning a set of pods from its current state to a defined desired state.</p>
<p>So there’s more we could add about Controllers but this is enough to get the basic idea. Again, don’t hesitate to dig through the Kubernetes documentation for more details about each specific Controller if you’re interested. But for now, let’s move on and talk about Services.</p>
<p>Services are very important. Services are Kubernetes’ way of exposing pods to external networks including the public internet. So if we wanna make our application reachable from a browser, we’re going to need a Service to set up the IP address and DNS name. The basic configuration needed is a network protocol, such as TCP as well as ports and some metadata such as a Service name. Now as with Controllers, there are a few different types of Services. There are ClusterIP services that only expose an internal IP address, suitable for apps that don’t need to be accessed from the public internet. Then you have NodePort services that expose the node’s IP address on a specific port. This might make sense depending on your firewall setup. You may not want your Kubernetes nodes, the actual VMs, to expose their IP addresses, even if its only on a specific port. And then, you also have LoadBalancer and ExternalName services, both of which work more closely with your cloud provider. The former, the LoadBalancer, it works with your provider’s LoadBalancer resources to expose a set of pods while the latter, the ExternalName type returns a CNAME record based a DNS name of your choosing.</p>
<p>So Controllers let us turn sets of running containers into resilient, updateable applications with predictable behavior. And Services let us control access to those applications by publishing them in various ways. Now, if you understand these five terms, cluster, node, pod, controller, service, then you now know enough Kubernetes to get your hands dirty. In the next short lesson, we’re gonna do just that. We’ll review setting up a cluster and preparing a container for deploy using GKE. It’s gonna be a blast. We’ll see you there.</p>
<h1 id="Cluster-and-Container-Configuration"><a href="#Cluster-and-Container-Configuration" class="headerlink" title="Cluster and Container Configuration"></a>Cluster and Container Configuration</h1><p>Welcome to part three. In this lesson, we’re gonna cover just two things; launching a cluster and setting up a Docker image for deploy as a pod in the cluster. We’ll go over both the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> console approach and the CLI, which can be used from your local computer or a remote shell environment or the GCP cloud shell tool from the browser, so let’s get right into it.</p>
<p>Now, recall that in order to have a cluster, we need to have nodes, which are just virtual machine instances. In the early days of Kubernetes, we would have to manually create those instances, install the Kubernetes agents, known as kubelets, and then, run some commands to wire it together. Now, with GKE, we can get a cluster up and running much faster. In the web console, it’s as simple as clicking on the Kubernetes Engine menu and then, selecting the blue Create Cluster to kick off the setup wizard. I recommend going through this sort of hand-holding wizard approach your first time to get a sense of what configuration is available. We can set a name, a number of nodes, a machine type, a geographic region, and some other config options. GKE has some really nice defaults and it has options for things like CPU-intensive or memory-intensive applications. Once you finish running through it, you’ll have a running Kubernetes cluster with some set of nodes.</p>
<p>Now, we should also take a moment to talk about node pools. Node pools are groups of nodes within a cluster that share the same configuration. This is a GKE-specific feature. It’s an extension of Kubernetes functionality essentially. When we create a cluster in GKE, it will by default create a set of nodes known as the default node pool. We can then just work with that node pool or we can add additional node pools with different configuration. So for example, we might want to have a set of nodes dedicated to in-memory caches, so we create some instances with lots of RAM and a particular network config, and that could be our cache node pool. Each node pool can use distinct virtual machine images, distinct instance type, and storage options.</p>
<p>So you can create node pools in the web console or using the CLI tools with the gcloud container node-pools command. With the console, you can also upgrade or delete node pools as well. So to do this with the console, simply go into the Kubernetes Engine menu and click Edit on the cluster you wanna change. There should be a node pools section with expandable menus. Click on the one you want to change, set the size value to the desired count, and click Save.</p>
<p>So that’s node pools. Now that we understand the makeup of clusters a little bit more, let’s go ahead and create one. The command to instantiate our basic cluster is as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud container clusters create cluster-name</span><br></pre></td></tr></table></figure>

<p>Now, this will just spin up a minimal cluster with default values and a specified name. You’ll likely have to at least add a –zone argument if you don’t have a default geographic zone configured. The container clusters create command actually has a lot of optional flags to configure all of the things we saw in the web console, stuff like machine type and region, number of nodes, etc.</p>
<p>Now, after running this command, at this point, all we have is a Kubernetes cluster running with no actual application. Now, we can launch an application in container form by using the deployment controller, that is by creating a deployment with a command like so. We do:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment app --image=$ImageRepo:$Tag</span><br></pre></td></tr></table></figure>

<p>This will create a running application here, named app, using an image of our choosing. So before we can run this, we need to deal with that <code>--image</code> argument. Kubernetes needs the location of your container registry, as well as the tag. Now, the container registry, it doesn’t actually have to be in Google Cloud. It could be Docker Hub or AWS ECR or somewhere else. As long as we can provide GKE with the right URL and credentials, we can access our images from anywhere.</p>
<p>Google Cloud, however, it does have a container registry service. In the console, all you have to do is enable it. You enable the Container Registry API and you can use it. It’s just a couple of clicks. We’ll have a documentation link <a target="_blank" rel="noopener" href="https://cloud.google.com/container-registry/docs/quickstart">here</a>. And when you push Docker images to the registry, you should see them in the UI. You can click on the image’s button in the Container Registry section and then, select individual images to see details. This is a really convenient way to check names, tags, dates, other information about your images.</p>
<p>So in our video demo, we’ll run through the Docker commands for building images, logging in, and pushing them to the repo. For our purposes here, we can use one of GCP’s sample applications for the image argument and we can run it as so:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment hello-server --image=gcr.io/google-samples/hello-app:1.0</span><br></pre></td></tr></table></figure>

<p>So that’s the hello app, tag 1.0, we run this and with that, we have deployed a container application to our GKE cluster. We can see cluster health in the console or by running kubectl or gcloud commands, and congratulations, you’ve got something running. However, this is just the start of the fun. We need to go a bit deeper. We need to understand what we just did.</p>
<p>In our next lesson, we’ll learn more about controllers, services, and pods. It’ll be a blast. See you there.</p>
<h1 id="Working-with-Pods-Services-and-Controllers"><a href="#Working-with-Pods-Services-and-Controllers" class="headerlink" title="Working with Pods, Services, and Controllers"></a>Working with Pods, Services, and Controllers</h1><p>Working with pods, services, and controllers. In the last lesson, we reviewed how to spin up a cluster and execute a simple deployment. By the end of it you had a simple app running, however, it probably all seemed a bit magical if you’re not already very familiar with GKE and Kubernetes. In this lesson, we’re going to break things down further to ensure you know what is really going on under the hood.</p>
<p>So let’s start by talking about the pod we launched. Remember this is our workload, usually defined by a single container taken from an image repository. We used a sample image from a public <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> repository. We deployed it by running a create deployment command using kubectl, the Kubernetes command line tool. The deployment is the controller for a given pod or set of pods.</p>
<p>So why did we use a deployment controller at all? Why bother using a controller in the first place? Couldn’t we just generate pods without that construct as overhead to deal with? Well, technically yes, we could just create pods, but this is bad practice. In general, with Kubernetes, we always want to use controllers when creating pods. This is, as the name implies, meant to give us more control. It reduces our maintenance and monitoring workload considerably to have a controller responsible for the state of pods. It lets us ensure that pods are healthy, that we have the right number, that the right networking is there, the right configuration, etc., etc. In general, in Kubernetes, we want to work with the highest level of abstraction possible, so we prefer to work with controllers instead of individual nodes or pods.</p>
<p>So, for now, we can get a bit more information about the pods that are running, by running this command: <code>kubectl get pods</code>. This will spit out some basic information about pods running in a cluster. We should see 1&#x2F;1 running, meaning the cluster expects one pod and sees one pod. We see its age, we see its status, we see the number of restarts. We can now add more pods to this cluster a few different ways. We can use the web console by navigating to Kubernetes Engine UI. And there we just click on the Workloads button and from there we can click on Deploy to launch more pods. There is a default nginx pod we can do just to test that out. And by default, this will create three pods out of the same container. And again it will do this by using the deployment controller.</p>
<p>We can get information about our deployments by running this command: <code>kubectl get deployments</code>. This will show us our available deployment controllers. Here we aren’t seeing nodes or pods, but rather deployments, which are kind of an abstraction level higher. So we will see the hello-server deployment which you might recall was the name we gave the deployment in the command. And also remember that a deployment is a type of controller, and crucially it can work with other controller types to both execute updates and ensure that our apps are in a desired healthy state. In this example, our pods are also running using the ReplicaSet controller.</p>
<p>So we can actually see this by running a command: <code>run kubectl get replicasets</code>. And we should see both the hello-server and the optional nginx-1 application if you launched it. They should both pop up since they’re both ReplicaSets. Now if we were to run a similar get command for DaemonSets or StatefulSets, <code>kubectl get daemonsets</code>, or something like that, it will return nothing, because the pods we have launched so far do not use those controllers.</p>
<p>ReplicaSets work really great for stateless apps that are easily distributed across a set of nodes. Recall that DaemonSets are designed for scenarios when we want to ensure that all or a specified set of nodes run a copy of the pod. And when we need that mapping of pods to nodes. When we care about hardware-level mapping, basically, that’s when we use a DaemonSet, and when we care about state, pod deployment order and persistent storage, then we will want to use a StatefulSet.</p>
<p>So now let’s talk a little bit about services. If we run <code>kubectl get services</code>, the only thing that should come up is something named Kubernetes, initially. This is the default service for the cluster that is unrelated to the pods that we have created. So how do we go about exposing our pods running as ReplicaSets to the public internet? Well to do that we need to create services, and this will create generating some additional config. We can create that config as a YAML and run a <code>kubectl apply,</code> run that command to execute that config in the cluster. We could also run a <code>gcloud</code> or <code>kubectl</code> command with some arguments. Or, easiest of all, we can go into our GKE console and click on Workloads. And from there we just click on the pod we care about, for example hello-server, and then just click the blue Expose button to generate a service. Once we do this it will ask for a port mapping before assigning an external IP. Now by default GKE will create a load balancer type service, but recall that there are other options here such as a ClusterIP or a NodePort service. </p>
<p>So pat yourself on the back. You now know enough Kubernetes and GKE to really kick some butt. You should have a sufficient grasp of pods, services, controllers to not only spin up a cluster, but also to deploy resilient services using appropriate controllers. To really lock in our understanding though, we need to see all of this in action. That’s what the next section, the video demonstration, is designed to do. See you there.</p>
<h1 id="Kubernetes-Demo"><a href="#Kubernetes-Demo" class="headerlink" title="Kubernetes Demo"></a>Kubernetes Demo</h1><p>Welcome to our video demonstration on GKE in GCP. That’s Google Kubernetes Engine in the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">Google Cloud Platform</a>. In this demo, we’re gonna do a few fun things. First, we’ll create a basic application package, using Docker, and then upload it to a registry. Second, we’ll create a GKE cluster and deploy our app into it and see it running. And then finally, we’ll expose the app to the internet before scaling it up and practicing deploying a new version of the app, an upgrade essentially. So we’ll do most of these things using the GCP Cloud Shell. And also, maybe occasionally the Web Console a bit. So feel free to follow along at home in your own account.</p>
<p>Now, this tutorial comes mostly from GKE’s own documentation. So we’ll <a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app">link</a> that below. You’re welcome to just go and look at that. It’ll help if you have questions. If you do plan to follow along, be sure that the Kubernetes API is enabled. You can do this by just going into Kubernetes Engine on the console. Or you can do it if you’re running from your local laptop or something, you can run the command:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud components install kubectl</span><br></pre></td></tr></table></figure>

<p>And it should just set it up for you. So make sure that’s ready to go. So let’s get started. Okay, so to start, we need a Docker image with our application code, we need to prepare that. So we could build our own Dockerfile from scratch and using our own app code and test everything, but we’re gonna be lazy. Instead, we’re just going to save some time by taking one of GCP’s test projects. So we can just clone that using git, like so. And once we have it cloned, we’ll just cd into the directories, we wanna use their hello-app, and what we need to do after that, so it’s actually four commands, we clone it, we cd into the directory, we’re gonna export a project ID environment variable, and then we’re gonna do the Docker build.</p>
<p>So we’re gonna do <code>export PROJECT_ID=[PROJECT_ID]</code>, which we can get right from the console. And then we’re gonna do a Docker build of that. So the only thing you might have to look up is the project ID. You can get it right from the console if you don’t know it. And there’s other ways to get it. We’ll have a <a target="_blank" rel="noopener" href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">link</a> for that. But once that’s done, you’ll build your image and you’ll have it locally in your Cloud Shell environment. </p>
<p>Now what we have to do after that is, so you can see it building, just give it another sec. But what we have to do after it’s finished building, see it’s finished right here, is we have to push the image to a Docker registry. Because we can’t play from our shell environment, we need it in Google Cloud registry. And that we can do pretty quickly. We can do that with just two commands. We need to first run this. This is <code>gcloud auth configure-docker</code>, and what this will do is it will set the right credentials for a Docker push command, for Docker to push the image to our container registry. And then we just run the Docker push command, which will take the image that we just built, and it will push it to our Google Cloud container registry with the ID that we set, and with the tag that we set, we have hello-app tagged with the v1, it’s our version one. So just give this a sec, it needs to execute. And there you go. Well, it was already there, but it’ll go for the first time, you’ll see it. </p>
<p>Okay, so now that we have the image in the Docker in the container registry, we need to deploy it somewhere, right? We need a cluster to deploy it too. So how can we do that? How can we get a GKE cluster running? There are a few ways to do this. But the simplest way, if we don’t already have a cluster running, is to use the <code>gcloud container clusters create</code> command. Now before we run that, we’re going to set the project ID in our environment.</p>
<p>Okay. And so after that, we have to set our zone, our region where we’re going to actually deploy or create the cluster. So we’ll do US east1-b. And so these two configuration options, the project ID and the zone, this just tells Gcloud what project to associate the cluster with, and where geographically to create it.</p>
<p>So once we have that configuration set, we can run the <code>container clusters create</code> command. Now we see a couple of warnings here, just issue going on around this time, November 2019. But after a while, it’ll take a few minutes, we will see the cluster come up. And it takes a minute because it has to actually create the VM instances that will host the Kubernetes infrastructure and it has to set up some networking. But once it completes, you should be able to see the instances with a gcloud list command. So eventually the cluster will come up, it’ll be finished, it’ll be in a healthy state.</p>
<p>We can see the instances if we run a <code>compute instances list</code> command. So you can see these two here are the two instances we created for GKE hello-cluster. And we can also see it from running a container clusters list. So we should see our cluster come up, and we should see it’s in status running two nodes. So that should be good. So now that we have a cluster running, and we have an image that’s been uploaded, how do we go about deploying that image into our cluster, right? It must be pretty involved, we have to set up deployment configuration, we have to download the image, we have to run it, we have to sync everything, you know, probably requires a lot of work, right? Well, no, actually, it’s pretty simple. We can do the deployment with a single command. We run <code>kubectl create deployment</code> like so, we pass a couple of arguments. We’re gonna call it hello-web, that’s the name of our deployment. And we’re going to pass in the image we wanna use with –image. And that’s our gcr.io. That’s the cluster, the container repository for GCP. And we’ll put in our project ID and the name and tag we had.</p>
<p>So this literally does everything, this one command. We’re leaning on the deployment controller functionality to do most of the legwork. So we’re telling Kubernetes to create a pod using that image. So we can actually see if we do <code>kubectl get pods</code>. And we’ll see there’s our, the pod, the image running in the deployment. And it’s going to create the deployment controller. That’s what keeps everything running. And it’s what moves us from the empty cluster to the desired state. It’ll keep things that way until we make a change.</p>
<p>So now let’s have some fun. Let’s go ahead and make a service. Let’s expose our app to the public internet. We can do that also with a single command. We’re just gonna do <code>kubectl expose deployment hello-web</code>. So this is similar in that we are working with the deployment controller again. So please note that we use expose on the deployment, not on the pod. The deployment is responsible for the overall app configuration. And as far as best practices go, in general with Kubernetes, we wanna work at the highest level of abstraction possible.</p>
<p>So we wanna work with controllers instead of pods or nodes. So in this command, we tell Kubernetes to create a service, a load balancer type service, set to accept traffic on port 80. And that is going to target, it’s gonna be route trafficked to port 8080 for the container. So we can actually see that as well, we can do <code>kubectl get service</code>. And we should see there’s our service running. There’s actually two. There’s the Kubernetes kind of default service here. And then there’s the new one, the load balancer one right here. And we can see it has an external IP address. And we can see if we can hit that. And we can, there is our application. Hello world, version one, host, yada, yada, yada. So pretty cool. We’ve got a running service exposed to the internet.</p>
<p><a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app">https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects">https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects</a></p>
<h1 id="Section-Two-Introduction"><a href="#Section-Two-Introduction" class="headerlink" title="Section Two Introduction"></a>Section Two Introduction</h1><p>Welcome to section two. In this second half of the course, we’ll be focusing on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a>‘s App Engine service. Since this course assumes you already have some familiarity with App Engine, we won’t spend time explaining what App Engine is, and instead, we’ll dive right into spinning up a basic app in the very next lesson. After that, we’ll get into some of the critical management components. First with a lesson on configuring application traffic, and then after that with a lesson on autoscaling. As with the previous section, we will end with a video demonstration, walking through the entire setup and deployment process. So if you’re ready, let’s dive in.</p>
<h1 id="Creating-a-Basic-App-Engine-App"><a href="#Creating-a-Basic-App-Engine-App" class="headerlink" title="Creating a Basic App Engine App"></a>Creating a Basic App Engine App</h1><p>Creating a basic App Engine app. This lesson will be a walkthrough of how to create a simple Python service using App Engine. Our focus will be on using the command line tools instead of the web console. While the console is nice for beginners thanks to its setup wizards and friendly UI, the terminal is really where our understanding is properly crystallized, so without further ado, let’s dive in.</p>
<p>Now, first, we need a proper shell environment. As before, we can work from any computer that has the Google Cloud SDK installed. We could connect to a remote server using SSH and install the gcloud tools there or perhaps easiest of all, we could just use the GCP Cloud shell tool from the console, right from our browser.</p>
<p>Now, whichever you choose, the first thing you’ll need to do from the command line is create your App Engine project. This is basically the highest level abstraction for your app. App Engine apps are tied to a project in your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> account, so you will need your project ID to run the app create command. You can get your project ID from the web console or from running <code>gcloud projects list</code>. So the command for creating your App Engine project will be <code>gcloud app create --project=$PROJECT_ID</code>, so this will create the project.</p>
<p>Now, to verify, you run <code>gcloud app describe</code>. So congratulations, we’re done, right? Well, not quite yet. We don’t just want some empty App Engine framework with no actual service running. We want to deploy something, so for this lesson, we’re going to just use the sample Python app from GCloud’s own documentation. Now, if you’re not very familiar with Python development, I recommend using the GCP Cloud shell, as it is a standardized environment, so you won’t have to worry about Python version or library dependencies. To set up the needed gcloud components, we start by just running this command:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcloud components install app-engine-python</span><br></pre></td></tr></table></figure>

<p>That will take care of the environment configuration for Python, and then, after that, we need to actually get the code. We need to clone that from GitHub, so we’ll do a Git clone. The <a target="_blank" rel="noopener" href="https://github.com/googlecloudplatform/python-docs-samples/">URL</a> will be posted below. And so, now, we have the code that we intend to deploy. The actual hello world app we want is in the following directory <a target="_blank" rel="noopener" href="https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/appengine/standard/hello_world">here</a>, it’s under python-docs-samples&#x2F;appengine&#x2F;standard&#x2F;hello_world. And then, if you wanna try it out and see what it does, you can actually run this command here, <code>dev_appserver.py app.yaml</code>.</p>
<p>Now, this will start the app server and run the service. Now, if you’re doing this from the GCP Cloud shell, all that you’re gonna see is some output showing that it’s running. This is a decent test, but it’s not really what we want. What we’d really like is to see is the app running in a browser and we can’t do that from running it on localhost in Cloud Shell, at least not trivially. If you happen to be doing this from your laptop or desktop, you can browse to the app. Using a browser, you can go to <a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080</a> and you should see your output.</p>
<p>Now, either way, we still need to actually deploy this thing, right? We need to make it into a real web service reachable from the public internet. Now, fortunately, this is pretty easy to do since it’s a completed project. All we have to do is, in that same hello_world directory, we run <code>gcloud app deploy</code>. This will use the app.yaml file in that directory for instructions, and then, actually deploy it to the public internet.</p>
<p>So to actually see it from the browser, you run <code>gcloud app browse</code>, and this will give us the URL that we can navigate to to see the app running. Now, for more fun, go ahead and look at the main.py code to see what the app actually does. Feel free to edit it. You can make it output a different message if you’d like. So with that, we can end our first lesson. You now know the basics of deploying an app in App Engine using the shell. This is, of course, a very simple case. We’ll need to go a bit deeper on the actual App Engine configuration, so that you can build confidence for using it in different scenarios. We’ll start on that in our next lesson. See you there.</p>
<h1 id="Configuring-Application-Traffic"><a href="#Configuring-Application-Traffic" class="headerlink" title="Configuring Application Traffic"></a>Configuring Application Traffic</h1><p>Section Two Part Three: Configuring Application Traffic. Welcome to Part Three. Now that we’ve gotten our feet wet with App Engine and actually deployed a basic app using the command line, let’s go a bit deeper by looking at how configuration works. Our goal in this lesson is to understand how to configure traffic to different parts of our App Engine environment. However, before we do that, we should review a bit about App Engine generally just to set the foundation.</p>
<p>So recall that there are two types of App Engine environments, Standard and Flexible. The core difference between the two is that the Flexible environment gives you direct control over your application runtime via docker files. You can enable root access to the underlying VM instances. So overall, this is a better option if you have a more unique environment use case, hence the name Flexible. Now the main trade off is that the Flexible environment is slower and it’s less resilient. Instances take minutes to deploy and start up and the instances are automatically restarted more frequently by GCP. You also don’t get access to all the same App Engine API, such as the users and images APIs, which are useful for scripting an environment automation. Another difference is that the Standard environment has a bit more flexibility with its autoscaling options. Now we’ll go into that in the next lesson.</p>
<p>But generally speaking, if you’re using App Engine to go serverless, you’re probably gonna wanna use the Standard environment for its greater speed, reliability, and feature richness. The Flexible environment is more for niche use cases. For most of the remaining lesson and demo content, we’ll be making use of the Standard environment. But keep in mind these differences with the Flexible environment in case you have need of it.</p>
<p>Now whichever environment you use, you’re going to configure your app with YAML files. Only one YAML file is absolutely required, and that is the app.yaml file. This is for application level settings. An application may be made up of multiple services. And these are configured in service.yaml files. You can have more than one service.yaml file in your applications root directory. Now for a fairly simple app, this is okay. But for something more complex, it is better practice to have separate subdirectories for each service.</p>
<p>So here’s a sample app.yaml file. This is the one for the Python app we launched in the last lesson. One line, “r<code>untime: python 37</code>“. As you can see, our app.yaml file is pretty simple. The only thing app.yaml really needs is runtime, is to specify a runtime and potentially a version parameter.</p>
<p>Now if we wanted to, we could break this out into a separate service by using a service.yaml file. And it would look pretty similar, with the only difference being that the service.yaml file generally starts with a field called “service”. So it would look something like this.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">service: python-service</span><br><span class="line">runtime: python37</span><br><span class="line">api_version: 1</span><br><span class="line">threadsafe: true</span><br></pre></td></tr></table></figure>

<p>We have a service name, runtime. And we can also put in an API version, and some other config. Now a more complex app might have several of these service.yaml files to create microservices, to create a microservices architecture within App Engine. Now aside from this, there are five other optional configuration files that can help you to extend your app’s overall functionality. And these are dispatch.yaml, queue.yaml, index.yaml, cron.yaml, and dos.yaml.</p>
<p>So real quick, let’s go through them. Dispatch.yaml is for overriding routing rules. You set this file in your application root directory and you use it to route incoming requests to specific services based on specific paths or host names in the URL.</p>
<p>Queue.yaml is for configuring push and pull task queues. This lets us define retry parameters, such as minimum back off time in seconds or, and a maximum task age, for example. We won’t be doing any queue-based services in this course. But we’ll link to the documentation. If it’s relevant to your use case, definitely take a look at this, queue.yaml.</p>
<p>Now dos.yaml is a security feature that lets you blacklist IP addresses or subnets to protect your application from DOS attacks.</p>
<p>And then cron.yaml lets you define scheduled tasks. You set a schedule, such as every 24 hours, or every Monday at six p.m. and then give it a URL for a task definition, much like a typical cron job. The task definitions should live in the specified path. Now, this is very handy for maintenance, monitoring, and other standard automated tasks that you might want to configure and have in one easy to work with location.</p>
<p>And then finally, there is the index.yaml. Now you may have noticed that in our simple Python app from last lesson, an index.yaml file was automatically generated. This file is a reference of properties on various application entities. For simple applications, you will not have to manually edit this file at all. App Engine will automatically update it for you.</p>
<p>So, now that we have some strong background on App Engine configuration and environments, let’s talk about traffic management. There are a few use cases that we need to consider. As mentioned above, we know that we can control service routing by using the dispatch.yaml file. But what about running multiple versions of a service for A&#x2F;B testing or migrating traffic to a single specified version?</p>
<p>To do these two things, we need to talk about traffic splitting and traffic migration. Splitting refers to taking a percentage of our traffic and directing it to one or more distinct versions of a service within our app. Migration refers to the opposite process, moving traffic that’s split among different versions to a single specified new version. So let’s start by talking about splitting. The first thing to know is that traffic splitting is automatically applied if the URLs in your service do not specify an app version. So if you have multiple service.yaml files with different versions of the same service and the URL in your app.yaml file do not have version parameters at all, then traffic will be split automatically and randomly. If you wish to be more precise, you can explicitly enable traffic splitting in the console or by using G Cloud CLI tools or the GCP API.</p>
<p>So for example, if we did this in the console, we would just go to the application page in App Engine and select the versions we want to split. Click on Split Traffic, and then just put in the percentage each service should receive.</p>
<p>Now we could do the same thing with the G Cloud CLI tool with a command like this. Here we have “<code>gcloud app services set-traffic</code>“ and a number of flags here. Now if you look at these flags here, there’s one option you really need to note. It’s the one that says “IP_OR_COOKIE” options in the dash dash split by flag. We have to tell App Engine whether to split traffic using IP address splitting or cookies.</p>
<p>IP address splitting is easier to do. It will just make App Engine hash the request’s IP address from zero to 999 and then route based on whatever random value it gets. This isn’t as precise as cookie routing, though, because of how IP addresses tend to be somewhat ephemeral, particularly from cellphone traffic and from the public internet. IP address splitting is also bad for traffic coming from internal <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> services because those services tend to utilize a small set of internal IP addresses. That will get stuck to the same version of your app.</p>
<p>So for better precision, you should use Cookie-based splitting. The trade-off here is that it will take a bit more setup because the application will look for a specific HTTP request header. So you may need to make a code change in your app to deal with this, but it will help ensure you get a precise split for traffic.</p>
<p>And then finally, let’s talk a little bit about traffic migration. In the Standard App Engine environment, we can choose to migrate traffic either immediately or gradually. In the Flexible environment, we can only do it immediately. There’s no option for gradual migration. When we do an immediate migration, you’ll generally see a spike in latency, as it causes all instances of the older version of your service or services to shut down. For a latency-sensitive application, this could be a deal-breaker, as you could see requests hang or it can drop as the traffic is rerouted.</p>
<p>For the Standard environment, the solution is to use gradual migration. This is configured in your app.yaml file with this one setting here, “<code>inbound_services: - warmup</code>“. And with that, you are informed and ready to deal with migration, splitting, and deployment in the App Engine world.</p>
<p>Phew, okay, so we made it through a deep dive into App Engine environments and configuration and traffic. You’re almost ready to really do some damage. In the next short lesson, we’ll talk about autoscaling and deployment. If you’re ready, let’s get going. Cheers.</p>
<h1 id="Autoscaling-App-Engine-Resources"><a href="#Autoscaling-App-Engine-Resources" class="headerlink" title="Autoscaling App Engine Resources"></a>Autoscaling App Engine Resources</h1><p>Autoscaling App Engine resources. To understand how scaling works in App Engine, it’s important to first remember that even though we think of App Engine as serverless, in reality, all of our code must run on virtual instances somewhere within <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a>. So when we talk about scaling, we’re talking about scaling that underlying hardware, the virtual instances of GCP.</p>
<p>Now, first and foremost, we have to be aware of instance types. From App Engine’s perspective, there are two types of instances: dynamic and resident. Dynamic instances automatically shut down based on usage, while resident instances will run all the time, no matter what. The former is better for cost optimization, while the latter can improve app performance since you won’t have to wait for instances to restart.</p>
<p>Next, we should talk about scaling types. There are actually three types of scaling in App Engine: Automatic, Basic, and Manual. Automatic scaling is the most common type. It will create dynamic instances based on specific metrics, such as response latencies or request rate, and you can also specify a minimum number of idle instances that will run as resident instances to maintain a baseline.</p>
<p>So next, there’s manual scaling. As you can guess, this is done manually. This uses resident instances only. When set, App Engine will continuously run these instances irrespective of load. You can use manual scaling for temporary load spikes or performance tests.</p>
<p>And then, finally, there is what is known as basic scaling. When your application receives requests, dynamic instances come up. When the app becomes idle, all dynamic instances shut down, so, obviously, this is not suitable for latency-sensitive continuously running services. It is great, however, for intermittent work that is not latency-sensitive, such as the occasional user-initiated script or batch job or service. Basic scaling is not available in the flexible App Engine environment, so do keep that in mind.</p>
<p>And congratulations, that’s it, you’ve made it through all three of our rigorous lessons on App Engine. You have enough theory to launch a billion-dollar startup. Now, let’s go and put that theory into practice. Let’s have some fun. We’re gonna do a video demonstration where we will take our Hello World app to the next level. See you there.</p>
<h1 id="App-Engine-Demo"><a href="#App-Engine-Demo" class="headerlink" title="App Engine Demo"></a>App Engine Demo</h1><p>Okay, welcome to our video demonstration for GCP App Engine. Now we’re gonna have some real fun in this little section here. We’re gonna actually play around a bit with a real app in the standard App Engine environment. Now, if you made it through the earlier four parts without too much trouble, you should have no problem following along with this demo, though, we do encourage students to actually try things out in their own <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> account, if possible.</p>
<p>So, there are gonna be basically three parts to this demo. First, we’ll quickly spin up a sample app in the standard environment. This will be identical to what we showed in part two, very quick. And then secondly, we will practice traffic splitting and migration by creating a second version of the service. So, that we’ll walk through how to configure traffic basically. And then finally, we’ll check out some scaling functionality, we’re gonna manually scale the app up and we’ll also go through the automatic and basic scaling system. We’ll just show how those are configured and that’s basically it.</p>
<p>So let’s get started. Welcome to our video demonstration of App Engine on GCP. We’re gonna have some real fun now. We’re gonna actually play around with a real app in App Engine using the standard environment. Now, if you made it through the earlier four parts without too much trouble, you should have no problem following along with this demo, we are going to encourage students to actually try things out in their own GCP account, if possible, but of course, feel free to just watch along. So, this demo has basically three parts.</p>
<p>First, we’ll review the how to spin up a sample app in the standard environment. This will be identical to what we showed in part two, we already have an app up and running for that. Secondly, we’ll practice traffic splitting and migration by creating a second version of the service. So, we’ll see the traffic shift. And then finally, we will check out some of the scaling functionality by manually scaling the app and walking through the automatic and basic scaling configuration systems. So that’s the basic plan.</p>
<p>So, now, let’s just review how we deploy a basic app. We start by ensuring that we have our project ID set in the environment. So, you’re gonna do an export project ID equals whatever your project name is. Here, it’s Test Project CA555. You can get your project ID and organization ID all from the dashboard pretty easily. So we’ll export that and what we need to do is get the actual App Engine code. We’ll just clone the sample project and that is from Google’s GitHub <a target="_blank" rel="noopener" href="https://github.com/googlecloudplatform/python-docs-samples/">directory</a>. So I do a git clone of that and then we will need to go into the directory with the App Engine configuration. So we’ll do this and that’ll take us into the Hello World app in standard App Engine, Python 3.7. Be sure to do the Python 3.7 version, there’s also a Python 2 version, but that’s pretty deprecated now.</p>
<p>So, once you’re there, you should see in the directory that it has the app.yaml, the main.py. That’s the actual Python code that we’re gonna run and a requirements file and a test file there. The app.yaml is what we’re interested in. That’s gonna, you know, be the actual configuration for App Engine. The only thing here now is the runtime definition, Python 3.7. And then the actual… The deployment command is pretty straightforward. We’re just gonna do gcloud app deploy to launch our app.</p>
<p>Now this one is already up. So it’ll go through it pretty quickly, but the very first time that you do this deploy, it might take a little while. You know, once it finishes, we should be able to go to the URL and see the code actually running. So, let’s just give this a second to complete. Okay and once that has completed, we can run this command, gcloud app browse to actually see the application in the browser. Sometimes you might see this error, did not detect your browser, go to this link instead, we can click on it and we can see it return “Hello world,’ which is really all the code is doing, we go into the main.py and we can see that it’s really just a server returning “Hello world.” That’s all it is.</p>
<p>So our app is up and running. Just like that, we have a basic app running in GCP using App Engine. So next, what we wanna do is to test how to do traffic splitting and migration and in order to do this, we’ll need a second version of our service and doing this is gonna require a little bit of file manipulation. Specifically, we’re gonna need to create two directories with the same service.yaml file and we’ll make a minor change in the Python code so that we can see that there are in fact, two different versions of the service running.</p>
<p>So, let’s go about creating those two versions of the service. The way we’re gonna do that is we’re going to create two directories. We’re gonna create a v1 and a v2 directory and then we’re, you know, we’ll see that we have them in the same directory as all of our code right now. So what we wanna do is copy all of our code into those directories. So we have a v1 and we will have everything in v2.</p>
<p>Now, it is important to copy everything. If you try to copy only the yaml and the Python code, it will fail. The requirements that TXT file is very important. If you look at it, you’ll see that it is what tells App Engine to install Flask and what version of it. So if you forget that, it’ll try to deploy an app, but it will be broken because it won’t have Flask. So make sure you do that. So, if we go into, let’s say, v2. and we look at our code here, what we’re gonna need to do is change the app.yaml file into a service.yaml file. So we can just change it to service.yaml. And then to make it its own… To make it a little bit more unique… Oh, so yeah, we keep the same configuration here, but to make the code different, we’re gonna go into the main.py and we’re gonna have it return something different. So instead of “Hello world,” we’ll say this is the other version. Keep it very clear.</p>
<p>So, now we can actually deploy this as another version of the service by doing a similar command, we’ll do gcloud app deploy and this time we’ll pass in service.yaml. And, again, it’ll use the same… It’s the same basic default service, but now it’ll have another version with this different code. So hit the deploy, we confirm, it’ll take a minute to come up. So let’s just wait for that.</p>
<p>Okay, so now that that has finished deploying, we can see again, gcloud app browse if we wanna get the URL for the service, but before we do that, just to verify that we have two versions running, in the console, we can check and see this. We can see it in two places, in the services page, there should be a two under the number of versions there. So we can click on that or we can click on versions and we can reload this. Sorry, here. We can see that if we refresh it, there are two versions here. One of them is getting 100% of the traffic, the other is not.</p>
<p>Now, if we do check the versions here, this is the one getting 100% of the traffic. If we click on it, it’ll return the service output, which is the other version, the new one instead of “Hello world.” If we click on this one, this is the older one that outputs “Hello world.” Now, this is not the official service URL. This is not the app URL. This is just for those individual versions of the service. If we look at the app URL, gcloud app browse, we will get this app URL, testprojectca555.appspot.com and it’ll give us this is the other version. It’ll do that because 100% of the traffic is going to the new version that we just deployed. You can see it has a more recent timestamp. So, the console gives us these two URLs to see the two versions of the service, but the service itself, remember the traffic allocation is what determines what a user will see.</p>
<p>So now we have to think about splitting the traffic and migration. So that’s gonna take a little bit of work, but nothing too tricky. So, let’s start by talking about traffic splitting. If we wanna do traffic splitting, we can do that from the command line, but it’s a little bit clearer in demo if we use the console. So, what we’re gonna do is click on traffic split here and remember that there are IP address and cookies splitting types. For when you need a very precise split, you use the cookie type, though it takes a little more configuration. The IP address one is a little bit simpler. There’s also this random option here, you can see. So we’re gonna pick that and we’re gonna click on, you know, the other version of the service here, these are the two versions running and we can decide what percentage we want. Let’s say we want 50&#x2F;50. So half the time, it should randomly choose the “Hello world” output and half the time it should choose the other one.</p>
<p>So we click save, it’ll update our traffic settings, just give it a moment to execute. Okay, that should be done. Now we go back to our Services page. Still, there are two versions there. Go to the versions page, we can see there’s a 50&#x2F;50 split. So, if that’s the case, we can test this pretty clearly by going to the app URL and half the time we should get hello world, half the time we should get this is the other version. So let’s click on it. There’s “Hello world.” Reload. This is the other version. Reload. “Hello world.” Reload. Well, there’s “Hello world” again, but as you can see, it’s randomly choosing 50&#x2F;50 between the two versions of the app. So we have the two versions of the service. So we have successfully split our traffic and the nice thing is that migrating back to a single version is similarly very easy.</p>
<p>We can do it through the web console with just a few clicks. What we do is we click on the version that we wanna migrate to, let’s say the newer one, we wanna get that back to 100% of the traffic, all you do is click migrate traffic and confirm. And then in a matter of seconds, 100% of the traffic will go to the newest version of the service. Executed, now we’re back at 100%. We should only get this is the other version now. If we reload it, we can see that that’s what’s happening. Right, there’s three times, so. So there you go, splitting and migration, pretty straight forward.</p>
<p>So let’s do some scaling. Scaling, there’s a few different versions of scaling within App Engine. In the standard environment, the easiest one to test since it won’t require any traffic manipulation, it would be manual scaling and manual scaling, we can configure at the level of individual services by just going to our service.yaml and what we’re gonna do is add a little bit of configuration.</p>
<p>Now one thing to note, another reason we’re showing manual scaling is that automatic scaling is enabled by default in the standard environment. There’s also the basic scaling option, but for demonstration purposes, this is actually much neater, much cooler. So how do we add scaling? Well, what we do is we add the manual scaling config to our yaml here, and we’re gonna set that to instance count of, let’s say five. Let’s say we want five instances backing our app. So what we do is we edit that and we redeploy it and we should see it. Oh. Actually, we have a light slight error there. We wanna make that into instances. Sorry. And then that should come up with five instances behind our app. So let’s just give that a sec to run and of course, be careful about the name of your configuration, instances not instance.</p>
<p>Okay, so the deploy completed and as we can see, a third version of the service came up because we did the app deploy again and this version will have five instances of traffic behind it, five instances behind it to manage the traffic. We can see here, they all just came up around the same time. So, the scaling is actually pretty fast. You’ll recall from the lesson that there is an option for more gradual scaling if you want or gradual migrations, but we’re doing just immediate changes here with the deploy command.</p>
<p>Now we can also migrate back with the same method by just deleting that configuration. We can go into service.yaml and just kill that and then just deploy it again. And again, by default, you know, we create a new service and all traffic goes to that… A new version of the service and all traffic will be split toward that new version. Now that’s a default that you can override by changing the splitting options or by migration.</p>
<p>So, let’s see if the instances go away for this new version. We’re doing our update, let’s give that a second to run. Okay, so the update completed. So now, again, if we go to our versions here, we’ll refresh, we’ll see there’s a fourth version now. It’s getting all the traffic and if we look at our instance count, there are no instances. They’re all gone because the scaling config was deleted.</p>
<p>So we can see, it’s really that simple to manually adjust the scaling with your App Engine services. So that’s basically it, congrats. That’s it for our App Engine demo. We’ve done quite a bit in just a few minutes. We spun up a basic app, we created multiple versions, we split traffic, we ran a migration, we scaled everything up and down, pretty cool stuff. You should have a good basic understanding. Thanks for toughing it out and cheers.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>So congratulations! You made it! Give yourself a pat on the back because it has been a long and tough ride. We went through a lot of material so before we start celebrating let’s take a minute to briefly review what we’ve learned.</p>
<p>By completing this course you should have a very thorough understanding of how to configure and manage GCP’s App Engine and Kubernetes Engine services. You should be ready to spin up a cluster in GKE, create a Kubernetes deployment, and set up an autoscaling in App Engine with a basic app. Basically you should be ready to set up or take over administration of a <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/google/">GCP</a> compute environment. For clarity’s sake, let’s review the two learning objectives.</p>
<p>Number one: Learn how to manage and configure GCP Kubernetes Engine resources. We did this in section one. We not only covered how to manipulate cluster configuration using the web console and SDK, but we also reviewed basic Kubernetes concepts just in case you aren’t familiar with them.</p>
<p>Number two: learn how to manage and configure GCP App Engine resources. This was done in section two. We focused on setting up App Engine standing environment and autoscaling to help you establish that truly serverless experience.</p>
<p>With these two learning outcomes solidified in your mind, you should now be ready to dig in and build your own applications from scratch. You should also have the conceptual foundation necessary to branch out to other infrastructure platforms if you’d like. Go you!</p>
<p>Now that you are done I’d like to end by inviting you to send feedback about the course to <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. We really appreciate your comments, questions, suggestions, really anything. Congratulations again on fighting through the whole course and good luck in your future endeavors.</p>
<h1 id="3Kubernetes-Concepts"><a href="#3Kubernetes-Concepts" class="headerlink" title="3Kubernetes Concepts"></a>3<strong>Kubernetes Concepts</strong></h1><p><a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/">Kubernetes documentation</a></p>
<h1 id="4Cluster-and-Container-Configuration"><a href="#4Cluster-and-Container-Configuration" class="headerlink" title="4Cluster and Container Configuration"></a>4<strong>Cluster and Container Configuration</strong></h1><p><a target="_blank" rel="noopener" href="https://cloud.google.com/container-registry/docs/quickstart">Container Registry documentation</a></p>
<h1 id="6Kubernetes-Demo"><a href="#6Kubernetes-Demo" class="headerlink" title="6Kubernetes Demo"></a>6<strong>Kubernetes Demo</strong></h1><p><a target="_blank" rel="noopener" href="https://cloud.google.com/kubernetes-engine/docs/tutorials/hello-app">GKE Documentation</a></p>
<h1 id="8Creating-a-Basic-App-Engine-App"><a href="#8Creating-a-Basic-App-Engine-App" class="headerlink" title="8Creating a Basic App Engine App"></a>8<strong>Creating a Basic App Engine App</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/googlecloudplatform/python-docs-samples/">Python Docs samples</a></p>
<h1 id="11App-Engine-Demo"><a href="#11App-Engine-Demo" class="headerlink" title="11App Engine Demo"></a>11<strong>App Engine Demo</strong></h1><p><a target="_blank" rel="noopener" href="https://github.com/googlecloudplatform/python-docs-samples/">Google GitHub directory</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:11" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:11-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:41:32" itemprop="dateModified" datetime="2022-11-20T19:41:32-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Managing-Encryption-Keys-With-Google-Cloud-KMS-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:10" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:10-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:40:34" itemprop="dateModified" datetime="2022-11-20T19:40:34-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Improve-Cloud-SQL-Infrastructure-Using-High-Availability-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Designing-a-Google-Cloud-Infrastructure-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Designing-a-Google-Cloud-Infrastructure-11/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Designing-a-Google-Cloud-Infrastructure-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:08" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:08-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:35:56" itemprop="dateModified" datetime="2022-11-20T19:35:56-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Designing-a-Google-Cloud-Infrastructure-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Designing-a-Google-Cloud-Infrastructure-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<h1 id="Course-Introduction"><a href="#Course-Introduction" class="headerlink" title="Course Introduction"></a>Course Introduction</h1><p>Welcome to designing a Google Cloud infrastructure. I’m Guy Hummel, and I’ll be showing you how to build an enterprise IT solution in Google platform.</p>
<p>To get the most from this course, unless you already have a lot of experience using Google Cloud, you should take the Google Cloud Platform Fundamentals and Systems Operations courses to get a solid understanding of the different components of Google Cloud. In this course, I’ll be showing you how to use these building blocks to construct an enterprise class application architecture.</p>
<p>We’re going to use a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/case-study-1/">case study</a> as an example of how to apply enterprise principles to a design. I’ll start by explaining how you would take an organization’s requirements, and translate them into the appropriate <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/compute-1/">compute</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/storage-2/">storage</a>, and network components in Google Cloud. I’ll also show you how to make it a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> design.</p>
<p>Then I’ll cover how to secure the environment, including how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/authentication-2/">authenticate</a> and give permissions to people as well as to applications using <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/service-accounts-1/">service accounts</a>, how to encrypt your data, and how to comply with a rigorous security standard like PCI DSS.</p>
<p>Finally, we’ll wrap up with how to design a solution that can recover from <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disasters</a>.</p>
<p>All right, if you’re ready to learn how to create an enterprise class architecture for your Google Cloud infrastructure, then let’s get started.</p>
<h1 id="Case-Study"><a href="#Case-Study" class="headerlink" title="Case Study"></a>Case Study</h1><p>Suppose you’ve been hired to help a company called Great Inside, which offers interior design software as a service.</p>
<p>Great Inside makes its money by selling subscriptions to its web-based interior design application. It also has a free version that’s supported by advertising. Their customers are primarily in North America, but they hope to expand in Europe and Asia at some point in the future.</p>
<p>The company has grown slowly for five years, but recently closed a venture capital round, brought in experienced executives, and is now growing more quickly. The company’s existing infrastructure is not capable of scaling up quickly enough, so they would like to move to the cloud.</p>
<p>Great Inside started off with a Microsoft-centric infrastructure and then migrated to a LAMP stack. The only Microsoft infrastructure left is the payment processing system and an Active Directory server. They would like to retire their Microsoft servers in the future, other than Active Directory. But that isn’t a priority right now, and the company would like to move both types of servers to the cloud. They’ve also started a pilot project using a NoSQL database.</p>
<p>Since they accept credit cards, they need to be PCI DSS compliant. Since their volume is increasing, they need to ensure that their payment processing environment meets a higher level of compliance. Note that Great Inside passes the validation and processing of credit card information to a certified payment processor.</p>
<p>They would like to improve their disaster recovery solution. At the moment, they’re backing their data up to a cloud service, but it would take them a long time to recover from a disaster.</p>
<p>Their existing technical environment is all in a single data center.</p>
<p>They have three types of databases. MySQL for the interior design application, Microsoft SQL Server for payment processing, and a NoSQL database in the development environment.</p>
<p>They have two types of web and application servers. Apache and Tomcat are running on six servers, each with 2 dual-core CPUs, 24GB of RAM, and two mirrored 200GB disks. These servers are for their interior design application. IIS is running on four servers- two customer-facing and two internal, each with a dual-core CPU, 16GB of RAM and two mirrored 250GB disks. These servers are for payment processing.</p>
<p>They have a variety of infrastructure servers, including Active Directory and a file server for internal documents, etc.</p>
<p>Here are their business requirements. Scale easily to handle rapid growth, move as much of the development, test, and production infrastructure as possible to the cloud, and increase performance, reliability, and security while reducing management overhead.</p>
<p>And their technical requirements are: connect the data center’s network with the cloud environment’s network, encrypt all data, design <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> into all tiers, and create a <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disaster recovery</a> solution that will reduce recovery time to a few hours, rather than a day.</p>
<p>I should mention up front that some aspects of this case study may not be completely realistic. It’s simplified so we can go through it in a reasonable amount of time, but it has just enough complexity to allow us to cover the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">key topics</a>.</p>
<h1 id="Compute"><a href="#Compute" class="headerlink" title="Compute"></a>Compute</h1><p>Although most Google Cloud designs include virtual machine instances, that doesn’t mean VMs are your only option for compute resources. Before you start designing a solution using only Compute Engine instances, you should consider App Engine and Kubernetes Engine.</p>
<p>App Engine is designed for people who don’t want to manage an application’s underlying infrastructure. App Engine provisions and scales all of the resources your application needs behind the scenes, without any human intervention required. That sounds great, doesn’t it? So why wouldn’t you use App Engine?</p>
<p>The main reason is that it is much easier to develop a new application on App Engine than it is to migrate an existing one to it. So if you’re developing an application from scratch, then App Engine may be a good choice. If you have an existing application, then you’ll need to check if App Engine supports the programming languages your app is written in and if your app has any operating system dependencies (such as only being able to run on Windows, which isn’t supported by App Engine).</p>
<p>You’ll also need to look at your application’s architecture to see if it would be able to run on App Engine without having to re-architect it. App Engine is designed for microservices-based apps, so if your existing application has a monolithic architecture, then it might require some work to migrate it.</p>
<p>For all of these reasons, it’s usually advisable to use App Engine only for new applications rather than existing ones.</p>
<p>The next option is Kubernetes Engine. It provides many of the benefits of App Engine, in that you don’t have to worry about the underlying operating system running your application. It also handles scaling, although you have to configure that yourself first. It does require more management than App Engine, but it doesn’t require as much management as Compute Engine.</p>
<p>The ideal case for using Kubernetes Engine is, of course, if your application already runs in containers, especially Docker containers, since that’s what Kubernetes Engine supports. On the other hand, if your application will only run on certain operating systems, especially Windows, then it won’t run in Kubernetes Engine.</p>
<p>If you have an existing app that does not currently run in containers, then you might want to see if it’s possible to containerize it so you can take advantage of Kubernetes Engine.</p>
<p>If your existing application runs on virtual machines, then the easiest way to migrate it to Google Cloud is to use Compute Engine instances. If it doesn’t run on virtual machines, then you’ll have to virtualize it before you can run it on Google Cloud.</p>
<p>Although Compute Engine requires more management than App Engine or Kubernetes Engine, it does give you ultimate flexibility. For example, you could run an application that requires Windows, a specific network driver, and high-performance GPUs.</p>
<p>Since our case study involves an existing application that doesn’t currently run in containers, we’re going to choose Compute Engine for our design.</p>
<p>The case study company, GreatInside, currently has 6 machines running Apache and Tomcat, and 4 machines running IIS. Let’s have a look at Google’s predefined machine types . We need to decide how many vCPUs and how much memory to use. Memory is pretty straightforward. Our existing machines have 24GB for the Tomcat servers and 16GB for the IIS servers. VCPUs are more complicated, though.</p>
<p>The existing Tomcat servers have two dual-core CPUs and the IIS servers have one dual-core CPU. How does that translate into vCPUs? Some people say that cores and vCPUs are equivalent, but that’s not quite true. A vCPU on a Compute Engine instance is implemented as a single hyper-thread on an Intel Xeon processor. Since each Xeon processor has 2 hyperthreads, that means you need to multiply the number of cores by 2 to get the number of threads, and thus the number of vCPUs.</p>
<p>So our Tomcat servers have the equivalent of 8 vCPUs (4 cores times 2) and our IIS servers have the equivalent of 4 vCPUs (2 cores times 2). Of course, if we really wanted to be accurate, we’d need to take into account things like the clock speed of the CPUs, but we’re not going to go that far.</p>
<p>So, we need 8 vCPUs and 24GB of RAM for the Tomcat servers and 4 vCPUs and 16GB of RAM for the IIS servers. Do any of the predefined machine types match these requirements? Well, the n1-standard-4 is almost identical to the IIS server requirements. It has 4 vCPUs and 15GB of RAM. Having one less gig of RAM is probably fine, but you can monitor it in production to make sure it’s sufficient.</p>
<p>The Tomcat servers are another story, though. The closest match is the n1-standard-8, which has 8 vCPUs and 30GB of memory. That’s 6GB more than we need, so we should consider a custom machine type. We can select the exact size we need. With this custom configuration, it says it will cost $190.54 per month. Let’s see how that compares to the n1-standard-8. That costs $194.58 per month, which is more expensive, but only 2% more.</p>
<p>I should mention that there are a couple of ways to reduce those costs: sustained-use discounts and committed-use discounts. If you know that you’re going to be running an instance continuously for a long period of time, then you can pay much less by purchasing either a one-year or three-year contract, which is called a committed-use contract. This will typically reduce the cost by up to 57%. However, that’s a pretty big commitment, so Google provides a way to reduce costs without signing a long-term contract. You start getting an automatic discount after an instance runs for more than 25% of a month, and the discount increases the longer the instance runs during that month. For most machine types, you’ll receive a sustained-use discount of 30% if you run the instance for the entire month.</p>
<p>Okay, let’s get back to our case study. Since IIS and SQL Server run on Windows, we’ll need to figure out how to license them. Let’s start with IIS. For Windows Server itself, you can either use Google’s pay-as-you-go Windows licensing or you can bring your own license. </p>
<p>There are two ways to use Google’s pay-as-you-go Windows licensing. The first way is to create a new instance with one of the pre-configured Windows Server boot disks . The second way is to import a Windows VM. There are two options for importing a VM. The first option is to import a virtual disk and turn it into an image that you can use to create a Compute Engine instance. That’s quite simple to do, but it’s not meant for migrating mission-critical applications or migrating a large number of VMs in an automated fashion.</p>
<p>A more sophisticated option is to use Cloud Migrate for Compute Engine. This service makes replicas of existing VMs you have running on-premises or on another cloud platform. It will take care of the many steps that are needed to migrate important applications. </p>
<p>If you want to bring your own Windows licenses, then you can run your Windows VMs on sole-tenant nodes, which are dedicated physical servers that are not shared with other customers.</p>
<p>If you need to run any Microsoft applications, then you’ll need licenses for those too, of course, but Microsoft is more flexible with its application licensing than with Windows licensing. If your organization has active Software Assurance contracts for its Microsoft applications, then you can move those licenses to either Compute Engine instances or sole-tenant nodes.</p>
<p>Now let’s move on to SQL Server. You can use any of the options I just mentioned, but fortunately, there are also easier options for SQL Server. One option is to create instances with pre-configured SQL Server boot disks . These include pay-as-you-go licenses for both Windows Server and SQL Server. The second option is to use Cloud SQL, which is a managed service. I’ll tell you more about it in the next video.</p>
<p>For premium Linux OSs (such as Red Hat or SUSE), licensing is much simpler. You can either create an instance with a pre-configured boot disk or you can import your Linux VM. In both cases, you can either use a Google pay-as-you-go license or bring your own license.</p>
<p>I should mention one other Compute Engine option – preemptible VMs. They’re up to 80% cheaper than regular instances, but since Google can remove them with only 30 seconds’ notice, you would usually only use them as disposable instances for things like big data batch jobs. That doesn’t fit our use case, so we’ll stick with regular instances.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><p>Each of the instances for the Tomcat and IIS servers will come with a standard persistent boot disk by default, but we might need something different. There are many options for instance storage, including Standard Persistent Disk, SSD Persistent Disk, Local SSD, RAM Disk, and Cloud Storage.</p>
<p>Standard Persistent Disks are magnetic drives. Their main advantage is low cost. SSD Persistent Disks (or solid state disks) have up to 4 times the throughput and up to 40 times the I&#x2F;O operations per second of a Standard Persistent Disk, so if you need high performance, SSDs are a must.</p>
<p>But SSD Persistent Disks aren’t even your fastest option. Local SSDs are up to 600 times as fast as Standard Persistent Disks in IOPS and up to 15 times as fast in throughput.</p>
<p>Why are Local SSDs so much faster than SSD Persistent Disks, which are obviously both using SSD technology? Well, it’s because Local SSDs are not redundant and are directly attached to an instance. That gives them major speed advantages, but with high risk because if they suffer a hardware failure, then your data will be gone. Furthermore, Local SSDs disappear when you stop or delete an instance, so you should only use them for temporary data that you can afford to lose, such as a cache.</p>
<p>There are a couple more disadvantages of Local SSDs too. First, they are only available in one size – 375GB, which is kind of an awkward number. Second, they can’t be used as boot disks.</p>
<p>If you need even faster storage, then you can use a RAM disk, which essentially makes a chunk of memory look like a filesystem. Although RAM disks are the fastest option, they’re even less durable than Local SSDs, so they’re only suitable for temporary data. It’s also an expensive option because RAM is much more expensive than SSDs.</p>
<p>One more option is Cloud Storage. This is kind of a weird way to add storage to an instance because a bucket is object storage rather than block storage. That means it can’t be used as a root disk and it may be unreliable as a mounted filesystem. So why would you ever use it? The first advantage of using Cloud Storage is that multiple instances can write to a bucket at the same time. You can’t do that with persistent disks, which can only be shared between instances in read-only mode. The danger is that one instance could overwrite changes made by another instance, so your application would have to take that into account.</p>
<p>The second advantage is that an instance can access a bucket in a different zone or region, which is great for sharing data globally, especially if it’s read-only data, which would avoid the overwriting problem.</p>
<p>However, Cloud Storage usually isn’t a good option for instance storage. It is good for general-purpose file serving, though, so it would be a potential choice for replacing GreatInside’s internal file server if they want to move it to the cloud. To do this, you’d need to use Cloud Storage FUSE, which is open source software that translates object storage names into a file and directory system. Essentially, it makes Cloud Storage buckets look like network file systems. A better choice, though, would be Cloud Filestore, which is a fully-supported file sharing service that’s designed specifically for this purpose. It’s compatible with NFS version 3.</p>
<p>So, which instance storage option should we use for our instances? Since performance is important, we should use something faster than Standard Persistent Disks. SSD Persistent Disks are many times faster than standard ones, so they’d be a good choice. Should we consider Local SSDs or RAM disks? Well, neither of those can be boot disks, so we would have to use them in addition to a persistent boot disk. The higher performance wouldn’t outweigh the extra cost and complexity of using one of these options, though, so we should just stick with SSD Persistent Disks. Furthermore, since persistent disks are redundant, we don’t need to have two mirrored disks on each instance like GreatInside does in its existing data center. We can just have a single persistent boot disk on each instance.</p>
<p>As for the size, we can specify the exact amount we need, so for the Tomcat servers, we should use one 200GB disk on each instance, and for the IIS servers, we should use one 250GB disk on each.</p>
<p>Next, we need to look at our database options. Google Cloud has 5 different database services: Cloud SQL, Cloud Datastore, Bigtable, BigQuery, and Cloud Spanner.</p>
<p>Cloud SQL is a relational database. It’s a managed service for MySQL, PostgreSQL, or Microsoft SQL Server. It’s suitable for everything from blogs to ERP and CRM to ecommerce.</p>
<p>Cloud Datastore is a NoSQL database service. Unlike a relational database, such as Cloud SQL, it is horizontally scalable. A relational database can scale vertically, meaning that you can run it on a more powerful VM to handle more transactions, but there are obviously limits to the size of a VM. You can also scale a relational database horizontally for reads by using read replicas, but most relational databases can’t scale horizontally for writes. That is a major problem that is solved by NoSQL databases.</p>
<p>Because of this and because it’s an eventually consistent database, Cloud Datastore is faster than Cloud SQL. It’s best suited to relatively simple data and queries, especially key-value pairs. Typical examples include user profiles, product catalogs, and game state. For complex queries, Cloud SQL is a better choice.</p>
<p>Cloud Bigtable is also a NoSQL database. It’s designed to scale into the petabyte range with high throughput and low latency. It does not support ACID transactions, so it shouldn’t be used for transaction processing. It’s best suited for storing huge amounts of single-keyed data. If you have less than one terabyte of data, then Bigtable is not the best solution. It can handle big data in real-time or in batch processing. Typical examples are Internet of Things applications and product recommendations.</p>
<p>BigQuery also handles huge amounts of data, but it’s more of a data warehouse. It’s something you use after data is collected, rather than being a transactional system. It’s best suited to aggregating data from many sources and letting you search it using SQL queries. In other words, it’s good for OLAP (that is, Online Analytical Processing) and business intelligence reporting.</p>
<p>Google’s newest database service is Cloud Spanner, which seems to combine the best of all worlds. It’s a relational database that also scales horizontally. That is, it combines the best features of traditional databases like Cloud SQL and the best features of NoSQL databases like Cloud Datastore. So why wouldn’t you use it for all of your database needs? Well, mostly because it’s more expensive than the other options. Also, if your application is written specifically for a particular database, such as MySQL, then Cloud SQL would be a better choice, unless you can rewrite it to work with Cloud Spanner. </p>
<p>So use Cloud Spanner when you need a relational database that is massively scalable. Typical uses are financial services and global supply chain applications.</p>
<p>Now, which database services should GreatInside use? It currently has two production databases – MySQL for the interior design application and SQL Server for payment processing. There are two ways you could migrate the MySQL database to Google Cloud. You could use Cloud SQL or run MySQL on a regular instance. Considering that GreatInside wants to reduce system management tasks, Cloud SQL would be the best choice since it’s a fully managed MySQL service, with automatic replication and backups.</p>
<p>For SQL Server, you have the same two options. You could use Cloud SQL, or you could run it on a regular instance. Again, Cloud SQL is the best choice.</p>
<p>GreatInside does have one more database – their experimental NoSQL datastore. Since the development team is still evaluating this technology, you should talk to them about trying Cloud Datastore. They should also try App Engine because Cloud Datastore works best when used with App Engine.</p>
<p>And that’s it for storage and databases.</p>
<h1 id="High-Availability"><a href="#High-Availability" class="headerlink" title="High Availability"></a>High Availability</h1><p>Before we talk about networks, we need to talk about how we can make our applications highly available.</p>
<p>If you have an application that’s running on only one VM instance, then, of course, it’s a single point of failure, and if it goes down, your application goes down. So, at a minimum, you should always have at least two VMs for every component of your solution. But where should those instances be located?</p>
<p>When you create a VM instance, it gets created in a particular zone, such as us-central1-a. A zone is an isolated location. You can think of a zone as a data center or an isolated portion of a data center.</p>
<p>If you put both instances in the same zone, then both of them could potentially go down if there’s a problem in that zone. So, you should put the instances in different zones. For performance reasons, you may need to put them in zones that are in the same region, such as us-central1. Notice that the zone name is just the region name with a dash and a letter at the end. All of the zones in a region have high-bandwidth, low-latency network connections between them, so if instances that are spread across a region need to mirror data with each other, then they can do this quickly.</p>
<p>Although “region” sounds like a geographic area, it’s just a data center campus in one location. For example, all of the zones in the us-central1 region are in Council Bluffs, Iowa. So, for maximum availability, you may also want to distribute your instances across different regions.</p>
<p>For a higher level of availability, you can use autoscaling instance groups. This was covered extensively in the “Google Cloud Platform: Systems Operations” course, so I’ll just go over the highlights.</p>
<p>An instance group consists of identical instances that perform processing for your application. If one of the instances fails, then a health check will notice this and replace the instance with a new one. If the load on the instance group gets too high, then the autoscaler will add more instances to maintain good application performance.</p>
<p>To ensure availability even if an entire zone fails, you should distribute the instances across multiple zones. Luckily, this is very easy to do. You just have to select “Multizone” when you’re creating the instance group.</p>
<p>If you want to make sure you’ll still have enough instances to handle the load if an entire zone goes down, then you should overprovision by 50%. For example, if your instances are spread across 3 zones and you need 6 instances to handle your normal traffic load, then you should provision 9 instances. That way if one of the zones goes down (which would take out 3 of the instances), you’ll still have 6 instances left in the two remaining zones.</p>
<p>You can either overprovision by 50% at all times or you could save money by just setting the upper limit on your autoscaler to at least 50% more than the normal number of instances. If you decide to depend on the autoscaler during a zone failure, then the instances in your remaining two zones will be very heavily loaded until the autoscaler provisions additional instances, so only choose this option if you can tolerate this temporary performance degradation.</p>
<p>Since GreatInside has 6 web tier instances for its main application, this is how it should be set up. For the 2 customer-facing IIS instances in the payment processing system, you’d set an upper limit of 3 instances, which is 50% more than the 2 instances that it normally needs.</p>
<p>To make the instance group work as a high availability solution, you’ll need a couple of other components. First, the instance group has to be behind a load balancer that will distribute incoming requests to different instances. Second, the instances cannot have any stateful data. Otherwise, the same instance would have to handle all requests from a given user. Although you can enable the “session affinity” option in this situation, it will ruin your high availability since a failed instance will impact all of the users on it.</p>
<p>Since most applications do have stateful data, you have to put it on other components, such as a database or Cloud Storage. Unfortunately, that just moves the availability issue to a different layer, but fortunately, Google Cloud has good ways to handle storage availability.</p>
<p>If Cloud Storage is sufficient for your stateful data needs, then you’re covered because Cloud Storage is automatically replicated either across zones in a region (for the Regional type) or across regions (for the Multi-Region type).</p>
<p>If you need a database for your stateful data, then there are different availability solutions depending on the data service.</p>
<p>With Cloud SQL, you can simply check the “High availability” box when you create a Cloud SQL instance. This will create a failover replica in another zone. In the event of a failure, Cloud SQL will automatically fail over to the replica. This option is available for MySQL, PostgreSQL, and SQL Server.</p>
<p>Since Cloud Datastore is a NoSQL database, it scales horizontally, which makes high availability easier than with Cloud SQL. Cloud Datastore automatically replicates data across zones in a region. When you create a Datastore instance, you specify which region and it does the rest.</p>
<p>Bigtable is also a NoSQL database that scales horizontally, but if you want it to replicate across multiple zones, then you’ll have to configure it to do that. You can even configure it to support replication across regions if you need that. But in its simplest configuration, it only stores data in a single zone, which gives it higher performance. It’s still stored redundantly in that configuration but within the same zone.</p>
<p>BigQuery automatically replicates data within a region, but it’s a data warehouse, so it’s not suitable for real-time stateful data storage.</p>
<p>Cloud Spanner also automatically replicates data within a region, so it’s highly available out of the box, and unlike Cloud SQL, it doesn’t need a failover replica, which is a less available solution.</p>
<p>In summary, if a NoSQL database is sufficient for your application, then Cloud Datastore is your best choice for storing stateful data. If you need to use a relational database, then either use Cloud SQL and enable high availability or use Cloud Spanner for even higher availability if you’re willing to pay a higher price.</p>
<p>Since GreatInside is going to use Cloud SQL for both MySQL and SQL Server, then we just need to enable the high availability option when we create those databases.</p>
<p>And that’s it for this lesson.</p>
<h1 id="Networks"><a href="#Networks" class="headerlink" title="Networks"></a>Networks</h1><p>Now we know all of the components we want to use and we just need to connect them together with networks. Google provides what are called Virtual Private Clouds, or VPCs, but I’m just going to call them networks.</p>
<p>There are 5 layers in <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">Google Cloud</a> you can use to isolate and manage resources: organizations, folders, projects, networks, and subnetworks.</p>
<p>You aren’t required to have organizations or folders, but they can be useful, especially for large companies.</p>
<p>Projects are required, though. You use them to provide a level of separation between resources. Not only are resources in different projects unable to communicate with each other, but they’re even in different billing accounts. Projects also have separate security controls, so for example, you could give Bob in QA the highest level of access in the Test Environment project, but a lower level of access in the Production Environment project.</p>
<p>Each project has one default network that comes with preset configurations and firewall rules to make it easier to get started, but you can customize it, or you can create up to 4 additional networks (for a total of 5). If 5 networks per project isn’t enough for you, then you can request a quota increase to support up to 15 networks in each project.</p>
<p>A network belongs to only one project, a subnet belongs to only one network, and an instance belongs to only one subnet.</p>
<p>Instances in the same subnet or even different subnets within the same network can communicate with each other. Subnetworks are used to group and manage resources.</p>
<p>A network spans all regions, but each subnet can only be in one region. A subnet allows you to define an IP address range and a default gateway for the instances you put in it. The IP address ranges of the different subnets must be non-public (such as 10.0.0.0) and must not overlap, but other than that, there are no restrictions on them. For example, they can be different sizes. They must be IPv4 addresses, though, because Compute Engine doesn’t support IPv6 yet.</p>
<p>A network can have either automatic or custom subnets. With automatic, the subnets are created for you, one in each region. With custom, you create them yourself. If you discover that you need to customize a network with automatic subnets, then you can convert it to custom mode, but once you do, you cannot convert it back to automatic mode.</p>
<p>On the default network, instances within the same subnet can communicate with each other over any TCP or UDP port, as well as with ICMP. Instances in the same network can communicate with each other, regardless of which subnets they’re in, because Google Cloud creates routes between all of the subnets in a network. However, the default network’s firewall rules only allow ssh, rdp, and icmp traffic between subnets.</p>
<p>If you don’t want instances in different subnets to be able to reach each other, then you can change the firewall rules to deny traffic between them.</p>
<p>Note that only the default network comes with predefined firewall rules. When you create a new network, it doesn’t have any firewall rules. However, the instances in that network will still be able to communicate with the Internet, assuming they have external IP addresses, because all outgoing traffic from instances is allowed. Only incoming traffic is blocked. And when an instance sends a request over the Internet, the incoming response is allowed, so two-way traffic is enabled at that point.</p>
<p>Each network includes a local DNS server so VM instances can refer to each other by name. The fully qualified domain name for an instance is [HOSTNAME].c.[PROJECT_ID].internal. This name is tied to the internal IP address of the instance. An Instance does not know its external IP address and name. That translation is handled elsewhere in the network.</p>
<p>To reach Internet resources, each VM needs an external IP address. An ephemeral external IP address is created for each VM by default, but an ephemeral address gets replaced with another one if you stop and restart the instance, so if you want an instance to always have the same IP address, then you need to assign a static IP address to it.</p>
<p>Since IPv4 addresses are a scarce resource, Google doesn’t want customers to waste them. So you’re not charged for having static IP addresses as long as you’re using them. But if a static IP address is not associated with a VM instance or if it’s associated with an instance that’s not running, then you’ll be charged for it.</p>
<p>Normally, if a VM needs to send requests to other Google services, such as Cloud Storage, then by default, it has to do so using a public IP address rather than an internal one. This is problematic if you don’t want any of your internal network communications to go over the Internet. However, if you enable the Private Google Access option in a subnet, then VMs in that subnet can connect to Google services using internal IP addresses, so their requests will go over Google’s network rather than the Internet.</p>
<p>If you want instances in different projects to communicate with each other, then you have three options: the Internet, VPC Network Peering, or a Shared VPC. Connecting over the Internet is slower, less secure, and more expensive than the other two options, so it’s not usually the best choice.</p>
<p>The simplest alternative is VPC Network Peering. This allows two VPCs to connect over a private RFC 1918 space, that is, using non-routable internal IP addresses, such as 10.x.y.z. In other words, they don’t need public IP addresses, and they communicate over Google’s network. Not only can you do this for VPCs in different projects, but you can even use it to connect VPCs in different organizations. To make this work, both sides have to set up a peering association. If only one side sets up a peering association with the other VPC, then the networks won’t be able to communicate with each other. Also bear in mind that there can’t be any overlapping IP ranges in the two networks. You’ll notice in this example that the two ranges are not overlapping.</p>
<p>A more complicated option is to use a Shared VPC. The idea is that instances in different projects can share the same network. This is kind of a weird idea. If you’ve put resources in different projects, you probably want them to be managed separately, so why would you get them to use the same network? In most cases, it’s to enforce security standards. For example, if you want to use the same firewall rules across all of your projects, then this is a good way to do that.</p>
<p>To set up a Shared VPC, you need to designate one of the projects as the host project and the others as service projects. The host project is the one that contains the Shared VPC. Instances in the service projects can use subnets in the Shared VPC. This is made possible by giving Service Project Admins the authority to create and manage instances in the Shared VPC but nothing more. Meanwhile, the Shared VPC Admins have full control over the network. Note that all of the projects in this arrangement have to be part of the same organization.</p>
<p>OK, we’ve gone over a lot of networking topics. Now how should we apply these concepts to GreatInside?</p>
<p>At a minimum, we should create separate projects for the Development, Test, and Production environments. Inside each project, we should stick with the default network. There’s no need to add any additional ones. We should also stick with automatic subnetworks. The only subnetwork we need right now is one in the US, such as us-central1, since we don’t currently have any plans to expand into other parts of the world. When GreatInside decides to add instances overseas, then they can be added to the other regional subnets.</p>
<p>The default firewall rules should also be fine, since they only allow internal traffic plus ssh, icmp, http, and https. We should remove the rule that allows rdp traffic in the Production network, though, since we don’t have any Windows instances in it.</p>
<p>We don’t want the Production, Development, and Test environments to be part of the same network, so we don’t need a Shared VPC. In fact, we don’t want them to communicate with each other at all, so we don’t need to use VPC Network Peering either.</p>
<p>By the way, you probably noticed that everything I’ve shown so far is only for the interior design application. I’m going to get into the details of how to set up the payment processing environment in the Legislation and Compliance lesson.</p>
<p>One last item is that we have to decide which components need external IP addresses. That’s easy in this case because the load balancer is the only one that needs an external IP address (and ideally it should be a static address). Users will connect to the web instances through the load balancer, so the web instances only need internal IP addresses, and for security reasons, that’s all they should have.</p>
<p>That does raise the question of how a system administrator could connect to them for troubleshooting, though. One way is to give your administrators access to the internal network by interconnecting it with the company’s on-premises network. That’s something that GreatInside has already requested, so let’s see how to do that. There are three ways: Cloud VPN, Cloud Interconnect, and Direct Peering.</p>
<p>Cloud VPN lets you set up a virtual private network connection between your own network and Google Cloud. To do this, you need to have a peer VPN gateway in your own network and it needs to use IPsec to connect to the Cloud VPN Gateway and encrypt traffic. You can have multiple tunnels to a single VPN gateway.</p>
<p>By itself, Cloud VPN requires you to make changes to static routes on your tunnels manually. But if you use Google Cloud Router, then the routes will be updated dynamically using BGP (that is, Border Gateway Protocol). Network topology changes are propagated automatically.</p>
<p>The second way to connect is called Cloud Interconnect. Instead of connecting over the Internet, you can use an enterprise-grade connection to Google’s network edge. There are two ways to do this: Dedicated Interconnect and Partner Interconnect. If your internal network extends into a colocation facility where Google has a point of presence, then you can connect your network to Google’s. This is called Dedicated Interconnect. It’s a great solution that provides higher bandwidth and lower latency than a connection over the public internet. It’s a bit expensive, though, because the minimum bandwidth is 10 Gbps.</p>
<p>If you don’t have a presence in a supported colocation facility or you want to pay for a connection that’s smaller than 10 Gbps, you can use Partner Interconnect. With this option, you connect to a service provider that has a presence in a supported colocation facility. You can purchase a monthly contract for connections as small as 50 Mbps and as large as 10 Gbps. </p>
<p>The third way is to use Peering. This is similar to Cloud Interconnect because you connect your network to Google’s network at a point of presence either directly (which is called Direct Peering) or through a service provider (which is called Carrier Peering). One big difference with peering is that it doesn’t cost anything. So why would anyone pay for Cloud Interconnect when they could peer with Google for free? Well, because with Cloud Interconnect you get a direct connection between your on-premises network and one of your VPCs in Google Cloud. You have full control over the routing between your networks. If you want to change a route, you can change it on your on-premises router, and it will be picked up by BGP. Although the peering option uses BGP, too, it’s done at the most basic level. It doesn’t create any custom routes in your VPC network.</p>
<p>Since we don’t have requirements for low latency and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a> between the company network and Google Cloud, we should go with Cloud VPN to connect. We should also use Cloud Router so network routes will be updated dynamically.</p>
<p>And that’s it for networks.</p>
<h1 id="Authentication"><a href="#Authentication" class="headerlink" title="Authentication"></a>Authentication</h1><p>The first step in giving secure access to your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">Google Cloud infrastructure</a> is to decide how to authenticate your users. By default, Google Cloud Platform requires users to have a Google account to access it. But if you have more than a handful of users, then you’ll want to find a centralized way to manage your user accounts. The solution is to use the G Suite Global Directory. You don’t have to use G Suite products like Google Docs, you can just use G Suite for user management.</p>
<p>Most organizations already have a user directory, so the best policy is usually to manage users in your existing directory, and then synchronize the account information in G Suite. There are three ways to do this: Google Cloud Directory Sync or GCDS, the Google Apps Admin SDK, or a third party connector.</p>
<p>Google Cloud Directory Sync is the easiest solution if you have either Active Directory or an LDAP server. It synchronizes users, groups, and other data from your existing directory to your Google Cloud Domain Directory. GCDS runs inside your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/networks-1/">network</a> on a machine that you control.</p>
<p>It’s a one-way synchronization, so GCDS doesn’t modify your existing directory. Of course the synchronization can’t be a one-time event. It has to happen on a regular basis to keep your Google Directory up-to-date.</p>
<p>To make authentication even easier for your users, you can implement single sign-on or SSO. Google Cloud Platform supports SAML 2.0-based SSO. If your system doesn’t support SAML 2.0, then you can use a third party plugin.</p>
<p>Once you’ve implemented SSO, then when a user would normally have to login, Google will redirect your authentication system. If the user is already authenticated in your system, then they don’t have to login to Google Cloud separately. If they aren’t already logged in, then they’re prompted to login.</p>
<p>In order for this to work, your users must have a matching account in Google’s Directory. So you still need to use GCDS or one of the other synchronization options.</p>
<p>In our <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/case-study-1/">case study</a>, since we have an active directory server, we’ll use GCDS for synchronization and also implement single sign-on.</p>
<p>And that’s it for authentication.</p>
<h1 id="Roles"><a href="#Roles" class="headerlink" title="Roles"></a>Roles</h1><p>To give a user permission to access particular Google Cloud resources, you assign a role to them. Basic roles act at the project level. There are 3 basic roles available: Owner, Editor, and Viewer. There are also fine-grained roles for individual resources. These are called predefined roles. (They were previously known as curated roles.) For example, the Cloud SQL Viewer role gives read-only access to Cloud SQL resources.</p>
<p>You can assign roles at different levels of the hierarchy, that is, at the organization, folder, project, and resource levels. If you assign roles to the same user at different levels, then their effective permissions are the union of the permissions at the different levels.</p>
<p>For example, if you granted Marie the Viewer role at the organization level and the Editor role at the project level, then she would have Editor permissions for all of the resources in that project. The Viewer role at the organization level would not override the Editor role at the project level. Similarly, if you assigned them in the opposite way, with the Editor role at the organization level and the Viewer role at the project level, Marie would still have the Editor role for all of the resources in that project because the project-level permissions would not override the organization-level permissions.</p>
<p>There are a few principles you should apply when setting roles and permissions. </p>
<p>First, use the principle of least privilege when granting roles. That is, assign roles with the least permissions required for people to do what they need.</p>
<p>Second, whenever possible, assign roles to groups instead of to individuals. Then, when you need to grant a role to a user, you can just add them to the group. Not only is this easier to manage, but it also ensures consistent privileges among members of a particular group. You can also use a descriptive group name that makes it clear why group members need those permissions.</p>
<p>Third, keep tight control of who can add members to groups and change policies. If you don’t, then people could give themselves or others more privileges than they should have.</p>
<p>Fourth, to make sure that inappropriate policy changes aren’t made, audit all policy changes by checking the Cloud Audit Logs, which record project-level permission changes.</p>
<p>Now let’s apply these principles to GreatInside. First, you would grant the project owner role to a few key system administrators. Owners are the only ones who can change policies (unless you grant users the Organization Administrator role). You should always have more than one owner. Otherwise, if that person is unavailable or leaves the organization, it would be difficult to for someone else to take their place as owner. So avoid that situation by giving the owner role to several people, but choose wisely because owners can do just about anything. </p>
<p>Similarly, you would have a small number of G Suite administrators who could add users to groups.</p>
<p>Obviously, there would be a large number of users who would need permissions, so I’m not going to talk about every type of user, but I’ll give a couple of examples. One example would be a network administration group that you would grant the Compute Network Admin role to.</p>
<p>Another example would be a QA team. You could grant their group the editor role on the Test Environment project and the viewer role on the Production Environment project. Alternatively, if the QA people don’t need full access to the Test Environment, then you could grant them several predefined roles, such as Compute Instance Admin, Cloud SQL Admin, and Compute Storage Admin.</p>
<p>Regarding audit logs, someone would need to take on the responsibility of checking for policy changes. The Admin Activity audit logs are viewable by all project members, so you wouldn’t need to grant access to the person who does the checking.</p>
<p>And that’s it for roles.</p>
<h1 id="Service-Accounts"><a href="#Service-Accounts" class="headerlink" title="Service Accounts"></a>Service Accounts</h1><p>Now that you have user authentication and permissions figured out, it’s time to plan how your applications will access the Cloud Platform services it needs to use. To avoid embedding credentials in an application, you need to use service accounts. For example, if an application uses Cloud Datastore as a database, then it needs to have authorization to use the Datastore API.</p>
<p>You would accomplish this by enabling Datastore API access on any VM instances that will be involved in the part of the application that uses the database. By default, all VM instances run as the Compute Engine default service account. If you want something different, then you can create your own.</p>
<p>A service account has an email address and a public&#x2F;private key pair that it uses to prove its identity. Your instances use that identity when communicating with other Cloud Platform services. However, by default, an instance running as the Compute Engine default service account has limited scope in how it can interact with other services. For example, by default an instance can only read from Cloud Storage and can’t write to it.</p>
<p>To give an instance more permissions, you need to set the scope when you’re creating the VM. So, in the case of interacting with Datastore, you have to enable access to the Datastore API. You also have to enable the Datastore API at the project level, but you only have to do that once.</p>
<p>Then your application code has to obtain credentials from the service account whenever it uses the Datastore API. Google Cloud Platform uses OAuth 2.0 for API authentication and authorization. There are two ways to do it: Application Default Credentials and access tokens.</p>
<p>The easiest way is to use Google Cloud Client Libraries. They use Application Default Credentials (or ADC) to authenticate with Google APIs and send requests to those APIs. One great feature of ADC is that you can test your application locally and then deploy it to Google Cloud without changing the application code. </p>
<p>Here’s how it works. To run your code outside Google Cloud Platform, such as in your on-premise data center or on another cloud platform, create a service account and download its credentials file to the servers where the code will be running. Then set the environment variable GOOGLE_APPLICATION_CREDENTIALS to the path of the credentials file.</p>
<p>So while you’re developing locally, the application can authenticate using the credentials file and when you run it on a production instance, it will authenticate using the instance’s service account. This works because ADC allows applications to get credentials from multiple sources.</p>
<p>The second way is to use OAuth2 access tokens to directly connect to the API without going through a client library. One reason you’d have to use this method is if your application needs to request access to user data.</p>
<p>The way it works is the application requests an access token from the metadata server and then uses the token to make an API request. Tokens are short-lived, so your application needs to request new ones regularly.</p>
<p>If you need to write shell scripts that access other Cloud Platform services, then you can use gcloud and gsutil commands to make API calls. These two tools are included by default in most Compute Engine images and they automatically use the instance’s service account to authenticate with APIs.</p>
<p>So what service accounts would you need to create for GreatInside? The load balancer and the web instances communicate over HTTPS, so you don’t a service account for that. Since the Tomcat instances communicate with the MySQL database in Cloud SQL, you would need a service account for that. Similarly, the IIS instances communicate with SQL Server in Cloud SQL, so you’d need a service account for that, too. There may be a need for other service accounts when we add more features to our architecture, such as disaster recovery, but we’ll cover that later.</p>
<p>And that’s it for service accounts.</p>
<h1 id="Data-Protection-and-Encryption"><a href="#Data-Protection-and-Encryption" class="headerlink" title="Data Protection and Encryption"></a>Data Protection and Encryption</h1><p>Protecting data is critical in any organization. Google Cloud Platform is very strong in this area because of its default encryption policies. Before we get into encryption, though, let’s look at Access Control Lists (or ACLs).</p>
<p>ACLs specify who has access to Cloud Storage buckets and objects in buckets. I’m not going to cover this topic in depth, but there are a few things to keep in mind when you’re deciding what ACLs to apply to your Cloud Storage.</p>
<p>First, there are actually five different mechanisms for controlling access to Cloud Storage: IAM permissions, ACLs, Signed URLs, Signed Policy Documents, and Firebase Security Rules. With so many different ways to control access, you have to be careful not to create conflicting permissions. Start with the first two: IAM permissions and ACLs.</p>
<p>IAM permissions work at the project level. For example, you can specify that a user has full control of all the objects in all of the buckets in your project, but cannot create, modify, or delete the buckets themselves. So they’re a nice way to grant broad access to buckets and objects, but if you want to set fine-grained access, such as which buckets or objects a particular group can read, then you need to use ACLs.</p>
<p>The confusing thing about using these two mechanisms is that you have to look at both of them to get a complete picture of access permissions. For example, you could list the ACLs for a bucket and see that only Bob has been granted write access, but it wouldn’t show that Jill has also been granted write access to all buckets by IAM. For this reason, whenever possible, you should try to use either IAM or ACLs, but not both.</p>
<p>Another potential source of confusion is that bucket and object ACLs are independent of each other. The ACLs on a bucket do not affect the ACLs on objects inside that bucket. For example, you might think that Jane doesn’t have access to the objects in a bucket because she hasn’t been granted access to the bucket itself, but she could have been granted access to any of the objects in the bucket.</p>
<p>So you should keep a couple of principles in mind. First, apply the principle of least privilege. Grant users and groups only as much access as they need. Second, keep your access control as simple as possible. Try to use as few control mechanisms as you can.</p>
<p>If GreatInside decides to replace its internal file server using Cloud Storage, then the best way to secure the files would be to use ACLs. You would create groups to match the teams in the company and create ACLs that give those groups access to the appropriate resources. For example, you could create a bucket for each group. Then for each bucket, you would make the associated group a writer of the bucket. Finally, you would set the object default permissions so that any new objects uploaded to the bucket would get the same permissions and everyone in the group would have full access. If the company’s needs aren’t that simple, then you would set more complex ACLs.</p>
<p>Now let’s move on to encryption. To ensure that your data is encrypted at all times, it needs to be encrypted when it’s in storage (also known as “at rest”) and when it is being sent over a network (also known as “in flight”). Google Cloud Platform takes care of both of these situations.</p>
<p>Encryption in flight is handled very simply. All of the Cloud Platform services are accessible only by API (even when you’re using other methods, such as the Cloud Console or the gcloud command, they’re making API calls under the hood). And all API communication is encrypted using SSL&#x2F;TLS channels. Furthermore, every request has to include a time-limited authentication token, so the token can’t be used by an attacker after it expires. Of course, for any communications between your Google Cloud infrastructure and outside parties, such as website visitors, you have to use SSL&#x2F;TLS yourself to encrypt the traffic.</p>
<p>Encryption at rest is just as simple if you’re willing to leave it to Google because Cloud Platform encrypts all customer data at rest by default.</p>
<p>So without you having to do anything, all of your data will be encrypted both at rest and in flight. Then why isn’t this the end of this lesson? Well, because your organization might want to take on some of the encryption responsibilities itself.</p>
<p>There are actually two layers of encryption for data at rest. First, the data is broken into subfile chunks, and each chunk is encrypted with an individual data encryption key (or DEK). These keys are stored near the data to ensure low latency and high availability. The DEKs are then encrypted with a key encryption key (or KEK). The keys are AES-256 symmetric encryption keys.</p>
<p>Google always manages the data encryption keys, but your organization can manage the key encryption keys if that’s your preference. There are two options for doing this: Customer-managed encryption keys or Customer-supplied encryption keys.</p>
<p>With the customer-managed option, you use the Cloud KMS service to create, rotate (or automatically rotate), and destroy your encryption keys. The keys are hosted on Google Cloud. You can have as many keys as you want, even millions of them if you actually need that many. You can set user-level permissions on individual keys using IAM and monitor their use with Cloud Audit Logging.</p>
<p>Cloud KMS is a nice service, but why wouldn’t you just let Google manage your key encryption keys and not have to deal with it yourself? The biggest reason is compliance with standards or regulatory requirements, such as HIPAA (for health information) or PCI (for credit card information).</p>
<p>If your organization requires that you generate your own keys and&#x2F;or that they’re managed on-premises, then you have to use Customer-supplied encryption keys. Be aware that this option is only available for Cloud Storage and Compute Engine.</p>
<p>With CSEK, Google doesn’t store your key. You have to provide your key for each operation, and your key is purged from Google Cloud after the operation is complete. Here’s how to do it from the command line with each of the two supported services. To encrypt the disk on a Compute Engine instance, you add the csek-key-file flag and point it to a file that contains the key. To encrypt data you’re uploading to Cloud Storage, you have to do it a bit differently. Rather than adding an encryption flag to the gsutil command, you need to add the encryption key to your .boto file, which is the configuration file for the gsutil command. Then all of your gsutil commands will use that key.</p>
<p>It only stores an SHA256 hash of the key as a way to uniquely identify the key that was used to encrypt the data. When you make a request to read or write the data in the future, your key can be validated against the hash. The hash cannot be used to decrypt your data.</p>
<p>There’s a big risk in using this method, though. If you lose your keys, you won’t ever be able to read your data again, and you’ll end up deleting it so you won’t be paying storage charges for unreadable data.</p>
<p>So far all of the encryption methods we’ve covered, including default encryption, Cloud KMS, and CSEK have been examples of server-side encryption. This is where your data is encrypted after Google Cloud receives your data. The only major difference between the 3 methods is where the key comes from. But there is another way. It’s client-side encryption. This means that you encrypt the data before you send it to Google Cloud. Google won’t even know that it’s already encrypted and it will encrypt it again. When you read your data back, Google Cloud will decrypt it on the server side first and then you’ll decrypt your own layer of encryption on the client side. The same warning applies - if you lose your keys, your data will effectively be gone.</p>
<p>Since our case study includes credit card information, we’ll need to be PCI DSS compliant, so we should use Cloud KMS to manage our keys. I’ll talk more about PCI compliance in the next lesson.</p>
<h1 id="Legislation-and-Compliance"><a href="#Legislation-and-Compliance" class="headerlink" title="Legislation and Compliance"></a>Legislation and Compliance</h1><p>Google Cloud Platform has passed annual audits for some of the most important security standards, including SOC 1, 2, and 3, ISO 27001, and PCI DSS. It also complies with HIPAA, CSA STAR, the EU-US Privacy Shield Framework, and MPAA controls, none of which require annual audits.</p>
<p>So if your organization is required to comply with any of these standards, then you know that Google has done its part. But this is a shared responsibility because <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">your security processes and applications</a> running on top of Google’s infrastructure also need to comply.</p>
<p>You’ll notice that Network is listed for both Google and the customer. That’s because Google takes care of some parts of networking and the customer takes care of the rest. One of the most interesting areas of shared responsibility for network security is protecting against distributed denial of service (or DDoS) attacks.</p>
<p>Google provides many features to help deal with DDoS attacks, but it’s up to the customer to use them properly. Here are some of the techniques.</p>
<p>Reduce the attack surface by</p>
<ul>
<li>Isolating and securing your deployment with firewall rules</li>
<li>Google also provides anti-spoofing protection by default</li>
</ul>
<p>Isolate your internal traffic from the external world by</p>
<ul>
<li>Deploying instances without public IPs unless necessary</li>
</ul>
<p>Use Load Balancing</p>
<ul>
<li>Because a load balancer acts as a proxy that hides your internal instances</li>
</ul>
<p>Use Cloud Armor</p>
<ul>
<li>This service is specifically designed to provide DDoS defense</li>
<li>And it works with Load Balancing</li>
<li>It protects against layer 3 and layer 4 DDoS attacks</li>
</ul>
<p>Google Cloud also enforces API rate limits and resource quotas to prevent a spike in one customer’s activity from affecting other Cloud Platform customers.</p>
<p>Now it’s time to get back to the PCI DSS standard and how to comply with it. If your organization accepts credit card payments, then you need to comply with this standard or you could be fined. More importantly, if you have security flaws that allow hackers to steal credit card information from your systems, then it would be very damaging to both your customers and your reputation.</p>
<p>In the case study, GreatInside provides an interface for collecting credit card information, but it passes the validation and processing of the information to a Certified Payment Processor. This makes the company an SAQ A-EP merchant in PCI lingo. I’ll go over Google’s recommendations for how this type of merchant could comply with PCI DSS.</p>
<p>First, you have to check that the other parties involved (that is, Google Cloud and the payment processor) are certified for your volume of transactions (since there are different PCI DSS merchant levels based on the number of transactions). Google Cloud Platform has the highest level of PCI DSS certification, so that’s not a concern, but you’ll have to check your payment processor’s certification level because your volume might exceed their certification level.</p>
<p>Here’s a suggested architecture to handle the company’s credit card processing. Here’s how it works. A customer enters their credit card information in a form on your website. Then your payment-processing application sends the information to the external payment processor. Now the payment processor tells your application whether the card was accepted or declined. After that your payment processing application sends some or all of the response data to your core application, so it knows how to proceed with this customer.</p>
<p>You also need to log and monitor all of these interactions. Every instance involved in payment processing sends its logs to Stackdriver Logging and its alerts to Stackdriver Monitoring.</p>
<p>Now let’s move on to how you would set this up. To reduce the number of systems that need to be PCI-compliant, you have to fully isolate your payment-processing environment from the rest of your production environment. The best way to do this is to use a separate Google Cloud account, rather than just a separate project within your main account.</p>
<p>Then use IAM to grant access only to people who absolutely need to work on the payment-processing environment, such as people who will be deploying new versions of the application or managing the systems. These people must also pass a background check first.</p>
<p>To create the instances, you should first create your own Linux image that’s based on one of the preconfigured boot disk images and that contains the bare minimum of additional software needed to run your application. Then use this custom image when creating all of your VM instances.</p>
<p>To secure the network, create firewall rules that only allow three types of inbound traffic:</p>
<ul>
<li>HTTPS traffic from the load balancer to the payment form servers, so that customers can reach your payments page</li>
<li>Credit card authorization responses from the external payment processor to your internal payment authorization servers, and</li>
<li>VPN traffic from your internal office network to the VPN Gateway, so your authorized people can manage and audit the application and systems.</li>
</ul>
<p>Then create firewall rules for outbound traffic. There’s only one type of outbound traffic you need to allow – HTTPS traffic from the payment form servers to the external payment processor, so they can send credit card authorization requests.</p>
<p>Now all of the traffic in and out of the network is locked down, but you’ll also have to open up internal traffic, such as:</p>
<ul>
<li>From all of the instances to Google’s NTP servers for time synchronization, and</li>
<li>SSH traffic from the VPN Gateway to all of the instances, so authorized people can access the systems for maintenance</li>
</ul>
<p>OK, let’s move on to deploying your application. To be compliant, you have to make sure you’re deploying the correct application every time, that it’s deployed securely, and that no other software packages are installed during the deployment. If you don’t already have an automated deployment tool, then you might want to use Cloud Deployment Manager, which could automate the creation of everything in your payment-processing environment, even the firewall rules. It could also help you create an audit trail of deployments.</p>
<p>Since you’ve used the same custom Linux image for all of your instances, you’ll need to install additional software on each instance. For example, some instances may need a web server, while others don’t, and each instance should only have the software it needs, which will reduce your security risks. To make this process consistent and reliable, it should be automated as well. The easiest way to automate software installation and configuration is to use a configuration management tool such as Chef, Puppet, or Ansible. Cloud Academy has courses on all three of these tools, so check one out if you’re not familiar with how to use any of them. </p>
<p>There are a few packages that you’ll want to install on all instances. First there’s iptables. You can set it up to log all network activity to and from each instance. This data is required for PCI DSS compliance audits.</p>
<p>Second, each instance needs the Stackdriver Monitoring and Logging agents so it can send logs and alerts.</p>
<p>Third, each instance should run an Intrusion Detection System (or IDS) to alert you to suspicious activity.</p>
<p>Finally, your configuration management tool needs to securely retrieve and launch the latest version of your application. </p>
<p>Even with an automated deployment, you’d still need to verify the integrity of the software being deployed. You could do this by running an automated checksum comparison against each package as it’s installed. You could also run an automated code analysis tool to check for security flaws.</p>
<p>Now let’s move on to logging. To be compliant, every step in the payment-processing environment has to be monitored and recorded. All instance activity and all user activity must be logged. Stackdriver Logging is a great service for collecting logs. You can record network traffic to and from your instances by enabling VPC Flow Logs on each subnet in your VPC.</p>
<p>By the way, you might think that we need to assign a service account to the instances so they can write logs to Stackdriver, but the default service account for VM instances already grants write access to Stackdriver, so you don’t need to configure that yourself.</p>
<p>I mentioned that user activity needs to be logged, but you also need to log the activity of people who have administrative access to the environment. The easiest way is to log all shell commands.</p>
<p>The amount of log information generated by all of this is likely to be very large, so you might want to export your Stackdriver logs to BigQuery if you need to do some complex analysis.</p>
<p>In addition to logging, you also need to set up real-time monitoring alerts, such as when your IDS detects any intrusion attempts.</p>
<p>After your environment is implemented, but before any production traffic flows through it, you have to validate the environment, either by contracting a Qualified Security Assessor if you’re a Level 1 merchant or by filling out the Self Assessment Questionnaire if you’re not a Level 1 Merchant.</p>
<p>Wow, that was a lot of work, wasn’t it? Well, if you’re going to be handling credit card information, you’ll be happy when your rigorous security design prevents damaging incidents.</p>
<h1 id="Disaster-Recovery"><a href="#Disaster-Recovery" class="headerlink" title="Disaster Recovery"></a>Disaster Recovery</h1><p>In an earlier lesson, we covered how to design a highly available architecture that will keep running even if an instance fails, by using load balancers, instance groups, and redundant databases. However, there are more catastrophic events that might occur. I’m not talking about an entire city getting destroyed or anything like that (although it would be good to have an architecture that could handle that). But much smaller incidents can be disastrous too. For example, one of your databases could become corrupt. This is actually worse than the database server going down because it may take a while before you realize there’s a problem, and in the meantime, the corruption problem could get worse.</p>
<p>To recover from this sort of disaster, you need backups along with transactional log files from the corrupted database. That way you can roll back to a known-good state. Each type of database has its own method for doing this.</p>
<p>If you’re using Cloud SQL to run a MySQL database (which we are for the interior design application), then you should enable automated backups and point-in-time recovery. Then if your database becomes corrupt, you can restore it from a backup or use point-in-time recovery to bring the database back to a specific point in time. </p>
<p>If you’re using Cloud SQL for SQL Server, then you should enable automated backups. At this time, Cloud SQL does not support point-in-time recovery for SQL Server, so you can only restore a database to the point when a specific backup was taken. For both types of databases, Cloud SQL retains up to 7 automated backups for each instance.</p>
<p>If you’re hosting a database on Compute Engine instances directly, then you’ll have to configure backups and transaction logging yourself. For example, suppose that instead of using Cloud SQL for SQL Server, we ran SQL Server on a Compute Engine instance. Then we’d need to set up our own disaster recovery solution for it. Luckily, Google has a very detailed white paper on this topic. I’ll give you the highlights.</p>
<p>First, set up an automated task that copies the SQL Server database backups to Google Cloud Storage. This is where we would finally need a service account because instances can’t write to Cloud Storage by default. The SQL Server instances need to have a service account with the Storage Object Creator role. Another way to do it would be to set a Cloud Storage access scope for the instance, but service accounts are more flexible.</p>
<p>Once the database is being backed up, then if disaster strikes, you would spin up a new SQL Server instance. Either use one of Google’s preconfigured SQL Server images or your own custom disk image. It doesn’t mention this in the whitepaper, but it’s the sensible thing to do and I’ll talk about it more in a minute. Next, you can use an open-source script to restore the database and re-execute the events in the log files up to the point in time desired.</p>
<p>When you’re designing a disaster recovery solution, you need to consider RPO and RTO. RPO stands for Recovery Point Objective. This is the maximum length of time when data can be lost. It affects your backup and recovery strategy because, for example, if it’s acceptable to lose an entire day’s worth of work, then you can just recover using the previous night’s backups. If you have a short RPO, which is usually the case, then you need to make sure you are constantly backing up your data, and when recovering from database corruption, you have to carefully consider which point in time to recover to.</p>
<p>RTO stands for Recovery Time Objective. This is the maximum length of time that your application can be offline and still meet the service levels your customers expect (usually in a service level agreement).</p>
<p>In the SQL Server example, I suggested using either one of Google’s preconfigured SQL Server images or your own custom disk image that has SQL Server installed and configured. The advantage of having a custom disk image is that it helps you meet your recovery time objective because it reduces the amount of time it takes to get a new SQL Server instance running. If you have to configure SQL Server manually, that could significantly impact how long it takes to recover from a disaster.</p>
<p>As with everything, though, there are tradeoffs. If your SQL Server implementation is customized, then you’ll have to weigh the benefits of fast recovery time against the maintenance effort required to keep your custom image up-to-date . If you have a very short RTO, then you may have no choice but to maintain a custom disk image. You might be able to ease the maintenance required, though, by using a startup script to perform some of the customization. Since the startup script resides on either the metadata server or Cloud Storage, you can change it without having to create a new disk image.</p>
<p>In some cases, you may want to run an application from your own data center or from another cloud platform and use Google Cloud as a disaster recovery solution. There are many ways you could do this, but I’ll go over a couple of common designs.</p>
<p>The first way is to continuously replicate your database to an instance on Google Cloud. Then you would set up a monitoring service that would watch for failures. In the event of a disaster, the monitoring service would trigger a spin-up of an instance group and load balancer for the web tier of the application. The only part you would need to do manually is to change the DNS record to point to the load balancer’s IP address. You could use Cloud DNS or another DNS service for this.</p>
<p>This is already a low-cost solution because the only Google Cloud resource that needs to run all the time is the database instance. But you can reduce the costs even further by running the database on the smallest machine type capable of running the database service. Then if there’s a disaster, you would delete the instance, but with the option to keep the persistent disk, and spin up a bigger instance with the saved disk attached. Of course, this solution would require more manual intervention and would lengthen your downtime, so you wouldn’t want to do this if you have a short RTO.</p>
<p>If you want to reduce your downtime as much as possible, or even keep running in the event of hardware failures, you could serve your application from both your on-premises environment and your Google Cloud environment at all times. That way if you have an on-premise failure, the Google Cloud environment would already be running and serving customers. It would just need to scale up to handle the extra load, which would be automatic if you use an autoscaling instance group.</p>
<p>To make this hybrid solution work, you would need to use a DNS service that supports weighted routing, so it could split incoming traffic between the two environments. In the event of a failure, you would need to disable DNS routing to the failed environment.</p>
<p>And that’s it for disaster recovery.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>I hope you enjoyed learning how to <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/course-introduction-8/">design a Google Cloud infrastructure</a>. Now you know how to map <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/compute-1/">compute</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/storage-2/">storage</a>, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/networks-1/">network</a> requirements into Google Cloud, and secure your infrastructure with <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/authentication-2/">authentication</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/roles-1/">roles</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/service-accounts-1/">service accounts</a>, ACLs, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/data-protection-and-encryption-1/">encryption</a>. You also know some of the ways to design <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/high-availability-1/">high availability</a>, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/designing-a-google-cloud-infrastructure/disaster-recovery-1/">disaster recovery</a>, and PCIDSS compliance into your solution.</p>
<p>To learn more about Google Cloud platform, you can read Google’s online documentation. You can also try one of the other Google courses on Cloud Academy, and watch for new ones, because we’re always developing new courses.</p>
<p>If you have any questions or comments, please let me know on the Cloud Academy community forums. Thanks and keep on learning.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Scaling-an-Application-Through-a-Google-Cloud-Managed-Instance-Group-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Scaling-an-Application-Through-a-Google-Cloud-Managed-Instance-Group-10/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Scaling-an-Application-Through-a-Google-Cloud-Managed-Instance-Group-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:07" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:07-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:39:32" itemprop="dateModified" datetime="2022-11-20T19:39:32-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Scaling-an-Application-Through-a-Google-Cloud-Managed-Instance-Group-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Scaling-an-Application-Through-a-Google-Cloud-Managed-Instance-Group-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/19/GCP-Professional-Architect-Create-a-Network-Infrastructure-with-Google-Virtual-Private-Cloud-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/19/GCP-Professional-Architect-Create-a-Network-Infrastructure-with-Google-Virtual-Private-Cloud-9/" class="post-title-link" itemprop="url">GCP-Professional-Architect-Create-a-Network-Infrastructure-with-Google-Virtual-Private-Cloud-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-19 00:14:05" itemprop="dateCreated datePublished" datetime="2022-11-19T00:14:05-04:00">2022-11-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-20 19:39:54" itemprop="dateModified" datetime="2022-11-20T19:39:54-04:00">2022-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GCP-Professional-Architect/" itemprop="url" rel="index"><span itemprop="name">GCP-Professional-Architect</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/19/GCP-Professional-Architect-Create-a-Network-Infrastructure-with-Google-Virtual-Private-Cloud-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/19/GCP-Professional-Architect-Create-a-Network-Infrastructure-with-Google-Virtual-Private-Cloud-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/23/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><span class="page-number current">24</span><a class="page-number" href="/page/25/">25</a><span class="space">&hellip;</span><a class="page-number" href="/page/266/">266</a><a class="extend next" rel="next" href="/page/25/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2653</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
