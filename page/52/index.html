<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hang&#39;s Blog">
<meta property="og:url" content="https://example.com/page/52/index.html">
<meta property="og:site_name" content="Hang&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Hang Zhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://example.com/page/52/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hang's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hang's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-the-Elastic-File-System-13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-the-Elastic-File-System-13/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Introduction-to-the-Elastic-File-System-13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:18" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:18-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:12:46" itemprop="dateModified" datetime="2022-11-27T20:12:46-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-the-Elastic-File-System-13/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-the-Elastic-File-System-13/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Configuring-a-Static-Website-With-S3-And-CloudFront-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Configuring-a-Static-Website-With-S3-And-CloudFront-12/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Configuring-a-Static-Website-With-S3-And-CloudFront-12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:16" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:16-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:10:56" itemprop="dateModified" datetime="2022-11-27T20:10:56-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Configuring-a-Static-Website-With-S3-And-CloudFront-12/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Configuring-a-Static-Website-With-S3-And-CloudFront-12/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Handling-S3-Objects-Events-With-Lifecycle-Policies-and-Server-Access-Logging-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Handling-S3-Objects-Events-With-Lifecycle-Policies-and-Server-Access-Logging-11/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Handling-S3-Objects-Events-With-Lifecycle-Policies-and-Server-Access-Logging-11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:15" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:15-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:09:32" itemprop="dateModified" datetime="2022-11-27T20:09:32-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Handling-S3-Objects-Events-With-Lifecycle-Policies-and-Server-Access-Logging-11/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Handling-S3-Objects-Events-With-Lifecycle-Policies-and-Server-Access-Logging-11/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Using-S3-Bucket-Policies-and-Conditions-to-Restrict-Specific-Permissions-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Using-S3-Bucket-Policies-and-Conditions-to-Restrict-Specific-Permissions-10/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Using-S3-Bucket-Policies-and-Conditions-to-Restrict-Specific-Permissions-10</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:13" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:13-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:09:38" itemprop="dateModified" datetime="2022-11-27T20:09:38-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Using-S3-Bucket-Policies-and-Conditions-to-Restrict-Specific-Permissions-10/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Using-S3-Bucket-Policies-and-Conditions-to-Restrict-Specific-Permissions-10/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Storage-SAA-C03-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Storage-SAA-C03-9/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Storage-SAA-C03-9</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:12" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:12-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 19:58:50" itemprop="dateModified" datetime="2022-11-27T19:58:50-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Storage-SAA-C03-9/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Storage-SAA-C03-9/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Storage-SAA-C03-Introduction"><a href="#Storage-SAA-C03-Introduction" class="headerlink" title="Storage (SAA-C03) Introduction"></a>Storage (SAA-C03) Introduction</h1><p>Hello, and welcome to this course on storage in AWS, where we’re here to help you on your journey to prepare for the AWS Certified Solutions Architect - Associate certification. Before we get started, I’d like to introduce myself. My name is Danny Jessee, and I am one of the trainers here at Cloud Academy, specializing in AWS – Amazon Web Services – and AWS certifications.</p>
<p>In this course, the AWS team will be presenting a series of lectures that introduce the various storage services currently available in AWS that may be covered on the exam. Feel free to contact me with any questions using the details shown on the screen, or you can always get in touch with us here at Cloud Academy by sending an email to <a href="mailto:&#x73;&#117;&#112;&#x70;&#111;&#x72;&#116;&#64;&#99;&#108;&#x6f;&#117;&#100;&#97;&#99;&#97;&#100;&#101;&#109;&#x79;&#x2e;&#99;&#111;&#109;">&#x73;&#117;&#112;&#x70;&#111;&#x72;&#116;&#64;&#99;&#108;&#x6f;&#117;&#100;&#97;&#99;&#97;&#100;&#101;&#109;&#x79;&#x2e;&#99;&#111;&#109;</a>, where one of our Cloud experts will reply to your question.</p>
<p>This course has been specifically curated to help you pass the AWS Certified Solutions Architect - Associate exam and is ideal for anyone who is looking to learn more about the various storage services in AWS in preparation for the exam. The objective of this course is to provide an introduction to storage services in AWS for solution architects, including:</p>
<ul>
<li>The Amazon Simple Storage Service, known as S3;</li>
<li>Amazon Elastic File System, or EFS; </li>
<li>Amazon FSx; and</li>
<li>Amazon Elastic Block Store, or EBS.</li>
</ul>
<p>We’ll also discuss hybrid cloud storage services and on-premises data backup solutions using AWS Storage Gateway, as well as AWS services that can assist with large-scale data storage, migration, and transfer both into and out of AWS using the AWS Snow family and AWS DataSync.</p>
<p>Together with the other courses in this learning path, we’ll cover all of the key tools, technologies, and concepts from the AWS Certified Solutions Architect - Associate exam guide and ensure that you are fully prepared to sit this exam.</p>
<p>The AWS Certified Solutions Architect - Associate certification has been designed for anyone who has experience designing cloud solutions that use AWS services to meet current and future business requirements, as well as architectures that are secure, resilient, high-performing, and cost-optimized in accordance with the AWS Well-Architected Framework. All of the AWS Cloud concepts introduced in this course will be explained and reinforced from the ground up.</p>
<p>Here at Cloud Academy, we strive to keep our content current to provide the best training available. If you have any feedback, positive or negative, or if you notice anything that needs to be updated or corrected for the next release cycle, please reach out to us at <a href="mailto:&#x73;&#117;&#112;&#112;&#x6f;&#114;&#116;&#x40;&#x63;&#108;&#x6f;&#x75;&#100;&#97;&#99;&#x61;&#100;&#101;&#x6d;&#121;&#x2e;&#x63;&#x6f;&#109;">&#x73;&#117;&#112;&#112;&#x6f;&#114;&#116;&#x40;&#x63;&#108;&#x6f;&#x75;&#100;&#97;&#99;&#x61;&#100;&#101;&#x6d;&#121;&#x2e;&#x63;&#x6f;&#109;</a>. Thank you!</p>
<h1 id="AWS-Storage-Services"><a href="#AWS-Storage-Services" class="headerlink" title="AWS Storage Services"></a>AWS Storage Services</h1><p>Hello, and welcome to this very quick lecture where I want to give a brief introduction as to why there are so many different AWS storage services to choose from. As we know, more and more organizations are moving and migrating to the cloud, for the many benefits the cloud brings, such as flexibility, scalability, cost efficiencies, security, and more. AWS offers many different services that allows for almost any migration of a solution or new solution to exist, and take advantage of these benefits. This means that from a foundational and infrastructure as a service perspective, AWS has to provide services, components, and features that provide these core infrastructure elements, covering compute, storage, database, and network, and AWS does this very well. This course is going to focus on the storage element of these components. </p>
<p>So, why does AWS provide so many different storage services, if all you need to do is store your data in the cloud? Well, it’s effectively the same reasons why you have range of storage products and solutions in your own on-premise environment. For example, you are likely using different storage devices, such as a storage area network, known as a SAN, network attached storage, known as a NAS, directly attached storage, and also taped backup, to name but a few. Now, for this course, it’s not important to understand in detail what each of these solutions are and do, however, the point I’m trying to make here is that they all perform the same function, the ability to store data. But at the same time, each solution also provides different benefits and features, such as cost variants, storage capacity, security features, such as encryption and access control, varied levels of durability and availability, different read&#x2F;write speeds, different accessibility options, different media types, some can be auditable and traceable, and also use case, such as backup and file storage. </p>
<p>AWS is fully aware that not all of your data is to be treated exactly the same and that sometimes, data can require very specific requirements. This is the reason why AWS has so many different storage services available, to allow you to select the most appropriate service for your needs. Understanding which AWS storage can provide these features and more is critical to being able to select the most appropriate service, allowing you to implement an effective and efficient solution. Data storage can be categorized between block, file, and object storage. So, what’s the difference between these and AWS? </p>
<p>Block storage. Block storage stores the data in chunks of data known as blocks, and these blocks are stored in a volume, and attached to a single instance. They generally provide very low latency, and can be considered similar to your directly attached disks within your own data center. </p>
<p>File storage. Your data is stored as separate files within a series of directories, forming a data structure hierarchy. The data is then stored on top of a file system, and provides shared access, allowing for multiple users to access the data. File storage in AWS can be associated to your network attached storage systems you may have in your own data center. </p>
<p>Object Storage. Each object does not conform to a data structure hierarchy. Instead, it exists across a flat address space, and is referenced by a unique key. Each object can also have associated metadata to help categorize and identify the object. Now that you have an understanding of why AWS has curated and developed a range of storage services for you to select, let me now start by introducing each of these services to provide information on exactly what the service is and does, and highlighting its key features, and when and why you might select the service.</p>
<h1 id="Overview-of-Amazon-S3"><a href="#Overview-of-Amazon-S3" class="headerlink" title="Overview of Amazon S3"></a>Overview of Amazon S3</h1><p>Hello and welcome to this lecture where I will introduce the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon Simple Storage Service</a>, commonly known as S3. Amazon S3 is probably the most heavily used storage service that is provided by AWS simply down to the fact that it can be a great fit for many different use cases, as well as integrating with many different <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. Amazon S3 is a fully managed, object-based storage service that is highly available, highly durable, very cost-effective, and widely accessible.</p>
<p>The service itself is promoted as having unlimited storage capabilities making Amazon S3 extremely scalable, far more scalable than your own on-premise storage solution could ever be. There are, however, limitations on the individual size of a single file that it can support. The smallest file size that it supports is zero bytes and the largest file size is five terabytes. Although there is this size limitation on a maximum file size, it’s one that many of us will not perceive as an ongoing inhibitor in the majority of use cases.</p>
<p>The service operates an object storage service which means each object uploaded does not conform to a data structure hierarchy like a file system would, instead its architecture exists across a flat address space and is referenced by a unique URL. Now, if you compare this to file storage, where your data is stored as separate files within a series of directories forming a data structure hierarchy much like your own files are on your own laptop or computer then S3 is very different in comparison. S3 is a regional service and so when uploading data you as the customer are required to specify the regional location for that data to be placed in.</p>
<p>By specifying your region for your data Amazon S3 will then store and duplicate your uploaded data multiple times across multiple availability zones within that region to increase both its durability and availability. For more information on regional availability zones and other AWS global infrastructure components, please see the following blog <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/aws-global-infrastructure/">here</a>. Objects stored in S3 have a durability of ninety-nine point nine nine nine nine nine nine nine nine nine percent, known as eleven nines of durability and so the likelihood of losing data is extremely rare and this is down to the fact that S3 stores multiple copies of the same data in different availability zones. The availability of S3 data objects is dependent on the storage class used and this can range from 99.5% to 99.99%.</p>
<p>The difference between availability and durability is this: when looking at availability AWS ensures that the uptime of Amazon S3 is between 99.5% to 99.99%, depending on the storage class, to enable you to access your stored data. The durability percentage refers to the probability of maintaining your data without it being lost through corruption, degradation of data, or other unknown potential damaging effects. When uploading objects to Amazon S3, a specific structure is used to locate your data in the flat address space.</p>
<p>To store objects in S3, you first need to define and create a bucket. You can think of a bucket as a container for your data. This bucket name must be completely unique, not just within the region you specify, but globally against all other S3 buckets that exist, of which there are many millions. And this is because of the flat address space, you simply can’t have a duplicate name. Once you have created your bucket you can then begin to upload your data within it. By default your account can have up to a hundred buckets, but this is a soft limit and a request to increase this can be made with AWS. Any object uploaded to your buckets are given a unique object key to identify it.</p>
<p>In addition to your bucket, you can if required create folders within the bucket to aid with categorization of your objects for easier data management. Although folders can provide additional management from a data organization point of view, I want to reiterate that Amazon S3 is not a file system and many features of Amazon S3 work at the bucket level and not a specific folder level and so the unique object key for every object contains the bucket, any folders that are present, and also the name of the file itself. Let me now provide a quick overview via a demonstration of the Amazon S3 console and I’ll show you how to create a bucket within the service and upload an object to that bucket and then show you the unique object key of that object. Okay so I’m currently logged into my AWS management console and I can find amazon S3 under the storage category which is down here. So if I select S3 and this has taken me to the S3 dashboard.</p>
<p>Now up here we have our buckets and this list that we have here are a list of buckets that I have already created in my account so this is the bucket name which is the unique bucket identifier. And over here we have the region in which that bucket exists in, so we have some in London some in Ireland some over in US as well, and also, the date created. Over here we have access whether these buckets can be accessed by the public or not. I’m not going to dive too deep into access and security the buckets at this stage, as this is just more of an introduction to give you an overview of the console, but we do have other courses that focus on security.</p>
<p>Now, if I go into one of these buckets, for example, this one here cloudacademyaudio, we can see that I have created a folder in here called Stuart. There’s no other objects, it’s just a folder we can see that by this little icon here, so there’s no actual objects in here, this is just a folder that I created just to help me manage and categorize any objects that I do upload. If I select that folder, I can see I have two more folders here. If I go into this one, for example, I can see that I have an object I have a PNG file. So this is an object that I have uploaded to S3 and we can see that it’s in the courses folder under Stuart under the cloudacademyaudio bucket.</p>
<p>Now, if I select this object, I can get some information about it. I can open it and download it etc. We can see the last time it was modified, the storage class that it belongs to, and I’ll be talking more about <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/storage-classes/">storage classes</a> in the next lecture, if there’s any encryption at rest activated, the file size, and here is the unique identifier of the key for this object. So we can see that the key comprises of any folders within the bucket and then also the object name at the end. And this here is the unique identifier of this object on S3, so it gives it a URL.</p>
<p>Now if I want to open this I can simply click on open and as we can see, it’s just an image file, or I can download the object if I want to. So I just wanted to show you there how you can use folders within your bucket and also what the object key looks like as well. So now if I go back to the console, the main dashboard where all my buckets are, I want to show you how to create a bucket quickly. It’s very simple simply click on create bucket, then we need to give it a unique bucket name.</p>
<p>Now remember, this has to be a globally unique name, so if I type in stubucketdemo and then I can select a region that I want this bucket to be in, I’m just going to select the London region, and if I want to, I can copy settings from an existing bucket, but it’s going to go through the different screens to show you the options quickly that you can have when you’re creating a bucket. Click on Next. Here’s some management, we have some management options such as versioning and server access logging. Versioning keeps all versions of an object in the same bucket and server access logging logs requests for access to your bucket. You can also use key value pair tags, you can activate object level logging which will record any API activity with CloudTrail associated with your objects and you can also encrypt your objects as well. I’m just gonna leave all those options as default for this demonstration. Click on next.</p>
<p>Here we can set different permissions, I’m just gonna leave the default block all public access so this will prevent anyone from outside of my VPC accessing any data within my bucket. Click on next. We just have a review of the settings that we selected and then all you need to do is click on create bucket. So if I scroll down to my bucket that I just created, which was stubucketdemo. If I select that, we can see here that I’ve got no folders and no objects. So if you want to create a folder, you simply click on create folder, just give it a name. If you want to add any encryption you can do so here. I’m just going to use none as a default. Now, I can either add an object directly under this bucket or I can add it into that folder. Let me just add it directly under the bucket name of stubucketdemo.</p>
<p>So to upload an object you simply click on upload, add files, select your object that you’d like to upload or objects, click on next, you know we have some permissions here as to who can read or write to the object, and as we can see here, we have the block public access setting turned on for this bucket. Click on next. Now here we have our storage classes and depending on what storage class we select it will affect the durability, the availability, and also the cost of your object being stored. Now, I’m gonna go deeper into the different storage classes in the next lecture so I won’t go over this too deep now. For the sake of this demonstration I’m just going to select the standard storage class.</p>
<p>Then we have a review page and then simply upload. And then we have my object that’s been uploaded to my bucket. Again, if I select it, I can see the object key and also the unique object URL as well. So that’s just a very quick demonstration to show you what the S3 console looks like, how to create buckets, how to create folders, and also upload objects as well, just so hopefully you can piece things together a little bit easier if you’ve not used Amazon S3 before.</p>
<h1 id="Storage-Classes"><a href="#Storage-Classes" class="headerlink" title="Storage Classes"></a>Storage Classes</h1><p>Hello and welcome to this lecture covering <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-s3/introduction/">Amazon S3</a> storage classes. As we just saw in the demonstration, I had an option to select which storage class I wanted my uploaded object to reside in. Amazon S3 offers these different storage classes to allow you to select a class based on performance features and costs and it’s down to you to select the storage class that you require for the data. The storage classes available are as follows S3 Standard, S3 Intelligent Tiering, S3 Standard Infrequent Access, S3 One Zone Infrequent Access, S3 Glacier, and S3 Glacier Deep Archive.</p>
<p>S3 Standard. This storage class is considered a general-purpose storage class. It is ideal for a range of use cases where you need high throughput with low latency with the added ability of being able to access your data frequently. By copying data to multiple availability zones, S3 Standard offers eleven nines of durability across multiple availability zones, meaning the OData remains protected against a single availability zone failure. It also offers a 99.99% availability across the year, which is the highest availability that S3 offers. From a security standpoint this storage class also has the added support of SSL, Secure Sockets Layer, for encrypting data in transit in addition to encryption options for when the data is at rest. With management features such as lifecycle rules, objects in S3 Standard can automatically be moved to another storage class. For those unfamiliar with life cycle rules, they provide an automatic method of managing the life of your data while it is being stored on Amazon S3. By adding a life cycle wall to a bucket you are able to configure and set specific criteria that can automatically move your data from one class to another or delete it from Amazon S3 altogether. You may want to do this as a cost saving exercise by moving data to a cheaper storage class after a set period of time.</p>
<p>S3 Intelligent Tiering. This storage class is ideal for those circumstances where the frequency of access to the object is unknown. Effectively, we have unpredictable data access patterns and so by using this storage class, it can help to optimize your storage costs. Depending on your data access patterns of objects in the Intelligent Tiering Class, S3 will move your objects between two different tiers, these being frequent and infrequent access. Now, these classes are a part of the Intelligent Tiering Class itself and are separate from the existing storage classes I listed earlier. When the objects are moved to Intelligent Tiering, they are placed within the frequent access tier, which is the more expensive of the two tiers. If an object is not accessed for 30 days then AWS will automatically move the object to the cheaper tier known as the infrequent access tier. Once that same object is accessed again, it will automatically be moved back to the frequent tier. Much like S3 Standard, S3 Intelligent Tiering also offers 11 nines of durability across multiple availability zones offering protection against the loss of a single AZ. However, its availability isn’t quite as high as S3 Standard as it set at 99.9%. This storage class also has the added support of SSL for encrypting data in transit in addition to encryption options for when the data is at rest. S3 Intelligent Tiering also supports the lifecycle rules and matches the same performance throughput and low latency as S3 Standard.</p>
<p>S3 Standard infrequent access. This can be seen as the equivalent to the infrequent tier from the Intelligent Tiering class as it is designed for data that does not need to be accessed as frequently as data within the Standard tier, and yet still offers high throughput and low latency access, much like S3 Standard does. As with all other S3 storage classes, it carries that 11 9s durability across multiple AZs, again by copying your objects to multiple availability zones within a single region to protect against AZ outages. It shares the same availability as Intelligent Tiering of 99.9 percent. As a result, this storage class comes at a cheaper cost than S3 Standard. Common security features such as SSL for encryption in transit and data at rest encryption is supported as well as management controls such as lifecycle rules to automatically move objects to an alternate storage class based on your requirements.</p>
<p>S3 One Zone Infrequent Access. By now you can probably assume what this storage class comprises of based off of the previous classes that I’ve already discussed. However, again, being an infrequent storage class it is designed for objects that are unlikely to be accessed frequently. It also carries the same throughput and low latency. However, the durability, although remaining at eleven nines only exists across a single availability zone. As the name implies to this class it is one zone, as in one availability zone. So the objects will be copied multiple times to different storage locations within the same availability zone instead of across multiple availability zones. This results in a 20% storage cost reduction when compared to S3 Standard. One Zone IA does, however, offer the lowest level of availability which is currently 99.5 percent and this is down to the fact that your data is being stored in a single availability zone. Should the AZ storing your data become unavailable then you will lose access to your data or even worse it may become completely lost should the AZ be destroyed in a catastrophic event. Again, life cycle rules and encryption mechanisms are in place to protect your data both in transit and at rest.</p>
<p>S3 Glacier. The next two storage classes are associated with S3 Glacier which is used for archival data. Firstly let me explain more about S3 Glacier, as it can be accessed separately from the Amazon S3 service but closely interacts with it S3 Glacier storage classes directly interact with the Amazon S3 lifecycle rules discussed previously. However, the fundamental difference with the Amazon Glacier storage classes come at a fraction of the cost when it comes to storing the same amount of data than the S3 storage classes. So what’s the catch? Well, it doesn’t provide you the same features as Amazon S3 but more importantly, it doesn’t provide you instant access to your data.</p>
<p>So what do Amazon Glacier classes offer exactly? Well, they offer an extremely low-cost long term durable storage solution which is often referred to as cold storage, ideally suited for long term backup and archival requirements. It’s capable of storing the same data types as Amazon S3, effectively any object, however, like I just mentioned it doesn’t provide instant access to your data. In addition to this, there are other fundamental differences which makes this service fit for purpose for other use cases. The service itself has 11 nines of durability making this just as durable as Amazon S3. Again this is achieved by replicating the data across multiple different availability zones within a single region but it provides the storage at a considerably lower cost compared to that of Amazon S3. And this is because retrieval of data stored in Glacier is not an instant access retrieval process. When retrieving your data it can take up to several hours to gain access to it depending on certain criteria. The data structure within Glacier is centered around vaults and Archives. Buckets and folders are not used. They are purely used for S3.</p>
<p>A Glacier vault simply acts as a container for Glacier archives. These vaults are regional and as such during the creation of these vaults, you are asked to supply the region in which they will reside. Within these vaults, we then have our data which is stored as an archive and these archives can be any object similar to S3. Thankfully you can have unlimited archives within your Glacier vaults, so from a capacity perspective, it follows the same rule as S3. Effectively you have access to an unlimited quantity of storage for your archives and vaults. Now whereas Amazon S3 provided a nice graphical user interface to view, manage, and retrieve your data within buckets and folders, Amazon Glacier does not offer this service.</p>
<p>The Glacier dashboard within AWS management console allows you to create your vaults, set data retrieval policies, and event notifications. When it comes to moving data into S3 Glacier for the first time it’s effectively a two-step process. Firstly, you need to create your vaults as your container for your archives and this could be completed using the Glacier console. Secondly, you need to move your data into the Glacier vault using the available API or SDKs. As you may be thinking, there’s also another method of moving your data into Glacier and this is by using the S3 lifecycle rules that I discussed earlier. When it comes to retrieving your archives, which is your data, you will again have to use some form of code to do so, either the APIs, SDKs or the AWS CLI. Either way, you must first create an archival retrieval job, then request access to all or part of that archive.</p>
<p>Now you have more of an understanding of S3 Glacier, let me review the two S3 Glacier storage classes. Firstly, S3 Glacier. This is the default Standard storage class within S3 Glacier offering a highly secure using in transit and at rest encryption low-cost and durable storage solution. The durability matches that of other S3 storage classes, being 11 9s across multiple availability zones, and the availability of S3 Glacier is 99.9%. It’s simple to add data to this storage class using the S3 put APIs is in addition to S3 lifecycle rules. However, it does offer a variety of retrieval options depending on how urgently you need the data back, each offering a different price point. These being expedited, Standard, and bulk.</p>
<p>Expedited. This is used when you have an urgent requirement to retrieve your data but the request has to be less than 250 megabytes. The data is then made available to you in one to five minutes and this is the most expensive retrieval option of the three.</p>
<p>Standard. This can be used to retrieve any of your archives no matter their size but your data will be available in three to five hours, so much longer than the expedited option and this is the second most expensive of the three options.</p>
<p>And finally, bulk. This option is used to retrieve petabytes of data at a time, however, this typically takes between five and twelve hours to complete. This is the cheapest of the retrieval options so it really depends on how much data and how quickly you need it, as the retrieval speed and cost to be made by your retrieval option.</p>
<p>S3 Glacier Deep Archive. Out of all the storage classes offered by S3, Glacier Deep Archive is the cheapest and again being a Glacier class, it focuses on long-term storage. This is an ideal storage class for circumstances that require specific data retention regulations and compliance with minimal access, such as those within the financial or health sector where data records might need to be legally retained for seven years or even longer. The durability and availability matches that of S3 Glacier with eleven 9s durability across multiple AZss with 99.9% availability.</p>
<p>Adding data into deep archive follows the same processes as S3 Glacier, using S3 put APIs in addition to S3 lifecycle rules. Deep Archive, however, does not offer multiple retrieval options. Instead, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> states that the retrieval of the data will be within 12 hours or less. To summarize some of the common features between the storage classes, this table clearly shows how they differ. As you can see, the main difference of the classes is the durability and availability percentages, in addition to the pricing.</p>
<p>So when selecting your class for your data you really need to be asking yourself the following questions: how critical is my data? Does it require the highest level of durability? How reproducible is the data? Can it be easily created again if need be? and how often is the data likely to be accessed? For detailed information on Amazon S3 pricing covering all storage classes discussed please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/introduction/">Understanding and Optimizing Costs with AWS Storage Services</a>.</p>
<h1 id="Versioning"><a href="#Versioning" class="headerlink" title="Versioning"></a>Versioning</h1><p>Hello and welcome to this lecture covering the first element of the S3 Bucket Properties, this being versioning.</p>
<p>This is a bucket feature that allows for multiple versions of the same object to exist. This is useful to allow you to retrieve previous versions of a file, or recover a file should it subjected to accidental deletion, or intended malicious deletion of an object.</p>
<p>Versioning is managed automatically against objects when you overwrite or delete an object in a bucket that has versioning enabled. To ensure you are always working with the most current version of an object, only the latest version of the object is displayed within the console. However, you are able to review all versions of an object should you need to.</p>
<p>Versioning is not enabled by default, however, once you have enabled it, you can’t disable it, instead, you can only suspend it on the bucket which will prevent any further versions from being created of your objects, but it will keep all existing versions of objects up to the point of suspension. With this in mind, this means your buckets can be in the form of any one of these 3 states: Unversioned (this is the default state for buckets), Versioning-enabled, and versioning-suspended.</p>
<p>With versioning enabled you should also bear in mind that you will incur additional storage costs as you will be storing multiple versions of the same file, and one of the costing metrics of S3 is how much data storage you use.</p>
<p>So now we have a clearer understanding of what versioning is, let’s dive a little deeper into the process and its configuration.</p>
<p>To enable versioning is very easy and there a couple of ways of doing so. Firstly from within the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Management Console, you can either enable it during the creation of a bucket or enable on an existing bucket. </p>
<p>When creating a new bucket, you can enable it during the 2nd step, ‘configure options’ as shown. The first option at the top of the screen is ‘Versioning’. To enable it for your new bucket, simply select the checkbox. </p>
<p>To enable it on an existing bucket, select the bucket from within the S3 dashboard and select ‘Properties’. You will see a number of tiles relating to different properties o the bucket, the first one shown will be ‘Versioning’ and it will display as disabled. Select the tile and then select ‘Enable versioning’ and ‘Save.’ The tile will now show as ‘Enabled.’</p>
<p>Once you have successfully enabled versioning on your bucket a new parameter value is visible when you go into your buckets, this being ‘Versions’ and displays two options: ‘Show’ or ‘Hide.’ The default is ‘Hide’, however, if you select ‘Show’ you can see the ‘Version ID’ of the object.</p>
<p>For any new objects that are uploaded AFTER versioning has been enabled, the object will receive a new ‘Version ID’, such as the example shown. If you enabled versioning on an existing bucket with objects already in them, then their ‘Version ID’ will be displayed as ‘null’ until they have been modified or deleted. At which point they will receive a new Version ID.</p>
<p>Every time the same object is modified in some way it will receive a new Version ID, and the latest change will be the one object displayed within the bucket.</p>
<p>As you can see from the example here, I have versioning enabled on bucket ‘s3deepdive’ with an object entitled ‘s3-versioning.docx’. You can see that I have modified this file a number of times, and each time it has been saved to the same bucket, via a PUT API call, the object has received a new Version ID. The most current version is at the top and defined by the note in brackets ‘Latest version’, so the object is not overwritten, a new object is created with a new version ID. If I were to click on ‘Hide’ then only this version would be displayed.</p>
<p>From this view showing multiple versions I can open and load any of those previous versions if I needed to recover from an older file, or if I wanted to see what had changed. This view helps to demonstrate that using versioning can make your storage costs grow significantly if you have thousands of rapidly changing files.</p>
<p>Let me explain what happens when you delete objects in a version enabled bucket. Let’s assume we have our same s3-versioning.docx file and I chose to delete it from within the Console. Assuming I was ‘hiding’ the versions in the bucket, so I’m simply showing current versions only, the object would be deleted and not visible in the bucket. However, the object has not been deleted. If I clicked on ‘Show’ versions, then I would see the following.</p>
<p>Here we can see we have our existing 3 versions, all with the same Version IDs as expected, however, we have a 4th version at the top identified with a Delete Marker in brackets.</p>
<p>So what’s happened here? Well basically, this delete marker version becomes the current version of the object and ensures that any GET request to access the object will return a ‘404 not found value’. However, ALL other versions still exist, including the same version of the file that was submitted for deletion, so you can still view and open these files from the console when you have all versions shown, or by specifying the VersionID in a GET API call to call a specific version.</p>
<p>If you wanted to permanently delete an object in a versioned bucket then you must use the DELETE Object versionId in an AWS SDK, specifying the exact version that you want to delete. If you delete the version with the delete marker, then the object would reappear in your bucket using the latest version of the object available.</p>
<h1 id="Server-Access-Logging"><a href="#Server-Access-Logging" class="headerlink" title="Server-Access Logging"></a>Server-Access Logging</h1><p>Hello and welcome to this lecture where I am going to look at what Server-Access Logging is. In a nutshell, when server-access logging is enabled on a bucket it captures details of requests that are made to that bucket and its objects. Logging is important when it comes to security, root-cause cause analysis following incidents, and it can also be required to conform to specific audit and governance certifications.</p>
<p>Server-access logging, however, is not guaranteed and is conducted on a best-effort basis by S3. The logs themselves are collated and sent every few hours, or potentially sooner. There is no hard and fast rule that dictates that every request will be captured and that you will receive a log for a specific request within a set time frame.</p>
<p>Enabling logging Enabling access logging on your buckets is a very simple process using the S3 Management Console.</p>
<p>Enable server-access logging on an existing bucket Firstly select your bucket, and from the Properties tab you will see the Server-access logging tile. By default, this setting is disabled, as you can see. To enable it simply select the tile and you will be presented with the following screen. Select enable logging, and this gives you 2 options to complete the configuration. Firstly you need to select a target bucket. This target bucket will be used to store any logs created by enabling server access logging on your source bucket, which must be in the same region. For management and organization, you can additionally add a target prefix that s3 will add to the logs from your source bucket. When you have selected your Target bucket and added an optional prefix, select save.</p>
<p>Additionally, you can also enable logging on your bucket during its creation. Again, you must select a Target bucket and an optional prefix.</p>
<p>To allow S3 to write access logs to a target bucket, it will, of course, require specific permissions. These permissions will require write access for a group known as the Log Delivery group, which is a pre-defined Amazon S3 group used to deliver log files to your target buckets. If the configuration of your access logging is configured using the management console, then the enablement of logging automatically adds the Log Delivery group to the ACL (Access Control List) of the target bucket, allowing the relevant access. However, if you were to configure the access logging using the S3 API or AWS SDKs, then you would need to manually configure these permissions manually, more information on this process can be found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/enable-logging-programming.html">here</a>.</p>
<p>Following the example above, if I look at the Access Control List under the Permissions tab of the target bucket ‘s3deepdivelogging’ I can see that the Log Delivery Group has both Write and Read access to the bucket.</p>
<p>Before I continue, there are some points regarding the configuration of server-access logging that you need to be aware of. Firstly, and as I’ve already mentioned, both the source and target buckets should be in the same region, and it’s a best practice that different buckets are used for each. Also, the permissions of the S3 Access Log Group can only be assigned via Access Control Lists and not through bucket policies, so when manually setting the permissions for this via an SDK, you must update the ACL. Finally, if you have encryption enabled on your target bucket, access logs will only be delivered if this is set to SSE-S3 (Server-side encryption managed by S3) as encryption with KMS (Key Management Service) is not supported.</p>
<p>When the logs start arriving in the target bucket, the names will be presented following a standard naming pattern. In this example of S3 access logs, I entered a target prefix of ‘logs’ and so all of the logs will start with that prefix. Following any prefix that has been set, the naming convention is as follows: YYYY-mm-DD-HH-MM-SS-UniqueString&#x2F; This defines the year, followed by the month, followed by the day. Then the time in hours, minutes and seconds, and finally a unique string to avoid duplication of log names.</p>
<p>Let me now take a look at the contents of one of these log files to understand the information that they contain.</p>
<p>This is an example of a single entry in one of the logs, which is seperated by space-delimited fields.</p>
<p>Let me break this down into each section so you can see how it’s constructed and what each element respresents:</p>
<p>Bucket owner - Represents the canonical user ID of the owner of the Source bucket. The canonical user ID is used for cross-account access via bucket policies.</p>
<p>Bucket - This shows the name of the bucket related to the request.</p>
<p>Time - This is a timestamp of the request in UTC (Coordinated Universal Time).</p>
<p>Remote IP Address - Represents the internet address of the identity carrying out the request.</p>
<p>Requester - For authenticated users, this field will show the IAM identity. For any unauthenticated users a hyphen (-) would be displayed instead.</p>
<p>Request ID - A random string to identify each request.</p>
<p>Operation - This will display the operation of the request that was carried out.</p>
<p>Key - The “key” part of the request, URL encoded, or if no key parameter is used then a hyphen will be displayed as in this example. A hyphen in any field of the request indicates that the available data was not known or was not applicable for the request.</p>
<p>Request URI - This represents the Request-URI element of the HTTP request.</p>
<p>HTTP Status - This displays the HTTP status returned from the request as a numeric value.</p>
<p>Error Code - If an error was experienced, then S3 will return the error code received.</p>
<p>Bytes Sent - The number of bytes sent as a response.</p>
<p>Object Size - The size of the object in question in the request.</p>
<p>Total Time: - Measured in milliseconds, it represents how long the request took from receiving the request to the last byte of sending a response.</p>
<p>Turn-Around Time - This shows how long it took S3 to process the request.</p>
<p>Referer - The value is taken from the HTTP referer header, however, in this case, there was none present and so a hyphen is represented as the value.</p>
<p>User-Agent - This shows the value taken from the HTTP user-agent header.</p>
<p>Version ID - If present, this will show the Version ID of the request.</p>
<p>Host Id - The x-amz-id-2 or Amazon S3 extended request ID. The x-amz-id-2 header is a token that is used together with the x-amz-request-id header to help <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> troubleshoot problems.</p>
<p>Signature Version - This will show which signature version was used to authenticate the request.</p>
<p>Cipher Suite - If SSL was used it will show which cipher suite was used. If HTTP was used, then a hyphen would be shown instead.</p>
<p>Authentication Type - This shows the type of authentication used for the request.</p>
<p>Host Header - Represents the endpoints used to connect to Amazon S3 in the request.</p>
<p>TLS Version - This shows which version of TLS was used by the client.</p>
<h1 id="Static-Website-Hosting"><a href="#Static-Website-Hosting" class="headerlink" title="Static Website Hosting"></a>Static Website Hosting</h1><p>Hello and welcome to this lecture which will look at how S3 buckets can be configured to host a static website.</p>
<p>This lecture is not designed to show you how to create a static website using your own domain, instead, it will just focus on the S3 Bucket configurations.</p>
<p>If you are looking to create a simple and static website that requires no server-side scripting of any kind, then this can easily be hosted with one of your Amazon S3 buckets.</p>
<p>Let me explain the bucket configuration required to enable this. So from the Properties page of your chosen S3 bucket, select the Static Website Hosting tile. By default, static website hosting is disabled. When this tile is selected you are presented with three options, Use this bucket to host a website, Redirect requests, and Disabled.</p>
<p>In addition to these options, there is also a region-specific website endpoint shown for your bucket. This endpoint allows users to access your website via that URL. However, there a couple of points to understand when using your S3 endpoint as your website URL address. Firstly, it does not support HTTPS requests. The bucket and its contents must be marked as publicly accessible. And it does not support Requester Pays. And I’ll be discussing more on Requester Pays later in this course.</p>
<p>If you select the option of Use this bucket to host a website you need to provide additional parameters in its configuration.</p>
<p>Firstly, you need to add an index document. And your index document will be the default, or home page of your static website. The error document will be the page that is displayed when an error occurs. And these documents must be located within your bucket. The redirection rules allow you to use XML to create advanced redirect requests to specific content. For advanced information on how to create these redirection rules please see the following <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html#advanced-conditional-redirects">link</a>.</p>
<p>Once you have added your index and error document and optional redirection rules, select Save.</p>
<p>The other option available to you is Redirect Requests. And this option allows you to redirect all traffic to your website endpoint. In this example, any traffic destined for the following endpoint will be redirected to <a target="_blank" rel="noopener" href="http://www.mywebsite.co.uk/">www.mywebsite.co.uk</a>. The target destination could also be another bucket configured for static website hosting. The Protocol field allows you to enter which protocol should be used during the redirect.</p>
<p>For people to be able to access your website hosted within your S3 bucket, then the bucket must be accessible to the public, so we need to ensure that the permissions are set correctly. By default your S3 buckets are blocked to the public. If you select your bucket and then permissions tab, you will see the current settings for your bucket under the Block all public access.</p>
<p>Here you can see that by default all access is blocked to the public. You must edit these settings and uncheck the Block all public access setting, save and confirm those changes.</p>
<p>Once your bucket is publicly readable, you will then need to add a bucket policy to allow the public to read your objects within your bucket. So to do so, you need to select the Bucket Policy tab from within the permissions of your Bucket and enter following policy, replacing the text in red with the name of your bucket being used for your static website.</p>
<p>This policy allows everyone to have the action of <code>s3:GetObject</code> from your bucket hosting your website And you will be notified via a warning that this will make all objects publicly accessible, accept the warning as this is the outcome that you are trying to configure.</p>
<p>So at this stage, you would have added your index.html document, configured your bucket to be publicly accessible, and also added a bucket policy to allow the objects within your bucket to be accessed and read by the public. All that is left to do now is to test it using your website endpoint. It is also possible to set up your static website with your own customized domain name instead of the automatically generated website endpoint created by S3.</p>
<p>In addition to serving your content via CloudFront, however, these configurations and topics are out of scope for this course. I now want to show you a very quick demonstration of the points I just walked through so you can see it via the console, so let’s take a look.</p>
<p>Okay, so I’m at the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> Management Console or the front screen. So firstly, I need to go to Amazon S3, which is under Storage. And this will take me to my S3 dashboard with all of my buckets that I have. Now I’ve already created a bucket and it’s called stustaticwebsite. So if we have a look at this bucket, we can see that there’s no objects in here. And if we look at the properties, we can see that static website hosting is currently disabled. And if we go across to the Permissions, we can see that all public access is currently blocked and there is no bucket policy.</p>
<p>So let me start by unblocking the public access. So if I go across to the Block public access tab, select Edit, and then uncheck the Block all public access and then Save. And here just needs typing confirm to confirm that I want to make those changes. And now I can see here that public access settings has been updated successfully. And the Block all public access now shows as off. From here, now I want to add my index file. So I’ve already created a file called Myindex.html so I just gotta cross and find that. There we have it there, Myindex.html, select Open. Click on Next.</p>
<p>Now under the Manage public permissions, I want to grant read access to this object. ‘Cause I want people to be able to see my index file to my website. Click on Next. Select your storage class, I’ll just select Standard for this demonstration. And then click Upload. So that’s Myindex.html file. Now if I go across to Properties, and enable static website hosting. So if I select on the tile, and use this bucket to host a website. Enter the name of my index file. And here you can add the error document as well if you want and any redirection rules. For this demonstration, I’m just gonna leave it as Myindex.html. And up here we can see the endpoint that will be used to access our website as well. Let’s click on Save</p>
<p>So now we’ve enabled static website hosting, we’ve added our index file to the bucket, we’ve changed the permissions to ensure that public access is allowed, however, we still need to add our bucket policy. So if I paste my policy in that I’ve already created. And we can see here that this allows any principal, the <code>s3:GetObject</code> for this bucket here. Click on Save. And you’ll be issued a warning to say that you have provided public access to this bucket. We highly recommend that you never grant any kind of public access to your S3 bucket. This is intended, so you can just ignore that warning.</p>
<p>Okay, so now we’ve unblocked all public access, we’ve entered a bucket policy that allows anyone to access this bucket using the <code>s3:GetObject</code> action, we’ve added our HTML file to the bucket, and we’ve also enabled static website hosting. So all we need to do now to test it is to select the endpoint. And there we go. And this is the Myindex.html file that I added to the bucket. So if we just go back to the bucket so you can have a look at that. If I open that file, you can see that it’s the same file. And that’s how you quickly and simply configure static website hosting.</p>
<h1 id="Object-Level-Logging"><a href="#Object-Level-Logging" class="headerlink" title="Object-Level Logging"></a>Object-Level Logging</h1><p>Hello and welcome to this short lecture which will introduce you to the object level logging capabilities with your S3 buckets.</p>
<p>This feature is actually more closely related to the AWS CloudTrail service than S3 in a way, as it’s AWS CloudTrail that performs logging activities against Amazon S3 data events. These data events are specific API calls used in S3, such as <code>GetObject</code>, <code>DeleteObject</code>, and <code>PutObject</code>.</p>
<p>So what is CloudTrail? CloudTrail is a service that has a primary function to record and track all AWS API requests made. These API calls can be programmatic requests initiated from a user using an SDK, the AWS command-line interface, from within the AWS management console or even from a request made by another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service.</p>
<p>When an API request is initiated, AWS CloudTrail captures the request as an event and records this event within a log file which is then stored on S3. Each API call represents a new event within the log file. CloudTrail also records and associates other identifying metadata with all the events. For example, the identity of the caller, the timestamp of when the request was initiated and the source IP address.</p>
<p>We have a detailed course on AWS CloudTrail which an be found <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">here</a> which will provide a deep insight into the service and its full capabilities.</p>
<p>Capturing S3 data events can be configured in 2 ways: Firstly, if you want to capture data events for all or some of your S3 buckets, then you can configure this from within one of your Trails using the AWS CloudTrail console itself as shown here. Secondly, if it’s not already enabled via AWS CloudTrail for your bucket you can configure it at the bucket level using the Properties tab. Selecting the Object-level logging tile will present you with options to configure it.</p>
<p>As you can see, due to its integration with AWS CloudTrail you will be asked to select an existing trail from the same region to capture your S3 data events for this bucket. In this example, I have used my ‘Trail_Demo’ trail. You must also select which type of events you would like to capture, either just Read events or Write events, or both. Once you have made your selection, simply select Create and Object-level logging will be enabled and AWS CloudTrail will capture any S3 Data events associated with this bucket.</p>
<p>For more information on where your CloudTrail logs are stored and accessed, and how to interpret your CloudTrail logs, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-cloudtrail-introduction/aws-cloudtrail-an-introduction-1/">here</a>.</p>
<h1 id="Object-Lock"><a href="#Object-Lock" class="headerlink" title="Object Lock"></a>Object Lock</h1><p>Hello and welcome to this lecture looking at the Object lock property which is considered an ‘advanced’ property of an S3 bucket.</p>
<p>This feature is often used to meet a level of compliance known as WORM, meaning Write Once Read Many. It allows you to offer a level of protection against your objects in your bucket and prevents them from being deleted, either for a set period of time that is defined by you or alternatively prevents it from being deleted until the end of time! The ability to add retention periods using Object Lock help S3 to comply with regulations such as FINRA, the Financial Industry Regulatory Authority.</p>
<p>Setting Object Lock on a bucket can only be achieved at the time of the creation of the bucket. If you attempted to enable it on an existing bucket by clicking on the Object Lock tile in the bucket properties, you would receive the following error.</p>
<p>To enable and configure object lock during the creation of the bucket, you first need to ensure that you have Versioning enabled. Without first enabling versioning, it is NOT possible to enable object lock, which can be found under the ‘Advanced’ setting of Step 2 ‘Configure Options’ during creating your bucket.</p>
<p>Once you have created your bucket with object lock enabled it will be permanently enabled and can’t be disabled.</p>
<p>Although your bucket is now configured for ‘object lock’, any object your place into it at this stage is NOT automatically protected, to ensure they are you need to enable some default options on the bucket first. </p>
<p>When you select the Object-lock tile, which will now say ‘Permanently enabled.’</p>
<p>You will be presented with two retention modes, and the settings selected here will define the default retention of an object when it is added to the bucket and therefore applying the required protection that object lock provides.</p>
<p>These retention modes are Governance Mode and Compliance Mode.</p>
<p>By enabling Governance Mode it prevents your users from performing a delete or an overwrite of any of the versions of your objects in the bucket throughout the duration set by the retention period. However, if you have very specific permissions, including <code>s3:BypassGovernanceMode</code>, <code>s3:GetObjectLockConfiguration</code>, <code>s3:GetObjectRetention</code>, then a user will still be able to delete an object version within the retention period or change any retention settings set on the bucket.</p>
<p>When setting Governance Mode you will be asked to add a retention period in days and therefore defines how long the object is protected by object lock preventing it from being deleted. When an object is added to the bucket, a timestamp is added to the metadata reflecting the retention period. When the retention period is over, the object can then be deleted again.</p>
<p>Compliance Mode. The key difference between Compliance Mode and Governance Mode is that there are NO users that can override the retention periods set or delete an object, and that also includes your <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> root account which has the highest privileges. Essentially, any object added to a bucket configured for Compliance Mode means that the object will remain for the duration of the retention period.</p>
<p>Again, much like with Governance Mode, you will be asked to enter a retention period based upon a number of days.</p>
<p>You can also set object-lock on a per-object by object basis if you didn’t want to set a default retention mode of Governance or Compliance. To do so, you need to select the object-lock option of the object’s properties itself. When doing so, you will see the following screen.</p>
<p>Again, you can set either the governance or compliance retention mode for that specific object. The ‘Retain until date’ shows that this object is already bound by a retention mode with a retention period, and as a result, it shows the date in which this object is to be protected until. When this date has passed, the object is no longer protected and can be deleted.</p>
<p>The legal hold element only appears for object versions and not at the bucket level and acts much like a retention period and prevents the object from being deleted, however, legal holds do not have an expiration date. Therefore, the object will remain protected until a user with permissions of s3:PutObjectLegalHold disables the legal hold on the object. If an object is already protected by a retention period, a legal hold can also be placed on the object. When the retention period expires, the object will still be protected by the legal hold regardless of the fact that the retention period has expired.</p>
<h3 id="Lectures"><a href="#Lectures" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/versioning/">Versioning</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/server-access-logging/">Server-Access Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/website-hosting/">Static Website Hosting</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/object-level-logging/">Object-Level Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/default-encryption/">Default Encryption</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/tags/">Tags</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/transfer-acceleration/">Transfer Acceleration</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/events/">Events</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/requester-pays/">Requester Pays</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/summary/">Summary</a></p>
<h1 id="Tags"><a href="#Tags" class="headerlink" title="Tags"></a>Tags</h1><p>Hello and welcome to this very short lecture which explains the use of cost allocation tags against your S3 buckets.</p>
<p>It is likely when using Amazon S3 that you are using it for a variety of different use cases and solutions, across multiple business units and departments, each with different cost centers. This can make it difficult to manage budgets across your organization. Using bucket tags, known as S3 cost allocation tags, you can assign key-value pairs at the bucket level to help with categorization. For example, let’s suppose you had 3 different buckets each with 2 key-value pairs, Project and Environment.</p>
<p>Using these tags, we can see that each bucket belongs to a different ‘Project’, and also that 2 of them are considered ‘Production’ and another is ‘Test’, based on the environment Key. Tags like this can be used across all <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services and help you to manage, categorize, and organize your resources in a variety of ways.</p>
<p>Using the Cost Explorer from within your AWS Billing and Cost Management, you can report on these Key values, for example, you could identify and highlight the costs associated with your resources that were tagged with the project, CloudAcademy. This will highlight all the AWS resources that had this key value pair allowing you to get a full understanding of the project costs for that particular project, in this case, ‘CloudAcademy’.</p>
<p>To add your tags to your bucket, select your bucket and from the Properties tab select Tags Select the tile and configure your tags as required and click save. One point to note is that you must activate your cost allocation tags from within AWS Billing before they will show up on any reports. To do this, go to your AWS Billing and Cost Management dashboard from within the AWS Management Console, select Cost Allocation tags and then activate any user-defined tags that you created.</p>
<h3 id="Lectures-1"><a href="#Lectures-1" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/versioning/">Versioning</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/server-access-logging/">Server-Access Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/website-hosting/">Static Website Hosting</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/object-level-logging/">Object-Level Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/default-encryption/">Default Encryption</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/object-lock/">Object Lock</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/transfer-acceleration/">Transfer Acceleration</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/events/">Events</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/requester-pays/">Requester Pays</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/summary/">Summary</a></p>
<h1 id="Transfer-Acceleration"><a href="#Transfer-Acceleration" class="headerlink" title="Transfer Acceleration"></a>Transfer Acceleration</h1><p>Hello and welcome to this lecture covering how you can speed up your long-distance S3 data transfers using Transfer Acceleration.</p>
<p>When transferring data into or out of Amazon S3 from and to your remote client, or to another AWS region, transfer acceleration can dramatically speed up the process by utilizing another <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> service, Amazon CloudFront.</p>
<p>Amazon CloudFront is a content delivery network service (CDN), which essentially provides a means of distributing traffic worldwide via edge locations. AWS edge locations are sites deployed in major cities and highly populated areas across the globe. More information on Amazon CloudFront can be found in our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/cloudfront/overview-2/">here</a>.</p>
<p>When transferring data to S3 from your client with transfer acceleration enabled at the bucket level, the request will go via one of the CloudFront Edge Locations, from here the transfer request will then be routed through a high speed optimized AWS network path to Amazon S3.</p>
<p>When using transfer acceleration you should be aware that there is a cost. Whereas normal data transfer into amazon S3 is free from the internet, with transfer acceleration, this is a cost associated per GB depending on which edge location is used. Also, there is an increased cost for any data transferred OUT of S3, either to the internet or to another Region, again due to the edge location acceleration involved.</p>
<p>To enable transfer acceleration is very simple to do. Select your bucket within the S3 console, select ‘Properties’ and then select the Transfer Acceleration tile.</p>
<p>You can then either ‘Enable’ it or ‘Suspend’ as required. You will also notice that you will be given an Endpoint, in this case, the endpoint for the bucket is as shown.</p>
<p>As a result, to enable transfer acceleration your bucket name must be DNS compliant and not contain any periods at all. Also, to make use of the transfer acceleration feature itself, any requests, such as GET or PUT to the bucket, must use this new transfer acceleration endpoint.</p>
<p>One final point to make with transfer acceleration is that there are a couple of S3 operations that it does not support, these being: GET Service (list buckets), PUT Bucket (create bucket), DELETE Bucket, and Cross region copies using PUT Object - Copy.</p>
<h1 id="Requester-Pays"><a href="#Requester-Pays" class="headerlink" title="Requester Pays"></a>Requester Pays</h1><p>Hello and welcome to this short lecture which looks at an advanced property of an S3 bucket, Requester Pays.</p>
<p>As the name implies, when this feature is configured any costs associated with requests and data transfer becomes the responsibility of the requester instead of the bucket owner. The bucket owner will still, however, pay for the storage costs associated with the objects stored in the bucket.</p>
<p>A fundamental condition of enabling requester pays is ensuring that all access is authenticated to your bucket, as anonymous access requests will not be able to take advantage of the requester pays attribute. This is because <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> would not know which account to charge the request and data download to. By authenticating requests, it allows a trace back to the identity and to which AWS account that identity is originating from, and the cost is then transferred to that account.</p>
<p>To enable requester pays is very simply, select the ‘Requester Pays’ tile from the properties of the required bucket. The options available are either to enable requester pays or disable requester pays. Once your decision has been made, select Save. From this point, any POST, GET or HEAD requests to the bucket must include x-amz-request-payer in the header and this parameter confirms that the requester is aware that there are cost implications associated with that request for requester pays.</p>
<h3 id="Lectures-2"><a href="#Lectures-2" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/versioning/">Versioning</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/server-access-logging/">Server-Access Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/website-hosting/">Static Website Hosting</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/object-level-logging/">Object-Level Logging</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/default-encryption/">Default Encryption</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/object-lock/">Object Lock</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/tags/">Tags</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/transfer-acceleration/">Transfer Acceleration</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/events/">Events</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-s3-bucket-properties-management-features-maintain-data/summary/">Summary</a></p>
<h1 id="Using-Policies-to-Control-Access"><a href="#Using-Policies-to-Control-Access" class="headerlink" title="Using Policies to Control Access"></a>Using Policies to Control Access</h1><p>Hello, and welcome to this lecture that is going to look at the different methods of using policies and permissions to gain access to S3 resources, which include both S3 buckets and objects within those buckets. We can control access to S3 resources via policies using both identity-based policies and resource-based policies.</p>
<p>Identity-based policies are attached to the IAM identity requiring access, and then using IAM permission policies, either in-line or managed, they can then be associated to the user, a group that the user belongs to, or via a role that the user has permission to assume. Identity-based policies define the resource in the policy.</p>
<p>For example, the bucket name, as you can see in this example policy shown here. So this identity-based policy can be associated with a user and will permit them to use all S3 actions within the bucket, s3deepdive. </p>
<p>You can of course use conditions within your policies to manage and refine access to your resources even further. For example, in this policy, the identity would be able to perform the actions CreateBucket, DeleteBucket, and PutObject on the s3deepdive bucket on the condition that their source IP address was within the range of the CIDR block 10.1.0.0&#x2F;16.</p>
<p>For a full list of actions, resource types, and condition keys to use within policies, please see the AWS reference <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html">here</a>.</p>
<p>Now, resource-based policies differ in the fact that the policy is associated with a resource rather than the identity. So from an Amazon S3 perspective, resource-based policies come in the form of Access Control Lists and bucket policies. Because of this, you need to define within the policy who will be allowed or denied access. This is managed differently depending on if you’re using bucket policies or Access Control Lists.</p>
<p>Let me first focus on bucket policies. A bucket policy is written in JSON and is directly attached to your bucket as another means of granting and restricting access control. And by default, when you create a bucket, no bucket policy exists.</p>
<p>To add a modified bucket policy, you must select the bucket, go to permissions, and then select the Edit button in the bucket policy section. From here, you have one of three different options. You can either freely write your own bucket policy in the JSON window, view some policy examples, or use the AWS policy generator to help you create your bucket policy.</p>
<p>When working with bucket policies, you must specify a principle element, which defines the principal who is associated with the action and effect defined in the statement. Let’s look an example bucket policy to make it clearer. This bucket policy allows the actions, s3 DeleteObject and s3 PutObject, for the s3deepdive bucket for the principal Stuart.</p>
<p>You’ll notice that with the identity-based policy shown previously, I didn’t need to add the Principle element. It’s important to note that the permissions defined within the statements of a bucket policy apply to the bucket and the objects that reside within the bucket.</p>
<p>So now, we are aware that you can use both IAM policies to control access to S3 data and bucket policies, you might be wondering when you should use one over the other. So let me take a look at that in more detail. You might want to use IAM if you want to centrally manage your access control methods all in the one service, that being IAM.</p>
<p>Also, if you have multiple permissions to apply to a large number of buckets, the management of doing so might be easier to do from within IAM access across one or two policies rather than creating separate bucket policies that you would have to attach to each of your buckets. Also, IAM has the added advantage of being able to control access for more than one service at a time within its policies, whereas bucket policies only control access to the S3 bucket and its objects.</p>
<p>On the flip side, you might want to use bucket policies if you want to maintain your security policies within S3 alone. You can also grant cross-account access using bucket policies without having to create and assume roles that are created within IAM.</p>
<p>For example, all you’d need to do is to create a bucket policy as shown and attach it to your bucket, for example, s3deepdive, replacing the text in red with the external account. In the external account, users then simply have to be delegated access to the same bucket to be able to access the bucket.</p>
<p>Also, another reason you might want to use bucket policies is because IAM policies can be a maximum of two kilobytes in size for users, five kilobytes for groups, and 10 kilobytes for roles. However, bucket policies can reach a size of 20 kilobytes.</p>
<p>Now, you can of course use both IAM policies and bucket policies to control access. They are not mutually exclusive. They can both be used together to control access. I should come to how multiple access controls are evaluated and their logic after we look at S3 Access Control Lists, ACLs, which as I mentioned previously are another resource-based access control method.</p>
<p>S3 Access Control Lists, or ACLs, allow you to control access to buckets in addition to specific objects within a bucket by groupings and AWS accounts. One advantage of being able to use ACLs is that you can set different permissions per object.</p>
<p>ACLs do not follow the same JSON format as policies defined by IAM and bucket policies. Instead, they are far less granular, and different permissions can be applied depending if you are applying an ACL at the bucket level or the object level.</p>
<p>Due to the basic structure of an ACL, it is not possible to implicitly deny access using ACLs. Neither are you able to implement conditional elements, like we saw earlier when I mentioned identity-based access. An example of default ACL for a bucket looks as shown.</p>
<p>As you can see here, you can specify different permissions for different predefined S3 groups defined by the Grantee column. Permissions are based on the grantee of which there are four:</p>
<ul>
<li>The bucket owner. This’ll be your own AWS account and will have full control over all objects and the bucket itself.</li>
<li>Everyone (public access). Permissions set against this grantee would mean anyone can access using the permissions applied, providing the object had been made public. These requests can be signed, authenticated, or unsigned, unauthenticated.</li>
<li>Authenticated users. This option will allow IAM users from any AWS account to access the object via signed request, authenticated.</li>
<li>S3 log delivery group. This is a group used to deliver log files when server access logs has been configured, and the bucket is used to store and write log files to.</li>
</ul>
<p>By editing the ACL at the bucket level, you’ll be presented with the following. This shows you the different levels of permissions that can be applied to the bucket. It will show you the permissions given to the grantee groups for the bucket, which are either List or Write. You can also see what permissions have been given to enable the grantee access to either read or write against the bucket ACL.</p>
<p>For clarification about what these permissions actually grant, they are as follows. List, which allows the grantee to list objects in the bucket. Write, which allows a grantee to create, overwrite, and delete objects in the bucket. Bucket ACL Read, which allows the grantee to read the ACL of the bucket. And Bucket ACL Write, which allows the grantee to write the ACL for the bucket.</p>
<p>You may have noticed that you can also add access for another account as a grantee. To configure this access, you’ll be required to enter the canonical ID of the AWS account that you would like to have access. So we have seen what ACL looks like at the bucket level.</p>
<p>Let’s now take a quick look at what it looks like at the object level. Here’s an ACL of an object within a bucket. As you can see, again, by default, the resource owner has full control over the object. Also, you will notice that you can add access for another account.</p>
<p>Again, you’ll need to add the appropriate canonical AWS account ID and the relevant permissions. You can also specify public access if you have public access enabled, which I will explain more on later. In this example, we can see that the Everyone group has read access to the object.</p>
<p>To modify these permissions, I could simply click on the radio button next to the group and modify the settings as shown. Note, there is no write permission when working with object ACLs, unlike bucket ACLs.</p>
<p>Again, let’s take a closer look at what actual permissions are being applied depending on the option chosen for the object. Read object allows the grantee to read the object data and its metadata. Read object permissions allows the grantee to read the object ACL. Write object permissions allows the grantee to write the ACL for the object.</p>
<p>So we have now looked at a number of different methods to implement access controls with various permissions to your S3 buckets and objects. We looked at IAM policies, S3 bucket policies, and finally S3 Access Control Lists. But what would happen if you used all of these access control methods? How would the policy evaluation logic operate? Which would take precedence?</p>
<p>Let’s say, for example, someone tries to access an object in a bucket that has S3 ACLs attached in addition to bucket permissions and also have their own IAM permissions. How is this access governed if there are conflicting permissions to the object in the bucket that they are trying to access? All of these policies will be viewed together to determine the resulting access and will handle any permission conflict in accordance with the principle of least privileged.</p>
<p>Essentially, by default, AWS states that access is denied to an object, even without an explicit Deny within any policy. To gain access, there has to be an Allow within a policy that the principal is associated to or defined by within a bucket policy or ACL. If there was no Deny defined, but there is an Allow within a policy, then access will be authorized. However, if there is a single Deny associated with the principal to a specific object, then even if an Allow does exist, this explicit denial will always take precedence overruling the Allow and access will not be authorized.</p>
<p>So going back to our example, we have a user called Stuart who has the following identity-based policy associated. The bucket s3deepdive also has the attached bucket policy. The ACL permissions on the s3deepdive bucket give the bucket owner full control to the bucket and its objects. The user Stuart is a part of the same AWS account as the bucket owner.</p>
<p>So what are the resulting permissions? The IAM permissions grant Stuart all actions to the s3deepdive bucket, the bucket policy denies Stuart access to delete buckets and objects within the s3deepdive bucket, and the ACL allows access for Stuart, as he is a part of the account relating to the bucket owner.</p>
<p>So as we know, a Deny always takes precedence over an Allow, meaning Stuart will have access to the s3deepdive bucket to perform all S3 actions apart from deleting the bucket or any of its objects.</p>
<h1 id="Managing-Public-Access-to-Your-Buckets"><a href="#Managing-Public-Access-to-Your-Buckets" class="headerlink" title="Managing Public Access to Your Buckets"></a>Managing Public Access to Your Buckets</h1><p>Hello and welcome to this lecture where I’m going to be discussing how to block public access to your buckets. Over the years, we’ve all seen news articles of instances where organizations have left themselves exposed by leaving customer and confidential information within unprotected AWS buckets, allowing access to the general public. This has resulted in huge security breaches and has left those organizations answering difficult questions in addition to recovering from financial penalties.</p>
<p>As a response to these mistakes made by these organizations and the resulting repercussions, AWS has continually worked to improve the security around Amazon S3, to prevent instances such as these from happening again.</p>
<p>So in this lecture, we’re going to be looking at the methods that can be applied to ensure that you do not follow the same steps and fail to protect your buckets from public access. When creating a new bucket in S3, there’s an option that’s dedicated to helping you protect your bucket from public access. And by default, you can see that there’s a checkbox that’s ticked, which blocks all public access.</p>
<p>As a result, you have to actively change this setting to allow public access. If you do need some public access to this bucket, then you can turn off the setting, and it allows you to select for additional options that can be used to filter public access.</p>
<p>So you can block public access to buckets and objects granted through new access controllers, block public access to buckets and objects granted through any access controllers, block public access to buckets and objects granted through new public bucket or access point policies, and block public and cross account access to buckets and objects, through any public bucket or access point policy.</p>
<p>This allows you to allow some public access based on certain security controls and block others. You don’t have to select any or you can have a combination of the four shown. Once you’ve made your selection, you can review the settings on your bucket by selecting it and viewing the Permissions tab.</p>
<p>In this screenshot, you can see that the selected bucket has all public access blocks. However, these settings can be changed by selecting the edit button. Because all public access to this bucket is blocked, I will see a blue information notice if I were to configure the bucket policy or ACL for this bucket, as you can see here.</p>
<p>As a result, if I tried to allow any kind of public or cross account access for the bucket policy or ACL, then access would still not be allowed as the bucket still has the block all public access setting enabled. Let me show you what would happen if you try to update the bucket ACL and object ACL with these block all public access settings in place.</p>
<p>Okay, so I’m not going to be AWS management console and the S3 dashboard. Now what I want to do, is go into my S3 bucket. So I’ll go into the S3 deep dive bucket, go across two permissions. Now here straightaway we can see that I have the block or public access setting on.</p>
<p>Now if I scroll down to the ACL settings, so here we have the bucket ACL, we can see that the everyone group currently does not have any access. Now if I wanted to try and change this, I can go across to edit, and then select list and read access for the everyone group.</p>
<p>Now if I scroll down, it gives me wanting to say if I grant these permissions then anyone in the world can access the objects in this bucket. So I have to confirm that via tick box as an extra level of confirmation. If I then click on save changes, I get an error. It says I don’t have permissions to edit these ACL settings, with a response of access denied. And that is because we have the block all public access on. So this overrides the ACL.</p>
<p>Let’s now take a look at the ACL of the objects. So if I select objects, I’ve just got one object in here a screenshot. So if I select that, and then scroll down to the ACL settings, so this is the ACL of the object. And again, we can see that the everyone group does not have access. So if I was to edit this object, ACL, it doesn’t even give me the option, it’s grayed out. And it says at the top here, public access is blocked, because block public access settings are turned on for this bucket.</p>
<p>Okay, so now what I want to do, is to go back to the settings of the bucket itself. And this time, I’m going to edit this option and allow public access. So I’m just gonna untick that and save changes. It gives me a warning to say that if I do this, then anyone can access objects in my bucket, see how to type in confirm. And now if I go down to the ACL, again, it gives me a warning to say that AWS doesn’t recommend granting access to the everyone grantee, and if we go to Edit we can see that these have already been activated because we allowed public access.</p>
<p>But again, AWS does its best to highlight that this is a potential security risk by having these warning signs next to it as well. And again, you have to confirm that you understand the effect of these changes to apply them. If you save changes, I no longer get the error message.</p>
<p>So let’s do the same on the object. If I select my object again, go down to the ACL settings, we can see that it has the read, you understand the effects of these changes, just save changes and that’s that. So we can now see that the object is accessible by the everyone group, and also the bucket is as well. Just check those permissions again, we can see that the everyone group has the list and read.</p>
<p>Now what has to happen if I edit the public access settings again? So if I click on edit, and then blocked all public access, confirm those changes. And then I checked the ACL again, it has removed the access for the everyone group. So as soon as I enabled that block or public access setting, AWS updates all the settings in the bucket and the objects to remove that access.</p>
<p>So we can see here that’s been removed from the bucket. And just for clarification, let’s check out the object, have a look at the ACL there. And again, we can see it has been removed. So it’s a very powerful setting to quickly remove all public access, regardless of the permissions that you’ve already applied.</p>
<h1 id="Cross-Origin-Resource-Sharing-CORS-with-S3"><a href="#Cross-Origin-Resource-Sharing-CORS-with-S3" class="headerlink" title="Cross Origin Resource Sharing (CORS) with S3"></a>Cross Origin Resource Sharing (CORS) with S3</h1><p>Hello, and welcome to this short lecture covering Cross Origin Resource Sharing, known as CORS in Amazon S3. At a high level, CORS allows specific resources on a webpage to be requested from a different domain than its own. And this allows you to build client-side web applications. And then if required, you can utilize CORS support to access resources stored in S3.</p>
<p>Let’s take a look at how to configure CORS for a bucket which as you might expect involves the use of policies and these policies are embedded in the CORS configuration of the bucket itself which can be found under the Permissions tab.</p>
<p>Let’s take a look at an example which has a single rule. The following policy allows you to use PUT, POST, and DELETE from the origin of <a target="_blank" rel="noopener" href="http://www.cloudacademy.com/">www.cloudacademy.com</a>. The AllowedHeaders element of the policy determines which headers are allowed in a preflight request through the Access-Control-Request-Headers header, which is used by browsers to let the server know which HTTP header the client might send when the actual request is made. And in this case, all headers will be allowed to be used in a preflight request.</p>
<p>Using this example, when the bucket receives a preflight request from a browser, S3 will evaluate the policy associated with the bucket for its CORS configuration and will process the first matching rule in the policy. A match is made when the following conditions in the rule are met.</p>
<p>The requestor’s Origin header matches an entry made in the AllowedOrigins element. The method used in the request, for example a POST or DELETE operation is matched in the AllowedMethods element. And finally, the headers used within the requests Access-Control-Request-Headers header with a preflight request matches a value in the AllowedHeader element.</p>
<p>The ExposeHeader element in the policy is used to define a header in the response that is allowed to be made by customer applications. For a full reference to the common S3 response headers, take a look at the common response headers in the S3 API Reference Guide found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonResponseHeaders.html">here</a>.</p>
<p>Your CORS policy can contain more than one rule. For example, the following policy contains two rules. The first rule is the same as the one we looked at earlier and the second rule only allows PUT and POST operations following the origin of aws.com.</p>
<h1 id="The-Case-for-Lifecycle-Configurations"><a href="#The-Case-for-Lifecycle-Configurations" class="headerlink" title="The Case for Lifecycle Configurations"></a>The Case for Lifecycle Configurations</h1><p>Consider the following scenario: you have a data lake workload in a versioned S3 bucket that grows at a very fast and consistent pace. Tons of overwrites are occurring daily and large objects are being uploaded via multipart upload. </p>
<p>And because bad things often happen to good people - that means, that from the data management perspective, you may have incomplete multipart uploads every so often, and tons of out-of-date non-current versions of your data. I’m sure you can probably imagine how costly it is to run this data swamp - I mean, lake. </p>
<p>So it’s important that you consider every cost tool at your disposal. And one of these tools is to manage the storage lifecycle of your data, by moving your data to lower cost storage classes, or deleting data you no longer need. You can, of course, move and delete your data manually, but that can be difficult to manage at scale. So, there are two ways to automate this process: </p>
<p>The first way is to use the S3 Intelligent-Tiering storage class. You’ll pay a monthly object monitoring and automation charge, and in return S3 Intelligent-Tiering will monitor your object access patterns, and automatically move your objects between three tiers: frequent, infrequent and archival. S3 Intelligent-Tiering is recommended for data access patterns that are unknown or unpredictable, and is meant to give a more “hands-off” approach to managing your data lifecycle. </p>
<p>The second approach is by using Lifecycle configurations. You can use lifecycle configurations to transition data to a lower cost S3 storage class, or to delete data. Lifecycle configurations additionally provide options to clean up incomplete multipart uploads and manage noncurrent versions of your data - which ultimately, helps reduce storage spend. </p>
<p>Using lifecycle configurations is the most cost-effective strategy when your objects and workloads have a defined lifecycle and follow predictable patterns of usage. </p>
<p>For example, a defined access pattern may be that you use S3 for logging and only access your logs frequently for at most, a month. After that month, you may not need real-time access, but due to company data retaining policies, you cannot delete them for a year. </p>
<p>With this information, you can create a solid lifecycle configuration based on this access pattern. You could create an S3 Lifecycle configuration that transitions objects from the S3 standard storage class to the S3 Glacier Flexible Retrieval storage class after 30 days. By simply changing the storage class of your objects, you will begin to see significant cost savings in your overall storage spend. And after 365 days, you can then delete the objects and continue to save on costs. </p>
<p>You may find that a lot of your data follows a similar access pattern: you slowly stop needing real-time access to your data, and can eventually delete the data after a certain period of time passes. Or you may have data that you need to save to meet some compliance or governance regulation that can be moved from S3 Standard to archival storage and left alone for long periods of time. Or perhaps, you have a ton of objects in S3 Standard storage and you want to transition all of those objects into the S3- Intelligent Tiering storage class. </p>
<p>If these patterns sound similar to your use case, then using lifecycle configurations makes sense for your workload. </p>
<p>In summary, Lifecycle configurations are an important cost tool that can enable you to delete or transition old unused versions of your objects, clean up incomplete multipart uploads, transition objects to lower cost storage tiers and delete objects that are no longer needed. </p>
<h1 id="Components-of-a-Lifecycle-Configuration"><a href="#Components-of-a-Lifecycle-Configuration" class="headerlink" title="Components of a Lifecycle Configuration"></a>Components of a Lifecycle Configuration</h1><p>An S3 Lifecycle configuration is technically an XML file. While you won’t need to know XML to create lifecycle configurations in the AWS console, the XML format is helpful in understanding the various components to see how it all works under the hood. For that reason, I’ll be describing the anatomy of a lifecycle configuration using XML throughout this video. </p>
<p>Each lifecycle configuration contains a set of rules. Each rule is broken up into four components: ID, Filters, Status, and Actions. </p>
<p>The ID uniquely identifies the lifecycle rule - you can consider this the name of the rule. This is important, as one lifecycle configuration can have up to 1000 rules, and the ID can help you keep track of what rule does what. </p>
<p>The filters section defines WHICH objects in your bucket you’d like to take action on. You can choose to apply actions to all objects or a subset of objects in a bucket. If you choose a subset of objects, you can filter based on prefix, object tag, or object size. Or to be very granular, you could filter based on a combination of these attributes. For example, you can create a filter that transitions all objects with the prefix ProjectBlue&#x2F;, if they also are tagged with the Classification tag value “Secret”. Notice that when you combine multiple filters, you have to use the keyword “And” in XML. </p>
<p>For object size, I can transition or expire objects if they’re greater than a specific size, less than a specific size, or if they’re in a range between two size bounds. For example, I can create a filter, where I’m transitioning my objects if they’re larger than 500 Bytes, but smaller than 10,000 Bytes. Keep in mind that the maximum filter size is 5TB.</p>
<p>The next component is status. With the status field, you can enable and disable each lifecycle rule. This can be helpful when you’re testing lifecycle configurations out, as you figure out what the best rules for your workload are. Once you change the status to “disabled”, S3 won’t run the actions defined in that rule - essentially stopping the lifecycle action. And when you’re ready to run those actions on your objects again, you can always change the status back to enabled. </p>
<p>The last, and arguably the most important component is the actions section. This is where you define WHERE you want your objects to move to - are you transitioning them to another storage class or are you deleting them? </p>
<p>There are six main actions that you can use: Transition, expiration, NoncurrentVersionTransition, NoncurrentVersionExpiration, ExpiredObjectDeleteMarker, and the AbortIncompleteMultipartUpload action. The first two actions are transition actions and expiration actions. Transition actions enable you to move data automatically between S3 storage classes. </p>
<p>Expiration actions enable you to automate the deletion of your objects in S3. For both transition and expiration actions, you can define when to move or delete these objects based on object age. And age is based on when the object was last created or modified. </p>
<p>Here’s an example of how both of these actions are structured in XML. In this example, the first action transitions all objects that are prefixed with ProjectBlue&#x2F; to the S3 Glacier Flexible Retrieval storage class 365 days after they were created. The second action says after 2,550 days - which is 7 years, delete the objects prefixed with ProjectBlue&#x2F; because they aren’t needed any longer. </p>
<p>If you have versioning turned on for your bucket, transition actions and expiration actions only work for the current version of your object. If you want to transition noncurrent versions of your object, you must use the NoncurrentVersionTransition action. </p>
<p>Similarly, if you want to delete noncurrent versions of your object, you must use the NoncurrentVersionExpiration action. For both the NoncurrentVersionTransition and NoncurrentVersionExpiration actions, you can define when to move or delete objects based on two things. </p>
<ul>
<li>First is the number of days since the object has been noncurrent, which Amazon calculates as the number of days since the object was overwritten or deleted. </li>
<li>And second, is the maximum number of versions to retain. This is helpful when you want to save a few versions to rollback to for data protection, while removing old versions of your object to save on storage spend.</li>
</ul>
<p>Here’s an example of a lifecycle configuration that uses both a noncurrent version transition and a noncurrent version expiration action. In this rule, all noncurrent versions are moved to S3 Standard - Infrequent Access 30 days after they become noncurrent. In addition, it deletes all noncurrent versions 730 days, or two years, after they become noncurrent, while retaining the 3 latest versions. Keep in mind, you can choose to retain any number of versions between 1 and 100. </p>
<p>There are additional special actions that you can take as well. One of them is the Expired object delete marker action. This is helpful if you have objects that have zero versions, that only have a delete marker left. This is referred to as the expired object delete marker, and you can use this action to remove them. </p>
<p>And last, there is the AbortIncompleteMultipartUpload action. If you have incomplete multipart uploads that you need to clean up, you should use this action. With this action, you can specify the maximum time, in days, that your multipart uploads can remain in progress. </p>
<p>For example, you can specify that your multipart uploads can remain in progress for 14 days. If the upload does not complete in 14 days, S3 will delete it. In summary, S3 lifecycle configurations are made up of an ID, filters, status, and actions. That’s all for this one!</p>
<h1 id="Creating-a-Lifecycle-Configuration-Demo"><a href="#Creating-a-Lifecycle-Configuration-Demo" class="headerlink" title="Creating a Lifecycle Configuration Demo"></a>Creating a Lifecycle Configuration Demo</h1><p>In this video, I’ll configure an S3 Lifecycle configuration using both the AWS Console and the AWS CLI. This S3 Lifecycle configuration will transition a subset of objects from an S3 bucket. </p>
<p>In this S3 bucket, I have a combination of cat photos and beach photos. I currently access my cat photos all of the time. But for my beach photos, I find I only access them frequently the first month. After that period, I still want to keep them in storage for a year just in case anyone asks me about my vacation. However, no one ever asks, so I might as well store them in a low cost storage class after that first 30 days. And after a year, I’ll take a new vacation, so I can just delete these old beach photos. Using this information, I can create a lifecycle configuration based on my known access patterns. </p>
<p>I’ll begin in the S3 console, and I’ll click on my bucket named lifecycle-configuration-demo-console. Inside my bucket, you can see my objects: a wide range of cat photos, and then I have a folder called beach. If I click on this folder, I can then see my beach photos. </p>
<p>To create a lifecycle configuration to move my beach photos, I can click on the management tab, and under lifecycle rules, I can click “create lifecycle rule”. </p>
<p>Here, I will enter the name, in this case I will call it BeachPhotosRule. Then I can choose to either limit it based on prefix or apply it to all photos. Since I want to leave my cat photos untouched, I will limit it based on a filter. And under prefix, I will type in my folder name beach&#x2F;. </p>
<p>Notice how I can choose to filter based on object tag, or minimum and maximum object size as well. However, for this demo, I’ll only be filtering by prefix. Moving down to the actions section, I can choose where my objects go. </p>
<p>In this case, I don’t have versioning turned on with this bucket and I’m not using multipart uploads, so I only need to select the option to transition current versions of objects and the option to expire current versions of objects. </p>
<p>To transition my beach photos, I can choose which storage class I’d like to move them to after 30 days. Since I’ll rarely be accessing my photos, an archival storage class is the best option. For this demo, I’ll choose Glacier Flexible Retrieval. And I’ll specify that this transition should happen 30 days after the object is created. </p>
<p>It’s giving me a warning saying I will incur a per object fee to transition my objects to Glacier, as well as an additional object metadata storage charge, since the S3 service adds 32 KB of storage for object metadata that’s used when restoring objects. Even though it will add on additional cost, this is fine with me, so I will check the box and move on. </p>
<p>Now I get to choose when to delete my objects. S3 Glacier Flexible Retrieval has a 90 day retention period, so as long as this period is longer than 90 days, I won’t incur an additional fee. Since I take my vacations yearly, I will choose 365 days to delete my photos. </p>
<p>And that is all I need to set up. If I weren’t talking through all the options, this would take no time at all. And the best part, is it generates a timeline that I can review to ensure I made all the right choices before clicking “create rule”. </p>
<p>Now, for those of you who don’t enjoy clicking around the AWS console: creating this using the CLI is just as easy. One thing about configuring lifecycle configurations using the CLI is that you have to provide a JSON template file defining your configuration. If you’re more comfortable or familiar with XML, you can always configure it in XML and use any one of the free XML to JSON converter websites to do the conversion for you. </p>
<p>However, I have already created a JSON file, and I’ll run through it quickly. I have the ID, or name of the rule which is BeachPhotosRule. Then I have the filter, which uses the prefix of my folder beach&#x2F;. The status is enabled. I have a transition to the S3 Glacier Flexible Retrieval storage class after 30 days. And I’m deleting my beach photos after 365 days. </p>
<p>As you can see, it’s the same exact lifecycle configuration. Going to my terminal, I can then use the following command. </p>
<p>And then dictate my file path for my lifecycle configuration, beginning with file:&#x2F;&#x2F; and then the file name. And then I can specify which bucket to apply this to, using –bucket, and then my bucket name which, in this case is lifecycle-configuration-demo-cli</p>
<p>And it looks like it was successful. To verify, I can go back to the S3 console, find my demo-cli bucket. Click on the management tab, click on the lifecycle configuration, and verify that the details are correct. And that’s it for this one - see you next time! </p>
<h1 id="Other-Considerations-and-Limitations"><a href="#Other-Considerations-and-Limitations" class="headerlink" title="Other Considerations and Limitations"></a>Other Considerations and Limitations</h1><p>For this video, I want you to think about the S3 storage classes as a staircase. S3 Standard storage is at the top of the staircase, while S3 Glacier Deep Archive is at the bottom of the staircase. And all of the other storage classes are in between. </p>
<p>With lifecycle configurations, this staircase only goes one way: down. Once you transition data down the staircase to a lower-cost storage class, you can’t move objects back up. For example, let’s say I move my data to S3 Standard-Infrequent Access. Once my data transitions to that storage class, I can’t use a lifecycle configuration to move my data back to S3 Standard. This is also true if I move my data to S3 One Zone - IA, I can’t move it back to S3 Intelligent-Tiering, S3 Standard-IA, or S3 Standard. </p>
<p>This becomes important if I’m using archival storage. If I reach the bottom of the staircase, by moving my data to the lowest cost storage class, S3 Glacier Deep Archive, I can’t transition my data back to any other storage tier. </p>
<p>Lifecycle Configuration costs follow a similar staircase model. I categorize these costs in two ways: minimum storage duration fees, and storage transition costs. Both of which increase as you move down the staircase. </p>
<p>For example, let’s take storage transition costs. You get charged when you move data to other storage classes and this fee increases as you move down the staircase. For example, at the top of the staircase, you’re charged $0.01 for every 1,000 lifecycle transition requests when objects are moved from S3 Standard to the S3 Standard-IA storage class. </p>
<p>As you go down the staircase, all the way to S3 Glacier Deep Archive, this cost increases, and can be up to $0.05 for every 1000 transition requests. </p>
<p>While it might not seem like a huge cost, it can stack up over time, especially if you’re consistently moving data to archival storage. For example, if you need to transition millions of small objects to archival storage, that transition cost can be very high. To minimize this cost, you should consider transitioning mostly large objects that need to be retained over long periods of time. You can also consider aggregating several small objects into one large object to save on this fee as well. </p>
<p>The second cost factor related to lifecycle configurations is minimum storage duration fees. Most storage classes have a minimum storage duration that requires you to keep data in a storage class for a certain period of time before you delete, overwrite, or transition those objects. </p>
<p>These minimum storage duration periods increase as you go down the staircase as well. For example, S3 Standard and S3 Intelligent-Tiering have no minimum storage duration. Infrequent access tiers like S3 Standard-IA and S3 One Zone - IA have a minimum storage duration of 30 days. Archival storage tiers like S3 Glacier Instant Retrieval and S3 Glacier Flexible Retrieval have a minimum storage duration of 90 days, while S3 Glacier Deep Archive has a minimum storage duration of 180 days.</p>
<p>So what happens if you delete or overwrite these objects before the minimum storage duration is reached? You get charged. For example, say you transition an object into S3 Glacier Deep Archive for 30 days, and then delete it. In this case, you will still be charged for the full 180 days of storage. </p>
<p>So when you’re setting up your lifecycle configurations, ensure that you’re keeping the limitations and costs in mind. That’s all for this one! See you next time!</p>
<h1 id="What-is-the-Amazon-Elastic-File-System"><a href="#What-is-the-Amazon-Elastic-File-System" class="headerlink" title="What is the Amazon Elastic File System?"></a>What is the Amazon Elastic File System?</h1><p>Hello and welcome to this lecture where I will explain what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-file-system-1137/course-introduction/">Amazon EFS</a> service is and how it fits into the storage ecosystem. Let me start by taking a step back and looking at where the EFS service fits in within the world of AWS storage. Firstly, I want to look at the array of AWS storage offerings and compare a few of them. <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> has more storage solutions than I’m going to discuss in this course and I will probably continue to add more in the future. But I’m just going to focus on three different services. The reason I’ve selected these three is that, at first glance, they may seem similar and many people can be unsure which of these solutions to choose from to fit their current storage requirements. </p>
<p>Amazon Simple Storage Service or S3 is an object storage solution. Object storage stores everything as a single object, not in small chunks or blocks. With this type of storage, you upload a file and if the file changes to replace it, the entire file will be replaced. This type of storage is best for situations where files are written once and then accessed many times. It’s not optimal for situations that require both heavy read and write access at the same time. So Amazon S3 is usually used for storage of large files such as video files, images, static websites, and backup archives. For example, Netflix uses S3 for their data streaming service. They upload large movie files once and then subscribers access and play the movies many, many times. </p>
<p>The next service is the Amazon Elastic Block Store or EBS, and it’s block-level storage. Files are not stored as single objects. They’re stored in small chunks of blocks so that only the portion of the file that is changed will be updated. This type of storage is optimized for low latency access and when fast, concurrent read and write operations are needed. EBS provides persistent block storage volumes for use with a single EC2 instance. As described, EBS is persistent, meaning that even if you stop or terminate an EC2 instance that’s using EBS, the data on the EBS volume remains intact. You should use this type of storage like a computer hard drive where you store operating system files, applications and other files you wish to obtain for use with your EC2 instance.</p>
<p>Amazon Elastic File System, or EFS, is considered file-level storage and is also optimized for low latency access, but unlike EBS, it supports access by multiple EC2 instances at once. It appears to users like a file manager interface and uses standard file system semantics such as locking files, renaming files, updating files and uses a hierarchy structure. This is just like what we’re used to on standard premise-based systems. This type of storage allows you to store files that are accessible to network resources. </p>
<p>Before diving deep on EFS, let me discuss how people are traditionally used to accessing network files and resources. In traditional premises-based networks, users access files by browsing network resources that connect to a server, perhaps via a mapped drive that has been configured for them, and once they connect, they will see a tree view of available folders and files. This functionality is generally provided by various local area network systems such as file servers or storage area network, a SAN, or network-attached storage, a NAS. </p>
<p>Now let’s move on from the traditional premises-based solutions and talk about cloud-based solutions, specifically within AWS and the Amazon Elastic File System service. EFS provides simple, scalable file storage for use with Amazon EC2 instances. Much like traditional file servers, or a SAN or a NAS, Amazon EFS provides the ability for users to browse cloud network resources. EC2 instances can be figured to access Amazon EFS instances using configured mount points. Now, mount points can be created in multiple availability zones that attach to multiple EC2 instances. So, much like your traditional land servers, EC2 instances are connected to a network file system, Amazon EFS. So from a user standpoint, the result is the same. The user accesses network resources just as they always have done except for now, it’s done using cloud resources. </p>
<p>EFS is a fully managed, highly available and durable service that allows you to create shared file systems that can easily scale to petabytes in size with low latency access. EFS has been designed to maintain a high level of throughput in addition to low latency access response, and these performance factors make EFS a desirable storage solution for a wide variety of workloads, and use cases and can meet the demands of tens, hundreds or even thousands of EC2 instances concurrently. Being a managed service, there is no need for you to provision any file servers to manage the storage elements or provide any maintenance of those servers. This makes it a very simple option to provide file-level storage within your environment. It uses standard operating system APIs, so any application that is designed to work with standard operating system APIs will work with EFS. It supports both NFS versions 4.1 and 4.0, and uses standard file system semantics such as strong consistency and file locking. It’s replicated across availability zones in a single region making EFS a highly reliable storage service. </p>
<p>As the file system can be accessed by multiple instances, it makes it a very good storage option for applications that scale across multiple instances allowing for parallel access of data. The EFS file system is also regional, and so any application deployments that span across multiple availability zones can all access the same file systems providing a level of high availability of your application storage layer. At the time of writing this course, EFS is not currently available within all regions. For a list of supported regions, please visit the following link: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region">https://docs.aws.amazon.com/general/latest/gr/rande.html#elasticfilesystem-region</a>. </p>
<p>That now brings me to the end of this lecture.</p>
<h1 id="Storage-Classes-and-Performance-Options"><a href="#Storage-Classes-and-Performance-Options" class="headerlink" title="Storage Classes and Performance Options"></a>Storage Classes and Performance Options</h1><p>Hello and welcome to this lecture, where I will be discussing the different storage class options that EFS provides in addition to how you can alter and configure certain performance factors depending on your use case of EFS. Now, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/introduction/">Amazon EFS</a> offers two different storage classes to you, which each offer different levels of performance and cost, these being Standard and Infrequent Access, known as IA. The Standard storage class is the default storage used when using EFS. However, Infrequent Access is generally used if you’re storing data on EFS that is rarely accessed. By selecting this class, it offers a cost reduction on your storage. </p>
<p>The result of the cheaper storage means that there is an increased first-byte latency impact when both reading and writing data in this class when compared to that of Standard. The costs are also managed slightly differently. When using IA, you are charged for the amount of storage space used, which is cheaper than that compared to Standard. However, with IA, you are also charged for each read and write you make to the storage class. This helps to ensure that you only use this storage class for data that is not accessed very frequently, for example, data that might be required for auditing purposes or historical analysis. </p>
<p>With the Standard storage class, you are only charged on the amount of storage space used per month. Both storage classes are available in all regions where EFS is supported. And importantly, they both provide the same level of availability and durability. If you are familiar with S3, then you may also be familiar with S3 lifecycle policies for data management. Within EFS, a similar feature exists known as EFS lifecycle management. When enabled, EFS will automatically move data between the Standard storage class and the IA storage class. This process occurs when a file has not been read or written to for a set period of days, which is configurable, and your options for this period range include 14, 30, 60, or 90 days. </p>
<p>Depending on your selection, EFS will move the data to the IA storage class to save on cost once that period has been met. However, as soon as that same file is accessed again, the timer is reset, and it is moved back to the Standard storage class. Again, if it has not been accessed for a further period, it will then be moved back to IA. Every time a file is accessed, its lifecycle management timer is reset. The only exceptions to data not being moved to the IA storage class is for any files that are below 128K in size and any metadata of your files, which will all remain in the Standard storage class. </p>
<p>If your EFS file system was created after February 13th, 2019, then the life cycle management feature can be switched on or off. Let me now take a look at the different performance modes that EFS offers. AWS are where the EFS can be used for a number of different use cases and workloads, and as such, each use case might require a change of performance from a throughput, IOPS, and latency point of view. As a result, AWS has introduced two different performance modes that can be defined during the creation of your EFS file system. These being General Purpose, and Max I&#x2F;O.</p>
<p>Now, General Purpose is a default performance mode and is typically used for most use cases. For example, home directories and general file-sharing environments. It offers an all-round performance and low latency file operation, and there is a limitation of this mode allowing only up to 7,000 file system operations per second to your EFS file system. If, however, you have a huge scale architecture, where your EFS file system is likely to be used by many thousands of EC2 instances concurrently, and will exceed 7,000 operations per second, then you’ll need to consider Max I&#x2F;O. Now, this mode offers virtually unlimited amounts of throughput and IOPS. The downside is, however, that your file operation latency will take a negative hit over that of General Purpose. </p>
<p>The best way to determine which performance option that you need is to run tests alongside your application. If your application sits comfortably within the limit of 7,000 operations per second, then General Purpose will be best suited, with the added plus point of lower latency. However, if your testing confirms 7,000 operations per second may be reached or exceeded, then select Max I&#x2F;O.</p>
<p>When using the General Purpose mode of operations, EFS provides a CloudWatch metric percent I&#x2F;O limit, which will allow you to view operations per second as a percentage of the top 7,000 limit. This allows you to make the decision to migrate and move to the Max I&#x2F;O file system, should your operations be reaching that limit. </p>
<p>In addition to the two performance modes, EFS also provides two different throughput modes, and throughput is measured by the rate of mebibytes. The two modes offered are Bursting Throughput and Provisioned Throughput. Data throughput patterns on file systems generally go through periods of relatively low activity with occasional spikes in burst usage, and EFS provisions throughput capacity to help manage this random activity of high peaks.</p>
<p>With the Bursting Throughput mode, which is the default mode, the amount of throughput scales as your file system grows. So the more you store, the more throughput is available to you. The default throughput available is capable of bursting to 100 mebibytes per second, however, with the standard storage class, this can burst to 100 mebibytes per second per tebibyte of storage used within the file system.</p>
<p>So, for example, presume you have five tebibytes of storage within your EFS file system. Your burst capacity could reach 500 mebibytes per second. The duration of throughput bursting is reflected by the size of the file system itself. Through the use of credits, which are accumulated during periods of low activity, operating below the baseline rate of throughput set at 50 mebibytes per tebibyte of storage used, which determines how long EFS can burst for. Every file system can reach its baseline throughput 100% of the time. By accumulating, getting credits, your file system can then burst above your baseline limit. The number of credits will dictate how long this throughput can be maintained for, and the number of burst credits for your file system can be viewed by monitoring the CloudWatch metric of BurstCreditBalance. </p>
<p>If you are finding that you’re running out of burst credits too often, then you might need to consider using the Provisioned Throughput mode. Provisioned Throughput allows you to burst above your allocated allowance, which is based upon your file system size. So if your file system was relatively small but the use case for your file system required a high throughput rate, then the default bursting throughput options may not be able to process your request quick enough. In this instance, you would need to use provisioned throughput. However, this option does incur additional charges, and you’ll pay additional costs for any bursting above the default option of bursting throughput. That brings me to the end of this lecture, now I want to shift my focus on creating and connecting to an EFS file system from a Linux based instance.</p>
<h1 id="Creating-an-EFS-File-System"><a href="#Creating-an-EFS-File-System" class="headerlink" title="Creating an EFS File System"></a>Creating an EFS File System</h1><p>Hello and welcome to this lecture, which will be a demonstration on how to create and mount your <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/introduction/">EFS file system</a> from within the AWS Management Console. But before I show you the demonstration, I just want to discuss the mounting process first, just to make it a little bit easier to follow as we go through the demonstration. </p>
<p>EFS offers two methods to connect your Linux-based EC2 instance to your EFS file system. Both use a process called mounting whereby you mount a target to the EFS file system on your instance. The original method available with EFS used the standard Linux NFS client to perform the mount. Since then, a new method has been developed, and this newer method is now the preferred option, and this uses the EFS mount helper. </p>
<p>For the remainder of this lecture, I shall be focusing on mounting your EFS file system via the EFS mount helper. For more information on how to mount your file system using the Linux NFS client, please see the following link: <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-old.html">https://docs.aws.amazon.com/efs/latest/ug/mounting-fs-old.html</a></p>
<p>The EFS mount helper is a utility that has to be installed on your EC2 instance. This utility has been designed to simplify the entire mount process by using predefined recommended mounting options that are commonly used within the NFS client. It also provides built-in login capabilities to help with any troubleshooting that might be required and are stored in the following location: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/var/log/amazon/efs</span><br></pre></td></tr></table></figure>

<p>In addition to mounting an EFS file system to running instances, you can also use the EFS mount helper to automatically connect to EFS during the boot process, as well as by editing the &#x2F;etc&#x2F;fstab configuration file. Before using the EFS mount helper to connect to your EFS file system from your EC2 instances, there are a couple of prerequisites required to be in place. First and foremost, you need to ensure that you have created and configured your EFS file system, in addition to your EFS mount targets. You must have an EC2 instance running with the EFS mount helper installed, and this instance will be used to connect to the EFS file system.</p>
<p>The instance must also be in the VPC and configured to use the Amazon DNS servers with DNS hostnames enabled. You must have a security group configured allowing the NFS file system NFS access to your Linux instance, and you must also be able to connect to your Linux instance. I now want to provide a quick demonstration on how to create an EFS file system from within the AWS Management Console. During the demonstration, I’ll cover configuration points related to the following: EC2 security groups, mount targets, lifecycle management, throughput mode, performance mode, and encryption. And once our EFS file system’s created, I’ll then show you how to mount EFS using the EFS mount helper. </p>
<p>Let’s take a look. Just before I dive into the demonstration, I just wanna show the infrastructure that I’ve already pre-built for this. So I’ve created my VPC already, and within this VPC, I have a public subnet with an Internet gateway attached. And inside this public subnet, I have two instances, one here and one here. And I’ll be using these in the demonstration just to show you how EFS is working. </p>
<p>Now, I’ve also created a security group that associates these two instances that allows me to click to these instances using SSH. So it’s a very simple setup, just a VPC with a public subnet, with a couple of instances running in them with a security group associated to them, allowing me SSH access. Now, over here, we have EFS itself, and during the demonstration, I’m going to create another security group for EFS. </p>
<p>Now, this security group is going to allow NFS access from these two instances into my EFS mount points that I’ll be creating. So that would be the first part of the demonstration of creating this new security group, allowing my instances to write to EFS using the NFS protocol. Okay, let’s get started. </p>
<p>Okay, so I’m at the AWS Management Console, and as already discussed, I have my two EC2 instances created with the security group allowing me to SSH to them, but now I need to create a security group for EFS, that needs to be associated with the mount points to allow the EC2 instances to write to the EFS file system. </p>
<p>So let’s go ahead and create that now. So if I go to EC2 to my security groups, and down the left side here, click on security groups, now I need to create my security group by clicking on the Create Security Group button. Let’s call this my EFS security group, and the same for the description, just make sure I have the right VPC selected. Now, for the inbound rule, I need to click on Add Rule, and for the type, if I scroll down to NFS and leave the default port range as 2049. Now the source needs to be the EC2 instances that will be writing to the file system. I already have a security group created that both of my EC2 instances are associated with so I can just select that security group there. Alternatively, you could have selected the IP addresses or the IP network range that the EC2 instances are associated with. </p>
<p>Once you’ve configured that, simply click on Create. Okay, so now we have our security group created, now we can go ahead and create our elastic file system. So if we go to Services, type in EFS, now, if you’ve not used EFS before, this will just create a splash screen. So firstly, we need to create a file system by clicking on the blue button. And as you can see, there are three different steps to creating your file system. </p>
<p>Firstly, we need to configure the file system access. And we can see here that it says, “An Amazon EFS file system is accessed by EC2 instances running inside one of your VPCs. Instances connect to your file system using a network interface called a mount target. Each mount target has an IP address, which we (AWS) will assign automatically or you can simply specify your own.” </p>
<p>So firstly, I need to select the VPC that I want this to be associated with, and then down here, we can select our mount targets. Now, for this demonstration, I’m just going to be using the EC2 instances that are within my public subnet, but if you have multiple availability zones across your region, then you can create a mount target within each one, but like I say, for this demonstration, I’m just going to be using the public subnet. </p>
<p>Now, here the security group, this is where you want to add the security group that I just created, and I named that EFS Security Group, so here it is here. So as per this security group, this mount target will allow inbound NFS traffic from the source specified in the security group, which were my EC2 instances. So that allows those EC2 instances to write to the EFS file system. Click on Next. </p>
<p>Now, we’re on to Step Two, where we can configure optional settings. You can add tags if you wish, so we have a name, so we can call this My_EFS. Lifecycle management, which, as I discussed in the previous lecture, we can select 14 days, 30, 60 or 90. Let’s just select 30 for this demonstration. So what that means is any files that are not accessed for 30 days will be moved to the Infrequent Access storage class to save on cost. And then as soon as they are accessed again, then it will be moved back into the Standard storage class. Here we have our two throughput modes, Bursting or Provisioned. </p>
<p>If you select Provisioned, then you can enter your range of throughput in there. For this demonstration, I’m just going to leave it as Bursting. Then we have our performance mode, General Purpose, or Max I&#x2F;O. Again, for this demonstration, we’re just gonna leave it the default General Purpose. And then finally, we can enable encryption at rest, simply by selecting this tickbox.  And as you can see, EFS has selected this default KMS master key. And as we can see, this is the default master key that will be used by EFS. If we had our own master key that we wanted to select, then they’ll be within that list there, we can select an alternate one, or if you want to use a KMS from a different account, then you can simply enter the ARN&#x2F;ID there. For this demonstration, let’s just leave it as the default KMS master key that EFS has selected, then click on Next Step. </p>
<p>Now, this is the final step where we can review and create our EFS file system. As we can see, we just review the options we’ve selected, make sure we have the right availability zones selected, the security group, and also optional settings as well. Once you’re happy, simply scroll down and click on Create File System. </p>
<p>And there we have it, success. So our EFS file system is now created. And we can see here this is the name we gave it, the file system ID, account size, and the number of mount targets, which is just one, because we just wanted it to be available within our public subnet. The different options that we selected during its creation, and down here we can see that the mount target is currently being created. We give that a refresh to see if it’s been done now. Unfortunately, that’s still creating. So whilst that’s still creating, let me just show you something else. </p>
<p>Over here we have our DNS name of our file system as well. And here AWS gives you some instructions on how to mount your EC2 instances to your newly created EFS file system. So let’s take a look at this because this is what we’re going to be doing, we’re going to be mounting EC2 from our local VPC. So the first couple of steps just explain that you need to create your EC2 instances and set up the relevant security groups, allowing you to connect to your client using SSH, etc. </p>
<p>The next step relates to the EFS mount helper, which is what I discussed previously. And here we have the command to install the EFS mount helper. Okay, so let me now just flip across to my two instances, and we’ll install the EFS mount helper using this command here. So if I just copy that. Now, here I have my two instances, I’ve just changed the colors just to make it easier to follow. </p>
<p>So on the yellow one, let’s just change to superuser, we’ll paste in that command, which will install the EFS mount helper, and that’s done, and then we’ll just do the same on this one as well. So now we have the EFS mount helper installed on both of my instances. So if we go back to EFS, we can check the next step. We didn’t have to worry about these two commands here because this is only used if we’re not using the EFS mount helper, but we are, so we don’t have to worry about that. </p>
<p>Now, this section shows how to actually mount your file system. Firstly, we need to create a directory that will be associated with EFS. Now, you can name this directory anything you want. In this example, they’re just using EFS, so let’s just use the same. So let’s create a directory for both of these. So on that instance, and also the green instance. So now we have our directories created. If we go back to EFS, now, the final command is actually the mounting of the EFS itself to that new directory. So we have two commands here we can use. So we have this one at the top here, which we’ll use the EFS mount helper to mount our newly created EFS file system, or if we want to add TLS encryption, then we can use this option here.</p>
<p>For this demonstration, I’m just going to use the top one. So what this will do, it will mount our newly created file system to the directory that we just created. So let me go back to our two instances. Just paste in that command, and again, on the green instance, and that’s now done. So what I have now is two EC2 instances with the EFS mount helper installed, we’ve created an EFS directory, and we’ve mounted our mount target from our file system to be associated with that EFS directory. </p>
<p>Now, let me just clear the screen in both of these to make it a little clearer, and we’ll test this out. So on the yellow instance, let’s change to the EFS directory, and let’s create a new file. So I’m just gonna call this stu.txt. And I’m just going to create a file saying, “This is created on the yellow instance.” And if I escape, colon and quit. So now I’ve created that text file on the yellow instance. In theory, the green instance should be able to see that same text file because it’s associated to the same EFS mount point. So if I change to the EFS directory here, and then list the files, we can see that we have the stu.txt file. And that’s because both of the EFS directories on each EC2 instance is now associated to my EFS mount target.</p>
<p>Now what we can do from this green instance is we can edit that file. So if we go into the file, and as we can see, this is the file that we created on the yellow instance. Now, here I can just say, “This has been updated by the green instance.” And again, if I escape out of that and quit, and if I go back over to the yellow instance, and take a look at that same file, we should see that it’s been updated. And there you have it! So that shows that this file is being stored on EFS and not locally on each of your two instances. So it’s a shared location, and all we’ve done is simply created a new directory on each of those instances, and associated the EFS file server with each of those directories. And it’s as simple as that. Thank you.</p>
<h1 id="Managing-EFS-Security"><a href="#Managing-EFS-Security" class="headerlink" title="Managing EFS Security"></a>Managing EFS Security</h1><p>Hello, and welcome to this lecture dedicated to the security of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/introduction/">Elastic File System</a>, where I shall be looking at access control and the permissions required to both operate and create your EFS file system. I will also dive into encryption, as this topic is always of importance when storing data, and so if your data is of a sensitive nature, then I’ll explain how EFS manages data encryption for you. Of course, there are other elements of security that are touched on in the previous lecture, where I covered the necessary security groups that need to be in place. </p>
<p>Before you can create and manage your EFS file system, you need to ensure that you have the correct permissions to do so. To initially create your EFS file system, you need to ensure that you have allow access for the following services:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">elasticfilesystem:CreateFileSystem</span><br><span class="line">elasticfilesystem:CreateMountTarget   </span><br><span class="line">ec2:DescribeSubnet</span><br><span class="line">ec2:CreateNetworkInterface</span><br><span class="line">ec2:DescribeNetworkInterfaces</span><br></pre></td></tr></table></figure>

<p>As you can see, there are five permissions, two of which relate to EFS, and three relate to EC2. The EFS permissions allow you to create your file system in addition to any mount targets that are required. The EC2 permissions are required to allow actions carried out by the CreateMountTarget action. When applying these permissions to your policies, the resource for the elastic file system actions will point to the following resource:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Resource&quot;: &quot;arn:aws:elasticfilesystem:us-west-2:account-id:file-system/*</span><br></pre></td></tr></table></figure>

<p>where your AWS account ID should replace the text in red (“account-id”). A resource is not required for the EC2 actions and, as a result, the value will be represented via a wildcard. The below shows the full example of what this policy should look like.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">  &quot;Statement&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;Sid&quot; : &quot;PermissionToCreateEFSFileSystem&quot;,  </span><br><span class="line">      &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">      &quot;Action&quot;: [</span><br><span class="line">        &quot;elasticfilesystem:CreateFileSystem&quot;,</span><br><span class="line">        &quot;elasticfilesystem:CreateMountTarget&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;Resource&quot;: &quot;arn:aws:elasticfilesystem:region-id:file-system/*&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">     &quot;Sid&quot; : &quot;PermissionsRequiredForEC2&quot;,</span><br><span class="line">      &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">      &quot;Action&quot;: [</span><br><span class="line">        &quot;ec2:DescribeSubnets&quot;,</span><br><span class="line">        &quot;ec2:CreateNetworkInterface&quot;,</span><br><span class="line">        &quot;ec2:DescribeNetworkInterfaces&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;Resource&quot;: &quot;*&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<p>In addition to these policies, you’ll also need the following permissions to manage EFS using the AWS management console:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">  &quot;Statement&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;Sid&quot; : &quot;Stmt1AddtionalEC2PermissionsForConsole&quot;,  </span><br><span class="line">      &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">      &quot;Action&quot;: [</span><br><span class="line">        &quot;ec2:DescribeAvailabilityZones&quot;,</span><br><span class="line">        &quot;ec2:DescribeSecurityGroups&quot;,</span><br><span class="line">        &quot;ec2:DescribeVpcs&quot;,</span><br><span class="line">        &quot;ec2:DescribeVpcAttribute&quot;</span><br><span class="line"> </span><br><span class="line">      ],</span><br><span class="line">      &quot;Resource&quot;: &quot;*&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    &#123;</span><br><span class="line">     &quot;Sid&quot; : &quot;Stmt2AdditionalKMSPermissionsForConsole&quot;,</span><br><span class="line">      &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">      &quot;Action&quot;: [</span><br><span class="line">        &quot;kms:ListAliases&quot;,</span><br><span class="line">        &quot;kms:DescribeKey&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;Resource&quot;: &quot;*&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>These permissions allow the console to view EFS resources, query EC2, allowing it to display VPCs, availability zones, and security groups, and enable KMS actions if encryption is enabled on the EFS file system. For more information on creating IAM policies in addition to roles, groups, and users, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/">here</a>.</p>
<p>If your data contains sensitive information or if your organization has specific policies regarding the protection of data, requiring the implementation of encryption, then you need to be aware of how EFS handles its process. EFS supports both encryption at rest and in transit. Let’s take a look at how both of these are achieved. </p>
<p>Encryption at rest. You may remember during the demonstration I gave earlier when I created an elastic file system that there was a checkbox for encrypting the file system. This checkbox enables you to create an EFS file system that maintains encryption at rest. This uses another AWS service, the key management service known as KMS, to manage your encryption keys. As you can see in the image, a KMS master key, a CMK, is required. A customer master key is the main key type within KMS. This key can encrypt data of up to four kilobytes in size, however, it is typically used in relation to your data encryption keys. The CMK can generate, encrypt, and decrypt these data encryption keys, which are then used outside of the KMS service by other AWS services to perform encryption against your data, for example, EFS. </p>
<p>It’s important to understand there are two types of customer master keys. Firstly, those which are managed and created by you and I as customers of AWS, which can either be created using KMS, or by importing key material from existing key management applications into a new CMK, and secondly, those that are managed and created by AWS themselves. In the example in the image, the CMK selected is an AWS managed master key. </p>
<p>The CMKs which are managed by AWS are used by other AWS services that have the ability to interact with KMS directly to perform an encryption against data, for example EFS, to perform its encryption at rest across its file systems. These AWS managed keys can only be used by the corresponding AWS service that created them within the particular region, as KMS is a regional service. These CMKs that are used by the services are generally created the first time you implement encryption using that particular service. For more information on KMS and how it works, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/">here</a>.</p>
<p>Encryption in transit. If you need to ensure your data remains secure between the EFS file system and your end client, then you need to implement encryption in transit. The encryption is enabled through the utilization of the TLS protocol, which is transport layer security, when you perform your mounting of your EFS file system. The best way to do this is to use the EFS mount helper as I did earlier in a previous demonstration. The command used to implement the use of TLS for in-transit encryption is as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mount -t efs  -o tls fs-12345678:/ /mnt/efs</span><br></pre></td></tr></table></figure>

<p>This will ensure that the mount helper creates a client stunnel process using TLS version 1.2. <em>‘Stunnel is an open-source multi-platform application used to provide a universal TLS&#x2F;SSL tunneling service. Stunnel can be used to provide secure encrypted connections for clients or servers that do not speak TLS or SSL natively.’</em> <em>(Wikipedia.)</em> This stunnel process is used to listen out for any traffic, using NFS, which it then redirects to the client stunnel process. That brings me to the end of this lecture. Next, I’ll be focusing on how to import data into your EFS file system.</p>
<h1 id="Importing-Data"><a href="#Importing-Data" class="headerlink" title="Importing Data"></a>Importing Data</h1><p>Hello and welcome to this short lecture, which will provide a high-level overview of how you can import on-premise data into EFS. If you’re looking to use EFS, then it’s likely that you already have a dataset in mind that you think would be better served using this service. For example, your home directories for your employees that might be served on local file servers on-premises. So how would you go about moving this data into EFS to use the benefits that it offers? </p>
<p>The recommended course of action is to use another service called AWS DataSync. This service is specifically designed to help you securely move and migrate and synchronize data for your existing on-premises site into AWS Storage Services such as <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-amazon-efs-to-create-elastic-file-systems-for-linux-based-workloads/introduction/">Amazon EFS</a> or Amazon S3 with simplicity and ease. The data transfer can either be accomplished over a direct connect link or over the internet. To sync source files from your on-premises environment, you must download the DataSync agent as a VMware ESXi to your site. The agent is configured with the source and destination target and associated with your AWS account, and logically sits in between your on-premise file system and your EFS file system. </p>
<p>DataSync is also very useful if you want to transfer files between EFS file systems either within the same AWS account or cross-account and owned by a third-party. To help with the management and implementation of this transfer, AWS has created an AWS DataSync in-cloud quick start and scheduler, which can be found <a target="_blank" rel="noopener" href="https://github.com/aws-samples/amazon-efs-tutorial/tree/master/in-cloud-transfer">here</a> on GitHub.</p>
<p>Within the overview documentation of this tool, the following use cases are provided. You can migrate an NFS file system from Amazon EC2 to Amazon EFS within the same AWS region. Replicate an NFS file system from Amazon EC2 in one AWS region to an Amazon EFS file system in a different AWS region for disaster recovery. You can migrate an Amazon EFS file system from EFS standard with no lifecycle management to an EFS file system with lifecycle management enabled.</p>
<p>File systems with lifecycle management enabled will automatically move to a lower-cost Infrequent Access storage class based on a predefined lifecycle policy. You can migrate an Amazon EFS file system from one performance mode to another performance mode within the same AWS region, and replicate an Amazon EFS file system from one AWS region to another Amazon EFS file system in a different AWS region for disaster recovery. The configuration and implementation of the AWS DataSync tool is out of scope of this course. However, you can find more information on this service <a target="_blank" rel="noopener" href="https://aws.amazon.com/datasync/">here</a>. </p>
<p>That brings me to the end of this lecture. </p>
<h1 id="EC2-Instance-Storage"><a href="#EC2-Instance-Storage" class="headerlink" title="EC2 Instance Storage"></a>EC2 Instance Storage</h1><p>Hello, and welcome to this lecture covering EC2 Instance Level Storage. Which is referred to as an instance store volume. The first point to make about EC2 instance store volumes, is that the volumes physically reside on the same host that provides your EC2 instance itself, acting as local disc drives, allowing you to store data locally to that instance. Up until now within this course we have discussed persistent storage options. But instance store volumes provide ephemeral storage for you EC2 instances. </p>
<p>Ephemeral storage means that the block level storage that it provides offers no means of persistency. Any data stored on these volumes is considered temporary. With this in mind, it is not recommended to store critical or valuable data on these ephemeral instance store volumes, as it could be lost, should an event occur. By an event, let me explain under what conditions that your data would be lost, should it be stored on one of these volumes. </p>
<p>If your instance is either stopped or terminated, then any data that you have stored on that instance store volume associated with this instance will be deleted without any means of data recovery. However, if your instance was simply rebooted, your data would remain intact. Although, you can control when your instances are stopped or terminated, giving you the opportunity to either back-up the data or move it to another persistent volume store, such as the elastic block store service. Sometimes this control is not always possible. Let’s consider you had critical data stored on an ephemeral instance store volume and then the underlying host that provided your EC2 instance and storage failed. You had no warning that this failure was going to occur, and as a result of this failure, the instance was stopped or terminated. Now all of your data on these volumes is lost. When a stop and start, or termination occurs, all the blocks on the storage volume are reset, essentially wiping data. So, you might be thinking, why use these volumes? What use do they have if there is a chance that you are going to lose data? They do, in fact, have a number of benefits. </p>
<p>From a cost perspective, the storage used is included in the price of the EC2 instance. So, you don’t have an additional spend on storage cost. The I&#x2F;O speed on these volumes can far exceed those provided by the alternative instance block storage, EBS for example. When using store optimized instance families, such as the I3 Family, it’s potentially possible to reach 3.3 million random read IOPS, and 1.4 million write IOPS. With speeds like this, it makes it ideal to handle the high demands of no SQL databases. However, any persistent data required would need to be replicate or copied to a persistent data store in this scenario. Instance store volumes are generally used for data that is frequently changing; that doesn’t need to be retained, as such, they are great to be used as a cache or buffer. They are also commonly used for service within a load balancing group, where data is replicated across the fleet such as a web server pool. </p>
<p>Not all instance types support instance store volumes. So, if you do have a need where these instance store volumes would work for your use case, then be sure to check the latest AWS documentation to ascertain if the instance type you’re looking to use supports the volume. The size of your volumes, however, will increase as you increase the EC2 instance size. </p>
<p>From a security stance, instance store volumes don’t offer any additional security features. As to be honest, they are not separate service like the previous storage options I have already explained. They are simply storage volumes attached to the same host on the EC2 instance, and they are provided as a part of the EC2 service. So, they effectively have the same security mechanisms provided by EC2. This can be IAM policies dictating which instances can and can’t be launched, and what action you can perform on the EC2 instance, itself. If you have data that needs to remain persistent, or that needs to be accessed and shared by others, then EC2 instance store volumes are not recommended. If you need to use block level storage and want a quick and easy method to maintain persistency, then there is another block level service that is recommended. This being the elastic block store service.</p>
<h1 id="Amazon-Elastic-Block-Store-EBS"><a href="#Amazon-Elastic-Block-Store-EBS" class="headerlink" title="Amazon Elastic Block Store (EBS)"></a>Amazon Elastic Block Store (EBS)</h1><p>In this lecture, I shall be talking about the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-block-store-ebs-1060/course-introduction/">Amazon Elastic Block Store</a> service, known as EBS, which provides storage to your EC2 instances via EBS volumes, which offer different benefits to that of instance store volumes used with some EC2 instances.</p>
<p>EBS provides persistent and durable block level storage. As a result, EBS volumes offer far more flexibility with regards to managing the data when compared to data stored on instance store volumes. EBS volumes can be attached to your EC2 instances, and are primarily used for data that is rapidly changing that might require a specific Input&#x2F;Output operations Per Second rate, also known as IOPS.</p>
<p>EBS volumes are independent of the EC2 instance, meaning that they exist as two different resources. They are logically attached to the instance instead of directly attached like instance store volumes. From a connectivity perspective, only a single EBS volume can only ever be attached to a single EC2 instance. However, multiple EBS volumes can be attached to a single instance.</p>
<p>Due to the EBS ability to enforce persistence of data, it doesn’t matter if your instances are intentionally or unintentionally stopped, restarted, or even terminated, the data will remain intact when configured to do so. EBS also offers the ability to provide point in time backups of the entire volume as and when you need to. These backups are known as snapshots and you can manually invoke a snapshot of your volume at any time, or use Amazon CloudWatch events to perform an automated schedule of backups to be taken at a specific date or time that can be recurring.</p>
<p>The snapshots themselves are then stored on Amazon S3 and so are very durable and reliable. They are also incremental, meaning that each snapshot will only copy data that has changed since the previous snapshot was taken. Once you have a snapshot of an EBS volume, you can then create a new volume from that snapshot. So, if for any reason you lost access to your EBS volume through or incident or disaster, you can recreate the data volume from an existing snapshot and then attach that volume to a new EC2 instance. To add additional flexibility and resilience, it is possible to copy a snapshot from one region to another.</p>
<p>Looking at the subject of high availability and resiliency, your EBS volumes are, by default, created with reliability in mind. Every write to a EBS volume is replicated multiple times within the same availability zone of your region to help prevent the complete loss of data. This means that your EBS volume itself is only available in a single availability zone. As a result, should your availability zone fail, you will lose access to your EBS volume. Should this occur, you can simply recreate the volume from your previous snapshot and attach it to another instance in another availability zone.</p>
<p>There are two types of EBS volumes available. Each have their own characteristics. These being SSD backed storage, solid state drive, and HDD backed storage, hard disk drive. This allows you to optimize your storage to fit your requirements from a cost to performance perspective.</p>
<p>SSD backed storage is better suited for scenarios that work with smaller blocks. Such as databases using transactional workloads. Or often as boot volumes for your EC2 instances. Whereas HDD backed volumes are designed for better workloads that require a higher rate of throughput, such as processing big data and logging information. So, essentially working with larger blocks of data.</p>
<p>These volume types can be broken down even further. Looking at the following table we can see how different volumes can be used for both SSD and HDD volumes types.</p>
<p>You can see that depending on the use case for your EBS volume you can select the most appropriate type. Each of these volumes also offer different performance factors which include: Volume size, Max IOPS per volume, Max throughput per volume, Max IOPS per instance, Max Throughput per instance, and Dominant performance.</p>
<p>The performance of volumes change frequently and so for the latest information on these volume types, please refer to the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">here</a>.</p>
<p>For those who are not familiar with provisioned IOPS (input&#x2F;output operations per second) volumes, they deliver enhanced predictable performance for applications requiring I&#x2F;O intensive workloads. When working with these volumes you also have the ability to specify at IOPS rate during the creation of a new EBS volume, and when the volume is attached to an EBS-optimized instance, EBS will deliver the IOPS defined and required within 10%, 99.9% of the time throughout the year.</p>
<p>The throughput optimized HDD volumes are designed for frequently accessed data and are ideally suited to work well with large data sets requiring throughput-intensive workloads, such as data streaming, big data, and log processing. These volumes will deliver the expected throughput 99% of the time over a given year, and an important point to make is that these volumes can’t be used as boot volumes for your instances.</p>
<p>The cold HDD volumes offer the lowest cost compared to all other EBS volumes types. They are suited for workloads that are large in size and accessed infrequently. They will deliver the expected throughput 99% of the time over a given year, and again, it is not possible to use these as boot volumes for your EC2 instances.</p>
<p>One great feature of EBS is its ability to enhance the security of your data, both at rest and when in transit, through data encryption. This is especially useful when you have sensitive data, such as personally identifiable information, stored in your EBS volume. And in this case, you may be required to have some form of encryption from a regulatory or governance perspective. EBS offers a very simple encryption mechanism. Simple in the fact that you don’t have to worry about managing the data keys to perform the encryption process yourself. It’s all managed and implemented by EBS. All you are required to do is to select if you want the volume encrypted or not during its creation via a checkbox.</p>
<p>The encryption process uses the AES-256 encryption algorithm and provides its encryption process by interacting with another AWS service, the key management service, known as KMS. KMS uses customer master keys, CMKs, enabling the encryption of data across a range of AWS services, such as EBS in this instance.</p>
<p>To learn more about the Key Management Service, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Any snapshot taken from an encrypted volume will also be encrypted, and also any volume created from this encrypted snapshot will also be encrypted. You should also be aware that this encryption option is only available on selected instance types.</p>
<p>One final point to make on EBS encryption is that you can create a default region setting that ensures that all EBS volumes created will be encrypted by default.</p>
<p>For a detailed overview of exactly how this encryption process works, please take a look at the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/how-to-encrypt-an-ebs-volume-the-new-amazon-ebs-encryption/">blog post</a>.</p>
<p>As EBS volumes are separate to EC2 instances, you can create an EBS volume in a couple of different ways from within the management console. During the creation of a new instance and attach it at the time of launch, or from within the EC2 dashboard of the AWS management console as a standalone volume ready to be attached to an instance when required. When creating an EBS volume during an EC2 instance launch, at step four of creating that instance, you are presented with the storage configuration options. Here you can either create a new blank volume or create it from an existing snapshot. You can also specify the size and the volume type, which we discussed previously. Importantly, you can decide what happens to the volume when the instance terminates. You can either have the volume to be deleted with the termination of the EC2 instance, or retain the volume, allowing you to maintain the data and attach it to another EC2 instance. Lastly, you also have the option of encrypting the data if required.</p>
<p>You can also create the EBS volume as a standalone volume. By selecting the volume option under EBS from within the EC2 dashboard of the management console, you can create a new EBS volume where you’ll be presented with the following screen.</p>
<p>Here you will have many of the same options. However, you can specify which availability zone that the volume will exist in, allowing you to attach it to any EC2 instance within that same availability zone. As you might remember, EBS volumes can only be attached to EC2 instances that exist within the same availability zone.</p>
<p>EBS volumes also offer the additional flexibility of being able to resize them elastically should the requirement arise. Perhaps you’re running out of disk space and need to scale up your volume. This can be achieved by modifying the volume within the console or via the AWS CLI. You can also perform the same resize of the volume by creating a snapshot of your existing volume, and then creating a new volume from that snapshot with an increased capacity size.</p>
<p>As we can see, EBS offers a number of benefits over EC2 instance store volumes. But EBS is not well suited for all storage requirements. For example, if you only needed temporary storage or multi-instance storage access, then EBS is not recommended, as EBS volumes can only be accessed by one instance at a time. Also, if you needed very high durability and availability of data storage, then you would be better suited to use Amazon S3 or EFS, the elastic file system.</p>
<p>That now brings me to the end of this lecture and to the end of this introductory course and you should now have a greater understanding of the Amazon Elastic Block Store service and how it can be used as a storage option for your EC2 instances.</p>
<p>If you have any feedback, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="Working-with-EBS-Multi-Attach"><a href="#Working-with-EBS-Multi-Attach" class="headerlink" title="Working with EBS Multi-Attach"></a>Working with EBS Multi-Attach</h1><p>In this lecture, I shall be talking about the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/introduction-to-amazon-elastic-block-store-ebs-1060/course-introduction/">Amazon Elastic Block Store</a> service, known as EBS, which provides storage to your EC2 instances via EBS volumes, which offer different benefits to that of instance store volumes used with some EC2 instances.</p>
<p>EBS provides persistent and durable block level storage. As a result, EBS volumes offer far more flexibility with regards to managing the data when compared to data stored on instance store volumes. EBS volumes can be attached to your EC2 instances, and are primarily used for data that is rapidly changing that might require a specific Input&#x2F;Output operations Per Second rate, also known as IOPS.</p>
<p>EBS volumes are independent of the EC2 instance, meaning that they exist as two different resources. They are logically attached to the instance instead of directly attached like instance store volumes. From a connectivity perspective, only a single EBS volume can only ever be attached to a single EC2 instance. However, multiple EBS volumes can be attached to a single instance.</p>
<p>Due to the EBS ability to enforce persistence of data, it doesn’t matter if your instances are intentionally or unintentionally stopped, restarted, or even terminated, the data will remain intact when configured to do so. EBS also offers the ability to provide point in time backups of the entire volume as and when you need to. These backups are known as snapshots and you can manually invoke a snapshot of your volume at any time, or use Amazon CloudWatch events to perform an automated schedule of backups to be taken at a specific date or time that can be recurring.</p>
<p>The snapshots themselves are then stored on Amazon S3 and so are very durable and reliable. They are also incremental, meaning that each snapshot will only copy data that has changed since the previous snapshot was taken. Once you have a snapshot of an EBS volume, you can then create a new volume from that snapshot. So, if for any reason you lost access to your EBS volume through or incident or disaster, you can recreate the data volume from an existing snapshot and then attach that volume to a new EC2 instance. To add additional flexibility and resilience, it is possible to copy a snapshot from one region to another.</p>
<p>Looking at the subject of high availability and resiliency, your EBS volumes are, by default, created with reliability in mind. Every write to a EBS volume is replicated multiple times within the same availability zone of your region to help prevent the complete loss of data. This means that your EBS volume itself is only available in a single availability zone. As a result, should your availability zone fail, you will lose access to your EBS volume. Should this occur, you can simply recreate the volume from your previous snapshot and attach it to another instance in another availability zone.</p>
<p>There are two types of EBS volumes available. Each have their own characteristics. These being SSD backed storage, solid state drive, and HDD backed storage, hard disk drive. This allows you to optimize your storage to fit your requirements from a cost to performance perspective.</p>
<p>SSD backed storage is better suited for scenarios that work with smaller blocks. Such as databases using transactional workloads. Or often as boot volumes for your EC2 instances. Whereas HDD backed volumes are designed for better workloads that require a higher rate of throughput, such as processing big data and logging information. So, essentially working with larger blocks of data.</p>
<p>These volume types can be broken down even further. Looking at the following table we can see how different volumes can be used for both SSD and HDD volumes types.</p>
<p>You can see that depending on the use case for your EBS volume you can select the most appropriate type. Each of these volumes also offer different performance factors which include: Volume size, Max IOPS per volume, Max throughput per volume, Max IOPS per instance, Max Throughput per instance, and Dominant performance.</p>
<p>The performance of volumes change frequently and so for the latest information on these volume types, please refer to the <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> documentation found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html">here</a>.</p>
<p>For those who are not familiar with provisioned IOPS (input&#x2F;output operations per second) volumes, they deliver enhanced predictable performance for applications requiring I&#x2F;O intensive workloads. When working with these volumes you also have the ability to specify at IOPS rate during the creation of a new EBS volume, and when the volume is attached to an EBS-optimized instance, EBS will deliver the IOPS defined and required within 10%, 99.9% of the time throughout the year.</p>
<p>The throughput optimized HDD volumes are designed for frequently accessed data and are ideally suited to work well with large data sets requiring throughput-intensive workloads, such as data streaming, big data, and log processing. These volumes will deliver the expected throughput 99% of the time over a given year, and an important point to make is that these volumes can’t be used as boot volumes for your instances.</p>
<p>The cold HDD volumes offer the lowest cost compared to all other EBS volumes types. They are suited for workloads that are large in size and accessed infrequently. They will deliver the expected throughput 99% of the time over a given year, and again, it is not possible to use these as boot volumes for your EC2 instances.</p>
<p>One great feature of EBS is its ability to enhance the security of your data, both at rest and when in transit, through data encryption. This is especially useful when you have sensitive data, such as personally identifiable information, stored in your EBS volume. And in this case, you may be required to have some form of encryption from a regulatory or governance perspective. EBS offers a very simple encryption mechanism. Simple in the fact that you don’t have to worry about managing the data keys to perform the encryption process yourself. It’s all managed and implemented by EBS. All you are required to do is to select if you want the volume encrypted or not during its creation via a checkbox.</p>
<p>The encryption process uses the AES-256 encryption algorithm and provides its encryption process by interacting with another AWS service, the key management service, known as KMS. KMS uses customer master keys, CMKs, enabling the encryption of data across a range of AWS services, such as EBS in this instance.</p>
<p>To learn more about the Key Management Service, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/amazon-web-services-key-management-service-kms/kms-encryption-introduction/">here</a>.</p>
<p>Any snapshot taken from an encrypted volume will also be encrypted, and also any volume created from this encrypted snapshot will also be encrypted. You should also be aware that this encryption option is only available on selected instance types.</p>
<p>One final point to make on EBS encryption is that you can create a default region setting that ensures that all EBS volumes created will be encrypted by default.</p>
<p>For a detailed overview of exactly how this encryption process works, please take a look at the following <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/how-to-encrypt-an-ebs-volume-the-new-amazon-ebs-encryption/">blog post</a>.</p>
<p>As EBS volumes are separate to EC2 instances, you can create an EBS volume in a couple of different ways from within the management console. During the creation of a new instance and attach it at the time of launch, or from within the EC2 dashboard of the AWS management console as a standalone volume ready to be attached to an instance when required. When creating an EBS volume during an EC2 instance launch, at step four of creating that instance, you are presented with the storage configuration options. Here you can either create a new blank volume or create it from an existing snapshot. You can also specify the size and the volume type, which we discussed previously. Importantly, you can decide what happens to the volume when the instance terminates. You can either have the volume to be deleted with the termination of the EC2 instance, or retain the volume, allowing you to maintain the data and attach it to another EC2 instance. Lastly, you also have the option of encrypting the data if required.</p>
<p>You can also create the EBS volume as a standalone volume. By selecting the volume option under EBS from within the EC2 dashboard of the management console, you can create a new EBS volume where you’ll be presented with the following screen.</p>
<p>Here you will have many of the same options. However, you can specify which availability zone that the volume will exist in, allowing you to attach it to any EC2 instance within that same availability zone. As you might remember, EBS volumes can only be attached to EC2 instances that exist within the same availability zone.</p>
<p>EBS volumes also offer the additional flexibility of being able to resize them elastically should the requirement arise. Perhaps you’re running out of disk space and need to scale up your volume. This can be achieved by modifying the volume within the console or via the AWS CLI. You can also perform the same resize of the volume by creating a snapshot of your existing volume, and then creating a new volume from that snapshot with an increased capacity size.</p>
<p>As we can see, EBS offers a number of benefits over EC2 instance store volumes. But EBS is not well suited for all storage requirements. For example, if you only needed temporary storage or multi-instance storage access, then EBS is not recommended, as EBS volumes can only be accessed by one instance at a time. Also, if you needed very high durability and availability of data storage, then you would be better suited to use Amazon S3 or EFS, the elastic file system.</p>
<p>That now brings me to the end of this lecture and to the end of this introductory course and you should now have a greater understanding of the Amazon Elastic Block Store service and how it can be used as a storage option for your EC2 instances.</p>
<p>If you have any feedback, positive or negative, please do contact us at <a href="mailto:support@cloudacademy.com">support@cloudacademy.com</a>. Your feedback is greatly appreciated. Thank you for your time and good luck with your continued learning of cloud computing. Thank you.</p>
<h1 id="What-is-Amazon-Data-Lifecycle-Manager"><a href="#What-is-Amazon-Data-Lifecycle-Manager" class="headerlink" title="What is Amazon Data Lifecycle Manager?"></a>What is Amazon Data Lifecycle Manager?</h1><p>What is Amazon Data Lifecycle Manager? Time to get some definition stuff out of the way. As a quick recap, what are EBS volumes? Amazon Elastic Block Store isn’t easy to use scalable, high performance block storage service designed for Amazon Elastic Compute Cloud or Amazon EC2. What are snapshots? Snapshots are a convenient way to back up your block level data regardless of where it resides. These are point in time snapshots of an entire data on your EBS volume. Since this course isn’t about EBS or the creation of snapshots, we won’t be getting into too many details on the different types of storage available. But in order to demonstrate Data Lifecycle Manager, I’ll be walking through creating an EBS volume, creating a snapshot from that volume, along with how to create Amazon DLM policies.</p>
<p>Now, time to answer the question, what is Amazon DLM? DLM is Amazon Data Lifecycle Manager that allows you to automate the creation, deletion, and retention of EBS snapshots, and EBS backed AMI’s. Basically, backups for your EBS data. Back in the day we had to deploy an EC2 instance, install a script, add some access keys, set up a Windows scheduler or a cron job to create snapshots or remove them after a period of time. Amazon Data Lifecycle Manager is provided to you at no extra cost.</p>
<p>Now, keep in mind that EBS volumes and snapshots are all storage and will cost you for their use and size. Since the birth of AWS Cloud, one of the key cloud optimization tools that AWS has provided was tags, make sure you always tag your instances. AMI’s, Volumes, and Snapshots. If you aren’t tagging your resources, Amazon DLM won’t work, automation won’t work, and scheduling won’t work. DLM can also be used in conjunction with other AWS services. For example, Amazon CloudWatch Events. AWS CloudTrail to provide a complete backup solution for your EC2 instances. Now, this concludes the introductory lecture on what is Amazon DLM.</p>
<h1 id="Amazon-FSx"><a href="#Amazon-FSx" class="headerlink" title="Amazon FSx"></a>Amazon FSx</h1><p>Amazon FSx is another storage service that focuses on file systems, much like EFS. However, FSx comes in 2 forms, firstly Amazon FSx for Windows File Server, and also Amazon FSx for Lustre. Each FSx option has been designed for very different needs and requirements. We can summarize the differences as shown in the following table.</p>
<p>Amazon FSx for Windows File Server Provides a fully managed native Microsoft Windows file system on <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. You can easily move and migrate your windows-based workloads requiring file storage. The solution is built upon Windows Server, it operates as shared file storage. It has full support for: SMB protocol, Windows NTFS, Active Directory (AD) integration, and Distributed File System (DFS). And it uses SSD storage for enhanced performance and throughput providing sub-millisecond latencies.</p>
<p>Amazon FSx for Lustre is a fully managed file system designed for compute-intensive workloads, for example, Machine Learning and high-performance computing. It has the ability to process massive data sets. Performance can run up to hundreds of GB per second of throughput, millions of IOPS, and sub-millisecond latencies. It has integration with Amazon S3 and supports and supports cloud-bursting workloads from on-premises over Direct Connect and VPN connections.</p>
<p>As you can see, they are both designed to solve very different solutions depending on your workloads.</p>
<p>The pricing for each of these options operates in a different way, firstly, let’s take a look at the FSx for Windows, which has 3 price points: Capacity, Throughput, and Backups.</p>
<p>Again, much like EFS, there are no setup fees for the use of this service, however, you do pay for the amount of storage capacity that you consume. This is priced on the average storage provisioned per month and uses the metric of gigabyte-months and offers varied pricing between a single or multi-AZ deployment.</p>
<p>In addition to the actual storage that you use there is also a cost of for the amount of throughput that you configure for your file systems, this metric is based upon MBps-months. Again, cost variations exist between single and multi-AZ deployment. One point to bear in mind is that any data transfer costs when using multi-AZ is included in the pricing you see for the multi-AZ deployment.</p>
<p>Because Amazon FSx performs incremental backups (either manual or automatic) of your file systems, it optimizes your storage costs as only the changes since the last backup are saved. Much like your storage capacity, backup storage is also charged based on the average metric of gigabyte-months, meaning the average amount of capacity you have used in the month. </p>
<p>This table shows the cost for both single and multi-az deployments for each of the price points I just discussed. As you can see, there is a significant increase in cost should you need to deploy your FSx file system across multiple AZs.</p>
<p>One benefit that you currently get with Amazon FSx for Windows File Server over Amazon FSx for Lustre is that of data deduplication. This can be activated for your file system and can save you up to 80% is storage costs depending on what data you are storing. Data deduplication simply means FSx will automatically store duplicate files or portions of files a single time, this helps you save on your storage capacity costs and remains invisible to the user that the data has been deduplicated.</p>
<p>As you can see in the table, savings range from 30% - 80% depending on what type of content is being saved within your file system. Data deduplication doesn’t add any additional costs, so it’s a great way to help optimize and reduce your storage costs.</p>
<p>The costs associated with Amazon FSx for Lustre differs to that of Windows File Server and offers a simpler cost model.</p>
<p>Again, there are no set up fees and you only pay for the storage capacity that you use, based on the GB-month metric. For data transfer between availability zones you will pay the standard data transfer costs for the associated region.</p>
<p>Depending on your workload and use case for your data you will use either Windows Server or Lustre. The Windows Server option does offer more flexibility from a pricing point of view with the added advantage of data deduplication to help save up to 80% of storage costs. Although it does also have a cost associated with throughput capacity, which Amazon Lustre does not.</p>
<p>For more information on Amazon FSx for Lustre, please see our existing blog post <a target="_blank" rel="noopener" href="https://cloudacademy.com/blog/amazon-fsx-for-lustre-makes-high-performance-computing-more-accessible/">here</a>.</p>
<h1 id="AWS-Storage-Gateway"><a href="#AWS-Storage-Gateway" class="headerlink" title="AWS Storage Gateway"></a>AWS Storage Gateway</h1><p>Hello and welcome to this lecture. AWS Storage Gateway allows you to provide a gateway between your own data center’s storage systems such as your SAN, NAS or DAS and Amazon S3 and Glacier.</p>
<p>The Storage Gateway itself can either be installed as software or physical hardware appliance that can be stored within your own data center which allows integration between your on-premise storage and that of <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a>. This connectivity can allow you to scale your storage requirements both securely and cost-efficiently.</p>
<p>Storage Gateway offers different configurations and options allowing you to use the service to fit your needs. It offers file, volume and tape gateway configurations which you can use to help with your DR and data backup solutions, and each of these come with different price points.</p>
<p>File Gateways. File gateways allow you to securely store your files as objects within S3. This connectivity acts as a type of file share allowing you to mount or map drives to an S3 bucket as if the share was held locally on your own corporate network. In addition to this, a local on-premise cache is also provisioned for accessing your most recently accessed files to optimize latency which also helps to reduce egress traffic costs.</p>
<p>As this service integrates largely with Amazon S3, much of the pricing is based upon S3 price points. As we can see in the table below for the London region, there are 2 metrics of pricing associated with File Gateways, Storage pricing and Request pricing. As the storage used for this type of Gateway solely relies on S3, the actual storage costs are as per the Amazon S3 storage class price at the time of use which I reviewed in a previous lecture.</p>
<p>When we look at the request pricing there is a small cost per GB associated to data written to S3 by the Storage Gateway, up to a maximum of $125.00 per gateway per month. Also, the first 100 GB is free. All other request types again are as per Amazon S3’s pricing.</p>
<p>Volume Gateways. Volume Gateways can be figured in one of two different ways: Stored volume gateways and cached volume gateways.</p>
<p>Stored volume gateways are often used as a way to backup your local storage volumes to Amazon S3 as EBS snapshots whilst ensuring your entire data library also remains locally on-premise for very low latency data access. Volumes created and configured within the storage gateway are backed by Amazon S3 and are mounted as iSCSI devices that your applications can then communicate with.</p>
<p>With Cached volume gateways, the primary data storage is actually on Amazon S3 rather than your own local storage solution. However cache volume gateways utilize your local data storage as a buffer and the cache for recently accessed data to help maintain low latency, hence the name, Cache Volumes.</p>
<p>Although volume gateways still utilize Amazon S3, they do not, however, follow the S3 pricing mechanism like File Gateways do. Stored volume gateways create EBS snapshots, which are then stored on S3, however, they are billed as Amazon EBS snapshots. The cached volumes, however, are charged on a per GB-month metric of data stored.</p>
<p>Any requests are priced similarly to File gateways in that they are billed on a per GB basis of data written by the Gateway, up to a maximum of $125.00 per gateway per month. Also, the first 100 GB is free, in addition to any deletes to EBS volumes or snapshots also remain free.</p>
<p>Tape Gateways. The final option with AWS Storage Gateway is a tape gateway known as Gateway VTL. Virtual Tape Library. This allows you again to back up your data to S3 from your own corporate data center in addition to being able to leverage the storage classes within Glacier for data archiving for a far lower cost than S3. Virtual Tape Library is essentially a cloud-based tape backup solution replacing physical components with virtual ones.</p>
<p>From a cost point of view, you should be aware of Virtual Tapes. These are a virtual equivalent to a physical backup tape cartridge and any data stored on the virtual tapes are backed by AWS S3&#x2F;Glacier and appear in the virtual tape library. A Virtual Tape Library, VTL, as you may have guessed are virtual equivalents to a tape library that contain your virtual tapes.</p>
<p>Much like both the File and Volume gateways, the pricing is split across storage and request pricing.</p>
<p>There are 3 different options for Storage pricing of Tape Gateways and these are S3, S3 Glacier, and S3 Glacier Deep Archive. All of which are charged at per GB-month of data stored. Generally, if you are using Tape Gateways you are looking to take advantage of the very low price points of Glacier and Deep Archive which offer significant savings as you can see in the table.</p>
<p>Request pricing also offers a range of different cost metrics depending on the type of action and storage class used.</p>
<p>Much like File and Volume gateways, there is a small cost per GB associated to data written to S3 by the Storage Gateway, up to a maximum of $125.00 per gateway per month. Also, the first 100 GB is free.</p>
<p>For any virtual tape retrieval requests that are being stored on S3 Glacier classes, you will also pay a per GB cost, with Deep Archive providing a more expensive retrieval rate.</p>
<p>Any request that results in your moving your virtual tapes between your S3 Glacier storage class and S3 Glacier Deep Archive you will pay a fee per GB of data moved.</p>
<p>If you do select the Glacier storage classes and you delete your virtual tapes, within a set time period (90 days for S3 Glacier and 180 days for Deep Archive), then you will be charged a prorated charge per month per GB.</p>
<p>AWS Storage Gateway uses a variety of storage options, from Amazon S3, EBS Snapshots, Amazon S3 Glacier and S3 Glacier Deep Archive, and the cost of each is dependent on the type of gateway required which will, of course, be dictated by your use case.</p>
<p>File gateways pricing is very simple and essentially follows the pricing metrics of Amazon S3, apart from a per-GB request for data writes. Volume gateways again offer a simple pricing structure but uses the per-GB metric for volumes in addition to any snapshots of the volumes priced at EBS snapshot costings.</p>
<p>Tape gateways offer additional complexity due to the range of storage classes that it can use. So when using Tape gateways, understand the retrieval times for your data based on the S3 Glacier and Deep Archive classes as you might be able to save a considerable amount when using these classes depending on the criticality of the data you might need to retrieve.</p>
<p>For a deeper give on all things AWS Storage Gateway, please see our existing course <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/using-aws-storage-for-on-premise-backup/introduction-50/">here</a> which will dive deeper into this service and how each gateway type operates and works with regards to connectivity.</p>
<h3 id="Lectures-3"><a href="#Lectures-3" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/amazon-s3-and-glacier/">Amazon S3 and Glacier</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/amazon-elastic-file-system-efs/">Amazon Elastic File System</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/amazon-fsx/">Amazon FSx</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/amazon-backup/">Amazon Backup</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/costs-summary/">Summary</a></li>
</ul>
<h1 id="AWS-Backup"><a href="#AWS-Backup" class="headerlink" title="AWS Backup"></a>AWS Backup</h1><p>Hello and welcome to this lecture which will focus on AWS Backup. If you are new to this service, then at a high level, this is a solution to help you manage and implement backups across a number of different supported <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> services. For the latest supported services offered by AWS backup, please see the AWS documentation found <a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/aws-backup/latest/devguide/whatisbackup.html">here</a>. It is also possible to automate the backup of data from on-premises thanks to its connectivity with the AWS Storage Gateway service.</p>
<p>AWS Backup acts as a central hub to manage backups across your environment, across multiple regions, centralizing management and providing full auditability in addition to assisting with specific compliance controls. Having a managed service monitor and control your backups allows for all logging to be consolidated in a single place, in addition to seeing the status of completed backups and perform and restores required.</p>
<p>The service itself uses backup features from existing services, so for example, if you were to manage your EBS backups, AWS Backup would manage these through the EBS Snapshot feature as a way of performing the backup.</p>
<p>When using AWS Backup you will need to create backup policies or backup plans. These simply determine the exact requirements that you need for your backups and contain information such as:</p>
<ul>
<li>A backup schedule</li>
<li>Backup window</li>
<li>Lifecycle rules, such as the transition of data to cold storage after a set period</li>
<li>A backup vault, which is where your backups are stored and encrypted through the use of KMS encryption keys</li>
<li>Regional copies</li>
<li>Tags</li>
</ul>
<p>Once you have created your backup plans, you can then assign resources to them. This allows you to create multiple backup plans each with different criteria to meet the backup needs of different types of resources. Through the use of tags, you can associate multiple resources at once using tag-based backup policies, this ensures you capture all of the required resources at once within your plan.</p>
<p>From a cost perspective, the only real optimization available is when you are using services that support both warm and cold storage, where data is transitioned between the two via lifecycle rules which are configured within the backup plan. At the time of writing this course, the only service that supports the use of lifecycle policies is the Elastic File System, EFS. However, this will likely change over time so be sure to review the product pricing page.</p>
<p>Warm storage is backed by Amazon S3 storage providing millisecond access time. Cold storage is as expected backed by the Glacier storage class, with an approximate restore time of 3-5 hours offering a lower price point per GB-month than warm storage.</p>
<p>For backup storage when using AWS Backup, all charges use the metric GB-month, and depending on the resource type used, will depend on how much AWS Backup charges per GB-month.</p>
<p>However, with backup, comes the inevitable restore, and here there is also a cost implication. Again, for resources that support both warm and cold storage options, it is again more cost-effective when restoring from cold storage, however, the retrieval time is a lot longer. Much like Backup storage costs, all pricing where applicable is charged by GB-month, and I say where applicable as for many of the services it is free to restore your data as shown in the table.</p>
<p>AWS Backup is a great way to centralize, monitor and gain a bird’s-eye view of your backups from multiple regions. Through the use of Backup plans, you can configure different requirements for different resources and implement life-cycles rules for supported resources to help in the optimization of your backup costs through warm and cold storage, backed by Amazon S3 and Glacier respectively. There are two pricing points that you need to be aware of, those of Backup Storage, and then again the restoration of your data. When possible, implement life-cycle rules to ensure you gain maximum benefit of cheaper storage where possible.</p>
<h3 id="Lectures-4"><a href="#Lectures-4" class="headerlink" title="Lectures"></a>Lectures</h3><ul>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/introduction/">Introduction</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/amazon-s3-and-glacier/">Amazon S3 and Glacier</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/amazon-elastic-file-system-efs/">Amazon Elastic File System</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/amazon-fsx/">Amazon FSx</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/aws-storage-gateway/">AWS Storage Gateway</a></li>
<li><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/understanding-optimizing-costs-with-aws-storage-services/costs-summary/">Summary</a></li>
</ul>
<h1 id="What-is-the-Snow-Family"><a href="#What-is-the-Snow-Family" class="headerlink" title="What is the Snow Family?"></a>What is the Snow Family?</h1><p>In this lecture I want to answer 2 simple questions:</p>
<ol>
<li>What is the snow family </li>
<li>and what does it consist of?</li>
</ol>
<p>So firstly, what is it? The snow family consists of a range of physical hardware devices that are all designed to enable you to transfer data into <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> from the edge or beyond the Cloud, such as your Data Center, but they can also be used to transfer data out of AWS too, for example, from Amazon S3 back to your Data Centre. </p>
<p>It’s unusual when working with the cloud to be talking about physical devices or components, normally your interactions and operations with AWS generally happen programmatically via a browser or command line interface. The snow family is different, instead, you will be sent a piece of hardware packed with storage and compute capabilities to perform the required data transfer outside of AWS, and when complete, the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">device</a> is then sent back to AWS for processing and the data uploaded to Amazon S3.</p>
<p>You can perform data transfers from as little as a few terabytes using an AWS snowcone all the way up to a staggering 100 petabytes using a single AWS snowmobile, and I’ll be talking more about these different snow family members shortly. Now of course when we are talking about migrating and transferring data at this magnitude, using traditional network connectivity is sometimes simply not feasible from a time perspective. For example, let’s assume you needed to transfer just 1petabye of data over a 1gbps using Direct Connect it would take 104 Days, 5 Hours 59 Minutes, 59.25 Seconds, not forgetting the cost of the data transfer fees too! </p>
<p>In addition to these devices packing some serious storage capacity for data transfer, some of them also come fitted with compute power, allowing you to run usable EC2 instances that have been designed for the snow family enabling your applications to run operations in often remote and difficult to reach environments, even without having a data center in sight, and when working with a lack of persistent networking connectivity or power. For example, the snowcone comes with the ability to add battery packs increasing their versatility. The enablement of running EC2 instances makes it possible to use these devices at the edge to process and analyze data much closer to the source.   </p>
<p>So let’s now take a look at what the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/introduction/">snow family</a> consists of to get a better understanding of what these devices are.</p>
<p>As you can see from this table, both from a physical and capacity perspective, the snowcone is the smallest followed by the snowball and finally the snowmobile. You may also notice that the snowball comes in 3 choices, compute optimized, compute optimized with GPU, and storage optimized, each offering a different use case, however, each of these 3 offerings all come in the same size device. </p>
<h1 id="Which-Snow-Device-Do-I-Need"><a href="#Which-Snow-Device-Do-I-Need" class="headerlink" title="Which Snow Device Do I Need?"></a>Which Snow Device Do I Need?</h1><p>In this lecture, I will be looking at different scenarios to help you decide which <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/introduction/">snow device</a> to use and when.</p>
<p>So we have the following snow devices AWS Snowcone, AWS Snowball, and AWS Snowmobile. </p>
<p>The AWS Snowcone is the smallest of the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/what-is-the-snow-family/">snow family</a>, this has been designed to be lightweight, easily portable, allowing you to easily use the device pretty much anywhere and under any conditions due to the ruggedness of the casing, and the added advantage of being able to run on battery should a persistent mains connection not be available. It can easily fit into a standard backpack, and <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> have even demonstrated that the snowcone can be attached to a drone emphasising its portability and versatility. Packed with 8TB of storage and an EC2 instance, this device is perfect for taking your computing needs way beyond the cloud and your Data Centre allowing you to capture, process, and analyze data, perhaps via IoT sensors, which can then be shipped back to AWS for data transfer, or you could even use AWS DataSync to transfer the data on-line over your traditional network connectivity. </p>
<p>For those unaware of AWS DataSync, it’s a service that allows you to easily and securely transfer data from your Snowcone or your on-premise data center, to AWS storage services. It can also be used to manage data transfer between 2 different AWS storage services too, so it’s a great service to help you migrate, manage, replace and move data between different storage locations.  </p>
<p>This is essentially the elder sibling of the Snowcone, it’s bigger in size and it contains a greater amount of storage and compute power. This brings a new set of use cases for this device, it’s primarily used for large scale data transfer operations, up to 80TB at a time, both in and out of AWS. The devices themselves can be rack mounted in your data centre, and if need be clustered in groups of 5-10 devices. Unlike Snowcones, they can’t be powered by battery expansion packs, and they are not as portable, for example, you can’t stick a snowball in a backpack and walk up a mountain, or strap it to a drone! </p>
<p>The Storage optimized snowball is targeted for data migrations and transfers with its storage being compatible with both S3 object storage and EBS volumes. The Compute optimized snowball is a great option if you need to handle compute intensive edge computing workloads in disconnected environments. From a storage perspective, it also comes with 42 TB of usable HDD capacity which comes compatible with EBS volumes and S3 object storage. The Compute Optimized with GPU option is used to accelerate AI, HPC, and graphics, which is great when working with video analysis and graphic intensive use cases. </p>
<p>Both the Snowcone and Snowball can be used for many of the same use cases which I will reference in just a moment, so if that’s the case, when would you use the snowcone over the snowball and vise versa?</p>
<p>You would use the snowcone if you:</p>
<ul>
<li>Needed a portable device that you could easily carry to difficult to reach locations and situations</li>
<li>Only needed a maximum of 8TB storage</li>
<li>If you needed the ability to perform on-line data transfer using AWS DataSync, preventing you the need to send the Snowcone back to AWS for an off-line data transfer</li>
<li>If you didn’t have a consistent power support and you needed the support of a battery pack</li>
</ul>
<p>Alternatively, you would use the Snowball device if you: </p>
<ul>
<li>Didn’t need to provide mobility to the snow device and it could remain in one location for a set period of time</li>
<li>Needed to transfer data of up to 80TB</li>
<li>Needed the ability to run enhanced graphics processing by using the Compute Optimized with GPU option</li>
<li>Had a requirement to transfer data using S3 API’s</li>
<li>Required the use of usable SSD Storage </li>
<li>Needed to optimized network ports that could reach speeds of up to 100Gbit, as Snowcones only have network port speeds of 10Gbit</li>
<li>Needed to cluster your snowballs. Clustering allows you to order between 5-10 snowball devices, acting as a single pool of resources. This allows you to gain a larger storage capacity, and also enhance the level of durability of the data should a snowball fail.  Clustering is only an option if you are looking to simply perform local compute and storage workloads without transferring any data back to AWS.</li>
<li>Needed to rack mount your devices providing the opportunity to implement temporary installations of both compute and storage</li>
<li>Required the snow device to be HIPAA compliant</li>
</ul>
<p>Ok, so that should provide a better understanding of how the Snowcone and Snowball differ. Let me now run through a couple of scenarios of when you might use these devices in the real world.</p>
<p>Both the snowcone and Snowballs are perfectly suited to provide a level of portable edge computing allowing you to collect data from wireless sensors or networked resources, for example in locations such as industrial warehouses or manufacturing plants, where you might need to collect environmental metric data. By collecting and gathering data it can then be transferred to AWS offline, or if using the snowcone it can be transferred on-line using AWS DataSync, which can then be analyzed at scale using other AWS services.</p>
<p>With storage capabilities of up to 80TB of usable HDD storage from a single snowball, it easily allows you to provide a means of securely storing and transferring a large amount of data into AWS, and you can run multiple snowballs in parallel allowing you to transfer petabytes of data if required. Being of rugged design and portable, the devices can be used in remote locations, such as mining and oil sectors, or even in the travel industry, fitted to trucks, trains, and boats, providing a mechanism of easily collecting data and then transferring it back to AWS.</p>
<p>Another common use case is from within the media and entertainment industry, the Snowcone and snowball can be used as a way to aggregate data from multiple sources before shipping it back to AWS for transfer into Amazon S3. You might get video and audio data from multiple feeds, especially if you are working in the film industry, this data can then be aggregated to your snowball device and shipped back to AWS for further processing and editing from your wider production team.</p>
<p>So we have covered Snowball and Snowcone, but let’s now turn the attention to the AWS Snowmobile, what is the primary use case for this? Well, it’s quite simple, the AWS Snowmobile is used to transfer MASSIVE amounts of data from a single location, up to 100PB per snowmobile which arrives on a truck as a ruggedized shipping container. When you are talking about data transfer of this scale you are normally looking at migrating entire data centers to a new location, or migrating entire storage libraries or repositories, and so AWS snowmobile is a great solution to help you with this when you need it done quickly, securely and cost-efficiently. You can run multiple snowmobiles in parallel which will allow you to transfer Exabytes of data! Generally, you would use AWS snowmobiles if you needed to transfer more than 10petabytes of data, anything less than this then you might want to consider using multiple snowball devices. </p>
<h1 id="Key-Features-of-the-Snow-Family"><a href="#Key-Features-of-the-Snow-Family" class="headerlink" title="Key Features of the Snow Family"></a>Key Features of the Snow Family</h1><p>The snowcone and the snowball are similar in their size and construction with regards to the casing compared to that of the snowmobile which is in a class of its own, I mean it comes on a Truck! </p>
<p>So let me talk about the Snowcone and Snowball for a moment. These two hardware appliances share some key features between them that I want to highlight. </p>
<p>When transferring data to these devices it is automatically encrypted to protect the data. This encryption is backed by keys generated by the Key Management Service (KMS). To enhance the security of the device, the encryption keys are not stored on the device during transit. If you would like to learn more about the Key Management Service, then you can see our existing course here. </p>
<p>Following on from encryption, another security feature that these devices have in common is that their enclosure is Anti-tamper in addition to specific verification checks on the boot environment when the device is first switched on. These measurements and checks help to validate the integrity of the data to ensure that it has not been interfered with at all during transit.</p>
<p>In addition to their enclosure being secure by design, it is also highly rugged and able to withstand the harshest of environments, for example, the snowcone can operate in conditions of -32ºC&#x2F;-25.6ºF to 63ºC&#x2F;145.4ºF. It’s all windproof, dustproof and water-resistant! The enclosures are designed to withstand operational vibrations and shockproof should you drop the device.</p>
<p>Each of the snow devices uses an E Ink shipping label which is pre-loaded with the delivery details entered on the associated job. When the <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/running-operations-large-scale-data-transfer-non-aws-environments-aws-snow-family-1741/which-snow-device-do-i-need/">snow device</a> leaves AWS premises it can be tracked via SNS, text messaging and the AWS Management Console. When the device is prepared to be returned to AWS premises, the E Ink automatically updates with the appropriate location</p>
<p>As these devices are packed full of data, it’s essential that the data is deleted once the transfer has been completed and the snow device is no longer required. As a result, <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> carries out a secure erase which meets the National Institute of Standards and Technology, more commonly known as NIST for the sanitization of the media and storage. </p>
<p>Before I finish this lecture I just want to point out a couple of features related to the snowmobile. </p>
<p>Each snowmobile is sent with a connector rack allowing you to connect it to the backbone network of your own data center, as a result, this rack comes with up to 2 kilometers of network cabling.</p>
<p>The Snowmobile is designed to operate within ambient temperature up to 85F (29.4C). If the temperature exceeds this then an additional auxiliary chiller unit can be supplied by AWS following a site assessment survey. If there isn’t sufficient power to feed the snowmobile at the data center then AWS can also send a separate generator to power the snowmobile, however, this requires the same space required to home a snowmobile.</p>
<p>As with the Snowcone and snowball, the snowmobile also encrypts data backed by the Key Management Service. Also, the snowmobile is only ever operated by AWS personnel and can also be escorted by an additional security vehicle during the transit of the container to and from premises, in addition to having GPS tracking available. As added security, the container is also protected via 24&#x2F;7 video surveillance systems and alarms.</p>
<h1 id="What-is-AWS-DataSync"><a href="#What-is-AWS-DataSync" class="headerlink" title="What is AWS DataSync?"></a>What is AWS DataSync?</h1><p>In this lecture I am going to answer the question, what is AWS DataSync? If you’ve been working with different AWS storage services for any length of time then you may have already come across this service. AWS DataSync is a service that allows you to easily and securely transfer data from your on-premise data center to <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> storage services. It can also be used to manage data transfer between 2 different AWS storage services too, so it’s a great service to help you migrate, manage, replace and move data between different storage locations. </p>
<p>At the time of writing this course, <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/introduction/">AWS DataSync</a> supports the ability to work with data stored on Network File Systems shares, Server Message Block shares, and any self-managed object storage, in addition to the following AWS services:</p>
<ul>
<li>Amazon S3</li>
<li>Amazon Elastic File System</li>
<li>Amazon FSx for Windows File Server</li>
<li>AWS Snowcone</li>
</ul>
<p>When performing data transfer operations, whether this be from on-premises or between AWS storage services, DataSync support AWS VPC Endpoints and so its able to utilise the high bandwidth, low latency AWS network to it’s advantage, this helps to both simplify the management of the request and automate your data transfer across secure infrastructure. For more information on AWS Endpoints, please see our AWS Networking lecture found here: <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/aws-networking-features-essential-for-a-solutions-architect/vpc-endpoints/">https://cloudacademy.com/course/aws-networking-features-essential-for-a-solutions-architect/vpc-endpoints/</a></p>
<p>With data transfer speed a being a key factor for a data transfer services, AWS Data Sync comes with its own purpose-built data transfer network protocol in addition to a parallel and multithreaded architecture to rapidly perform data transfer, this means that each DataSync task has the potential of utilizing 10 Gbps over a network link between your own on-prem data center and your AWS environment.</p>
<p>Obviously when working with data, especially when moving it between 2 points, security is a key concern. As a result AWS DataSync provides 2 levels that provide end-to-end security, these being encryption, in addition to data validation. </p>
<p>From an encryption perspective, encryption in transit is implemented by encrypting the data using the Transport Layer Security (TLS) protocol. When data reaches an AWS service, it also supports encryption at rest mechanisms that EFS and FSx for Windows service offers, in addition to the default encryption at rest option for Amazon S3.  </p>
<p>The 2nd point, Data Validation, ensures that your data arrives at its destination in one piece, exactly as it was when it left the source ensuring that it wasn’t compromised or damaged in any way during its transit. This additional check helps to validate the consistency of your data that was written to the AWS storage service, and that its a perfect match from when it left its source location.</p>
<p>From a cost perspective, AWS DataSync usesa flat pricing strategy based on a per-gigabyte of data transferred, this makes it easy to predict avoiding any unexpected costs.</p>
<h3 id="Lectures-5"><a href="#Lectures-5" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/aws-datasync-use-cases/">AWS DataSync Use Cases</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/aws-datasync-architecture/">AWS DataSync Architecture</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/summary/">Summary</a></p>
<h1 id="AWS-DataSync-Use-Cases"><a href="#AWS-DataSync-Use-Cases" class="headerlink" title="AWS DataSync Use Cases"></a>AWS DataSync Use Cases</h1><p>Let me run through a couple of scenarios whereby you might choose to use <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/introduction/">AWS DataSync</a> in a live environment. </p>
<p>Firstly one that will probably be familiar to a lot of people, and that is archiving data into cold storage. You may have been running your own on-premise storage services for a long time, and you have a lot of data residing in that storage architecture that would be better suited for cloud storage, in particular that of the Amazon S3 Glacier storage classes, perhaps Glacier Deep Archive which is the cheapest storage solution provided by AWS primarily used for archiving rarely accessed data. By utilising AWS DataSync you can schedule a task to migrate this data from your own data centre into one of the Glacier storage services. This then ensures the durability of the data at a very low cost without the worry of maintaining that data yourself on-premises. This would also likely allow you to remove any old and perhaps legacy storage solution that you were using to store this data yourself. </p>
<p>The next use case might be related to the need to implement steady and active data migrations to <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">Amazon</a> S3, Amazon FSx for Windows File Server or EFS on a regular basis. Depending on your workload will depend on which service you are migrating to. You might want to consider this option for potential backup purposes or DR, for example you might migrate data you EFS for a standby file system in the cloud should your primary on-site file system suffer an outage.  </p>
<p>Finally, let’s assume you are utilising a hybrid cloud solution, utilising services and solutions both on premises and in AWS. Occasionally you might need to harness the power and speed that many of the AWS services have, this is especially true if you are working with Machine learning or trying to process data sets in a short period of time. You could migrate the data into AWS for this additional processing and analysis and then migrate data back with any results into your Data Centre when your operations have completed.</p>
<h3 id="Lectures-6"><a href="#Lectures-6" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/what-is-aws-datasync/">What is AWS DataSync?</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/aws-datasync-architecture/">AWS DataSync Architecture</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/summary/">Summary</a></p>
<h1 id="AWS-DataSync-Architecture"><a href="#AWS-DataSync-Architecture" class="headerlink" title="AWS DataSync Architecture"></a>AWS DataSync Architecture</h1><p>To understand how <a target="_blank" rel="noopener" href="https://cloudacademy.com/library/amazon-web-services/">AWS</a> DataSync works at an operational level, we first need to understand how the architecture of the service is put together and the different components involved to carry out a DataSync task.</p>
<p>This table shows the current source and destination locations that DataSync can transfer files from and to.</p>
<p>And so based on this I want to look at the architecture of DataSync from two different perspectives, these being:</p>
<ul>
<li>When transferring data from your own managed storage environment to AWS</li>
<li>And secondly when transferring data between 2 different AWS storage services, such as Amazon S3 to Amazon EFS</li>
</ul>
<p>So, let’s look at the first scenario first whereby we have our own self-managed storage solution on-premises and we need to use AWS DataSync to move this data into Amazon S3, so what’s involved? </p>
<p>When performing data transfer from on-premises then we need to configure an <strong>Agent</strong>, a <strong>Location</strong> and a <strong>Task</strong>.</p>
<p>The Agent will be used on the customer side, so it sits outside of AWS, and it’s just a virtual machine supported by VMware ESXi, KVM or Microsoft Hyper-V hypervisors, so it should be compatible with your existing infrastructure. The agent itself is used to both read and write data to your own storage solution and can be generated and configured from within the AWS Management dashboard which can then be downloaded. </p>
<p>Next we have a location, and the location identifies the endpoint of a DataSync task. So as a result, everytime you create a DataSync task you will need to specify the source location and the destination location, dictating where you want to move data from and to. You can create locations for:</p>
<ul>
<li>Network File Systems (NFS)</li>
<li>Server Message Blocks (SMB)</li>
<li>Self-managed object storage</li>
<li>Amazon EFS</li>
<li>Amazon FSx for Windows File Server</li>
<li>Amazon S3</li>
</ul>
<p>Again, these locations can be configured from within the AWS Management Console.</p>
<p>The task contains the details of the operation that you are trying to carry out and perform with DataSync, so it will contain the locations that were created and specified for both the source and destination, in addition to the configuration and conditions of how the data transfer will take place. </p>
<p>For example, you can configure the type integrity and data verification checks to take place, or if you want to transfer all data in the source location, or just data that has changed since the last task was performed. You can also specify if you want to overwrite or delete files.</p>
<p>If you only want to transfer specific files from the source, then you can apply pattern filters enabling you to restrict which files to include or exclude from the transfer in the source location. </p>
<p>Finally, you can also specify logging details which integrate with Amazon CloudWatch Logs to help you identify any failures or errors. For more information on Amazon CloudWatch, please see our existing courses here: </p>
<p><strong>An overview of Amazon CloudWatch:</strong> <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/an-overview-of-amazon-cloudwatch-1222/?context_resource=lp&amp;context_id=40">https://cloudacademy.com/course/an-overview-of-amazon-cloudwatch-1222/?context_resource=lp&amp;context_id=40</a></p>
<p>So the taks essentially outlines exactly what will happen during the data transfer process.</p>
<p>From an architecture perspective, the process looks as shown here.</p>
<p>We have the on-premises server holding our storage data with the AWS DataSync agent installed as a virtual machine. Two locations would have been created, with the source pointing to the on-premises server and the destination to an S3 bucket. The task would then be configured to transfer the data conforming to the setting configured within the task, and when it runs AWS DataSync will transfer the data using encryption-in-transit over TLS to the Amazon S3 destination bucket. All logging information would then be stored in Amazon CloudWatch if configured to do so.</p>
<p>Before I move on, I just want to highlight that the DataSync tasks will only copy your storage data, it doesn’t include any file systems permissions or settings.</p>
<p>Let me now quickly explain how the process works if you were to transfer data from one AWS storage service to another, for example, from Amazon S3 to Amazon EFS.</p>
<p>This time, let’s look at the infrastructure to begin with.</p>
<p>As you can see, in this process we do not use the Agent, however, we will continue to create 2 locations, a source location for Amazon S3 and a destination location for EFS. Also in addition to these locations, we will also have to create a Task. So the process remains very similar to that of when transferring data from on-premises, however, we don’t need to use the DataSync agent. </p>
<p>Again, it’s important to note that when using DataSync it will not copy and configuration relating to the source storage option, for example if you were to copy from one S3 bucket to another S3 bucket, then it would only move the data, it would not copy any bucket-level settings or permissions.</p>
<h3 id="Lectures-7"><a href="#Lectures-7" class="headerlink" title="Lectures"></a>Lectures</h3><p><a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/introduction/">Introduction</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/what-is-aws-datasync/">What is AWS DataSync?</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/aws-datasync-use-cases/">AWS DataSync Use Cases</a> - <a target="_blank" rel="noopener" href="https://cloudacademy.com/course/performing-data-transfers-using-aws-datasync-1700/summary/">Summary</a></p>
<h1 id="Storage-Summary"><a href="#Storage-Summary" class="headerlink" title="Storage Summary"></a>Storage Summary</h1><p>The “Storage” section is now complete. So, a great welcome making it this far. So, what did we cover? We looked at Amazon S3, the Elastic File System, also our old favorites from the compute course, EBS and EC2 instance storage. And we also touched on Amazon Fsx, Storage Gateway and Amazon Backup. Now in the exam prep, I want to use this time to ramp up and helping you prepare and pass any questions on storage. </p>
<p>So, we touching on some of the most common elements that you might see in the exam. So, let’s take a look. So, let me start with Amazon S3. Now you need to know this service pretty well as you’ll be definitely getting a few questions on this service. So firstly, some key points: Is highly available, highly durable, very cost-effective, and widely accessible. It’s great for use cases such as data lakes, data backups, building websites, and much more. However, there are some key elements that you do need to know. I would say without hesitation you’ll experience some sort of question that we’ll reference to storage classes that exist, and these are usually relating to cost optimization or the speed of data retrieval. </p>
<p>Now, remember that Glacier Storage classes are designed for long-term data storage providing the most cost-optimized solution. But the drawback is, that they do not offer instant data retrieval. Whereas S3 Storage classes, do offer that instant data retrieval but are more expensive as a result. Now, as I discussed in the previous course there are a number of different storage classes available for S3 but you need to have an insight into when to use one over the other for optimization and news case point. </p>
<p>For example, if you had a workload that provided unpredictable pattern access and looking to provide a cost-effective storage solution on S3, you might use S3 Intelligent Tiering over Standard. Or, if you wanted instant access to objects for the lowest cost point where your data could be easily reproduced, if lost, then you would use S3 One Zone Infrequent Access. So, let’s look at a question where knowledge of storage classes comes into play.</p>
<p>Now, storage classes aren’t the only element of S3 that you need to understand to be prepared for questions covering S3. You should certainly understand S3 Versioning, Lifecycle Rules, Transfer Acceleration, and Basic Security Controls. </p>
<p>You’ll be expected to be able to determine when it’s best to use Versioning and Lifecycle Rules to manage your data on S3. So, you might be given a scenario about how you need to keep data highly accessible for 90 days after which it won’t be needed to be accessed anymore, but it will be needed to keep for legal reasons. So, what would you implement to enforce this behavior? Would you add Versioning? Well, no, because this is used to allow to cover from previous versions if changes to your objects are made or if they are deleted. </p>
<p>Would you use Lifecycle Rules? Yes, certainly, this provides an automatic method of moving your data between storage classes based on time periods. So, you can move your data from S3 Standard to S3 Glacier. Now, one final point on S3, before we look at a question is to ensure that you familiarize yourself with the options to control access to your S3 buckets. Now, you can either use identity-based policies through IAM, resource-based policies using bucket policies, S3 Access Control Lists, in-built public protection settings on the bucket or Cross Origin Resource Sharing.</p>
<p>Next, we looked at the Elastic File System and this is a scalable network file storage service for use with Amazon EC2 instances that can easily scale to petabytes in size with low latency access providing support to thousands of EC2 instances at once. So, this is very different from S3. Where S3 is used for object storage, EFS is used as file system storage. Again, however, it does have storage classes and varied performance options for you to optimize your file system with. </p>
<p>So, make sure you know the difference between the Standard and Infrequent storage classes in addition to performance modes including general purpose and MAX I&#x2F;O, but also the throughput modes of Bursting Throughput and Provisioned Throughput as well. So, just have a recap of those and just understand when you might use each of those individually. Now, you might receive questions asking you to select the most appropriate performance and throughput mode based on a particular workload. Now, knowing these difference will help you quickly and easily eliminate any wrong answers and help you find the correct answer. </p>
<p>I would also recommend you just understand some of the underlying architecture from a connectivity perspective. So, familiarize yourself with mount points and the part that they play with how your EC2 instances connect EFS in using these mount points. Also, if you receive any questions relating to encryption with EFS, then remember it offers both encryption at rest backed by KMS like most AWS services, but importantly, it also supports in transit encryption too which can be configured during the mounting process.</p>
<p>Okay, so moving on from EFS, we also looked at the Elastic Block Store and we cover this in the compute section as well, but in this section would cover it at a greater depth. Looking into the service as a whole not just from an EC2 stand point. So right off the bat, the key points for EBS to remember for the exam are that: It is persistent data. Meaning the data will not be lost if you terminate the instance that the EBS filling is attached to. It’s a really flexible storage option for your EC2 instances. And your knowledge of this flexibility will be assessed in the exam. </p>
<p>One element to really focused on for the certification are the EBS snapshots; how they work, where they are stored, and also how they work when encryption is applied. This is all covered in the course. So, ensure you understand these key points. Now you might be presented with a scenario where you have an unencrypted EBS volume that now needs to be encrypted within a different region. How would you go about doing this? Well, one option will be is to take a snapshot of the volume, copy this snapshot to the right region, and then create a new volume from this copied snapshot and select encryption during the volume creation. </p>
<p>So, you need to understand what you can and can’t do with a snapshot. For example, you can’t create an unencrypted volume from an encrypted snapshot. Similarly, you can’t create an unencrypted snapshot from an encrypted volume.</p>
<p>Okay, moving on. We also looked at FSx at a high level. It’s not mentioned on the exam at any significant level but you should be aware of it, and what it is, and when you might use it. So remember, then it’s another file system storage service much like EFS, but also note that FSx comes in two flavors. FSx for Windows which provides a Windows File Server used as a fully managed native Windows File system on AWS, and it uses the Server Message Block protocol, SMB. The other being FSx for Lustre which is a fully managed Linux-based File System, and this is designed for compute intensive workloads and high performance computing. </p>
<p>So, as long as you remember those components for the exam you shouldn’t need much more than that when it comes to FSx. Now, the final service I want to talk about is AWS Storage Gateway. Again, there are some key points to focus on. Firstly, is use case. It provides a gateway between your own data center storage systems and Amazon S3, and Glacier. For giving you a hybrid storage solution with unlimited space. Now, secondly, there were three types of gateway that you need to know. The file, volume, and tape gateway configuration. Now in the exam, you’ll be given a scenario and asked which solution would be best based on specific criteria. So, ensure you have a good understanding of the differences between the three.</p>
<p>So, just a quick wrap up before we move on to the next section. If you get any questions about unlimited object storage think S3 and about storage classes and how Glacier is used for long-term data storage. How you can use Lifecycle policies and Versioning to help with data management. Transfer acceleration for getting data into S3 faster. If you get any questions about persistence of data with EC2 instances, think EBS volumes, block storage, EBS snapshots as backups, and encryption is also possible. If questions appear related to network file systems, think EFS running the NFS protocol. </p>
<p>Mount points for connecting your EC2 instances, multiple availability zones, encryption in transit and at-rest, and thousands of concurrent connections. If any questions relate to Windows file systems using the Server Message Block protocol, think Amazon FSx for Windows, or if anything relates to file systems for high-performance computing using Linux instances think Amazon FSx Lustre. Now lastly, if any scenarios appear talking about backing up data between your own corporate data center and AWS using S3 Glacier, think AWS Storage Gateway either using File, Volume or Tape Gateways. Okay, that’s it for me, now you’re ready to tackle the next section.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-2-of-2-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-2-of-2-8/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-2-of-2-8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:11" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:11-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:09:18" itemprop="dateModified" datetime="2022-11-27T20:09:18-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-2-of-2-8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-2-of-2-8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Compute-SAA-C03-2-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-1-of-2-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-1-of-2-7/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-1-of-2-7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:09" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:09-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:09:10" itemprop="dateModified" datetime="2022-11-27T20:09:10-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-1-of-2-7/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Knowledge-Check-Compute-SAA-C03-1-of-2-7/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><object data="Knowledge-Check-Compute-SAA-C03-1-of-2.pdf" type="application/pdf" width="100%" height="600"></object></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-AWS-Lambda-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-AWS-Lambda-6/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Introduction-to-AWS-Lambda-6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:08" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:08-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:17:30" itemprop="dateModified" datetime="2022-11-27T20:17:30-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-AWS-Lambda-6/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Introduction-to-AWS-Lambda-6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Working-with-Amazon-EC2-Auto-Scaling-Groups-and-Network-Load-Balancer-5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Working-with-Amazon-EC2-Auto-Scaling-Groups-and-Network-Load-Balancer-5/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Working-with-Amazon-EC2-Auto-Scaling-Groups-and-Network-Load-Balancer-5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:06" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:06-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:10:00" itemprop="dateModified" datetime="2022-11-27T20:10:00-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Working-with-Amazon-EC2-Auto-Scaling-Groups-and-Network-Load-Balancer-5/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Working-with-Amazon-EC2-Auto-Scaling-Groups-and-Network-Load-Balancer-5/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://example.com/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-EC2-Instance-Linux-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/123456.gif">
      <meta itemprop="name" content="Hang Zhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hang's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-EC2-Instance-Linux-4/" class="post-title-link" itemprop="url">AWS-Solution-Architect-Associate-Create-Your-First-Amazon-EC2-Instance-Linux-4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-11-18 22:13:05" itemprop="dateCreated datePublished" datetime="2022-11-18T22:13:05-04:00">2022-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-11-27 20:12:10" itemprop="dateModified" datetime="2022-11-27T20:12:10-04:00">2022-11-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS-Solution-Architect-Associate/" itemprop="url" rel="index"><span itemprop="name">AWS-Solution-Architect-Associate</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-EC2-Instance-Linux-4/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/11/18/AWS-Solution-Architect-Associate-Create-Your-First-Amazon-EC2-Instance-Linux-4/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/51/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/51/">51</a><span class="page-number current">52</span><a class="page-number" href="/page/53/">53</a><span class="space">&hellip;</span><a class="page-number" href="/page/274/">274</a><a class="extend next" rel="next" href="/page/53/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Hang Zhang"
      src="/images/123456.gif">
  <p class="site-author-name" itemprop="name">Hang Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2736</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zh375636" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zh375636" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hang.zhang.network@gmail.com" title="E-Mail → mailto:hang.zhang.network@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hang Zhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>



// 在最后添加
<script src="/js/code-unfold.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : '40GUvOAaitQh1SZe7ua9cvss-MdYXbMMI',
      appKey     : 'iUO2yON0j55WMdu59zx12dCG',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
